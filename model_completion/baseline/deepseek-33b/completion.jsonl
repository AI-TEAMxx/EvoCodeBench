{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"\\n{cmd}\\n\")\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f'Executing command: {cmd}')\n    exit_status = os.system(cmd)\n    print(f'Command exited with status: {exit_status}')\n    return exit_status\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    # Check if the input is a NumPy array\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input must be a NumPy array.\")\n\n    # Check if the input is a square matrix\n    if x.shape[0] != x.shape[1]:\n        return False\n\n    # Check if the input is Hermitian\n    if not np.allclose(x, x.conj().T):\n        return False\n\n    # Check if the input is positive definite\n    if not np.all(np.linalg.eigvals(x) > 0):\n        return False\n\n    try:\n        # Attempt to perform Cholesky decomposition\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        return False\n\n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        return False\n\n    # Check if the matrix is positive definite\n    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "coord.inv_contract", "completion": "  return z / (1 + np.sqrt(1 + np.sum(z**2)))"}
{"namespace": "coord.inv_contract", "completion": "  # Compute the inverse of the contract function for the input vector\n  return z / (1 + np.abs(z))"}
{"namespace": "coord.inv_contract", "completion": "  z_inv = (z - 1) / (z + 1)\n\n  return z_inv"}
{"namespace": "coord.inv_contract", "completion": "  return z / (1 + np.abs(z))\n"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n  import math\n\n  if np.linalg.norm(z) == 0:\n    return z\n  else:\n    return z * (1 / math.sqrt(1 + np.linalg.norm(z)))"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  # Compute the inverse of the contract function\n  z_inv = np.exp(z) - 1\n\n  # Return the inverse of the contract function\n  return z_inv\n"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  # Compute the norm of the input vector\n  z_norm = np.linalg.norm(z)\n\n  # If the norm is less than or equal to 1, return the input vector as is\n  if z_norm <= 1:\n    return z\n  # If the norm is greater than 1, scale the input vector to have a magnitude of 1\n  else:\n    return z / z_norm\n"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  z = np.array(z)\n  z_norm = np.linalg.norm(z)\n  z_norm_inv = 1/z_norm\n  z_scaled = z * z_norm_inv\n  return z_scaled\n\n"}
{"namespace": "coord.inv_contract", "completion": "  return z / (1 + np.sqrt(1 + np.sum(np.square(z))))\n"}
{"namespace": "coord.inv_contract", "completion": "  return 2 * z / (1 + z**2)\n"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  z = np.array(z)\n  z = z / (1 + np.abs(z))\n\n  return z\n"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  z = np.array(z)\n  if z.ndim == 1:\n    z = z[:, np.newaxis]\n  return z / np.sqrt(np.sum(z**2, axis=0))\n\n"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  z_norm = np.linalg.norm(z)\n  if z_norm <= 1:\n    return z\n  else:\n    return z / (z_norm ** 2)\n"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  if np.linalg.norm(z) == 0:\n    return z\n  else:\n    return z / np.linalg.norm(z)\n"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n  import math\n\n  # Compute the norm of the input vector\n  norm = np.linalg.norm(z)\n\n  # Compute the inverse of the contract function\n  inv_contract_z = (1 / (1 - norm**2)) * z\n\n  return inv_contract_z\n\n"}
{"namespace": "coord.inv_contract", "completion": "  z = np.array(z)\n  if z.shape[0] == 1:\n    z = z.reshape(1, -1)\n  z_inv = np.zeros(z.shape)\n  for i in range(z.shape[0]):\n    z_inv[i] = np.sqrt(z[i]**2 + 1) - 1\n  return z_inv\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Compute the norm of the input vector\n  norm_z = np.linalg.norm(z)\n\n  # Scale the input vector to have a magnitude of 1/norm_z\n  return z / norm_z\n"}
{"namespace": "coord.inv_contract", "completion": "  # Initialize the output vector\n  z_inv = np.zeros_like(z)\n\n  # Compute the magnitude of the input vector\n  z_mag = np.linalg.norm(z)\n\n  # Scale the input vector based on the magnitude\n  if z_mag > 0:\n    z_inv = z / z_mag\n\n  return z_inv\n\n\n"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n  import math\n\n  z = np.array(z)\n  z_scaled = np.zeros(len(z))\n\n  for i in range(len(z)):\n    z_scaled[i] = z[i] * math.sqrt(2) / (1 + math.sqrt(1 + z[i]**2))\n\n  return z_scaled\n"}
{"namespace": "coord.inv_contract", "completion": "  # Compute the norm of the input vector\n  z_norm = np.linalg.norm(z)\n\n  # Check if the norm is greater than 1\n  if z_norm > 1:\n    # If so, scale the vector by a factor of 1/z_norm^2\n    return z / z_norm**2\n  else:\n    # Otherwise, return the original vector\n    return z\n\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    from functools import wraps\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\")\n            c.execute(\"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\", (func_name, str(args), str(kwargs)))\n            result = c.fetchone()\n            if result:\n                return result[0]\n            else:\n                result = func(*args, **kwargs)\n                c.execute(\"INSERT INTO cache VALUES (?, ?, ?, ?)\", (func_name, str(args), str(kwargs), result))\n                conn.commit()\n                conn.close()\n                return result\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\")\n            c.execute(\"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\", (func_name, str(args), str(kwargs)))\n            result = c.fetchone()\n            if result:\n                return result[0]\n            else:\n                result = func(*args, **kwargs)\n                c.execute(\"INSERT INTO cache VALUES (?, ?, ?, ?)\", (func_name, str(args), str(kwargs), result))\n                conn.commit()\n                conn.close()\n                return result\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n            cursor.execute(\n                \"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\"\n            )\n            cursor.execute(\n                \"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = cursor.fetchone()\n            if result:\n                return result[0]\n            else:\n                result = func(*args, **kwargs)\n                cursor.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, str(args), str(kwargs), result),\n                )\n                conn.commit()\n                conn.close()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import functools\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\n                \"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\"\n            )\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = c.fetchone()\n            if result:\n                return result[0]\n            else:\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, str(args), str(kwargs), result),\n                )\n                conn.commit()\n                conn.close()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import functools\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\")\n            key = (func_name, str(args), str(kwargs))\n            c.execute(\"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\", key)\n            result = c.fetchone()\n            if result:\n                return result[0]\n            else:\n                result = func(*args, **kwargs)\n                c.execute(\"INSERT INTO cache VALUES (?, ?, ?, ?)\", (func_name, str(args), str(kwargs), result))\n                conn.commit()\n                return result\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n            cursor.execute(\n                \"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\"\n            )\n            cursor.execute(\n                \"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = cursor.fetchone()\n            if result:\n                return result[0]\n            else:\n                result = func(*args, **kwargs)\n                cursor.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, str(args), str(kwargs), result),\n                )\n                conn.commit()\n                conn.close()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import pickle\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args BLOB, kwargs BLOB, result BLOB)\")\n            c.execute(\"SELECT result FROM cache WHERE func_name = ? AND args = ? AND kwargs = ?\", (func_name, pickle.dumps(args), pickle.dumps(kwargs)))\n            result = c.fetchone()\n            if result:\n                return pickle.loads(result[0])\n            else:\n                result = func(*args, **kwargs)\n                c.execute(\"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\", (func_name, pickle.dumps(args), pickle.dumps(kwargs), pickle.dumps(result)))\n                conn.commit()\n                conn.close()\n                return result\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\n                \"CREATE TABLE IF NOT EXISTS cache (func_name text, args text, kwargs text, result text)\"\n            )\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = c.fetchone()\n            if result:\n                return result[0]\n            else:\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache VALUES (?, ?, ?, ?)\",\n                    (func_name, str(args), str(kwargs), result),\n                )\n                conn.commit()\n                conn.close()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import functools\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Create a table to store the function outputs if it doesn't exist\n            cursor.execute(\n                \"\"\"\n                CREATE TABLE IF NOT EXISTS cache (\n                    func_name TEXT,\n                    args TEXT,\n                    kwargs TEXT,\n                    result TEXT\n                )\n            \"\"\"\n            )\n\n            # Check if the result is already cached in the database\n            cursor.execute(\n                \"\"\"\n                SELECT result FROM cache\n                WHERE func_name = ? AND args = ? AND kwargs = ?\n                \"\"\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = cursor.fetchone()\n\n            if result:\n                # If the result is cached, return it\n                return result[0]\n            else:\n                # If the result is not cached, compute it and store it in the database\n                result = func(*args, **kwargs)\n                cursor.execute(\n                    \"\"\"\n                    INSERT INTO cache (func_name, args, kwargs, result)\n                    VALUES (?, ?, ?, ?)\n                    \"\"\",\n                    (func_name, str(args), str(kwargs), result),\n                )\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute('''CREATE TABLE IF NOT EXISTS cache\n                         (func_name text, args text, kwargs text, result text)''')\n            c.execute('SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?',\n                      (func_name, str(args), str(kwargs)))\n            result = c.fetchone()\n            if result:\n                return result[0]\n            else:\n                result = func(*args, **kwargs)\n                c.execute('INSERT INTO cache VALUES (?,?,?,?)',\n                          (func_name, str(args), str(kwargs), result))\n                conn.commit()\n                conn.close()\n                return result\n        return wrapper\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Create the table if it doesn't exist\n            cursor.execute(\n                \"\"\"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\"\"\"\n            )\n\n            # Check if the result is already cached in the database\n            cursor.execute(\n                \"\"\"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\"\"\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = cursor.fetchone()\n\n            if result:\n                # If the result is cached, return it\n                return result[0]\n            else:\n                # If the result is not cached, compute it and store it in the database\n                result = func(*args, **kwargs)\n                cursor.execute(\n                    \"\"\"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\"\"\",\n                    (func_name, str(args), str(kwargs), result),\n                )\n                conn.commit()\n                conn.close()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import functools\n\n    def decorator(func):\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Create a table to store the function outputs if it doesn't exist\n            cursor.execute(\n                \"\"\"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\"\"\"\n            )\n\n            # Check if the result is already in the cache\n            cursor.execute(\n                \"\"\"SELECT result FROM cache WHERE func_name = ? AND args = ? AND kwargs = ?\"\"\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = cursor.fetchone()\n\n            if result is not None:\n                # If the result is in the cache, return it\n                return result[0]\n            else:\n                # If the result is not in the cache, compute it and store it\n                result = func(*args, **kwargs)\n                cursor.execute(\n                    \"\"\"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\"\"\",\n                    (func_name, str(args), str(kwargs), result),\n                )\n                conn.commit()\n                conn.close()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Create a table to store the function outputs if it doesn't exist\n            cursor.execute(\n                \"\"\"\n                CREATE TABLE IF NOT EXISTS cache (\n                    func_name TEXT,\n                    args TEXT,\n                    kwargs TEXT,\n                    result TEXT\n                )\n            \"\"\"\n            )\n\n            # Check if the function output is already cached in the database\n            cursor.execute(\n                \"\"\"\n                SELECT result FROM cache\n                WHERE func_name = ? AND args = ? AND kwargs = ?\n            \"\"\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = cursor.fetchone()\n\n            if result is not None:\n                # If the function output is already cached, return it\n                return result[0]\n            else:\n                # If the function output is not cached, compute it and store it in the database\n                result = func(*args, **kwargs)\n                cursor.execute(\n                    \"\"\"\n                    INSERT INTO cache (func_name, args, kwargs, result)\n                    VALUES (?, ?, ?, ?)\n                \"\"\",\n                    (func_name, str(args), str(kwargs), result),\n                )\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import functools\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create a table to store the function outputs if it doesn't exist\n            c.execute(\n                \"\"\"\n                CREATE TABLE IF NOT EXISTS cache (\n                    func_name TEXT,\n                    args TEXT,\n                    kwargs TEXT,\n                    result TEXT\n                )\n            \"\"\"\n            )\n\n            # Check if the function output is already cached in the database\n            c.execute(\n                \"\"\"\n                SELECT result FROM cache\n                WHERE func_name = ? AND args = ? AND kwargs = ?\n            \"\"\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = c.fetchone()\n\n            if result:\n                # If the output is cached, return it\n                return result[0]\n            else:\n                # If the output is not cached, compute it and store it in the database\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"\"\"\n                    INSERT INTO cache (func_name, args, kwargs, result)\n                    VALUES (?, ?, ?, ?)\n                \"\"\",\n                    (func_name, str(args), str(kwargs), result),\n                )\n                conn.commit()\n                conn.close()\n                return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    from functools import wraps\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Create a table to store the function outputs if it doesn't exist\n            cursor.execute(\n                \"\"\"\n                CREATE TABLE IF NOT EXISTS cache (\n                    func_name TEXT,\n                    args TEXT,\n                    kwargs TEXT,\n                    result TEXT\n                )\n            \"\"\"\n            )\n\n            # Check if the result is already in the database\n            cursor.execute(\n                \"\"\"\n                SELECT result FROM cache\n                WHERE func_name = ? AND args = ? AND kwargs = ?\n                \"\"\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = cursor.fetchone()\n\n            if result:\n                # Return the cached result\n                return result[0]\n\n            # Compute the result\n            result = func(*args, **kwargs)\n\n            # Store the result in the database\n            cursor.execute(\n                \"\"\"\n                INSERT INTO cache (func_name, args, kwargs, result)\n                VALUES (?, ?, ?, ?)\n                \"\"\",\n                (func_name, str(args), str(kwargs), result),\n            )\n            conn.commit()\n\n            # Close the database connection\n            conn.close()\n\n            return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create a table to store the function outputs if it doesn't exist\n            c.execute(\n                \"\"\"CREATE TABLE IF NOT EXISTS cache (\n                    func_name TEXT,\n                    args TEXT,\n                    kwargs TEXT,\n                    result BLOB\n                )\"\"\"\n            )\n\n            # Check if the result is already in the cache\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = c.fetchone()\n\n            if result:\n                # If the result is in the cache, return it\n                return pickle.loads(result[0])\n            else:\n                # If the result is not in the cache, compute it and store it in the cache\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, str(args), str(kwargs), pickle.dumps(result)),\n                )\n                conn.commit()\n                conn.close()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import functools\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\n                \"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\"\n            )\n            key = (func_name, str(args), str(kwargs))\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\", key\n            )\n            result = c.fetchone()\n            if result is None:\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    key + (str(result),),\n                )\n                conn.commit()\n            else:\n                result = eval(result[0])\n            conn.close()\n            return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\")\n            c.execute(\"SELECT result FROM cache WHERE func_name = ? AND args = ? AND kwargs = ?\", (func_name, str(args), str(kwargs)))\n            result = c.fetchone()\n            if result is not None:\n                return result[0]\n            else:\n                result = func(*args, **kwargs)\n                c.execute(\"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\", (func_name, str(args), str(kwargs), result))\n                conn.commit()\n                conn.close()\n                return result\n        return wrapper\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    from functools import wraps\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Create the table if it doesn't exist\n            cursor.execute('''CREATE TABLE IF NOT EXISTS cache\n                                (func_name TEXT, args TEXT, kwargs TEXT, result TEXT, PRIMARY KEY (func_name, args, kwargs))''')\n\n            # Check if the result is already in the database\n            args_str = str(args)\n            kwargs_str = str(kwargs)\n            cursor.execute('''SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?''',\n                           (func_name, args_str, kwargs_str))\n            result = cursor.fetchone()\n\n            if result:\n                # Return the cached result\n                return result[0]\n            else:\n                # Compute the result and store it in the database\n                result = func(*args, **kwargs)\n                cursor.execute('''INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)''',\n                               (func_name, args_str, kwargs_str, result))\n                conn.commit()\n\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import functools\n\n    def decorator(func):\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Create the table to store function outputs if it doesn't exist\n            cursor.execute(\n                f\"CREATE TABLE IF NOT EXISTS {func_name} (args TEXT PRIMARY KEY, result TEXT)\")\n\n            # Check if the result of the function call is already in the database\n            args_str = str(args) + str(kwargs)\n            cursor.execute(\n                f\"SELECT result FROM {func_name} WHERE args = ?\", (args_str,))\n            result = cursor.fetchone()\n\n            if result:\n                # If the result is already in the database, return it\n                return result[0]\n            else:\n                # If the result is not in the database, compute it and store it\n                result = func(*args, **kwargs)\n                cursor.execute(\n                    f\"INSERT INTO {func_name} (args, result) VALUES (?, ?)\", (args_str, result))\n                conn.commit()\n                return result\n\n        return wrapper\n    return decorator\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__} x_min must be less than x_max\")\n\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__} y_min must be less than y_max\")\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"Invalid bounding box values: x_min ({values['x_min']}) must be less than x_max ({values['x_max']})\"\n        )\n\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"Invalid bounding box values: y_min ({values['y_min']}) must be less than y_max ({values['y_max']})\"\n        )\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = (\n        values[\"x_min\"],\n        values[\"x_max\"],\n        values[\"y_min\"],\n        values[\"y_max\"],\n    )\n\n    if x_min >= x_max:\n        raise ValueError(\n            f\"Invalid bounding box: x_min ({x_min}) must be less than x_max ({x_max}).\"\n        )\n\n    if y_min >= y_max:\n        raise ValueError(\n            f\"Invalid bounding box: y_min ({y_min}) must be less than y_max ({y_max}).\"\n        )\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] > values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__} x_min must be less than x_max\")\n    if values[\"y_min\"] > values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__} y_min must be less than y_max\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] > values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__} x_min must be less than x_max\")\n    if values[\"y_min\"] > values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__} y_min must be less than y_max\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"{cls.__name__} x_min value must be less than x_max value.\"\n        )\n\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__} y_min value must be less than y_max value.\"\n        )\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"Invalid bounding box values for {cls.__name__}: x_min must be less than x_max\"\n        )\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"Invalid bounding box values for {cls.__name__}: y_min must be less than y_max\"\n        )\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__} x_min must be less than x_max\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__} y_min must be less than y_max\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__} x_min must be less than x_max\")\n\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__} y_min must be less than y_max\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__} x_min must be less than x_max\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__} y_min must be less than y_max\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"Invalid bounding box values for {cls.__name__}: x_min must be less than x_max\"\n        )\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"Invalid bounding box values for {cls.__name__}: y_min must be less than y_max\"\n        )\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values.values()\n    if x_min >= x_max:\n        raise ValueError(\n            f\"Invalid bounding box values for {cls.__name__}: x_min ({x_min}) must be less than x_max ({x_max}).\"\n        )\n    if y_min >= y_max:\n        raise ValueError(\n            f\"Invalid bounding box values for {cls.__name__}: y_min ({y_min}) must be less than y_max ({y_max}).\"\n        )\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__} x_min must be less than x_max\")\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__} y_min must be less than y_max\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values.values()\n\n    if x_min >= x_max:\n        raise ValueError(f\"Invalid bounding box: x_min ({x_min}) must be less than x_max ({x_max}).\")\n\n    if y_min >= y_max:\n        raise ValueError(f\"Invalid bounding box: y_min ({y_min}) must be less than y_max ({y_max}).\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"Invalid bounding box: x_min ({values['x_min']}) must be less than x_max ({values['x_max']})\"\n        )\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"Invalid bounding box: y_min ({values['y_min']}) must be less than y_max ({values['y_max']})\"\n        )\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] > values[\"x_max\"]:\n        raise ValueError(\n            f\"Invalid bounding box: x_min ({values['x_min']}) is greater than x_max ({values['x_max']})\"\n        )\n    if values[\"y_min\"] > values[\"y_max\"]:\n        raise ValueError(\n            f\"Invalid bounding box: y_min ({values['y_min']}) is greater than y_max ({values['y_max']})\"\n        )\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"Invalid bounding box: x_min ({values['x_min']}) must be less than x_max ({values['x_max']}).\"\n        )\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"Invalid bounding box: y_min ({values['y_min']}) must be less than y_max ({values['y_max']}).\"\n        )\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"{cls.__name__} bounding box x_min must be less than x_max, \"\n            f\"got x_min: {values['x_min']} and x_max: {values['x_max']}\"\n        )\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__} bounding box y_min must be less than y_max, \"\n            f\"got y_min: {values['y_min']} and y_max: {values['y_max']}\"\n        )\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values.values()\n    if x_min >= x_max or y_min >= y_max:\n        raise ValueError(f\"Invalid bounding box values for {cls.__name__}: {values}\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values.values()\n    if x_min >= x_max or y_min >= y_max:\n        raise ValueError(f\"Invalid bounding box values for {cls.__name__}: x_min={x_min}, x_max={x_max}, y_min={y_min}, y_max={y_max}\")\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of each column in mat0 and mat1\n  norms0 = np.sum(mat0 ** 2, axis=0)\n  norms1 = np.sum(mat1 ** 2, axis=0)\n\n  # Compute the dot product between each pair of columns in mat0 and mat1\n  dots = mat0.T.dot(mat1)\n\n  # Compute the squared distances using the mathematical property\n  sq_dist = norms0[:, np.newaxis] + norms1[np.newaxis, :] - 2 * dots\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  n0 = mat0.shape[1]\n  n1 = mat1.shape[1]\n\n  mat0_norms = np.sum(mat0 ** 2, axis=0)\n  mat1_norms = np.sum(mat1 ** 2, axis=0)\n\n  mat0_dot_mat1 = np.dot(mat0.T, mat1)\n\n  mat0_norms_repeated = np.tile(mat0_norms, (n1, 1)).T\n  mat1_norms_repeated = np.tile(mat1_norms, (n0, 1))\n\n  sq_dist = mat0_norms_repeated + mat1_norms_repeated - 2 * mat0_dot_mat1\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of each column in mat0 and mat1\n  norm0 = np.sum(mat0 ** 2, axis=0)\n  norm1 = np.sum(mat1 ** 2, axis=0)\n\n  # Compute the dot product between each pair of columns in mat0 and mat1\n  dot_prod = mat0.T.dot(mat1)\n\n  # Compute the squared Euclidean distance between each pair of columns in mat0 and mat1\n  sq_dist = norm0[:, np.newaxis] + norm1[np.newaxis, :] - 2 * dot_prod\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  return np.maximum(0, np.sum(mat0 ** 2, axis=0)[:, np.newaxis] + np.sum(mat1 ** 2, axis=0) - 2 * mat0.T @ mat1)\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  n0 = mat0.shape[1]\n  n1 = mat1.shape[1]\n\n  mat0_norm = np.sum(mat0 ** 2, axis=0)\n  mat1_norm = np.sum(mat1 ** 2, axis=0)\n\n  mat0_mat1_prod = mat0.T @ mat1\n\n  sq_dist = np.tile(mat0_norm, (n1, 1)) + np.tile(mat1_norm, (n0, 1)).T - 2 * mat0_mat1_prod\n\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # Check if mat1 is provided, if not, set it to mat0\n  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of each column in mat0 and mat1\n  norms0 = np.sum(mat0 ** 2, axis=0)\n  norms1 = np.sum(mat1 ** 2, axis=0)\n\n  # Compute the dot products between all pairs of columns in mat0 and mat1\n  dots = np.dot(mat0.T, mat1)\n\n  # Compute the squared Euclidean distance matrix using the mathematical property\n  sq_dist = norms0[:, np.newaxis] + norms1[np.newaxis, :] - 2 * dots\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  n0 = mat0.shape[1]\n  n1 = mat1.shape[1]\n\n  mat0_norms = np.sum(mat0 ** 2, axis=0)\n  mat1_norms = np.sum(mat1 ** 2, axis=0)\n\n  mat0_dot_mat1 = mat0.T @ mat1\n\n  sq_dist = np.tile(mat0_norms, (n1, 1)) + np.tile(mat1_norms, (n0, 1)).T - 2 * mat0_dot_mat1\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of the columns in each matrix\n  norms0 = np.sum(mat0 ** 2, axis=0)\n  norms1 = np.sum(mat1 ** 2, axis=0)\n\n  # Compute the dot product between the columns in the two matrices\n  dot_prod = np.dot(mat0.T, mat1)\n\n  # Compute the squared Euclidean distance using the mathematical property\n  sq_dist = norms0[:, np.newaxis] + norms1[np.newaxis, :] - 2 * dot_prod\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # Check if mat1 is provided, if not, set it to mat0\n  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norm of each column in mat0 and mat1\n  norm_0 = np.sum(mat0 ** 2, axis=0)\n  norm_1 = np.sum(mat1 ** 2, axis=0)\n\n  # Compute the dot product between each pair of columns in mat0 and mat1\n  dot_prod = mat0.T.dot(mat1)\n\n  # Compute the squared Euclidean distance between each pair of columns\n  sq_dist = norm_0[:, np.newaxis] + norm_1[np.newaxis, :] - 2 * dot_prod\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the dot product of the matrices\n  dot_prod = mat0.T @ mat1\n\n  # Compute the squared norms of the columns of the matrices\n  mat0_norms = np.sum(mat0 ** 2, axis=0)\n  mat1_norms = np.sum(mat1 ** 2, axis=0)\n\n  # Compute the squared distances using the dot product and norms\n  sq_dist = mat0_norms[:, np.newaxis] + mat1_norms[np.newaxis, :] - 2 * dot_prod\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  import numpy as np\n  # Check if mat1 is provided, if not set it to mat0\n  if mat1 is None:\n    mat1 = mat0\n  # Compute the norm of each column in mat0 and mat1\n  norm0 = np.sum(mat0 ** 2, axis=0)\n  norm1 = np.sum(mat1 ** 2, axis=0)\n  # Compute the dot product between each pair of columns in mat0 and mat1\n  dot = np.dot(mat0.T, mat1)\n  # Compute the squared Euclidean distance between each pair of columns\n  dist = norm0[:, np.newaxis] + norm1[np.newaxis, :] - 2 * dot\n  # Set negative distances to zero\n  dist[dist < 0] = 0\n  return dist\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # Compute the squared norm of each column in mat0\n  norm0 = np.sum(mat0 ** 2, axis=0)\n\n  # If mat1 is not provided, compute the squared distances among columns of mat0\n  if mat1 is None:\n    # Compute the dot product between each pair of columns in mat0\n    dot_prod = np.dot(mat0.T, mat0)\n    # Compute the squared distances by subtracting twice the dot product from the sum of squared norms\n    sq_dist = norm0[:, np.newaxis] + norm0[np.newaxis, :] - 2 * dot_prod\n    # Set negative distances to zero\n    sq_dist[sq_dist < 0] = 0\n  else:\n    # Compute the squared norm of each column in mat1\n    norm1 = np.sum(mat1 ** 2, axis=0)\n    # Compute the dot product between each pair of columns in mat0 and mat1\n    dot_prod = np.dot(mat0.T, mat1)\n    # Compute the squared distances by subtracting twice the dot product from the sum of squared norms\n    sq_dist = norm0[:, np.newaxis] + norm1[np.newaxis, :] - 2 * dot_prod\n    # Set negative distances to zero\n    sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # Compute the dot product between all pairs of columns in mat0 and mat1\n  dot_prod = mat0.T @ mat1\n\n  # Compute the squared norms of each column in mat0 and mat1\n  norm_0 = np.sum(mat0 ** 2, axis=0)\n  norm_1 = np.sum(mat1 ** 2, axis=0)\n\n  # Compute the squared Euclidean distance between all pairs of columns in mat0 and mat1\n  sq_dist = norm_0[:, None] - 2 * dot_prod + norm_1[None, :]\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  norm0 = np.sum(mat0 ** 2, axis=0)\n  norm1 = np.sum(mat1 ** 2, axis=0)\n  dot = mat0.T.dot(mat1)\n  dist = norm0[:, None] + norm1[None, :] - 2 * dot\n  dist[dist < 0] = 0\n\n  return dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the norms of each column in mat0\n  norm0 = np.sum(mat0 ** 2, axis=0)\n\n  # Compute the norms of each column in mat1\n  norm1 = np.sum(mat1 ** 2, axis=0)\n\n  # Compute the dot product between each column pair\n  dot_prod = mat0.T @ mat1\n\n  # Compute the squared Euclidean distance\n  sq_dist = np.abs(norm0[:, np.newaxis] + norm1[np.newaxis, :] - 2 * dot_prod)\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of each column vector in mat0 and mat1\n  norm0 = np.sum(mat0 ** 2, axis=0)\n  norm1 = np.sum(mat1 ** 2, axis=0)\n\n  # Compute the dot product between each pair of columns in mat0 and mat1\n  dot_prod = mat0.T @ mat1\n\n  # Compute the squared Euclidean distance matrix\n  sq_dist = np.abs(norm0[:, np.newaxis] + norm1[np.newaxis, :] - 2 * dot_prod)\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # Compute the squared norms of each column vector in mat0\n  mat0_sq_norms = np.sum(mat0 ** 2, axis=0)\n\n  # If mat1 is provided, compute the squared norms of each column vector in mat1\n  if mat1 is not None:\n    mat1_sq_norms = np.sum(mat1 ** 2, axis=0)\n  # If mat1 is not provided, use mat0 for both sets of vectors\n  else:\n    mat1 = mat0\n    mat1_sq_norms = mat0_sq_norms\n\n  # Compute the dot product between each column vector pair in mat0 and mat1\n  dot_prod = mat0.T @ mat1\n\n  # Compute the squared Euclidean distance matrix using the mathematical property\n  sq_dist = np.abs(mat0_sq_norms[:, np.newaxis] + mat1_sq_norms - 2 * dot_prod)\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the norms of each column in mat0 and mat1\n  norm_mat0 = np.sum(mat0 ** 2, axis=0)\n  norm_mat1 = np.sum(mat1 ** 2, axis=0)\n\n  # Compute the dot product between each column of mat0 and each column of mat1\n  dot_product = np.dot(mat0.T, mat1)\n\n  # Compute the squared Euclidean distance between each column of mat0 and each column of mat1\n  sq_dist = np.maximum(0, norm_mat0[:, np.newaxis] + norm_mat1[np.newaxis, :] - 2 * dot_product)\n\n  return sq_dist\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  import numpy as np\n  from scipy.linalg import norm\n\n  if mat1 is None:\n    mat1 = mat0\n\n  n0 = mat0.shape[1]\n  n1 = mat1.shape[1]\n\n  mat0_norm = norm(mat0, axis=0)\n  mat1_norm = norm(mat1, axis=0)\n\n  mat0_norm_sq = mat0_norm ** 2\n  mat1_norm_sq = mat1_norm ** 2\n\n  mat0_mat1_dot = mat0.T.dot(mat1)\n\n  dist_sq = np.tile(mat0_norm_sq, (n1, 1)).T + np.tile(mat1_norm_sq, (n0, 1)) - 2 * mat0_mat1_dot\n\n  dist_sq[dist_sq < 0] = 0\n\n  return dist_sq\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  import numpy as np\n\n  if mat1 is None:\n    mat1 = mat0\n\n  n0 = mat0.shape[1]\n  n1 = mat1.shape[1]\n\n  mat0_norms = np.sum(mat0 ** 2, axis=0)\n  mat1_norms = np.sum(mat1 ** 2, axis=0)\n\n  mat0_dot_mat1 = np.dot(mat0.T, mat1)\n\n  sq_dist = mat0_norms[:, np.newaxis] + mat1_norms[np.newaxis, :] - 2 * mat0_dot_mat1\n\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    elif path == \"\":\n        return True\n    elif path.startswith(\"file://\"):\n        return True\n    elif path.startswith(\"http://\"):\n        return True\n    elif path.startswith(\"https://\"):\n        return True\n    elif path.startswith(\"gs://\"):\n        return True\n    elif path.startswith(\"hdfs://\"):\n        return True\n    elif path.startswith(\"s3://\"):\n        return True\n    elif path.startswith(\"wasbs://\"):\n        return True\n    else:\n        return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return True\n    if path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"wasbs://\"):\n        return True\n    if path.startswith(\"file://\"):\n        return True\n    if path.startswith(\"hdfs://\"):\n        return True\n    if path.startswith(\"dbfs:/\"):\n        return True\n    if path.startswith(\"abfss://\"):\n        return True\n    if path.startswith(\"adl://\"):\n        return True\n    if path.startswith(\"adls://\"):\n        return True\n    if path.startswith(\"abfs://\"):\n        return True\n    if path.startswith(\"s3a://\"):\n        return True\n    if path.startswith(\"s3n://\"):\n        return True\n    if path.startswith(\"s3p://\"):\n        return True\n    if path.startswith(\"s3k://\"):\n        return True\n    if path.startswith(\"s3q://\"):\n        return True\n    if path.startswith(\"s3r://\"):\n        return True\n    if path.startswith(\"s3s://\"):\n        return True\n    if path.startswith(\"s3t://\"):\n        return True\n    if path.startswith(\"s3u://\"):\n        return True\n    if path.startswith(\"s3v://\"):\n        return True\n    if path.startswith(\"s3w://\"):\n        return True\n    if path.startswith(\"s3x://\"):\n        return True\n    if path.startswith(\"s3y://\"):\n        return True\n    if path.startswith(\"s3z://\"):\n        return True\n    if path.startswith(\"s30://\"):\n        return True\n    if path.startswith(\"s31://\"):\n        return True\n    if path"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n\n    if path == \"\":\n        return True\n\n    if path.startswith(\"$\"):\n        return True\n\n    if path.startswith(\"~\"):\n        return True\n\n    if path.startswith(\"./\"):\n        return True\n\n    if path.startswith(\"../\"):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"s3://\") or path.startswith(\"gs://\"):\n        return False\n    return True\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    return path is None or path == \"\" or path.startswith(\"s3://\") or path.startswith(\"gs://\")\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n\n    if path == \"\":\n        return True\n\n    if path.startswith(\"http://\"):\n        return True\n\n    if path.startswith(\"https://\"):\n        return True\n\n    if path.startswith(\"s3://\"):\n        return True\n\n    if path.startswith(\"gs://\"):\n        return True\n\n    if path.startswith(\"hdfs://\"):\n        return True\n\n    if path.startswith(\"file://\"):\n        return True\n\n    if path.startswith(\"s3a://\"):\n        return True\n\n    if path.startswith(\"s3n://\"):\n        return True\n\n    if path.startswith(\"wasbs://\"):\n        return True\n\n    if path.startswith(\"wasbns://\"):\n        return True\n\n    if path.startswith(\"wasbss://\"):\n        return True\n\n    if path.startswith(\"adl://\"):\n        return True\n\n    if path.startswith(\"abfs://\"):\n        return True\n\n    if path.startswith(\"abfss://\"):\n        return True\n\n    if path.startswith(\"adls://\"):\n        return True\n\n    if path.startswith(\"abfs://\"):\n        return True\n\n    if path.startswith(\"abfss://\"):\n        return True\n\n    if path.startswith(\"s3n://\"):\n        return True\n\n    if path.startswith(\"s3a://\"):\n        return True\n\n    if path.startswith(\"s3n://\"):\n        return True\n\n    if path.startswith(\"s3a://\"):\n        return True\n\n    if path.startswith(\"s3n://\"):\n        return True\n\n    if path.startswith(\"s3a://\"):\n        return True\n\n    if path.startswith(\"s3n://\"):\n        return True\n\n    if path.startswith(\""}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\":\n        return True\n    elif path.startswith(\"file://\"):\n        return True\n    elif path.startswith(\"s3://\"):\n        return True\n    elif path.startswith(\"gs://\"):\n        return True\n    elif path.startswith(\"hdfs://\"):\n        return True\n    elif path.startswith(\"http://\"):\n        return True\n    elif path.startswith(\"https://\"):\n        return True\n    elif path.startswith(\"sftp://\"):\n        return True\n    elif path.startswith(\"ftp://\"):\n        return True\n    elif path.startswith(\"ftps://\"):\n        return True\n    elif path.startswith(\"webdav://\"):\n        return True\n    elif path.startswith(\"webdavs://\"):\n        return True\n    elif path.startswith(\"smb://\"):\n        return True\n    elif path.startswith(\"adl://\"):\n        return True\n    elif path.startswith(\"az://\"):\n        return True\n    elif path.startswith(\"abfs://\"):\n        return True\n    elif path.startswith(\"abfss://\"):\n        return True\n    elif path.startswith(\"azure://\"):\n        return True\n    elif path.startswith(\"azureblob://\"):\n        return True\n    elif path.startswith(\"azurefile://\"):\n        return True\n    elif path.startswith(\"azurepath://\"):\n        return True\n    elif path.startswith(\"azurequeue://\"):\n        return True\n    elif path.startswith(\"azuretable://\"):\n        return True\n    elif path.startswith(\"azurecosmos://\"):\n        return True\n    elif path.startswith(\"azureeventhub://\"):\n        return True\n    elif path.startswith(\"azureservicebus://\"):\n        return True\n    elif path.startswith(\"azurekeyvault://\"):\n        return True"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"file://\") or path.startswith(\"s3://\"):\n        return True\n    return False\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    return (\n        path is None\n        or path == \"\"\n        or path.startswith(\"s3://\")\n        or path.startswith(\"gs://\")\n        or path.startswith(\"hdfs://\")\n    )\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    elif path == \"\":\n        return True\n    elif path.startswith(\"http://\"):\n        return True\n    elif path.startswith(\"https://\"):\n        return True\n    elif path.startswith(\"ftp://\"):\n        return True\n    elif path.startswith(\"s3://\"):\n        return True\n    elif path.startswith(\"gs://\"):\n        return True\n    elif path.startswith(\"hdfs://\"):\n        return True\n    else:\n        return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    return (\n        path is None\n        or path == \"\"\n        or path.startswith(\"gs://\")\n        or path.startswith(\"hdfs://\")\n        or path.startswith(\"s3://\")\n        or path.startswith(\"dbfs:/\")\n        or path.startswith(\"file:/\")\n        or path.startswith(\"abfss://\")\n    )\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return True\n    if path.startswith(\"file://\"):\n        return True\n    if path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"hdfs://\"):\n        return True\n    if path.startswith(\"abfss://\"):\n        return True\n    if path.startswith(\"adl://\"):\n        return True\n    if path.startswith(\"wasbs://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"s3://\") or path.startswith(\"gs://\"):\n        return False\n    return True\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n\n    if path == \"\":\n        return True\n\n    if path.startswith(\"~\"):\n        return True\n\n    if path.startswith(\"$\"):\n        return True\n\n    if path.startswith(\"${\"):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    return not path or path.startswith((\"s3://\", \"gs://\", \"hdfs://\", \"file://\"))\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"/\"):\n        return True\n    if path.startswith(\"~/\"):\n        return True\n    if path.startswith(\"~\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    return (\n        path is None\n        or path == \"\"\n        or path.startswith(\"s3://\")\n        or path.startswith(\"gs://\")\n        or path.startswith(\"hdfs://\")\n    )\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\":\n        return True\n    if path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\":\n        return True\n\n    special_prefixes = [\"/tmp/\", \"/var/\", \"/home/\", \"/etc/\", \"/usr/\", \"/dev/\", \"/proc/\", \"/sys/\"]\n\n    for prefix in special_prefixes:\n        if path.startswith(prefix):\n            return True\n\n    return False\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return True\n    if path.startswith(\"file://\"):\n        return True\n    if path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"hdfs://\"):\n        return True\n    if path.startswith(\"dbfs:/\"):\n        return True\n    if path.startswith(\"v3io://\"):\n        return True\n    if path.startswith(\"http://\"):\n        return True\n    if path.startswith(\"https://\"):\n        return True\n    if path.startswith(\"mlrun-api://\"):\n        return True\n    if path.startswith(\"store://\"):\n        return True\n    if path.startswith(\"v3io:///\"):\n        return True\n    if path.startswith(\"v3io-sidecar://\"):\n        return True\n    if path.startswith(\"pv://\"):\n        return True\n    if path.startswith(\"catalog://\"):\n        return True\n    if path.startswith(\".\"):\n        return True\n    if path.startswith(\"/\"):\n        return True\n    if path.startswith(\"..\"):\n        return True\n    return False\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            assets_names = np.array(list(items.keys()))\n        else:\n            assets_names = np.array(assets_names)\n        items = np.array([items.get(k, fill_value) for k in assets_names])\n    elif isinstance(items, npt.ArrayLike):\n        items = np.array(items)\n    else:\n        raise ValueError(\n            f\"{name} must be a dictionary or array-like, not {type(items)}\"\n        )\n\n    if dim == 1:\n        if items.shape != (n_assets,):\n            raise ValueError(\n                f\"{name} must have shape ({n_assets},), not {items.shape}\"\n            )\n    elif dim == 2:\n        if items.shape != (n_groups, n_assets):\n            raise ValueError(\n                f\"{name} must have shape ({n_groups}, {n_assets}), not {items.shape}\"\n            )\n    else:\n        raise ValueError(f\"dim must be 1 or 2, not {dim}\")\n\n    return items\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} is a dictionary, so you must pass assets_names\"\n            )\n\n        items = np.array(\n            [items.get(asset, fill_value) for asset in assets_names]\n        )\n\n    items = np.array(items)\n\n    if dim == 1:\n        items = items.reshape(n_assets)\n    elif dim == 2:\n        items = items.reshape(-1, n_assets)\n    else:\n        raise ValueError(f\"dim can only be 1 or 2, not {dim}\")\n\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if dim == 1:\n            if assets_names is None:\n                raise ValueError(\n                    \"assets_names must be provided when items is a dictionary\"\n                )\n            items = np.array([items.get(name, fill_value) for name in assets_names])\n        elif dim == 2:\n            if assets_names is None:\n                raise ValueError(\n                    \"assets_names must be provided when items is a dictionary\"\n                )\n            items = np.array(\n                [\n                    [items.get(name, fill_value) for name in assets_names]\n                    for _ in range(items[assets_names[0]].shape[0])\n                ]\n            )\n        else:\n            raise ValueError(\"dim must be 1 or 2\")\n\n    if isinstance(items, npt.ArrayLike):\n        items = np.array(items)\n\n    if dim == 1:\n        if items.ndim != 1:\n            raise ValueError(f\"{name} must be a 1-dimensional array\")\n        if items.shape[0] != n_assets:\n            raise ValueError(f\"{name} must have {n_assets} elements\")\n    elif dim == 2:\n        if items.ndim != 2:\n            raise ValueError(f\"{name} must be a 2-dimensional array\")\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} columns (one for each asset)\"\n            )\n    else:\n        raise ValueError(\"dim must be 1 or 2\")\n\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"assets_names must be provided when {name} is a dictionary\")\n        items = np.array([items.get(asset, fill_value) for asset in assets_names])\n    elif isinstance(items, npt.ArrayLike):\n        items = np.array(items)\n    else:\n        raise TypeError(f\"{name} must be a dictionary or array-like\")\n\n    if dim == 1:\n        if items.shape != (n_assets,):\n            raise ValueError(f\"{name} must have shape (n_assets,)\")\n    elif dim == 2:\n        if items.shape != (n_groups, n_assets):\n            raise ValueError(f\"{name} must have shape (n_groups, n_assets)\")\n    else:\n        raise ValueError(f\"dim must be 1 or 2\")\n\n    return items\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            assets_names = np.array(list(items.keys()))\n        if dim == 1:\n            array = np.full(n_assets, fill_value)\n            for i, name in enumerate(assets_names):\n                if name in items:\n                    array[i] = items[name]\n        else:\n            array = np.full((n_groups, n_assets), fill_value)\n            for i, name in enumerate(assets_names):\n                if name in items:\n                    array[:, i] = items[name]\n    else:\n        array = np.array(items)\n\n    if dim == 1:\n        if array.shape != (n_assets,):\n            raise ValueError(f\"{name} must have shape (n_assets,).\")\n    else:\n        if array.shape != (n_groups, n_assets):\n            raise ValueError(f\"{name} must have shape (n_groups, n_assets).\")\n\n    return array\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        items = np.array(list(items.values()))\n        if assets_names is not None:\n            items = items[assets_names]\n        else:\n            items = items[np.arange(n_assets)]\n        if dim == 2:\n            items = np.tile(items, (n_groups, 1))\n    elif isinstance(items, np.ndarray):\n        if dim == 2:\n            if items.shape == (n_assets,):\n                items = np.tile(items, (n_groups, 1))\n            elif items.shape == (n_groups, n_assets):\n                pass\n            else:\n                raise ValueError(\n                    f\"The {name} array should have shape ({n_assets},) or ({n_groups}, {n_assets})\"\n                )\n        elif dim == 1:\n            if items.shape == (n_assets,):\n                pass\n            else:\n                raise ValueError(f\"The {name} array should have shape ({n_assets},)\")\n    else:\n        items = np.array(items)\n        if dim == 2:\n            if items.shape == (n_assets,):\n                items = np.tile(items, (n_groups, 1))\n            elif items.shape == (n_groups, n_assets):\n                pass\n            else:\n                raise ValueError(\n                    f\"The {name} array should have shape ({n_assets},) or ({n_groups}, {n_assets})\"\n                )\n        elif dim == 1:\n            if items.shape == (n_assets,):\n                pass\n            else:\n                raise ValueError(f\"The {name} array should have shape ({n_assets},)\")\n\n    if dim == 1:\n        if items.shape != (n_assets,):\n            raise ValueError(f\"The {name} array should have shape ({n_assets},)\")\n    elif dim == 2:\n        if items.shape != (n_groups, n_assets):\n            raise ValueError"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if dim == 1:\n        if isinstance(items, dict):\n            if assets_names is None:\n                raise ValueError(\n                    \"assets_names must be provided when items is a dictionary\"\n                )\n            items = np.array([items.get(name, fill_value) for name in assets_names])\n        if isinstance(items, np.ndarray):\n            items = items.reshape((-1))\n        if not isinstance(items, np.ndarray):\n            items = np.array(items)\n        if items.ndim != 1:\n            raise ValueError(\n                f\"{name} must be a 1-dimensional array or dictionary with {n_assets} elements\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} elements, but has {items.shape[0]}\"\n            )\n    elif dim == 2:\n        if isinstance(items, dict):\n            if assets_names is None:\n                raise ValueError(\n                    \"assets_names must be provided when items is a dictionary\"\n                )\n            items = np.array(\n                [items.get(name, fill_value) for name in assets_names]\n            ).T\n        if isinstance(items, np.ndarray):\n            items = items.T\n        if not isinstance(items, np.ndarray):\n            items = np.array(items).T\n        if items.ndim != 2:\n            raise ValueError(\n                f\"{name} must be a 2-dimensional array or dictionary with {n_assets} elements\"\n            )\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} elements, but has {items.shape[1]}\"\n            )\n    else:\n        raise ValueError(\"dim must be 1 or 2\")\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if dim == 1:\n            if assets_names is None:\n                raise ValueError(\n                    f\"{name} is a dictionary, so assets_names must be provided.\"\n                )\n            else:\n                items_array = np.array(\n                    [items.get(asset, fill_value) for asset in assets_names]\n                )\n        elif dim == 2:\n            if assets_names is None:\n                raise ValueError(\n                    f\"{name} is a dictionary, so assets_names must be provided.\"\n                )\n            else:\n                items_array = np.array(\n                    [\n                        [items.get(asset, fill_value) for asset in assets_names]\n                        for _ in range(n_groups)\n                    ]\n                )\n        else:\n            raise ValueError(f\"{name} has invalid dimension: {dim}\")\n    elif isinstance(items, (np.ndarray, list, tuple)):\n        if dim == 1:\n            items_array = np.array(items)\n        elif dim == 2:\n            items_array = np.array(items)\n        else:\n            raise ValueError(f\"{name} has invalid dimension: {dim}\")\n    else:\n        raise ValueError(f\"{name} must be a dictionary, numpy array, or list.\")\n\n    if items_array.ndim != dim:\n        raise ValueError(f\"{name} has invalid dimension: {items_array.ndim}\")\n\n    if dim == 1:\n        if items_array.shape != (n_assets,):\n            raise ValueError(\n                f\"{name} has invalid shape: {items_array.shape}. Expected shape: {(n_assets,)}\"\n            )\n    elif dim == 2:\n        if items_array.shape != (n_groups, n_assets):\n            raise ValueError(\n                f\"{name} has invalid shape: {items_array.shape}. Expected shape: {(n_groups, n_assets)}\"\n            )\n    else:\n        raise ValueError"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if dim == 1:\n        if isinstance(items, dict):\n            if assets_names is None:\n                raise ValueError(\n                    f\"{name} is a dictionary but assets_names is None. \"\n                    \"Please provide the assets_names.\"\n                )\n            items = np.array([items.get(asset, fill_value) for asset in assets_names])\n        elif not isinstance(items, np.ndarray):\n            items = np.array(items)\n        if items.ndim != 1:\n            raise ValueError(\n                f\"{name} must be a 1-dimensional array or dictionary with {n_assets} elements.\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} elements, but has {items.shape[0]} elements.\"\n            )\n        return items\n    elif dim == 2:\n        if isinstance(items, dict):\n            if assets_names is None:\n                raise ValueError(\n                    f\"{name} is a dictionary but assets_names is None. \"\n                    \"Please provide the assets_names.\"\n                )\n            items = np.array(\n                [\n                    [items.get(asset, fill_value) for asset in assets_names]\n                    for _ in range(n_groups)\n                ]\n            )\n        elif not isinstance(items, np.ndarray):\n            items = np.array(items)\n        if items.ndim != 2:\n            raise ValueError(\n                f\"{name} must be a 2-dimensional array or dictionary with {n_groups}x{n_assets} elements.\"\n            )\n        if items.shape != (n_groups, n_assets):\n            raise ValueError(\n                f\"{name} must have shape ({n_groups}, {n_assets}), but has shape {items.shape}.\"\n            )\n        return items\n    else:\n        raise ValueError(\"dim must be 1 or 2.\")\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Convert items to numpy array\n    items = np.array(items)\n\n    # If items is 1D and dim is 2, reshape to 2D\n    if items.ndim == 1 and dim == 2:\n        items = items.reshape(-1, 1)\n\n    # If items is 2D and dim is 1, check if it can be reshaped to 1D\n    if items.ndim == 2 and dim == 1:\n        if items.shape[0] == 1 or items.shape[1] == 1:\n            items = items.ravel()\n        else:\n            raise ValueError(\n                f\"{name} must be 1D. Got shape {items.shape} instead.\"\n            )\n\n    # If items is 1D and dim is 1, check if it can be reshaped to 2D\n    if items.ndim == 1 and dim == 2:\n        if items.size == n_assets:\n            items = items.reshape(1, -1)\n        else:\n            raise ValueError(\n                f\"{name} must have shape (1, {n_assets}) or ({n_assets},). Got shape {items.shape} instead.\"\n            )\n\n    # If items is 2D and dim is 2, check if it has the expected shape\n    if items.ndim == 2 and dim == 2:\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"{name} must have shape (n_groups, {n_assets}). Got shape {items.shape} instead.\"\n            )\n\n    # If items is a dictionary, convert it to numpy array\n    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"assets_names must be provided when items is a dictionary.\"\n            )\n        items = np.array([items.get(name, fill_value) for name in assets_names])\n\n    # If items is still a dictionary after conversion, raise an error\n    if isinstance(items, dict):\n        raise ValueError("}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if dim == 1:\n            if assets_names is None:\n                assets_names = np.array(list(items.keys()))\n            items_array = np.full(n_assets, fill_value)\n            for i, asset in enumerate(assets_names):\n                if asset in items:\n                    items_array[i] = items[asset]\n            return items_array\n        else:\n            if assets_names is None:\n                assets_names = np.array(list(items.keys()))\n            items_array = np.full((n_groups, n_assets), fill_value)\n            for i, asset in enumerate(assets_names):\n                if asset in items:\n                    items_array[:, i] = items[asset]\n            return items_array\n    elif isinstance(items, (np.ndarray, npt.NDArray)):\n        if dim == 1:\n            items_array = np.array(items)\n            if len(items_array.shape) == 1:\n                if items_array.shape[0] != n_assets:\n                    raise ValueError(\n                        f\"{name} must be a 1D array with length {n_assets}\"\n                    )\n                return items_array\n            elif len(items_array.shape) == 2:\n                if items_array.shape[0] != 1 and items_array.shape[1] != 1:\n                    raise ValueError(\n                        f\"{name} must be a 1D array with length {n_assets}\"\n                    )\n                return items_array.flatten()\n            else:\n                raise ValueError(f\"{name} must be a 1D array with length {n_assets}\")\n        else:\n            items_array = np.array(items)\n            if len(items_array.shape) == 1:\n                if items_array.shape[0] != n_assets:\n                    raise ValueError(\n                        f\"{name} must be a 2D array with shape ({n_groups}, {n_assets})\"\n                    )\n                return"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"assets_names must be provided when items is a dictionary.\"\n            )\n\n        items = np.array(\n            [items.get(asset, fill_value) for asset in assets_names],\n            dtype=np.float64,\n        )\n\n    if dim == 1:\n        if items.ndim == 1:\n            items = items.reshape(-1, 1)\n        elif items.ndim == 2:\n            if items.shape[0] == 1:\n                items = items.reshape(-1, 1)\n            else:\n                raise ValueError(\n                    f\"{name} must have shape (n_assets,) or (n_assets, 1), \"\n                    f\"but it has shape {items.shape}.\"\n                )\n        else:\n            raise ValueError(\n                f\"{name} must have shape (n_assets,) or (n_assets, 1), \"\n                f\"but it has shape {items.shape}.\"\n            )\n    elif dim == 2:\n        if items.ndim == 1:\n            items = items.reshape(1, -1)\n        elif items.ndim == 2:\n            if items.shape[0] == n_assets:\n                items = items.reshape(1, -1)\n            else:\n                raise ValueError(\n                    f\"{name} must have shape (n_groups, n_assets) or (1, n_assets), \"\n                    f\"but it has shape {items.shape}.\"\n                )\n        else:\n            raise ValueError(\n                f\"{name} must have shape (n_groups, n_assets) or (1, n_assets), \"\n                f\"but it has shape {items.shape}.\"\n            )\n    else:\n        raise ValueError(f\"dim must be 1 or 2, but it is {dim}.\")\n\n    return items\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if dim == 1:\n            items = np.array([items[asset] for asset in assets_names])\n        elif dim == 2:\n            items = np.array([[items[asset] for asset in assets_names]])\n        else:\n            raise ValueError(\n                f\"Invalid dim value. Expected 1 or 2, got {dim} for {name}.\"\n            )\n    elif isinstance(items, npt.ArrayLike):\n        items = np.array(items)\n    else:\n        raise TypeError(f\"Invalid type for {name}. Expected dict or array-like.\")\n\n    if dim == 1:\n        if items.ndim != 1:\n            raise ValueError(f\"Invalid shape for {name}. Expected (n_assets,), got {items.shape}.\")\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"Invalid number of assets for {name}. Expected {n_assets}, got {items.shape[0]}.\"\n            )\n        if np.isnan(items).any():\n            raise ValueError(f\"Missing values found in {name}.\")\n    elif dim == 2:\n        if items.ndim != 2:\n            raise ValueError(\n                f\"Invalid shape for {name}. Expected (n_groups, n_assets), got {items.shape}.\"\n            )\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"Invalid number of assets for {name}. Expected {n_assets}, got {items.shape[1]}.\"\n            )\n        if np.isnan(items).any():\n            raise ValueError(f\"Missing values found in {name}.\")\n    else:\n        raise ValueError(f\"Invalid dim value. Expected 1 or 2, got {dim} for {name}.\")\n\n    return items\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if dim == 1:\n            if assets_names is None:\n                assets_names = np.array(list(items.keys()))\n            else:\n                assets_names = np.array(assets_names)\n            if assets_names.shape[0] != n_assets:\n                raise ValueError(\n                    f\"The number of assets in {name} ({assets_names.shape[0]}) does not match the expected number ({n_assets}).\"\n                )\n            array = np.full(n_assets, fill_value)\n            for i, asset in enumerate(assets_names):\n                if asset in items:\n                    array[i] = items[asset]\n        elif dim == 2:\n            if assets_names is None:\n                assets_names = np.array(list(items.keys()))\n            else:\n                assets_names = np.array(assets_names)\n            if assets_names.shape[0] != n_assets:\n                raise ValueError(\n                    f\"The number of assets in {name} ({assets_names.shape[0]}) does not match the expected number ({n_assets}).\"\n                )\n            array = np.full((n_groups, n_assets), fill_value)\n            for i, asset in enumerate(assets_names):\n                if asset in items:\n                    array[:, i] = items[asset]\n        else:\n            raise ValueError(\n                f\"Invalid dimension value. Expected 1 or 2, got {dim} for {name}.\"\n            )\n    else:\n        array = np.array(items)\n        if array.shape != (n_assets,) and array.shape != (n_groups, n_assets):\n            raise ValueError(\n                f\"The shape of {name} ({array.shape}) does not match the expected shape ({(n_groups, n_assets)}) for dim={dim}.\"\n            )\n\n    return array\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            assets_names = np.array(list(items.keys()))\n        else:\n            assets_names = np.array(assets_names)\n        items = np.array(list(items.values()))\n\n    elif isinstance(items, (list, np.ndarray, pd.Series)):\n        items = np.array(items)\n\n    else:\n        raise TypeError(\n            f\"{name} must be either a dictionary or an array-like structure. \"\n        )\n\n    if dim == 1:\n        if items.ndim != 1:\n            raise ValueError(f\"{name} must be a 1-dimensional array.\")\n\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have shape (n_assets,), but it has shape {items.shape}.\"\n            )\n\n        if assets_names is not None:\n            missing_assets = set(assets_names) - set(items)\n            if missing_assets:\n                items = np.append(items, np.full(len(missing_assets), fill_value))\n\n    elif dim == 2:\n        if items.ndim != 2:\n            raise ValueError(f\"{name} must be a 2-dimensional array.\")\n\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"{name} must have shape (n_groups, n_assets), but it has shape {items.shape}.\"\n            )\n\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\"assets_names must be provided when items is a dictionary\")\n        if dim == 1:\n            items_array = np.array(\n                [items.get(asset, fill_value) for asset in assets_names],\n                dtype=float,\n            )\n        elif dim == 2:\n            items_array = np.array(\n                [\n                    [items.get(asset, fill_value) for asset in assets_names]\n                    for _ in range(n_assets)\n                ],\n                dtype=float,\n            )\n        else:\n            raise ValueError(\"dim must be either 1 or 2\")\n    else:\n        items_array = np.array(items, dtype=float)\n\n    if items_array.ndim != dim:\n        raise ValueError(\n            f\"{name} must be a {dim}D array. The shape of {name} is {items_array.shape}\"\n        )\n\n    if dim == 1:\n        if items_array.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} elements. The shape of {name} is {items_array.shape}\"\n            )\n    elif dim == 2:\n        if items_array.shape[0] != n_assets or items_array.shape[1] != n_assets:\n            raise ValueError(\n                f\"{name} must have shape ({n_assets}, {n_assets}). The shape of {name} is {items_array.shape}\"\n            )\n\n    return items_array\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            assets_names = np.array(list(items.keys()))\n        else:\n            assets_names = np.array(assets_names)\n        items = np.array([items.get(name, fill_value) for name in assets_names])\n    elif isinstance(items, npt.ArrayLike):\n        items = np.array(items)\n    else:\n        raise TypeError(f\"{name} must be either a dictionary or an array-like structure.\")\n\n    if dim == 1:\n        if items.shape != (n_assets,):\n            raise ValueError(f\"{name} must have shape (n_assets,).\")\n    elif dim == 2:\n        if items.shape[0] != n_assets:\n            raise ValueError(f\"{name} must have shape (n_groups, n_assets).\")\n    else:\n        raise ValueError(\"dim must be either 1 or 2.\")\n\n    return items\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Check if items is a dictionary\n    if isinstance(items, dict):\n        # Check if assets_names is provided\n        if assets_names is None:\n            raise ValueError(\n                \"assets_names must be provided when items is a dictionary.\"\n            )\n        # Check if items has the same length as assets_names\n        if len(items) != len(assets_names):\n            raise ValueError(\n                \"items and assets_names must have the same length.\"\n            )\n        # Check if all keys in items are in assets_names\n        if not all(k in assets_names for k in items):\n            raise ValueError(\n                \"All keys in items must be present in assets_names.\"\n            )\n        # Create an array of fill_value with the same shape as items\n        items = np.full(len(items), fill_value)\n        # Fill the array with values from items\n        for i, k in enumerate(assets_names):\n            if k in items:\n                items[i] = items[k]\n    # Check if items is array-like\n    elif isinstance(items, npt.ArrayLike):\n        # Convert items to a numpy array\n        items = np.array(items)\n    else:\n        # Raise an error if items is neither a dictionary nor array-like\n        raise TypeError(\n            f\"{name} must be either a dictionary or array-like.\"\n        )\n\n    # Check if items is 1D or 2D\n    if items.ndim == 1:\n        # Reshape items to 2D if dim is 2\n        if dim == 2:\n            items = items.reshape(1, -1)\n    elif items.ndim == 2:\n        # Check if items has the correct number of columns\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} columns.\"\n            )\n    else:\n        # Raise an error if items has more than 2 dimensions\n        raise ValueError(\n            f\"{name} must be either 1D or 2"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if dim == 1:\n            items = np.array([items.get(a, fill_value) for a in assets_names])\n        elif dim == 2:\n            items = np.array(\n                [\n                    [items.get(f\"{a}_group_{g}\", fill_value) for a in assets_names]\n                    for g in range(n_groups)\n                ]\n            )\n    else:\n        items = np.array(items)\n\n    if dim == 1:\n        assert items.shape == (\n            n_assets,\n        ), f\"{name} must have shape (n_assets,).\"\n    elif dim == 2:\n        assert items.shape == (\n            n_groups,\n            n_assets,\n        ), f\"{name} must have shape (n_groups, n_assets).\"\n\n    return items\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Convert items to numpy array\n    items = np.array(items, dtype=float)\n\n    # If items is a dictionary, fill missing values\n    if isinstance(items, dict):\n        if assets_names is None:\n            assets_names = np.array(list(items.keys()))\n        items = np.array(\n            [items.get(name, fill_value) for name in assets_names], dtype=float\n        )\n\n    # If items is a scalar, replicate it for all assets\n    if np.isscalar(items):\n        items = np.full(n_assets, items, dtype=float)\n\n    # Ensure items is 1D array\n    items = np.ravel(items)\n\n    # Check and raise exceptions for array shape\n    if dim == 1:\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"{name} must have shape ({n_assets},) or {n_assets} elements\"\n            )\n    elif dim == 2:\n        if len(items) != n_assets * n_groups:\n            raise ValueError(\n                f\"{name} must have shape ({n_assets}, {n_groups}) or {n_assets * n_groups} elements\"\n            )\n    else:\n        raise ValueError(\"dim must be either 1 or 2\")\n\n    # Reshape items to 2D array\n    items = items.reshape(n_assets, dim)\n\n    return items\n\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        return js['class_name'](\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n            **{\n                k: v\n                for k, v in data.items()\n                if k in js['class_name'].__init__.__code__.co_varnames\n            }\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent with the required parameters\n        agent = js['class_name'](\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n            **data['required_parameters']\n        )\n\n        # Set optional parameters if they exist in the dictionary\n        if 'optional_parameters' in data:\n            for key, value in data['optional_parameters'].items():\n                setattr(agent, key, value)\n\n        return agent"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        return cls(\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n            **{\n                k: v\n                for k, v in data.items()\n                if k in inspect.signature(cls).parameters\n            }\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent object with the required parameters\n        micro_agent = js['class_name'](agent_lifecycle, openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the input dictionary\n        micro_agent.id = data.get('id', None)\n        micro_agent.name = data.get('name', None)\n        micro_agent.description = data.get('description', None)\n        micro_agent.prompt = data.get('prompt', None)\n        micro_agent.model = data.get('model', None)\n        micro_agent.temperature = data.get('temperature', None)\n        micro_agent.max_tokens = data.get('max_tokens', None)\n        micro_agent.top_p = data.get('top_p', None)\n        micro_agent.frequency_penalty = data.get('frequency_penalty', None)\n        micro_agent.presence_penalty = data.get('presence_penalty', None)\n        micro_agent.best_of = data.get('best_of', None)\n        micro_agent.stop = data.get('stop', None)\n        micro_agent.n = data.get('n', None)\n        micro_agent.stream = data.get('stream', None)\n        micro_agent.logprobs = data.get('logprobs', None)\n        micro_agent.echo = data.get('echo', None)\n        micro_agent.suffix = data.get('suffix', None)\n        micro_agent.agent_lifecycle = agent_lifecycle\n        micro_agent.openai_wrapper = openai_wrapper\n\n        # Return the deserialized MicroAgent object\n        return micro_agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        return js['class_name'](\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n            **data\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        if \"name\" in data:\n            name = data[\"name\"]\n        else:\n            name = None\n\n        if \"verbose\" in data:\n            verbose = data[\"verbose\"]\n        else:\n            verbose = None\n\n        if \"agent_type\" in data:\n            agent_type = data[\"agent_type\"]\n        else:\n            agent_type = None\n\n        if \"llm_type\" in data:\n            llm_type = data[\"llm_type\"]\n        else:\n            llm_type = None\n\n        if \"prompt\" in data:\n            prompt = data[\"prompt\"]\n        else:\n            prompt = None\n\n        if \"input_variables\" in data:\n            input_variables = data[\"input_variables\"]\n        else:\n            input_variables = None\n\n        if \"output_parser\" in data:\n            output_parser = data[\"output_parser\"]\n        else:\n            output_parser = None\n\n        if \"max_tokens\" in data:\n            max_tokens = data[\"max_tokens\"]\n        else:\n            max_tokens = None\n\n        if \"temperature\" in data:\n            temperature = data[\"temperature\"]\n        else:\n            temperature = None\n\n        if \"top_p\" in data:\n            top_p = data[\"top_p\"]\n        else:\n            top_p = None\n\n        if \"n\" in data:\n            n = data[\"n\"]\n        else:\n            n = None\n\n        if \"best_of\" in data:\n            best_of = data[\"best_of\"]\n        else:\n            best_of = None\n\n        if \"stream\" in data:\n            stream = data[\"stream\"]\n        else:\n            stream = None\n\n        if \"stop\" in data:\n            stop = data[\"stop\"]\n        else:\n            stop = None\n\n        if \"timeout\" in data:\n            timeout = data[\"timeout\"]\n        else:\n            timeout = None\n\n        if \"logprobs\" in data:\n            logprobs = data[\"logpro"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent object with the agent_lifecycle and openai_wrapper arguments\n        agent = js['class_name'](agent_lifecycle, openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the dictionary's key-value pairs\n        agent.name = data.get('name', None)\n        agent.description = data.get('description', None)\n        agent.prompt_template = data.get('prompt_template', None)\n        agent.prompt_input_variables = data.get('prompt_input_variables', None)\n        agent.prompt_output_variables = data.get('prompt_output_variables', None)\n        agent.tool_names = data.get('tool_names', None)\n        agent.tool_descriptions = data.get('tool_descriptions', None)\n        agent.tool_prompt_templates = data.get('tool_prompt_templates', None)\n        agent.tool_prompt_input_variables = data.get('tool_prompt_input_variables', None)\n        agent.tool_prompt_output_variables = data.get('tool_prompt_output_variables', None)\n        agent.tool_output_parsers = data.get('tool_output_parsers', None)\n        agent.tool_output_variable_mappings = data.get('tool_output_variable_mappings', None)\n        agent.tool_action_variables = data.get('tool_action_variables', None)\n        agent.tool_action_descriptions = data.get('tool_action_descriptions', None)\n        agent.tool_action_examples = data.get('tool_action_examples', None)\n        agent.tool_action_output_parsers = data.get('tool_action_output_parsers', None)\n        agent.tool_action_output_variable_mappings = data.get('tool_action_output_variable_mappings', None)\n        agent.tool_action_costs = data.get('tool_action"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent with the required parameters\n        agent = js['class_name'](agent_lifecycle=agent_lifecycle, openai_wrapper=openai_wrapper)\n\n        # Set the agent's attributes based on the dictionary's key-value pairs\n        agent.id = data.get('id')\n        agent.name = data.get('name')\n        agent.description = data.get('description')\n        agent.is_active = data.get('is_active')\n        agent.is_archived = data.get('is_archived')\n        agent.is_public = data.get('is_public')\n        agent.is_shared = data.get('is_shared')\n        agent.is_deleted = data.get('is_deleted')\n        agent.is_favorite = data.get('is_favorite')\n        agent.is_read_only = data.get('is_read_only')\n        agent.is_template = data.get('is_template')\n        agent.is_template_shared = data.get('is_template_shared')\n        agent.is_template_deleted = data.get('is_template_deleted')\n        agent.is_template_favorite = data.get('is_template_favorite')\n        agent.is_template_read_only = data.get('is_template_read_only')\n        agent.is_template_public = data.get('is_template_public')\n        agent.is_template_archived = data.get('is_template_archived')\n        agent.is_template_active = data.get('is_template_active')\n        agent.is_template_shared_with_me = data.get('is_template_shared_with_me')\n        agent.is_template_shared_with_organization = data.get('is_template_shared_with_organization')\n        agent.is_template_shared_with_world = data.get('is_template_shared_with_world')\n        agent.is_template_shared_with_domain = data.get('"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        return js['class_name'](\n            data.get('name', None),\n            data.get('description', None),\n            data.get('prompt', None),\n            data.get('temperature', 0.5),\n            data.get('max_tokens', 100),\n            data.get('top_p', 1),\n            data.get('frequency_penalty', 0),\n            data.get('presence_penalty', 0),\n            data.get('stop', None),\n            data.get('best_of', 1),\n            data.get('echo', False),\n            data.get('logprobs', None),\n            data.get('user', None),\n            data.get('agent_type', None),\n            data.get('lifecycle_state', agent_lifecycle),\n            openai_wrapper\n        )"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent with the required parameters\n        micro_agent = js['class_name'](agent_lifecycle=agent_lifecycle, openai_wrapper=openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the input dictionary\n        micro_agent.name = data.get('name', '')\n        micro_agent.description = data.get('description', '')\n        micro_agent.agent_type = data.get('agent_type', '')\n        micro_agent.agent_type_description = data.get('agent_type_description', '')\n        micro_agent.is_active = data.get('is_active', False)\n        micro_agent.is_deleted = data.get('is_deleted', False)\n        micro_agent.is_deleted_at = data.get('is_deleted_at', None)\n        micro_agent.is_deleted_by = data.get('is_deleted_by', None)\n        micro_agent.is_deleted_by_name = data.get('is_deleted_by_name', None)\n        micro_agent.is_deleted_by_email = data.get('is_deleted_by_email', None)\n        micro_agent.is_deleted_by_username = data.get('is_deleted_by_username', None)\n        micro_agent.is_deleted_by_image = data.get('is_deleted_by_image', None)\n        micro_agent.is_deleted_by_thumbnail = data.get('is_deleted_by_thumbnail', None)\n        micro_agent.is_deleted_by_workspace = data.get('is_deleted_by_workspace', None)\n        micro_agent.is_deleted_by_workspace_name = data.get('is_deleted_by_workspace_name', None)\n        micro_agent.is_deleted_by_workspace_image = data.get('is_deleted_by_workspace_image', None)\n        micro_agent.is_"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        return js['class_name'](\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n            **data\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent with the required arguments\n        microagent = js['class_name'](\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n\n        # Set optional attributes if they exist in the dictionary\n        if 'agent_id' in data:\n            microagent.agent_id = data['agent_id']\n        if 'agent_type' in data:\n            microagent.agent_type = data['agent_type']\n        if 'agent_name' in data:\n            microagent.agent_name = data['agent_name']\n        if 'agent_description' in data:\n            microagent.agent_description = data['agent_description']\n        if 'agent_prompt' in data:\n            microagent.agent_prompt = data['agent_prompt']\n        if 'agent_scope' in data:\n            microagent.agent_scope = data['agent_scope']\n        if 'agent_version' in data:\n            microagent.agent_version = data['agent_version']\n        if 'agent_dictionary' in data:\n            microagent.agent_dictionary = data['agent_dictionary']\n        if 'agent_rewards' in data:\n            microagent.agent_rewards = data['agent_rewards']\n        if 'agent_punishments' in data:\n            microagent.agent_punishments = data['agent_punishments']\n        if 'agent_abilities' in data:\n            microagent.agent_abilities = data['agent_abilities']\n        if 'agent_personality' in data:\n            microagent.agent_personality = data['agent_personality']\n        if 'agent_emotions' in data:\n            microagent.agent_emotions = data['agent_emotions']\n        if 'agent_mood' in data:\n            microagent.agent_mood = data['agent_mood']\n        if 'agent_goals' in data:\n            microagent.agent_goals = data['agent_goals']\n        if 'agent_behav"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent object with the required parameters\n        micro_agent = js['class_name'](agent_lifecycle=agent_lifecycle, openai_wrapper=openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the input dictionary\n        micro_agent.name = data.get('name', None)\n        micro_agent.description = data.get('description', None)\n        micro_agent.init_system_message = data.get('init_system_message', None)\n        micro_agent.agent_type = data.get('agent_type', None)\n        micro_agent.agent_scope = data.get('agent_scope', None)\n        micro_agent.agent_scope_class = data.get('agent_scope_class', None)\n        micro_agent.agent_scope_object = data.get('agent_scope_object', None)\n        micro_agent.agent_scope_object_type = data.get('agent_scope_object_type', None)\n        micro_agent.agent_scope_object_name = data.get('agent_scope_object_name', None)\n        micro_agent.agent_scope_object_description = data.get('agent_scope_object_description', None)\n        micro_agent.agent_scope_object_image_url = data.get('agent_scope_object_image_url', None)\n        micro_agent.agent_scope_object_image_data = data.get('agent_scope_object_image_data', None)\n        micro_agent.agent_scope_object_image_type = data.get('agent_scope_object_image_type', None)\n        micro_agent.agent_scope_object_image_name = data.get('agent_scope_object_image_name', None)\n        micro_agent.agent_scope_object_image_description = data.get('agent_scope_object_image_description', None)\n        micro_agent.agent_scope_object_image_size = data.get('agent_scope_object_image_size', None)\n        micro_agent.agent_scope_object_image"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent object with the agent_lifecycle and openai_wrapper arguments\n        micro_agent = js['class_name'](agent_lifecycle=agent_lifecycle, openai_wrapper=openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the input dictionary's key-value pairs\n        micro_agent.id = data.get('id')\n        micro_agent.name = data.get('name')\n        micro_agent.description = data.get('description')\n        micro_agent.prompt = data.get('prompt')\n        micro_agent.model = data.get('model')\n        micro_agent.max_tokens = data.get('max_tokens')\n        micro_agent.temperature = data.get('temperature')\n        micro_agent.top_p = data.get('top_p')\n        micro_agent.frequency_penalty = data.get('frequency_penalty')\n        micro_agent.presence_penalty = data.get('presence_penalty')\n        micro_agent.best_of = data.get('best_of')\n        micro_agent.n = data.get('n')\n        micro_agent.stop = data.get('stop')\n        micro_agent.logprobs = data.get('logprobs')\n        micro_agent.echo = data.get('echo')\n        micro_agent.agent_lifecycle = data.get('agent_lifecycle')\n        micro_agent.openai_wrapper = data.get('openai_wrapper')\n\n        # Return the deserialized MicroAgent object\n        return micro_agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent object with the required parameters\n        agent = js['class_name'](agent_lifecycle, openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the input dictionary\n        agent.name = data.get('name', None)\n        agent.description = data.get('description', None)\n        agent.model = data.get('model', None)\n        agent.temperature = data.get('temperature', None)\n        agent.max_tokens = data.get('max_tokens', None)\n        agent.top_p = data.get('top_p', None)\n        agent.frequency_penalty = data.get('frequency_penalty', None)\n        agent.presence_penalty = data.get('presence_penalty', None)\n        agent.stop = data.get('stop', None)\n        agent.best_of = data.get('best_of', None)\n        agent.n = data.get('n', None)\n        agent.logprobs = data.get('logprobs', None)\n        agent.echo = data.get('echo', None)\n        agent.suffix = data.get('suffix', None)\n        agent.agent_type = data.get('agent_type', None)\n        agent.prompt_template = data.get('prompt_template', None)\n        agent.examples = data.get('examples', None)\n        agent.examples_file = data.get('examples_file', None)\n        agent.examples_variables = data.get('examples_variables', None)\n        agent.prompt_variables = data.get('prompt_variables', None)\n        agent.prompt_variables_values = data.get('prompt_variables_values', None)\n        agent.prompt_variables_explanations = data.get('prompt_variables_explanations', None)\n        agent.prompt_variables_examples = data.get('prompt_variables_examples', None)\n        agent.prompt_variables_data_type = data.get('prom"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent with the required arguments\n        microagent = js['class_name'](agent_lifecycle, openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the dictionary's key-value pairs\n        microagent.id = data.get('id', None)\n        microagent.name = data.get('name', None)\n        microagent.description = data.get('description', None)\n        microagent.prompt = data.get('prompt', None)\n        microagent.completion = data.get('completion', None)\n        microagent.inputs = data.get('inputs', None)\n        microagent.outputs = data.get('outputs', None)\n        microagent.type = data.get('type', None)\n        microagent.status = data.get('status', None)\n        microagent.created_at = data.get('created_at', None)\n        microagent.updated_at = data.get('updated_at', None)\n\n        # Return the deserialized MicroAgent object\n        return microagent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent with the agent_lifecycle and openai_wrapper arguments\n        agent = js['class_name'](agent_lifecycle=agent_lifecycle, openai_wrapper=openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the dictionary's key-value pairs\n        agent.name = data.get('name', '')\n        agent.description = data.get('description', '')\n        agent.prompt = data.get('prompt', '')\n        agent.input_variables = data.get('input_variables', [])\n        agent.output_variables = data.get('output_variables', [])\n        agent.prompt_template = data.get('prompt_template', '')\n        agent.memory = data.get('memory', [])\n        agent.memory_variables = data.get('memory_variables', [])\n        agent.memory_initialization_template = data.get('memory_initialization_template', '')\n        agent.memory_compression_template = data.get('memory_compression_template', '')\n        agent.memory_retrieval_template = data.get('memory_retrieval_template', '')\n        agent.memory_summary_template = data.get('memory_summary_template', '')\n        agent.memory_length = data.get('memory_length', 0)\n        agent.memory_compression = data.get('memory_compression', False)\n        agent.memory_compression_ratio = data.get('memory_compression_ratio', 0)\n        agent.memory_compression_chunk_overlap = data.get('memory_compression_chunk_overlap', 0)\n        agent.memory_compression_chunk_size = data.get('memory_compression_chunk_size', 0)\n        agent.memory_retrieval_mode = data.get('memory_retrieval_mode', '')\n        agent.memory_retrieval_chunk_size = data.get('memory_retrieval_chunk_size', 0)"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent with the required parameters\n        agent = js['class_name'](agent_lifecycle, openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the input dictionary\n        agent.name = data['name']\n        agent.description = data['description']\n        agent.prompt = data['prompt']\n        agent.examples = data['examples']\n        agent.input_variables = data['input_variables']\n        agent.output_variables = data['output_variables']\n        agent.model = data['model']\n        agent.temperature = data['temperature']\n        agent.max_tokens = data['max_tokens']\n        agent.top_p = data['top_p']\n        agent.frequency_penalty = data['frequency_penalty']\n        agent.presence_penalty = data['presence_penalty']\n        agent.stop = data['stop']\n        agent.best_of = data['best_of']\n        agent.n = data['n']\n        agent.logprobs = data['logprobs']\n        agent.echo = data['echo']\n        agent.suffix = data['suffix']\n        agent.max_retries = data['max_retries']\n        agent.retry_interval = data['retry_interval']\n        agent.verbose = data['verbose']\n        agent.save_agent = data['save_agent']\n        agent.save_path = data['save_path']\n\n        # Return the deserialized MicroAgent object\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent with the provided lifecycle state and OpenAI wrapper\n        agent = js['class_name'](agent_lifecycle=agent_lifecycle, openai_wrapper=openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the input dictionary\n        agent.id = data.get('id')\n        agent.name = data.get('name')\n        agent.description = data.get('description')\n        agent.agent_type = data.get('agent_type')\n        agent.is_active = data.get('is_active')\n        agent.is_private = data.get('is_private')\n        agent.is_ready = data.get('is_ready')\n        agent.is_busy = data.get('is_busy')\n        agent.is_deleted = data.get('is_deleted')\n        agent.is_starred = data.get('is_starred')\n        agent.is_upgradable = data.get('is_upgradable')\n        agent.is_shared = data.get('is_shared')\n        agent.is_public = data.get('is_public')\n        agent.is_collaborative = data.get('is_collaborative')\n        agent.is_template = data.get('is_template')\n        agent.is_forkable = data.get('is_forkable')\n        agent.is_local = data.get('is_local')\n        agent.is_remote = data.get('is_remote')\n        agent.is_example = data.get('is_example')\n        agent.is_temporary = data.get('is_temporary')\n        agent.is_trial = data.get('is_trial')\n        agent.is_subscription = data.get('is_subscription')\n        agent.is_subscribed = data.get('is_subscribed')\n        agent.is_premium = data.get('is_premium')\n        agent.is_pro = data.get('is_pro')\n        agent.is_free = data"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent with the lifecycle state and OpenAI wrapper\n        microagent = js['class_name'](agent_lifecycle, openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the input dictionary\n        microagent.id = data.get('id')\n        microagent.name = data.get('name')\n        microagent.description = data.get('description')\n        microagent.prompt_template = data.get('prompt_template')\n        microagent.prompt_input_variables = data.get('prompt_input_variables')\n        microagent.output_parser = data.get('output_parser')\n        microagent.output_key = data.get('output_key')\n        microagent.on_success_destination = data.get('on_success_destination')\n        microagent.on_failure_destination = data.get('on_failure_destination')\n        microagent.on_finish_destination = data.get('on_finish_destination')\n        microagent.model = data.get('model')\n        microagent.temperature = data.get('temperature')\n        microagent.max_tokens = data.get('max_tokens')\n        microagent.top_p = data.get('top_p')\n        microagent.frequency_penalty = data.get('frequency_penalty')\n        microagent.presence_penalty = data.get('presence_penalty')\n        microagent.stop = data.get('stop')\n        microagent.best_of = data.get('best_of')\n        microagent.timeout = data.get('timeout')\n\n        # Return the deserialized MicroAgent object\n        return microagent\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  srgb = xnp.clip(srgb, 0, 1)\n\n  linear = xnp.where(\n    srgb <= 0.0404482362771082,\n    srgb / 12.92,\n    xnp.power((srgb + 0.055) / 1.055, 2.4)\n  )\n\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  srgb = xnp.clip(srgb, 0, 1)\n\n  linear = xnp.where(\n    srgb <= 0.0404482362771082,\n    srgb / 12.92,\n    xnp.power((srgb + 0.055) / 1.055, 2.4)\n  )\n\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  srgb = xnp.clip(srgb, 0, 1)\n  linear = xnp.where(srgb <= 0.0404482362771082, srgb / 12.92, xnp.power((srgb + 0.055) / 1.055, 2.4))\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  srgb = xnp.clip(srgb, 0, 1)\n  linear = xnp.where(\n    srgb <= 0.0404482362771082,\n    srgb / 12.92,\n    xnp.power((srgb + 0.055) / 1.055, 2.4)\n  )\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  srgb = xnp.clip(srgb, 0, 1)\n  srgb_linear = xnp.where(\n    srgb <= 0.0404482362771082,\n    srgb / 12.92,\n    xnp.power((srgb + 0.055) / 1.055, 2.4)\n  )\n\n  return srgb_linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  srgb = xnp.asarray(srgb)\n  linear = xnp.where(srgb <= 0.0404482362771082, srgb / 12.92, xnp.power((srgb + 0.055) / 1.055, 2.4))\n\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  srgb = xnp.clip(srgb, 0, 1)\n  linear = xnp.where(srgb <= 0.0404482362771082, srgb / 12.92, xnp.power((srgb + 0.055) / 1.055, 2.4))\n\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  srgb = xnp.clip(srgb, 0, 1)\n  linear = xnp.where(srgb <= 0.04045, srgb / 12.92, ((srgb + 0.055) / 1.055) ** 2.4)\n\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  srgb = xnp.clip(srgb, 0, 1)\n\n  linear = xnp.where(\n    srgb <= 0.0404482362771082,\n    srgb / 12.92,\n    xnp.power((srgb + 0.055) / 1.055, 2.4),\n  )\n\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  srgb = xnp.clip(srgb, 0, 1)\n  return xnp.where(srgb <= 0.0404482362771082, srgb / 12.92, xnp.power((srgb + 0.055) / 1.055, 2.4))\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Set the epsilon value if not provided\n  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  # Convert sRGB values to linear space using a piecewise function\n  linear = xnp.where(srgb <= 0.0404482362771082, srgb / 12.92, xnp.power((srgb + 0.055) / 1.055, 2.4))\n\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Set the epsilon value if not provided\n  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  # Convert the sRGB values to linear space using the piecewise function\n  linear = xnp.where(\n    srgb <= 0.0404482362771082,\n    srgb / 12.92,\n    xnp.power((srgb + 0.055) / 1.055, 2.4)\n  )\n\n  # Return the converted linear color values\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  # Convert sRGB values to linear space\n  linear = xnp.where(srgb <= 0.04045, srgb / 12.92, xnp.power((srgb + 0.055) / 1.055, 2.4))\n\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  srgb = xnp.clip(srgb, 0.0, 1.0)\n  linear = xnp.where(\n    srgb <= 0.04045,\n    srgb / 12.92,\n    ((srgb + 0.055) / 1.055) ** 2.4\n  )\n\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  srgb = xnp.clip(srgb, 0.0, 1.0)\n\n  linear = xnp.where(\n    srgb <= 0.04045,\n    srgb / 12.92,\n    ((srgb + 0.055) / 1.055) ** 2.4,\n  )\n\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Define the sRGB to linear conversion function\n  def srgb_to_linear_fn(x):\n    return x / 12.92 if x <= 0.04045 else ((x + 0.055) / 1.055) ** 2.4\n\n  # Convert sRGB values to linear space\n  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  return xnp.where(srgb <= 0.04045, srgb / 12.92, ((srgb + 0.055) / 1.055) ** 2.4)\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Set the epsilon value if not provided\n  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  # Convert sRGB values to linear space\n  linear = xnp.where(srgb <= 0.04045, srgb / 12.92, xnp.power((srgb + 0.055) / 1.055, 2.4))\n\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Set the epsilon value if not provided\n  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  # Convert the input sRGB values to a NumPy array\n  srgb = xnp.array(srgb)\n\n  # Clip the input values to the range [0, 1]\n  srgb = xnp.clip(srgb, 0, 1)\n\n  # Apply the piecewise function to convert sRGB to linear space\n  linear = xnp.where(\n    srgb <= 0.0404482362771082,\n    srgb / 12.92,\n    xnp.power((srgb + 0.055) / 1.055, 2.4)\n  )\n\n  # Return the converted linear space values\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Convert input to array if not already\n  srgb = xnp.asarray(srgb)\n\n  # Set eps to machine epsilon if not provided\n  if eps is None:\n    eps = xnp.finfo(srgb.dtype).eps\n\n  # Apply piecewise function to convert sRGB to linear space\n  linear = xnp.where(\n    srgb <= 0.04045,\n    srgb / 12.92,\n    xnp.power((srgb + 0.055) / 1.055, 2.4)\n  )\n\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Convert input to float32 if necessary\n  srgb = xnp.asarray(srgb, dtype=xnp.float32)\n\n  # Set eps if not provided\n  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  # Define a lambda function for the piecewise function\n  f = lambda x: x / 12.92\n  g = lambda x: ((x + eps) / (1 + eps)) ** 2.4\n\n  # Apply the piecewise function to the sRGB values\n  linear = xnp.where(srgb <= 0.04045, f(srgb), g(srgb))\n\n  return linear\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import splrep, splev\n\n  # Adjust the spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Compute the spline representation of x\n  tck = splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times\n  x_output = splev(t_output, tck)\n\n  return x_output\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import UnivariateSpline\n\n  # Adjust spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit spline to x\n  spline = UnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate spline at t_output\n  x_output = spline(t_output)\n\n  return x_output\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import splrep, splev\n\n  # Adjust the spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Compute the spline coefficients using splrep\n  tck = splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times using splev\n  x_output = splev(t_output, tck)\n\n  return x_output\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import UnivariateSpline\n\n  # Adjust the spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Create a spline object using the input signal x and input times t_input\n  spline = UnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n\n  # Extrapolate the signal at the output times t_output\n  x_output = spline(t_output)\n\n  return x_output\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  import scipy.interpolate as interpolate\n\n  # Adjust the spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Create the spline object\n  spline = interpolate.UnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times\n  x_output = spline(t_output)\n\n  return x_output"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Import packages\n  import numpy as np\n  from scipy.interpolate import splrep, splev\n\n  # Get the number of points in x\n  N = len(x)\n\n  # Adjust the spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, N - 1)\n\n  # Compute the spline coefficients\n  tck = splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times\n  x_output = splev(t_output, tck)\n\n  return x_output\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import make_interp_spline\n\n  if spline_degree >= len(t_input):\n    spline_degree = len(t_input) - 1\n\n  x_spline = make_interp_spline(t_input, x, k=spline_degree, s=smoothness)\n  x_interpolated = x_spline(t_output)\n\n  return x_interpolated\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import UnivariateSpline\n\n  if len(x) == 1:\n    return np.ones(len(t_output)) * x[0]\n\n  if len(x) == 2:\n    return np.ones(len(t_output)) * np.mean(x)\n\n  if len(x) == 3:\n    return np.ones(len(t_output)) * np.mean(x)\n\n  if len(x) == 4:\n    return np.ones(len(t_output)) * np.mean(x)\n\n  if len(x) == 5:\n    return np.ones(len(t_output)) * np.mean(x)\n\n  if len(x) == 6:\n    return np.ones(len(t_output)) * np.mean(x)\n\n  if len(x) == 7:\n    return np.ones(len(t_output)) * np.mean(x)\n\n  if len(x) == 8:\n    return np.ones(len(t_output)) * np.mean(x)\n\n  if len(x) == 9:\n    return np.ones(len(t_output)) * np.mean(x)\n\n  if len(x) == 10:\n    return np.ones(len(t_output)) * np.mean(x)\n\n  if len(x) == 11:\n    return np.ones(len(t_output)) * np.mean(x)\n\n  if len(x) == 12:\n    return np.ones(len(t_output)) * np.mean(x)\n\n  if len(x) == 13:\n    return np.ones(len(t_output)) * np.mean(x)\n\n  if len(x) == 14:\n    return np.ones(len(t_output)) * np.mean(x)\n\n  if len(x) == 15:\n    return np.ones(len(t_output)) * np.mean(x)\n\n  if len("}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import splrep, splev\n\n  # Convert input and output times to numpy arrays\n  t_input = np.array(t_input)\n  t_output = np.array(t_output)\n\n  # Adjust the spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Compute the spline representation of x\n  tck = splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline representation at the output times\n  x_output = splev(t_output, tck)\n\n  return x_output"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import InterpolatedUnivariateSpline\n\n  # Adjust the spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Create an InterpolatedUnivariateSpline object\n  spline = InterpolatedUnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n\n  # Extrapolate the signal at the output times\n  x_output = spline(t_output)\n\n  # Return the interpolated signal\n  return x_output\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  from scipy.interpolate import splrep, splev\n\n  # Compute the spline representation of the input signal\n  tck = splrep(t_input, x, k=min(spline_degree, len(x) - 1), s=smoothness)\n\n  # Evaluate the spline representation at the output times\n  x_output = splev(t_output, tck)\n\n  return x_output\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import UnivariateSpline\n\n  # Adjust spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Create a spline object\n  spline = UnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n\n  # Interpolate the signal at the output times\n  x_output = spline(t_output)\n\n  # Return the interpolated signal\n  return x_output\n\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Import necessary libraries\n  import numpy as np\n  from scipy.interpolate import splrep, splev\n\n  # Calculate the number of points in x\n  n = len(x)\n\n  # Adjust the spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, n - 1)\n\n  # Calculate the weights for the smoothing spline\n  w = np.ones(n)\n  w[0] = smoothness\n  w[-1] = smoothness\n\n  # Fit a smoothing spline to the data\n  tck = splrep(t_input, x, k=spline_degree, w=w)\n\n  # Evaluate the smoothing spline at the output times\n  x_output = splev(t_output, tck)\n\n  # Return the interpolated values\n  return x_output\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import make_interp_spline\n\n  # Compute the number of input and output times\n  n_input = len(t_input)\n  n_output = len(t_output)\n\n  # Adjust the spline degree to be at most one less than the number of input times\n  spline_degree = min(spline_degree, n_input - 1)\n\n  # Compute the smoothing factor\n  smoothing_factor = smoothness * (n_input - 1)\n\n  # Create a spline interpolation of the input signal\n  spline = make_interp_spline(t_input, x, k=spline_degree, s=smoothing_factor)\n\n  # Evaluate the spline at the output times\n  x_output = spline(t_output)\n\n  return x_output\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  import scipy.interpolate\n\n  if spline_degree >= len(x):\n    spline_degree = len(x) - 1\n\n  tck = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n  return scipy.interpolate.splev(t_output, tck)\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import UnivariateSpline\n\n  if spline_degree > len(x) - 1:\n    spline_degree = len(x) - 1\n\n  t_input = np.array(t_input)\n  t_output = np.array(t_output)\n  x = np.array(x)\n\n  s = UnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n  x_interp = s(t_output)\n\n  return x_interp\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Import packages\n  import numpy as np\n  import scipy.interpolate as interpolate\n\n  # Get the number of input and output times\n  n_input = len(t_input)\n  n_output = len(t_output)\n\n  # Get the number of points in x\n  n_points = len(x)\n\n  # Adjust the spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, n_points - 1)\n\n  # Create a spline object with the given degree and smoothness\n  spline = interpolate.UnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n\n  # Initialize an empty array to store the interpolated values\n  x_interpolated = np.zeros(n_output)\n\n  # Interpolate the values of x at the output times\n  for i in range(n_output):\n    x_interpolated[i] = spline(t_output[i])\n\n  return x_interpolated\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import UnivariateSpline\n\n  # Check if t_input and t_output are sorted\n  if not np.all(np.diff(t_input) >= 0):\n      raise ValueError(\"t_input must be sorted in ascending order\")\n  if not np.all(np.diff(t_output) >= 0):\n      raise ValueError(\"t_output must be sorted in ascending order\")\n\n  # Check if t_output is within the range of t_input\n  if t_output[0] < t_input[0] or t_output[-1] > t_input[-1]:\n      raise ValueError(\"t_output must be within the range of t_input\")\n\n  # Adjust spline_degree to be at most one less than the number of points in x\n  spline_degree = min(len(x) - 1, spline_degree)\n\n  # Fit the spline to the data\n  spl = UnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n\n  # Interpolate the data at the specified output times\n  x_interp = spl(t_output)\n\n  return x_interp\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Import necessary libraries\n  import numpy as np\n  import scipy.interpolate as interpolate\n\n  # Check that x is a 1D array\n  if x.ndim != 1:\n    raise ValueError(\"x must be a 1D array\")\n\n  # Check that t_input and t_output are 1D arrays\n  if t_input.ndim != 1 or t_output.ndim != 1:\n    raise ValueError(\"t_input and t_output must be 1D arrays\")\n\n  # Check that t_input and t_output are sorted\n  if not np.all(np.diff(t_input) >= 0) or not np.all(np.diff(t_output) >= 0):\n    raise ValueError(\"t_input and t_output must be sorted\")\n\n  # Check that t_input and t_output are within the range of t_input\n  if np.min(t_output) < np.min(t_input) or np.max(t_output) > np.max(t_input):\n    raise ValueError(\"t_output must be within the range of t_input\")\n\n  # Adjust spline_degree to be at most one less than the number of points in x\n  if spline_degree >= len(x):\n    spline_degree = len(x) - 1\n\n  # Fit a spline to the data\n  spline = interpolate.UnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n\n  # Interpolate the data at the output times\n  x_output = spline(t_output)\n\n  return x_output"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import UnivariateSpline\n\n  # Check if t_input is a numpy array\n  if not isinstance(t_input, np.ndarray):\n    t_input = np.array(t_input)\n\n  # Check if t_output is a numpy array\n  if not isinstance(t_output, np.ndarray):\n    t_output = np.array(t_output)\n\n  # Check if x is a numpy array\n  if not isinstance(x, np.ndarray):\n    x = np.array(x)\n\n  # Check if t_input and t_output have the same shape\n  if t_input.shape != x.shape:\n    raise ValueError(\"t_input and x must have the same shape\")\n\n  # Check if t_input is strictly increasing\n  if not np.all(np.diff(t_input) > 0):\n    raise ValueError(\"t_input must be strictly increasing\")\n\n  # Check if t_output is within the range of t_input\n  if not np.all(t_output >= t_input[0]) or not np.all(t_output <= t_input[-1]):\n    raise ValueError(\"t_output must be within the range of t_input\")\n\n  # Adjust spline_degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Create a spline object with the specified degree and smoothness\n  spline = UnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times\n  x_output = spline(t_output)\n\n  return x_output\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0].isupper() and word[1].isupper():\n            return word.upper()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        elif word[0].islower() and word[1].isupper():\n            return word.lower()\n        else:\n            return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0].isupper() and word[1].isupper():\n            return word.upper()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        else:\n            return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0].isupper() and word[1].isupper():\n            return word.upper()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        else:\n            return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0].isupper() and word[1].isupper():\n            return word.upper()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        elif word[0].islower() and word[1].isupper():\n            return word.lower()\n        else:\n            return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0:2].isupper():\n            return word.upper()\n        elif word[0:2].islower():\n            return word.capitalize()\n        else:\n            return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word.upper()\n    elif word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    elif word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    elif word.isupper():\n        return word\n    else:\n        if word[0].isupper() and word[1].isupper():\n            return word.upper()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        else:\n            return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # Check if the word is entirely in uppercase\n    if word.isupper():\n        # Return the word as is if it is entirely in uppercase\n        return word\n\n    # Check if the word is entirely in lowercase\n    if word.islower():\n        # Return the word as is if it is entirely in lowercase\n        return word\n\n    # Check if the first two letters are uppercase\n    if word[:2].isupper():\n        # Convert the word to uppercase if the first two letters are uppercase\n        return word.upper()\n\n    # Check if the first two letters are lowercase\n    if word[:2].islower():\n        # Convert the word to lowercase if the first two letters are lowercase\n        return word.lower()\n\n    # Check if the first letter is uppercase and the second letter is lowercase\n    if word[0].isupper() and word[1].islower():\n        # Capitalize the word if the first letter is uppercase and the second letter is lowercase\n        return word.capitalize()\n\n    # If none of the above conditions are met, return the word as is\n    return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0].isupper() and word[1].isupper():\n            return word.upper()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        elif word[0].islower() and word[1].islower():\n            return word.lower()\n        else:\n            return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    elif word.isupper():\n        return word\n    else:\n        if word[0].isupper() and word[1].isupper():\n            return word.upper()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        else:\n            return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[1:].isupper():\n            return word.upper()\n        elif word[1:].islower():\n            return word.lower()\n        else:\n            return word.capitalize()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper() or word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word.upper()\n    elif word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    elif word[0].islower() and word[1].isupper():\n        return word.lower()\n    else:\n        return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    elif word.isupper():\n        return word\n    else:\n        if word[0].isupper() and word[1].isupper():\n            return word.upper()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        else:\n            return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0].isupper() and word[1].isupper():\n            return word.upper()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        elif word[0].islower() and word[1].islower():\n            return word.lower()\n        else:\n            return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0].isupper() and word[1].isupper():\n            return word.upper()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        else:\n            return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    elif word.isupper():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    elif word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    elif word[0].islower() and word[1].isupper():\n        return word.lower()\n    elif word[0].islower() and word[1].islower():\n        return word.lower()\n    else:\n        return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # Check if the word is entirely in uppercase\n    if word.isupper():\n        # If the word is entirely in uppercase, return the word as is\n        return word\n\n    # Check if the word is entirely in lowercase\n    elif word.islower():\n        # If the word is entirely in lowercase, return the word as is\n        return word\n\n    # Check if the first two letters of the word are in uppercase\n    elif word[:2].isupper():\n        # If the first two letters of the word are in uppercase, return the word as is\n        return word\n\n    # Check if the first letter of the word is in uppercase\n    elif word[0].isupper():\n        # If the first letter of the word is in uppercase, return the word as is\n        return word\n\n    # Check if the first two letters of the word are in lowercase\n    elif word[:2].islower():\n        # If the first two letters of the word are in lowercase, return the word as is\n        return word\n\n    # Check if the first letter of the word is in lowercase\n    elif word[0].islower():\n        # If the first letter of the word is in lowercase, return the word as is\n        return word\n\n    # If none of the above conditions are met, return the word as is\n    return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0].isupper() and word[1].isupper():\n            return word.upper()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        else:\n            return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word.upper()\n    elif word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    elif word[0].islower() and word[1].isupper():\n        return word.lower()\n    elif word[0].islower() and word[1].islower():\n        return word\n    else:\n        return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper() or word.islower():\n        return word\n\n    # if lower no uppers after\n    if word[0].islower() and word[1:].islower():\n        return word.lower()\n\n    # if upper no\n    if word[0].isupper() and word[1:].isupper():\n        return word.upper()\n\n    # if mixed case, convert to lowercase\n    if word[0].isupper() and word[1:].islower():\n        return word.lower()\n\n    # if mixed case, convert to uppercase\n    if word[0].islower() and word[1:].isupper():\n        return word.upper()\n\n    # if mixed case, capitalize the first letter\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n\n    return word\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__} {field.name} must contain only boolean values, not {v.dtype}\"\n        )\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}.{field.name} must be a binary array. Found {v.dtype}.\"\n        )\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if v.dtype != bool:\n        raise ValueError(\n            f\"{cls.__name__} field {field.name} must be binary (bool), not {v.dtype}\"\n        )\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if v.dtype != bool:\n        raise ValueError(\n            f\"{cls.__name__} {field.name} must be binary, but found {v.dtype}.\"\n        )\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__} {field.name} must be a binary array. Found {v.dtype}.\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__} '{field.name}' must be binary, but dtype is '{v.dtype}'\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if v.dtype != bool:\n        raise ValueError(\n            f\"{cls.__name__} {field.name} must be binary (bool) data type. \"\n            f\"Found dtype {v.dtype}.\"\n        )\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.array_equal(v, v.astype(bool)):\n        raise ValueError(f\"{cls.__name__} {field.name} must be binary, not {v.dtype}\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if v.dtype != np.bool_:\n        raise ValueError(f\"{cls.__name__} {field.name} must be binary (dtype=np.bool_)\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}.{field.name} must contain only boolean values\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}.{field.name} must be a binary array, but found {v.dtype}\"\n        )\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}.{field.name} must be a binary array, not {v.dtype}.\"\n        )\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__} {field.name} must be binary (bool), not {v.dtype}\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__} {field.name} must contain only boolean values.\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if v.dtype != bool:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array, but got {v.dtype}\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.array_equal(v, v.astype(bool)):\n        raise ValueError(\n            f\"{cls.__name__} field {field.name} must contain only boolean values, not {v.dtype}\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__} {field.name} must be binary (bool), not {v.dtype}\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if v.dtype != bool:\n        raise ValueError(\n            f\"{cls.__name__} {field.name} must be binary, found {v.dtype} instead\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__} '{field.name}' must be binary (dtype=bool).\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__} field '{field.name}' must be binary (dtype=bool). \"\n            f\"Found {v.dtype}.\"\n        )\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input array\n  x_norm = np.linalg.norm(x, axis=(1, 2))\n\n  # Apply the isotropic scaling operation\n  x_scaled = x / x_norm[:, None, None]\n\n  return x_scaled\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  norm = np.linalg.norm(x, axis=0)\n  return x / norm\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = np.linalg.norm(x)\n  return x / x_norm\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Compute the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Apply the isotropic scaling operation\n  scaled_x = x / norm\n\n  # Return the scaled array\n  return scaled_x\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Apply the isotropic scaling operation\n  return x / norm\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = np.linalg.norm(x)\n  x_scaled = x / x_norm\n  return x_scaled\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Apply the isotropic scaling operation to the input array\n  scaled_x = x / norm\n\n  return scaled_x\n\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input array\n  norm_x = np.linalg.norm(x)\n\n  # Apply the isotropic scaling operation to the input array\n  scaled_x = np.multiply(x, 1 / norm_x)\n\n  # Return the scaled array\n  return scaled_x\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Apply the isotropic scaling operation\n  return np.sqrt(3) * x / norm\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  import numpy as np\n  import math\n\n  # Calculate the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Check if the norm is greater than 1\n  if norm > 1:\n    # If the norm is greater than 1, calculate the scaling factor\n    scale_factor = 1 / norm\n    # Apply the scaling operation to the input array\n    x = x * scale_factor\n  else:\n    # If the norm is less than or equal to 1, no scaling is necessary\n    pass\n\n  # Return the scaled version of the input array\n  return x\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Apply the isotropic scaling operation\n  scaled_x = x / norm\n\n  return scaled_x\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Calculate the scaled version of the input array\n  scaled = x / norm\n\n  # Return the scaled version of the input array\n  return scaled\n\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  import numpy as np\n\n  # Calculate the norm of the input array\n  x_norm = np.linalg.norm(x, ord=2)\n\n  # Apply the isotropic scaling operation\n  x_scaled = x / x_norm\n\n  return x_scaled\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  import numpy as np\n  import tensorflow as tf\n\n  # Compute the norm of the input array\n  norm_x = tf.norm(x, ord='euclidean')\n\n  # Calculate the scaling factor based on the norm and a constant value\n  scale_factor = norm_x / (1 + 5 * norm_x / 3)\n\n  # Apply the scaling operation to the input array\n  scaled_x = x * scale_factor\n\n  # Return the scaled array\n  return scaled_x\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Apply the isotropic scaling operation\n  scaled_x = x / norm\n\n  return scaled_x\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  norm_x = np.linalg.norm(x)\n  if norm_x == 0:\n    return x\n  else:\n    return x / norm_x\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  import numpy as np\n  from numpy.linalg import norm\n\n  return np.sqrt(3) * x / norm(x, 2)\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  import numpy as np\n  norm = np.sqrt(np.sum(np.power(x, 2)))\n  return x / norm\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Compute the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Apply the isotropic scaling operation\n  scaled_x = x * (1 - (1 / norm))\n\n  return scaled_x\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input array\n  x_norm = np.linalg.norm(x)\n\n  # Apply the isotropic scaling operation\n  x_scaled = x / (x_norm + 1e-6)\n\n  return x_scaled\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    with open(summary_path, 'r') as f:\n        summary_data = json.load(f)\n\n    summary_df = pd.DataFrame(summary_data)\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    with open(summary_path, 'r') as f:\n        summary_data = json.load(f)\n\n    summary_df = pd.DataFrame(summary_data)\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    df = pd.read_csv(summary_path, index_col=0)\n\n    for col in dict_columns:\n        df[col] = df[col].apply(lambda x: ast.literal_eval(x))\n\n    return df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    summary_df = pd.read_csv(summary_path)\n\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Read the summary file into a DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Convert specified columns that contain dictionary-like strings into actual dictionaries\n    if dict_columns is None:\n        dict_columns = ['module_params']\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file into a pandas DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Convert specified columns that contain dictionary-like strings into actual dictionary objects\n    if dict_columns is not None:\n        for column in dict_columns:\n            summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file into a pandas DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Set the default value for dict_columns to ['module_params'] if not provided\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Convert dictionary-like strings in specified columns to actual dictionaries\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file into a pandas DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # If dict_columns is not provided, default to 'module_params' column\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Convert specified columns that contain dictionary-like strings into actual dictionaries\n    for col in dict_columns:\n        if col in summary_df.columns:\n            summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file into a pandas DataFrame\n    summary_df = pd.read_csv(summary_path, index_col=0)\n\n    # Convert the specified columns that contain dictionary-like strings into actual dictionaries\n    if dict_columns is None:\n        dict_columns = ['module_params']\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    df = pd.read_csv(summary_path)\n    df = df.fillna('')\n\n    for col in dict_columns:\n        df[col] = df[col].apply(lambda x: ast.literal_eval(x) if x else {})\n\n    return df\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    df = pd.read_csv(summary_path, index_col=0)\n    for col in dict_columns:\n        df[col] = df[col].apply(lambda x: ast.literal_eval(x))\n\n    return df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Read the summary file into a DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Convert the specified columns from dictionary-like strings to actual dictionaries\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    with open(summary_path, 'r') as f:\n        summary_dict = json.load(f)\n\n    summary_df = pd.DataFrame(summary_dict)\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file into a DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Convert specified columns to dictionary objects\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return summary_df\n\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    with open(summary_path, 'r') as file:\n        summary_data = json.load(file)\n\n    df = pd.DataFrame(summary_data)\n\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: ast.literal_eval(x))\n\n    return df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Read the summary file into a pandas DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # If dict_columns is not provided, default to 'module_params'\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Convert dictionary-like strings in specified columns to actual dictionaries\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file into a DataFrame\n    summary = pd.read_csv(summary_path)\n\n    # If dict_columns is not provided, default to ['module_params']\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Convert dictionary-like strings in the specified columns to actual dictionaries\n    for col in dict_columns:\n        summary[col] = summary[col].apply(ast.literal_eval)\n\n    return summary\n\n"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  if mode == 'fast':\n    det = np.linalg.det(cov)\n  elif mode == 'accurate':\n    det = np.exp(np.log(np.linalg.det(cov)))\n  else:\n    raise ValueError('Invalid mode. Choose either \"fast\" or \"accurate\".')\n\n  if det <= 0:\n    raise ValueError('Invalid determinant.')\n\n  n = cov.shape[0]\n  isotropic_cov = det**(1/n) * np.eye(n)\n\n  return isotropic_cov\n"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  if mode == 'fast':\n    # Compute the determinant of the input covariance matrix or matrices\n    det = np.linalg.det(cov)\n\n    # Check if the determinant is invalid\n    if det <= 0:\n      # If the determinant is invalid, raise an error\n      raise ValueError(\"Invalid determinant\")\n\n    # Compute the isotropic covariance matrix or matrices\n    cov = cov / det ** (1 / cov.shape[-1])\n\n  elif mode == 'accurate':\n    # Compute the logarithm of the determinant of the input covariance matrix or matrices\n    logdet = np.linalg.slogdet(cov)[1]\n\n    # Check if the logarithm of the determinant is invalid\n    if logdet == -np.inf:\n      # If the logarithm of the determinant is invalid, raise an error\n      raise ValueError(\"Invalid logarithm of determinant\")\n\n    # Compute the isotropic covariance matrix or matrices\n    cov = cov / np.exp(logdet / cov.shape[-1])\n\n  else:\n    # If the mode is invalid, raise an error\n    raise ValueError(\"Invalid mode\")\n\n  # Return the isotropic covariance matrix or matrices\n  return cov"}
{"namespace": "coord.isotropize", "completion": "  # Check if the input is a single covariance matrix or a stack of covariance matrices\n  if cov.ndim == 2:\n    cov = cov[None]\n\n  # Compute the determinant of the input covariance matrices\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is non-negative\n  if not np.all(det >= 0):\n    raise ValueError('Invalid determinant')\n\n  # Compute the logarithm of the determinant\n  log_det = np.log(det)\n\n  # Check if the logarithm of the determinant is finite\n  if not np.all(np.isfinite(log_det)):\n    raise ValueError('Invalid logarithm of determinant')\n\n  # Compute the isotropic covariance matrices\n  if mode == 'fast':\n    # Compute the isotropic covariance matrices using the determinant directly\n    cov_iso = cov / det[:, None, None]\n  elif mode == 'accurate':\n    # Compute the isotropic covariance matrices using the logarithm of the determinant\n    cov_iso = cov / np.exp(log_det)[:, None, None]\n  else:\n    raise ValueError('Invalid mode')\n\n  # Return the isotropic covariance matrix or matrices\n  return cov_iso[0] if cov.shape[0] == 1 else cov_iso"}
{"namespace": "coord.isotropize", "completion": "  # Check if the input is a single covariance matrix or a stack of covariance matrices\n  if cov.ndim == 2:\n    cov = cov[None, ...]\n\n  # Compute the determinant of the input covariance matrices\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is valid\n  if np.any(det <= 0):\n    raise ValueError('Invalid determinant encountered in input covariance matrices.')\n\n  # Compute the logarithm of the determinant\n  logdet = np.log(det)\n\n  # Compute the isotropic covariance matrices\n  if mode == 'fast':\n    cov_iso = cov / det[..., None, None]\n  elif mode == 'accurate':\n    cov_iso = cov / np.exp(logdet[..., None, None])\n  else:\n    raise ValueError('Invalid mode specified. Please choose either \"fast\" or \"accurate\".')\n\n  # Return the isotropic covariance matrices\n  return cov_iso.squeeze()"}
{"namespace": "coord.isotropize", "completion": "  # Check if the input is a single covariance matrix or a batch of covariance matrices\n  if cov.ndim == 2:\n    # If the input is a single covariance matrix, compute the isotropic covariance matrix\n    return _isotropize_single(cov, mode)\n  elif cov.ndim == 3:\n    # If the input is a batch of covariance matrices, compute the isotropic covariance matrices\n    return _isotropize_batch(cov, mode)\n  else:\n    # Raise an error if the input is not a valid covariance matrix or a batch of covariance matrices\n    raise ValueError(\"Input must be a single covariance matrix or a batch of covariance matrices\")\n"}
{"namespace": "coord.isotropize", "completion": "  # Get the number of dimensions of the covariance matrix\n  ndim = cov.shape[-1]\n\n  # Compute the determinant of the covariance matrix\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is invalid (i.e., less than or equal to zero)\n  if det <= 0:\n    # If the determinant is invalid, raise an error\n    raise ValueError(\"Invalid determinant: {}\".format(det))\n\n  # Compute the logarithm of the determinant\n  logdet = np.log(det)\n\n  # Check if the logarithm of the determinant is invalid (i.e., less than or equal to zero)\n  if logdet <= 0:\n    # If the logarithm of the determinant is invalid, raise an error\n    raise ValueError(\"Invalid logarithm of determinant: {}\".format(logdet))\n\n  # Compute the isotropic covariance matrix\n  if mode == 'fast':\n    # If the mode is 'fast', use the determinant directly\n    cov_iso = np.eye(ndim) * (det ** (1.0 / ndim))\n  elif mode == 'accurate':\n    # If the mode is 'accurate', use the logarithm of the determinant\n    cov_iso = np.eye(ndim) * (np.exp(logdet / ndim))\n  else:\n    # If the mode is not recognized, raise an error\n    raise ValueError(\"Invalid mode: {}\".format(mode))\n\n  # Return the isotropic covariance matrix\n  return cov_iso\n\n"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the input covariance matrices\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is invalid (i.e., negative or zero)\n  if det <= 0:\n    raise ValueError(\"Invalid determinant encountered.\")\n\n  # Compute the logarithm of the determinant for 'accurate' mode\n  if mode == 'accurate':\n    det = np.log(det)\n\n  # Compute the dimensionality of the input covariance matrices\n  dim = cov.shape[-1]\n\n  # Compute the isotropic covariance matrix or matrices with the same determinant\n  cov_iso = det ** (-1 / dim) * np.eye(dim)\n\n  return cov_iso"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  # Compute the determinant of the input covariance matrix or matrices\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is invalid\n  if np.isinf(det) or np.isnan(det) or det <= 0:\n\n    # If the determinant is invalid, raise an error\n    raise ValueError(\"Invalid determinant\")\n\n  # Compute the logarithm of the determinant\n  logdet = np.log(det)\n\n  # Check if the logarithm of the determinant is invalid\n  if np.isinf(logdet) or np.isnan(logdet):\n\n    # If the logarithm of the determinant is invalid, raise an error\n    raise ValueError(\"Invalid logarithm of determinant\")\n\n  # Compute the number of dimensions of the input covariance matrix or matrices\n  ndims = cov.ndim\n\n  # Check if the input is a single covariance matrix or a batch of covariance matrices\n  if ndims == 2:\n\n    # If the input is a single covariance matrix, compute the isotropic covariance matrix with the same determinant\n    cov = np.eye(cov.shape[0]) * (logdet / cov.shape[0])\n\n  elif ndims == 3:\n\n    # If the input is a batch of covariance matrices, compute the isotropic covariance matrices with the same determinant\n    cov = np.eye(cov.shape[1]) * (logdet / cov.shape[1])\n\n  else:\n\n    # If the input is not a valid covariance matrix or a batch of covariance matrices, raise an error\n    raise ValueError(\"Invalid input\")\n\n  # Return the isotropic covariance matrix or matrices with the same determinant as the input covariance matrix or matrices\n  return cov"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n  from scipy.linalg import eigh\n\n  if mode == 'accurate':\n    det = np.log(np.linalg.det(cov))\n    cov_det = np.exp(det / cov.shape[0])\n  elif mode == 'fast':\n    det = np.linalg.det(cov)\n    cov_det = det ** (1 / cov.shape[0])\n  else:\n    raise ValueError(\"Invalid mode. Please choose 'fast' or 'accurate'.\")\n\n  if np.isnan(cov_det) or np.isinf(cov_det):\n    raise ValueError(\"Invalid determinant. Please check your input covariance matrix or matrices.\")\n\n  eigval, eigvec = eigh(cov)\n  eigval = np.maximum(eigval, 0)\n  eigval_sqrt = np.sqrt(eigval)\n  cov_iso = eigvec @ np.diag(eigval_sqrt) @ eigvec.T\n  cov_iso *= cov_det / np.linalg.det(cov_iso)\n\n  return cov_iso"}
{"namespace": "coord.isotropize", "completion": "  # Check if the input is a matrix or a list of matrices\n  if len(cov.shape) == 2:\n    cov = [cov]\n\n  # Compute the determinant of the covariance matrices\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is valid\n  if mode == 'fast':\n    if det <= 0:\n      raise ValueError(\"Invalid determinant encountered in 'fast' mode.\")\n  else:\n    if np.any(np.isnan(det)) or np.any(det <= 0):\n      raise ValueError(\"Invalid determinant encountered in 'accurate' mode.\")\n\n  # Compute the logarithm of the determinant\n  if mode == 'accurate':\n    logdet = np.log(det)\n\n  # Compute the isotropic covariance matrices\n  out = []\n  for i in range(len(cov)):\n    if mode == 'fast':\n      out.append(cov[i] / det)\n    else:\n      out.append(cov[i] / np.exp(logdet[i]))\n\n  # Return the isotropic covariance matrices\n  return np.array(out)"}
{"namespace": "coord.isotropize", "completion": "  # Check if the covariance matrix or matrices is a single matrix or a collection of matrices\n  if cov.ndim == 2:\n    cov = cov[np.newaxis, :, :]\n\n  # Compute the determinant of the covariance matrix or matrices\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is valid\n  if np.any(det <= 0):\n    raise ValueError(\"Invalid determinant\")\n\n  # Compute the logarithm of the determinant\n  logdet = np.log(det)\n\n  # Compute the dimension of the covariance matrix or matrices\n  D = cov.shape[1]\n\n  # Compute the isotropic covariance matrix or matrices\n  if mode == 'fast':\n    # Compute the isotropic covariance matrix or matrices using the determinant directly\n    cov_iso = cov / det[:, np.newaxis, np.newaxis] * np.eye(D)\n  elif mode == 'accurate':\n    # Compute the isotropic covariance matrix or matrices using the logarithm of the determinant\n    cov_iso = cov / np.exp(logdet[:, np.newaxis, np.newaxis] / D) * np.eye(D)\n  else:\n    # Raise an error if the mode is invalid\n    raise ValueError(\"Invalid mode\")\n\n  # Return the isotropic covariance matrix or matrices\n  return cov_iso"}
{"namespace": "coord.isotropize", "completion": "  # Check if the input is a single covariance matrix or a stack of covariance matrices\n  if cov.ndim == 2:\n    cov = cov[None, ...]\n\n  # Compute the determinant of the input covariance matrices\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is valid\n  if np.any(det <= 0):\n    raise ValueError(\"Invalid determinant encountered.\")\n\n  # Compute the logarithm of the determinant for numerical stability\n  logdet = np.log(det)\n\n  # Compute the number of dimensions in the covariance matrices\n  dim = cov.shape[-1]\n\n  # Compute the isotropic covariance matrices\n  if mode == 'fast':\n    # Compute the isotropic covariance matrices using the determinant directly\n    cov_iso = np.tile(np.eye(dim), (cov.shape[0], 1, 1)) * det ** (1 / dim)\n  elif mode == 'accurate':\n    # Compute the isotropic covariance matrices using the logarithm of the determinant for numerical stability\n    cov_iso = np.tile(np.eye(dim), (cov.shape[0], 1, 1)) * np.exp(logdet / dim)\n  else:\n    raise ValueError(\"Invalid mode specified.\")\n\n  # Check if the output is a single covariance matrix or a stack of covariance matrices\n  if cov_iso.shape[0] == 1:\n    cov_iso = cov_iso[0, ...]\n\n  return cov_iso\n\n"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  if mode == 'fast':\n    det = np.linalg.det(cov)**(1/cov.shape[0])\n  else:\n    det = np.exp(np.log(np.linalg.det(cov))/cov.shape[0])\n\n  if np.isnan(det) or np.isinf(det):\n    raise ValueError('Invalid determinant.')\n\n  if cov.ndim == 2:\n    return det * np.eye(cov.shape[0])\n  else:\n    return np.tile(det * np.eye(cov.shape[1]), (cov.shape[0], 1, 1))\n\n"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the input covariance matrix or matrices\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is valid\n  if det <= 0:\n    raise ValueError(\"Invalid determinant\")\n\n  # Compute the logarithm of the determinant for 'accurate' mode\n  if mode == 'accurate':\n    logdet = np.log(det)\n\n  # Compute the shape of the input covariance matrix or matrices\n  cov_shape = cov.shape\n\n  # Check if the input covariance matrix or matrices is a single matrix\n  if len(cov_shape) == 2:\n    # Compute the number of rows and columns of the input covariance matrix\n    n, m = cov_shape\n\n    # Check if the input covariance matrix is square\n    if n != m:\n      raise ValueError(\"Input covariance matrix is not square\")\n\n    # Compute the isotropic covariance matrix\n    if mode == 'fast':\n      # Compute the isotropic covariance matrix using the determinant directly\n      cov_is = cov * np.eye(n) / det\n    elif mode == 'accurate':\n      # Compute the isotropic covariance matrix using the logarithm of the determinant\n      cov_is = cov * np.eye(n) * np.exp(-logdet / n)\n\n  # Check if the input covariance matrix or matrices is a stack of matrices\n  elif len(cov_shape) == 3:\n    # Compute the number of matrices, number of rows, and number of columns of the input covariance matrix or matrices\n    n, m, k = cov_shape\n\n    # Check if the input covariance matrix or matrices are square\n    if n != m:\n      raise ValueError(\"Input covariance matrices are not square\")\n\n    # Compute the isotropic covariance matrices\n    if mode == 'fast':\n      # Compute the isotropic covariance matrices using the determinant directly\n      cov_is = cov * np.eye(n)[None, :, :] / det[:, None, None]\n    elif mode == 'accurate':\n      # Compute the isotropic covariance matrices using the logarithm of the determinant"}
{"namespace": "coord.isotropize", "completion": "  # Get the shape of the covariance matrix\n  cov_shape = cov.shape\n\n  # Check if the covariance matrix is a single matrix or a collection of matrices\n  if len(cov_shape) == 2:\n    # If the covariance matrix is a single matrix, compute the isotropic covariance matrix\n    return _isotropize_single(cov, mode)\n  else:\n    # If the covariance matrix is a collection of matrices, compute the isotropic covariance matrices for each matrix\n    return _isotropize_multiple(cov, mode)\n"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  # Get the determinant of the covariance matrix\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is valid\n  if det <= 0:\n    raise ValueError(\"Invalid determinant\")\n\n  # Compute the logarithm of the determinant\n  log_det = np.log(det)\n\n  # Get the shape of the covariance matrix\n  shape = cov.shape\n\n  # Check if the covariance matrix is 2D\n  if len(shape) == 2:\n    # If the covariance matrix is 2D, compute the isotropic covariance matrix\n    if mode == 'fast':\n      # Compute the isotropic covariance matrix using the determinant\n      cov_iso = cov / det\n    elif mode == 'accurate':\n      # Compute the isotropic covariance matrix using the logarithm of the determinant\n      cov_iso = cov / np.exp(log_det / shape[0])\n  else:\n    # If the covariance matrix is not 2D, compute the isotropic covariance matrices\n    if mode == 'fast':\n      # Compute the isotropic covariance matrices using the determinant\n      cov_iso = cov / det[:, None, None]\n    elif mode == 'accurate':\n      # Compute the isotropic covariance matrices using the logarithm of the determinant\n      cov_iso = cov / np.exp(log_det / shape[1])\n\n  return cov_iso\n\n"}
{"namespace": "coord.isotropize", "completion": "  # Check if the input is a single covariance matrix or a collection of covariance matrices\n  if cov.ndim == 2:\n    cov = cov[None, ...]\n\n  # Compute the determinant of the input covariance matrices\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is valid\n  if np.any(det <= 0):\n    raise ValueError('Invalid determinant')\n\n  # Compute the logarithm of the determinant\n  if mode == 'accurate':\n    det = np.log(det)\n\n  # Compute the isotropic covariance matrices\n  if mode == 'accurate':\n    cov = cov / np.exp(det[..., None, None] / cov.shape[-1])\n  else:\n    cov = cov / det[..., None, None]\n\n  # Return the isotropic covariance matrix or matrices\n  return cov[0] if cov.shape[0] == 1 else cov\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the determinant of the input covariance matrix or matrices\n    det = np.linalg.det(cov)\n    # Check if the determinant is invalid (less than or equal to zero)\n    if det <= 0:\n      # Raise an error if the determinant is invalid\n      raise ValueError(\"Invalid determinant encountered in 'fast' mode.\")\n    # Compute the logarithm of the determinant\n    logdet = np.log(det)\n  elif mode == 'accurate':\n    # Compute the logarithm of the determinant of the input covariance matrix or matrices\n    _, logdet = np.linalg.slogdet(cov)\n    # Check if the determinant is invalid (less than or equal to zero)\n    if logdet <= 0:\n      # Raise an error if the determinant is invalid\n      raise ValueError(\"Invalid determinant encountered in 'accurate' mode.\")\n  else:\n    # Raise an error if the mode is invalid\n    raise ValueError(\"Invalid mode. Choose either 'fast' or 'accurate'.\")\n\n  # Compute the dimension of the input covariance matrix or matrices\n  ndim = cov.shape[-1]\n  # Compute the identity matrix of the same shape as the input covariance matrix or matrices\n  I = np.eye(ndim, dtype=cov.dtype)\n  # Compute the isotropic covariance matrix or matrices\n  cov_iso = I * np.exp(logdet / ndim)\n\n  # Return the isotropic covariance matrix or matrices\n  return cov_iso\n\n\n"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  # Check if the input covariance matrix is a single matrix or a collection of matrices\n  if cov.ndim == 2:\n    cov = cov[np.newaxis, :, :]\n\n  # Compute the determinant of the input covariance matrices\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is non-zero\n  if np.any(det == 0):\n    raise ValueError(\"Input covariance matrices must have non-zero determinant.\")\n\n  # Compute the logarithm of the determinant\n  logdet = np.log(det)\n\n  # Check if the logarithm of the determinant is finite\n  if np.any(np.isinf(logdet)):\n    raise ValueError(\"Input covariance matrices must have finite determinant.\")\n\n  # Compute the number of input covariance matrices\n  n = cov.shape[0]\n\n  # Compute the dimensionality of the input covariance matrices\n  d = cov.shape[1]\n\n  # Compute the isotropic covariance matrices\n  if mode == 'fast':\n    # Compute the isotropic covariance matrices using the determinant directly\n    cov_iso = cov / det[:, np.newaxis, np.newaxis]\n  elif mode == 'accurate':\n    # Compute the isotropic covariance matrices using the logarithm of the determinant\n    cov_iso = cov / np.exp(logdet[:, np.newaxis, np.newaxis])\n  else:\n    raise ValueError(\"Invalid mode. Choose either 'fast' or 'accurate'.\")\n\n  # Check if the output covariance matrices have finite values\n  if not np.all(np.isfinite(cov_iso)):\n    raise ValueError(\"Output covariance matrices must have finite values.\")\n\n  # Return the isotropic covariance matrix or matrices\n  return cov_iso\n\n"}
{"namespace": "coord.isotropize", "completion": "  # Import modules\n  import numpy as np\n\n  # Check if the input is a matrix or a tensor\n  if cov.ndim == 2:\n    cov = cov[np.newaxis, ...]\n\n  # Get the number of matrices\n  n_matrices = cov.shape[0]\n\n  # Get the shape of the matrices\n  n, m = cov.shape[1:]\n\n  # Check if the matrices are square\n  if n != m:\n    raise ValueError(\"The input matrices must be square.\")\n\n  # Compute the determinant of the covariance matrices\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is valid\n  if np.any(det <= 0):\n    raise ValueError(\"The determinant of the input matrices must be positive.\")\n\n  # Compute the logarithm of the determinant\n  if mode == 'accurate':\n    logdet = np.log(det)\n\n  # Compute the isotropic covariance matrices\n  if mode == 'fast':\n    isotropic_cov = det**(-1/m) * np.eye(n)\n  elif mode == 'accurate':\n    isotropic_cov = np.exp(logdet / m) * np.eye(n)\n\n  # Repeat the isotropic covariance matrices for each input matrix\n  isotropic_cov = np.repeat(isotropic_cov[np.newaxis, ...], n_matrices, axis=0)\n\n  return isotropic_cov\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Run a task with specified parameters.\")\n\n    parser.add_argument(\n        \"--task\",\n        type=str,\n        required=True,\n        help=\"The task description, specifying what task should be performed.\",\n    )\n\n    parser.add_argument(\n        \"--upload-files\",\n        type=str,\n        nargs=\"+\",\n        help=\"List of files to upload, allowing multiple files to be specified.\",\n    )\n\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        help=\"Model identifier for the task, specifying which model to use.\",\n    )\n\n    parser.add_argument(\n        \"--record-dir\",\n        type=str,\n        help=\"Directory to record task execution logs, specifying where to save the logs.\",\n    )\n\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"auto\",\n        help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\",\n    )\n\n    parser.add_argument(\n        \"--quiet\",\n        action=\"store_true\",\n        default=False,\n        help=\"If set, the program runs in quiet mode with minimal output.\",\n    )\n\n    parser.add_argument(\n        \"--max-subtask-chain-length\",\n        type=int,\n        help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\",\n    )\n\n    parser.add_argument(\n        \"--enable-ask-human-for-help\",\n        action=\"store_true\",\n        default=False,\n        help=\"Flag to enable asking for human assistance during task execution.\",\n    )\n\n    parser.add_argument(\n        \"--max-plan-refine-chain-length\",\n        type=int,\n        help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\",\n    )\n\n    parser.add_argument(\n        \"--"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(\n        description=\"A command-line interface (CLI) tool for executing tasks using the OpenAI API.\"\n    )\n    parser.add_argument(\n        \"--task\",\n        type=str,\n        required=True,\n        help=\"Task description, specifying what task should be performed.\",\n    )\n    parser.add_argument(\n        \"--upload-files\",\n        type=str,\n        nargs=\"+\",\n        help=\"List of files to upload, allowing multiple files to be specified.\",\n    )\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        help=\"Model identifier for the task, specifying which model to use.\",\n    )\n    parser.add_argument(\n        \"--record-dir\",\n        type=str,\n        help=\"Directory to record task execution logs, specifying where to save the logs.\",\n    )\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"auto\",\n        help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\",\n    )\n    parser.add_argument(\n        \"--quiet\",\n        action=\"store_true\",\n        help=\"If set, the program runs in quiet mode with minimal output.\",\n    )\n    parser.add_argument(\n        \"--max-subtask-chain-length\",\n        type=int,\n        help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\",\n    )\n    parser.add_argument(\n        \"--enable-ask-human-for-help\",\n        action=\"store_true\",\n        help=\"Flag to enable asking for human assistance during task execution.\",\n    )\n    parser.add_argument(\n        \"--max-plan-refine-chain-length\",\n        type=int,\n        help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\",\n    )\n    parser.add_argument(\n        \"--max-plan-tree-"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"CLI for task execution.\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"Files to upload.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts.\")\n    parser.add_argument(\"--config-file\", type=str, default=os.getenv(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file.\")\n\n    args = parser.parse_args()\n\n    return args"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Run a task.\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance during task execution\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file\")\n    return parser.parse_args()\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(\n        description=\"A tool for executing tasks using the specified model.\"\n    )\n\n    parser.add_argument(\n        \"--task\",\n        type=str,\n        required=True,\n        help=\"The task description, specifying what task should be performed.\",\n    )\n\n    parser.add_argument(\n        \"--upload-files\",\n        type=str,\n        nargs=\"*\",\n        help=\"List of files to upload, allowing multiple files to be specified.\",\n    )\n\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        help=\"Model identifier for the task, specifying which model to use.\",\n    )\n\n    parser.add_argument(\n        \"--record-dir\",\n        type=str,\n        help=\"Directory to record task execution logs, specifying where to save the logs.\",\n    )\n\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"auto\",\n        help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\",\n    )\n\n    parser.add_argument(\n        \"--quiet\",\n        action=\"store_true\",\n        default=False,\n        help=\"If set, the program runs in quiet mode with minimal output.\",\n    )\n\n    parser.add_argument(\n        \"--max-subtask-chain-length\",\n        type=int,\n        help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\",\n    )\n\n    parser.add_argument(\n        \"--enable-ask-human-for-help\",\n        action=\"store_true\",\n        help=\"Flag to enable asking for human assistance during task execution.\",\n    )\n\n    parser.add_argument(\n        \"--max-plan-refine-chain-length\",\n        type=int,\n        help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\",\n    )\n\n    parser.add_argument(\n       "}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description\")\n    parser.add_argument(\"--upload-files\", nargs=\"+\", type=str, help=\"Files to upload\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance during task execution\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file\")\n    args = parser.parse_args()\n    return args\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command line interface for executing tasks.')\n\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='Files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    return parser.parse_args()\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Parse command line arguments\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description\")\n    parser.add_argument(\"--upload-files\", nargs=\"*\", type=str, help=\"Files to upload\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs\")\n    parser.add_argument(\"--mode\", type=str, choices=[\"auto\", \"manual\"], default=\"auto\", help=\"Operational mode\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts\")\n    parser.add_argument(\"--config-file\", type=str, default=os.getenv(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file\")\n    return parser.parse_args()\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Execute a task using the specified model.')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', nargs='+', type=str, help='Files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.getenv('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    return parser.parse_args()\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Automated task execution program')\n\n    parser.add_argument('--task', type=str, required=True,\n                        help='Task description, specifying what task should be performed')\n    parser.add_argument('--upload-files', type=str, nargs='+', default=[],\n                        help='List of files to upload, allowing multiple files to be specified')\n    parser.add_argument('--model', type=str, default=None,\n                        help='Model identifier for the task, specifying which model to use')\n    parser.add_argument('--record-dir', type=str, default=None,\n                        help='Directory to record task execution logs, specifying where to save the logs')\n    parser.add_argument('--mode', type=str, default='auto',\n                        help='Operational mode, which can be \"auto\" or \"manual\", specifying how the task should be executed')\n    parser.add_argument('--quiet', action='store_true', default=False,\n                        help='Run the program in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, default=None,\n                        help='Maximum length of subtask chain, specifying how long a subtask chain can be')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', default=False,\n                        help='Flag to enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, default=None,\n                        help='Maximum length of plan refinement chain, specifying the limit for refining plans')\n    parser.add_argument('--max-plan-tree-depth', type=int, default=None,\n                        help='Maximum depth of the plan tree, specifying how deep the plan tree can be')\n    parser.add_argument('--max-plan-tree-width', type=int, default=None,\n                        help='Maximum width of the plan tree, spec"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description\")\n    parser.add_argument(\"--upload-files\", nargs=\"*\", type=str, help=\"Files to upload\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file\")\n\n    return parser.parse_args()\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command line arguments for the program.')\n\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, choices=['auto', 'manual'], default='auto', help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.getenv('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    return parser.parse_args()\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Run a task with the specified arguments.')\n    parser.add_argument('--task', type=str, required=True, help='Task description.')\n    parser.add_argument('--upload-files', nargs='+', type=str, help='Files to upload.')\n    parser.add_argument('--model', type=str, help='Model identifier.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs.')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode.')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts.')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file.')\n    return parser.parse_args()\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Run a task with the specified options.')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='Files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.getenv('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    return parser.parse_args()\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(\n        description=\"CLI for the AI Assistant\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description\")\n    parser.add_argument(\n        \"--upload-files\",\n        type=str,\n        nargs=\"+\",\n        default=[],\n        help=\"List of files to upload\",\n    )\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs\")\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        choices=[\"auto\", \"manual\"],\n        default=\"auto\",\n        help=\"Operational mode\",\n    )\n    parser.add_argument(\n        \"--quiet\",\n        action=\"store_true\",\n        help=\"Run in quiet mode with minimal output\",\n    )\n    parser.add_argument(\n        \"--max-subtask-chain-length\",\n        type=int,\n        help=\"Maximum length of subtask chain\",\n    )\n    parser.add_argument(\n        \"--enable-ask-human-for-help\",\n        action=\"store_true\",\n        help=\"Enable asking for human assistance during task execution\",\n    )\n    parser.add_argument(\n        \"--max-plan-refine-chain-length\",\n        type=int,\n        help=\"Maximum length of plan refinement chain\",\n    )\n    parser.add_argument(\n        \"--max-plan-tree-depth\",\n        type=int,\n        help=\"Maximum depth of the plan tree\",\n    )\n    parser.add_argument(\n        \"--max-plan-tree-width\",\n        type=int,\n        help=\"Maximum width of the plan tree\",\n    )\n    parser.add_argument(\n        \"--max-retry-times\",\n        type=int"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Process command line arguments.')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', nargs='+', type=str, help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.getenv('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n    return parser.parse_args()\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Task description')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n    args = parser.parse_args()\n    return args\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Run a task with specified parameters.\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"Files to upload.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs.\")\n    parser.add_argument(\"--mode\", type=str, choices=[\"auto\", \"manual\"], default=\"auto\", help=\"Operational mode.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts.\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file.\")\n\n    return parser.parse_args()\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Parse command line arguments.\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description.\")\n    parser.add_argument(\"--upload-files\", nargs=\"+\", type=str, help=\"Files to upload.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts.\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file.\")\n\n    return parser.parse_args()\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Run a task with optional subtasks and models.\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file\")\n    return parser.parse_args()\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{field.name} must be a list of 2D points\")\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{field.name} must be a list of 2D points\")\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{field.name} must be a list of 2D points\")\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{field.name} must be a list of 2D points\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{field.name} must be a list of 2D points\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{field.name} must be a list of 2D points\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{field.name} must be a list of 2D points.\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{field.name} must be a list of 2D points with shape (_, 2)\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{field.name} must be a list of points with shape (_, 2). Got {v.shape}.\"\n        )\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{field.name} must be a list of 2D points, i.e. shape (_, 2)\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{field.name} must be a list of 2D points, with shape (_, 2).\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__} {field.name} must be a list of 2D points\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}.{field.name} must be a list of 2D points (i.e. shape (_, 2))\"\n        )\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{field.name} must be a list of 2D points with shape (_, 2), not {v.shape}.\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{field.name} must be a list of points with shape (_, 2), not {v.shape}\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f'{field.name} must be a list of 2D points')\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points with shape (_, 2)\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{cls.__name__} {field.name} must be a list of 2D points with shape (_, 2).\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{field.name} must be a list of 2D points (list of lists with 2 elements).\"\n        )\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    character_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n    return character_set[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    char_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n    if n < len(char_set):\n        return char_set[n]\n    else:\n        raise ValueError(\"Input integer out of range\")\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    character_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n    return character_set[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    character_set = 'abcdefghijklmnopqrstuvwxyz0123456789_'\n    return character_set[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    character_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n    return character_set[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    char_set = 'abcdefghijklmnopqrstuvwxyz0123456789_'\n    if n < 0 or n >= len(char_set):\n        raise ValueError(\"Input must be a non-negative integer less than {}\".format(len(char_set)))\n    return char_set[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    char_set = 'abcdefghijklmnopqrstuvwxyz0123456789_'\n\n    # Check if the input integer is within the valid range\n    if n < 0 or n >= len(char_set):\n        raise ValueError(\"Input integer out of range\")\n\n    # Return the corresponding character from the character set\n    return char_set[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    CHAR_SET = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n    if n < 0:\n        raise ValueError(\"Input must be a positive integer\")\n    if n >= len(CHAR_SET):\n        raise ValueError(\"Input exceeds the character set limit\")\n    return CHAR_SET[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    char_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n\n    # Check if the input integer is within the range of the character set\n    if n < 0 or n >= len(char_set):\n        raise ValueError(\"Input integer out of range\")\n\n    # Return the character at the specified index in the character set\n    return char_set[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n    if n < len(charset):\n        return charset[n]\n    else:\n        raise ValueError(\"Input integer out of range\")\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    character_set = 'abcdefghijklmnopqrstuvwxyz0123456789_'\n    if n < 0 or n >= len(character_set):\n        raise ValueError(\"Input integer out of range\")\n    return character_set[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n    if n < 0:\n        raise ValueError(\"Input must be a non-negative integer\")\n    if n >= len(charset):\n        raise ValueError(\"Input exceeds the size of the character set\")\n    return charset[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    char_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n    if n < 0 or n >= len(char_set):\n        raise ValueError(\"Input must be a non-negative integer less than {}\".format(len(char_set)))\n    return char_set[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    char_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n\n    # Check if the input integer is within the range of the character set\n    if 0 <= n < len(char_set):\n        # Return the character at the specified index in the character set\n        return char_set[n]\n    else:\n        # Raise a ValueError if the input integer is out of range\n        raise ValueError(\"Input integer out of range\")\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    characters = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n\n    # Check if the input integer is within the range of the character set\n    if n < 0 or n >= len(characters):\n        raise ValueError(\"Input integer out of range\")\n\n    # Return the encoded character corresponding to the input integer\n    return characters[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    char_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n\n    # Check if the input integer is within the valid range\n    if n < 0 or n >= len(char_set):\n        raise ValueError(\"Input integer must be between 0 and {}\".format(len(char_set)))\n\n    # Return the corresponding character from the character set\n    return char_set[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = 'abcdefghijklmnopqrstuvwxyz0123456789_'\n\n    # Check if the input integer is within the valid range\n    if 0 <= n < 37:\n        # Return the corresponding character from the character set\n        return char_set[n]\n    else:\n        # Raise an error if the input integer is out of range\n        raise ValueError(\"Input integer out of range\")\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    char_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n\n    # Check if the input is within the valid range\n    if n < 0 or n >= len(char_set):\n        raise ValueError(\"Input out of range\")\n\n    # Select the character from the character set based on the input integer\n    return char_set[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    char_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n\n    # Calculate the length of the character set\n    char_set_len = len(char_set)\n\n    # Calculate the index of the character to be returned\n    index = n % char_set_len\n\n    # Return the character at the calculated index\n    return char_set[index]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    CHARSET = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n\n    # Check if the input is a positive integer\n    if not isinstance(n, int) or n < 0:\n        raise ValueError(\"Input must be a non-negative integer\")\n\n    # Check if the input is within the range of the character set\n    if n >= len(CHARSET):\n        raise ValueError(\"Input is out of range\")\n\n    # Return the character at the specified index in the character set\n    return CHARSET[n]\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, eps)) + jnp.log(value_at_zero) - jnp.log(eps)"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.where(x < eps, value_at_zero, x))"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x < eps, value_at_zero, x)\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  x = jnp.where(x < eps, jnp.full_like(x, value_at_zero), x)\n  return jnp.log(x)"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, eps)) + jnp.log(value_at_zero) - jnp.log(eps)"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.where(jnp.abs(x) < eps, value_at_zero, x))"}
{"namespace": "spin_math.safe_log", "completion": "  x = jnp.where(x < eps, jnp.ones_like(x) * value_at_zero, x)\n  return jnp.log(x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, eps)) + jnp.log(jnp.maximum(1.0 - x, eps)) - jnp.log(value_at_zero)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  x = jnp.where(x < eps, value_at_zero, x)\n  return jnp.log(x)\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, eps)) + jnp.where(x < eps, jnp.log(value_at_zero) - jnp.log(eps), 0.)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  x = jnp.where(x < eps, value_at_zero, x)\n  return jnp.log(x)\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_log = jnp.where(x < eps, jnp.log(value_at_zero), jnp.log(x))\n  return safe_log\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  x = jnp.where(x < eps, jnp.ones_like(x) * value_at_zero, x)\n  return jnp.log(x)\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, eps)) + value_at_zero\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, eps)) + value_at_zero\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x < eps, jnp.ones_like(x) * value_at_zero, x)\n  return jnp.log(safe_x)\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.where(x < eps, jnp.full_like(x, value_at_zero), x))\n"}
{"namespace": "spin_math.safe_log", "completion": "  x = jnp.clip(x, a_min = eps)\n  return jnp.log(x) - jnp.log(eps) + jnp.log(value_at_zero)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  # Clamp the input array to a small value to avoid NaNs in the logarithm computation\n  x = jnp.clip(x, eps)\n\n  # Compute the logarithm of the clamped array\n  return jnp.log(x) + value_at_zero\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  x = jnp.where(x < eps, jnp.ones_like(x) * value_at_zero, x)\n  return jnp.log(x)\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_id, intervals in workers_intervals.items():\n        for interval in intervals:\n            size = interval[1] - interval[0]\n            indexes[worker_id] += size\n            if indexes[worker_id] >= interval[1]:\n                chunk_indexes[worker_id] = interval[0]\n                indexes[worker_id] = interval[0]\n    return chunk_indexes, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker_id, intervals in workers_intervals.items():\n        for interval in intervals:\n            indexes[worker_id] += interval[1] - interval[0]\n    return indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_id, intervals in workers_intervals.items():\n        chunk_indexes[worker_id] = 0\n        for interval in intervals:\n            size = interval[1] - interval[0]\n            indexes[worker_id] += size\n            if indexes[worker_id] >= interval[1]:\n                chunk_indexes[worker_id] += 1\n                indexes[worker_id] = interval[0]\n\n    return chunk_indexes, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker_idx, intervals in workers_intervals.items():\n        for interval in intervals:\n            size = interval[1] - interval[0]\n            indexes[worker_idx] += size\n            if indexes[worker_idx] >= len(intervals):\n                indexes[worker_idx] = 0\n\n    return indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_idx, intervals in workers_intervals.items():\n        for interval in intervals:\n            size = interval[1] - interval[0]\n            indexes[worker_idx] += size\n            if indexes[worker_idx] >= size:\n                indexes[worker_idx] -= size\n                chunk_indexes[worker_idx] = interval[0]\n    return chunk_indexes, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker in workers_intervals:\n        interval = workers_intervals[worker][indexes[worker]]\n        indexes[worker] = indexes[worker] + 1\n        if indexes[worker] == len(workers_intervals[worker]):\n            indexes[worker] = 0\n    return indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker, intervals in workers_intervals.items():\n        chunk_indexes[worker] = 0\n        for interval in intervals:\n            if indexes[worker] < interval[-1] - interval[0]:\n                indexes[worker] += 1\n            else:\n                indexes[worker] = 0\n                chunk_indexes[worker] += 1\n    return chunk_indexes, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_id, intervals in workers_intervals.items():\n        chunk_indexes[worker_id] = indexes[worker_id]\n        indexes[worker_id] += len(intervals)\n    return chunk_indexes, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_id, intervals in workers_intervals.items():\n        current_index = indexes[worker_id]\n        chunk_index = -1\n        for interval in intervals:\n            size = interval[1] - interval[0]\n            if current_index >= size:\n                current_index -= size\n            else:\n                chunk_index += 1\n                break\n        chunk_indexes[worker_id] = chunk_index\n        indexes[worker_id] = current_index\n    return chunk_indexes, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {worker: 0 for worker in workers_intervals}\n    for worker in workers_intervals:\n        interval = workers_intervals[worker][chunk_indexes[worker]]\n        indexes[worker] += interval[1] - interval[0]\n        if indexes[worker] >= sum(interval):\n            chunk_indexes[worker] += 1\n            indexes[worker] = 0\n    return chunk_indexes, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker_id, intervals in workers_intervals.items():\n        for interval in intervals:\n            indexes[worker_id] += interval[-1] - interval[0]\n\n    return indexes\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker, intervals in workers_intervals.items():\n        for i, interval in enumerate(intervals):\n            chunk_size = interval[1] - interval[0]\n            indexes[worker] += chunk_size\n            if indexes[worker] >= interval[1]:\n                indexes[worker] = interval[0]\n                workers_intervals[worker][i] = (interval[0], interval[0] + chunk_size)\n\n    return workers_intervals, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker, intervals in workers_intervals.items():\n        index = indexes[worker]\n        chunk_index = 0\n        for interval in intervals:\n            if index < interval[1]:\n                break\n            chunk_index += 1\n            index -= interval[1] - interval[0]\n        chunk_indexes[worker] = chunk_index\n        indexes[worker] = index\n    return chunk_indexes, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    # Initialize the chunk index for each worker to 0\n    chunk_indexes = {worker_id: 0 for worker_id in workers_intervals.keys()}\n\n    # Iterate through each worker's intervals\n    for worker_id, intervals in workers_intervals.items():\n\n        # Update the current index for the worker based on the size of the current interval\n        indexes[worker_id] += intervals[chunk_indexes[worker_id]][1] - intervals[chunk_indexes[worker_id]][0]\n\n        # If the current index is greater than or equal to the size of the current interval, move to the next interval and update the chunk index\n        if indexes[worker_id] >= intervals[chunk_indexes[worker_id]][1] - intervals[chunk_indexes[worker_id]][0]:\n            chunk_indexes[worker_id] += 1\n            indexes[worker_id] = 0\n\n    return chunk_indexes, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n\n    for worker_index, intervals in workers_intervals.items():\n        current_index = indexes[worker_index]\n        chunk_indexes[worker_index] = current_index\n\n        for interval in intervals:\n            current_index += interval[-1] - interval[0]\n            indexes[worker_index] = current_index\n\n    return chunk_indexes, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_id, intervals in workers_intervals.items():\n        chunk_indexes[worker_id] = indexes[worker_id]\n        for interval in intervals:\n            if len(interval) < 2:\n                raise ValueError(\n                    \"Each interval should be a list or tuple with at least two elements.\"\n                )\n            if interval[1] - interval[0] < 0:\n                raise ValueError(\n                    \"The second element of the interval should be greater than or equal to the first element.\"\n                )\n            indexes[worker_id] += interval[1] - interval[0]\n    return chunk_indexes, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_idx, intervals in workers_intervals.items():\n        current_idx = indexes[worker_idx]\n        chunk_index = 0\n        for interval in intervals:\n            size = interval[1] - interval[0]\n            if current_idx >= size:\n                current_idx -= size\n                chunk_index += 1\n            else:\n                break\n        chunk_indexes[worker_idx] = chunk_index\n        indexes[worker_idx] = current_idx\n    return chunk_indexes, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_index = {}\n    for worker_id, intervals in workers_intervals.items():\n        chunk_index[worker_id] = 0\n        for interval in intervals:\n            if len(interval) > 1:\n                indexes[worker_id] += interval[1] - interval[0]\n            else:\n                indexes[worker_id] += 1\n\n    return chunk_index, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker_id in workers_intervals:\n        interval = workers_intervals[worker_id][indexes[worker_id]]\n        indexes[worker_id] += interval[1] - interval[0]\n        if indexes[worker_id] >= len(interval):\n            indexes[worker_id] = 0\n            workers_intervals[worker_id].pop(0)\n\n    return workers_intervals, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_id in workers_intervals:\n        chunk_indexes[worker_id] = 0\n    for worker_id in workers_intervals:\n        chunk_size = workers_intervals[worker_id][indexes[worker_id]][1] - workers_intervals[worker_id][indexes[worker_id]][0]\n        if indexes[worker_id] + 1 < len(workers_intervals[worker_id]):\n            indexes[worker_id] += 1\n            chunk_indexes[worker_id] = indexes[worker_id]\n        else:\n            indexes[worker_id] = 0\n            chunk_indexes[worker_id] = indexes[worker_id]\n    return indexes, chunk_indexes\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == \"grid\":\n    coordinates = coordinates.astype(np.int32)\n    coordinates = np.clip(coordinates, 0, np.array(values.shape[:3]) - 1)\n    return values[coordinates[:, 0], coordinates[:, 1], coordinates[:, 2]]\n  elif datastructure == \"hash\":\n    return values[coordinates]\n  else:\n    raise ValueError(f\"Invalid datastructure: {datastructure}\")"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust coordinates to match the voxel grid\n    coordinates = coordinates.astype(int)\n    coordinates = coordinates + 0.5\n\n    # Sample from the voxel grid\n    sampled_values = sample_grid(values, coordinates)\n\n  elif datastructure == 'hash':\n    # Adjust coordinates to match the hash data structure\n    coordinates = coordinates.astype(int)\n\n    # Sample from the hash data structure\n    sampled_values = sample_hash(values, coordinates)\n\n  else:\n    raise ValueError(\"Invalid datastructure. Only 'grid' and 'hash' are supported.\")\n\n  return sampled_values"}
{"namespace": "grid_utils.trilerp", "completion": "  # Check if the datastructure is valid\n  if datastructure not in ['grid', 'hash']:\n      raise ValueError(\"Invalid datastructure. Please use 'grid' or 'hash'.\")\n\n  # Adjust the coordinates based on the datastructure\n  if datastructure == 'grid':\n      coordinates = coordinates.copy()\n      coordinates[..., 0] = (coordinates[..., 0] * (values.shape[0] - 1))\n      coordinates[..., 1] = (coordinates[..., 1] * (values.shape[1] - 1))\n      coordinates[..., 2] = (coordinates[..., 2] * (values.shape[2] - 1))\n  elif datastructure == 'hash':\n      coordinates = coordinates.copy()\n      coordinates[..., 0] = (coordinates[..., 0] * (values.shape[0] - 1))\n      coordinates[..., 1] = (coordinates[..., 1] * (values.shape[1] - 1))\n      coordinates[..., 2] = (coordinates[..., 2] * (values.shape[2] - 1))\n\n  # Perform trilinear interpolation based on the datastructure\n  if datastructure == 'grid':\n      return _trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n      return _trilerp_hash(values, coordinates)\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f\"Invalid datastructure: {datastructure}. Supported datastructures are 'grid' and 'hash'.\")\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == \"grid\":\n    # Adjust the coordinates to match the voxel grid\n    adjusted_coordinates = coordinates + 0.5\n    # Sample from the voxel grid using trilinear interpolation\n    sampled_values = trilinear_interpolation(values, adjusted_coordinates)\n  elif datastructure == \"hash\":\n    # Sample from the hash data structure using trilinear interpolation\n    sampled_values = trilinear_interpolation(values, coordinates)\n  else:\n    raise ValueError(\"Invalid datastructure. Only 'grid' or 'hash' are supported.\")\n\n  return sampled_values"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust coordinates to match the voxel grid\n    coordinates = coordinates.copy()\n    coordinates[:, 0] = (coordinates[:, 0] + 1) / 2 * (values.shape[1] - 1)\n    coordinates[:, 1] = (coordinates[:, 1] + 1) / 2 * (values.shape[2] - 1)\n    coordinates[:, 2] = (coordinates[:, 2] + 1) / 2 * (values.shape[3] - 1)\n\n    # Perform trilinear interpolation\n    x, y, z = coordinates[:, 0], coordinates[:, 1], coordinates[:, 2]\n    x0, y0, z0 = np.floor(x).astype(int), np.floor(y).astype(int), np.floor(z).astype(int)\n    x1, y1, z1 = x0 + 1, y0 + 1, z0 + 1\n\n    # Clip values to the range of the voxel grid\n    x0 = np.clip(x0, 0, values.shape[1] - 1)\n    y0 = np.clip(y0, 0, values.shape[2] - 1)\n    z0 = np.clip(z0, 0, values.shape[3] - 1)\n    x1 = np.clip(x1, 0, values.shape[1] - 1)\n    y1 = np.clip(y1, 0, values.shape[2] - 1)\n    z1 = np.clip(z1, 0, values.shape[3] - 1)\n\n    # Sample values from the voxel grid\n    v000 = values[:, x0, y0, z0]\n    v001 = values[:, x0, y0, z1]\n    v010 = values[:, x0, y1, z0]\n    v011 = values[:, x0, y1, z1]\n    v"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust coordinates to be within the bounds of the grid\n    coordinates = np.clip(coordinates, 0, np.array(values.shape[:3]) - 1)\n\n    # Get the indices of the corners of the cube containing the point\n    lower_indices = np.floor(coordinates).astype(int)\n    upper_indices = np.ceil(coordinates).astype(int)\n\n    # Get the coordinates within the cube\n    cube_coordinates = coordinates - lower_indices\n\n    # Get the values of the corners of the cube\n    lower_values = values[lower_indices[:, 0], lower_indices[:, 1], lower_indices[:, 2], :]\n    upper_values = values[upper_indices[:, 0], upper_indices[:, 1], upper_indices[:, 2], :]\n\n    # Interpolate along the x-axis\n    x_interpolated_values = lower_values + (upper_values - lower_values) * cube_coordinates[:, 0:1]\n\n    # Interpolate along the y-axis\n    y_interpolated_values = x_interpolated_values + (upper_values - x_interpolated_values) * cube_coordinates[:, 1:2]\n\n    # Interpolate along the z-axis\n    interpolated_values = y_interpolated_values + (upper_values - y_interpolated_values) * cube_coordinates[:, 2:3]\n\n    return interpolated_values\n\n  elif datastructure == 'hash':\n    # Adjust coordinates to be within the bounds of the grid\n    coordinates = np.clip(coordinates, 0, np.array(values.shape[:2]) - 1)\n\n    # Get the indices of the corners of the cube containing the point\n    lower_indices = np.floor(coordinates).astype(int)\n    upper_indices = np.ceil(coordinates).astype(int)\n\n    # Get the coordinates within"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f\"Invalid datastructure: {datastructure}. Supported datastructures are 'grid' and 'hash'.\")\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f\"Invalid datastructure: {datastructure}. Supported datastructures are 'grid' and 'hash'.\")\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust coordinates for sampling from a 3D voxel grid\n    coordinates = coordinates.astype(np.float32)\n    coordinates = coordinates + 0.5\n    coordinates = np.clip(coordinates, 0, values.shape[1] - 1)\n\n    # Sample from the 3D voxel grid using trilinear interpolation\n    sampled_values = ndimage.map_coordinates(values, coordinates, order=1, mode='nearest')\n  elif datastructure == 'hash':\n    # Adjust coordinates for sampling from a hashed data structure\n    coordinates = coordinates.astype(np.int32)\n\n    # Sample from the hashed data structure using nearest neighbor interpolation\n    sampled_values = values[coordinates]\n  else:\n    raise ValueError(\"Invalid datastructure. Supported datastructures are 'grid' and 'hash'.\")\n\n  return sampled_values"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == \"grid\":\n    return trilerp_grid(values, coordinates)\n  elif datastructure == \"hash\":\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f\"Invalid datastructure: {datastructure}\")\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}. Only \"grid\" and \"hash\" are supported.')\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f\"Invalid datastructure: {datstructure}. Only 'grid' and 'hash' are supported.\")\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f\"Invalid datastructure: {datastructure}. Must be 'grid' or 'hash'.\")\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    coordinates = coordinates[..., [2, 1, 0]]\n    return grid_trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_trilerp(values, coordinates)\n  else:\n    raise ValueError(\"Invalid datastructure. Only 'grid' and 'hash' are supported.\")\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == \"grid\":\n    # Adjust coordinates to match the shape of the voxel grid\n    coordinates = coordinates.reshape(-1, 3)\n    coordinates = coordinates.astype(np.int32)\n    coordinates = coordinates[:, [2, 1, 0]]\n\n    # Clip coordinates to the bounds of the voxel grid\n    coordinates[:, 0] = np.clip(coordinates[:, 0], 0, values.shape[0] - 1)\n    coordinates[:, 1] = np.clip(coordinates[:, 1], 0, values.shape[1] - 1)\n    coordinates[:, 2] = np.clip(coordinates[:, 2], 0, values.shape[2] - 1)\n\n    # Extract the values at the four corners of the voxel\n    voxel_values = values[coordinates[:, 0], coordinates[:, 1], coordinates[:, 2]]\n\n    # Compute the interpolation weights\n    weights = coordinates - np.floor(coordinates)\n\n    # Compute the interpolated values\n    interpolated_values = (\n        voxel_values[..., :-1] * (1 - weights[:, 0:1]) * (1 - weights[:, 1:2]) * (1 - weights[:, 2:3])\n        + voxel_values[..., 1:] * weights[:, 0:1] * (1 - weights[:, 1:2]) * (1 - weights[:, 2:3])\n        + voxel_values[..., :-1] * (1 - weights[:, 0:1]) * weights[:, 1:2] * (1 - weights[:, 2:3])\n        + voxel_values[..., 1:] * weights[:, 0:1] * weights[:, 1:2] * (1 - weights[:, 2:3])\n        + voxel_values[..., :-1] * (1 - weights[:, 0:1]) * (1 - weights[:, 1:2"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f\"Invalid datastructure: {datstructure}\")\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    coordinates = coordinates.astype(np.int32)\n    return grid_trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_trilerp(values, coordinates)\n  else:\n    raise ValueError(f\"Unknown data structure {datastructure}\")\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == \"grid\":\n    # Adjust coordinates to account for the fact that the grid's origin is in the center\n    # of the first voxel, not at the corner\n    adjusted_coordinates = coordinates.copy()\n    adjusted_coordinates[..., 0] += 0.5\n    adjusted_coordinates[..., 1] += 0.5\n    adjusted_coordinates[..., 2] += 0.5\n\n    # Sample from the grid using trilinear interpolation\n    interpolated_values = ndimage.map_coordinates(\n        values, adjusted_coordinates.T, order=1, mode=\"nearest\"\n    )\n\n    return interpolated_values\n\n  elif datastructure == \"hash\":\n    # Sample from the hash data structure using trilinear interpolation\n    interpolated_values = []\n    for coord in coordinates:\n      # Find the 8 nearest neighbors to the coordinate\n      neighbors = find_nearest_neighbors(coord, values)\n\n      # Compute the weights for each neighbor based on the distance\n      weights = compute_weights(coord, neighbors)\n\n      # Interpolate the values using the weights\n      interpolated_value = np.dot(weights, neighbors)\n\n      # Append the interpolated value to the list\n      interpolated_values.append(interpolated_value)\n\n    return np.array(interpolated_values)\n\n  else:\n    raise ValueError(\n        \"Invalid datastructure. Please choose 'grid' or 'hash' as the datastructure.\"\n    )"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the input is valid\n  if v < 1:\n    raise ValueError(\"The tessellation factor must be greater than or equal to 1.\")\n\n  # Initialize the weights matrix\n  weights = np.zeros((v + 1, v + 1))\n\n  # Compute the weights\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      weights[i, j] = scipy.special.binom(v, i) * scipy.special.binom(v - i, j)\n\n  # Normalize the weights\n  weights = weights / np.sum(weights)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the input is valid\n  if v < 1:\n    raise ValueError(\"The tessellation factor must be greater than or equal to 1.\")\n\n  # Initialize the weights array\n  weights = np.zeros((v + 1, v + 1, 3))\n\n  # Iterate over the rows of the weights array\n  for i in range(v + 1):\n\n    # Iterate over the columns of the weights array\n    for j in range(v + 1 - i):\n\n      # Compute the barycentric coordinates for the current point\n      weights[i, j, 0] = i / v\n      weights[i, j, 1] = j / v\n      weights[i, j, 2] = 1 - (i + j) / v\n\n  # Return the weights array\n  return weights\n\n\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Assert that the tessellation factor is greater than or equal to 1\n  assert v >= 1, \"The tessellation factor must be greater than or equal to 1.\"\n\n  # Initialize an empty list to store the barycentric weights\n  weights = []\n\n  # Iterate over the rows of the triangle\n  for i in range(v + 1):\n\n    # Iterate over the columns of the triangle\n    for j in range(v + 1 - i):\n\n      # Compute the barycentric coordinates for the current point\n      x = i / v\n      y = j / v\n      z = 1 - x - y\n\n      # Append the barycentric coordinates to the list of weights\n      weights.append([x, y, z])\n\n  # Convert the list of weights to a numpy array\n  weights = np.array(weights)\n\n  # Return the numpy array of weights\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  import numpy as np\n\n  if v < 1:\n    raise ValueError(\"Tesselation factor must be greater than or equal to 1.\")\n\n  # Initialize the weights array with zeros\n  weights = np.zeros(((v+1)*(v+2))//2, dtype=int)\n\n  # Compute the weights for each vertex of the triangle\n  for i in range(v+1):\n    for j in range(v+1-i):\n      index = i*(v+1-i)//2 + j\n      weights[index] = (v-i)*(v-j)\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / np.sum(weights)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if v is an integer\n  if not isinstance(v, int):\n    raise ValueError(\"v must be an integer\")\n\n  # Check if v is greater than or equal to 1\n  if v < 1:\n    raise ValueError(\"v must be greater than or equal to 1\")\n\n  # Generate the integer weights for each vertex of the triangle\n  weights = np.zeros((v + 1, v + 1, 3))\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      weights[i, j, 0] = i\n      weights[i, j, 1] = j\n      weights[i, j, 2] = v - i - j\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / v\n\n  return weights\n\n\n\n\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the tessellation factor is valid\n  if v < 1:\n    raise ValueError(\"Tessellation factor must be greater than or equal to 1.\")\n\n  # Initialize the weights array with zeros\n  w = np.zeros(((v + 1) * (v + 2) // 2, 3))\n\n  # Compute the weights for the first row of the triangle\n  w[0, :] = np.array([v, 0, 0])\n  for i in range(1, v + 1):\n    w[i, :] = w[i - 1, :] + np.array([-1, 1, 0])\n\n  # Compute the weights for the remaining rows of the triangle\n  for i in range(1, v + 1):\n    for j in range(v + 1 - i):\n      w[i * (v + 1) + j, :] = w[i * (v + 1) + j - 1, :] + np.array([1, -1, 1])\n\n  # Normalize the weights to get the barycentric coordinates\n  w[:, 0] /= v\n  w[:, 1] /= v\n  w[:, 2] = 1 - w[:, 0] - w[:, 1]\n\n  return w"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  import numpy as np\n  from scipy.special import binom\n\n  # Check if the input value is valid\n  if v < 1:\n    raise ValueError(\"The input value must be greater than or equal to 1.\")\n\n  # Initialize the weights array\n  weights = np.zeros((v + 1, v + 1))\n\n  # Iterate over the rows and columns of the weights array\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      # Compute the binomial coefficient using the scipy.special.binom function\n      weights[i, j] = binom(v, i) * binom(v - i, j) * 2 ** (-v)\n\n  # Return the weights array\n  return weights\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Assert that the tessellation factor is an integer greater than or equal to 1\n  assert isinstance(v, int) and v >= 1, \"v must be an integer greater than or equal to 1\"\n\n  # Compute the number of points in the tessellated triangle\n  num_points = (v + 1) * (v + 2) // 2\n\n  # Initialize an array to store the barycentric weights\n  weights = np.zeros((num_points, 3))\n\n  # Initialize the index of the current point\n  index = 0\n\n  # Loop over the rows of the triangle\n  for i in range(v + 1):\n    # Loop over the columns of the triangle\n    for j in range(v + 1 - i):\n      # Compute the barycentric coordinates of the current point\n      weights[index, 0] = i / v\n      weights[index, 1] = j / v\n      weights[index, 2] = 1 - (i + j) / v\n\n      # Normalize the barycentric coordinates to get the barycentric weights\n      weights[index, :] /= np.sum(weights[index, :])\n\n      # Increment the index of the current point\n      index += 1\n\n  # Return the barycentric weights\n  return weights\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Assert that the tessellation factor is greater than or equal to 1\n  assert v >= 1, \"Tessellation factor must be greater than or equal to 1.\"\n\n  # Initialize an empty list to store the weights\n  weights = []\n\n  # Loop through the rows of the triangle\n  for i in range(v + 1):\n\n    # Loop through the columns of the triangle\n    for j in range(v + 1 - i):\n\n      # Compute the weight for the current point\n      weight = (1 - i / v) * (1 - j / v) * i / v\n\n      # Append the weight to the list of weights\n      weights.append(weight)\n\n  # Convert the list of weights to a numpy array\n  weights = np.array(weights)\n\n  # Return the weights\n  return weights\n\n\n\n\n\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  import numpy as np\n\n  # Check that the tessellation factor is valid\n  if v < 1:\n    raise ValueError(\"The tessellation factor must be greater than or equal to 1.\")\n\n  # Generate the integer weights for each vertex of the triangle\n  weights = np.array([[i, j, v - i - j] for i in range(v + 1) for j in range(v + 1 - i)])\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / v\n\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Import necessary libraries\n  import numpy as np\n\n  # Initialize an empty list to store the weights\n  weights = []\n\n  # Iterate over the rows of the triangle\n  for i in range(v + 1):\n\n    # Iterate over the columns of the triangle\n    for j in range(v + 1 - i):\n\n      # Calculate the weight for the current point\n      weight = np.array([i, j, v - i - j])\n\n      # Append the weight to the list of weights\n      weights.append(weight)\n\n  # Convert the list of weights to a numpy array\n  weights = np.array(weights)\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / v\n\n  # Return the barycentric weights\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Assert that the tessellation factor is greater than or equal to 1\n  assert v >= 1\n\n  # Initialize the weights array\n  weights = np.zeros((v + 1, v + 1, 3))\n\n  # Compute the weights for each point in the triangle\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      weights[i, j, 0] = i / v\n      weights[i, j, 1] = j / v\n      weights[i, j, 2] = 1 - (i + j) / v\n\n  # Normalize the weights to get the barycentric coordinates\n  weights /= np.sum(weights, axis=(0, 1))\n\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Import necessary libraries\n  import numpy as np\n  from itertools import product\n\n  # Check if the input is valid\n  assert v >= 1, \"The tessellation factor must be greater than or equal to 1.\"\n\n  # Generate integer weights for each vertex of the triangle\n  weights = np.array(list(product(range(v + 1), repeat=3)))\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / v\n\n  # Return the barycentric weights\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the input is valid\n  if v < 1:\n    raise ValueError(\"The tessellation factor must be greater than or equal to 1.\")\n\n  # Initialize the weights array\n  weights = np.zeros((v + 1, v + 1))\n\n  # Iterate over the rows of the weights array\n  for i in range(v + 1):\n    # Iterate over the columns of the weights array\n    for j in range(v + 1 - i):\n      # Compute the weight for the current point\n      weight = math.comb(v, i) * math.comb(v - i, j)\n      # Store the weight in the weights array\n      weights[i, j] = weight\n\n  # Normalize the weights array\n  weights /= np.sum(weights)\n\n  # Return the weights array\n  return weights\n\n\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  import numpy as np\n\n  if v < 1:\n    raise ValueError(\"The tessellation factor must be greater than or equal to 1.\")\n\n  # Generate integer weights for each vertex of the triangle\n  weights = np.array([[i, j, v - i - j] for i in range(v + 1) for j in range(v + 1 - i)])\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / v\n\n  return weights\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  assert v >= 1\n  if v == 1:\n    return np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n\n  else:\n    weights = np.zeros((3, 3))\n    weights[0, :] = np.array([v, 0, 0])\n    weights[1, :] = np.array([0, v, 0])\n    weights[2, :] = np.array([0, 0, v])\n\n    for i in range(1, v):\n      weights = np.vstack((weights, np.array([i, v - i, 0])))\n      weights = np.vstack((weights, np.array([0, i, v - i])))\n\n    for i in range(1, v):\n      for j in range(1, v - i):\n        weights = np.vstack((weights, np.array([i, j, v - i - j])))\n\n    weights = weights / v\n    return weights\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if v is an integer\n  if not isinstance(v, int):\n    raise TypeError(\"v must be an integer\")\n\n  # Check if v is greater than or equal to 1\n  if v < 1:\n    raise ValueError(\"v must be greater than or equal to 1\")\n\n  # Initialize an empty list to store the weights\n  weights = []\n\n  # Loop through all possible values of i and j\n  for i in range(v+1):\n    for j in range(v+1-i):\n      # Compute the weight for this point\n      weight = (1-i/v)*(1-j/v)\n\n      # Add the weight to the list\n      weights.append(weight)\n\n  # Convert the list to a numpy array\n  weights = np.array(weights)\n\n  # Normalize the weights\n  weights /= np.sum(weights)\n\n  return weights\n\n\n\n\n\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Initialize an empty list to store the weights\n  weights = []\n\n  # Iterate through the rows of the triangle\n  for i in range(v+1):\n    # Iterate through the columns of the triangle\n    for j in range(v+1-i):\n      # Compute the weight for the current point\n      weight = (i+j)/(v+1)\n      # Append the weight to the list of weights\n      weights.append(weight)\n\n  # Convert the list of weights to a numpy array\n  weights = np.array(weights)\n\n  # Reshape the weights array to have the same shape as the triangle\n  weights = weights.reshape((v+1, v+1))\n\n  # Return the weights\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(\"The tessellation factor must be greater than or equal to 1.\")\n\n  weights = []\n\n  for i in range(v):\n    for j in range(v - i):\n      weights.append([i, j, v - i - j])\n\n  weights = np.array(weights)\n  weights = weights / v\n\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  import numpy as np\n\n  # Check that the tessellation factor is valid\n  assert v >= 1, \"The tessellation factor must be greater than or equal to 1.\"\n\n  # Initialize the weights array\n  w = np.zeros(((v+1)*(v+2)//2, 3))\n\n  # Loop over the rows of the weights array\n  for i in range(v+1):\n    # Loop over the columns of the weights array\n    for j in range(v+1-i):\n      # Compute the index of the current weight in the flattened array\n      idx = i*(i+1)//2 + j\n      # Set the weights for the current point\n      w[idx, 0] = i / v\n      w[idx, 1] = j / v\n      w[idx, 2] = 1 - w[idx, 0] - w[idx, 1]\n\n  # Normalize the weights\n  w /= np.sum(w, axis=1)[:, np.newaxis]\n\n  return w\n\n"}
{"namespace": "linspline.query", "completion": "  import numpy as np\n\n  # Ensure the spline is valid\n  if len(t) != len(v):\n    raise ValueError(\"The time and value arrays must have the same length.\")\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"The time points must be strictly increasing.\")\n\n  # Interpolate the spline at the query points\n  vq = np.interp(tq, t, v, left=0, right=0)\n\n  return vq\n"}
{"namespace": "linspline.query", "completion": "  # Ensure the spline is valid\n  assert len(t) == len(v), \"The number of time points and values must be the same.\"\n  assert all(np.diff(t) > 0), \"The time points must be strictly increasing.\"\n\n  # Interpolate the spline at the query points\n  vq = np.interp(tq, t, v, left=0, right=0)\n\n  return vq\n\n"}
{"namespace": "linspline.query", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Check that the time points are strictly increasing\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"Time points must be strictly increasing\")\n\n  # Check that the time points and values have the same length\n  if len(t) != len(v):\n    raise ValueError(\"Time points and values must have the same length\")\n\n  # Check that the query points are within the range of the time points\n  if np.any(tq < t[0]) or np.any(tq > t[-1]):\n    raise ValueError(\"Query points must be within the range of the time points\")\n\n  # Create a linear interpolation function using the time points and values\n  f = interp1d(t, v, kind='linear', fill_value='extrapolate')\n\n  # Evaluate the interpolation function at the query points\n  vq = f(tq)\n\n  return vq"}
{"namespace": "linspline.query", "completion": "  import numpy as np\n\n  # Check if tq is an array\n  if not isinstance(tq, np.ndarray):\n    tq = np.array(tq)\n\n  # Check if t is an array\n  if not isinstance(t, np.ndarray):\n    t = np.array(t)\n\n  # Check if v is an array\n  if not isinstance(v, np.ndarray):\n    v = np.array(v)\n\n  # Check if tq is a 1D array\n  if tq.ndim != 1:\n    raise ValueError(\"tq must be a 1D array\")\n\n  # Check if t is a 1D array\n  if t.ndim != 1:\n    raise ValueError(\"t must be a 1D array\")\n\n  # Check if v is a 1D array\n  if v.ndim != 1:\n    raise ValueError(\"v must be a 1D array\")\n\n  # Check if tq is sorted\n  if not np.all(np.diff(tq) >= 0):\n    raise ValueError(\"tq must be sorted\")\n\n  # Check if t is sorted\n  if not np.all(np.diff(t) >= 0):\n    raise ValueError(\"t must be sorted\")\n\n  # Check if tq is within the range of t\n  if tq[0] < t[0] or tq[-1] > t[-1]:\n    raise ValueError(\"tq must be within the range of t\")\n\n  # Check if t and v have the same length\n  if len(t) != len(v):\n    raise ValueError(\"t and v must have the same length\")\n\n  # Initialize the output array\n  vq = np.zeros_like(tq)\n\n  # Find the indices of the knots that bracket each query point\n  i = np.searchsorted(t, tq, side='right') - 1\n\n  # Find the indices of the knots that are within the range of tq\n  i_valid = np.logical_"}
{"namespace": "linspline.query", "completion": "  # Check that the input arrays have the same length\n  assert len(t) == len(v), \"t and v must have the same length\"\n\n  # Check that the time points are in increasing order\n  assert all(t[i] <= t[i+1] for i in range(len(t)-1)), \"t must be in increasing order\"\n\n  # Check that the query points are within the range of the time points\n  assert all(t[0] <= tq[i] <= t[-1] for i in range(len(tq))), \"tq must be within the range of t\"\n\n  # Initialize an array to store the interpolated values\n  vq = np.zeros_like(tq)\n\n  # Loop over the query points\n  for i in range(len(tq)):\n\n    # Find the index of the first time point that is greater than the current query point\n    j = np.searchsorted(t, tq[i])\n\n    # If the query point is before the first time point, set the interpolated value to 0\n    if j == 0:\n      vq[i] = 0\n\n    # If the query point is after the last time point, set the interpolated value to 0\n    elif j == len(t):\n      vq[i] = 0\n\n    # Otherwise, use linear interpolation to find the interpolated value\n    else:\n      vq[i] = v[j-1] + (v[j] - v[j-1]) * (tq[i] - t[j-1]) / (t[j] - t[j-1])\n\n  # Return the interpolated values\n  return vq\n"}
{"namespace": "linspline.query", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Check if the time points are in ascending order\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"Time points must be in ascending order\")\n\n  # Check if the time points and values have the same length\n  if len(t) != len(v):\n    raise ValueError(\"Time points and values must have the same length\")\n\n  # Check if the query points are within the range of the time points\n  if np.any(tq < t[0]) or np.any(tq > t[-1]):\n    raise ValueError(\"Query points must be within the range of the time points\")\n\n  # Create an interpolation function using the linear spline\n  f = interp1d(t, v, kind='linear', fill_value=0, bounds_error=False)\n\n  # Evaluate the interpolation function at the query points\n  vq = f(tq)\n\n  return vq\n"}
{"namespace": "linspline.query", "completion": "  # Import necessary libraries\n  import numpy as np\n\n  # Check if the time points are in ascending order\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"Time points must be in ascending order\")\n\n  # Check if the time points and values have the same length\n  if len(t) != len(v):\n    raise ValueError(\"Time points and values must have the same length\")\n\n  # Check if the query points are within the range of the original time points\n  if np.any(tq < t[0]) or np.any(tq > t[-1]):\n    raise ValueError(\"Query points must be within the range of the original time points\")\n\n  # Find the indices of the time points that are just before the query points\n  i = np.searchsorted(t, tq, side='right') - 1\n\n  # Find the values of the linear spline at the query points\n  vq = np.zeros_like(tq)\n  vq[i >= 0] = v[i[i >= 0]] + (v[i[i >= 0] + 1] - v[i[i >= 0]]) / (t[i[i >= 0] + 1] - t[i[i >= 0]]) * (tq[i >= 0] - t[i[i >= 0]])\n\n  # Set the values outside the original range to 0\n  vq[tq < t[0]] = 0\n  vq[tq > t[-1]] = 0\n\n  return vq"}
{"namespace": "linspline.query", "completion": "  # Ensure the spline is valid\n  assert len(t) == len(v), \"The number of time points and values must be equal.\"\n  assert all(np.diff(t) > 0), \"The time points must be strictly increasing.\"\n\n  # Initialize the output array\n  vq = np.zeros_like(tq)\n\n  # Find the index of the first knot that is greater than or equal to the first query point\n  i = 0\n  while t[i] < tq[0] and i < len(t) - 1:\n    i += 1\n\n  # If the first knot is greater than the first query point, set the first value to 0\n  if i == 0:\n    vq[0] = 0\n\n  # Loop over the query points\n  for j in range(len(tq)):\n    # If the query point is before the first knot, set the value to 0\n    if tq[j] < t[0]:\n      vq[j] = 0\n    # If the query point is after the last knot, set the value to 0\n    elif tq[j] > t[-1]:\n      vq[j] = 0\n    # If the query point is between two knots, interpolate the value\n    else:\n      # Find the index of the knot that is just before the query point\n      while t[i] < tq[j]:\n        i += 1\n      # Interpolate the value using the two knots on either side of the query point\n      vq[j] = v[i - 1] + (v[i] - v[i - 1]) * (tq[j] - t[i - 1]) / (t[i] - t[i - 1])\n\n  return vq\n"}
{"namespace": "linspline.query", "completion": "  # Import packages\n  import numpy as np\n\n  # Check that the time points are increasing\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"Time points must be increasing.\")\n\n  # Check that the time points and values have the same length\n  if len(t) != len(v):\n    raise ValueError(\"Time points and values must have the same length.\")\n\n  # Check that the query points are within the range of the time points\n  if not np.all((tq >= t[0]) & (tq <= t[-1])):\n    raise ValueError(\"Query points must be within the range of the time points.\")\n\n  # Initialize an array to store the interpolated values\n  vq = np.zeros_like(tq)\n\n  # Find the indices of the time points that are less than or equal to the query points\n  idx = np.searchsorted(t, tq, side='right') - 1\n\n  # Compute the interpolated values at the query points\n  vq = np.where(idx >= 0, (v[idx+1] - v[idx]) / (t[idx+1] - t[idx]) * (tq - t[idx]) + v[idx], 0)\n\n  # Return the interpolated values\n  return vq"}
{"namespace": "linspline.query", "completion": "  import numpy as np\n\n  # Check that the input arrays have the same length\n  if len(t) != len(v):\n    raise ValueError(\"The input arrays 't' and 'v' must have the same length.\")\n\n  # Check that the time points are in ascending order\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"The time points in 't' must be in ascending order.\")\n\n  # Check that the query points are within the range of the time points\n  if np.min(tq) < np.min(t) or np.max(tq) > np.max(t):\n    raise ValueError(\"The query points must be within the range of the time points.\")\n\n  # Initialize the output array with zeros\n  vq = np.zeros_like(tq)\n\n  # Loop over the query points and interpolate the values\n  for i in range(len(tq)):\n    # Find the index of the time point immediately preceding the query point\n    j = np.searchsorted(t, tq[i]) - 1\n\n    # Check if the query point is before the first time point\n    if j < 0:\n      vq[i] = 0\n    # Check if the query point is after the last time point\n    elif j >= len(t) - 1:\n      vq[i] = 0\n    # Interpolate the value between the two nearest time points\n    else:\n      vq[i] = v[j] + (v[j + 1] - v[j]) * (tq[i] - t[j]) / (t[j + 1] - t[j])\n\n  return vq\n\n"}
{"namespace": "linspline.query", "completion": "  import numpy as np\n\n  # Ensure that the spline is valid\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"The time points must be strictly increasing.\")\n\n  # Ensure that the spline is defined over the entire range of query points\n  if t[0] > tq[0] or t[-1] < tq[-1]:\n    raise ValueError(\"The spline must be defined over the entire range of query points.\")\n\n  # Interpolate the values at the query points\n  vq = np.interp(tq, t, v, left=0, right=0)\n\n  return vq\n\n"}
{"namespace": "linspline.query", "completion": "  import numpy as np\n  import sys\n\n  # Ensure that the spline is valid\n  if not np.all(np.diff(t) > 0):\n    sys.exit('Error: The time points in t must be strictly increasing')\n\n  # Ensure that tq is a numpy array\n  tq = np.array(tq)\n\n  # Find the indices of the knots that bracket each query point\n  indices = np.searchsorted(t, tq, side='right') - 1\n\n  # Set the indices to 0 if they are negative\n  indices[indices < 0] = 0\n\n  # Set the indices to the last index if they are greater than the last index\n  indices[indices > len(t) - 2] = len(t) - 2\n\n  # Compute the interpolation weights\n  weights = (tq - t[indices]) / (t[indices + 1] - t[indices])\n\n  # Compute the interpolated values\n  vq = (1 - weights) * v[indices] + weights * v[indices + 1]\n\n  # Set the extrapolated values to 0\n  vq[tq < t[0]] = 0\n  vq[tq > t[-1]] = 0\n\n  return vq\n\n"}
{"namespace": "linspline.query", "completion": "  # Import necessary modules\n  import numpy as np\n  import sys\n\n  # Check that t and v are the same length\n  if len(t) != len(v):\n    print(\"Error: t and v must have the same length.\")\n    sys.exit(1)\n\n  # Check that t is sorted\n  if not np.all(np.diff(t) > 0):\n    print(\"Error: t must be sorted in ascending order.\")\n    sys.exit(1)\n\n  # Check that tq is sorted\n  if not np.all(np.diff(tq) > 0):\n    print(\"Error: tq must be sorted in ascending order.\")\n    sys.exit(1)\n\n  # Check that tq is within the range of t\n  if tq[0] < t[0] or tq[-1] > t[-1]:\n    print(\"Error: tq must be within the range of t.\")\n    sys.exit(1)\n\n  # Find the index of the first time point in t that is greater than or equal to the first time point in tq\n  i = np.searchsorted(t, tq[0])\n\n  # Find the index of the last time point in t that is less than or equal to the last time point in tq\n  j = np.searchsorted(t, tq[-1])\n\n  # Extract the relevant time points and values\n  t_relevant = t[i:j+1]\n  v_relevant = v[i:j+1]\n\n  # Initialize the interpolated values array\n  vq = np.zeros_like(tq)\n\n  # Loop over the query points\n  for k in range(len(tq)):\n\n    # Find the index of the first time point in t_relevant that is greater than or equal to the current query point\n    l = np.searchsorted(t_relevant, tq[k])\n\n    # If the current query point is less than the first time point in t_relevant, set the interpolated value to 0\n    if l == 0:\n      vq"}
{"namespace": "linspline.query", "completion": "  import numpy as np\n\n  # Check that t and v are the same length\n  if len(t) != len(v):\n    raise ValueError(\"t and v must be the same length\")\n\n  # Check that t is strictly increasing\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"t must be strictly increasing\")\n\n  # Check that tq is within the range of t\n  if not np.all(np.logical_and(tq >= t[0], tq <= t[-1])):\n    raise ValueError(\"tq must be within the range of t\")\n\n  # Find the indices of the knots that bracket each query point\n  indices = np.searchsorted(t, tq, side='right') - 1\n\n  # Find the values of the knots that bracket each query point\n  t_bracket = t[indices]\n\n  # Find the values of the knots that bracket each query point\n  v_bracket = v[indices]\n\n  # Find the values of the knots that bracket each query point\n  v_next = v[indices + 1]\n\n  # Find the slopes between each pair of knots\n  slopes = (v_next - v_bracket) / (t[indices + 1] - t_bracket)\n\n  # Find the values of the linear spline at each query point\n  vq = v_bracket + slopes * (tq - t_bracket)\n\n  # Set the values outside the range of t to 0\n  vq[tq < t[0]] = 0\n  vq[tq > t[-1]] = 0\n\n  return vq\n\n"}
{"namespace": "linspline.query", "completion": "  # Import packages\n  import numpy as np\n\n  # Check that the spline is valid\n  if not np.all(np.diff(t) > 0):\n    raise ValueError('Time points must be strictly increasing')\n\n  # Check that the spline is valid\n  if len(t) != len(v):\n    raise ValueError('Time and value arrays must be the same length')\n\n  # Initialize an array to store the interpolated values\n  vq = np.zeros_like(tq)\n\n  # Loop over each query point\n  for i in range(len(tq)):\n\n    # Find the index of the knot that is immediately before the query point\n    j = np.searchsorted(t, tq[i]) - 1\n\n    # If the query point is before the first knot, set the interpolated value to 0\n    if j < 0:\n      vq[i] = 0\n\n    # If the query point is after the last knot, set the interpolated value to 0\n    elif j >= len(t) - 1:\n      vq[i] = 0\n\n    # Otherwise, use linear interpolation to find the interpolated value\n    else:\n      vq[i] = (tq[i] - t[j]) * (v[j + 1] - v[j]) / (t[j + 1] - t[j]) + v[j]\n\n  # Return the interpolated values\n  return vq\n\n"}
{"namespace": "linspline.query", "completion": "  import numpy as np\n\n  # Check that the input arrays are the same size\n  if len(t) != len(v):\n    raise ValueError(\"t and v must have the same length\")\n\n  # Check that the time points are in ascending order\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"t must be in ascending order\")\n\n  # Check that the time points are not all the same\n  if np.all(np.diff(t) == 0):\n    raise ValueError(\"t must have at least two unique values\")\n\n  # Check that the query points are within the range of the time points\n  if np.min(tq) < np.min(t) or np.max(tq) > np.max(t):\n    raise ValueError(\"tq must be within the range of t\")\n\n  # Find the indices of the time points that bracket each query point\n  indices = np.searchsorted(t, tq, side='left')\n\n  # Set the indices to the last time point if they are outside the range of the time points\n  indices = np.clip(indices, 1, len(t) - 1)\n\n  # Find the values of the linear spline at the query points\n  vq = np.zeros_like(tq)\n  for i in range(len(tq)):\n    vq[i] = v[indices[i] - 1] + (v[indices[i]] - v[indices[i] - 1]) * (tq[i] - t[indices[i] - 1]) / (t[indices[i]] - t[indices[i] - 1])\n\n  return vq\n\n\n\n\n\n\n"}
{"namespace": "linspline.query", "completion": "  # Import modules\n  import numpy as np\n\n  # Check that the spline is valid\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"The time points must be strictly increasing.\")\n\n  # Check that the spline and query points are compatible\n  if not np.all(tq >= t[0]) or not np.all(tq <= t[-1]):\n    raise ValueError(\"The query points must be within the range of the spline.\")\n\n  # Find the index of the first time point that is greater than or equal to each query point\n  i = np.searchsorted(t, tq, side='left')\n\n  # Find the interpolation weights\n  w = (tq - t[i-1]) / (t[i] - t[i-1])\n\n  # Interpolate the values at the query points\n  vq = v[i-1] * (1 - w) + v[i] * w\n\n  # Set the extrapolated values to 0\n  vq[tq < t[0]] = 0\n  vq[tq > t[-1]] = 0\n\n  return vq\n"}
{"namespace": "linspline.query", "completion": "  import numpy as np\n\n  # Check that the input arrays have the same length\n  if len(t) != len(v):\n    raise ValueError(\"The length of 't' and 'v' must be the same.\")\n\n  # Sort the time points and corresponding values\n  t, v = zip(*sorted(zip(t, v)))\n\n  # Check that the time points are strictly increasing\n  if not all(np.diff(t) > 0):\n    raise ValueError(\"The time points in 't' must be strictly increasing.\")\n\n  # Check that the query points are within the range of the time points\n  if not np.all((tq >= t[0]) & (tq <= t[-1])):\n    raise ValueError(\"The query points must be within the range of the time points.\")\n\n  # Find the indices of the query points in the time points array\n  tq_indices = np.searchsorted(t, tq)\n\n  # Find the indices of the time points that are less than or equal to the query points\n  t_le_tq = np.where(t <= tq)[0]\n\n  # Find the indices of the time points that are greater than the query points\n  t_gt_tq = np.where(t > tq)[0]\n\n  # Initialize the output array\n  vq = np.zeros_like(tq)\n\n  # Set the values for the query points that are less than or equal to the first time point to 0\n  vq[tq <= t[0]] = 0\n\n  # Set the values for the query points that are greater than the last time point to 0\n  vq[tq >= t[-1]] = 0\n\n  # Set the values for the query points that are within the range of the time points\n  vq[tq_indices[tq_indices != 0] == tq_indices] = v[tq_indices[tq_indices != 0] - 1]\n\n  # Set the values for the query points that are between two time points\n  for i in range(len(t_le"}
{"namespace": "linspline.query", "completion": "  # Import modules\n  import numpy as np\n\n  # Ensure that t and v are arrays\n  t = np.array(t)\n  v = np.array(v)\n\n  # Ensure that t and v are 1D arrays\n  if (len(t.shape) > 1) or (len(v.shape) > 1):\n    raise ValueError(\"t and v must be 1D arrays.\")\n\n  # Ensure that t and v have the same length\n  if len(t) != len(v):\n    raise ValueError(\"t and v must have the same length.\")\n\n  # Ensure that t is sorted in ascending order\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"t must be sorted in ascending order.\")\n\n  # Ensure that tq is an array\n  tq = np.array(tq)\n\n  # Ensure that tq is 1D\n  if len(tq.shape) > 1:\n    raise ValueError(\"tq must be a 1D array.\")\n\n  # Find the index of the first knot that is greater than the first query point\n  i = np.argmax(t > tq[0])\n\n  # If no such knot exists, return an array of zeros with the same length as tq\n  if i == 0:\n    return np.zeros_like(tq)\n\n  # If the first knot is greater than the first query point, return an array of zeros with the same length as tq\n  elif i == len(t):\n    return np.zeros_like(tq)\n\n  # If the first knot is less than or equal to the first query point, set i to i-1\n  else:\n    i = i - 1\n\n  # Initialize an empty list to store the interpolated values\n  vq = []\n\n  # Loop over the query points\n  for tqi in tq:\n\n    # If the current query point is less than the first knot, append a zero to the list\n    if tqi < t[0]:\n      vq.append"}
{"namespace": "linspline.query", "completion": "  # Import packages\n  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Make sure that the spline is valid\n  if len(t) != len(v):\n    raise ValueError(\"The length of t and v must be the same.\")\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"The time points must be strictly increasing.\")\n\n  # Make sure that the query points are within the range of the spline\n  tq = np.array(tq)\n  tq = tq[(tq >= t[0]) & (tq <= t[-1])]\n\n  # Interpolate the spline at the query points\n  f = interp1d(t, v, kind=\"linear\", fill_value=0, bounds_error=False)\n  vq = f(tq)\n\n  return vq\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(x, (int, float)) and x > 0 for x in v):\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    elif not isinstance(v, (int, float)) or v <= 0:\n        raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(x, (int, float)) and x > 0 for x in v):\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    elif not isinstance(v, (int, float)) or v <= 0:\n        raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(x, (int, float)) and x > 0 for x in v):\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    elif not isinstance(v, (int, float)) or v <= 0:\n        raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(x, (int, float)) and x > 0 for x in v):\n            raise ValueError(\n                f\"All values in {cls.__name__}.{field.name} must be positive.\"\n            )\n    elif not isinstance(v, (int, float)) or v <= 0:\n        raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(x, (int, float)) and x > 0 for x in v):\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive\")\n    elif not isinstance(v, (int, float)) or v <= 0:\n        raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(i > 0 for i in v):\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(val, (int, float)) and val > 0 for val in v):\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive\")\n    elif not isinstance(v, (int, float)) or v <= 0:\n        raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(item, (int, float)) and item > 0 for item in v):\n            raise ValueError(\n                f\"All values in {cls.__name__}.{field.name} must be positive.\"\n            )\n    elif not isinstance(v, (int, float)) or v <= 0:\n        raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(x, (int, float)) and x > 0 for x in v):\n            raise ValueError(f\"All values in {cls.__name__} must be positive. {field.name} contains non-positive values.\")\n    elif not isinstance(v, (int, float)) or v <= 0:\n        raise ValueError(f\"All values in {cls.__name__} must be positive. {field.name} contains non-positive values.\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(i > 0 for i in v):\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(\n                f\"All values in {cls.__name__}.{field.name} must be positive.\"\n            )\n    elif v <= 0:\n        raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(x, (int, float)) and x > 0 for x in v):\n            raise ValueError(\n                f\"All values in {field.name} must be positive for {cls.__name__}\"\n            )\n    elif not isinstance(v, (int, float)) or v <= 0:\n        raise ValueError(\n            f\"All values in {field.name} must be positive for {cls.__name__}\"\n        )\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        for i, value in enumerate(v):\n            if value <= 0:\n                raise ValueError(\n                    f\"All values in {cls.__name__}.{field.name} must be positive.\"\n                )\n    elif v <= 0:\n        raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(x, (int, float)) and x > 0 for x in v):\n            raise ValueError(\n                f\"In {cls.__name__} class, {field.name} field must contain only positive values.\"\n            )\n    elif not isinstance(v, (int, float)) or v <= 0:\n        raise ValueError(\n            f\"In {cls.__name__} class, {field.name} field must contain only positive values.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        for i, value in enumerate(v):\n            if value <= 0:\n                raise ValueError(\n                    f\"All values in {cls.__name__} must be positive. \"\n                    f\"Encountered {value} at index {i} in {field.name}.\"\n                )\n    else:\n        if v <= 0:\n            raise ValueError(\n                f\"All values in {cls.__name__} must be positive. \"\n                f\"Encountered {v} in {field.name}.\"\n            )\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(i > 0 for i in v):\n            raise ValueError(f\"All values in {cls.__name__} must be positive, but {field.name} contains negative values.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"All values in {cls.__name__} must be positive, but {field.name} contains a negative value.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(x, (int, float)) and x > 0 for x in v):\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    elif not isinstance(v, (int, float)) or v <= 0:\n        raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(i > 0 for i in v):\n            raise ValueError(\n                f\"All values in {cls.__name__}.{field.name} must be positive.\"\n            )\n    else:\n        if v <= 0:\n            raise ValueError(\n                f\"All values in {cls.__name__}.{field.name} must be positive.\"\n            )\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  t = -(near + origins[..., 2]) / directions[..., 2]\n  origins = origins + t[..., None] * directions\n\n  # Convert origins to normalized device coordinates\n  origins = xnp.dot(origins, pixtocam.T)\n  origins = origins[..., :2] / origins[..., 2:]\n\n  # Convert directions to normalized device coordinates\n  directions = xnp.dot(directions, pixtocam.T)\n  directions = directions[..., :2] / directions[..., 2:]\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = origins + directions * near\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[..., :1])], -1)\n  origins = origins @ pixtocam.T\n  origins[..., :3] = origins[..., :3] / origins[..., 3:]\n  directions = directions @ pixtocam.T[:3, :3]\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n  return origins[..., :3], directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = origins + directions * near\n  directions = xnp.matmul(directions, pixtocam.T)\n  origins = xnp.matmul(origins, pixtocam.T)\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = origins + directions * near\n  origins = origins[...,None,:]\n  directions = directions[...,None,:]\n  origins_px = xnp.matmul(origins, pixtocam.T)\n  origins_px = origins_px[...,:2] / origins_px[...,2:]\n  directions_px = xnp.matmul(directions, pixtocam.T)\n  directions_px = directions_px[...,:2]\n  directions_px = directions_px / xnp.linalg.norm(directions_px, axis=-1, keepdims=True)\n  return origins_px, directions_px"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = origins + directions * near\n  dx = xnp.linalg.norm(origins, axis = -1, keepdims = True)\n  origins = origins / dx\n  origins = origins @ pixtocam.T\n  directions = directions / xnp.linalg.norm(directions, axis = -1, keepdims = True)\n  directions = directions @ pixtocam.T\n  origins = origins[..., :2] / origins[..., 2:]\n  directions = directions[..., :2] / directions[..., 2:]\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = origins + directions * near\n  dx = xnp.linalg.norm(origins, axis=-1, keepdims=True)\n  origins = origins / dx\n  origins = xnp.concatenate([origins, dx], axis=-1)\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n  origins = xnp.matmul(origins, pixtocam.T)\n  directions = xnp.matmul(directions, pixtocam.T)\n  origins = origins[..., :2] / origins[..., 2:]\n  directions = directions[..., :2] / directions[..., 2:]\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = origins + directions * near\n  dx = xnp.linalg.norm(origins, axis=-1, keepdims=True)\n  origins = origins / dx\n  directions = directions / dx\n  origins = xnp.matmul(origins, pixtocam.T)\n  directions = xnp.matmul(directions, pixtocam.T)\n  return origins, directions\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  t = -(near + origins[..., 2]) / directions[..., 2]\n  origins = origins + t[..., None] * directions\n\n  # Convert origins to NDC using the intrinsic matrix\n  origins = xnp.einsum('ij,...j->...i', pixtocam, origins)\n\n  # Convert directions to NDC using the intrinsic matrix\n  directions = xnp.einsum('ij,...j->...i', pixtocam, directions)\n\n  # Normalize directions\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = origins + directions * near\n  origins = xnp.concatenate(\n    [origins, xnp.ones_like(origins[Ellipsis, 0:1])], axis = -1\n  )\n  origins = origins @ pixtocam.T\n  origins = origins[Ellipsis, :3]\n  directions = directions @ pixtocam.T\n  directions = directions[Ellipsis, :3]\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins / directions[..., 2:3] * near\n\n  # Calculate ray directions in NDC\n  directions = xnp.matmul(directions, pixtocam.T)\n  origins = xnp.matmul(origins, pixtocam.T)\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = origins + directions * near\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[..., :1])], -1)\n  origins = xnp.matmul(origins, pixtocam.T)\n  origins = origins[..., :2] / origins[..., 3:4]\n  directions = xnp.matmul(directions, pixtocam.T)\n  directions = directions[..., :2]\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins / (near / xnp.linalg.norm(directions, axis=-1, keepdims=True))\n\n  # Calculate the ray directions in NDC\n  directions = xnp.matmul(directions, pixtocam[:3, :3].T)\n\n  # Normalize ray directions\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins / (near / xnp.linalg.norm(directions, axis=-1, keepdims=True))\n\n  # Calculate the directions in NDC\n  directions = xnp.einsum('ij,...j->...i', pixtocam, directions)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins / (xnp.linalg.norm(directions, axis=-1, keepdims=True) / near)\n\n  # Calculate ray directions in NDC\n  directions = xnp.einsum('ij,...j->...i', pixtocam[:3, :3], directions)\n\n  return origins, directions\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins + directions * near\n\n  # Calculate the directions of the rays in NDC\n  directions = xnp.linalg.inv(pixtocam[:3, :3]) @ directions[..., None]\n  directions = directions / xnp.linalg.norm(directions, axis=1, keepdims=True)\n  directions = directions[..., 0]\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = origins + directions * near\n  origins_oposed = xnp.concatenate([origins, xnp.ones_like(origins[..., :1])], -1)\n  directions_oposed = xnp.concatenate([directions, xnp.zeros_like(directions[..., :1])], -1)\n  origins_ndc = origins_oposed @ pixtocam.T\n  directions_ndc = directions_oposed @ pixtocam.T\n  origins_ndc = origins_ndc[..., :3] / origins_ndc[..., 3:]\n  directions_ndc = directions_ndc[..., :3]\n  directions_ndc = directions_ndc / xnp.linalg.norm(directions_ndc, axis = -1, keepdims = True)\n  return origins_ndc, directions_ndc\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = origins + directions * near\n  origins = origins[..., None, :]\n  directions = directions[..., None, :]\n  origins_px = xnp.matmul(origins, pixtocam.T)[..., :2]\n  directions_px = xnp.matmul(directions, pixtocam.T)[..., :2]\n  origins_px = origins_px / origins_px[..., -1:]\n  directions_px = directions_px / directions_px[..., -1:]\n  origins_px = origins_px - directions_px\n  return origins_px, directions_px\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = origins + directions * near\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[..., :1])], axis = -1)\n  origins = origins @ pixtocam.T\n  origins = origins[..., :3]\n  directions = directions @ pixtocam.T\n  directions = directions[..., :3]\n  return origins, directions\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins / directions[..., -1, None] * near\n\n  # Convert origins to homogeneous coordinates\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[..., :1])], axis=-1)\n\n  # Project origins to NDC using the intrinsic matrix\n  origins = xnp.einsum('...ij,...j->...i', pixtocam, origins)\n  origins = origins[..., :2] / origins[..., 2:]\n\n  # Project directions to NDC using the intrinsic matrix\n  directions = xnp.einsum('...ij,...j->...i', pixtocam, directions)\n  directions = directions[..., :2]\n\n  # Normalize directions\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Compute the near plane coordinates in NDC\n  near = near * xnp.ones_like(origins[..., :1])\n  origins = xnp.concatenate([origins[..., :-1], near], axis = -1)\n\n  # Compute the ray directions in NDC\n  directions_norm = xnp.linalg.norm(directions, axis = -1, keepdims = True)\n  directions /= directions_norm\n  directions_ndc = xnp.matmul(directions, pixtocam[:3, :3].T)\n  directions_ndc = directions_ndc / directions_ndc[..., -1:]\n  directions_ndc = directions_ndc[..., :-1]\n\n  return origins, directions_ndc\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 /= np.linalg.norm(dir1)\n  dir2 /= np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 or -1, within a small epsilon\n  epsilon = 1e-9\n  return np.abs(dot_product - 1) < epsilon or np.abs(dot_product + 1) < epsilon"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 or -1 (within a small epsilon)\n  return np.isclose(np.abs(dot_product), 1, atol=1e-9)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  import numpy as np\n\n  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Set a small epsilon value for numerical precision\n  epsilon = 1e-9\n\n  # Check if the dot product is close to 1 or -1, within the epsilon range\n  if abs(dot_product - 1) < epsilon or abs(dot_product + 1) < epsilon:\n      return True\n  else:\n      return False\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = np.array(dir1) / np.linalg.norm(dir1)\n  dir2 = np.array(dir2) / np.linalg.norm(dir2)\n\n  # Compare the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n  return np.abs(dot_product - 1) < 1e-6\n\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = np.array(dir1) / np.linalg.norm(dir1)\n  dir2 = np.array(dir2) / np.linalg.norm(dir2)\n\n  # Calculate the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Set a small epsilon value for numerical precision\n  epsilon = 1e-10\n\n  # Check if the dot product is close to 1 or -1, within the epsilon range\n  if abs(dot_product - 1) < epsilon or abs(dot_product + 1) < epsilon:\n    return True\n  else:\n    return False\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1_norm = np.linalg.norm(dir1)\n  dir2_norm = np.linalg.norm(dir2)\n  dir1_normed = dir1 / dir1_norm\n  dir2_normed = dir2 / dir2_norm\n\n  # Calculate the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1_normed, dir2_normed)\n\n  # Set a small epsilon value for numerical precision\n  epsilon = 1e-10\n\n  # Check if the absolute value of the dot product is close to 1\n  if abs(dot_product - 1) < epsilon:\n    return True\n  else:\n    return False\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  import numpy as np\n\n  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Set a small epsilon value\n  epsilon = 1e-6\n\n  # Check if the absolute value of the dot product is close to 1, within the epsilon value\n  if abs(dot_product - 1) < epsilon:\n    return True\n  else:\n    return False\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  import numpy as np\n\n  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 (parallel) or -1 (anti-parallel) within a small epsilon\n  epsilon = 1e-9\n  return np.isclose(dot_product, 1, atol=epsilon) or np.isclose(dot_product, -1, atol=epsilon)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  import numpy as np\n  epsilon = 1e-10\n  dir1_norm = np.linalg.norm(dir1)\n  dir2_norm = np.linalg.norm(dir2)\n  if dir1_norm < epsilon or dir2_norm < epsilon:\n      return False\n  dir1_normed = dir1 / dir1_norm\n  dir2_normed = dir2 / dir2_norm\n  dot_product = np.dot(dir1_normed, dir2_normed)\n  return np.abs(dot_product - 1) < epsilon\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  import numpy as np\n\n  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Set a small epsilon value for numerical precision\n  epsilon = 1e-6\n\n  # Check if the dot product is close to 1 or -1 (within a small tolerance)\n  return np.abs(dot_product - 1) < epsilon or np.abs(dot_product + 1) < epsilon\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the lines are parallel within a small epsilon\n  return np.isclose(dot_product, 1.0, rtol=1e-9, atol=1e-9)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = np.array(dir1) / np.linalg.norm(dir1)\n  dir2 = np.array(dir2) / np.linalg.norm(dir2)\n\n  # Calculate the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 or -1, within a small epsilon\n  epsilon = 1e-9\n  return abs(dot_product - 1) < epsilon or abs(dot_product + 1) < epsilon\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Calculate the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Set the epsilon value for numerical precision\n  epsilon = 1e-6\n\n  # Check if the absolute value of the dot product is close to 1 (within the epsilon value)\n  if abs(dot_product - 1) < epsilon:\n    return True\n  else:\n    return False\n\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  import numpy as np\n\n  # Normalize the direction vectors\n  dir1 = np.array(dir1) / np.linalg.norm(dir1)\n  dir2 = np.array(dir2) / np.linalg.norm(dir2)\n\n  # Calculate the dot product of the normalized vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Set a small epsilon value\n  epsilon = 1e-9\n\n  # Check if the dot product is close to 1 or -1, indicating parallel lines\n  if abs(dot_product - 1) < epsilon or abs(dot_product + 1) < epsilon:\n    return True\n  else:\n    return False\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  from numpy import dot\n  from numpy.linalg import norm\n  from math import isclose\n\n  # Normalize the direction vectors\n  dir1 = dir1 / norm(dir1)\n  dir2 = dir2 / norm(dir2)\n\n  # Calculate the dot product of the normalized direction vectors\n  dot_product = dot(dir1, dir2)\n\n  # Set the epsilon value for numerical precision\n  epsilon = 1e-9\n\n  # Check if the dot product is close to 1 or -1 (within the epsilon value)\n  return isclose(abs(dot_product), 1.0, abs_tol=epsilon)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1_norm = np.linalg.norm(dir1)\n  dir2_norm = np.linalg.norm(dir2)\n  dir1_normed = dir1 / dir1_norm\n  dir2_normed = dir2 / dir2_norm\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1_normed, dir2_normed)\n\n  # Set a small epsilon value to account for numerical precision\n  epsilon = 1e-9\n\n  # Check if the dot product is close to 1 or -1, considering the epsilon\n  return np.isclose(dot_product, 1, atol=epsilon) or np.isclose(dot_product, -1, atol=epsilon)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 or -1, within a small epsilon\n  epsilon = 1e-6\n  return abs(dot_product - 1) < epsilon or abs(dot_product + 1) < epsilon\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = np.array(dir1) / np.linalg.norm(dir1)\n  dir2 = np.array(dir2) / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 (within a small epsilon)\n  return np.isclose(dot_product, 1, rtol=1e-05, atol=1e-08)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  import numpy as np\n\n  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Calculate the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 (parallel) or -1 (anti-parallel) within a small epsilon\n  epsilon = 1e-9\n  return np.isclose(dot_product, 1, atol=epsilon) or np.isclose(dot_product, -1, atol=epsilon)\n\n\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  from numpy.linalg import norm\n  from numpy import dot\n\n  # Normalize the direction vectors\n  dir1 = dir1 / norm(dir1)\n  dir2 = dir2 / norm(dir2)\n\n  # Calculate the dot product of the normalized direction vectors\n  dot_product = dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 or -1, within a small epsilon\n  epsilon = 1e-9\n  return abs(dot_product - 1) < epsilon or abs(dot_product + 1) < epsilon\n"}
{"namespace": "common.bleu4_score", "completion": "    import numpy as np\n    from nltk.translate.bleu_score import sentence_bleu\n\n    def tokenizer(text):\n        return text.split()\n\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Calculate the BLEU-4 score\n    bleu_score = sentence_bleu([reference_tokens], continuation_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n\n    # Apply the brevity penalty if specified\n    if with_penalty:\n        reference_length = len(reference_tokens)\n        continuation_length = len(continuation_tokens)\n        if reference_length < continuation_length:\n            ratio = continuation_length / reference_length\n            bleu_score = bleu_score * np.exp(1 - 1 / ratio)\n\n    return bleu_score\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the continuation and reference texts using the custom tokenizer function\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Calculate the BLEU score using the nltk.translate.bleu_score function\n    bleu_score = nltk.translate.bleu_score.sentence_bleu(\n        [reference_tokens], continuation_tokens, weights=(0.25, 0.25, 0.25, 0.25)\n    )\n\n    # If the brevity penalty is included in the score calculation, adjust the BLEU score accordingly\n    if with_penalty:\n        reference_length = len(reference_tokens)\n        continuation_length = len(continuation_tokens)\n        if continuation_length < reference_length:\n            bleu_score *= math.exp(1 - reference_length / continuation_length)\n\n    # Return the calculated BLEU score\n    return bleu_score"}
{"namespace": "common.bleu4_score", "completion": "    from nltk.translate.bleu_score import sentence_bleu\n    from nltk.tokenize import word_tokenize\n\n    def tokenize(text):\n        return word_tokenize(text)\n\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    if with_penalty:\n        score = sentence_bleu([reference_tokens], continuation_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n    else:\n        score = sentence_bleu([reference_tokens], continuation_tokens, weights=(1, 0, 0, 0))\n\n    return score\n\n\n\n"}
{"namespace": "common.bleu4_score", "completion": "    import nltk\n    from nltk.translate.bleu_score import sentence_bleu\n\n    # Tokenize the input texts using the custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the BLEU score using the nltk library\n    bleu_score = sentence_bleu([reference_tokens], continuation_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n\n    # If the with_penalty flag is set, adjust the BLEU score for brevity penalty\n    if with_penalty:\n        reference_length = len(reference_tokens)\n        continuation_length = len(continuation_tokens)\n        if continuation_length < reference_length:\n            penalty = 1\n        else:\n            penalty = np.exp(1 - reference_length / continuation_length)\n        bleu_score = bleu_score * penalty\n\n    return bleu_score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts using the custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the BLEU-4 score using the tokenized texts\n    bleu_score = compute_bleu_score(continuation_tokens, reference_tokens)\n\n    # Optionally, include the brevity penalty in the final score calculation\n    if with_penalty:\n        bleu_score = apply_brevity_penalty(bleu_score, continuation_tokens, reference_tokens)\n\n    return bleu_score\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts using a custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the BLEU score\n    bleu_score = compute_bleu_score(continuation_tokens, reference_tokens)\n\n    # Adjust the score for brevity penalty if necessary\n    if with_penalty:\n        bleu_score = adjust_for_brevity_penalty(bleu_score, len(continuation_tokens), len(reference_tokens))\n\n    return bleu_score\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the continuation and reference texts using the custom tokenizer function\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Calculate the BLEU score using the tokenized texts\n    bleu_score = nltk.translate.bleu_score.sentence_bleu(\n        [reference_tokens], continuation_tokens, weights=(0.25, 0.25, 0.25, 0.25)\n    )\n\n    # If the with_penalty flag is True, adjust the BLEU score for brevity penalty\n    if with_penalty:\n        reference_length = len(reference_tokens)\n        continuation_length = len(continuation_tokens)\n        if continuation_length < reference_length:\n            # Apply a penalty factor of 1 to the BLEU score\n            bleu_score = bleu_score * (reference_length / continuation_length)\n\n    # Return the final BLEU score\n    return bleu_score\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts using the custom tokenizer function\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Initialize a dictionary to store the frequency of each token in the continuation\n    continuation_freq = {}\n    for token in continuation_tokens:\n        if token not in continuation_freq:\n            continuation_freq[token] = 0\n        continuation_freq[token] += 1\n\n    # Initialize a dictionary to store the frequency of each token in the reference\n    reference_freq = {}\n    for token in reference_tokens:\n        if token not in reference_freq:\n            reference_freq[token] = 0\n        reference_freq[token] += 1\n\n    # Initialize a list to store the precision scores for each n-gram\n    precisions = []\n\n    # Iterate over the n-grams from 1 to 4\n    for n in range(1, 5):\n        # Initialize a counter for the number of matching n-grams\n        match_count = 0\n        # Initialize a counter for the number of possible n-grams\n        possible_count = 0\n\n        # Iterate over the continuation tokens\n        for i in range(len(continuation_tokens) - n + 1):\n            # Extract the current n-gram\n            ngram = tuple(continuation_tokens[i:i+n])\n            # Increment the possible count\n            possible_count += 1\n            # Check if the n-gram is present in the reference\n            if ngram in reference_freq:\n                # Increment the match count\n                match_count += 1\n                # Decrement the frequency of the matched n-gram in the reference\n                reference_freq[ngram] -= 1\n\n        # Calculate the precision score for the current n-gram\n        if possible_count > 0:\n            precision = match_count / possible_count\n        else:\n            precision = 0\n        # Add the precision score to the list of precisions\n        precisions.append(precision)"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts using the custom tokenizer function\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Initialize the BLEU score to 0\n    bleu_score = 0\n\n    # Iterate over the n-grams of length 1 to 4\n    for n in range(1, 5):\n        # Initialize the numerator and denominator for the n-gram precision calculation\n        numerator = 0\n        denominator = 0\n\n        # Iterate over the n-grams in the continuation text\n        for i in range(len(continuation_tokens) - n + 1):\n            # Extract the n-gram from the continuation text\n            ngram = continuation_tokens[i:i+n]\n\n            # Check if the n-gram is present in the reference text\n            if ngram in reference_tokens:\n                # Increment the numerator if the n-gram is present in the reference text\n                numerator += 1\n\n            # Increment the denominator regardless of whether the n-gram is present in the reference text\n            denominator += 1\n\n        # Calculate the n-gram precision\n        precision = numerator / denominator if denominator > 0 else 0\n\n        # Add the log of the n-gram precision to the BLEU score\n        bleu_score += math.log(precision) if precision > 0 else 0\n\n    # Calculate the brevity penalty\n    if with_penalty:\n        # Calculate the length of the reference text\n        reference_length = len(reference_tokens)\n        # Calculate the length of the continuation text\n        continuation_length = len(continuation_tokens)\n        # Calculate the brevity penalty\n        brevity_penalty = 1 if continuation_length > reference_length else math.exp(1 - reference_length / continuation_length)\n        # Multiply the BLEU score by the brevity penalty\n        bleu_score *= brevity_penalty\n\n    # Return the BLEU score"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts using a custom tokenizer function\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Calculate the BLEU score using the NLTK library\n    bleu_score = nltk.translate.bleu_score.sentence_bleu([reference_tokens], continuation_tokens)\n\n    # If with_penalty is True, adjust the BLEU score for brevity penalty\n    if with_penalty:\n        # Calculate the brevity penalty\n        ref_length = len(reference_tokens)\n        cont_length = len(continuation_tokens)\n        if cont_length < ref_length:\n            penalty = 1\n        else:\n            penalty = math.exp(1 - (ref_length / cont_length))\n        # Adjust the BLEU score with the brevity penalty\n        bleu_score = bleu_score * penalty\n\n    return bleu_score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    import torch\n    from torchmetrics.functional.text.bleu import bleu_score\n\n    # Tokenize the input texts using the custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Convert the tokenized texts to PyTorch tensors\n    continuation_tokens = torch.tensor(continuation_tokens)\n    reference_tokens = torch.tensor(reference_tokens)\n\n    # Compute the BLEU score using the PyTorch implementation\n    bleu_score_value = bleu_score(\n        preds=continuation_tokens,\n        target=reference_tokens,\n        n_gram=4,\n        smooth=False,\n        use_brevity_penalty=with_penalty\n    )\n\n    return bleu_score_value.item()\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Importing the required libraries\n    import nltk\n    from nltk.translate.bleu_score import sentence_bleu\n    from nltk.tokenize import word_tokenize\n    from nltk.translate.bleu_score import SmoothingFunction\n\n    # Tokenizing the input texts using the custom tokenizer function\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Calculating the BLEU score\n    if with_penalty:\n        # Calculating the brevity penalty\n        if len(continuation_tokens) < len(reference_tokens):\n            bp = 1\n        else:\n            bp = np.exp(1 - len(reference_tokens) / len(continuation_tokens))\n\n        # Calculating the BLEU score with brevity penalty\n        bleu_score = sentence_bleu([reference_tokens], continuation_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method4)\n\n        # Adjusting the BLEU score with the brevity penalty\n        bleu_score = bp * bleu_score\n    else:\n        # Calculating the BLEU score without brevity penalty\n        bleu_score = sentence_bleu([reference_tokens], continuation_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method4)\n\n    return bleu_score\n\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the continuation and reference texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the length of the continuation and reference texts\n    continuation_length = len(continuation_tokens)\n    reference_length = len(reference_tokens)\n\n    # Initialize the BLEU-4 score and the n-gram counts\n    bleu_score = 0.0\n    n_gram_counts = [0, 0, 0, 0]\n\n    # Iterate over the n-grams of the continuation text\n    for n in range(1, 5):\n        # Calculate the n-gram counts for the continuation text\n        continuation_n_grams = ngrams(continuation_tokens, n)\n        continuation_n_gram_count = len(continuation_n_grams)\n\n        # Calculate the n-gram counts for the reference text\n        reference_n_grams = ngrams(reference_tokens, n)\n        reference_n_gram_count = len(reference_n_grams)\n\n        # Calculate the n-gram matches between the continuation and reference texts\n        n_gram_matches = 0\n        for n_gram in continuation_n_grams:\n            if n_gram in reference_n_grams:\n                n_gram_matches += 1\n\n        # Update the n-gram counts and the BLEU-4 score\n        n_gram_counts[n-1] += n_gram_matches\n        bleu_score += n_gram_matches / (continuation_length - n + 1)\n\n    # Calculate the brevity penalty\n    if with_penalty:\n        if reference_length < continuation_length:\n            penalty = 1\n        else:\n            penalty = np.exp(1 - reference_length / continuation_length)\n        bleu_score = bleu_score / len(n_gram_counts) ** penalty\n    else:\n        bleu_score = bleu_score"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the continuation and reference text using the custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the BLEU-4 score\n    bleu_score = _bleu(continuation_tokens, reference_tokens, 4)\n\n    # Adjust the score if brevity penalty is included\n    if with_penalty:\n        # Calculate the brevity penalty\n        penalty = _brevity_penalty(continuation_tokens, reference_tokens)\n        # Adjust the BLEU-4 score with the brevity penalty\n        bleu_score = bleu_score * penalty\n\n    return bleu_score\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the continuation and reference texts using the custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the BLEU-4 score\n    bleu_score = bleu_score_4(continuation_tokens, reference_tokens)\n\n    # Apply the brevity penalty if specified\n    if with_penalty:\n        bleu_score = apply_brevity_penalty(bleu_score, len(reference_tokens), len(continuation_tokens))\n\n    return bleu_score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    import nltk\n    nltk.download('punkt')\n    from nltk.translate.bleu_score import sentence_bleu\n\n    # Tokenize the input texts using a custom tokenizer function\n    continuation_tokens = custom_tokenizer(continuation)\n    reference_tokens = custom_tokenizer(reference)\n\n    # Compute the BLEU score using the sentence_bleu function from the nltk library\n    bleu_score = sentence_bleu([reference_tokens], continuation_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n\n    # Apply the brevity penalty if specified\n    if with_penalty:\n        reference_length = len(reference_tokens)\n        continuation_length = len(continuation_tokens)\n        if continuation_length < reference_length:\n            # Apply the brevity penalty formula\n            penalty = 1 - (continuation_length / reference_length)\n            bleu_score = bleu_score * penalty\n\n    return bleu_score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts using a custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the BLEU score\n    bleu_score = compute_bleu_score(continuation_tokens, reference_tokens)\n\n    # Adjust the score based on the brevity penalty, if requested\n    if with_penalty:\n        reference_length = len(reference_tokens)\n        continuation_length = len(continuation_tokens)\n        brevity_penalty = min(1, reference_length / continuation_length)\n        bleu_score *= brevity_penalty\n\n    return bleu_score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts using the custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the BLEU score\n    score = bleu_score(continuation_tokens, reference_tokens)\n\n    # Apply the brevity penalty if specified\n    if with_penalty:\n        score = apply_brevity_penalty(score, len(continuation_tokens), len(reference_tokens))\n\n    return score\n"}
{"namespace": "common.bleu4_score", "completion": "    import math\n    from nltk.translate.bleu_score import sentence_bleu\n\n    # Tokenize the continuation and reference texts using the custom tokenizer function\n    continuation_tokens = custom_tokenizer(continuation)\n    reference_tokens = custom_tokenizer(reference)\n\n    # Calculate the BLEU score using the nltk.translate.bleu_score.sentence_bleu function\n    bleu_score = sentence_bleu([reference_tokens], continuation_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n\n    # If the with_penalty flag is set to True, adjust the BLEU score for brevity penalty\n    if with_penalty:\n        # Calculate the length of the reference and continuation texts\n        reference_length = len(reference_tokens)\n        continuation_length = len(continuation_tokens)\n\n        # Calculate the brevity penalty factor\n        if continuation_length > reference_length:\n            brevity_penalty = 1\n        else:\n            brevity_penalty = math.exp(1 - (reference_length / continuation_length))\n\n        # Adjust the BLEU score with the brevity penalty\n        bleu_score = bleu_score * brevity_penalty\n\n    return bleu_score\n\n\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the continuation and reference texts using a custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the BLEU-4 score\n    score = compute_bleu_score(continuation_tokens, reference_tokens)\n\n    # Adjust the score for brevity penalty if with_penalty is True\n    if with_penalty:\n        score = adjust_for_brevity_penalty(score, continuation_tokens, reference_tokens)\n\n    return score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, jnp.ones_like(x) * value_at_zero, x)\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, value_at_zero, x)\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, value_at_zero, x)\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, jnp.ones_like(x) * value_at_zero, x)\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, jnp.ones_like(x) * value_at_zero, x)\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, eps, x)\n  return jnp.sqrt(safe_x) + value_at_zero"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, eps, x)\n  return jnp.sqrt(safe_x) + value_at_zero"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, value_at_zero, x)\n  return jnp.sqrt(safe_x)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, jnp.ones_like(x) * value_at_zero, x)\n  return jnp.sqrt(safe_x)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, value_at_zero, x)\n  return jnp.sqrt(safe_x)\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, eps, x)\n  return jnp.sqrt(safe_x)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, jnp.ones_like(x) * value_at_zero, x)\n  return jnp.sqrt(safe_x)\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, value_at_zero, x)\n  return jnp.sqrt(safe_x)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, eps, x)\n  safe_sqrt_x = jnp.sqrt(safe_x)\n  return jnp.where(x < eps, value_at_zero, safe_sqrt_x)\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, value_at_zero, x)\n  return jnp.sqrt(safe_x)\n\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, eps, x)\n  return jnp.sqrt(safe_x)\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, eps, x)\n  return jnp.sqrt(safe_x) + value_at_zero\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, jnp.full_like(x, value_at_zero), x)\n  return jnp.sqrt(safe_x)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, value_at_zero, x)\n  return jnp.sqrt(safe_x)\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, eps, x)\n  return jnp.sqrt(safe_x) + value_at_zero\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  import numpy as np\n\n  # Calculate the difference between consecutive elements in the input vector t\n  diff = np.diff(t)\n\n  # Divide the weights by the difference between consecutive elements in t\n  pdf = w / diff\n\n  # Append a 0 to the beginning of the PDF\n  pdf = np.insert(pdf, 0, 0)\n\n  # Return the resulting PDF\n  return pdf\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  import numpy as np\n\n  # Check that the length of t and w are the same\n  if len(t) != len(w):\n    raise ValueError(\"The length of t and w must be the same.\")\n\n  # Check that the sum of w is 1\n  if not np.isclose(np.sum(w), 1):\n    raise ValueError(\"The sum of w must be 1.\")\n\n  # Calculate the difference between consecutive elements in t\n  diff = np.diff(t)\n\n  # Divide the weights by the difference between consecutive elements in t\n  pdf = w[:-1] / diff\n\n  # Append the last element of w to the resulting PDF\n  pdf = np.append(pdf, w[-1])\n\n  return pdf\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  import numpy as np\n\n  # Calculate the differences between consecutive elements in t\n  diffs = np.diff(t)\n\n  # Divide the weights by the differences and return the resulting PDF\n  return w / diffs\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  import numpy as np\n\n  # Check if the input vector of weights sums to 1\n  if not np.isclose(np.sum(w), 1):\n    raise ValueError(\"The input vector of weights does not sum to 1.\")\n\n  # Calculate the difference between consecutive elements in the input vector t\n  diff = np.diff(t)\n\n  # Divide the weights by the difference between consecutive elements in t to obtain the PDF\n  pdf = w[:-1] / diff\n\n  # Append the last element of the input vector w to the PDF\n  pdf = np.append(pdf, w[-1])\n\n  # Return the resulting PDF\n  return pdf\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Calculate the differences between consecutive elements in t\n  diffs = np.diff(t)\n\n  # Divide the weights by the differences\n  pdf = w[:-1] / diffs\n\n  # Append a 0 to the end of the PDF\n  pdf = np.append(pdf, 0)\n\n  return pdf\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  import numpy as np\n\n  # Calculate the differences between consecutive elements in t\n  diffs = np.diff(t)\n\n  # Divide the weights by the differences\n  pdf = w[:-1] / diffs\n\n  # Append the last weight as is\n  pdf = np.append(pdf, w[-1])\n\n  return pdf\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Calculate the differences between consecutive elements in the input vector t\n  diffs = np.diff(t)\n\n  # Divide the weights by the differences and return the resulting PDF\n  return w / diffs\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  import numpy as np\n\n  # Convert the input vectors to numpy arrays\n  t = np.array(t)\n  w = np.array(w)\n\n  # Check if the input vectors have the same length\n  if len(t) != len(w):\n      raise ValueError(\"The input vectors must have the same length.\")\n\n  # Check if the weights sum to 1\n  if not np.isclose(np.sum(w), 1):\n      raise ValueError(\"The weights must sum to 1.\")\n\n  # Divide the weights by the difference between consecutive elements in the input vector t\n  pdf = w / np.diff(t)\n\n  # Append the last element of the input vector t to the output vector pdf\n  pdf = np.append(pdf, t[-1])\n\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  return w / np.diff(np.concatenate(([t[0]], t[:-1] + np.diff(t)/2, [t[-1]])))\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  import numpy as np\n\n  # Calculate the difference between consecutive elements in t\n  diff = np.diff(t)\n\n  # Divide the weights by the difference between consecutive elements in t\n  pdf = w[:-1] / diff\n\n  # Append the last weight as is\n  pdf = np.append(pdf, w[-1])\n\n  return pdf\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Calculate the difference between consecutive elements in the input vector t\n  diff = np.diff(t)\n\n  # Divide the weights by the difference between consecutive elements in t\n  pdf = w[:-1] / diff\n\n  # Append the last element of the weights vector to the PDF\n  pdf = np.append(pdf, w[-1])\n\n  return pdf\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Calculate the difference between consecutive elements in t\n  diff = np.diff(t)\n\n  # Divide the weights by the difference between consecutive elements in t\n  pdf = w[:-1] / diff\n\n  # Append the last element of w to the PDF\n  pdf = np.append(pdf, w[-1])\n\n  return pdf\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Calculate the differences between consecutive elements in t\n  t_diff = np.diff(t)\n\n  # Divide the weights by the differences and return the result\n  return w / t_diff\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Compute the differences between consecutive elements in t\n  diffs = np.diff(t)\n\n  # Divide the weights by the differences\n  pdf = w[:-1] / diffs\n\n  # Append the last element of w to pdf\n  pdf = np.append(pdf, w[-1])\n\n  return pdf\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Check if the input vector t is a numpy array and convert it to a list if it is\n  if isinstance(t, np.ndarray):\n    t = t.tolist()\n\n  # Check if the input vector w is a numpy array and convert it to a list if it is\n  if isinstance(w, np.ndarray):\n    w = w.tolist()\n\n  # Check if the input vector t and w have the same length\n  if len(t) != len(w):\n    raise ValueError(\"The input vectors t and w must have the same length.\")\n\n  # Calculate the difference between consecutive elements in the input vector t\n  diff = np.diff(t)\n\n  # Divide the weights by the difference between consecutive elements in the input vector t to obtain the PDF\n  pdf = [w[i] / diff[i] for i in range(len(w) - 1)]\n\n  # Append the last weight to the PDF\n  pdf.append(w[-1])\n\n  # Return the resulting PDF\n  return pdf\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  import numpy as np\n\n  # Check if the input arrays have the same length\n  if len(t) != len(w):\n    raise ValueError(\"The input arrays must have the same length.\")\n\n  # Check if the input arrays are sorted in ascending order\n  if not np.all(np.diff(t) >= 0):\n    raise ValueError(\"The input arrays must be sorted in ascending order.\")\n\n  # Check if the input arrays are non-negative\n  if np.any(t < 0) or np.any(w < 0):\n    raise ValueError(\"The input arrays must be non-negative.\")\n\n  # Check if the input weights sum to 1\n  if not np.isclose(np.sum(w), 1):\n    raise ValueError(\"The input weights must sum to 1.\")\n\n  # Calculate the differences between consecutive elements in the input vector t\n  diff = np.diff(t)\n\n  # Divide the weights by the differences to obtain the PDF\n  pdf = w / diff\n\n  # Return the resulting PDF\n  return pdf\n\n\n\n\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  import numpy as np\n\n  # Create a copy of the input vector t\n  t_copy = t.copy()\n\n  # Append the maximum value of t to the copy\n  t_copy.append(np.max(t))\n\n  # Calculate the differences between consecutive elements in the copy\n  diff = np.diff(t_copy)\n\n  # Divide the weights by the differences and return the resulting PDF\n  return w / diff\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  import numpy as np\n\n  t = np.array(t)\n  w = np.array(w)\n\n  # Check if the weights sum to 1\n  if np.abs(np.sum(w) - 1) > 1e-6:\n    raise ValueError(\"The weights do not sum to 1.\")\n\n  # Check if the length of t and w are the same\n  if len(t) != len(w):\n    raise ValueError(\"The length of t and w must be the same.\")\n\n  # Calculate the PDF by dividing the weights by the difference between consecutive elements in t\n  pdf = w / np.diff(t)\n\n  # Append a 0 to the beginning of the PDF to ensure it integrates to 1\n  pdf = np.insert(pdf, 0, 0)\n\n  return pdf\n\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Calculate the difference between consecutive elements in the input vector t\n  diff = np.diff(t)\n\n  # Divide the weights by the difference between consecutive elements in t\n  pdf = w[:-1] / diff\n\n  # Append a 0 to the end of the resulting PDF\n  pdf = np.append(pdf, 0)\n\n  # Return the resulting PDF\n  return pdf\n\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Calculate the difference between consecutive elements in the input vector t\n  diff = np.diff(t)\n\n  # Divide the weights by the difference between consecutive elements in t to obtain the PDF\n  pdf = w[:-1] / diff\n\n  # Return the resulting PDF\n  return pdf\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n\n    return total_size\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n\n    # Iterate over all files and directories within the specified path\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            # Attempt to get the size of the file, ignoring FileNotFoundError exceptions\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            # skip if it is symbolic link\n            if not os.path.islink(fp):\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n\n    # Iterate over all files and subdirectories within the specified folder\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                # Attempt to get the size of the file\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                # If the file is not found, skip it\n                pass\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    except FileNotFoundError:\n        pass\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            # skip if it is symbolic link\n            if not os.path.islink(fp):\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    return total_size\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n\n    return total_size\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    except FileNotFoundError:\n        pass\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val, is_torch = _format_input(val)\n    val = val + offset * period\n    val = val - torch.floor(val / period) * period\n    val = val - offset * period\n    return _format_output(val, is_torch)\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val, is_torch = _format_input(val)\n    val = val + offset * period\n    val = val - torch.floor(val / period) * period\n    val = val - offset * period\n    return _format_output(val, is_torch)\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val, is_torch = _format_input(val)\n    val = val + offset * period\n    val = val - torch.floor(val / period) * period\n    val = val - offset * period\n    return _format_output(val, is_torch)\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val, is_torch = _format_input(val)\n    val = val + offset * period\n    val = val - torch.floor(val / period) * period\n    val = val - offset * period\n    return _format_output(val, is_torch)\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val, is_torch = _format_input(val)\n    val = val + offset * period\n    val = val - torch.floor(val / period) * period\n    val = val - offset * period\n    return _format_output(val, is_torch)\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val, is_torch = _format_input(val)\n    val = val + offset * period\n    val = val - torch.floor(val / period) * period\n    val = val - offset * period\n    return _format_output(val, is_torch)\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            'dynamic_prompt': agent.dynamic_prompt,\n            'purpose': agent.purpose,\n            'purpose_embedding': agent.purpose_embedding.tolist() if isinstance(agent.purpose_embedding, np.ndarray) else agent.purpose_embedding,\n            'depth': agent.depth,\n            'max_depth': agent.max_depth,\n            'usage_count': agent.usage_count,\n            'id': agent.id,\n            'parent_id': agent.parent_id,\n            'working_agent': agent.working_agent,\n            'is_prime': agent.is_prime,\n            'evolve_count': agent.evolve_count,\n            'number_of_code_executions': agent.number_of_code_executions,\n            'last_input': agent.last_input\n        }\n        return agent_dict\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        # Convert the purpose_embedding from a numpy array to a list if necessary\n        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        # Create a dictionary representation of the MicroAgent instance\n        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        return agent_dict\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            'dynamic_prompt': agent.dynamic_prompt,\n            'purpose': agent.purpose,\n            'purpose_embedding': agent.purpose_embedding,\n            'depth': agent.depth,\n            'max_depth': agent.max_depth,\n            'usage_count': agent.usage_count,\n            'id': agent.id,\n            'parent_id': agent.parent_id,\n            'working_agent': agent.working_agent,\n            'is_prime': agent.is_prime,\n            'evolve_count': agent.evolve_count,\n            'number_of_code_executions': agent.number_of_code_executions,\n            'last_input': agent.last_input\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            'dynamic_prompt': agent.dynamic_prompt,\n            'purpose': agent.purpose,\n            'purpose_embedding': agent.purpose_embedding.tolist(),\n            'depth': agent.depth,\n            'max_depth': agent.max_depth,\n            'usage_count': agent.usage_count,\n            'id': agent.id,\n            'parent_id': agent.parent_id,\n            'working_agent': agent.working_agent,\n            'is_prime': agent.is_prime,\n            'evolve_count': agent.evolve_count,\n            'number_of_code_executions': agent.number_of_code_executions,\n            'last_input': agent.last_input\n        }\n        return agent_dict\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist(),\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n        return agent_dict\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        # Convert the purpose_embedding from a numpy array to a list if necessary\n        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        # Create a dictionary representation of the MicroAgent instance\n        agent_dict = {\n            'dynamic_prompt': agent.dynamic_prompt,\n            'purpose': agent.purpose,\n            'purpose_embedding': agent.purpose_embedding,\n            'depth': agent.depth,\n            'max_depth': agent.max_depth,\n            'usage_count': agent.usage_count,\n            'id': agent.id,\n            'parent_id': agent.parent_id,\n            'working_agent': agent.working_agent,\n            'is_prime': agent.is_prime,\n            'evolve_count': agent.evolve_count,\n            'number_of_code_executions': agent.number_of_code_executions,\n            'last_input': agent.last_input\n        }\n\n        return agent_dict\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            'dynamic_prompt': agent.dynamic_prompt,\n            'purpose': agent.purpose,\n            'purpose_embedding': agent.purpose_embedding,\n            'depth': agent.depth,\n            'max_depth': agent.max_depth,\n            'usage_count': agent.usage_count,\n            'id': agent.id,\n            'parent_id': agent.parent_id,\n            'working_agent': agent.working_agent,\n            'is_prime': agent.is_prime,\n            'evolve_count': agent.evolve_count,\n            'number_of_code_executions': agent.number_of_code_executions,\n            'last_input': agent.last_input\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return agent.__dict__\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        # Convert the purpose_embedding from a numpy array to a list if necessary\n        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        # Return a dictionary representation of the MicroAgent instance\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if isinstance(agent.purpose_embedding, np.ndarray) else agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n        return agent_dict\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    assert len(items) == len(weights)\n    assert all(w >= 0 for w in weights)\n    assert num_bins > 0\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item in the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        bin_index = min(bin_weights, key=bin_weights.get)\n        bins[bin_index].append(item)\n        bin_weights[bin_index] += weight\n\n    return bins, bin_weights\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be the same.\")\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n    if any(w < 0 for w in weights):\n        raise ValueError(\"All weights must be non-negative.\")\n\n    # Sort the items by weight in descending order\n    items_sorted = [x for _, x in sorted(zip(weights, items), reverse=True)]\n    weights_sorted = sorted(weights, reverse=True)\n\n    # Initialize the bins and their weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item in the bin with the current lowest total weight\n    for item, weight in zip(items_sorted, weights_sorted):\n        bin_index = min(bin_weights, key=bin_weights.get)\n        bins[bin_index].append(item)\n        bin_weights[bin_index] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    assert len(items) == len(weights), \"The number of items and weights must be the same.\"\n    assert all(w >= 0 for w in weights), \"All weights must be non-negative.\"\n    assert num_bins > 0, \"The number of bins must be positive.\"\n\n    # Sort the items by weight in descending order.\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their total weights.\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item into the bin with the current lowest total weight.\n    for item, weight in sorted_items:\n        # Find the bin with the current lowest total weight.\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin and update its total weight.\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items is equal to the number of weights\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items must be equal to the number of weights.\")\n\n    # Check that all weights are positive\n    if any(w <= 0 for w in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Check that num_bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"num_bins must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item in the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin and update its weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    assert len(items) == len(weights), \"The number of items and weights must be equal.\"\n    assert all(w >= 0 for w in weights), \"All weights must be non-negative.\"\n    assert num_bins > 0, \"The number of bins must be positive.\"\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their total weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Iterate over the sorted items and place each one into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin and update its total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    assert len(items) == len(weights)\n    assert all(w > 0 for w in weights)\n    assert num_bins > 0\n\n    # Sort the items by weight in descending order\n    items_sorted = [item for _, item in sorted(zip(weights, items), reverse=True)]\n    weights_sorted = sorted(weights, reverse=True)\n\n    # Initialize the bins and their total weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place the items in the bins greedily\n    for item, weight in zip(items_sorted, weights_sorted):\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin and update its total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Create a dictionary to store the items in each bin\n    bins = {i: [] for i in range(num_bins)}\n    # Create a dictionary to store the total weight of each bin\n    bin_weights = {i: 0 for i in range(num_bins)}\n    # Sort the items by weight in descending order\n    items_sorted = [item for _, item in sorted(zip(weights, items), reverse=True)]\n    # Place each item into the bin with the current lowest total weight\n    for item in items_sorted:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n        # Add the item to the bin\n        bins[min_bin].append(item)\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weights[items.index(item)]\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be the same.\")\n\n    if num_bins < 1:\n        raise ValueError(\"The number of bins must be at least 1.\")\n\n    if any(w < 0 for w in weights):\n        raise ValueError(\"All weights must be non-negative.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their total weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Distribute the items into the bins\n    for item, weight in sorted_items:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin with the lowest total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Create a list of tuples containing the item and its weight\n    items_and_weights = list(zip(items, weights))\n\n    # Sort the items by weight in descending order\n    items_and_weights.sort(key=lambda x: x[1], reverse=True)\n\n    # Create a dictionary to store the items in each bin\n    bins = {i: [] for i in range(num_bins)}\n\n    # Create a dictionary to store the total weight of each bin\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Iterate over the items and place them in the bin with the current lowest total weight\n    for item, weight in items_and_weights:\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin\n        bins[min_bin].append(item)\n\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items and weights are equal\n    assert len(items) == len(weights)\n\n    # Check that all weights are positive\n    assert all(weight >= 0 for weight in weights)\n\n    # Check that the number of bins is positive\n    assert num_bins > 0\n\n    # Create a dictionary to store the items in each bin\n    bins = {i: [] for i in range(num_bins)}\n\n    # Create a dictionary to store the total weight of each bin\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Distribute the items greedily into the bins\n    for item, weight in sorted_items:\n        # Find the bin with the current lowest total weight\n        bin_index = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin and update the total weight of the bin\n        bins[bin_index].append(item)\n        bin_weights[bin_index] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of items is less than the number of bins\n    if len(items) < num_bins:\n        raise ValueError(\"Number of items must be greater than or equal to number of bins.\")\n\n    # Check if the number of weights is not equal to the number of items\n    if len(weights) != len(items):\n        raise ValueError(\"Number of weights must be equal to the number of items.\")\n\n    # Check if any weight is negative\n    if any(weight < 0 for weight in weights):\n        raise ValueError(\"All weights must be non-negative.\")\n\n    # Check if the number of bins is negative or zero\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive.\")\n\n    # Create a list of tuples containing the item and its weight\n    item_weights = list(zip(items, weights))\n\n    # Sort the items by weight in descending order\n    item_weights.sort(key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins dictionary with empty lists for each bin\n    bins = {i: [] for i in range(num_bins)}\n\n    # Initialize the bin weights dictionary with zeros for each bin\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Iterate over the sorted items and place them in the bin with the current lowest total weight\n    for item, weight in item_weights:\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item and its weight to the bin\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Initialize the output dictionaries\n    bin_contents = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Sort the items by weight in descending order\n    items_sorted = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Distribute the items greedily into the bins\n    for item, weight in items_sorted:\n        # Find the bin with the current lowest total weight\n        bin_index = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin and update its total weight\n        bin_contents[bin_index].append(item)\n        bin_weights[bin_index] += weight\n\n    return bin_contents, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items and weights are equal\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be equal\")\n\n    # Check that all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive\")\n\n    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive\")\n\n    # Sort the items and weights by weight in descending order\n    sorted_items = [item for _, item in sorted(zip(weights, items), reverse=True)]\n    sorted_weights = sorted(weights, reverse=True)\n\n    # Initialize the bins and their total weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item in the bin with the current lowest total weight\n    for item, weight in zip(sorted_items, sorted_weights):\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Place the item in the bin with the current lowest total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    assert len(items) == len(weights)\n    assert all(w > 0 for w in weights)\n    assert num_bins > 0\n\n    # Create a list of tuples containing the items and their weights\n    items_and_weights = list(zip(items, weights))\n\n    # Sort the items by weight in descending order\n    items_and_weights.sort(key=lambda x: x[1], reverse=True)\n\n    # Initialize a dictionary to store the items in each bin\n    bins = {i: [] for i in range(num_bins)}\n\n    # Initialize a dictionary to store the total weight of each bin\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Iterate through the items and place them in the bin with the current lowest total weight\n    for item, weight in items_and_weights:\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin\n        bins[min_bin].append(item)\n\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    assert len(items) == len(weights), \"The number of items and weights must be the same.\"\n    assert all(weight > 0 for weight in weights), \"All weights must be positive.\"\n\n    # Create a list of tuples containing the item and its weight\n    item_weights = list(zip(items, weights))\n\n    # Sort the items by weight in descending order\n    item_weights.sort(key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and the total weights\n    bins = {i: [] for i in range(num_bins)}\n    total_weights = {i: 0 for i in range(num_bins)}\n\n    # Distribute the items greedily\n    for item, weight in item_weights:\n        # Find the bin with the current lowest total weight\n        min_bin = min(total_weights, key=total_weights.get)\n\n        # Add the item to the bin and update the total weight\n        bins[min_bin].append(item)\n        total_weights[min_bin] += weight\n\n    return bins, total_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Distribute the items greedily\n    for item, weight in sorted_items:\n        # Find the bin with the current lowest total weight\n        bin_index = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin and update its weight\n        bins[bin_index].append(item)\n        bin_weights[bin_index] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of items and weights are equal\n    assert len(items) == len(weights)\n\n    # Check if the number of bins is positive\n    assert num_bins > 0\n\n    # Check if all weights are positive\n    assert all(weight > 0 for weight in weights)\n\n    # Initialize the bin dictionary and the bin weights dictionary\n    bin_dict = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Sort the items and weights by weight in descending order\n    sorted_items_and_weights = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Iterate over the sorted items and weights\n    for item, weight in sorted_items_and_weights:\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item and weight to the bin\n        bin_dict[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    # Return the bin dictionary and the bin weights dictionary\n    return bin_dict, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    assert len(items) == len(weights), \"The length of items and weights must be the same.\"\n    assert all(w >= 0 for w in weights), \"All weights must be non-negative.\"\n    assert num_bins > 0, \"The number of bins must be positive.\"\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their total weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items and weights match\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must match\")\n\n    # Check that all weights are positive\n    if any(w < 0 for w in weights):\n        raise ValueError(\"All weights must be positive\")\n\n    # Check that there are at least as many items as bins\n    if len(items) < num_bins:\n        raise ValueError(\"There must be at least as many items as bins\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place the items into the bins greedily\n    for item, weight in sorted_items:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Place the item into the bin with the lowest total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    assert len(items) == len(weights), \"The length of 'items' and 'weights' must be the same.\"\n    assert all(w > 0 for w in weights), \"All weights must be positive.\"\n    assert num_bins > 0, \"The number of bins must be positive.\"\n\n    # Sort the items by weight in descending order.\n    items_sorted = [item for _, item in sorted(zip(weights, items), reverse=True)]\n    weights_sorted = sorted(weights, reverse=True)\n\n    # Initialize the bins with empty lists and total weights of 0.\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item into the bin with the current lowest total weight.\n    for item, weight in zip(items_sorted, weights_sorted):\n        bin_index = min(bin_weights, key=bin_weights.get)\n        bins[bin_index].append(item)\n        bin_weights[bin_index] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = f\"{func_name}{args}{kwargs}\".encode()\n        return hashlib.sha256(data).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Convert the arguments and keyword arguments to a string representation\n        args_str = str(args)\n        kwargs_str = str(kwargs)\n\n        # Combine the function name, arguments, and keyword arguments into a single string\n        data = func_name + args_str + kwargs_str\n\n        # Compute the SHA-256 hash of the data\n        hash_object = hashlib.sha256(data.encode())\n\n        # Return the hexadecimal digest of the hash\n        return hash_object.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Concatenate the function name, arguments, and keyword arguments\n        data = func_name + str(args) + str(kwargs)\n\n        # Compute the SHA-256 hash of the data\n        hash_object = hashlib.sha256(data.encode())\n\n        # Return the hexadecimal digest of the hash\n        return hash_object.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = func_name\n        for arg in args:\n            data += str(arg)\n        for key, value in kwargs.items():\n            data += str(key) + str(value)\n        return hashlib.sha256(data.encode('utf-8')).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Concatenate the function name, arguments, and keyword arguments into a single string\n        data = func_name + str(args) + str(kwargs)\n\n        # Compute the SHA-256 hash of the data\n        hash_object = hashlib.sha256(data.encode())\n\n        # Return the hexadecimal digest of the hash\n        return hash_object.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = func_name\n        for arg in args:\n            data += str(arg)\n        for key, value in kwargs.items():\n            data += str(key) + str(value)\n        return hashlib.sha256(data.encode()).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name,) + args + tuple(kwargs.items())\n        data_bytes = str(data).encode('utf-8')\n        return hashlib.sha256(data_bytes).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = f\"{func_name}{args}{kwargs}\".encode()\n        return hashlib.sha256(data).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        import hashlib\n\n        # Create a string representation of the function name and its arguments\n        data = f\"{func_name}{args}{kwargs}\".encode('utf-8')\n\n        # Compute the SHA-256 hash of the data\n        hash_object = hashlib.sha256(data)\n\n        # Return the hexadecimal digest of the hash\n        return hash_object.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = f'{func_name}{args}{kwargs}'.encode('utf-8')\n        return hashlib.sha256(data).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = func_name + str(args) + str(kwargs)\n        return hashlib.sha256(data.encode()).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name,) + args + tuple(sorted(kwargs.items()))\n        data_str = json.dumps(data, sort_keys=True)\n        return hashlib.sha256(data_str.encode('utf-8')).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        import hashlib\n        import json\n\n        data = {\n            'func_name': func_name,\n            'args': args,\n            'kwargs': kwargs\n        }\n\n        json_data = json.dumps(data, sort_keys=True)\n        hash_object = hashlib.sha256(json_data.encode())\n        return hash_object.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Create a JSON representation of the function name, arguments, and keyword arguments\n        data = json.dumps({'func_name': func_name, 'args': args, 'kwargs': kwargs})\n\n        # Compute the SHA-256 hash of the JSON representation\n        hash_object = hashlib.sha256(data.encode())\n\n        # Return the hexadecimal digest of the hash\n        return hash_object.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = func_name + str(args) + str(kwargs)\n        return hashlib.sha256(data.encode()).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Convert the function name and arguments to a string\n        data = f\"{func_name}{args}{kwargs}\".encode()\n\n        # Create a SHA-256 hash object and update it with the data\n        hash_object = hashlib.sha256()\n        hash_object.update(data)\n\n        # Return the hexadecimal digest of the hash object\n        return hash_object.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = f\"{func_name}{args}{kwargs}\".encode('utf-8')\n        return hashlib.sha256(data).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Concatenate the function name, arguments, and keyword arguments into a string\n        data = func_name + str(args) + str(kwargs)\n\n        # Encode the string as bytes using UTF-8 encoding\n        data_bytes = data.encode('utf-8')\n\n        # Compute the SHA-256 hash of the bytes\n        hash_object = hashlib.sha256(data_bytes)\n\n        # Get the hexadecimal representation of the hash\n        hash_hex = hash_object.hexdigest()\n\n        # Return the hexadecimal representation of the hash\n        return hash_hex\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        import hashlib\n        import json\n\n        # Create a JSON representation of the arguments and keyword arguments\n        data = {\n            'func_name': func_name,\n            'args': args,\n            'kwargs': kwargs\n        }\n\n        # Convert the JSON representation to a string\n        data_str = json.dumps(data)\n\n        # Compute the SHA-256 hash of the string representation\n        hash_obj = hashlib.sha256(data_str.encode())\n\n        # Get the hexadecimal digest of the hash\n        hash_str = hash_obj.hexdigest()\n\n        return hash_str\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        import hashlib\n        import json\n\n        # Convert the arguments and keyword arguments to JSON-serializable format\n        args_json = json.dumps(args, sort_keys=True)\n        kwargs_json = json.dumps(kwargs, sort_keys=True)\n\n        # Create a hash object and update it with the function name, arguments, and keyword arguments\n        hash_obj = hashlib.sha256()\n        hash_obj.update(func_name.encode())\n        hash_obj.update(args_json.encode())\n        hash_obj.update(kwargs_json.encode())\n\n        # Get the hexadecimal digest of the hash object\n        return hash_obj.hexdigest()\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    total_length = 0\n\n    for i in range(1, len(polygon)):\n        distance = np.linalg.norm(polygon[i] - polygon[i - 1])\n        if distance <= max_point_distance:\n            total_length += distance\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    total_length = 0\n    for i in range(len(polygon) - 1):\n        point1 = polygon[i]\n        point2 = polygon[i + 1]\n        distance = np.linalg.norm(point2 - point1)\n        if distance <= max_point_distance:\n            total_length += distance\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    length = 0.0\n    for i in range(1, len(polygon)):\n        distance = np.linalg.norm(polygon[i] - polygon[i - 1])\n        if distance <= max_point_distance:\n            length += distance\n    return length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    length = 0\n    for i in range(len(polygon) - 1):\n        if np.linalg.norm(polygon[i] - polygon[i + 1]) < max_point_distance:\n            length += np.linalg.norm(polygon[i] - polygon[i + 1])\n    return length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the Euclidean distances between consecutive points in the polygon\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Filter out distances that exceed the maximum point distance\n    filtered_distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon by summing the filtered distances\n    total_length = np.sum(filtered_distances)\n\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    polygon_length = 0\n    for i in range(1, len(polygon)):\n        distance = np.linalg.norm(polygon[i] - polygon[i-1])\n        if distance <= max_point_distance:\n            polygon_length += distance\n    return polygon_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points\n    distances = np.sqrt(np.sum((polygon[1:] - polygon[:-1]) ** 2, axis=1))\n\n    # Filter out distances that exceed the maximum distance\n    filtered_distances = distances[distances <= max_point_distance]\n\n    # Sum the remaining distances to get the total length of the polygon\n    total_length = np.sum(filtered_distances)\n\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    total_length = 0\n\n    for i in range(1, len(polygon)):\n        distance = np.linalg.norm(polygon[i] - polygon[i - 1])\n        if distance <= max_point_distance:\n            total_length += distance\n\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the total length to zero\n    total_length = 0\n\n    # Iterate over the rows of the polygon array\n    for i in range(polygon.shape[0]):\n        # If this is not the first row, compute the distance between this point and the previous point\n        if i > 0:\n            distance = np.linalg.norm(polygon[i] - polygon[i-1])\n            # If the distance is below the maximum distance threshold, add it to the total length\n            if distance <= max_point_distance:\n                total_length += distance\n\n    # Return the total length\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the total length to 0\n    total_length = 0\n\n    # Loop through the points in the polygon\n    for i in range(len(polygon)):\n        # Get the current point and the next point\n        point1 = polygon[i]\n        point2 = polygon[(i + 1) % len(polygon)]\n\n        # Compute the distance between the two points\n        distance = np.linalg.norm(point2 - point1)\n\n        # If the distance is below the maximum point distance, add it to the total length\n        if distance <= max_point_distance:\n            total_length += distance\n\n    return total_length\n\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the total length to zero\n    length = 0\n\n    # Iterate through the points in the polygon\n    for i in range(len(polygon)):\n        # Get the current point and the next point\n        point1 = polygon[i]\n        point2 = polygon[(i + 1) % len(polygon)]\n\n        # Compute the distance between the current point and the next point\n        distance = np.linalg.norm(point2 - point1)\n\n        # If the distance is below the maximum distance, add it to the total length\n        if distance <= max_point_distance:\n            length += distance\n\n    # Return the total length\n    return length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the total length to 0\n    total_length = 0\n\n    # Iterate over the rows of the polygon array\n    for i in range(len(polygon) - 1):\n\n        # Compute the distance between the current point and the next point\n        distance = np.linalg.norm(polygon[i + 1] - polygon[i])\n\n        # If the distance is below the maximum, add it to the total length\n        if distance < max_point_distance:\n            total_length += distance\n\n    # Return the total length\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the Euclidean distance between consecutive points in the polygon\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Filter out distances that exceed the maximum point distance\n    distances = distances[distances <= max_point_distance]\n\n    # Return the total length of the polygon\n    return np.sum(distances)\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the total length to zero\n    total_length = 0\n\n    # Iterate over the points in the polygon\n    for i in range(len(polygon)):\n        # Get the current point\n        point1 = polygon[i]\n\n        # Get the next point, wrapping around to the first point if we're at the end of the array\n        point2 = polygon[(i + 1) % len(polygon)]\n\n        # Compute the distance between the current and next points\n        distance = np.linalg.norm(point2 - point1)\n\n        # If the distance is below the maximum, add it to the total length\n        if distance <= max_point_distance:\n            total_length += distance\n\n    # Return the total length\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize a variable to store the total length\n    length = 0\n\n    # Iterate through the points in the polygon\n    for i in range(len(polygon) - 1):\n        # Calculate the distance between the current point and the next point\n        dist = np.linalg.norm(polygon[i] - polygon[i + 1])\n\n        # If the distance is below the maximum threshold, add it to the total length\n        if dist <= max_point_distance:\n            length += dist\n\n    return length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the total length to zero\n    total_length = 0\n\n    # Iterate over the rows of the polygon array, starting from the second row\n    for i in range(1, len(polygon)):\n        # Compute the Euclidean distance between the current point and the previous point\n        distance = np.linalg.norm(polygon[i] - polygon[i-1])\n\n        # If the distance is below the maximum threshold, add it to the total length\n        if distance < max_point_distance:\n            total_length += distance\n\n    # Return the total length\n    return total_length\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the Euclidean distance between consecutive points in the polygon\n    dist = np.sqrt(np.sum((polygon[1:] - polygon[:-1]) ** 2, axis=1))\n\n    # Filter out distances that exceed the maximum distance threshold\n    dist_filtered = dist[dist <= max_point_distance]\n\n    # Compute the total length of the polygon by summing the filtered distances\n    length = np.sum(dist_filtered)\n\n    return length\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    total_length = 0\n    for i in range(len(polygon) - 1):\n        distance = np.linalg.norm(polygon[i+1] - polygon[i])\n        if distance <= max_point_distance:\n            total_length += distance\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the total length to zero\n    total_length = 0\n\n    # Iterate over the rows of the polygon array\n    for i in range(polygon.shape[0]):\n\n        # If we're not at the last row, compute the distance between the current point and the next point\n        if i < polygon.shape[0] - 1:\n\n            # Compute the distance between the current point and the next point\n            distance = np.linalg.norm(polygon[i + 1] - polygon[i])\n\n            # If the distance is below the maximum point distance, add it to the total length\n            if distance < max_point_distance:\n\n                total_length += distance\n\n    # Return the total length of the polygon\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Filter out distances that exceed the maximum distance\n    filtered_distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon\n    total_length = np.sum(filtered_distances)\n\n    return total_length\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Compute the areas of all polygons\n    areas = [cv2.contourArea(polygon) for polygon in polygons]\n\n    # Find the largest polygon area\n    max_area = max(areas)\n\n    # Filter out polygons based on area\n    filtered_polygons = [\n        polygon\n        for polygon, area in zip(polygons, areas)\n        if area >= max_area * rel_tr and area >= abs_tr\n    ]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Calculate the area of each polygon\n    areas = [cv2.contourArea(polygon) for polygon in polygons]\n\n    # Find the largest area\n    largest_area = max(areas)\n\n    # Calculate the relative threshold\n    rel_threshold = rel_tr * largest_area\n\n    # Filter out polygons with an area below the relative threshold or the absolute threshold\n    filtered_polygons = [\n        polygon\n        for polygon, area in zip(polygons, areas)\n        if area >= rel_threshold and area >= abs_tr\n    ]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    # Find the polygon with the largest area\n    largest_area = max([cv2.contourArea(cnt) for cnt in polygons])\n\n    # Filter out polygons with area below the threshold\n    filtered_polygons = [\n        cnt\n        for cnt in polygons\n        if cv2.contourArea(cnt) > max(rel_tr * largest_area, abs_tr)\n    ]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Find the largest polygon in the list\n    largest_area = max([cv2.contourArea(cnt) for cnt in polygons])\n\n    # Filter out polygons based on area\n    filtered_polygons = [\n        cnt\n        for cnt in polygons\n        if cv2.contourArea(cnt) > rel_tr * largest_area and cv2.contourArea(cnt) > abs_tr\n    ]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    areas = np.array([Polygon(polygon).area for polygon in polygons])\n    max_area = areas.max()\n\n    filtered_polygons = [\n        polygon\n        for polygon, area in zip(polygons, areas)\n        if area > max(rel_tr * max_area, abs_tr)\n    ]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return []\n    else:\n        # Find the largest polygon area\n        largest_area = max([cv2.contourArea(cnt) for cnt in polygons])\n\n        # Filter out polygons based on area\n        filtered_polygons = [\n            cnt\n            for cnt in polygons\n            if cv2.contourArea(cnt) > max(rel_tr * largest_area, abs_tr)\n        ]\n\n        return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return []\n\n    areas = [polygon_area(polygon) for polygon in polygons]\n    max_area = max(areas)\n    filtered_polygons = [\n        polygon\n        for polygon, area in zip(polygons, areas)\n        if area >= max(abs_tr, rel_tr * max_area)\n    ]\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Calculate the area of each polygon\n    areas = [cv2.contourArea(polygon) for polygon in polygons]\n\n    # Find the largest polygon area\n    max_area = max(areas)\n\n    # Filter out polygons based on the relative threshold\n    filtered_polygons = [polygon for polygon, area in zip(polygons, areas) if area >= max_area * rel_tr]\n\n    # Filter out polygons based on the absolute threshold\n    filtered_polygons = [polygon for polygon in filtered_polygons if cv2.contourArea(polygon) >= abs_tr]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return []\n\n    largest_area = max([cv2.contourArea(cnt) for cnt in polygons])\n\n    return [\n        polygon\n        for polygon in polygons\n        if (cv2.contourArea(polygon) > largest_area * rel_tr) and (cv2.contourArea(polygon) > abs_tr)\n    ]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if rel_tr == 0 and abs_tr == 0:\n        return polygons\n\n    areas = np.array([cv2.contourArea(polygon) for polygon in polygons])\n\n    if rel_tr > 0:\n        rel_tr = np.max(areas) * rel_tr\n\n    return [polygon for polygon, area in zip(polygons, areas) if area >= max(rel_tr, abs_tr)]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Calculate the area of each polygon\n    areas = [cv2.contourArea(polygon) for polygon in polygons]\n\n    # Find the largest area\n    largest_area = max(areas)\n\n    # Filter out polygons based on the relative and absolute thresholds\n    filtered_polygons = [\n        polygon\n        for polygon, area in zip(polygons, areas)\n        if area >= rel_tr * largest_area and area >= abs_tr\n    ]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return []\n    areas = [Polygon(polygon).area for polygon in polygons]\n    max_area = max(areas)\n    if rel_tr == 0 and abs_tr == 0:\n        return polygons\n    if rel_tr == 0:\n        return [polygon for polygon, area in zip(polygons, areas) if area > abs_tr]\n    if abs_tr == 0:\n        return [polygon for polygon, area in zip(polygons, areas) if area > max_area * rel_tr]\n    return [\n        polygon\n        for polygon, area in zip(polygons, areas)\n        if area > max_area * rel_tr and area > abs_tr\n    ]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    # Calculate the area of each polygon\n    areas = [cv2.contourArea(p) for p in polygons]\n\n    # Find the largest polygon's area\n    max_area = max(areas)\n\n    # Calculate the relative threshold\n    rel_threshold = max_area * rel_tr\n\n    # Filter out polygons with an area below the relative threshold or the absolute threshold\n    filtered_polygons = [\n        p for p, area in zip(polygons, areas) if area >= rel_threshold and area >= abs_tr\n    ]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return []\n    elif len(polygons) == 1:\n        return polygons\n\n    areas = [cv2.contourArea(p) for p in polygons]\n    max_area = max(areas)\n    rel_tr = max_area * rel_tr\n\n    return [p for i, p in enumerate(polygons) if areas[i] > rel_tr and areas[i] > abs_tr]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    # Find the polygon with the largest area\n    largest_area = max(cv2.contourArea(polygon) for polygon in polygons)\n\n    # Define the absolute threshold based on the largest area\n    abs_threshold = abs_tr * largest_area\n\n    # Define the relative threshold based on the largest area\n    rel_threshold = rel_tr * largest_area\n\n    # Filter out polygons with area below the threshold\n    filtered_polygons = [\n        polygon\n        for polygon in polygons\n        if cv2.contourArea(polygon) > max(abs_threshold, rel_threshold)\n    ]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return []\n    areas = [Polygon(polygon).area for polygon in polygons]\n    max_area = max(areas)\n    filtered_polygons = [\n        polygon\n        for polygon, area in zip(polygons, areas)\n        if area >= max_area * rel_tr and area >= abs_tr\n    ]\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Calculate the area of each polygon\n    areas = [cv2.contourArea(p) for p in polygons]\n\n    # Find the index of the largest polygon\n    largest_idx = np.argmax(areas)\n\n    # Calculate the area of the largest polygon\n    largest_area = areas[largest_idx]\n\n    # Calculate the relative threshold\n    rel_threshold = rel_tr * largest_area\n\n    # Filter out polygons based on the relative and absolute thresholds\n    filtered_polygons = [\n        p for i, p in enumerate(polygons) if areas[i] > rel_threshold and areas[i] > abs_tr\n    ]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Calculate the area of each polygon\n    areas = [cv2.contourArea(p) for p in polygons]\n\n    # Find the largest polygon's area\n    max_area = max(areas)\n\n    # Filter out polygons based on area\n    filtered_polygons = [\n        p\n        for p, area in zip(polygons, areas)\n        if area > max(max_area * rel_tr, abs_tr)\n    ]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Find the polygon with the largest area\n    max_area = max([cv2.contourArea(c) for c in polygons])\n\n    # Filter out polygons that are too small\n    filtered_polygons = [\n        polygon\n        for polygon in polygons\n        if cv2.contourArea(polygon) > rel_tr * max_area or cv2.contourArea(polygon) > abs_tr\n    ]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Calculate the areas of all polygons\n    areas = [cv2.contourArea(polygon) for polygon in polygons]\n\n    # Find the index of the polygon with the largest area\n    largest_area_idx = np.argmax(areas)\n\n    # Calculate the threshold based on the largest polygon's area and the relative threshold\n    threshold = areas[largest_area_idx] * rel_tr\n\n    # Filter out polygons with area below the threshold\n    filtered_polygons = [\n        polygon for polygon, area in zip(polygons, areas) if area > threshold or area > abs_tr\n    ]\n\n    return filtered_polygons\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = (num_samples_yielded // num_workers) // batch_size\n    num_samples_remaining = (num_samples_yielded // num_workers) % batch_size\n    num_samples_per_worker_dict = {i: num_samples_per_worker for i in range(num_workers)}\n    for i in range(num_samples_remaining):\n        worker_index = i % num_workers\n        num_samples_per_worker_dict[worker_index] += 1\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker should process\n    num_samples_per_worker = (num_samples_yielded + num_workers - 1) // num_workers\n\n    # Calculate the number of samples each worker has processed\n    num_samples_processed = {i: min(num_samples_per_worker, num_samples_yielded - i * num_samples_per_worker) for i in range(num_workers)}\n\n    # Distribute any remaining samples\n    remaining_samples = num_samples_yielded - sum(num_samples_processed.values())\n    for i in range(remaining_samples):\n        worker_index = i % num_workers\n        num_samples_processed[worker_index] += 1\n\n    return num_samples_processed\n\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    samples_per_worker = (num_samples_yielded // num_workers) // batch_size\n    remaining_samples = num_samples_yielded - (samples_per_worker * batch_size * num_workers)\n    samples_per_worker_dict = {worker_idx: samples_per_worker for worker_idx in range(num_workers)}\n    for i in range(remaining_samples):\n        samples_per_worker_dict[i % num_workers] += 1\n    return samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed, given the total number of samples yielded, the batch size, and the number of workers.\n    num_samples_per_worker = (num_samples_yielded // batch_size) // num_workers\n\n    # Calculate the remaining number of samples.\n    num_remaining_samples = (num_samples_yielded // batch_size) % num_workers\n\n    # Create a dictionary where each key is a worker index (starting from 0) and its value is the number of samples that worker has processed.\n    num_samples_per_worker_dict = {i: num_samples_per_worker for i in range(num_workers)}\n\n    # Distribute any remaining samples among the workers.\n    for i in range(num_remaining_samples):\n        num_samples_per_worker_dict[i] += 1\n\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that will be left over after distributing the samples evenly among the workers\n    remaining_samples = num_samples_yielded % num_workers\n\n    # Initialize a dictionary to store the number of samples each worker has processed\n    samples_per_worker = {}\n\n    # Distribute the remaining samples among the workers\n    for i in range(num_workers):\n        if i < remaining_samples:\n            samples_per_worker[i] = num_samples_per_worker + 1\n        else:\n            samples_per_worker[i] = num_samples_per_worker\n\n    # Calculate the total number of samples that have been processed\n    total_samples_processed = sum(samples_per_worker.values())\n\n    # If the total number of samples processed is not equal to the total number of samples yielded, distribute the remaining samples among the workers\n    if total_samples_processed != num_samples_yielded:\n        remaining_samples = num_samples_yielded - total_samples_processed\n        for i in range(num_workers):\n            if remaining_samples > 0:\n                samples_per_worker[i] += 1\n                remaining_samples -= 1\n            else:\n                break\n\n    # Return the dictionary of samples per worker\n    return samples_per_worker\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker = [samples_per_worker for _ in range(num_workers)]\n\n    remainder = num_samples_yielded % num_workers\n    for i in range(remainder):\n        num_samples_per_worker[i] += 1\n\n    num_samples_per_worker = [num_samples_per_worker[i] // batch_size for i in range(num_workers)]\n\n    num_samples_per_worker = [num_samples_per_worker[i] * batch_size for i in range(num_workers)]\n\n    num_samples_per_worker = [num_samples_per_worker[i] + remainder if i < remainder else num_samples_per_worker[i] for i in range(num_workers)]\n\n    return {i: num_samples_per_worker[i] for i in range(num_workers)}\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_mod = num_samples_yielded % num_workers\n    num_samples_per_worker_mod_batch = num_samples_per_worker_mod // batch_size\n    num_samples_per_worker_mod_batch_mod = num_samples_per_worker_mod % batch_size\n\n    num_samples_per_worker_dict = {\n        i: num_samples_per_worker // batch_size for i in range(num_workers)\n    }\n    num_samples_per_worker_dict[0] += num_samples_per_worker_mod_batch\n    num_samples_per_worker_dict[0] += num_samples_per_worker_mod_batch_mod\n\n    return num_samples_per_worker_dict"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that each worker has processed in the last batch\n    num_samples_last_batch = num_samples_yielded % num_workers\n\n    # Calculate the number of samples that each worker has processed in the previous batches\n    num_samples_per_worker_prev_batches = num_samples_per_worker * num_workers\n\n    # Calculate the number of samples that each worker has processed in the last batch\n    num_samples_per_worker_last_batch = num_samples_last_batch + num_samples_per_worker\n\n    # Calculate the number of samples that each worker has processed in the previous batches\n    num_samples_per_worker_prev_batches = num_samples_per_worker_prev_batches + num_samples_per_worker_last_batch\n\n    # Calculate the number of samples that each worker has processed in the last batch\n    num_samples_per_worker_last_batch = num_samples_per_worker_last_batch - num_samples_last_batch\n\n    # Calculate the number of samples that each worker has processed in the previous batches\n    num_samples_per_worker_prev_batches = num_samples_per_worker_prev_batches - num_samples_per_worker_last_batch\n\n    # Calculate the number of samples that each worker has processed in the last batch\n    num_samples_per_worker_last_batch = num_samples_per_worker_last_batch - num_samples_last_batch\n\n    # Calculate the number of samples that each worker has processed in the previous batches\n    num_samples_per_worker_prev_batches = num_samples_per_worker_prev_batches - num_samples_per_worker_last_batch\n\n    # Calculate the number of samples that each worker has processed in the last batch\n    num_"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_remainder = num_samples_yielded % num_workers\n\n    num_samples_per_worker_dict = {worker_idx: num_samples_per_worker for worker_idx in range(num_workers)}\n\n    for worker_idx in range(num_samples_per_worker_remainder):\n        num_samples_per_worker_dict[worker_idx] += 1\n\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed.\n    num_samples_per_worker = (num_samples_yielded // num_workers) // batch_size\n\n    # Calculate the number of samples that have not been distributed among workers.\n    num_samples_remaining = (num_samples_yielded // num_workers) % batch_size\n\n    # Initialize a dictionary to store the number of samples each worker has processed.\n    num_samples_per_worker_dict = {}\n\n    # Distribute the remaining samples among the workers.\n    for worker_index in range(num_workers):\n        num_samples_per_worker_dict[worker_index] = num_samples_per_worker\n\n        # If there are remaining samples, distribute them among the workers.\n        if num_samples_remaining > 0:\n            num_samples_per_worker_dict[worker_index] += 1\n            num_samples_remaining -= 1\n\n    return num_samples_per_worker_dict\n\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_remainder = num_samples_yielded % num_workers\n\n    # Distribute any remaining samples among the workers\n    num_samples_per_worker_dict = {i: num_samples_per_worker for i in range(num_workers)}\n    for i in range(num_samples_per_worker_remainder):\n        num_samples_per_worker_dict[i] += 1\n\n    # Calculate the number of batches each worker has processed\n    num_batches_per_worker_dict = {i: num_samples_per_worker_dict[i] // batch_size for i in range(num_workers)}\n\n    # Calculate the number of samples each worker has processed after accounting for the remainder\n    num_samples_per_worker_dict_after_remainder = {i: num_batches_per_worker_dict[i] * batch_size for i in range(num_workers)}\n    num_samples_per_worker_dict_after_remainder_remainder = {i: num_samples_per_worker_dict[i] % batch_size for i in range(num_workers)}\n    for i in range(num_samples_per_worker_remainder):\n        num_samples_per_worker_dict_after_remainder[i] += num_samples_per_worker_dict_after_remainder_remainder[i]\n\n    return num_samples_per_worker_dict_after_remainder\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    samples_per_worker = num_samples_yielded // num_workers\n    remainder = num_samples_yielded % num_workers\n    samples_per_worker_list = [samples_per_worker] * num_workers\n    for i in range(remainder):\n        samples_per_worker_list[i] += 1\n    samples_per_worker_list = [batch_size * samples // batch_size for samples in samples_per_worker_list]\n    samples_per_worker_dict = {i: samples_per_worker_list[i] for i in range(num_workers)}\n    return samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the total number of samples that have been processed.\n    num_samples_processed = num_samples_yielded * batch_size\n\n    # Calculate the number of samples that each worker should process.\n    num_samples_per_worker = num_samples_processed // num_workers\n\n    # Calculate the number of remaining samples that need to be distributed among the workers.\n    num_remaining_samples = num_samples_processed % num_workers\n\n    # Initialize a dictionary to store the number of samples each worker has processed.\n    num_samples_per_worker_dict = {}\n\n    # Distribute the remaining samples among the workers.\n    for i in range(num_workers):\n        num_samples_per_worker_dict[i] = num_samples_per_worker\n        if i < num_remaining_samples:\n            num_samples_per_worker_dict[i] += 1\n\n    # Return the dictionary of samples per worker.\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker should process\n    num_samples_per_worker = (num_samples_yielded // num_workers) // batch_size\n\n    # Calculate the number of samples that will be left over after distributing the samples evenly among the workers\n    num_samples_left = (num_samples_yielded // num_workers) % batch_size\n\n    # Initialize a dictionary to store the number of samples each worker has processed\n    num_samples_processed = {}\n\n    # Distribute the remaining samples among the workers\n    for i in range(num_workers):\n        # If there are still samples left to distribute\n        if num_samples_left > 0:\n            # Give the current worker one more sample\n            num_samples_processed[i] = num_samples_per_worker + 1\n            num_samples_left -= 1\n        else:\n            # Give the current worker the standard number of samples\n            num_samples_processed[i] = num_samples_per_worker\n\n    return num_samples_processed\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed, given the total number of samples yielded, the batch size, and the number of workers\n    num_samples_per_worker = int(num_samples_yielded / num_workers)\n    num_samples_per_worker_list = [num_samples_per_worker for _ in range(num_workers)]\n    num_samples_per_worker_list[0] += num_samples_yielded % num_workers\n\n    # Distribute any remaining samples evenly among the workers\n    remaining_samples = num_samples_per_worker_list[0] % batch_size\n    num_samples_per_worker_list[0] -= remaining_samples\n    num_samples_per_worker_list[1:] = [num_samples_per_worker_list[i] + int(remaining_samples / (num_workers - 1)) for i in range(1, num_workers)]\n    num_samples_per_worker_list[1:(remaining_samples % (num_workers - 1)) + 1] = [num_samples_per_worker_list[i] + 1 for i in range(1, (remaining_samples % (num_workers - 1)) + 1)]\n\n    # Return a dictionary where each key is a worker index (starting from 0) and its value is the number of samples that worker has processed\n    return {i: num_samples_per_worker_list[i] for i in range(num_workers)}"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = (num_samples_yielded // num_workers) // batch_size\n    num_samples_remaining = (num_samples_yielded // num_workers) % batch_size\n\n    num_samples_processed_per_worker = {worker_idx: num_samples_per_worker for worker_idx in range(num_workers)}\n    for worker_idx in range(num_samples_remaining):\n        num_samples_processed_per_worker[worker_idx] += 1\n\n    return num_samples_processed_per_worker\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker should process\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that each worker should process, taking into account the batch size\n    num_samples_per_worker_with_batch_size = num_samples_per_worker // batch_size\n\n    # Calculate the number of remaining samples\n    num_remaining_samples = num_samples_yielded - (num_samples_per_worker * num_workers)\n\n    # Distribute the remaining samples among the workers\n    num_samples_per_worker_with_remaining = num_samples_per_worker_with_batch_size + (num_remaining_samples // num_workers)\n\n    # Create a dictionary to store the number of samples each worker has processed\n    num_samples_per_worker_dict = {}\n\n    # Loop through each worker and assign the number of samples it should process\n    for i in range(num_workers):\n        num_samples_per_worker_dict[i] = num_samples_per_worker_with_remaining\n\n    # Distribute the remaining samples among the workers\n    for i in range(num_remaining_samples % num_workers):\n        num_samples_per_worker_dict[i] += 1\n\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_list = [num_samples_per_worker] * num_workers\n    remaining_samples = num_samples_yielded % num_workers\n    for i in range(remaining_samples):\n        num_samples_per_worker_list[i] += 1\n    num_samples_per_worker_dict = {}\n    for i in range(num_workers):\n        num_samples_per_worker_dict[i] = num_samples_per_worker_list[i]\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples that each worker should process\n    samples_per_worker = (num_samples_yielded // num_workers) // batch_size\n\n    # Calculate the number of remaining samples that need to be distributed among the workers\n    remaining_samples = num_samples_yielded % (num_workers * samples_per_worker * batch_size)\n\n    # Initialize the dictionary to store the number of samples processed by each worker\n    samples_processed = {}\n\n    # Distribute the remaining samples among the workers\n    for i in range(num_workers):\n        samples_processed[i] = samples_per_worker * batch_size\n        if remaining_samples > 0:\n            samples_processed[i] += 1\n            remaining_samples -= 1\n\n    return samples_processed\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = (num_samples_yielded + num_workers - 1) // num_workers\n    num_samples_per_worker = (num_samples_per_worker + batch_size - 1) // batch_size * batch_size\n    num_samples_per_worker_list = [num_samples_per_worker for _ in range(num_workers)]\n    num_samples_per_worker_list[-1] = num_samples_yielded - (num_workers - 1) * num_samples_per_worker\n    return {i: num_samples_per_worker_list[i] for i in range(num_workers)}\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            if metadatas is not None:\n                filtered_metadatas.append(metadatas[i])\n            else:\n                filtered_metadatas.append(None)\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            if metadatas is not None:\n                filtered_metadatas.append(metadatas[i])\n            else:\n                filtered_metadatas.append(None)\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            if metadatas is not None:\n                filtered_metadatas.append(metadatas[i])\n            else:\n                filtered_metadatas.append(None)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            if metadatas is not None:\n                filtered_metadatas.append(metadatas[i])\n            else:\n                filtered_metadatas.append(None)\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            if metadatas is not None:\n                filtered_metadatas.append(metadatas[i])\n            else:\n                filtered_metadatas.append(None)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            if metadatas is not None:\n                filtered_metadatas.append(metadatas[i])\n            else:\n                filtered_metadatas.append(None)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            if metadatas is not None:\n                filtered_metadatas.append(metadatas[i])\n            else:\n                filtered_metadatas.append(None)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i, (result, value, metadata) in enumerate(zip(results, value, metadatas)):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    assert len(results) == len(value), \"Results and value lists must have the same length\"\n    assert len(results) == len(metadatas), \"Results and metadatas lists must have the same length\"\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    assert len(results) == len(value), \"results and value lists must have the same length.\"\n    assert len(results) == len(metadatas), \"results and metadatas lists must have the same length.\"\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the expected shape (_, 2)\n    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    # Extract x and y coordinates from the array\n    x = array[:, 0]\n    y = array[:, 1]\n\n    # Apply the Shoelace formula\n    area = 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the expected shape (_, 2)\n    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    # Extract x and y coordinates from the input array\n    x = array[:, 0]\n    y = array[:, 1]\n\n    # Calculate the area using the Shoelace formula\n    area = 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\n            \"The input array must have shape (_, 2), where _ can be any number of points.\"\n        )\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\n            f\"Input array must have shape (_, 2), but got {array.shape}\"\n        )\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the expected shape\n    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2).\")\n\n    # Extract the x and y coordinates of the polygon points\n    x = array[:, 0]\n    y = array[:, 1]\n\n    # Calculate the area using the Shoelace formula\n    area = 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    # Implementation of Shoelace formula\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    # Implementation of Shoelace formula\n    S1 = np.dot(x, np.roll(y, -1))\n    S2 = np.dot(y, np.roll(x, -1))\n    area = 0.5 * np.abs(S1 - S2)\n\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    S1 = np.sum(x * np.roll(y, -1))\n    S2 = np.sum(y * np.roll(x, -1))\n    A = 0.5 * np.abs(S1 - S2)\n\n    return A\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the expected shape\n    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2), where _ can be any number of points.\")\n\n    # Extract x and y coordinates from the input array\n    x = array[:, 0]\n    y = array[:, 1]\n\n    # Calculate the area using the Shoelace formula\n    area = 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the expected shape (_, 2)\n    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    # Extract the x and y coordinates of the polygon points\n    x = array[:, 0]\n    y = array[:, 1]\n\n    # Implement the Shoelace formula\n    area = 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    S1 = np.sum(x * np.roll(y, -1))\n    S2 = np.sum(y * np.roll(x, -1))\n\n    area = np.abs(S1 - S2) / 2\n\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    # Implementation of Shoelace formula\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the expected shape\n    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    # Apply the Shoelace formula\n    x = array[:, 0]\n    y = array[:, 1]\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2).\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    # Calculate the area using the Shoelace formula\n    area = (\n        0.5\n        * np.abs(\n            np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1))\n        )\n    )\n\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n    S1 = np.sum(x * np.roll(y, -1))\n    S2 = np.sum(y * np.roll(x, -1))\n    A = 0.5 * np.abs(S1 - S2)\n\n    return A\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\n            f\"The input array must have shape (_, 2), but got {array.shape}.\"\n        )\n    x = array[:, 0]\n    y = array[:, 1]\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v)\n    idx_lo = torch.clamp(idx_lo, 0, len(a) - 1)\n    idx_hi = torch.clamp(idx_lo + 1, 0, len(a) - 1)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    assert a.shape[:-1] == v.shape[:-1]\n\n    idx_lo = torch.zeros(a.shape[:-1], dtype=torch.int64, device=a.device)\n    idx_hi = torch.zeros(a.shape[:-1], dtype=torch.int64, device=a.device)\n\n    for i in range(a.shape[-1]):\n        idx_lo = idx_lo * (a[..., i] < v[..., i]) + (a.shape[-1] - 1) * (a[..., i] >= v[..., i])\n        idx_hi = idx_hi * (a[..., i] <= v[..., i]) + (a.shape[-1] - 1) * (a[..., i] > v[..., i])\n\n    return idx_lo, idx_hi\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    assert a.shape[-1] == v.shape[-1], \"a and v must have the same last dimension\"\n    assert a.ndim == v.ndim, \"a and v must have the same number of dimensions\"\n    assert a.is_contiguous(), \"a must be contiguous\"\n    assert v.is_contiguous(), \"v must be contiguous\"\n\n    idx_lo = torch.searchsorted(a, v, side=\"left\")\n    idx_hi = torch.searchsorted(a, v, side=\"right\")\n\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    assert a.shape[-1] == v.shape[-1]\n    assert a.shape[-1] == 1\n\n    a = a.flatten()\n    v = v.flatten()\n\n    idx_lo = torch.searchsorted(a, v)\n    idx_hi = torch.searchsorted(a, v, side=\"right\")\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if a and v have the same shape or if the last dimension of v is 1\n    if a.shape == v.shape or v.shape[-1] == 1:\n        # If so, use the original function\n        return torch.searchsorted(a, v)\n    # If not, reshape v to match the shape of a\n    v = v.reshape(a.shape)\n    # Use the original function to find the indices\n    return torch.searchsorted(a, v)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the input tensors have the same shape\n    if a.shape != v.shape:\n        raise ValueError(\"The input tensors must have the same shape.\")\n\n    # Check if the input tensors have at least one dimension\n    if len(a.shape) < 1:\n        raise ValueError(\"The input tensors must have at least one dimension.\")\n\n    # Check if the last dimension of the input tensors is compatible\n    if a.shape[-1] != v.shape[-1]:\n        raise ValueError(\"The last dimension of the input tensors must be compatible.\")\n\n    # Check if the input tensors are sorted\n    if not torch.all(a[:-1] <= a[1:]):\n        raise ValueError(\"The input tensors must be sorted in ascending order.\")\n\n    # Check if the input tensors have the same dtype\n    if a.dtype != v.dtype:\n        raise ValueError(\"The input tensors must have the same dtype.\")\n\n    # Check if the input tensors have the same device\n    if a.device != v.device:\n        raise ValueError(\"The input tensors must be on the same device.\")\n\n    # Check if the input tensors have the same requires_grad\n    if a.requires_grad != v.requires_grad:\n        raise ValueError(\"The input tensors must have the same requires_grad.\")\n\n    # Check if the input tensors have the same grad_fn\n    if a.grad_fn != v.grad_fn:\n        raise ValueError(\"The input tensors must have the same grad_fn.\")\n\n    # Check if the input tensors have the same grad\n    if a.grad is not None and v.grad is not None:\n        if a.grad.shape != v.grad.shape:\n            raise ValueError(\"The input tensors must have the same grad shape.\")\n        if a.grad.dtype != v.grad.dtype:\n            raise ValueError(\"The input tensors must have the same grad dtype.\")\n        if a.grad.device != v.grad.device:\n            raise ValueError(\""}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if a and v are valid tensors\n    assert isinstance(a, torch.Tensor) and isinstance(v, torch.Tensor), \"a and v must be valid tensors\"\n\n    # Check if a and v have the same device\n    assert a.device == v.device, \"a and v must have the same device\"\n\n    # Check if a and v have the same dtype\n    assert a.dtype == v.dtype, \"a and v must have the same dtype\"\n\n    # Check if a is sorted in ascending order\n    assert torch.all(a[1:] >= a[:-1]), \"a must be sorted in ascending order\"\n\n    # Check if a and v have the same number of dimensions\n    assert a.ndim == v.ndim, \"a and v must have the same number of dimensions\"\n\n    # Check if the last dimension of a and v are compatible\n    assert a.shape[-1] == v.shape[-1], \"the last dimension of a and v must be compatible\"\n\n    # Check if a and v have the same shape except for the last dimension\n    assert a.shape[:-1] == v.shape[:-1], \"a and v must have the same shape except for the last dimension\"\n\n    # Check if a and v are contiguous\n    assert a.is_contiguous() and v.is_contiguous(), \"a and v must be contiguous\"\n\n    # Check if a and v have the same memory layout\n    assert a.stride() == v.stride(), \"a and v must have the same memory layout\"\n\n    # Check if a and v have the same size\n    assert a.numel() == v.numel(), \"a and v must have the same size\"\n\n    # Check if a and v have the same stride\n    assert a.stride() == v.stride(), \"a and v must have the same stride\"\n\n    # Check if a and v have the same storage offset\n    assert a.storage_offset() == v.storage_offset(), \"a and v must have the same storage offset\"\n\n    #"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the input tensors are contiguous\n    if not a.is_contiguous():\n        raise ValueError(\"The input tensor a must be contiguous.\")\n    if not v.is_contiguous():\n        raise ValueError(\"The input tensor v must be contiguous.\")\n\n    # Check if the input tensors have the same device\n    if a.device != v.device:\n        raise ValueError(\"The input tensors a and v must be on the same device.\")\n\n    # Check if the input tensors have the same dtype\n    if a.dtype != v.dtype:\n        raise ValueError(\"The input tensors a and v must have the same dtype.\")\n\n    # Check if the input tensors have the same number of dimensions\n    if a.ndim != v.ndim:\n        raise ValueError(\"The input tensors a and v must have the same number of dimensions.\")\n\n    # Check if the input tensors have the same size in all but the last dimension\n    if a.shape[:-1] != v.shape[:-1]:\n        raise ValueError(\"The input tensors a and v must have the same size in all but the last dimension.\")\n\n    # Check if the input tensors have a last dimension of size greater than 0\n    if a.shape[-1] < 1 or v.shape[-1] < 1:\n        raise ValueError(\"The input tensors a and v must have a last dimension of size greater than 0.\")\n\n    # Check if the input tensors have the same dtype\n    if a.dtype != v.dtype:\n        raise ValueError(\"The input tensors a and v must have the same dtype.\")\n\n    # Check if the input tensors are sorted\n    if not torch.all(a[..., 1:] >= a[..., :-1]):\n        raise ValueError(\"The input tensor a must be sorted in ascending order.\")\n\n    # Check if the input tensors have the same device\n    if a.device != v.device:\n        raise ValueError(\"The input tensors a and v must be on the same device.\")\n\n   "}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if a is sorted\n    assert torch.all(a[1:] >= a[:-1]), \"a must be sorted in ascending order\"\n\n    # Get the shape of a and v\n    a_shape = a.shape\n    v_shape = v.shape\n\n    # Get the last dimension of a and v\n    a_last_dim = a_shape[-1]\n    v_last_dim = v_shape[-1]\n\n    # Get the number of dimensions of a and v\n    a_ndim = len(a_shape)\n    v_ndim = len(v_shape)\n\n    # Check if the last dimension of a and v are equal\n    if a_last_dim != v_last_dim:\n        raise ValueError(\"Last dimension of a and v must be equal\")\n\n    # Check if the number of dimensions of a and v are equal\n    if a_ndim != v_ndim:\n        raise ValueError(\"Number of dimensions of a and v must be equal\")\n\n    # Check if the shape of a and v except the last dimension are equal\n    if a_shape[:-1] != v_shape[:-1]:\n        raise ValueError(\"Shape of a and v except the last dimension must be equal\")\n\n    # Get the number of elements in a and v\n    a_numel = a.numel()\n    v_numel = v.numel()\n\n    # Check if the number of elements in a and v are equal\n    if a_numel != v_numel:\n        raise ValueError(\"Number of elements in a and v must be equal\")\n\n    # Check if a and v are contiguous\n    if not a.is_contiguous() or not v.is_contiguous():\n        raise ValueError(\"a and v must be contiguous\")\n\n    # Check if a and v are on the same device\n    if a.device != v.device:\n        raise ValueError(\"a and v must be on the same device\")\n\n    # Check if a and v are of the same dtype\n    if a.dtype != v.dtype"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Get the shape of a and v\n    a_shape = a.shape\n    v_shape = v.shape\n\n    # If the last dimension of v is different from that of a, reshape v to match a\n    if a_shape[-1] != v_shape[-1]:\n        v_shape = v_shape[:-1] + (a_shape[-1],)\n        v = v.reshape(v_shape)\n\n    # Create a new tensor with the same shape as v and fill it with the last element of a\n    v_last = torch.full(v_shape, a[-1], dtype=a.dtype, device=a.device)\n\n    # Find the indices where each element of v should be inserted into a\n    idx_lo = torch.searchsorted(a, v)\n    idx_hi = torch.searchsorted(a, v_last)\n\n    # Return the indices\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    if v.shape[-1] != a.shape[-1]:\n        raise ValueError(\"The last dimension of v must match the last dimension of a.\")\n\n    if v.shape[0] == 0:\n        return torch.zeros_like(v, dtype=torch.int64), torch.zeros_like(v, dtype=torch.int64)\n\n    if a.shape[0] == 0:\n        return torch.arange(v.shape[0], dtype=torch.int64), torch.arange(v.shape[0], dtype=torch.int64)\n\n    # Get the indices where the elements of v would be inserted into a\n    idx = torch.searchsorted(a, v)\n\n    # Adjust the indices to ensure that the elements are inserted between two elements of a\n    idx_lo = torch.max(torch.zeros_like(idx), idx - 1)\n    idx_hi = torch.min(torch.full_like(idx, a.shape[0] - 1), idx)\n\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the input tensors are contiguous and have the same dtype\n    assert a.is_contiguous() and v.is_contiguous() and a.dtype == v.dtype\n\n    # Get the last dimension of the input tensors\n    a_last_dim = a.shape[-1]\n    v_last_dim = v.shape[-1]\n\n    # Check if the last dimension of the input tensors are compatible\n    assert a_last_dim == v_last_dim\n\n    # Get the shape of the input tensors\n    a_shape = a.shape\n    v_shape = v.shape\n\n    # Check if the shape of the input tensors are compatible\n    assert a_shape[:-1] == v_shape[:-1]\n\n    # Check if the number of dimensions of the input tensors are compatible\n    assert len(a_shape) == len(v_shape)\n\n    # Check if the number of dimensions of the input tensors is greater than 1\n    assert len(a_shape) > 1\n\n    # Get the number of dimensions of the input tensors\n    ndim = len(a_shape)\n\n    # Get the number of elements in the input tensors\n    numel = a.numel()\n\n    # Check if the number of elements in the input tensors is greater than 0\n    assert numel > 0\n\n    # Get the number of elements in the input tensors\n    numel = a.numel()\n\n    # Check if the number of elements in the input tensors is greater than 0\n    assert numel > 0\n\n    # Get the number of elements in the input tensors\n    numel = a.numel()\n\n    # Check if the number of elements in the input tensors is greater than 0\n    assert numel > 0\n\n    # Get the number of elements in the input tensors\n    numel = a.numel()\n\n    # Check if the number of elements in the input tensors is greater than 0\n    assert numel > 0\n\n    # Get the number of elements in the input tensors\n    numel = a.numel"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v)\n    idx_hi = torch.maximum(idx_lo, torch.ones_like(idx_lo))\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    assert a.shape[:-1] == v.shape[:-1], \"The shape of a and v must match except for the last dimension.\"\n\n    # Get the last dimension of a and v\n    last_dim_a = a.shape[-1]\n    last_dim_v = v.shape[-1]\n\n    # Create a new tensor with the same shape as v and fill it with the last element of a\n    last_element_a = torch.full((*v.shape[:-1], 1), a[-1])\n    v_extended = torch.cat((v, last_element_a), dim=-1)\n\n    # Find the indices where v_extended is greater than a\n    idx_lo = torch.searchsorted(a, v_extended)\n\n    # Find the indices where v_extended is greater than or equal to a\n    idx_hi = torch.searchsorted(a, v_extended, right=True)\n\n    # Clamp the indices to the valid range\n    idx_lo = torch.clamp(idx_lo, 0, last_dim_a - 1)\n    idx_hi = torch.clamp(idx_hi, 0, last_dim_a - 1)\n\n    # Return the indices\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.zeros_like(v[..., 0], dtype=torch.long)\n    idx_hi = torch.zeros_like(v[..., 0], dtype=torch.long)\n    n_dim = v.shape[-1]\n    for i in range(n_dim):\n        idx_lo = idx_lo * 2\n        idx_hi = idx_hi * 2 + 1\n        mid = (a[..., i : i + 1] <= v[..., i : i + 1]).long()\n        idx_lo = idx_lo + mid\n        idx_hi = idx_hi + mid\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the input tensors have the same number of dimensions\n    assert a.ndim == v.ndim\n\n    # Check if the last dimension of the input tensors are compatible\n    assert a.shape[-1] == v.shape[-1]\n\n    # Get the number of dimensions of the input tensors\n    ndim = a.ndim\n\n    # Check if the input tensors have at least one dimension\n    assert ndim > 0\n\n    # Get the shape of the input tensors\n    shape = a.shape\n\n    # Check if the input tensors have at least one element in the last dimension\n    assert shape[-1] > 0\n\n    # Get the shape of the input tensors without the last dimension\n    shape = shape[:-1]\n\n    # Check if the input tensors have at least one element in all dimensions except the last dimension\n    assert all(s > 0 for s in shape)\n\n    # Get the number of elements in the last dimension of the input tensors\n    n = shape[-1]\n\n    # Check if the input tensors have at least two elements in the last dimension\n    assert n > 1\n\n    # Get the last dimension of the input tensors\n    last_dim = shape[-1]\n\n    # Check if the input tensors have at least one element in all dimensions except the last dimension\n    assert all(s > 0 for s in shape)\n\n    # Get the last dimension of the input tensors\n    last_dim = shape[-1]\n\n    # Check if the input tensors have at least one element in the last dimension\n    assert last_dim > 0\n\n    # Get the shape of the input tensors without the last dimension\n    shape = shape[:-1]\n\n    # Check if the input tensors have at least one element in all dimensions except the last dimension\n    assert all(s > 0 for s in shape)\n\n    # Get the number of elements in the last dimension of the input tensors\n    n = shape[-1]\n\n    # Check if the input tensors have at least two elements in the last dimension\n    assert n > 1\n\n    # Get the last dimension of the input tensors\n    last_dim ="}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the input tensors are 1D and have the same device\n    assert a.ndim == 1 and v.ndim == 1 and a.device == v.device\n\n    # Get the size of the input tensors\n    n = a.size(0)\n    m = v.size(0)\n\n    # Initialize the output tensors with zeros\n    idx_lo = torch.zeros(m, dtype=torch.int64, device=a.device)\n    idx_hi = torch.zeros(m, dtype=torch.int64, device=a.device)\n\n    # Initialize the indices for the binary search\n    lo = 0\n    hi = n - 1\n\n    # Iterate over the query points\n    for i in range(m):\n        # Get the current query point\n        v_i = v[i]\n\n        # Binary search to find the insertion indices\n        while lo <= hi:\n            mid = (lo + hi) // 2\n            if a[mid] < v_i:\n                lo = mid + 1\n            else:\n                hi = mid - 1\n\n        # Update the output indices\n        idx_lo[i] = lo\n        idx_hi[i] = hi\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the tensors are contiguous.\n    assert a.is_contiguous()\n    assert v.is_contiguous()\n\n    # Check if the tensors have the same number of dimensions.\n    assert a.ndim == v.ndim\n\n    # Check if the tensors have the same size in all dimensions except the last one.\n    for i in range(a.ndim - 1):\n        assert a.shape[i] == v.shape[i]\n\n    # Check if the tensors have at least two dimensions.\n    assert a.ndim >= 2\n\n    # Check if the tensors have the same dtype.\n    assert a.dtype == v.dtype\n\n    # Check if the tensors have the same device.\n    assert a.device == v.device\n\n    # Get the shape of the tensors.\n    a_shape = a.shape\n    v_shape = v.shape\n\n    # Get the number of dimensions of the tensors.\n    ndim = a.ndim\n\n    # Get the number of elements in the last dimension of the tensors.\n    n = a_shape[-1]\n\n    # Get the number of query points.\n    m = v_shape[-1]\n\n    # Get the number of batches.\n    b = v_shape[:-1]\n\n    # Get the strides of the tensors.\n    a_strides = a.stride()\n    v_strides = v.stride()\n\n    # Get the size of the last dimension of the tensors.\n    n_size = a_shape[-1]\n    m_size = v_shape[-1]\n\n    # Get the size of the last dimension of the tensors.\n    n_size = a_shape[-1]\n    m_size = v_shape[-1]\n\n    # Get the size of the last dimension of the tensors.\n    n_size = a_shape[-1]\n    m_size = v_shape[-1]\n\n    # Get the size of the last dimension of the tensors.\n    n_size = a_shape[-1]"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    assert a.shape[-1] == v.shape[-1], \"The last dimension of a and v should be the same.\"\n    assert len(a.shape) == len(v.shape), \"The number of dimensions of a and v should be the same.\"\n    assert (a[..., :-1] <= a[..., 1:]).all(), \"a should be sorted in ascending order.\"\n\n    # Get the indices where v should be inserted into a\n    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n\n    # Clamp the indices to the range of a\n    idx_lo = torch.clamp(idx_lo, min=0, max=a.shape[-2] - 1)\n    idx_hi = torch.clamp(idx_hi, min=0, max=a.shape[-2] - 1)\n\n    # If an element in v is out of the range of a, set idx_lo and idx_hi to the first or last index of a\n    idx_lo = torch.where(idx_lo == a.shape[-2], idx_lo - 1, idx_lo)\n    idx_hi = torch.where(idx_hi == 0, idx_hi + 1, idx_hi)\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    assert a.shape[-1] == v.shape[-1], \"Last dimension of a and v must match\"\n    assert (a[..., :-1] <= a[..., 1:]).all(), \"a must be sorted\"\n    assert (v[..., :-1] <= v[..., 1:]).all(), \"v must be sorted\"\n\n    # create a new tensor with the same shape as a\n    # and fill it with the values of a\n    a_new = torch.zeros_like(a)\n    a_new[..., :] = a[..., :]\n\n    # find the indices where v should be inserted into a\n    # by comparing each element of v with the last element of a\n    # if v is larger than the last element of a,\n    # set the index to the length of a\n    idx_lo = torch.searchsorted(a_new, v)\n    idx_lo = torch.where(idx_lo >= a.shape[-1], a.shape[-1] - 1, idx_lo)\n\n    # find the indices where v should be inserted into a\n    # by comparing each element of v with the first element of a\n    # if v is smaller than the first element of a,\n    # set the index to 0\n    idx_hi = torch.searchsorted(a_new, v)\n    idx_hi = torch.where(idx_hi == 0, 0, idx_hi - 1)\n\n    # return the indices\n    return idx_lo, idx_hi\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  K = xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n  return K\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  K = xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n  return K\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1]\n  ])\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1]\n  ])\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n"}
{"namespace": "coord.contract", "completion": "  x_sq = np.sum(x**2, axis=1, keepdims=True)\n  x_sq_scaled = x_sq / (1 + x_sq)\n  x_scaled = x * x_sq_scaled / np.sqrt(x_sq)\n\n  return x_scaled"}
{"namespace": "coord.contract", "completion": "  x_sq = x**2\n  x_sq_sum = x_sq.sum(axis=-1, keepdims=True)\n  x_sq_sum_exp = np.exp(x_sq_sum)\n  x_mag = np.sqrt(x_sq_sum_exp)\n  x_mag_inv = 1/x_mag\n  x_mag_inv_sq = x_mag_inv**2\n  x_mag_inv_sq_sum = x_mag_inv_sq.sum(axis=-1, keepdims=True)\n  x_mag_inv_sq_sum_exp = np.exp(x_mag_inv_sq_sum)\n  x_mag_inv_sq_sum_exp_inv = 1/x_mag_inv_sq_sum_exp\n  x_mag_inv_sq_sum_exp_inv_sq = x_mag_inv_sq_sum_exp_inv**2\n  x_mag_inv_sq_sum_exp_inv_sq_sum = x_mag_inv_sq_sum_exp_inv_sq.sum(axis=-1, keepdims=True)\n  x_mag_inv_sq_sum_exp_inv_sq_sum_exp = np.exp(x_mag_inv_sq_sum_exp_inv_sq_sum)\n  x_mag_inv_sq_sum_exp_inv_sq_sum_exp_inv = 1/x_mag_inv_sq_sum_exp_inv_sq_sum_exp\n  x_mag_inv_sq_sum_exp_inv_sq_sum_exp_inv_sq = x_mag_inv_sq_sum_exp_inv_sq_sum_exp_inv**2\n  x_mag_inv_sq_sum_exp_inv_sq_sum_exp_inv_sq_sum = x_mag_inv_sq_sum_exp_inv_sq_sum_exp_inv_sq.sum(axis=-1, keepdims=True)\n  x_mag_inv_sq_sum_exp_inv_sq_sum_exp_"}
{"namespace": "coord.contract", "completion": "  x_squared = np.sum(x**2, axis=1)\n  x_scaled = x / (1 + x_squared)\n\n  return x_scaled\n"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = np.sum(x**2, axis=1)\n  x_mag_sq = np.reshape(x_mag_sq, (x_mag_sq.shape[0], 1))\n  x_mag_sq = np.tile(x_mag_sq, (1, x.shape[1]))\n  x_mag_sq = np.sqrt(x_mag_sq)\n  x_mag_sq = np.reshape(x_mag_sq, (x_mag_sq.shape[0], 1))\n  x_mag_sq = np.tile(x_mag_sq, (1, x.shape[1]))\n  x = x / x_mag_sq\n\n  return x\n\n"}
{"namespace": "coord.contract", "completion": "  x_sq = x**2\n  mag_sq = x_sq.sum(axis=-1, keepdims=True)\n  return x * (mag_sq / (mag_sq + 1))\n"}
{"namespace": "coord.contract", "completion": "  x_squared = np.square(x)\n  x_sum = np.sum(x_squared, axis=-1, keepdims=True)\n  x_sqrt = np.sqrt(x_sum)\n  x_sqrt_squared = np.square(x_sqrt)\n  x_sqrt_cubed = x_sqrt_squared * x_sqrt\n  x_sqrt_quartic = x_sqrt_cubed * x_sqrt\n  x_sqrt_quintic = x_sqrt_quartic * x_sqrt\n  x_sqrt_sextic = x_sqrt_quintic * x_sqrt\n  x_sqrt_septic = x_sqrt_sextic * x_sqrt\n  x_sqrt_octic = x_sqrt_septic * x_sqrt\n  x_sqrt_nonic = x_sqrt_octic * x_sqrt\n  x_sqrt_decic = x_sqrt_nonic * x_sqrt\n  x_sqrt_undecic = x_sqrt_decic * x_sqrt\n  x_sqrt_duodecic = x_sqrt_undecic * x_sqrt\n  x_sqrt_tredecic = x_sqrt_duodecic * x_sqrt\n  x_sqrt_quattuordecic = x_sqrt_tredecic * x_sqrt\n  x_sqrt_quindecic = x_sqrt_quattuordecic * x_sqrt\n  x_sqrt_sexdecic = x_sqrt_quindecic * x_sqrt\n  x_sqrt_septdecic = x_sqrt_sexdecic * x_sqrt\n  x_sqrt_octdecic = x_sqrt_septdecic * x_sqrt\n  x_sqrt_novemdecic = x_sqrt_octdecic * x_sqrt\n  x_sqrt_vigintic = x_sqrt_novemdecic * x_sqrt\n  x_sqrt_unvigintic = x_sqrt_vigintic * x_sqrt\n  x_sqrt_duovigintic = x_sqrt_un"}
{"namespace": "coord.contract", "completion": "  x_sq = x**2\n  x_sq_sum = x_sq.sum(axis=1)\n  x_sq_sum_sqrt = np.sqrt(x_sq_sum)\n  x_sq_sum_sqrt_1m = 1 - x_sq_sum_sqrt\n  x_sq_sum_sqrt_1m_3d = np.expand_dims(x_sq_sum_sqrt_1m, axis=1)\n  x_sq_sum_sqrt_1m_3d_tiled = np.tile(x_sq_sum_sqrt_1m_3d, (1, x.shape[1]))\n  x_contracted = x_sq_sum_sqrt_1m_3d_tiled * x\n\n  return x_contracted"}
{"namespace": "coord.contract", "completion": "  return x / (1 + (x**2).sum(axis=1, keepdims=True)**0.5)\n"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = np.sum(x**2, axis=1)\n  x_mag_sq = x_mag_sq.reshape((x_mag_sq.shape[0],1))\n  x_mag_sq = np.tile(x_mag_sq, (1, x.shape[1]))\n  x_mag_sq = x_mag_sq/(1+x_mag_sq)\n  x_mag_sq = np.sqrt(x_mag_sq)\n  x = x*x_mag_sq\n\n  return x\n"}
{"namespace": "coord.contract", "completion": "  x_squared = x ** 2\n  magnitude_squared = x_squared.sum(axis=-1, keepdims=True)\n  x_scaled = x * ((1 - magnitude_squared) ** 2 / (magnitude_squared + 1))\n\n  return x_scaled\n"}
{"namespace": "coord.contract", "completion": "  x_squared = x**2\n  magnitude_squared = x_squared.sum(axis=-1, keepdims=True)\n  return x * (((2 * magnitude_squared) / (1 + magnitude_squared)) ** 2)\n\n"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = np.sum(x**2, axis=1)\n  return x / np.sqrt(1 + x_mag_sq)\n"}
{"namespace": "coord.contract", "completion": "  x_magnitude_squared = np.sum(x**2, axis=1)\n  x_scaled = x / (1 + x_magnitude_squared)\n  return x_scaled\n"}
{"namespace": "coord.contract", "completion": "  x_sq = np.sum(np.square(x), axis=1)\n  x_sq_sqrt = np.sqrt(x_sq)\n  x_sq_sqrt = np.expand_dims(x_sq_sqrt, axis=1)\n  x_sq_sqrt = np.tile(x_sq_sqrt, [1, x.shape[1]])\n  x_sq_sqrt = np.where(x_sq_sqrt == 0, 1, x_sq_sqrt)\n  x = np.divide(x, x_sq_sqrt)\n  return x\n\n"}
{"namespace": "coord.contract", "completion": "  x_sq = x**2\n  x_sq_sum = np.sum(x_sq, axis=-1, keepdims=True)\n  y = x * (x_sq_sum / (1 + x_sq_sum))**2\n\n  return y\n"}
{"namespace": "coord.contract", "completion": "  x_mag = np.linalg.norm(x, axis=1)\n  x_mag = np.expand_dims(x_mag, axis=1)\n  return x * (1 - 1 / (1 + x_mag ** 2))\n"}
{"namespace": "coord.contract", "completion": "  x_squared = x**2\n  x_squared_sum = x_squared.sum(axis=-1)\n  x_squared_sum_power = x_squared_sum**(0.5)\n  x_squared_sum_power_scaled = x_squared_sum_power**(-2)\n  x_scaled = x_squared_sum_power_scaled[..., np.newaxis] * x\n\n  return x_scaled"}
{"namespace": "coord.contract", "completion": "  x_squared = np.square(x)\n  x_squared_summed = np.sum(x_squared, axis=-1)\n  x_squared_summed_safe = np.where(x_squared_summed == 0, np.amin(x_squared_summed), x_squared_summed)\n  x_squared_summed_safe = np.where(x_squared_summed_safe < 1, x_squared_summed_safe, np.sqrt(x_squared_summed_safe))\n  x_scaled = x / x_squared_summed_safe[:, np.newaxis]\n  return x_scaled\n\n"}
{"namespace": "coord.contract", "completion": "  x_magnitude_squared = np.sum(x**2, axis=-1, keepdims=True)\n  return x * (1 - 1 / (1 + x_magnitude_squared))\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input points\n  mag_sq = np.sum(x**2, axis=1)\n\n  # Scale the points based on the magnitude squared and a specific mathematical formula\n  return x / (1 + mag_sq)**0.5\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    unit_index = 0\n\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n\n    return f\"{num_bytes:.2f} {units[unit_index]}\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    unit_index = 0\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    unit_index = 0\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    unit_index = 0\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n    unit_index = 0\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    index = 0\n    while num_bytes >= 1000 and index < len(units) - 1:\n        num_bytes /= 1000\n        index += 1\n    return f\"{num_bytes:.2f} {units[index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    unit_index = 0\n\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    unit_index = 0\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    unit_index = 0\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    unit_index = 0\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    index = 0\n    while num_bytes >= 1000 and index < len(units) - 1:\n        num_bytes /= 1000\n        index += 1\n    return f\"{num_bytes:.2f} {units[index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n    unit_index = 0\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    unit_index = 0\n\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    index = 0\n    while num_bytes >= 1000 and index < len(units) - 1:\n        num_bytes /= 1000\n        index += 1\n    return f\"{num_bytes:.2f} {units[index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n    unit_index = 0\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    unit_index = 0\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n    unit_index = 0\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    unit_index = 0\n\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    unit_index = 0\n\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    index = 0\n    while num_bytes >= 1000 and index < len(units) - 1:\n        num_bytes /= 1000\n        index += 1\n    return f\"{num_bytes:.2f} {units[index]}\"\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls, v, field):\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must have {nb_dimensions} dimensions, not {v.ndim}\"\n            )\n        return v\n\n    return validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls, v, field):\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{field.name} must be {nb_dimensions}D array\")\n        return v\n\n    return validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_dimensions(cls, v, field):\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"Array must have {nb_dimensions} dimensions.\")\n        return v\n\n    return validate_array_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_ndim(cls, v, field):\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{field.name} must be {nb_dimensions}D array\")\n        return v\n\n    return validate_array_ndim"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: Type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"Array must have {nb_dimensions} dimensions, but has {v.ndim} dimensions.\"\n            )\n        return v\n\n    return validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_dimensionality(cls, v, field):\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must have {nb_dimensions} dimensions. Got {v.ndim} dimensions.\"\n            )\n        return v\n\n    return validate_array_dimensionality\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_n_dimensions(cls, v, field):\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{field.name} must have {nb_dimensions} dimensions.\")\n        return v\n\n    return validate_array_n_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_dimensionality(cls, v: np.ndarray, field: fields.ModelField):\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"Array must have {nb_dimensions} dimensions, not {v.ndim}.\"\n            )\n        return v\n\n    return validate_array_dimensionality"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_ndimensions(cls, v, field):\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must have {nb_dimensions} dimensions, but has {v.ndim} dimensions\"\n            )\n        return v\n\n    return validate_array_ndimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_dimensions(cls, v, field):\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{field.name} must have {nb_dimensions} dimensions.\")\n        return v\n\n    return validate_array_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_dimensions(cls, v, field):\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be {nb_dimensions}D array. Got {v.ndim}D array instead.\"\n            )\n        return v\n\n    return validate_array_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_ndimensions(cls, v, field):\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be a {nb_dimensions}-dimensional array.\"\n            )\n        return v\n\n    return validate_array_ndimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: Type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.alias} must be a {nb_dimensions}D array. Received {v.ndim}D array.\"\n            )\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_ndim(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must have {nb_dimensions} dimensions, not {v.ndim}\"\n            )\n        return v\n\n    return validator(field.name, allow_reuse=True)(validate_array_ndim)\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: Type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.alias} must have {nb_dimensions} dimensions. \"\n                f\"{v.ndim} dimensions found.\"\n            )\n\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: Type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be {nb_dimensions}-dimensional. The input array has {v.ndim} dimensions.\"\n            )\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_n_dimensions(cls, v, field):\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"The array must have {nb_dimensions} dimensions. Received array with {v.ndim} dimensions.\"\n            )\n        return v\n\n    return validate_array_n_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_dimensions(cls, v: np.ndarray, field: fields.ModelField):\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"Array must have {nb_dimensions} dimensions. Given array has {v.ndim} dimensions.\"\n            )\n        return v\n\n    return validator(field.name, allow_reuse=True)(validate_array_dimensions)\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: Type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.alias} must be a {nb_dimensions}-dimensional array. \"\n                f\"Found an array with {v.ndim} dimensions.\"\n            )\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _is_array_n_dimensions(cls, v, field):\n\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"The number of dimensions of the {field.name} array must be {nb_dimensions}. The array passed has {v.ndim} dimensions.\"\n            )\n\n        return v\n\n    return _is_array_n_dimensions\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = onp.sqrt(x**2 + y**2 + z**2)\n  theta = onp.arccos(z / (r + eps))\n  phi = onp.arctan2(y, x)\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = onp.linalg.norm(cartesian_vector, axis = -1)\n  theta = onp.arccos(onp.clip(z / (r + eps), -1.0, 1.0))\n  phi = onp.arctan2(y, x)\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = onp.sqrt(x ** 2 + y ** 2 + z ** 2)\n  theta = onp.arctan2(onp.sqrt(x ** 2 + y ** 2), z)\n  phi = onp.arctan2(y, x)\n  theta = onp.where(theta < eps, eps, theta)\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  radius = onp.sqrt(x**2 + y**2 + z**2)\n  theta = onp.arccos(z / (radius + eps))\n  phi = onp.arctan2(y, x)\n\n  return radius, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(jnp.clip(z/jnp.maximum(r, eps), -1, 1))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = onp.linalg.norm(cartesian_vector, axis = -1)\n  theta = onp.arccos(onp.clip(z / (r + eps), -1., 1.))\n  phi = onp.arctan2(y, x)\n\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = onp.sqrt(x ** 2 + y ** 2 + z ** 2)\n  theta = onp.arccos(onp.clip(z / (r + eps), -1, 1))\n  phi = onp.arctan2(y, x)\n\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Convert the input to a numpy array\n  cartesian_vector = onp.asarray(cartesian_vector)\n\n  # Extract the x, y, and z coordinates\n  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  # Calculate the radius\n  r = onp.sqrt(x**2 + y**2 + z**2)\n\n  # Calculate the inclination angle\n  theta = onp.arccos(z / (r + eps))\n\n  # Calculate the azimuth angle\n  phi = onp.arctan2(y, x)\n\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = onp.sqrt(x**2 + y**2 + z**2)\n  theta = onp.arctan2(onp.sqrt(x**2 + y**2), z)\n  phi = onp.arctan2(y, x)\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = onp.sqrt(x**2 + y**2 + z**2)\n  theta = onp.arctan2(onp.sqrt(x**2 + y**2), z)\n  phi = onp.arctan2(y, x)\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = onp.sqrt(x ** 2 + y ** 2 + z ** 2)\n  theta = onp.arccos(onp.clip(z / r, -1, 1))\n  phi = onp.arctan2(y, x)\n\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  radius = onp.sqrt(x**2 + y**2 + z**2)\n\n  theta = onp.arccos(z / (radius + eps))\n\n  phi = onp.arctan2(y, x)\n\n  return radius, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = onp.linalg.norm(cartesian_vector, axis = -1)\n  theta = onp.arccos(onp.clip(z / r, -1.0, 1.0))\n  phi = onp.arctan2(y, x)\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = onp.linalg.norm(cartesian_vector, axis = -1)\n\n  theta = onp.arccos(z / onp.maximum(r, eps))\n\n  phi = onp.arctan2(y, x)\n\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Calculate the radius\n  r = onp.linalg.norm(cartesian_vector, axis=-1)\n\n  # Calculate the inclination (theta)\n  theta = onp.arccos(onp.clip(cartesian_vector[..., 2] / (r + eps), -1, 1))\n\n  # Calculate the azimuth (phi)\n  phi = onp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = onp.sqrt(x**2 + y**2 + z**2)\n  theta = onp.arctan2(onp.sqrt(x**2 + y**2), z)\n  phi = onp.arctan2(y, x)\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  radius = onp.sqrt(x**2 + y**2 + z**2)\n  theta = onp.arctan2(onp.sqrt(x**2 + y**2), z)\n  phi = onp.arctan2(y, x)\n\n  return radius, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.sqrt(x ** 2 + y ** 2 + z ** 2)\n  theta = jnp.arctan2(jnp.sqrt(x ** 2 + y ** 2), z)\n  phi = jnp.arctan2(y, x)\n  phi = jnp.where(phi < 0, phi + 2 * jnp.pi, phi)\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Convert the input to an array\n  cartesian_vector = onp.asarray(cartesian_vector)\n\n  # Check the shape of the input array\n  if cartesian_vector.shape[-1] != 3:\n    raise ValueError(\"The last dimension of the input array must be 3, corresponding to the x, y, and z coordinates.\")\n\n  # Extract the x, y, and z coordinates\n  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  # Calculate the radius\n  r = onp.sqrt(x**2 + y**2 + z**2)\n\n  # Calculate the inclination angle (theta)\n  theta = onp.arccos(z / (r + eps))\n\n  # Calculate the azimuth angle (phi)\n  phi = onp.arctan2(y, x)\n\n  # Return the spherical coordinates\n  return r, theta, phi\n\n"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n    from rouge import Rouge\n\n    def tokenize(text):\n        return list(jieba.cut(text))\n\n    rouge = Rouge()\n    scores = rouge.get_scores(continuation, reference, avg=True)\n    return scores['rouge-l']['f']\n\n"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n    from rouge import Rouge\n\n    def tokenize(text):\n        return list(jieba.cut(text))\n\n    rouge = Rouge()\n    scores = rouge.get_scores(continuation, reference, avg=True)\n    return scores['rouge-l']['f']\n\n"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n    from rouge import Rouge\n\n    def tokenize(text):\n        return list(jieba.cut(text))\n\n    rouge = Rouge()\n    scores = rouge.get_scores(continuation, reference, avg=True)\n    return scores['rouge-l']['f']\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Importing the necessary libraries\n    import jieba\n    from rouge import Rouge\n\n    # Tokenizing the generated text and reference text using jieba\n    continuation_tokens = list(jieba.cut(continuation))\n    reference_tokens = list(jieba.cut(reference))\n\n    # Computing the ROUGE-L score\n    rouge = Rouge()\n    scores = rouge.get_scores(continuation_tokens, reference_tokens)\n\n    # Returning the ROUGE-L score\n    return scores[0]['rouge-l']['f']"}
{"namespace": "common.rougeL_score", "completion": "    # imports\n    import jieba\n    from rouge import Rouge\n\n    # tokenize the texts using jieba\n    continuation_tokens = list(jieba.cut(continuation))\n    reference_tokens = list(jieba.cut(reference))\n\n    # compute the ROUGE-L score\n    rouge = Rouge()\n    scores = rouge.get_scores(continuation_tokens, reference_tokens)\n\n    # extract the ROUGE-L score\n    rouge_l_score = scores[0]['rouge-l']['f']\n\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Import the necessary libraries\n    import jieba\n    from rouge import Rouge\n\n    # Define the custom tokenizer function\n    def tokenizer(text):\n        return list(jieba.cut(text))\n\n    # Initialize the Rouge object with the custom tokenizer\n    rouge = Rouge(tokenizer=tokenizer)\n\n    # Compute the ROUGE-L score between the generated text and the reference text\n    scores = rouge.get_scores(continuation, reference)\n\n    # Extract the ROUGE-L score from the scores dictionary\n    rouge_l_score = scores[0]['rouge-l']['f']\n\n    # Return the ROUGE-L score\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Importing the necessary libraries\n    import jieba\n    from rouge import Rouge\n\n    # Tokenizing the continuation and reference texts using the custom tokenizer function\n    continuation_tokens = list(jieba.cut(continuation))\n    reference_tokens = list(jieba.cut(reference))\n\n    # Computing the ROUGE-L score using the Rouge class from the rouge library\n    rouge = Rouge()\n    scores = rouge.get_scores(continuation_tokens, reference_tokens, avg=True)\n\n    # Extracting the ROUGE-L score from the scores dictionary\n    rouge_l_score = scores[\"rouge-l\"][\"f\"]\n\n    # Returning the ROUGE-L score\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    from rouge import Rouge\n    from jieba import lcut\n\n    def tokenize(text: str) -> list:\n        return [token for token in lcut(text) if token]\n\n    rouge = Rouge()\n    scores = rouge.get_scores(continuation, reference, avg=True, ignore_empty=True)\n    return scores[\"rouge-l\"][\"f\"]\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Tokenize the continuation and reference text using the custom tokenizer function\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Compute the ROUGE-L score\n    rouge_score = rouge_l_sentence_level.score(reference_tokens, continuation_tokens)\n\n    # Return the ROUGE-L score\n    return rouge_score"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n    from rouge import Rouge\n\n    def tokenize(text):\n        return list(jieba.cut(text))\n\n    rouge = Rouge()\n    scores = rouge.get_scores(continuation, reference, avg=True)\n    return scores[\"rouge-l\"][\"f\"]\n"}
{"namespace": "common.rougeL_score", "completion": "    # Importing the necessary libraries\n    import jieba\n    import numpy as np\n    import re\n\n    # Defining a custom tokenizer function based on jieba\n    def tokenize(text):\n        return list(jieba.cut(text))\n\n    # Tokenizing the generated text and reference text\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Computing the length of the generated text and reference text\n    continuation_length = len(continuation_tokens)\n    reference_length = len(reference_tokens)\n\n    # Computing the length of the longest common subsequence between the generated text and reference text\n    lcs_length = np.zeros((continuation_length + 1, reference_length + 1))\n    for i in range(continuation_length + 1):\n        for j in range(reference_length + 1):\n            if i == 0 or j == 0:\n                lcs_length[i, j] = 0\n            elif continuation_tokens[i - 1] == reference_tokens[j - 1]:\n                lcs_length[i, j] = lcs_length[i - 1, j - 1] + 1\n            else:\n                lcs_length[i, j] = max(lcs_length[i - 1, j], lcs_length[i, j - 1])\n\n    # Computing the ROUGE-L score\n    rouge_l_score = lcs_length[continuation_length, reference_length] / reference_length\n\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Importing the necessary libraries\n    import jieba\n    from rouge import Rouge\n\n    # Tokenizing the reference and continuation text using jieba\n    reference_tokens = jieba.lcut(reference)\n    continuation_tokens = jieba.lcut(continuation)\n\n    # Calculating the ROUGE-L score using the Rouge library\n    rouge = Rouge()\n    scores = rouge.get_scores(continuation_tokens, reference_tokens)\n\n    # Extracting the ROUGE-L score from the scores dictionary\n    rouge_l_score = scores[0]['rouge-l']['f']\n\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n    import numpy as np\n\n    def tokenize(text: str) -> list:\n        \"\"\"\n        Tokenizes the input text using the jieba library.\n\n        Input Arguments\n        :param text: str, The text to be tokenized.\n        :return: list, A list of tokens obtained from the input text.\n        \"\"\"\n        return list(jieba.cut(text))\n\n    def lcs(X: list, Y: list) -> int:\n        \"\"\"\n        Computes the length of the longest common subsequence (LCS) between two input lists.\n\n        Input Arguments\n        :param X: list, The first input list.\n        :param Y: list, The second input list.\n        :return: int, The length of the LCS between the two input lists.\n        \"\"\"\n        m = len(X)\n        n = len(Y)\n        L = [[None] * (n + 1) for i in range(m + 1)]\n        for i in range(m + 1):\n            for j in range(n + 1):\n                if i == 0 or j == 0:\n                    L[i][j] = 0\n                elif X[i - 1] == Y[j - 1]:\n                    L[i][j] = L[i - 1][j - 1] + 1\n                else:\n                    L[i][j] = max(L[i - 1][j], L[i][j - 1])\n        return L[m][n]\n\n    def rouge_l_score(continuation: str, reference: str) -> float:\n        \"\"\"\n        Computes the ROUGE-L score between a generated text continuation and a reference text.\n\n        Input Arguments\n        :param continuation: str, The generated text whose quality is to be evaluated.\n        :param reference: str, The reference text against which the generated text is compared.\n        :return: float, The ROUGE-L score indicating the quality of the generated text in relation to the reference text.\n        \"\"\"\n        token_continuation = tokenize(continuation)"}
{"namespace": "common.rougeL_score", "completion": "    # Import the necessary libraries\n    import jieba\n    from rouge import Rouge\n\n    # Tokenize the continuation and reference texts using the custom tokenizer function\n    continuation_tokens = list(jieba.cut(continuation))\n    reference_tokens = list(jieba.cut(reference))\n\n    # Compute the ROUGE-L score between the tokenized continuation and reference texts\n    rouge = Rouge()\n    scores = rouge.get_scores(continuation_tokens, reference_tokens)\n\n    # Extract the ROUGE-L score from the scores dictionary and return it\n    return scores[0][\"rouge-l\"][\"f\"]\n\n"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n    import numpy as np\n\n    # Tokenize the continuation and reference texts using jieba\n    continuation_tokens = jieba.lcut(continuation)\n    reference_tokens = jieba.lcut(reference)\n\n    # Compute the length of the continuation and reference texts\n    continuation_length = len(continuation_tokens)\n    reference_length = len(reference_tokens)\n\n    # Compute the length of the longest common subsequence between the two texts\n    lcs_length = len(np.intersect1d(continuation_tokens, reference_tokens))\n\n    # Compute the ROUGE-L score\n    if continuation_length == 0 or reference_length == 0:\n        rouge_l_score = 0.0\n    else:\n        rouge_l_score = lcs_length / continuation_length\n\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n\n    def tokenize(text):\n        return list(jieba.cut(text))\n\n    def rouge_l_score(continuation, reference):\n        continuation_tokens = tokenize(continuation)\n        reference_tokens = tokenize(reference)\n\n        # Compute the length of the longest common subsequence\n        lcs_length = 0\n        dp = [[0] * (len(reference_tokens) + 1) for _ in range(len(continuation_tokens) + 1)]\n\n        for i in range(1, len(continuation_tokens) + 1):\n            for j in range(1, len(reference_tokens) + 1):\n                if continuation_tokens[i - 1] == reference_tokens[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                else:\n                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n\n                lcs_length = max(lcs_length, dp[i][j])\n\n        # Compute the ROUGE-L score\n        precision = lcs_length / len(continuation_tokens)\n        recall = lcs_length / len(reference_tokens)\n        f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n\n        return f1_score\n\n    return rouge_l_score(continuation, reference)\n\n"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n    import numpy as np\n\n    def tokenize(text):\n        return list(jieba.cut(text))\n\n    def compute_lcs_matrix(X, Y):\n        m, n = len(X), len(Y)\n        lcs_matrix = np.zeros((m + 1, n + 1))\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if X[i - 1] == Y[j - 1]:\n                    lcs_matrix[i, j] = lcs_matrix[i - 1, j - 1] + 1\n                else:\n                    lcs_matrix[i, j] = max(lcs_matrix[i - 1, j], lcs_matrix[i, j - 1])\n        return lcs_matrix\n\n    def compute_rougeL_score(continuation, reference):\n        X = tokenize(continuation)\n        Y = tokenize(reference)\n        lcs_matrix = compute_lcs_matrix(X, Y)\n        lcs_length = lcs_matrix[-1, -1]\n        precision = lcs_length / len(Y)\n        recall = lcs_length / len(X)\n        f1_score = 2 * precision * recall / (precision + recall)\n        return f1_score\n\n    return compute_rougeL_score(continuation, reference)\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Import required libraries\n    import jieba\n    from rouge import Rouge\n\n    # Define custom tokenizer function\n    def tokenizer(text: str) -> list:\n        \"\"\"\n        Custom tokenizer function that uses jieba to split text into tokens.\n\n        Input-Output Arguments\n        :param text: str, The text to be tokenized.\n        :return: list, The list of tokens obtained from the text.\n        \"\"\"\n        return list(jieba.cut(text))\n\n    # Initialize Rouge object with custom tokenizer\n    rouge = Rouge(tokenizer=tokenizer)\n\n    # Compute ROUGE-L score\n    scores = rouge.get_scores(continuation, reference)\n\n    # Extract ROUGE-L score\n    rouge_l_score = scores[0][\"rouge-l\"][\"f\"]\n\n    # Return ROUGE-L score\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Tokenize the continuation and reference texts using the custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the ROUGE-L score\n    rouge_l_score = rouge_l_score_function(continuation_tokens, reference_tokens)\n\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Importing the necessary libraries\n    import jieba\n    from rouge import Rouge\n\n    # Tokenizing the continuation and reference texts using jieba\n    continuation_tokens = jieba.cut(continuation)\n    reference_tokens = jieba.cut(reference)\n\n    # Computing the ROUGE-L score\n    rouge = Rouge()\n    scores = rouge.get_scores(' '.join(continuation_tokens), ' '.join(reference_tokens))\n\n    # Returning the ROUGE-L score\n    return scores[0]['rouge-l']['f']\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_standard(name)\n    except Exception as e:\n        return locate_fallback(name)\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_object(name)\n    except:\n        return locate_object_fallback(name)\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Attempt to locate the object using the standard method\n    try:\n        return locate(name)\n    except ImportError:\n        # Attempt to locate the object using the fallback method\n        try:\n            module_name, class_name = name.rsplit(\".\", 1)\n            module = import_module(module_name)\n            return getattr(module, class_name)\n        except (ImportError, AttributeError):\n            raise ImportError(f\"Could not locate object with name {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Try to locate the object using the standard method\n    try:\n        obj = locate(name)\n        if obj is not None:\n            return obj\n    except Exception as e:\n        pass\n\n    # Try to locate the object using a fallback method\n    try:\n        obj = fallback_locate(name)\n        if obj is not None:\n            return obj\n    except Exception as e:\n        pass\n\n    # If the object cannot be located by either method, raise an exception\n    raise Exception(f\"Could not locate object with name {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_standard(name)\n    except:\n        return locate_fallback(name)\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Try to locate the object using the standard method\n    try:\n        return locate(name)\n    except (ValueError, ImportError, AttributeError):\n        pass\n\n    # If the object cannot be located using the standard method, try to locate it using a fallback method\n    try:\n        return fallback_locate(name)\n    except (ValueError, ImportError, AttributeError):\n        pass\n\n    # If the object cannot be located by either method, raise an exception\n    raise ValueError(f\"Could not locate object with name '{name}'\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_standard(name)\n    except (ModuleNotFoundError, AttributeError):\n        return locate_fallback(name)\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Attempt to locate the object using the standard method\n    try:\n        return locate_standard(name)\n\n    # If the object cannot be located using the standard method, attempt to locate it using the fallback method\n    except Exception as e:\n        try:\n            return locate_fallback(name)\n\n        # If the object cannot be located using either method, raise an exception\n        except Exception as e:\n            raise Exception(f\"Failed to locate object with name '{name}'\") from e\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_standard(name)\n    except ImportError:\n        return locate_fallback(name)\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Split the name into module and class name components\n    module_name, class_name = name.rsplit(\".\", 1)\n\n    # Try to locate the module and class using the standard method\n    try:\n        module = __import__(module_name, fromlist=[class_name])\n        cls = getattr(module, class_name)\n        return cls\n    except (ImportError, AttributeError):\n        pass\n\n    # Try to locate the module and class using a fallback method\n    try:\n        module = __import__(module_name)\n        cls = getattr(module, class_name)\n        return cls\n    except (ImportError, AttributeError):\n        pass\n\n    # Raise an exception if the object cannot be located by either method\n    raise ValueError(f\"Unable to locate object with name '{name}'\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_standard(name)\n    except:\n        return locate_fallback(name)\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Split the fully qualified name into module and class names\n    module_name, class_name = name.rsplit(\".\", 1)\n\n    # Attempt to locate the object using the standard method\n    try:\n        module = importlib.import_module(module_name)\n        obj = getattr(module, class_name)\n        return obj\n\n    # If the object cannot be located using the standard method, attempt to locate it using a fallback method\n    except (ImportError, AttributeError):\n        # Split the module name into its components\n        components = module_name.split(\".\")\n\n        # Attempt to locate the object using a fallback method\n        for i in range(len(components), 0, -1):\n            try:\n                module_name = \".\".join(components[:i])\n                module = importlib.import_module(module_name)\n                obj = getattr(module, class_name)\n                return obj\n            except (ImportError, AttributeError):\n                pass\n\n    # If the object cannot be located by either method, raise an exception\n    raise ValueError(f\"Could not locate object with name '{name}'\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Try to locate the object using the standard method\n    try:\n        return locate(name)\n\n    # If the object cannot be located using the standard method, try to locate it using the fallback method\n    except (AttributeError, ImportError):\n        return fallback_locate(name)\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_standard(name)\n    except:\n        return locate_fallback(name)\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Split the name into module and class name\n    module_name, class_name = name.rsplit(\".\", 1)\n\n    # Try to locate the object using the standard method\n    try:\n        module = importlib.import_module(module_name)\n        obj = getattr(module, class_name)\n        return obj\n    except (ImportError, AttributeError):\n        pass\n\n    # Try to locate the object using the fallback method\n    try:\n        obj = locate_fallback(name)\n        return obj\n    except Exception as e:\n        raise ValueError(f\"Could not locate object with name '{name}': {e}\") from e\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_standard(name)\n    except Exception as e:\n        return locate_fallback(name)\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Split the name into module, submodule, and class_name\n    module_name, class_name = name.rsplit(\".\", 1)\n\n    # Try to import the module\n    try:\n        module = importlib.import_module(module_name)\n    except ImportError:\n        # If the module cannot be imported, try to locate the object using a fallback method\n        module = fallback_locate(module_name)\n        if module is None:\n            raise ImportError(f\"Could not locate module {module_name}\")\n\n    # Try to get the class from the module\n    try:\n        cls = getattr(module, class_name)\n    except AttributeError:\n        # If the class cannot be found, try to locate the object using a fallback method\n        cls = fallback_locate(f\"{module_name}.{class_name}\")\n        if cls is None:\n            raise AttributeError(f\"Could not locate class {class_name} in module {module_name}\")\n\n    return cls\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Split the name into module and class name\n    module_name, class_name = name.rsplit(\".\", 1)\n\n    # Attempt to locate the object using the standard method\n    try:\n        return getattr(importlib.import_module(module_name), class_name)\n    except (ModuleNotFoundError, AttributeError):\n        pass\n\n    # Attempt to locate the object using the fallback method\n    try:\n        return locate_fallback(name)\n    except Exception as e:\n        raise e\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_standard(name)\n    except:\n        return locate_fallback(name)\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_standard(name)\n    except Exception as e:\n        return locate_fallback(name)\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the buffer\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    import io\n\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n    return module\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    import io\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n    return module\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    import io\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the lengths of ids and scores match\n    if len(ids) != len(scores):\n        raise ValueError(\"The lengths of ids and scores must match.\")\n\n    # Check if the sum of weights is equal to 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of weights must equal 1.\")\n\n    # Check if the lengths of ids and weights match\n    if len(ids) != len(weights):\n        raise ValueError(\"The lengths of ids and weights must match.\")\n\n    # Check if the lengths of scores and weights match\n    if len(scores) != len(weights):\n        raise ValueError(\"The lengths of scores and weights must match.\")\n\n    # Check if the lengths of ids and scores match\n    if len(ids) != len(scores):\n        raise ValueError(\"The lengths of ids and scores must match.\")\n\n    # Check if the lengths of ids and weights match\n    if len(ids) != len(weights):\n        raise ValueError(\"The lengths of ids and weights must match.\")\n\n    # Check if the lengths of scores and weights match\n    if len(scores) != len(weights):\n        raise ValueError(\"The lengths of scores and weights must match.\")\n\n    # Check if the lengths of ids and scores match\n    if len(ids) != len(scores):\n        raise ValueError(\"The lengths of ids and scores must match.\")\n\n    # Check if the lengths of ids and weights match\n    if len(ids) != len(weights):\n        raise ValueError(\"The lengths of ids and weights must match.\")\n\n    # Check if the lengths of scores and weights match\n    if len(scores) != len(weights):\n        raise ValueError(\"The lengths of scores and weights must match.\")\n\n    # Check if the lengths of ids and scores match\n    if len(ids) != len(scores):\n        raise ValueError(\"The lengths of ids and scores must match.\")\n\n    # Check if the lengths of ids and weights match\n    if len(ids)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the lengths of the ids and scores tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\"The lengths of the ids and scores tuples must match.\")\n\n    # Check that the sum of the weights equals 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    # Check that the length of the weights tuple matches the length of the ids and scores tuples\n    if len(weights) != len(ids):\n        raise ValueError(\"The length of the weights tuple must match the length of the ids and scores tuples.\")\n\n    # Normalize the scores of each retrieval result\n    normalized_scores = []\n    for i in range(len(scores)):\n        max_score = max(scores[i])\n        min_score = min(scores[i])\n        normalized_scores.append([(score - min_score) / (max_score - min_score) for score in scores[i]])\n\n    # Combine the normalized scores using the provided weights\n    combined_scores = [sum([normalized_scores[j][i] * weights[j] for j in range(len(weights))]) for i in range(len(normalized_scores[0]))]\n\n    # Select the top_k results based on the combined scores\n    top_indices = sorted(range(len(combined_scores)), key=lambda i: combined_scores[i], reverse=True)[:top_k]\n    top_ids = [ids[i][j] for i in range(len(ids)) for j in top_indices]\n    top_scores = [combined_scores[i] for i in top_indices]\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Normalize the scores\n    scores = [normalize(s) for s in scores]\n\n    # Combine the scores using the convex combination method\n    fused_scores = [sum([w * s for w, s in zip(weights, scores_list)]) for scores_list in zip(*scores)]\n\n    # Select the top k results\n    top_indices = sorted(range(len(fused_scores)), key=lambda i: fused_scores[i], reverse=True)[:top_k]\n\n    # Extract the top k ids and scores\n    top_ids = [ids[i][j] for i, j in enumerate(top_indices)]\n    top_scores = [fused_scores[i] for i in top_indices]\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the lengths of ids and scores match\n    if len(ids) != len(scores):\n        raise ValueError(\"The lengths of ids and scores must match.\")\n\n    # Check if the lengths of ids and weights match\n    if len(ids) != len(weights):\n        raise ValueError(\"The lengths of ids and weights must match.\")\n\n    # Check if the sum of weights is equal to 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of weights must equal 1.\")\n\n    # Check if top_k is positive\n    if top_k <= 0:\n        raise ValueError(\"top_k must be positive.\")\n\n    # Check if the lengths of ids and scores are equal\n    if len(ids) != len(scores):\n        raise ValueError(\"The lengths of ids and scores must be equal.\")\n\n    # Check if the lengths of ids and weights are equal\n    if len(ids) != len(weights):\n        raise ValueError(\"The lengths of ids and weights must be equal.\")\n\n    # Check if the sum of weights is equal to 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of weights must equal 1.\")\n\n    # Check if top_k is positive\n    if top_k <= 0:\n        raise ValueError(\"top_k must be positive.\")\n\n    # Normalize the scores\n    normalized_scores = []\n    for i in range(len(scores)):\n        max_score = max(scores[i])\n        min_score = min(scores[i])\n        normalized_scores.append([(score - min_score) / (max_score - min_score) for score in scores[i]])\n\n    # Combine the scores using the convex combination method\n    combined_scores = [sum([normalized_scores[j][i] * weights[j] for j in range(len(scores))]) for i in range(len(scores[0]))]\n\n    # Select the top_k results\n    top_indices = sorted("}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights), \"The lengths of ids, scores, and weights must be equal.\"\n    assert sum(weights) == 1, \"The sum of weights must equal 1.\"\n\n    # Normalize the scores\n    normalized_scores = []\n    for score, weight in zip(scores, weights):\n        normalized_scores.append(score * weight)\n\n    # Combine the scores\n    combined_scores = sum(normalized_scores)\n\n    # Sort the scores in descending order\n    sorted_indices = np.argsort(combined_scores)[::-1]\n\n    # Select the top k results\n    top_indices = sorted_indices[:top_k]\n\n    # Get the top k ids and scores\n    top_ids = [ids[i][top_indices] for i in range(len(ids))]\n    top_scores = [scores[i][top_indices] for i in range(len(scores))]\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the lengths of ids and scores match\n    if len(ids) != len(scores):\n        raise ValueError(\"The lengths of ids and scores must match.\")\n\n    # Check if the sum of weights is equal to 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of weights must equal 1.\")\n\n    # Check if the length of weights matches the length of ids and scores\n    if len(weights) != len(ids):\n        raise ValueError(\"The length of weights must match the length of ids and scores.\")\n\n    # Normalize the scores\n    normalized_scores = []\n    for i in range(len(scores)):\n        normalized_scores.append([score / max(scores[i]) for score in scores[i]])\n\n    # Combine the scores using the convex combination method\n    combined_scores = []\n    for i in range(len(ids[0])):\n        combined_score = 0\n        for j in range(len(ids)):\n            combined_score += normalized_scores[j][i] * weights[j]\n        combined_scores.append(combined_score)\n\n    # Select the top k results\n    top_indices = sorted(range(len(combined_scores)), key=lambda i: combined_scores[i], reverse=True)[:top_k]\n    top_ids = [ids[i][index] for i, index in enumerate(top_indices)]\n    top_scores = [combined_scores[index] for index in top_indices]\n\n    return top_ids, top_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights)\n    assert sum(weights) == 1\n\n    # Normalize the scores\n    norm_scores = []\n    for s in scores:\n        norm_scores.append([score / sum(s) for score in s])\n\n    # Combine the scores\n    combined_scores = []\n    for i in range(len(ids[0])):\n        combined_score = 0\n        for j in range(len(ids)):\n            combined_score += norm_scores[j][i] * weights[j]\n        combined_scores.append(combined_score)\n\n    # Select the top k results\n    top_indices = sorted(range(len(combined_scores)), key=lambda i: combined_scores[i], reverse=True)[:top_k]\n\n    # Return the fused results\n    fused_ids = [ids[i][j] for i in range(len(ids)) for j in top_indices]\n    fused_scores = [combined_scores[j] for j in top_indices]\n\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights)\n    assert sum(weights) == 1\n\n    # Normalize the scores\n    norm_scores = [normalize_scores(s) for s in scores]\n\n    # Combine the scores using the convex combination method\n    fused_scores = [sum(s * w for s, w in zip(norm_scores, weights)) for s in zip(*norm_scores)]\n\n    # Sort the fused scores in descending order\n    sorted_indices = sorted(range(len(fused_scores)), key=lambda i: fused_scores[i], reverse=True)\n\n    # Select the top k results\n    top_indices = sorted_indices[:top_k]\n    top_ids = [ids[i][j] for i, j in enumerate(top_indices)]\n    top_scores = [fused_scores[i] for i in top_indices]\n\n    return top_ids, top_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # check that the length of the ids and scores tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\"The length of the ids and scores tuples must match.\")\n\n    # check that the length of the weights tuple matches the length of the ids and scores tuples\n    if len(weights) != len(ids):\n        raise ValueError(\"The length of the weights tuple must match the length of the ids and scores tuples.\")\n\n    # check that the sum of the weights is equal to 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    # check that the top_k parameter is a positive integer\n    if top_k <= 0:\n        raise ValueError(\"The top_k parameter must be a positive integer.\")\n\n    # normalize the scores\n    normalized_scores = [\n        [score / sum(score) for score in scores_list] for scores_list in scores\n    ]\n\n    # multiply the normalized scores by the weights\n    weighted_scores = [\n        [score * weight for score in scores_list]\n        for scores_list, weight in zip(normalized_scores, weights)\n    ]\n\n    # sum the weighted scores\n    fused_scores = [\n        sum(scores_list) for scores_list in zip(*weighted_scores)\n    ]\n\n    # select the top k ids and scores\n    top_indices = sorted(range(len(fused_scores)),\n                         key=lambda i: fused_scores[i],\n                         reverse=True)[:top_k]\n    fused_ids = [ids[i][j] for i, j in enumerate(top_indices)]\n    fused_scores = [fused_scores[i] for i in top_indices]\n\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Normalize scores\n    normalized_scores = [\n        [score / sum(score_list) for score in score_list]\n        for score_list in scores\n    ]\n\n    # Combine scores\n    combined_scores = [\n        sum(score * weight for score, weight in zip(score_list, weights))\n        for score_list in zip(*normalized_scores)\n    ]\n\n    # Sort and select top k results\n    sorted_results = sorted(zip(ids[0], combined_scores), key=lambda x: x[1], reverse=True)[:top_k]\n    fused_ids, fused_scores = zip(*sorted_results)\n\n    return fused_ids, fused_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the lengths of the ids and scores tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\"The lengths of the ids and scores tuples must match.\")\n\n    # Check that the sum of the weights equals 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    # Check that the lengths of the ids and scores tuples match the length of the weights tuple\n    if len(weights) != len(ids):\n        raise ValueError(\"The lengths of the ids, scores, and weights tuples must match.\")\n\n    # Initialize an empty dictionary to store the combined scores for each id\n    combined_scores = {}\n\n    # Loop through each retrieval result\n    for i in range(len(ids)):\n        # Loop through each id and score in the current retrieval result\n        for j in range(len(ids[i])):\n            # If the id is not already in the combined scores dictionary, add it with a score of 0\n            if ids[i][j] not in combined_scores:\n                combined_scores[ids[i][j]] = 0\n            # Add the weighted score for the current retrieval result to the combined score for the current id\n            combined_scores[ids[i][j]] += scores[i][j] * weights[i]\n\n    # Sort the combined scores dictionary by score in descending order\n    sorted_scores = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n\n    # Select the top_k ids and scores from the sorted scores dictionary\n    top_ids = [x[0] for x in sorted_scores[:top_k]]\n    top_scores = [x[1] for x in sorted_scores[:top_k]]\n\n    # Return the top_ids and top_scores as a tuple\n    return top_ids, top_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Normalize the scores\n    normalized_scores = []\n    for score_list in scores:\n        min_score = min(score_list)\n        max_score = max(score_list)\n        normalized_score_list = [(score - min_score) / (max_score - min_score) for score in score_list]\n        normalized_scores.append(normalized_score_list)\n\n    # Combine the scores using the convex combination method\n    combined_scores = [0] * len(ids[0])\n    for i in range(len(ids[0])):\n        for j in range(len(ids)):\n            combined_scores[i] += normalized_scores[j][i] * weights[j]\n\n    # Select the top_k results\n    top_indices = sorted(range(len(combined_scores)), key=lambda i: combined_scores[i], reverse=True)[:top_k]\n    top_ids = [ids[i][j] for i in range(len(ids)) for j in top_indices]\n    top_scores = [combined_scores[i] for i in top_indices]\n\n    return top_ids, top_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Normalize the scores\n    normalized_scores = []\n    for score in scores:\n        normalized_scores.append(\n            [(s - min(score)) / (max(score) - min(score)) for s in score])\n\n    # Compute the weighted sum of the normalized scores\n    weighted_scores = []\n    for i in range(len(normalized_scores[0])):\n        weighted_score = 0\n        for j in range(len(normalized_scores)):\n            weighted_score += normalized_scores[j][i] * weights[j]\n        weighted_scores.append(weighted_score)\n\n    # Sort the scores in descending order and select the top k\n    sorted_indices = sorted(range(len(weighted_scores)),\n                            key=lambda i: weighted_scores[i], reverse=True)\n    top_indices = sorted_indices[:top_k]\n\n    # Select the top k ids and scores\n    top_ids = [ids[i][j] for i in range(len(ids)) for j in top_indices]\n    top_scores = [weighted_scores[i] for i in top_indices]\n\n    return top_ids, top_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the length of ids and scores tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\"The length of ids and scores tuples must match.\")\n\n    # Check if the length of ids and weights tuples match\n    if len(ids) != len(weights):\n        raise ValueError(\"The length of ids and weights tuples must match.\")\n\n    # Check if the sum of weights is equal to 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of weights must equal 1.\")\n\n    # Initialize an empty list to store the fused scores\n    fused_scores = []\n\n    # Loop through each retrieval result\n    for i in range(len(ids)):\n\n        # Normalize the scores of the current retrieval result\n        current_scores = np.array(scores[i])\n        current_scores = current_scores / np.max(current_scores)\n\n        # Multiply the normalized scores by the corresponding weight\n        current_scores = current_scores * weights[i]\n\n        # Append the fused scores to the fused_scores list\n        fused_scores.append(current_scores)\n\n    # Sum the fused scores across all retrieval results\n    fused_scores = np.sum(fused_scores, axis=0)\n\n    # Select the top_k ids based on the fused scores\n    top_k_indices = np.argsort(fused_scores)[::-1][:top_k]\n    top_k_ids = [ids[i][j] for i, j in enumerate(top_k_indices)]\n    top_k_scores = [fused_scores[j] for j in top_k_indices]\n\n    # Return the top_k ids and scores\n    return top_k_ids, top_k_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the length of ids and scores match\n    if len(ids) != len(scores):\n        raise ValueError(\"The length of ids and scores must match.\")\n\n    # Check if the length of weights matches the length of ids and scores\n    if len(weights) != len(ids):\n        raise ValueError(\"The length of weights must match the length of ids and scores.\")\n\n    # Check if the sum of weights is equal to 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of weights must equal 1.\")\n\n    # Check if top_k is a positive integer\n    if not isinstance(top_k, int) or top_k <= 0:\n        raise ValueError(\"top_k must be a positive integer.\")\n\n    # Normalize the scores\n    normalized_scores = [\n        [score / sum(score) for score in scores_list]\n        for scores_list in scores\n    ]\n\n    # Compute the weighted scores\n    weighted_scores = [\n        [weight * scores_list[i] for scores_list in normalized_scores]\n        for i in range(len(ids[0]))\n    ]\n\n    # Compute the fused scores\n    fused_scores = [sum(scores) for scores in weighted_scores]\n\n    # Sort the fused scores and get the indices\n    sorted_indices = sorted(range(len(fused_scores)),\n                            key=lambda i: fused_scores[i],\n                            reverse=True)\n\n    # Select the top_k ids and scores\n    top_k_indices = sorted_indices[:top_k]\n    top_k_ids = [[ids[i][j] for i in range(len(ids))]\n                 for j in top_k_indices]\n    top_k_scores = [fused_scores[i] for i in top_k_indices]\n\n    return top_k_ids, top_k_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Normalize the scores\n    norm_scores = [np.array(score) / np.sum(score) for score in scores]\n\n    # Compute the weighted sum of the normalized scores\n    weighted_scores = np.sum([weight * norm_score for weight, norm_score in zip(weights, norm_scores)], axis=0)\n\n    # Sort the ids based on the weighted scores in descending order\n    sorted_indices = np.argsort(weighted_scores)[::-1]\n    sorted_ids = [ids[i][sorted_indices] for i in range(len(ids))]\n    sorted_scores = [weighted_scores[sorted_indices] for i in range(len(ids))]\n\n    # Select the top k ids and scores\n    top_ids = [sorted_ids[i][:top_k] for i in range(len(sorted_ids))]\n    top_scores = [sorted_scores[i][:top_k] for i in range(len(sorted_scores))]\n\n    return top_ids, top_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the length of the ids and scores tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\"The length of ids and scores tuples must match.\")\n\n    # Check if the length of the weights tuple matches the length of the ids and scores tuples\n    if len(weights) != len(ids):\n        raise ValueError(\"The length of weights tuple must match the length of ids and scores tuples.\")\n\n    # Check if the sum of the weights equals 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    # Initialize an empty list to store the fused scores\n    fused_scores = []\n\n    # Loop over the ids and scores of each retrieval result\n    for i in range(len(ids)):\n        # Normalize the scores of the current retrieval result\n        normalized_scores = [float(i) / sum(scores[i]) for i in scores[i]]\n\n        # Multiply the normalized scores by the corresponding weight\n        weighted_scores = [normalized_scores[j] * weights[i] for j in range(len(normalized_scores))]\n\n        # Append the weighted scores to the fused_scores list\n        fused_scores.append(weighted_scores)\n\n    # Sum the weighted scores of each retrieval result\n    fused_scores = [sum(x) for x in zip(*fused_scores)]\n\n    # Sort the fused scores in descending order and select the top_k results\n    top_k_indices = sorted(range(len(fused_scores)), key=lambda i: fused_scores[i], reverse=True)[:top_k]\n\n    # Initialize an empty list to store the fused ids\n    fused_ids = []\n\n    # Loop over the ids and scores of each retrieval result\n    for i in range(len(ids)):\n        # Select the top_k ids based on the top_k_"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights), \"Lengths of ids, scores, and weights must be equal.\"\n    assert sum(weights) == 1, \"Sum of weights must be equal to 1.\"\n\n    # Normalize the scores\n    normalized_scores = [\n        [score / max(s) for score in s] for s in scores\n    ]\n\n    # Weight the normalized scores\n    weighted_scores = [\n        [weight * score for score in s] for weight, s in zip(weights, normalized_scores)\n    ]\n\n    # Sum the weighted scores\n    summed_scores = [sum(s) for s in zip(*weighted_scores)]\n\n    # Get the top k indices\n    top_k_indices = sorted(range(len(summed_scores)), key=lambda i: summed_scores[i], reverse=True)[:top_k]\n\n    # Select the top k ids and scores\n    top_k_ids = [[ids[i][j] for i in range(len(ids))] for j in top_k_indices]\n    top_k_scores = [[scores[i][j] for i in range(len(scores))] for j in top_k_indices]\n\n    return top_k_ids, top_k_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights)\n    assert sum(weights) == 1\n\n    # Normalize scores\n    normalized_scores = [\n        [score / max(result) for score in result] for result in scores\n    ]\n\n    # Combine scores using convex combination\n    combined_scores = [\n        sum(weight * normalized_score for weight, normalized_score in zip(weights, result))\n        for result in zip(*normalized_scores)\n    ]\n\n    # Select top k results\n    top_k_indices = sorted(range(len(combined_scores)), key=lambda i: combined_scores[i], reverse=True)[:top_k]\n    top_k_ids = [[ids[i][j] for j in top_k_indices] for i in range(len(ids))]\n    top_k_scores = [[scores[i][j] for j in top_k_indices] for i in range(len(scores))]\n\n    return top_k_ids, top_k_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Normalize scores\n    scores = [normalize_scores(scores_i) for scores_i in scores]\n\n    # Combine scores using convex combination\n    scores_combined = [sum([weights[i] * scores_i for i in range(len(scores))]) for scores_i in zip(*scores)]\n\n    # Sort scores and ids based on the combined scores\n    sorted_scores, sorted_ids = zip(*sorted(zip(scores_combined, ids), reverse=True))\n\n    # Select top k ids and scores\n    top_k_scores = sorted_scores[:top_k]\n    top_k_ids = sorted_ids[:top_k]\n\n    return top_k_ids, top_k_scores\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n    elif x < 0.01:\n        return \"<0.01\"\n    elif x < 1:\n        return \"{:.2f}\".format(x)\n    elif x < 10:\n        return \"{:.2f}\".format(x)\n    elif x < 100:\n        return \"{:.1f}\".format(x)\n    elif x < 1000:\n        return \"{:.0f}\".format(x)\n    elif x < 10000:\n        return \"{:.0f}\".format(x)\n    elif x < 100000:\n        return \"{:.0f}\".format(x)\n    elif x < 1000000:\n        return \"{:.0f}\".format(x)\n    elif x < 10000000:\n        return \"{:.0f}\".format(x)\n    elif x < 100000000:\n        return \"{:.0f}\".format(x)\n    elif x < 1000000000:\n        return \"{:.0f}\".format(x)\n    elif x < 10000000000:\n        return \"{:.0f}\".format(x)\n    elif x < 100000000000:\n        return \"{:.0f}\".format(x)\n    elif x < 1000000000000:\n        return \"{:.0f}\".format(x)\n    elif x < 10000000000000:\n        return \"{:.0f}\".format(x)\n    elif x < 100000000000000:\n        return \"{:.0f}\".format(x)\n    elif x < 1000000000000000:\n        return \"{:.0f}\".format(x)\n    elif x < 100"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x is None:\n        return \"NaN\"\n    elif x == 0:\n        return \"0\"\n    elif x < 0.001:\n        return \"< 0.001\"\n    elif x < 1:\n        return f\"{x:.3f}\"\n    elif x < 10:\n        return f\"{x:.2f}\"\n    elif x < 100:\n        return f\"{x:.1f}\"\n    else:\n        if percent:\n            return f\"{x*100:.0f}%\"\n        else:\n            return f\"{x:.0f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n    if percent:\n        x *= 100\n    if x < 1e-2:\n        return f\"{x:.2e}\"\n    elif x < 0.1:\n        return f\"{x:.3f}\"\n    elif x < 1:\n        return f\"{x:.2f}\"\n    elif x < 1e2:\n        return f\"{x:.1f}\"\n    elif x < 1e4:\n        return f\"{x:.0f}\"\n    else:\n        return f\"{x:.2e}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x == x:\n        if percent:\n            x *= 100\n            return f\"{x:.2f}%\"\n        else:\n            if x < 1:\n                return f\"{x:.4f}\"\n            else:\n                return f\"{x:.2f}\"\n    else:\n        return str(x)"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n    else:\n        if percent:\n            x = x * 100\n        if x < 1:\n            return \"{:.2f}\".format(x)\n        elif x < 10:\n            return \"{:.1f}\".format(x)\n        elif x < 100:\n            return \"{:.0f}\".format(x)\n        elif x < 1000:\n            return \"{:.0f}\".format(x)\n        else:\n            return \"{:.0f}\".format(x)"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n    elif x == 0:\n        return \"0.00\"\n    elif x < 0.001:\n        return \"< 0.001\"\n    elif x < 1:\n        return f\"{x:.3f}\"\n    elif x < 10:\n        return f\"{x:.2f}\"\n    elif x < 100:\n        return f\"{x:.1f}\"\n    elif percent:\n        return f\"{x*100:.1f}%\"\n    else:\n        return f\"{x:.0f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n    if percent:\n        x *= 100\n    if abs(x) >= 10000000000:\n        return '{:.3f}B'.format(x / 10000000000)\n    if abs(x) >= 1000000000:\n        return '{:.3f}B'.format(x / 1000000000)\n    if abs(x) >= 100000000:\n        return '{:.3f}M'.format(x / 100000000)\n    if abs(x) >= 10000000:\n        return '{:.3f}M'.format(x / 10000000)\n    if abs(x) >= 1000000:\n        return '{:.3f}M'.format(x / 1000000)\n    if abs(x) >= 100000:\n        return '{:.3f}K'.format(x / 100000)\n    if abs(x) >= 10000:\n        return '{:.3f}K'.format(x / 10000)\n    if abs(x) >= 1000:\n        return '{:.3f}K'.format(x / 1000)\n    if abs(x) >= 100:\n        return '{:.3f}'.format(x)\n    if abs(x) >= 10:\n        return '{:.3f}'.format(x)\n    if abs(x) >= 1:\n        return '{:.3f}'.format(x)\n    if abs(x) >= 0.1:\n        return '{:.3f}'.format(x)\n    if abs(x) >= 0.01:\n        return '{:.3f}'.format(x)\n    if abs(x) >= 0.001:"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x is None:\n        return 'N/A'\n\n    if x == 0:\n        return '0'\n\n    if x < 0.001:\n        return '< 0.001'\n\n    if x < 1:\n        return f'{x:.3f}'\n\n    if x < 10:\n        return f'{x:.2f}'\n\n    if x < 100:\n        return f'{x:.1f}'\n\n    if x < 1000:\n        return f'{x:.0f}'\n\n    if x < 10000:\n        return f'{x:.0f}'\n\n    if x < 100000:\n        return f'{x / 1000:.0f}K'\n\n    if x < 1000000:\n        return f'{x / 1000:.1f}K'\n\n    if x < 10000000:\n        return f'{x / 1000000:.1f}M'\n\n    if x < 1000000000:\n        return f'{x / 1000000:.0f}M'\n\n    if x < 1000000000000:\n        return f'{x / 1000000000:.0f}B'\n\n    if x < 1000000000000000:\n        return f'{x / 1000000000000:.0f}T'\n\n    if x < 1000000000000000000:\n        return f'{x / 1000000000000000:.0f}Q'\n\n    if x < 1000000000000000000000:\n        return f'{x / 100000000"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n    elif percent:\n        return f'{x*100:.2f}%'\n    elif x < 1:\n        return f'{x:.4f}'\n    elif x < 10:\n        return f'{x:.3f}'\n    elif x < 100:\n        return f'{x:.2f}'\n    elif x < 1000:\n        return f'{x:.1f}'\n    else:\n        return f'{x:.0f}'\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x is None or x != x:\n        return str(x)\n    if percent:\n        x *= 100\n    if x < 1000:\n        return str(round(x, 2))\n    elif x < 1000000:\n        return str(round(x / 1000, 2)) + \"K\"\n    elif x < 1000000000:\n        return str(round(x / 1000000, 2)) + \"M\"\n    else:\n        return str(round(x / 1000000000, 2)) + \"B\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n\n    if abs(x) >= 1e9:\n        x = x / 1e9\n        suffix = \"B\"\n    elif abs(x) >= 1e6:\n        x = x / 1e6\n        suffix = \"M\"\n    elif abs(x) >= 1e3:\n        x = x / 1e3\n        suffix = \"K\"\n    else:\n        suffix = \"\"\n\n    if percent:\n        x = x * 100\n        suffix = \"%\"\n\n    if abs(x) >= 100:\n        n_decimals = 0\n    elif abs(x) >= 10:\n        n_decimals = 1\n    elif abs(x) >= 1:\n        n_decimals = 2\n    elif abs(x) >= 0.1:\n        n_decimals = 3\n    else:\n        n_decimals = 4\n\n    return f\"{x:.{n_decimals}f}{suffix}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n\n    if percent:\n        x = x * 100\n\n    if abs(x) >= 1000000000:\n        return f\"{x:.0f}\"\n    elif abs(x) >= 1000000:\n        return f\"{x:.0f}\"\n    elif abs(x) >= 1000:\n        return f\"{x:.0f}\"\n    elif abs(x) >= 1:\n        return f\"{x:.2f}\"\n    elif abs(x) >= 0.1:\n        return f\"{x:.2f}\"\n    elif abs(x) >= 0.01:\n        return f\"{x:.2f}\"\n    elif abs(x) >= 0.001:\n        return f\"{x:.3f}\"\n    elif abs(x) >= 0.0001:\n        return f\"{x:.4f}\"\n    elif abs(x) >= 0.00001:\n        return f\"{x:.5f}\"\n    else:\n        return f\"{x:.6f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n    elif percent:\n        return f\"{x*100:.2f}%\"\n    elif x < 1:\n        return f\"{x:.2f}\"\n    elif x < 10:\n        return f\"{x:.1f}\"\n    elif x < 100:\n        return f\"{x:.0f}\"\n    else:\n        return f\"{x:.0f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x is None:\n        return \"NaN\"\n    if x < 1:\n        if x == 0:\n            return \"0\"\n        if x < 0.01:\n            return \"< 0.01\"\n        if x < 0.001:\n            return \"< 0.001\"\n        if x < 0.0001:\n            return \"< 0.0001\"\n        if x < 0.00001:\n            return \"< 0.00001\"\n        if x < 0.000001:\n            return \"< 0.000001\"\n        if x < 0.0000001:\n            return \"< 0.0000001\"\n        if x < 0.00000001:\n            return \"< 0.00000001\"\n        if x < 0.000000001:\n            return \"< 0.000000001\"\n        if x < 0.0000000001:\n            return \"< 0.0000000001\"\n        if x < 0.00000000001:\n            return \"< 0.00000000001\"\n        if x < 0.000000000001:\n            return \"< 0.000000000001\"\n        if x < 0.0000000000001:\n            return \"< 0.0000000000001\"\n        if x < 0.00000000000001:\n            return \"< 0.00000000000001\"\n        if x < 0.000000000000001:\n            return \"< 0.00000"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if abs(x) < 1e-3:\n        return \"{:.2e}\".format(x)\n    elif abs(x) < 1:\n        return \"{:.2f}\".format(x)\n    elif abs(x) < 1e3:\n        return \"{:.1f}\".format(x)\n    else:\n        return \"{:.0f}\".format(x)\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n    else:\n        if percent:\n            x *= 100\n            return f\"{x:.2f}%\"\n        elif x < 0.001:\n            return f\"{x:.2e}\"\n        elif x < 1:\n            return f\"{x:.2f}\"\n        elif x < 100:\n            return f\"{x:.1f}\"\n        else:\n            return f\"{x:.0f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x is None or x != x:\n        return \"NaN\"\n    elif percent:\n        return f\"{x * 100:.1f}%\"\n    elif x == 0:\n        return \"0\"\n    elif x < 0.0001:\n        return f\"{x:.2e}\"\n    elif x < 1:\n        return f\"{x:.4f}\"\n    elif x < 100:\n        return f\"{x:.2f}\"\n    else:\n        return f\"{x:.0f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n    elif x < 0.0001:\n        return \"<0.0001\"\n    elif x < 0.001:\n        return \"{:.4f}\".format(x)\n    elif x < 0.01:\n        return \"{:.3f}\".format(x)\n    elif x < 0.1:\n        return \"{:.2f}\".format(x)\n    elif x < 1:\n        return \"{:.1f}\".format(x)\n    else:\n        if percent:\n            return \"{:.1f}%\".format(x * 100)\n        else:\n            return \"{:.0f}\".format(x)\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n    elif x == 0:\n        return \"0.00\"\n    elif x < 1e-3:\n        return f\"{x:.2e}\"\n    elif x < 1e-1:\n        return f\"{x:.3f}\"\n    elif x < 1:\n        return f\"{x:.2f}\"\n    elif x < 10:\n        return f\"{x:.1f}\"\n    elif x < 100:\n        return f\"{x:.0f}\"\n    elif x < 1000:\n        return f\"{x:.0f}\"\n    else:\n        if percent:\n            return f\"{x*100:.0f}%\"\n        else:\n            return f\"{x:.0f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n    elif x == 0:\n        return '0'\n    else:\n        if percent:\n            x = x * 100\n        if abs(x) >= 1000:\n            return f'{x:,.0f}'\n        elif abs(x) >= 100:\n            return f'{x:,.1f}'\n        elif abs(x) >= 10:\n            return f'{x:,.2f}'\n        elif abs(x) >= 1:\n            return f'{x:,.3f}'\n        elif abs(x) >= 0.1:\n            return f'{x:,.4f}'\n        elif abs(x) >= 0.01:\n            return f'{x:,.5f}'\n        elif abs(x) >= 0.001:\n            return f'{x:,.6f}'\n        else:\n            return f'{x:,.7f}'\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import psutil\n    import time\n\n    while True:\n        disk_usage = psutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / (1024 ** 3)\n        if free_space_in_gb < threshold_in_gb:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    threshold_in_bytes = threshold_in_gb * 1024 * 1024 * 1024\n\n    while True:\n        disk_usage = os.statvfs(input_dir)\n        free_space = disk_usage.f_frsize * disk_usage.f_bavail\n\n        if free_space < threshold_in_bytes:\n            break\n\n        time.sleep(sleep_time)\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import time\n    import os\n\n    threshold_in_bytes = threshold_in_gb * 1024 * 1024 * 1024\n\n    while True:\n        disk_usage = os.statvfs(input_dir)\n        free_space = disk_usage.f_frsize * disk_usage.f_bavail\n\n        if free_space < threshold_in_bytes:\n            break\n\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n    import psutil\n\n    # Convert the threshold from gigabytes to bytes\n    threshold_in_bytes = threshold_in_gb * 1024 * 1024 * 1024\n\n    # Get the disk usage of the input directory\n    disk_usage = psutil.disk_usage(input_dir)\n\n    # Wait until the free space is lower than the threshold\n    while disk_usage.free > threshold_in_bytes:\n        time.sleep(sleep_time)\n        disk_usage = psutil.disk_usage(input_dir)\n\n    # Print a message when the free space is lower than the threshold\n    print(f\"Free space in {input_dir} is now lower than {threshold_in_gb} GB.\")\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import time\n    import os\n    import psutil\n\n    threshold_in_bytes = threshold_in_gb * 1024 * 1024 * 1024\n\n    while True:\n        disk_usage = psutil.disk_usage(input_dir)\n        free_space = disk_usage.free\n        if free_space < threshold_in_bytes:\n            print(f\"Free space on {input_dir} is below {threshold_in_gb} GB. Current free space: {free_space / 1024 / 1024 / 1024:.2f} GB\")\n            time.sleep(sleep_time)\n        else:\n            print(f\"Free space on {input_dir} is above {threshold_in_gb} GB. Current free space: {free_space / 1024 / 1024 / 1024:.2f} GB\")\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import time\n    import os\n    import psutil\n\n    # Convert the threshold from gigabytes to bytes\n    threshold_in_bytes = threshold_in_gb * 1024 * 1024 * 1024\n\n    # Get the disk usage of the input directory\n    disk_usage = psutil.disk_usage(input_dir)\n\n    # Check if the free space is lower than the threshold\n    while disk_usage.free < threshold_in_bytes:\n        # Sleep for the specified time\n        time.sleep(sleep_time)\n\n        # Get the disk usage of the input directory again\n        disk_usage = psutil.disk_usage(input_dir)\n\n        # Print the current free space\n        print(f\"Current free space: {disk_usage.free / 1024 / 1024 / 1024:.2f} GB\")\n\n    # Print a message when the free space is higher than the threshold\n    print(f\"Free space is now higher than {threshold_in_gb} GB.\")\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        # Get the current disk usage statistics for the specified directory\n        disk_usage = os.statvfs(input_dir)\n\n        # Calculate the free disk space in gigabytes\n        free_space_gb = disk_usage.f_frsize * disk_usage.f_bavail / 1024 / 1024 / 1024\n\n        # Check if the free space is lower than the threshold\n        if free_space_gb < threshold_in_gb:\n            break\n\n        # Sleep for the specified time before checking the disk usage again\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import shutil\n    import time\n\n    while True:\n        total, used, free = shutil.disk_usage(input_dir)\n        free_in_gb = free // (2**30)\n        if free_in_gb < threshold_in_gb:\n            break\n        time.sleep(sleep_time)\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import shutil\n    import time\n\n    threshold_in_bytes = threshold_in_gb * 1024 * 1024 * 1024\n    while True:\n        _, _, free = shutil.disk_usage(input_dir)\n        if free < threshold_in_bytes:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import shutil\n    import time\n\n    while True:\n        total, used, free = shutil.disk_usage(input_dir)\n        free_gb = free // (2**30)\n        if free_gb < threshold_in_gb:\n            print(f\"Free disk space is now below {threshold_in_gb} GB. Stopping the process.\")\n            break\n        print(f\"Free disk space: {free_gb} GB\")\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import time\n    import shutil\n\n    # Convert the threshold from gigabytes to bytes\n    threshold_in_bytes = threshold_in_gb * 1024 * 1024 * 1024\n\n    # Get the total disk space and free disk space of the input directory\n    total_space, used_space, free_space = shutil.disk_usage(input_dir)\n\n    # Check if the free disk space is lower than the threshold\n    while free_space < threshold_in_bytes:\n        # Wait for a specified time before checking the disk usage again\n        time.sleep(sleep_time)\n\n        # Get the updated free disk space of the input directory\n        total_space, used_space, free_space = shutil.disk_usage(input_dir)\n\n    # Print a message when the free disk space is higher than the threshold\n    print(f\"Free disk space in {input_dir} is now higher than {threshold_in_gb} GB.\")\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    # Convert the threshold to bytes\n    threshold_in_bytes = threshold_in_gb * 1024 * 1024 * 1024\n\n    # Get the total size of the input directory\n    total_size = sum(os.path.getsize(os.path.join(dirpath, filename)) for dirpath, dirnames, filenames in os.walk(input_dir) for filename in filenames)\n\n    # Check the disk usage until the free space is lower than the threshold\n    while True:\n        # Get the free space on the disk\n        statvfs = os.statvfs(input_dir)\n        free_space = statvfs.f_frsize * statvfs.f_bavail\n\n        # Check if the free space is lower than the threshold\n        if free_space < threshold_in_bytes:\n            # Sleep for the specified time before checking again\n            time.sleep(sleep_time)\n        else:\n            # Exit the loop if the free space is higher than the threshold\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import psutil\n    import time\n\n    # Convert the threshold from gigabytes to bytes\n    threshold_in_bytes = threshold_in_gb * 1024 * 1024 * 1024\n\n    # Continuously check the disk usage until the free space is lower than the threshold\n    while True:\n        # Get the disk usage of the input directory\n        disk_usage = psutil.disk_usage(input_dir)\n\n        # Check if the free space is lower than the threshold\n        if disk_usage.free < threshold_in_bytes:\n            # If the free space is lower than the threshold, print a message and break the loop\n            print(f\"Free space in {input_dir} is lower than {threshold_in_gb} GB. Waiting for space to free up...\")\n            break\n        else:\n            # If the free space is still higher than the threshold, print a message and sleep for the specified time\n            print(f\"Free space in {input_dir} is still above {threshold_in_gb} GB. Sleeping for {sleep_time} seconds...\")\n            time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import shutil\n    import time\n\n    # Convert the threshold from gigabytes to bytes\n    threshold_in_bytes = threshold_in_gb * 1024 * 1024 * 1024\n\n    while True:\n        # Get the disk usage of the input directory\n        total, used, free = shutil.disk_usage(input_dir)\n\n        # Check if the free space is lower than the threshold\n        if free < threshold_in_bytes:\n            # If the free space is lower than the threshold, print a message and exit the function\n            print(f\"Free disk space on {input_dir} is lower than {threshold_in_gb} GB.\")\n            break\n\n        # If the free space is not lower than the threshold, wait for the specified amount of time before checking again\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import time\n    import shutil\n\n    while True:\n        total, used, free = shutil.disk_usage(input_dir)\n        free_in_gb = free // (2 ** 30)\n        if free_in_gb < threshold_in_gb:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import shutil\n    import time\n\n    # Get the total, used, and free space of the input directory\n    total, used, free = shutil.disk_usage(input_dir)\n\n    # Convert the free space to gigabytes\n    free_in_gb = free // (2**30)\n\n    # Wait until the free space is lower than the threshold\n    while free_in_gb > threshold_in_gb:\n        # Sleep for the specified amount of time\n        time.sleep(sleep_time)\n\n        # Get the updated free space of the input directory\n        total, used, free = shutil.disk_usage(input_dir)\n        free_in_gb = free // (2**30)\n\n        # Print the current free space\n        print(f\"Current free space: {free_in_gb} GB\")\n\n    # Print a message when the free space is lower than the threshold\n    print(f\"Free space is now lower than {threshold_in_gb} GB. Continuing with the process.\")\n\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import psutil\n    import time\n\n    while True:\n        disk_usage = psutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / (1024 ** 3)\n        if free_space_in_gb < threshold_in_gb:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import shutil\n    import time\n\n    # Convert threshold to bytes\n    threshold_in_bytes = threshold_in_gb * 1024 * 1024 * 1024\n\n    while True:\n        # Get disk usage of the specified directory\n        total, used, free = shutil.disk_usage(input_dir)\n\n        # Check if free space is lower than the threshold\n        if free < threshold_in_bytes:\n            break\n\n        # Sleep for a specified time before checking again\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        total, used, free = shutil.disk_usage(input_dir)\n        free_in_gb = free // (2**30)\n        if free_in_gb < threshold_in_gb:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import shutil\n    import time\n\n    # Convert the threshold from gigabytes to bytes\n    threshold_in_bytes = threshold_in_gb * 1024 * 1024 * 1024\n\n    # Get the disk usage of the input directory\n    total, used, free = shutil.disk_usage(input_dir)\n\n    # Check if the free space is lower than the threshold\n    while free < threshold_in_bytes:\n        # Sleep for the specified time\n        time.sleep(sleep_time)\n        # Get the disk usage of the input directory again\n        total, used, free = shutil.disk_usage(input_dir)\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in 't'\n  dt = t[1:] - t[:-1]\n\n  # Multiply the PDF values by the differences and sum them to obtain the weights\n  weights = np.sum(p[:-1] * dt)\n\n  return weights"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in 't'\n  diffs = np.diff(t)\n\n  # Multiply the PDF values in 'p' by the differences and sum the result\n  w = np.sum(p[:-1] * diffs)\n\n  # Return the normalized weights\n  return p / w"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in 't'\n  dt = np.diff(t)\n\n  # Multiply the PDF values by the differences to obtain the weights\n  w = p[:-1] * dt\n\n  # Normalize the weights to sum to 1\n  w = w / np.sum(w)\n\n  return w\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  import numpy as np\n\n  if len(t) != len(p):\n    raise ValueError(\"The length of 't' and 'p' must be the same.\")\n\n  if np.sum(p) != 1:\n    raise ValueError(\"The sum of 'p' must be 1.\")\n\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"The elements in 't' must be in ascending order.\")\n\n  w = np.zeros_like(p)\n  w[0] = p[0] * (t[1] - t[0])\n  w[1:-1] = p[1:-1] * (t[2:] - t[:-2])\n  w[-1] = p[-1] * (t[-1] - t[-2])\n\n  return w"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in 't'\n  diffs = np.diff(t)\n\n  # Multiply the PDF values by the differences to obtain the weights\n  weights = p[:-1] * diffs\n\n  # Normalize the weights so that they sum to 1\n  weights /= np.sum(weights)\n\n  return weights\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in 't'\n  diff = np.diff(t)\n\n  # Multiply the PDF values by the differences to obtain the weights\n  w = p[:-1] * diff\n\n  # Normalize the weights to sum to 1\n  w /= np.sum(w)\n\n  return w\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in 't'\n  diffs = np.diff(t)\n\n  # Multiply the PDF values by the differences to obtain the weights\n  weights = p[:-1] * diffs\n\n  # Normalize the weights to sum to 1\n  weights /= np.sum(weights)\n\n  return weights\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in 't'\n  diff = np.diff(t)\n\n  # Multiply the PDF values in 'p' by the differences and return the result\n  return p[:-1] * diff\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in 't'\n  diff = np.diff(t)\n\n  # Multiply the PDF values by the differences to obtain the weights\n  w = p[:-1] * diff\n\n  # Normalize the weights to sum to 1\n  w_norm = w / np.sum(w)\n\n  return w_norm\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  import numpy as np\n\n  # Calculate the differences between consecutive elements in 't'\n  diff = np.diff(t)\n\n  # Multiply the PDF values by the differences to obtain the weights\n  w = p[:-1] * diff\n\n  # Normalize the weights so that they sum to 1\n  w = w / np.sum(w)\n\n  return w\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  return p * np.diff(t)\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in 't'\n  dt = t[1:] - t[:-1]\n\n  # Multiply the PDF values 'p' by the differences 'dt' to obtain the weights\n  w = p[:-1] * dt\n\n  # Normalize the weights so that they sum to 1\n  w = w / np.sum(w)\n\n  return w\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in 't'\n  dt = np.diff(t)\n\n  # Multiply the PDF values in 'p' by the differences in 't'\n  w = p[:-1] * dt\n\n  # Normalize the weights to sum to 1\n  w /= np.sum(w)\n\n  return w\n\n\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in 't'\n  dt = np.diff(t)\n\n  # Multiply the PDF values in 'p' by the differences calculated from 't'\n  w = p[:-1] * dt\n\n  # Normalize the weights to sum to 1\n  w /= np.sum(w)\n\n  return w\n\n\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  import numpy as np\n\n  d = np.diff(t)\n  w = np.concatenate([p[0:1], p[1:]*d])\n  w = w/np.sum(w)\n\n  return w\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in 't'\n  diff = np.diff(t)\n\n  # Multiply the PDF values 'p' by the differences 'diff' to obtain the weights\n  weights = p[:-1] * diff\n\n  # Normalize the weights so that they sum to 1\n  weights /= np.sum(weights)\n\n  return weights\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  import numpy as np\n\n  # Calculate the differences between consecutive elements in 't'\n  dt = np.diff(t)\n\n  # Multiply the PDF values 'p' by the differences 'dt' to obtain the weights\n  w = p[:-1] * dt\n\n  # Normalize the weights to sum to 1\n  w /= np.sum(w)\n\n  return w\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  import numpy as np\n\n  # Calculate the differences between consecutive elements in 't'\n  diff = np.diff(t)\n\n  # Multiply the PDF values by the differences to obtain the weights\n  w = p[:-1] * diff\n\n  # Normalize the weights so they sum to 1\n  w /= np.sum(w)\n\n  return w\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in t\n  diffs = np.diff(t)\n\n  # Multiply the PDF values by the differences and sum the result\n  w = np.sum(p[:-1] * diffs)\n\n  # Return the vector of weights\n  return w\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in 't'\n  dt = t[1:] - t[:-1]\n\n  # Multiply the PDF values by the differences to obtain the weights\n  w = p[:-1] * dt\n\n  # Normalize the weights so they sum to 1\n  w /= np.sum(w)\n\n  return w\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"\\t\", \"\")\n    line_text = line_text.replace(\"\\n\", \"\")\n    line_text = line_text.replace(\"\\r\", \"\")\n\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"\\t\", \"\")\n    line_text = line_text.replace(\"\\n\", \"\")\n\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    return [line_text[i:i + 2] for i in range(0, len(line_text), 2)]"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = \"\".join(line_text.split())\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = []\n    for i in range(0, len(line_text), 2):\n        tokens.append(line_text[i:i+2])\n\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"\\t\", \"\")\n    line_text = line_text.replace(\"\\n\", \"\")\n    line_text = line_text.replace(\"\\r\", \"\")\n\n    return line_text\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Initialize the list of tokens\n    tokens = []\n\n    # Initialize the start index of the current token\n    start_index = 0\n\n    # Iterate over the characters in the modified text\n    for i in range(len(line_text)):\n        # If the current character is an uppercase letter and the previous character is a lowercase letter,\n        # add the previous token to the list of tokens and update the start index of the current token\n        if line_text[i].isupper() and i > 0 and line_text[i - 1].islower():\n            tokens.append(line_text[start_index:i])\n            start_index = i\n\n    # Add the last token to the list of tokens\n    tokens.append(line_text[start_index:])\n\n    return tokens"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    return list(line_text)\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    line_text = line_text.split()\n\n    return line_text\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.split(\"|\")\n\n    return line_text\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    return [line_text[i:i + 2] for i in range(0, len(line_text), 2)]\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Removing all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segmenting the modified text into smaller parts or tokens\n    line_text = line_text.split(\",\")\n\n    return line_text\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Removes all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segments the text into smaller parts or tokens\n    tokens = []\n    for i in range(len(line_text)):\n        tokens.append(line_text[i])\n\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Split the text into smaller parts or tokens\n    tokens = line_text.split()\n\n    # Return the list of tokens\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    modified_text = line_text.replace(' ', '')\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = [modified_text[i:i + 2] for i in range(0, len(modified_text), 2)]\n\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    modified_line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = modified_line_text.split(\".\")\n\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"\\n\", \"\")\n    line_text = line_text.replace(\"\\t\", \"\")\n    line_text = line_text.replace(\"\\r\", \"\")\n    line_text = line_text.replace(\"\\f\", \"\")\n\n    tokens = []\n    temp_token = \"\"\n    for char in line_text:\n        if char.isalnum():\n            temp_token += char\n        else:\n            if temp_token != \"\":\n                tokens.append(temp_token)\n                temp_token = \"\"\n            tokens.append(char)\n    if temp_token != \"\":\n        tokens.append(temp_token)\n\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(' ', '')\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = []\n    start = 0\n    for i in range(len(line_text)):\n        if line_text[i].isupper() and i > 0:\n            tokens.append(line_text[start:i])\n            start = i\n    tokens.append(line_text[start:])\n\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Split the text into smaller parts or tokens\n    tokens = line_text.split(\"|\")\n\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    return [line_text[i:i + 1] for i in range(0, len(line_text), 1)]\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Removing all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segmenting the modified text into smaller parts or tokens\n    return segment_text(line_text)\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zero weights must not exceed the total number of weights.\")\n\n    # Generate random weights that sum up to one\n    weights = np.random.random(n)\n    weights /= np.sum(weights)\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check if the number of zero weights is valid\n    if zeros > n:\n        raise ValueError(\"The number of zero weights must not exceed the total number of weights.\")\n\n    # Generate random weights that sum up to one\n    weights = np.random.dirichlet(np.ones(n), size=1).flatten()\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    assert n >= zeros, \"The number of zeros must not exceed the total number of weights.\"\n\n    if zeros == 0:\n        weights = np.random.random(n)\n        weights /= weights.sum()\n    else:\n        weights = np.random.random(n - zeros)\n        weights = np.append(weights, np.zeros(zeros))\n        np.random.shuffle(weights)\n        weights /= weights.sum()\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError('The number of zeros cannot exceed the total number of weights.')\n\n    weights = np.random.rand(n)\n    weights /= weights.sum()\n\n    if zeros > 0:\n        zero_indices = np.random.choice(range(n), zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check for input errors\n    if n <= 0:\n        raise ValueError(\"n must be a positive integer\")\n    if zeros < 0:\n        raise ValueError(\"zeros must be a non-negative integer\")\n    if zeros > n:\n        raise ValueError(\"zeros must not exceed n\")\n\n    # Generate random weights that sum up to one\n    weights = np.random.random(n)\n    weights /= np.sum(weights)\n\n    # Set specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zero-weight elements cannot exceed the total number of weights.\")\n\n    if zeros == n:\n        return np.zeros(n)\n\n    weights = np.random.random(n)\n    weights[np.random.choice(np.arange(n), size=zeros, replace=False)] = 0\n    weights = weights / np.sum(weights)\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Validate the inputs\n    if not isinstance(n, int):\n        raise TypeError(\"n must be an integer\")\n    if not isinstance(zeros, int):\n        raise TypeError(\"zeros must be an integer\")\n    if zeros > n:\n        raise ValueError(\"zeros cannot be greater than n\")\n\n    # Generate the weights\n    weights = np.random.random(n)\n    weights /= np.sum(weights)\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(n, size=zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check that the number of zeros does not exceed the total number of weights\n    if zeros > n:\n        raise ValueError(\"The number of zeros must not exceed the total number of weights.\")\n\n    # Generate an array of random weights that sum to one\n    weights = np.random.dirichlet(np.ones(n), size=1)[0]\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(np.arange(n), size=zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check that the number of elements set to zero does not exceed the total number of elements\n    if zeros > n:\n        raise ValueError(\"The number of elements set to zero cannot exceed the total number of elements.\")\n\n    # Generate random weights that sum up to one\n    weights = np.random.dirichlet(np.ones(n), size=1)[0]\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(np.arange(n), size=zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check if the number of zeros exceeds the total number of weights\n    if zeros > n:\n        raise ValueError(\"The number of zeros must not exceed the total number of weights.\")\n\n    # Generate random weights that sum to one\n    weights = np.random.dirichlet(np.ones(n), size=1)[0]\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        idx = np.random.choice(np.arange(n), size=zeros, replace=False)\n        weights[idx] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check the inputs\n    assert isinstance(n, int) and n > 0, \"n must be a positive integer\"\n    assert isinstance(zeros, int) and zeros >= 0, \"zeros must be a non-negative integer\"\n    assert zeros <= n, \"zeros must not exceed n\"\n\n    # Generate n random weights that sum to one\n    weights = np.random.dirichlet(np.ones(n), size=1)[0]\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    assert zeros <= n, \"The number of zero-weight elements must not exceed the total number of elements.\"\n\n    weights = np.random.rand(n)\n    weights /= np.sum(weights)\n    if zeros > 0:\n        zero_indices = np.random.choice(np.arange(n), size=zeros, replace=False)\n        weights[zero_indices] = 0\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    assert zeros <= n, \"Number of zero-weights must not exceed the total number of weights.\"\n\n    weights = np.random.random(n)\n    weights = weights / np.sum(weights)\n    weights[np.random.choice(np.arange(n), zeros, replace=False)] = 0\n    weights = weights / np.sum(weights)\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    assert zeros <= n, \"Number of zeros cannot exceed total number of elements.\"\n\n    if zeros == 0:\n        return np.random.dirichlet(np.ones(n), size=1)[0]\n    else:\n        weights = np.random.dirichlet(np.ones(n - zeros), size=1)[0]\n        weights = np.append(weights, np.zeros(zeros))\n        return np.random.permutation(weights)\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    assert n > 0, \"n must be greater than zero\"\n    assert zeros <= n, \"zeros must be less than or equal to n\"\n\n    # Generate n-1 random weights that sum up to one\n    weights = np.random.dirichlet(np.ones(n - zeros), size=1)[0]\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(n, size=zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check that the number of zeros does not exceed the total number of weights\n    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the total number of weights.\")\n\n    # Generate an array of n random weights that sum up to one\n    weights = np.random.dirichlet(np.ones(n), size=1)[0]\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        idx = np.random.choice(np.arange(n), zeros, replace=False)\n        weights[idx] = 0\n\n    return weights\n\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check if zeros exceeds n\n    if zeros > n:\n        raise ValueError(\"The number of zero-weight elements must not exceed the total number of elements.\")\n\n    # Generate random weights that sum up to one\n    weights = np.random.random(n)\n    weights /= np.sum(weights)\n\n    # Set specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    assert n >= zeros, \"The number of zeros cannot exceed the total number of weights.\"\n\n    w = np.random.rand(n)\n    w = w / np.sum(w)\n    w[:zeros] = 0\n    w = w / np.sum(w)\n\n    return w\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    assert n >= zeros, \"The number of zero-weight elements must not exceed the total number of elements.\"\n\n    weights = np.random.random(n)\n    weights /= weights.sum()\n    if zeros > 0:\n        idx = np.random.choice(n, zeros, replace=False)\n        weights[idx] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    assert zeros <= n, \"The number of zeros must not exceed the total number of weights.\"\n\n    # Generate an array of random weights that sum up to one\n    weights = np.random.random(n)\n    weights /= sum(weights)\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(np.arange(n), size=zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, **module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, **module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type=module_type, **module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, **module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, **module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_params = module_dict.pop('module_params')\n        module_params = {k: v for k, v in module_params.items() if v is not None}\n        module = cls(module_type, **module_params)\n        return module\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module = cls(module_type, **module_dict)\n        return module\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict['module_type']\n        module_params = module_dict.copy()\n        del module_params['module_type']\n        return cls(module_type, **module_params)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, **module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, **module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict['module_type']\n        module_params = {k: v for k, v in module_dict.items() if k != 'module_type'}\n        return cls(module_type, **module_params)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_class = cls.get_module_class(module_type)\n        return module_class(**module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module = cls(module_type)\n        module.set_parameters(module_dict)\n        return module\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_params = module_dict\n        return cls(module_type, **module_params)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type=module_type, **module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, **module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, **module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_cls = cls._get_module_cls(module_type)\n        return module_cls(**module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, **module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, **module_dict)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = instance[\"bbox\"]\n\n    # Calculate the center of the bounding box\n    bbox_center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n\n    # Calculate the top-left corner of the crop region\n    crop_top_left = (\n        max(bbox_center[0] - crop_size[0] / 2, 0),\n        max(bbox_center[1] - crop_size[1] / 2, 0),\n    )\n\n    # Calculate the bottom-right corner of the crop region\n    crop_bottom_right = (\n        min(bbox_center[0] + crop_size[0] / 2, image_size[0]),\n        min(bbox_center[1] + crop_size[1] / 2, image_size[1]),\n    )\n\n    # Calculate the dimensions of the crop region\n    crop_dims = (\n        crop_bottom_right[0] - crop_top_left[0],\n        crop_bottom_right[1] - crop_top_left[1],\n    )\n\n    # Adjust the top-left corner if the crop region is too large\n    if crop_dims[0] > crop_size[0]:\n        crop_top_left = (\n            crop_top_left[0] + (crop_dims[0] - crop_size[0]) / 2,\n            crop_top_left[1],\n        )\n    if crop_dims[1] > crop_size[1]:\n        crop_top_left = (\n            crop_top_left[0],\n            crop_top_left[1] + (crop_dims[1] - crop_size[1]) / 2,\n        )\n\n    # Create a CropTransform object with the calculated parameters\n    crop_transform = CropTransform(\n        crop_top_left[0],\n        crop_top_left[1],"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = instance['bbox']\n\n    # Calculate the center of the bounding box\n    center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n\n    # Calculate the size of the crop region\n    crop_width, crop_height = crop_size\n\n    # Calculate the size of the image\n    image_width, image_height = image_size\n\n    # Calculate the top-left corner of the crop region\n    crop_x = max(0, int(center[0] - crop_width / 2))\n    crop_y = max(0, int(center[1] - crop_height / 2))\n\n    # Adjust the crop region if it goes beyond the image boundaries\n    if crop_x + crop_width > image_width:\n        crop_x = image_width - crop_width\n    if crop_y + crop_height > image_height:\n        crop_y = image_height - crop_height\n\n    # Return the crop transform object\n    return CropTransform(crop_x, crop_y, crop_width, crop_height)"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Extract the bounding box from the instance\n    bbox = instance[\"bbox\"]\n\n    # Calculate the center of the bounding box\n    bbox_center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n\n    # Calculate the top-left corner of the cropping region\n    crop_top_left = (\n        max(bbox_center[0] - crop_size[0] / 2, 0),\n        max(bbox_center[1] - crop_size[1] / 2, 0),\n    )\n\n    # Calculate the bottom-right corner of the cropping region\n    crop_bottom_right = (\n        min(bbox_center[0] + crop_size[0] / 2, image_size[0]),\n        min(bbox_center[1] + crop_size[1] / 2, image_size[1]),\n    )\n\n    # Adjust the cropping region if it exceeds the image boundaries\n    if crop_bottom_right[0] - crop_top_left[0] < crop_size[0]:\n        crop_top_left = (\n            crop_bottom_right[0] - crop_size[0],\n            crop_top_left[1],\n        )\n    if crop_bottom_right[1] - crop_top_left[1] < crop_size[1]:\n        crop_top_left = (\n            crop_top_left[0],\n            crop_bottom_right[1] - crop_size[1],\n        )\n\n    # Create a CropTransform object with the calculated parameters\n    crop_transform = CropTransform(\n        crop_top_left[0], crop_top_left[1], crop_size[0], crop_size[1]\n    )\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Extract the bounding box coordinates from the instance dictionary\n    bbox = instance['bbox']\n\n    # Calculate the center of the bounding box\n    center_x = (bbox[0] + bbox[2]) / 2\n    center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the crop region based on the center and the desired crop size\n    crop_x1 = max(0, center_x - crop_size[1] / 2)\n    crop_y1 = max(0, center_y - crop_size[0] / 2)\n    crop_x2 = min(image_size[1], center_x + crop_size[1] / 2)\n    crop_y2 = min(image_size[0], center_y + crop_size[0] / 2)\n\n    # Adjust the crop region if it exceeds the image boundaries\n    if crop_x2 - crop_x1 < crop_size[1]:\n        crop_x2 = min(image_size[1], crop_x1 + crop_size[1])\n    if crop_y2 - crop_y1 < crop_size[0]:\n        crop_y2 = min(image_size[0], crop_y1 + crop_size[0])\n\n    # Create a CropTransform object with the adjusted crop region\n    crop_transform = CropTransform(crop_x1, crop_y1, crop_x2, crop_y2)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    crop_h, crop_w = crop_size\n    image_h, image_w = image_size\n    crop_type = instance.get(\"crop_type\", \"absolute\")\n\n    if crop_type == \"absolute\":\n        crop_y1 = max(round(bbox[1] - (crop_h - bbox[3]) / 2), 0)\n        crop_y2 = min(round(bbox[1] + (crop_h - bbox[3]) / 2), image_h)\n        crop_x1 = max(round(bbox[0] - (crop_w - bbox[2]) / 2), 0)\n        crop_x2 = min(round(bbox[0] + (crop_w - bbox[2]) / 2), image_w)\n\n    elif crop_type == \"absolute_range\":\n        crop_y1 = max(bbox[1] + instance[\"crop_box\"][1], 0)\n        crop_y2 = min(bbox[1] + instance[\"crop_box\"][3], image_h)\n        crop_x1 = max(bbox[0] + instance[\"crop_box\"][0], 0)\n        crop_x2 = min(bbox[0] + instance[\"crop_box\"][2], image_w)\n\n    elif crop_type == \"relative\":\n        crop_y1 = max(round(bbox[1] - (crop_h - bbox[3]) * instance[\"crop_box\"][1]), 0)\n        crop_y2 = min(round(bbox[1] + (crop_h - bbox[3]) * instance[\"crop_box\"][3]), image_h)\n        crop_x1 = max(round(bbox[0] - (crop_w - bbox[2]) * instance[\"crop_box\"][0]), 0)\n        crop_x2 = min(round(bbox[0] + (crop_"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Extract the bounding box coordinates from the instance annotation dictionary\n    bbox = instance['bbox']\n\n    # Calculate the center of the bounding box\n    bbox_center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n\n    # Calculate the cropping region based on the desired crop size and the bounding box center\n    crop_height, crop_width = crop_size\n    crop_top = max(0, int(bbox_center[1] - crop_height / 2))\n    crop_left = max(0, int(bbox_center[0] - crop_width / 2))\n    crop_bottom = min(image_size[0], int(bbox_center[1] + crop_height / 2))\n    crop_right = min(image_size[1], int(bbox_center[0] + crop_width / 2))\n\n    # Adjust the cropping region if it exceeds the image boundaries\n    if crop_bottom - crop_top < crop_height:\n        crop_top = max(0, crop_top + (crop_height - (crop_bottom - crop_top)) // 2)\n        crop_bottom = min(image_size[0], crop_top + crop_height)\n    if crop_right - crop_left < crop_width:\n        crop_left = max(0, crop_left + (crop_width - (crop_right - crop_left)) // 2)\n        crop_right = min(image_size[1], crop_left + crop_width)\n\n    # Create a CropTransform object with the calculated cropping region\n    crop_transform = CropTransform(crop_top, crop_left, crop_bottom, crop_right)\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = instance[\"bbox\"]\n\n    # Get the center of the bounding box\n    center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n\n    # Get the height and width of the bounding box\n    bbox_h = bbox[3] - bbox[1]\n    bbox_w = bbox[2] - bbox[0]\n\n    # Calculate the height and width of the crop\n    crop_h, crop_w = crop_size\n\n    # Calculate the top-left corner of the crop\n    crop_y = max(0, int(center[1] - crop_h / 2))\n    crop_x = max(0, int(center[0] - crop_w / 2))\n\n    # Adjust the top-left corner if the crop extends beyond the image boundaries\n    if crop_y + crop_h > image_size[0]:\n        crop_y = image_size[0] - crop_h\n    if crop_x + crop_w > image_size[1]:\n        crop_x = image_size[1] - crop_w\n\n    # Create and return the CropTransform object\n    return CropTransform(crop_y, crop_x, crop_h, crop_w)\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    from detectron2.data.transforms import CropTransform\n\n    # Get the bounding box of the instance\n    bbox = instance[\"bbox\"]\n\n    # Calculate the center of the bounding box\n    center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n\n    # Calculate the size of the crop region\n    crop_h, crop_w = crop_size\n\n    # Calculate the top-left corner of the crop region\n    crop_x1 = max(0, int(center[0] - crop_w / 2))\n    crop_y1 = max(0, int(center[1] - crop_h / 2))\n\n    # Calculate the bottom-right corner of the crop region\n    crop_x2 = min(image_size[1], int(center[0] + crop_w / 2))\n    crop_y2 = min(image_size[0], int(center[1] + crop_h / 2))\n\n    # Adjust the crop region if it exceeds the image boundaries\n    if crop_x1 < 0:\n        crop_x1 = 0\n        crop_x2 = crop_w\n    if crop_y1 < 0:\n        crop_y1 = 0\n        crop_y2 = crop_h\n    if crop_x2 > image_size[1]:\n        crop_x1 = image_size[1] - crop_w\n        crop_x2 = image_size[1]\n    if crop_y2 > image_size[0]:\n        crop_y1 = image_size[0] - crop_h\n        crop_y2 = image_size[0]\n\n    # Create a CropTransform object with the adjusted crop region\n    crop_transform = CropTransform(\n        crop_x1, crop_y1, crop_x2, crop_y2, image_size[1], image_size[0]\n    )\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Extract the bounding box of the instance\n    bbox = instance['bbox']\n\n    # Calculate the center of the bounding box\n    bbox_center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n\n    # Calculate the top-left corner of the crop\n    crop_top_left = (bbox_center[0] - crop_size[1] / 2, bbox_center[1] - crop_size[0] / 2)\n\n    # Adjust the top-left corner if it is outside the image boundaries\n    if crop_top_left[0] < 0:\n        crop_top_left = (0, crop_top_left[1])\n    if crop_top_left[1] < 0:\n        crop_top_left = (crop_top_left[0], 0)\n\n    # Calculate the bottom-right corner of the crop\n    crop_bottom_right = (crop_top_left[0] + crop_size[1], crop_top_left[1] + crop_size[0])\n\n    # Adjust the bottom-right corner if it is outside the image boundaries\n    if crop_bottom_right[0] > image_size[1]:\n        crop_bottom_right = (image_size[1], crop_bottom_right[1])\n    if crop_bottom_right[1] > image_size[0]:\n        crop_bottom_right = (crop_bottom_right[0], image_size[0])\n\n    # Calculate the top-left corner of the crop within the image boundaries\n    crop_top_left = (max(0, crop_top_left[0]), max(0, crop_top_left[1]))\n\n    # Calculate the bottom-right corner of the crop within the image boundaries\n    crop_bottom_right = (min(image_size[1], crop_bottom_right[0]), min(image_size[0], crop_bottom_right[1]))\n\n    # Calculate the dimensions of the crop\n   "}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    import torch\n    from detectron2.data.transforms import CropTransform\n\n    # Get the bounding box of the instance\n    bbox = instance[\"bbox\"]\n\n    # Calculate the center of the bounding box\n    center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n\n    # Calculate the size of the crop\n    crop_size_half = [crop_size[0] / 2, crop_size[1] / 2]\n\n    # Calculate the top-left corner of the crop\n    crop_top_left = [center[0] - crop_size_half[0], center[1] - crop_size_half[1]]\n\n    # Adjust the top-left corner if it goes out of the image boundaries\n    crop_top_left[0] = max(0, crop_top_left[0])\n    crop_top_left[1] = max(0, crop_top_left[1])\n\n    # Adjust the top-left corner if it goes out of the image boundaries\n    crop_top_left[0] = min(image_size[0] - crop_size[0], crop_top_left[0])\n    crop_top_left[1] = min(image_size[1] - crop_size[1], crop_top_left[1])\n\n    # Convert the top-left corner to integer values\n    crop_top_left = [int(crop_top_left[0]), int(crop_top_left[1])]\n\n    # Create the CropTransform object\n    crop_transform = CropTransform(\n        crop_top_left[0], crop_top_left[1], crop_size[0], crop_size[1]\n    )\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Extract the bounding box coordinates from the instance dictionary\n    x1, y1, x2, y2 = instance[\"bbox\"]\n\n    # Calculate the center coordinates of the bounding box\n    center_x = (x1 + x2) / 2\n    center_y = (y1 + y2) / 2\n\n    # Calculate the size of the crop region\n    crop_height, crop_width = crop_size\n\n    # Calculate the size of the image\n    image_height, image_width = image_size\n\n    # Calculate the top-left corner of the crop region\n    top = max(0, int(center_y - crop_height / 2))\n    left = max(0, int(center_x - crop_width / 2))\n\n    # Adjust the top-left corner if the crop region is too close to the bottom or right edge of the image\n    if top + crop_height > image_height:\n        top = image_height - crop_height\n    if left + crop_width > image_width:\n        left = image_width - crop_width\n\n    # Return the crop transform object\n    return CropTransform(top, left, crop_height, crop_width)\n\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    import torch\n    from detectron2.data.transforms import CropTransform\n\n    # Get the bounding box of the instance\n    bbox = instance['bbox']\n\n    # Calculate the center of the bounding box\n    center = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n\n    # Calculate the size of the crop region\n    crop_width, crop_height = crop_size\n\n    # Calculate the top-left corner of the crop region\n    crop_x = max(0, int(center[0] - crop_width / 2))\n    crop_y = max(0, int(center[1] - crop_height / 2))\n\n    # Adjust the crop region if it goes beyond the image boundaries\n    if crop_x + crop_width > image_size[0]:\n        crop_x = image_size[0] - crop_width\n    if crop_y + crop_height > image_size[1]:\n        crop_y = image_size[1] - crop_height\n\n    # Create the CropTransform object\n    crop_transform = CropTransform(crop_x, crop_y, crop_width, crop_height, image_size[0], image_size[1])\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = instance['bbox']\n\n    # Get the mode of the bounding box (e.g., \"xyxy\", \"xywh\", etc.)\n    bbox_mode = instance.get('bbox_mode', BoxMode.XYXY_ABS)\n\n    # Convert the bounding box to the desired format (e.g., \"xyxy\")\n    if bbox_mode != BoxMode.XYXY_ABS:\n        bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Get the center of the bounding box\n    center_x = (bbox[0] + bbox[2]) / 2\n    center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the crop region based on the center and the desired crop size\n    crop_x1 = max(0, center_x - crop_size[1] / 2)\n    crop_y1 = max(0, center_y - crop_size[0] / 2)\n    crop_x2 = min(image_size[1], center_x + crop_size[1] / 2)\n    crop_y2 = min(image_size[0], center_y + crop_size[0] / 2)\n\n    # Adjust the crop region if it exceeds the image boundaries\n    if crop_x1 < 0:\n        crop_x2 -= crop_x1\n        crop_x1 = 0\n    if crop_y1 < 0:\n        crop_y2 -= crop_y1\n        crop_y1 = 0\n    if crop_x2 > image_size[1]:\n        crop_x1 -= crop_x2 - image_size[1]\n        crop_x2 = image_size[1]\n    if crop_y2 > image_size[0]:\n        crop_y1 -= crop_y2 - image_size[0]\n        crop_y2 = image_size[0]\n\n    # Create a CropTransform object with the"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    from detectron2.data.transforms import CropTransform\n\n    bbox = instance[\"bbox\"]\n    mode = instance.get(\"bbox_mode\", 1)\n\n    assert mode == 1, \"Only bounding boxes in XYXY_ABS format are supported.\"\n\n    # Calculate the center of the bounding box\n    center_x = (bbox[0] + bbox[2]) / 2\n    center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the top-left corner of the crop region\n    crop_x = center_x - crop_size[0] / 2\n    crop_y = center_y - crop_size[1] / 2\n\n    # Adjust the crop region if it goes beyond the image boundaries\n    if crop_x < 0:\n        crop_x = 0\n    if crop_y < 0:\n        crop_y = 0\n    if crop_x + crop_size[0] > image_size[0]:\n        crop_x = image_size[0] - crop_size[0]\n    if crop_y + crop_size[1] > image_size[1]:\n        crop_y = image_size[1] - crop_size[1]\n\n    # Create the CropTransform object\n    crop_transform = CropTransform(crop_x, crop_y, crop_size[0], crop_size[1])\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    crop_size = list(crop_size)\n    image_size = list(image_size)\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    assert bbox_mode == 0, \"bbox_mode must be XYWH_ABS\"\n    x1, y1, x2, y2 = bbox\n    center_x = (x1 + x2) / 2\n    center_y = (y1 + y2) / 2\n\n    crop_x1 = max(0, int(center_x - crop_size[1] / 2))\n    crop_y1 = max(0, int(center_y - crop_size[0] / 2))\n    crop_x2 = min(image_size[1], int(center_x + crop_size[1] / 2))\n    crop_y2 = min(image_size[0], int(center_y + crop_size[0] / 2))\n\n    if crop_x2 - crop_x1 < crop_size[1]:\n        crop_x2 = min(image_size[1], crop_x1 + crop_size[1])\n    if crop_y2 - crop_y1 < crop_size[0]:\n        crop_y2 = min(image_size[0], crop_y1 + crop_size[0])\n\n    crop_transform = CropTransform(\n        crop_x1, crop_y1, crop_x2, crop_y2, image_size[1], image_size[0]\n    )\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Extract the bounding box coordinates from the instance annotation\n    bbox = instance[\"bbox\"]\n\n    # Calculate the center of the bounding box\n    center_y = (bbox[0] + bbox[2]) / 2\n    center_x = (bbox[1] + bbox[3]) / 2\n\n    # Adjust the crop size to ensure it fits within the image boundaries\n    crop_size = (min(image_size[0], max(crop_size[0], center_y)), min(image_size[1], max(crop_size[1], center_x)))\n\n    # Calculate the top-left corner of the crop region\n    crop_y1 = max(0, int(center_y - crop_size[0] / 2))\n    crop_x1 = max(0, int(center_x - crop_size[1] / 2))\n\n    # Adjust the crop size again to ensure it fits within the image boundaries\n    crop_size = (min(image_size[0], crop_size[0], center_y + crop_size[0] / 2 - crop_y1),\n                 min(image_size[1], crop_size[1], center_x + crop_size[1] / 2 - crop_x1))\n\n    # Create and return the CropTransform object\n    crop_transform = CropTransform(crop_x1, crop_y1, crop_size[1], crop_size[0])\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    crop_transform = T.CropTransform.make_crop_transform_with_instance(\n        crop_size, image_size, instance\n    )\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Unpack the crop size\n    crop_h, crop_w = crop_size\n\n    # Unpack the image size\n    image_h, image_w = image_size\n\n    # Unpack the bounding box coordinates from the instance annotation\n    x1, y1, x2, y2 = instance[\"bbox\"]\n\n    # Calculate the width and height of the bounding box\n    box_w = x2 - x1\n    box_h = y2 - y1\n\n    # Calculate the center coordinates of the bounding box\n    box_center_x = x1 + box_w / 2\n    box_center_y = y1 + box_h / 2\n\n    # Calculate the left and top coordinates of the cropping region\n    crop_left = max(0, box_center_x - crop_w / 2)\n    crop_top = max(0, box_center_y - crop_h / 2)\n\n    # Calculate the right and bottom coordinates of the cropping region\n    crop_right = min(image_w, crop_left + crop_w)\n    crop_bottom = min(image_h, crop_top + crop_h)\n\n    # Adjust the left and top coordinates if the cropping region is too wide or tall\n    if crop_right - crop_left < crop_w:\n        crop_left = max(0, crop_left - (crop_w - (crop_right - crop_left)))\n    if crop_bottom - crop_top < crop_h:\n        crop_top = max(0, crop_top - (crop_h - (crop_bottom - crop_top)))\n\n    # Adjust the right and bottom coordinates if the cropping region is too wide or tall\n    if crop_right - crop_left > crop_w:\n        crop_right = min(image_w, crop_right + (crop_w - (crop_right - crop_left)))\n    if crop_bottom - crop_top > crop_h:\n        crop_bottom = min(image_h, crop_bottom + (crop_h - (crop"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    from detectron2.data.transforms import CropTransform\n\n    # Get the bounding box of the instance\n    bbox = instance['bbox']\n\n    # Get the center of the bounding box\n    center = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n\n    # Get the dimensions of the image\n    img_h, img_w = image_size\n\n    # Calculate the top-left corner of the crop\n    crop_y = max(0, int(center[1] - crop_size[0] / 2))\n    crop_x = max(0, int(center[0] - crop_size[1] / 2))\n\n    # Calculate the bottom-right corner of the crop\n    crop_y2 = min(img_h, crop_y + crop_size[0])\n    crop_x2 = min(img_w, crop_x + crop_size[1])\n\n    # Adjust the top-left corner to ensure the crop is centered\n    crop_y = max(0, crop_y - (crop_y2 - crop_y) // 2)\n    crop_x = max(0, crop_x - (crop_x2 - crop_x) // 2)\n\n    # Adjust the bottom-right corner to ensure the crop is centered\n    crop_y2 = min(img_h, crop_y + crop_size[0])\n    crop_x2 = min(img_w, crop_x + crop_size[1])\n\n    # Create a CropTransform object with the calculated parameters\n    crop_transform = CropTransform(crop_y, crop_x, crop_y2, crop_x2)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = instance['bbox']\n\n    # Get the mode of the bounding box (e.g., 'xyxy' for top-left and bottom-right corners)\n    bbox_mode = instance['bbox_mode']\n\n    # Get the center of the bounding box\n    center_x, center_y = get_center(bbox, bbox_mode)\n\n    # Calculate the top-left corner of the crop\n    crop_x1 = max(0, center_x - crop_size[1] // 2)\n    crop_y1 = max(0, center_y - crop_size[0] // 2)\n\n    # Calculate the bottom-right corner of the crop\n    crop_x2 = min(image_size[1], center_x + crop_size[1] // 2)\n    crop_y2 = min(image_size[0], center_y + crop_size[0] // 2)\n\n    # Adjust the cropping region if it exceeds the image boundaries\n    if crop_x1 == 0:\n        crop_x2 = min(image_size[1], crop_x2 + crop_size[1] // 2 - center_x)\n    if crop_y1 == 0:\n        crop_y2 = min(image_size[0], crop_y2 + crop_size[0] // 2 - center_y)\n\n    # Create the CropTransform object with the calculated cropping region\n    crop_transform = CropTransform(crop_x1, crop_y1, crop_x2, crop_y2, image_size)\n\n    return crop_transform\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm = jnp.maximum(x_norm, grad_eps)\n  x_norm = jnp.where(x_norm < grad_eps, 1.0, x_norm)\n  x_norm = x / x_norm\n  return x_norm"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm = jnp.where(norm > grad_eps, norm, grad_eps)\n  return x / norm"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of x\n  sq_norm = jnp.sum(jnp.square(x), axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value to prevent exploding gradients in the backward pass\n  sq_norm = jnp.maximum(sq_norm, grad_eps)\n\n  # Normalize x by dividing it by the squared norm\n  x = x / jnp.sqrt(sq_norm)\n\n  return x"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of x\n  norm = jnp.sum(x ** 2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value to prevent exploding gradients\n  norm = jnp.maximum(norm, grad_eps)\n\n  # Divide x by the clamped squared norm to obtain the normalized array\n  return x / jnp.sqrt(norm)\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm of the input vector(s)\n  sq_norm = jnp.sum(jnp.square(x), axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value to prevent exploding gradients\n  sq_norm = jnp.maximum(sq_norm, grad_eps)\n\n  # Normalize the input vector(s) by dividing by the clamped squared norm\n  x_normalized = x / jnp.sqrt(sq_norm)\n\n  # Return the normalized array\n  return x_normalized"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of x along the last axis\n  norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value to prevent exploding gradients\n  norm = jnp.clip(norm, a_min=grad_eps)\n\n  # Divide x by the square root of the clamped squared norm to obtain the normalized output\n  return x / jnp.sqrt(norm)\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of x\n  x_sq_norm = jnp.sum(x ** 2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value in the forward pass\n  x_sq_norm = jnp.maximum(x_sq_norm, grad_eps)\n\n  # Divide x by the square root of the squared norm\n  x_normalized = x / jnp.sqrt(x_sq_norm)\n\n  # Compute the squared norm of the normalized x\n  x_normalized_sq_norm = jnp.sum(x_normalized ** 2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value in the backward pass\n  x_normalized_sq_norm = jnp.maximum(x_normalized_sq_norm, 1e-6)\n\n  # Multiply the normalized x by the square root of the squared norm\n  x_normalized = x_normalized * jnp.sqrt(x_normalized_sq_norm)\n\n  return x_normalized"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_sq_norm = jnp.sum(x ** 2, axis=-1, keepdims=True)\n  x_norm = jnp.sqrt(jnp.maximum(x_sq_norm, grad_eps))\n  return x / x_norm\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.maximum(jnp.linalg.norm(x, axis=-1, keepdims=True), grad_eps)\n  return x / norm\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm = jnp.maximum(x_norm, grad_eps)\n  x_norm = jnp.minimum(x_norm, 1.0 / jnp.sqrt(grad_eps))\n  return x / x_norm\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm = jnp.maximum(norm, grad_eps)\n  return x / norm\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of x along the last axis\n  sq_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value in the forward pass to prevent division by zero\n  sq_norm = jnp.maximum(sq_norm, grad_eps)\n\n  # Compute the inverse square root of the clamped squared norm\n  inv_sqrt = jnp.sqrt(sq_norm)\n\n  # Clamp the inverse square root to a maximum value in the backward pass to prevent exploding gradients\n  inv_sqrt = jnp.where(inv_sqrt > grad_eps, inv_sqrt, 1.0)\n\n  # Normalize x by dividing it by the inverse square root\n  x_norm = x / inv_sqrt\n\n  return x_norm\n\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of the input vector(s)\n  norm = jnp.sum(x * x, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value in the forward pass\n  norm = jnp.maximum(norm, grad_eps)\n\n  # Compute the reciprocal of the square root of the clamped squared norm\n  norm = jnp.sqrt(norm)\n  norm = jnp.reciprocal(norm)\n\n  # Apply the normalization in the forward pass\n  out = x * norm\n\n  # Compute the gradient of the normalization in the backward pass\n  def grad_fn(dy):\n    # Clamp the squared norm to a minimum value to prevent exploding gradients\n    norm = jnp.maximum(norm, grad_eps)\n    # Compute the gradient of the normalization\n    grad = dy * norm - x * jnp.sum(dy * x, axis=-1, keepdims=True) * norm ** 3\n    return grad\n\n  # Return the normalized array and the gradient function\n  return out, grad_fn\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of the input vector(s)\n  norm_sq = jnp.sum(x * x, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value in the forward pass\n  norm_sq = jnp.maximum(norm_sq, grad_eps)\n\n  # Compute the normalization factor\n  norm = jnp.sqrt(norm_sq)\n\n  # Divide the input vector(s) by the normalization factor\n  x = x / norm\n\n  # Clamp the squared norm to a minimum value in the backward pass\n  norm = jnp.where(norm_sq > grad_eps, norm, 1.0)\n\n  # Compute the gradient of the normalization operation\n  def grad_fn(dy):\n    # Compute the dot product of the input vector(s) and the gradient\n    dot = jnp.sum(x * dy, axis=-1, keepdims=True)\n\n    # Compute the gradient of the normalization operation\n    dx = (dy - x * dot) / norm_sq\n\n    # Clamp the gradient to a minimum value to prevent exploding gradients\n    dx = jnp.where(norm_sq > grad_eps, dx, 0.0)\n\n    return dx\n\n  return x, grad_fn\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of x\n  x_sqr_norm = jnp.sum(x ** 2, axis=-1, keepdims=True)\n\n  # Compute the clamped squared norm for the forward pass\n  x_sqr_norm_clamped = jnp.maximum(x_sqr_norm, grad_eps)\n\n  # Compute the clamped squared norm for the backward pass\n  x_sqr_norm_clamped_backward = jnp.maximum(x_sqr_norm, 1e-15)\n\n  # Compute the normalized array\n  x_normalized = x / jnp.sqrt(x_sqr_norm_clamped)\n\n  # Compute the gradients for the normalized array\n  def grad_normalized(grad_output):\n    # Compute the intermediate variable needed for the gradient computation\n    grad_x_normalized = grad_output * x_normalized\n    grad_x_normalized_sqr_norm = jnp.sum(grad_x_normalized ** 2, axis=-1, keepdims=True)\n\n    # Compute the gradients for the input array\n    grad_x = grad_x_normalized * (x_sqr_norm_clamped_backward ** -0.5) - x_normalized * (grad_x_normalized_sqr_norm * x_sqr_norm_clamped_backward ** -1.5)\n\n    # Return the gradients for the input array\n    return grad_x\n\n  # Return the normalized array and its gradients\n  return x_normalized, grad_normalized\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of x\n  sq_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Compute the gradient-safe denominator\n  denom = jnp.maximum(sq_norm, grad_eps**2)\n\n  # Compute the gradient-safe normalization\n  x_norm = x / jnp.sqrt(denom)\n\n  # Compute the gradient of the normalization\n  grad_denom = -0.5 * x_norm / jnp.sqrt(denom)\n  grad_x_norm = grad_denom * x\n\n  # Compute the gradient of the squared norm\n  grad_sq_norm = grad_denom * 0.5 * x_norm\n\n  # Compute the gradient of the input vector(s)\n  grad_x = grad_x_norm + grad_sq_norm * 2 * x\n\n  return x_norm, grad_x\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of x.\n  sq_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Compute the reciprocal of the squared norm, clamped to a minimum value to prevent exploding gradients.\n  # Clamp the value to prevent division by zero in the backward pass.\n  inv_norm = jnp.sqrt(jnp.maximum(sq_norm, grad_eps))\n\n  # Normalize x by multiplying it by the reciprocal of the squared norm.\n  x = x * jax.lax.rsqrt(sq_norm)\n\n  # Compute the gradient of the normalization operation using the chain rule.\n  def grad_fn(dy):\n    # Compute the gradient of the reciprocal of the squared norm using the chain rule.\n    d_inv_norm = -0.5 * dy * x\n\n    # Compute the gradient of the squared norm using the chain rule.\n    d_sq_norm = d_inv_norm * jnp.maximum(inv_norm, grad_eps)\n\n    # Compute the gradient of x using the chain rule.\n    dx = d_sq_norm * x\n\n    # Return the gradient of x.\n    return dx\n\n  # Return the normalized array and the gradient function.\n  return x, grad_fn"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of x along the last axis\n  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value to prevent division by zero or exploding gradients\n  clamped_squared_norm = jnp.maximum(squared_norm, grad_eps)\n\n  # Normalize x by dividing by the square root of the clamped squared norm\n  normalized_x = x / jnp.sqrt(clamped_squared_norm)\n\n  # Return the normalized array\n  return normalized_x\n\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm = jnp.where(norm < grad_eps, jnp.ones_like(norm), norm)\n  x = x / norm\n  return x\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of the input vector(s)\n  sq_norm = jnp.sum(x ** 2, axis=-1, keepdims=True)\n\n  # Compute the denominator for the forward pass\n  denom = jnp.maximum(sq_norm, grad_eps)\n\n  # Compute the normalized array for the forward pass\n  x_norm = x / jnp.sqrt(denom)\n\n  # Compute the denominator for the backward pass\n  denom_grad = jnp.maximum(sq_norm, 1e-6)\n\n  # Define a function to compute the gradient of the normalization operation\n  def grad_fn(dy):\n    # Compute the gradient of the denominator\n    denom_grad_grad = -0.5 * dy * x_norm * (denom_grad ** -1.5)\n    # Compute the gradient of the normalized array\n    dx = (dy * jnp.sqrt(denom) - jnp.sum(dy * x_norm, axis=-1, keepdims=True) * x_norm) / denom_grad_grad\n    return dx\n\n  # Return the normalized array and the gradient function\n  return x_norm, grad_fn\n\n\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = response.split('Use Agent[')[1].split(']')[0]\n        agent_input = response.split('Use Agent[')[1].split(']')[1].split(':')[1].strip()\n        return agent_name, agent_input\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name, agent_input = agent_info.split(':') if ':' in agent_info else (agent_info, '')\n        return agent_name, agent_input\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0]\n        agent_input = agent_info.split(':')[1] if len(agent_info.split(':')) > 1 else ''\n        return agent_name, agent_input\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Split the response string by 'Use Agent[' to extract the agent information\n        agent_info = response.split(\"Use Agent[\")[1]\n\n        # Split the agent information by ']' to extract the agent name and input text\n        agent_name, input_text = agent_info.split(\"]\")\n\n        # Split the agent name and input text by ':' to extract the input text\n        input_text = input_text.split(\":\")[1]\n\n        # Return the agent name and input text as a tuple\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name, agent_input = agent_info.split(':')\n        return agent_name, agent_input\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name, agent_input = agent_info.split(':')\n        return agent_name, agent_input\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0]\n        input_text = agent_info.split(':')[1] if len(agent_info.split(':')) > 1 else ''\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = response.split('Use Agent[')[1].split(']')[0]\n        agent_input = response.split(':')[1] if ':' in response else ''\n        return agent_name, agent_input\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_input = response.split(\":\")[1] if \":\" in response else \"\"\n        return agent_name, agent_input\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Split the response string by 'Use Agent[' to get the agent information\n        agent_info = response.split('Use Agent[')[1]\n\n        # Split the agent information by ']' to get the agent name and input text\n        agent_name, input_text = agent_info.split(']')\n\n        # Split the agent name by ':' to get the agent name\n        agent_name = agent_name.split(':')[0]\n\n        # Return the agent name and input text as a tuple\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Check if the response string contains the expected format\n        if 'Use Agent[' not in response or ']' not in response:\n            return None, None\n\n        # Extract the agent information from the response string\n        agent_info = response.split('Use Agent[')[1].split(']')[0]\n\n        # Split the agent information into the agent name and input text\n        agent_name, input_text = agent_info.split(':', 1) if ':' in agent_info else (agent_info, '')\n\n        # Return the agent name and input text\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Find the start and end indices of the agent information in the response string\n        start_index = response.find(\"Use Agent[\")\n        end_index = response.find(\"]\", start_index)\n\n        # Extract the agent information from the response string\n        agent_info = response[start_index + 9 : end_index]\n\n        # Split the agent information into the agent's name and input text (if present)\n        agent_name, input_text = agent_info.split(\":\") if \":\" in agent_info else (agent_info, \"\")\n\n        # Return the agent's name and input text as a tuple\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Extract the agent information from the response string\n        agent_info = response.split('Use Agent[')[1].split(']')[0]\n\n        # Split the agent information into the agent name and input text\n        agent_name, input_text = agent_info.split(':')\n\n        # Return the agent name and input text as a tuple\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = re.search(r'Use Agent\\[(.*?)\\]', response).group(1)\n        agent_input = re.search(r'Use Agent\\[.*?\\]:(.*?)$', response)\n        if agent_input:\n            agent_input = agent_input.group(1)\n        else:\n            agent_input = ''\n        return agent_name, agent_input\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0]\n        agent_input = agent_info.split(':')[1] if len(agent_info.split(':')) > 1 else ''\n        return agent_name, agent_input\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Split the response string into two parts, using the delimiter 'Use Agent['\n        response_parts = response.split('Use Agent[')\n\n        # If the response string does not contain the delimiter, return None\n        if len(response_parts) == 1:\n            return None\n\n        # Split the second part of the response string into two parts, using the delimiter ']'\n        agent_info_parts = response_parts[1].split(']')\n\n        # If the second part of the response string does not contain the delimiter, return None\n        if len(agent_info_parts) == 1:\n            return None\n\n        # Split the first part of the second part of the response string into two parts, using the delimiter ':'\n        agent_name_parts = agent_info_parts[0].split(':')\n\n        # If the first part of the second part of the response string does not contain the delimiter, return None\n        if len(agent_name_parts) == 1:\n            return None\n\n        # Extract the agent name and input text from the parts of the response string\n        agent_name = agent_name_parts[0]\n        agent_input = agent_name_parts[1] if len(agent_name_parts) > 1 else ''\n\n        # Return the agent name and input text as a tuple\n        return agent_name, agent_input\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_str = response.split('Use Agent[')[1].split(']')[0]\n        agent_name, input_text = agent_str.split(':')\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0]\n        agent_input = agent_info.split(':')[1] if ':' in agent_info else ''\n        return agent_name, agent_input\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = response.split('Use Agent[')[1].split(']')[0]\n        agent_input = response.split(':')[1] if ':' in response else ''\n        return agent_name, agent_input\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = re.search(r'Use Agent\\[(.*?)\\]', response).group(1)\n        agent_input = re.search(r':\\s*(.*)', response)\n        agent_input = agent_input.group(1) if agent_input else \"\"\n        return agent_name, agent_input\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 3, f\"Expects a 3-dimensional mask, got {segm.ndim}.\"\n                    # mask array\n                    masks.append(segm)\n                else:\n                    raise ValueError(f\"Cannot process {type(segm)} type {segm}\")\n            masks = BitMasks(masks)\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 3, f\"Expects a 3-dimensional mask, got {segm.ndim}.\"\n                    # mask array\n                    masks.append(segm)\n                else:\n                    raise ValueError(\n                        \"Cannot process segmentation of type '{}'\".format(type(segm))\n                    )\n            # take the first actually\n            masks = BitMasks(\n                torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n            )\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 3, f\"Expects a 3-dimensional mask, got {segm.ndim}.\"\n                    # mask array\n                    masks.append(segm)\n                else:\n                    raise ValueError(\n                        f\"Cannot process {type(segm)} type {segm} for polygon format\"\n                    )\n            masks = BitMasks(masks)\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = keypoints\n\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 3, f\"Expects a 3-dimensional mask, got {segm.ndim}.\"\n                    masks.append(segm)\n                else:\n                    raise ValueError(\n                        f\"Cannot process {type(segm)} type {segm} for polygon format\"\n                    )\n            masks = BitMasks(\n                torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n            )\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints("}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(PolygonMasks([segm]))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 3, f\"Expects a 3-dimensional mask, got {segm.ndim}.\"\n                    # mask array\n                    masks.append(segm)\n                else:\n                    raise ValueError(\n                        \"Cannot process segmentation of type '{}'\".format(type(segm))\n                    )\n            masks = BitMasks(masks)\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 3, f\"Expects a binary or encoded mask per instance. Got {segm.shape}.\"\n                    # uncompressed RLE\n                    masks.append(mask_util.decode(mask_util.encode(np.asarray(segm[:, :, None], order=\"F\", dtype=\"uint8\"))[0]))\n                else:\n                    raise ValueError(f\"Cannot process {type(segm)} type {segm}\")\n            masks = BitMasks(masks)\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints ="}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 3, f\"Expects a binary mask with 'ndim==3', got {segm.ndim}.\"\n                    masks.append(segm)\n                else:\n                    raise TypeError(f\"Invalid type for segmentation item: {type(segm)}\")\n            # take first dim as the semantic segmentation map\n            masks = BitMasks(\n                torch.stack([torch.as_tensor(m, dtype=torch.uint8) for m in masks])\n            )\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Ke"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 2, \"Expects a binary mask with 'L' shape\"\n                    masks.append(segm)\n                else:\n                    raise ValueError(\"Cannot convert {} to a mask!\".format(type(segm)))\n            # take first elem to avoid iterating in dataloader\n            masks = BitMasks(torch.stack(masks, dim=0))\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    target = target.remove(\"gt_masks\") if target.has(\"gt_"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 3, f\"Expects a binary mask with 'ndim==3', got {segm.ndim}.\"\n                    masks.append(segm)\n                else:\n                    raise TypeError(\n                        f\"Invalid type for segmentation item: {type(segm)} (expected list or dict or ndarray)\"\n                    )\n            # take first dim as semantic label\n            masks = BitMasks(\n                torch.stack([torch.as_tensor(m, dtype=torch.uint8) for m in masks])\n            )\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 3, f\"Expects a binary mask with 'L' shape, got {segm.shape}.\"\n                    masks.append(segm)\n                else:\n                    raise ValueError(\"Cannot convert segmentation of type '{}' to a bitmap mask!\"\n                                     .format(type(segm)))\n            # take first elem to avoid iterating in __getitem__\n            masks = BitMasks(torch.stack([torch.from_numpy(x) for x in masks]))\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = keyp"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(PolygonMasks([segm]))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 3, f\"Expects a 3-dimensional mask, got {segm.ndim}.\"\n                    # mask array\n                    masks.append(segm)\n                else:\n                    raise ValueError(\n                        \"Cannot process segmentation of type '{}'\".format(type(segm)))\n            masks = BitMasks(masks)\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = keypoints_to_heatmap(kpts, boxes.tensor)\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(PolygonMasks([segm]))\n                else:\n                    # uncompressed RLE\n                    masks.append(BitMasks(segm))\n    else:\n        masks = None\n    target.gt_masks = masks\n\n    keypoints = None\n    if annos[0].get(\"keypoints\", None) is not None:\n        keypoints = [obj.get(\"keypoints\", None) for obj in annos]\n        keypoints = PersonKeypoints(keypoints=keypoints, size=image_size)\n    target.gt_keypoints = keypoints\n\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(PolygonMasks([segm]))\n                else:\n                    # uncompressed RLE\n                    masks.append(mask_util.decode(segm))\n            masks = BitMasks(masks)\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = keypoints_to_heatmap(kpts, boxes.tensor)\n\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 3, f\"Expects a binary or encoded mask, got {segm.ndim}.\"\n                    # uncompressed RLE\n                    masks.append(mask_util.decode(mask_util.encode(np.asarray(segm, order=\"F\", dtype=\"uint8\"))))\n                else:\n                    raise ValueError(f\"Cannot convert {type(segm)} to BitMasks. Supported types are: polygons as list[list], COCO-style RLE as dict, full-image RLE as ndarray.\")\n            # take first dim as semantic label\n            masks = BitMasks.from_bitmasks(masks, *image_size, True).tensor\n        target.gt_masks = masks\n\n    if len(annos) and \"ke"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(PolygonMasks([segm]))\n                else:\n                    # RLE\n                    masks.append(mask_util.decode(segm))\n            masks = BitMasks(masks)\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = keypoints_to_heatmap(kpts, boxes.tensor)\n\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(PolygonMasks([segm]))\n                else:\n                    # uncompressed RLE\n                    masks.append(BitMasks(segm))\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    target = target.remove(\"gt_masks\") if target.has(\"gt_masks\") else target\n    target = target.remove(\"gt_keypoints\") if target.has(\"gt_keypoints\") else target\n\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(PolygonMasks([segm]))\n                else:\n                    # uncompressed RLE\n                    masks.append(BitMasks(segm))\n    else:\n        masks = None\n    target.gt_masks = masks\n\n    keypoints = None\n    if annos and \"keypoints\" in annos[0]:\n        keypoints = [obj.get(\"keypoints\", []) for obj in annos]\n        keypoints = PersonKeypoints(keypoints, boxes.tensor.shape[0])\n    target.gt_keypoints = keypoints\n\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(PolygonMasks([segm]))\n                else:\n                    # uncompressed RLE\n                    masks.append(BitMasks(segm))\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = keypoints\n\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(PolygonMasks([segm]))\n                else:\n                    # uncompressed RLE\n                    masks.append(mask_util.decode(segm))\n            masks = BitMasks(masks)\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(PolygonMasks([segm]))\n                else:\n                    # uncompressed RLE\n                    masks.append(BitMasks(segm))\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = keypoints\n\n    return target\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", join(\"~\", \"skfolio_data\"))\n    data_home = str(Path(data_home).expanduser().resolve())\n    try:\n        os.makedirs(data_home)\n    except (FileExistsError, OSError):\n        pass\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", join(\"~\", \"skfolio_data\"))\n    data_home = Path(data_home).expanduser()\n    data_home.mkdir(exist_ok=True, parents=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", join(\"~\", \"skfolio_data\")\n        )\n    data_home = str(Path(data_home).expanduser())\n    if not exists(data_home):\n        makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", join(\"~\", \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", join(\"~\", \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", join(\"~\", \"skfolio_data\")\n        )\n    data_home = str(Path(data_home).expanduser().resolve())\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", join(\"~\", \"skfolio_data\"))\n    data_home = Path(data_home).expanduser()\n    data_home.mkdir(exist_ok=True)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", join(\"~\", \"skfolio_data\"))\n    data_home = str(Path(data_home).expanduser().resolve())\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", join(\"~\", \"skfolio_data\")\n        )\n    data_home = Path(data_home).expanduser()\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get('SKFOLIO_DATA',\n                                  os.path.join('~', 'skfolio_data'))\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = str(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get('SKFOLIO_DATA',\n                                   os.path.join('~', 'skfolio_data'))\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", join(\"~\", \"skfolio_data\"))\n    data_home = str(Path(data_home).expanduser().resolve())\n    try:\n        os.makedirs(data_home)\n    except (FileExistsError, OSError):\n        pass\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", join(\"~\", \"skfolio_data\"))\n    data_home = str(Path(data_home).expanduser().resolve())\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", join(\"~\", \"skfolio_data\"))\n    data_home = str(Path(data_home).expanduser().resolve())\n    try:\n        os.makedirs(data_home)\n    except OSError as e:\n        if e.errno == errno.EEXIST and os.path.isdir(data_home):\n            pass\n        else:\n            raise ValueError(\"SKFOLIO_DATA could not be created.\")\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", join(\"~\", \"skfolio_data\")\n        )\n    data_home = str(Path(data_home).expanduser())\n    if not exists(data_home):\n        makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    # If data_home is not provided, try to get it from the environment variable 'SKFOLIO_DATA'\n    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", \"~/skfolio_data\")\n\n    # Convert data_home to a Path object\n    data_home = Path(data_home).expanduser()\n\n    # Create the directory if it doesn't exist\n    data_home.mkdir(exist_ok=True, parents=True)\n\n    # Return the path as a string\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get('SKFOLIO_DATA', join('~', 'skfolio_data'))\n    data_home = str(Path(data_home).expanduser())\n    if not exists(data_home):\n        makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get('SKFOLIO_DATA',\n                                   os.path.join('~', 'skfolio_data'))\n    data_home = str(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    cov = np.asarray(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    corr[corr < -1], corr[corr > 1] = -1, 1\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"Covariance matrix must be 2D.\")\n\n    if cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"Covariance matrix must be square.\")\n\n    std_dev = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std_dev, std_dev)\n\n    return corr, std_dev"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    cov = np.asarray(cov)\n    if cov.ndim == 0:\n        cov = np.array([[cov]])\n    elif cov.ndim == 1:\n        cov = np.diag(cov)\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    # Ensure the correlation matrix is symmetric\n    corr = (corr + corr.T) / 2\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"Covariance matrix must be a 2D array.\")\n\n    if cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"Covariance matrix must be square.\")\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    cov = np.asarray(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    corr[corr < -1], corr[corr > 1] = -1, 1  # handle outliers\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    cov = np.asarray(cov)\n    if cov.ndim == 0:\n        cov = np.array([[cov]])\n    elif cov.ndim == 1:\n        cov = np.diag(cov)\n\n    # Calculate the standard deviations for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    # Ensure the correlation matrix is symmetric\n    corr = (corr + corr.T) / 2\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Check if the input is a 2D array\n    if cov.ndim != 2:\n        raise ValueError(\"Input must be a 2D array.\")\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    cov = np.asarray(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    corr[corr < -1], corr[corr > 1] = -1, 1\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Check if the input is a 2D array\n    if cov.ndim != 2:\n        raise ValueError(\"Input must be a 2D array.\")\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    cov = np.asarray(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    corr[corr < -1], corr[corr > 1] = -1, 1  # numerical error\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"Covariance matrix must be 2-dimensional.\")\n\n    # Ensure the covariance matrix is square\n    if cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"Covariance matrix must be square.\")\n\n    # Calculate standard deviations for each variable\n    std_dev = np.sqrt(np.diag(cov))\n\n    # Calculate correlation matrix\n    corr = cov / np.outer(std_dev, std_dev)\n\n    return corr, std_dev"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    cov = np.asarray(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    corr[corr < -1], corr[corr > 1] = -1, 1  # numerical error\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim == 1:\n        cov = np.diag(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    corr[corr < -1], corr[corr > 1] = -1, 1  # numerical error\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim == 1:\n        cov = np.diag(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    corr[corr < -1], corr[corr > 1] = -1, 1  # numerical error\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim == 1:\n        cov = np.diag(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    corr[corr < -1], corr[corr > 1] = -1, 1\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure cov is a 2D array\n    cov = np.atleast_2d(cov)\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    # Handle potential numerical errors in the correlation matrix\n    corr[corr < -1], corr[corr > 1] = -1, 1  # Clip values to the valid range [-1, 1]\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    cov = np.atleast_2d(cov)\n\n    # Calculate the standard deviations for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    # Replace any NaN values with 0\n    corr[np.isnan(corr)] = 0\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    cov = np.atleast_2d(cov)\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    # Return the correlation matrix and standard deviations\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    cov = np.atleast_2d(cov)\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    # Replace any NaN or infinite values with 0\n    corr = np.nan_to_num(corr)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    corr[corr < -1], corr[corr > 1] = -1, 1\n    return corr, std\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    for module in model.modules():\n        module.training = False\n\n    return model"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    class FreezeTrainingMode(object):\n        def __init__(self, model):\n            self.model = model\n            self.original_training_values = {}\n\n        def __enter__(self):\n            for name, module in self.model.named_modules():\n                if hasattr(module, \"training\"):\n                    self.original_training_values[name] = module.training\n                    module.training = False\n\n        def __exit__(self, type, value, traceback):\n            for name, module in self.model.named_modules():\n                if hasattr(module, \"training\"):\n                    module.training = self.original_training_values[name]\n\n    return FreezeTrainingMode(model)"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    class FreezeTrainingMode(object):\n        def __init__(self, model):\n            self.model = model\n            self.original_training_values = {}\n\n        def __enter__(self):\n            for name, module in self.model.named_modules():\n                if hasattr(module, \"training\"):\n                    self.original_training_values[name] = module.training\n                    module.training = False\n\n        def __exit__(self, type, value, traceback):\n            for name, module in self.model.named_modules():\n                if hasattr(module, \"training\"):\n                    module.training = self.original_training_values[name]\n\n    return FreezeTrainingMode(model)"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    for module in model.modules():\n        classname = module.__class__.__name__\n        if classname.find('BatchNorm') != -1:\n            module.eval()\n            module.train = lambda _: None"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    class FreezeTrainingMode(object):\n        def __init__(self, model):\n            self.model = model\n            self.state = {}\n\n        def __enter__(self):\n            for m in self.model.modules():\n                if hasattr(m, 'training'):\n                    self.state[m] = m.training\n                    m.training = False\n\n        def __exit__(self, *args):\n            for m, t in self.state.items():\n                m.training = t\n\n    return FreezeTrainingMode(model)"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    class _FreezeTrainingMode(object):\n\n        def __init__(self, model):\n\n            self.model = model\n\n        def __enter__(self):\n\n            for module in self.model.modules():\n\n                if hasattr(module, 'training'):\n\n                    module.training = False\n\n        def __exit__(self, exc_type, exc_value, traceback):\n\n            for module in self.model.modules():\n\n                if hasattr(module, 'training'):\n\n                    module.training = True\n\n    return _FreezeTrainingMode(model)"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    class FreezeTrainingMode(object):\n        def __init__(self, model):\n            self.model = model\n\n        def __enter__(self):\n            for m in self.model.modules():\n                m.__class__.training = False\n\n        def __exit__(self, exc_type, exc_value, traceback):\n            for m in self.model.modules():\n                m.__class__.training = True\n\n    return FreezeTrainingMode(model)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    class FreezeTrainingMode(object):\n\n        def __init__(self, model):\n            self.model = model\n\n        def __enter__(self):\n            self.prev_training_values = {}\n            for module in self.model.modules():\n                self.prev_training_values[module] = module.training\n                module.__class__.training = False\n\n        def __exit__(self, type, value, traceback):\n            for module in self.model.modules():\n                module.__class__.training = self.prev_training_values[module]\n\n    return FreezeTrainingMode(model)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    class freeze_training_mode_context:\n        def __init__(self, model):\n            self.model = model\n            self.training_states = []\n\n        def __enter__(self):\n            for module in self.model.modules():\n                self.training_states.append(module.training)\n                module.training = False\n\n        def __exit__(self, type, value, traceback):\n            for module, training_state in zip(self.model.modules(), self.training_states):\n                module.training = training_state\n\n    return freeze_training_mode_context(model)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    class FreezeTrainingMode(object):\n\n        def __init__(self, model):\n\n            self.model = model\n            self.training_state = {}\n\n        def __enter__(self):\n\n            for name, module in self.model.named_modules():\n\n                self.training_state[name] = module.training\n                module.training = False\n\n        def __exit__(self, *args):\n\n            for name, module in self.model.named_modules():\n\n                module.training = self.training_state[name]\n\n    return FreezeTrainingMode(model)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    class FreezeTrainingMode(object):\n\n        def __init__(self, model):\n            self.model = model\n\n        def __enter__(self):\n            for module in self.model.modules():\n                module.__class__.training = True\n\n        def __exit__(self, exc_type, exc_val, exc_tb):\n            for module in self.model.modules():\n                module.__class__.training = False\n\n    return FreezeTrainingMode(model)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    class _FrozenTrainingMode(object):\n\n        \"\"\"\n        This class is a context manager that temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation. It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n        \"\"\"\n\n        def __init__(self, model):\n\n            \"\"\"\n            This method initializes the context manager by storing the original values of the \"training\" attributes of all submodules in the given model.\n            Input-Output Arguments\n            :param model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and store their original values.\n            :return: No return values. This method initializes the context manager by storing the original values of the \"training\" attributes of all submodules in the given model.\n            \"\"\"\n\n            self.model = model\n            self.training_values = {}\n\n        def __enter__(self):\n\n            \"\"\"\n            This method is called when the context manager is entered. It temporarily sets the \"training\" attribute of every submodule in the model to a constant value.\n            Input-Output Arguments\n            :return: No return values. This method temporarily sets the \"training\" attribute of every submodule in the model to a constant value.\n            \"\"\"\n\n            for module in self.model.modules():\n                self.training_values[module] = module.training\n                module.training = False\n\n        def __exit__(self, exc_type, exc_value, traceback):\n\n            \"\"\"\n            This method is called when the context manager is exited. It reverts the \"training\" attribute of every submodule in the model back to its original value.\n            Input-Output Arguments\n            :param exc_type: The type of exception that was raised, if any.\n            :param exc_value: The value of the exception that was raised, if any.\n            :param traceback: The traceback of the exception that was raised, if any.\n            :return: No return values. This method reverts the \"training\" attribute of every submodule"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules in the model\n    for submodule in model.modules():\n        # Check if the submodule is a Linear layer\n        if isinstance(submodule, torch.nn.Linear):\n            # Set the \"training\" attribute of the submodule to a constant value\n            submodule.training = False\n\n    # Define a context manager to revert the \"training\" attributes back to their original state\n    class freeze_training_mode(torch.nn.Module):\n        def __enter__(self):\n            # Iterate through all submodules in the model\n            for submodule in model.modules():\n                # Check if the submodule is a Linear layer\n                if isinstance(submodule, torch.nn.Linear):\n                    # Revert the \"training\" attribute of the submodule to its original state\n                    submodule.training = True\n\n        def __exit__(self, exc_type, exc_val, exc_tb):\n            # Iterate through all submodules in the model\n            for submodule in model.modules():\n                # Check if the submodule is a Linear layer\n                if isinstance(submodule, torch.nn.Linear):\n                    # Set the \"training\" attribute of the submodule to a constant value\n                    submodule.training = False\n\n    # Return the context manager\n    return freeze_training_mode()"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules in the model\n    for module in model.modules():\n        # Check if the submodule is a Conv2d module\n        if isinstance(module, torch.nn.Conv2d):\n            # Define a new class that inherits from the original Conv2d class\n            class Conv2d(torch.nn.Conv2d):\n                # Override the forward method\n                def forward(self, x):\n                    # Set the training attribute to False\n                    self.training = False\n                    # Call the original forward method\n                    return super().forward(x)\n\n            # Replace the original Conv2d class with the new one\n            module.__class__ = Conv2d\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules in the given model\n    for submodule in model.modules():\n\n        # Check if the submodule has a \"training\" attribute\n        if hasattr(submodule, \"training\"):\n\n            # Create a new class that inherits from the submodule's class\n            class FreezeAware(submodule.__class__):\n\n                # Define a new method that overrides the \"training\" attribute\n                def __init__(self, *args, **kwargs):\n\n                    # Call the parent class's constructor with the given arguments\n                    super().__init__(*args, **kwargs)\n\n                    # Annotate the \"training\" attribute as a constant\n                    self.training = torch.jit.annotate(torch.Tensor([0]), {})\n\n            # Replace the submodule's class with the new class\n            submodule.__class__ = FreezeAware\n\n    # Return a context manager that temporarily sets the \"training\" attribute of all submodules to a constant value\n    return torch.jit.frozen_module(model, sample_input=torch.ones(1, 3, 224, 224))"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def freeze_training_mode_impl(model):\n\n        \"\"\"\n        This function sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation. It iterates through all submodules and modifies their class definitions.\n        Input-Output Arguments\n        :param model: The model whose submodules' \"training\" attributes are to be annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n        :return: No return values. This function operates by side effects, modifying the class definitions of the model's submodules.\n        \"\"\"\n\n        for module in model.modules():\n            if hasattr(module, 'training'):\n                module.training = False\n\n    class FreezeTrainingMode:\n\n        \"\"\"\n        This class is a context manager that temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation. It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n        \"\"\"\n\n        def __init__(self, model):\n\n            \"\"\"\n            This function initializes the FreezeTrainingMode context manager. It takes a model as input and sets the \"training\" attribute of every submodule in that model to a constant value.\n            Input-Output Arguments\n            :param model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n            :return: No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context.\n            \"\"\"\n\n            freeze_training_mode_impl(model)\n\n        def __enter__(self):\n\n            \"\"\"\n            This function is called when entering the context manager. It does not perform any actions.\n            Input-Output Arguments\n            :return: No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context.\n            \"\"\"\n\n            pass\n\n        def __exit__(self, exc_type, exc_val, exc_"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Define a class that sets the \"training\" attribute of a module to a constant value\n    class _freeze_training_mode:\n        def __init__(self, module, training):\n            self.module = module\n            self.training = training\n\n        def __enter__(self):\n            self.prev_training = self.module.training\n            self.module.training = self.training\n\n        def __exit__(self, *args):\n            self.module.training = self.prev_training\n\n    # Iterate through all submodules in the model and freeze their \"training\" attribute\n    for m in model.modules():\n        if hasattr(m, \"training\"):\n            m.training = False\n\n    # Return a context manager that temporarily sets the \"training\" attribute of all submodules to False\n    return _freeze_training_mode(model, False)"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate over all submodules in the model\n    for module in model.modules():\n\n        # Check if the module has a \"training\" attribute\n        if hasattr(module, \"training\"):\n\n            # Store the original value of the \"training\" attribute\n            module.training_ = module.training\n\n            # Set the \"training\" attribute to a constant value\n            module.training = False\n\n    # Define a context manager that restores the original values of the \"training\" attributes\n    class FreezeTrainingMode:\n        def __enter__(self):\n            pass\n\n        def __exit__(self, type, value, traceback):\n\n            # Iterate over all submodules in the model\n            for module in model.modules():\n\n                # Check if the module has a \"training\" attribute\n                if hasattr(module, \"training\"):\n\n                    # Restore the original value of the \"training\" attribute\n                    module.training = module.training_\n\n    # Return the context manager\n    return FreezeTrainingMode()\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    class freeze_training_mode_context_manager:\n        def __init__(self, model):\n            self.model = model\n            self.training_mode_dict = {}\n\n        def __enter__(self):\n            for name, module in self.model.named_modules():\n                if hasattr(module, \"training\"):\n                    self.training_mode_dict[name] = module.training\n                    module.training = False\n\n        def __exit__(self, exc_type, exc_val, exc_tb):\n            for name, module in self.model.named_modules():\n                if hasattr(module, \"training\"):\n                    module.training = self.training_mode_dict[name]\n\n    return freeze_training_mode_context_manager(model)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules in the model\n    for m in model.modules():\n\n        # Check if the submodule is an instance of torch.nn.Module\n        if isinstance(m, torch.nn.Module):\n\n            # Set the \"training\" attribute of the submodule to a constant value\n            m.training = False\n\n    # Define a context manager that yields the model and reverts the \"training\" attribute of all submodules back to their original state\n    with torch.no_grad():\n\n        # Yield the model to the context\n        yield model\n\n        # Revert the \"training\" attribute of all submodules back to their original state\n        for m in model.modules():\n            if isinstance(m, torch.nn.Module):\n                m.training = True\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shapes(cls, values):\n        shape1 = values[field1].shape\n        shape2 = values[field2].shape\n        if shape1 != shape2:\n            raise ValueError(f\"Shapes of {field1} and {field2} do not match: {shape1} != {shape2}\")\n        return values\n\n    return validate_shapes"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate(cls, values):\n        shape1 = values[field1].shape\n        shape2 = values[field2].shape\n        if shape1 != shape2:\n            raise ValueError(f\"{field1} and {field2} must have the same shape. {field1} has shape {shape1} and {field2} has shape {shape2}.\")\n        return values\n\n    return validate"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate(cls, v, values):\n        shape1 = values[field1].shape\n        shape2 = values[field2].shape\n        if shape1 != shape2:\n            raise ValueError(f\"{field1} and {field2} must have the same shape. Got {shape1} and {shape2}\")\n        return v\n\n    return validate"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validator(model: BaseModel) -> None:\n        if model.__dict__[field1].shape != model.__dict__[field2].shape:\n            raise ValueError(\n                f\"{field1} and {field2} must have the same shape. {field1} shape: {model.__dict__[field1].shape}, {field2} shape: {model.__dict__[field2].shape}\"\n            )\n\n    return validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shapes_equal(cls, v, values, field):\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"Shapes of {field1} and {field2} do not match: {values[field1].shape} != {values[field2].shape}\"\n            )\n        return v\n\n    return validate_shapes_equal"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def _validate_shapes_equal(cls, values):\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{field1} and {field2} must have the same shape. \"\n                f\"{field1} has shape {values[field1].shape} and {field2} has shape {values[field2].shape}.\"\n            )\n        return values\n\n    return _validate_shapes_equal"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validator(cls, v, values, **kwargs):\n        if v.shape != values[field1].shape:\n            raise ValueError(f\"{field2} shape must match {field1} shape\")\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shapes_equal(cls, values):\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{field1} and {field2} must have the same shape. \"\n                f\"{field1} shape: {values[field1].shape}, {field2} shape: {values[field2].shape}\"\n            )\n        return values\n\n    return validator(field1, allow_reuse=True)(validate_shapes_equal)"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def _are_shapes_equal(cls, v: Any, values: Dict[str, Any], **kwargs: Any) -> Any:\n        if v.shape != getattr(values[field1], \"shape\", None):\n            raise ValueError(f\"{field1} and {field2} must have the same shape\")\n        return v\n\n    return validator(field2, allow_reuse=True)(_are_shapes_equal)\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validator(model: pydantic.BaseModel) -> None:\n        data1 = getattr(model, field1)\n        data2 = getattr(model, field2)\n        if data1.shape != data2.shape:\n            raise ValueError(f\"{field1} and {field2} must have the same shape\")\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def shape_equal_validator(cls, v, values, **kwargs):\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{field1} and {field2} must have the same shape.\"\n            )\n        return v\n\n    return shape_equal_validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shapes(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        This function is used to validate the shapes of two fields within a Pydantic model.\n\n        Input-Output Arguments\n        :param cls: The Pydantic model class.\n        :param values: A dictionary containing the values of the fields to be validated.\n        :return: A dictionary containing the validated values.\n        \"\"\"\n        shape1 = values[field1].shape\n        shape2 = values[field2].shape\n        if shape1 != shape2:\n            raise ValueError(\n                f\"Shapes of {field1} and {field2} do not match: {shape1} != {shape2}\"\n            )\n        return values\n\n    return validate_shapes\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validator(cls, v, values):\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(f\"{field1} and {field2} must have the same shape.\")\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validator(cls, v, values, **kwargs):\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{field1} and {field2} must have the same shape.\"\n            )\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shape(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        This function is a Pydantic validator that checks if two specified fields within a model have the same shape.\n\n        Input-Output Arguments\n        :param cls: The Pydantic model class that the validator is being applied to.\n        :param values: A dictionary containing the values of the fields being validated.\n        :return: A dictionary containing the validated values.\n        \"\"\"\n        value1 = values[field1]\n        value2 = values[field2]\n        if value1.shape != value2.shape:\n            raise ValueError(\n                f\"Shape mismatch between {field1} and {field2}: {value1.shape} != {value2.shape}\"\n            )\n        return values\n\n    return validate_shape"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate(cls, v, values):\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(f\"{field1} and {field2} must have the same shape\")\n        return v\n\n    return validator(field1, allow_reuse=True)(validate)\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validator(cls, v, values):\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{field1} and {field2} must have the same shape.\"\n            )\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate(cls, values):\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{field1} and {field2} must have the same shape. \"\n                f\"{field1} shape: {values[field1].shape}, \"\n                f\"{field2} shape: {values[field2].shape}\"\n            )\n        return values\n\n    return validate\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validator(model_class: Type[BaseModel], value: Any) -> Any:\n        if value is None:\n            return value\n        field1_value = getattr(model_class, field1)\n        field2_value = getattr(model_class, field2)\n        if field1_value.shape != field2_value.shape:\n            raise ValueError(f\"{field1} and {field2} must have the same shape\")\n        return value\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        This function is a validator function that compares the shapes of two data structures within a Pydantic model. It is used to ensure that the data structures have the same shape, which is necessary for certain operations that require matching dimensions.\n\n        Input-Output Arguments\n        :param cls: Any, The class object that contains the validator function.\n        :param values: Dict[str, Any], A dictionary containing the values of the fields being validated.\n        :return: Dict[str, Any], The input dictionary of values with no modifications.\n        \"\"\"\n        data1 = values.get(field1)\n        data2 = values.get(field2)\n\n        if data1 is not None and data2 is not None:\n            if data1.shape != data2.shape:\n                raise ValueError(\n                    f\"{field1} and {field2} must have the same shape.\"\n                )\n\n        return values\n\n    return validate"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(metric[\"name\"])\n            metric_params.append(metric.get(\"params\", {}))\n        else:\n            raise ValueError(f\"Invalid metric format: {metric}\")\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_name = metric.pop(\"name\")\n            metric_names.append(metric_name)\n            metric_params.append(metric)\n        else:\n            raise ValueError(f\"Invalid metric format: {metric}\")\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    if isinstance(metrics, list):\n        if all(isinstance(metric, str) for metric in metrics):\n            metric_names = metrics\n            metric_params = [{} for _ in metrics]\n        elif all(isinstance(metric, dict) for metric in metrics):\n            metric_names = [metric.pop(\"name\") for metric in metrics]\n            metric_params = metrics\n        else:\n            raise ValueError(\"Invalid metric format. Expected a list of strings or dictionaries.\")\n    else:\n        raise ValueError(\"Invalid metric format. Expected a list of strings or dictionaries.\")\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if all(isinstance(metric, str) for metric in metrics):\n            metric_names = metrics\n            metric_params = [{} for _ in metrics]\n        elif all(isinstance(metric, dict) for metric in metrics):\n            metric_names = [metric[\"name\"] for metric in metrics]\n            metric_params = [metric.get(\"params\", {}) for metric in metrics]\n        else:\n            raise ValueError(\"Invalid metrics format. Metrics should be a list of strings or dictionaries.\")\n    else:\n        raise ValueError(\"Invalid metrics format. Metrics should be a list.\")\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Initialize empty lists to store metric names and parameters\n    metric_names = []\n    metric_params = []\n\n    # Iterate over the input metrics\n    for metric in metrics:\n        if isinstance(metric, str):\n            # If the metric is a string, it is assumed to be a metric name\n            metric_names.append(metric)\n            # Add an empty dictionary as placeholder for metric parameters\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            # If the metric is a dictionary, it is assumed to contain metric details\n            # Extract the metric name from the dictionary\n            metric_name = metric.pop('name')\n            metric_names.append(metric_name)\n            # Add the remaining dictionary items as metric parameters\n            metric_params.append(metric)\n        else:\n            # Raise an error if the metric is not a string or dictionary\n            raise ValueError(f\"Invalid metric format: {metric}\")\n\n    # Return the extracted metric names and parameters as a tuple\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Initialize empty lists to store metric names and parameters\n    metric_names = []\n    metric_params = []\n\n    # Iterate over each metric in the input list\n    for metric in metrics:\n        # Check if the metric is a string (i.e., a metric name)\n        if isinstance(metric, str):\n            # If it's a string, add it to the list of metric names\n            metric_names.append(metric)\n            # Add an empty dictionary as placeholder for the metric parameters\n            metric_params.append({})\n        # Check if the metric is a dictionary (i.e., a metric with parameters)\n        elif isinstance(metric, dict):\n            # Extract the metric name from the dictionary\n            metric_name = list(metric.keys())[0]\n            # Add the metric name to the list of metric names\n            metric_names.append(metric_name)\n            # Add the metric parameters dictionary to the list of metric parameters\n            metric_params.append(metric[metric_name])\n\n    # Return the list of metric names and the list of metric parameters as a tuple\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if not metrics:\n        return [], []\n    if isinstance(metrics[0], str):\n        return metrics, [{}] * len(metrics)\n    else:\n        return [metric.pop(\"name\") for metric in metrics], metrics\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Initialize empty lists to store metric names and parameters\n    metric_names = []\n    metric_params = []\n\n    # Iterate through each metric in the input list\n    for metric in metrics:\n        # Check if the metric is a string (i.e., a metric name)\n        if isinstance(metric, str):\n            # If the metric is a string, add it to the list of metric names\n            metric_names.append(metric)\n            # Add an empty dictionary as a placeholder for the metric parameters\n            metric_params.append({})\n        # Check if the metric is a dictionary (i.e., a metric with parameters)\n        elif isinstance(metric, dict):\n            # If the metric is a dictionary, extract the metric name and parameters\n            metric_name = list(metric.keys())[0]\n            metric_param = metric[metric_name]\n            # Add the metric name to the list of metric names\n            metric_names.append(metric_name)\n            # Add the metric parameters to the list of metric parameters\n            metric_params.append(metric_param)\n        else:\n            # If the metric is neither a string nor a dictionary, raise a TypeError\n            raise TypeError(\n                \"Metrics must be either a string (metric name) or a dictionary (metric name: parameters).\"\n            )\n\n    # Return a tuple containing the list of metric names and the list of metric parameters\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if not isinstance(metrics, (list, tuple)):\n        raise ValueError(\n            \"Metrics must be a list or a tuple of strings or dictionaries.\"\n        )\n\n    if len(metrics) == 0:\n        raise ValueError(\"Metrics list cannot be empty.\")\n\n    metrics_names = []\n    metrics_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metrics_names.append(metric)\n            metrics_params.append({})\n        elif isinstance(metric, dict):\n            if \"name\" not in metric:\n                raise ValueError(\"Metric dictionary must contain a 'name' key.\")\n\n            metrics_names.append(metric[\"name\"])\n            metrics_params.append(metric)\n        else:\n            raise ValueError(\n                \"Metrics must be strings or dictionaries with 'name' key.\"\n            )\n\n    return metrics_names, metrics_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n    if isinstance(metrics, dict):\n        metrics = [metrics]\n    if not isinstance(metrics, list):\n        raise TypeError(f\"Metrics must be either a string, a dictionary, or a list, not {type(metrics)}\")\n    if not all(isinstance(metric, (str, dict)) for metric in metrics):\n        raise TypeError(f\"Metrics must be either strings or dictionaries, not {type(metrics[0])}\")\n    if all(isinstance(metric, str) for metric in metrics):\n        metrics = [{\"name\": metric} for metric in metrics]\n    if all(isinstance(metric, dict) for metric in metrics):\n        metrics = [metric for metric in metrics]\n    return [metric[\"name\"] for metric in metrics], metrics"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Check if the input is a list of strings or dictionaries\n    if isinstance(metrics, list):\n        # Check if the list is empty\n        if not metrics:\n            raise ValueError(\"The list of metrics is empty.\")\n        # Check if the list contains only strings or dictionaries\n        if all(isinstance(metric, str) for metric in metrics):\n            # Return the list of metric names and an empty list for metric parameters\n            return metrics, []\n        elif all(isinstance(metric, dict) for metric in metrics):\n            # Extract the metric names and parameters from the dictionaries\n            metric_names = [metric.pop(\"name\") for metric in metrics]\n            metric_params = metrics\n            # Return the list of metric names and a list of dictionaries containing metric parameters\n            return metric_names, metric_params\n        else:\n            # Raise an error if the list contains elements of mixed types\n            raise ValueError(\"The list of metrics contains elements of mixed types.\")\n    else:\n        # Raise an error if the input is not a list\n        raise TypeError(\"The input must be a list of strings or dictionaries.\")\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n    if isinstance(metrics, dict):\n        metrics = [metrics]\n    if not isinstance(metrics, list):\n        raise ValueError(f\"Invalid metrics format: {metrics}\")\n\n    metric_names = []\n    metric_params = []\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_name = metric.pop(\"name\")\n            metric_names.append(metric_name)\n            metric_params.append(metric)\n        else:\n            raise ValueError(f\"Invalid metric format: {metric}\")\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Initialize empty lists to store metric names and parameters\n    metric_names = []\n    metric_params = []\n\n    # Iterate over the input metrics\n    for metric in metrics:\n        if isinstance(metric, str):\n            # If the metric is a string, it is assumed to be a metric name\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            # If the metric is a dictionary, it is assumed to contain metric details\n            metric_name = metric.pop(\"name\")\n            metric_names.append(metric_name)\n            metric_params.append(metric)\n        else:\n            # If the metric is neither a string nor a dictionary, raise a TypeError\n            raise TypeError(\n                \"Metrics must be either strings or dictionaries with 'name' and metric parameters\"\n            )\n\n    # Return the extracted metric names and parameters as a tuple\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Initialize empty lists to store metric names and parameters\n    metric_names = []\n    metric_params = []\n\n    # Iterate through the input list\n    for metric in metrics:\n        if isinstance(metric, str):\n            # If the metric is a string, add it to the list of metric names\n            metric_names.append(metric)\n            # Add an empty dictionary as placeholder for the metric parameters\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            # If the metric is a dictionary, extract the metric name and parameters\n            metric_name = metric.pop(\"name\")\n            metric_params.append(metric)\n            metric_names.append(metric_name)\n        else:\n            # Raise an error if the metric is not a string or dictionary\n            raise ValueError(f\"Invalid metric format: {metric}\")\n\n    # Return the extracted metric names and parameters as a tuple\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(metric[\"name\"])\n            metric_params.append(metric.get(\"params\", {}))\n        else:\n            raise ValueError(\n                \"Invalid metric format. Metric must be a string or a dictionary with 'name' and 'params' keys.\"\n            )\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Initialize empty lists to store metric names and parameters\n    metric_names = []\n    metric_params = []\n\n    # Iterate over the input list of metrics\n    for metric in metrics:\n        # If the metric is a string, it represents a metric name\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        # If the metric is a dictionary, it contains metric parameters\n        elif isinstance(metric, dict):\n            # Extract the metric name and parameters\n            metric_name = metric.pop(\"name\")\n            metric_params.append(metric)\n            metric_names.append(metric_name)\n        # If the metric is neither a string nor a dictionary, raise an error\n        else:\n            raise ValueError(\"Metrics must be either strings or dictionaries.\")\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Initialize empty lists for metric names and parameters\n    metric_names = []\n    metric_params = []\n\n    # Iterate over the input list of metrics\n    for metric in metrics:\n        if isinstance(metric, str):\n            # If the metric is a string, it represents a metric name\n            metric_names.append(metric)\n            # Add an empty dictionary as placeholder for metric parameters\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            # If the metric is a dictionary, it contains both the metric name and its parameters\n            metric_name = metric.pop(\"name\")\n            metric_names.append(metric_name)\n            # Add the remaining dictionary items as metric parameters\n            metric_params.append(metric)\n        else:\n            # Raise an error if the metric is neither a string nor a dictionary\n            raise ValueError(f\"Invalid metric type: {type(metric)}\")\n\n    # Return the extracted metric names and parameters as a tuple\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    if isinstance(metrics, list):\n        metric_names = []\n        metric_params = []\n        for metric in metrics:\n            if isinstance(metric, str):\n                metric_names.append(metric)\n                metric_params.append({})\n            elif isinstance(metric, dict):\n                metric_names.append(metric.pop(\"name\"))\n                metric_params.append(metric)\n            else:\n                raise ValueError(\n                    f\"Invalid metric type: {type(metric)}. Expected str or dict.\"\n                )\n        return metric_names, metric_params\n    else:\n        raise ValueError(f\"Invalid metrics type: {type(metrics)}. Expected list.\")\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if not isinstance(metrics, (list, tuple)):\n        raise TypeError(\n            f\"The `metrics` argument must be a list or tuple, but got {type(metrics)}.\"\n        )\n\n    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            if \"name\" in metric:\n                metric_names.append(metric[\"name\"])\n                metric_params.append(metric)\n            else:\n                raise ValueError(\n                    f\"The metric dictionary must contain a 'name' key, but got {metric}.\"\n                )\n        else:\n            raise TypeError(\n                f\"The metric must be a string or a dictionary, but got {type(metric)}.\"\n            )\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n    if isinstance(metrics, dict):\n        metrics = [metrics]\n    if not isinstance(metrics, (list, tuple)):\n        raise TypeError(f\"Expected list or tuple, got {type(metrics)}\")\n    if not all(isinstance(metric, (str, dict)) for metric in metrics):\n        raise TypeError(f\"Expected all elements to be str or dict, got {[type(metric) for metric in metrics]}\")\n\n    metric_names = []\n    metric_params = []\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(metric.pop(\"name\"))\n            metric_params.append(metric)\n\n    return metric_names, metric_params\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Define a dictionary that maps function names to their inverses\n  fn_inv_dict = {\n    torch.sin: torch.arcsin,\n    torch.cos: torch.arccos,\n    torch.tan: torch.arctan,\n    torch.exp: torch.log,\n    torch.log: torch.exp,\n    torch.expm1: torch.log1p,\n    torch.log1p: torch.expm1,\n    torch.sqrt: lambda x: x ** 2,\n    lambda x: x ** 2: torch.sqrt,\n    lambda x: x ** 3: lambda x: x ** (1 / 3),\n    lambda x: x ** (1 / 3): lambda x: x ** 3,\n    lambda x: x ** 4: lambda x: x ** (1 / 4),\n    lambda x: x ** (1 / 4): lambda x: x ** 4,\n    lambda x: x ** 5: lambda x: x ** (1 / 5),\n    lambda x: x ** (1 / 5): lambda x: x ** 5,\n    lambda x: x ** 6: lambda x: x ** (1 / 6),\n    lambda x: x ** (1 / 6): lambda x: x ** 6,\n    lambda x: x ** 7: lambda x: x ** (1 / 7),\n    lambda x: x ** (1 / 7): lambda x: x ** 7,\n    lambda x: x ** 8: lambda x: x ** (1 / 8),\n    lambda x: x ** (1 / 8): lambda x: x ** 8,\n    lambda x: x ** 9: lambda x: x ** (1 / 9),\n    lambda x: x ** (1 / 9): lambda x: x ** 9,\n    lambda x: x ** 10: lambda x: x ** (1 / 10"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    t_clip = torch.clamp(t, t_near, t_far)\n    s_clip = fn(t_clip)\n    return s_clip\n\n  def s_to_t(s):\n    t = fn_inv(s)\n    t_clip = torch.clamp(t, t_near, t_far)\n    return t_clip\n\n  return t_to_s, s_to_t\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n        fn: inv for inv, fn in {\n            tf.math.atanh: tf.math.tanh,\n            tf.math.atan: tf.math.tan,\n            tf.math.acos: tf.math.cos,\n            tf.math.exp: tf.math.log,\n        }.items()\n    }[fn]\n\n  def t_to_s(t):\n    t_clip = tf.clip_by_value(t, t_near, t_far)\n    s_clip = fn(t_clip)\n    return (s_clip - fn(t_near)) / (fn(t_far) - fn(t_near))\n\n  def s_to_t(s):\n    return fn_inv(s * (fn(t_far) - fn(t_near)) + fn(t_near))\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Define a dictionary that maps functions to their inverses\n  fn_inverses = {\n      torch.tanh: torch.atanh,\n      torch.exp: torch.log,\n      torch.expm1: torch.log1p,\n      torch.sin: torch.arcsin,\n      torch.cos: torch.arccos,\n      torch.abs: lambda x: torch.sign(x) * x,\n      torch.reciprocal: lambda x: 1.0 / x,\n      torch.sqrt: lambda x: x ** 2,\n      torch.square: torch.sqrt,\n      torch.asinh: torch.sinh,\n      torch.acosh: torch.cosh,\n      torch.atanh: torch.tanh,\n      torch.log1p: torch.expm1,\n      torch.sinh: torch.asinh,\n      torch.cosh: torch.acosh,\n      torch.tanh: torch.atanh,\n      torch.log: torch.exp,\n      torch.sign: lambda x: torch.abs(x) * torch.sign(x),\n      lambda x: torch.reciprocal(x),\n      lambda x: torch.sqrt(x),\n      lambda x: x ** 2,\n      lambda x: torch.sin(x),\n      lambda x: torch.cos(x),\n      lambda x: torch.abs(x) * torch.sign(x),\n      lambda x: torch.reciprocal(x),\n      lambda x: torch.sqrt(x),\n      lambda x: x ** 2,\n      lambda x: torch.sin(x),\n      lambda x: torch.cos(x),\n      lambda x: torch.abs(x) * torch.sign(x),\n      lambda x: torch.reciprocal(x),\n      lambda"}
{"namespace": "coord.construct_ray_warps", "completion": "  # The function `fn` is used to transform ray distances. It is used in constructing the forward mapping from metric to normalized distances.\n  # The tensor `t_near` represents the near-plane distances in the metric space. It is used as the lower bound for clipping distances to ensure they fall within a valid range.\n  # The tensor `t_far` represents the far-plane distances in the metric space. It is used as the upper bound for clipping distances to ensure they fall within a valid range.\n  # The optional argument `fn_inv` is a function that is the inverse of the `fn` function. If not provided, an attempt is made to automatically determine the inverse based on a predefined mapping of functions to their inverses.\n\n  # The `fn_inv` argument is optional and can be provided or not. If it is not provided, the function attempts to automatically determine the inverse based on a predefined mapping of functions to their inverses.\n\n  # The `fn_inv` argument is optional and can be provided or not. If it is not provided, the function attempts to automatically determine the inverse based on a predefined mapping of functions to their inverses.\n\n  # The `fn_inv` argument is optional and can be provided or not. If it is not provided, the function attempts to automatically determine the inverse based on a predefined mapping of functions to their inverses.\n\n  # The `fn_inv` argument is optional and can be provided or not. If it is not provided, the function attempts to automatically determine the inverse based on a predefined mapping of functions to their inverses.\n\n  # The `fn_inv` argument is optional and can be provided or not. If it is not provided, the function attempts to automatically determine the inverse based on a predefined mapping of functions to their inverses.\n\n  # The `fn_inv` argument is optional and can be provided or not. If it is not provided, the function attempts to automatically determine the inverse based on a predefined mapping of functions to their inverses.\n\n  # The `fn_inv` argument is optional and can be provided or not. If it is not provided, the function attempts to automatically determine the inverse based on a predefined"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Define a mapping of functions to their inverses\n  fn_inv_dict = {\n      torch.sigmoid: torch.logit,\n      torch.log: torch.exp,\n      torch.exp: torch.log,\n      torch.sin: torch.asin,\n      torch.cos: torch.acos,\n      torch.sqrt: lambda x: x**2,\n      torch.square: torch.sqrt,\n      torch.abs: lambda x: torch.sign(x) * x,\n      torch.reciprocal: lambda x: 1.0 / x,\n      torch.log1p: torch.expm1,\n      torch.expm1: torch.log1p,\n      torch.erf: torch.erfinv,\n      torch.erfc: lambda x: 1.0 - torch.erf(x),\n      torch.erfinv: torch.erf,\n      torch.erfcinv: lambda x: 1.0 - torch.erfc(x),\n      torch.softplus: torch.log,\n      torch.log: torch.exp,\n      torch.exp: torch.log,\n      torch.sin: torch.asin,\n      torch.cos: torch.acos,\n      torch.sqrt: lambda x: x**2,\n      torch.square: torch.sqrt,\n      torch.abs: lambda x: torch.sign(x) * x,\n      torch.reciprocal: lambda x: 1.0 / x,\n      torch.log1p: torch.expm1,\n      torch.expm1: torch.log1p,\n      torch.erf: torch.erfinv,\n      torch.erfc: lambda x: 1.0 - torch.erf(x),\n      torch.erfinv: torch.erf,\n      torch.erfcinv: lambda"}
{"namespace": "coord.construct_ray_warps", "completion": "  fn_inv = {\n    torch.sigmoid: torch.logit,\n    torch.log: torch.exp,\n    torch.exp: torch.log,\n    torch.reciprocal: lambda x: 1. / x,\n    torch.square: torch.sqrt,\n    torch.sqrt: torch.square,\n    torch.sin: torch.asin,\n    torch.cos: torch.acos,\n    torch.tan: torch.atan,\n    torch.asin: torch.sin,\n    torch.acos: torch.cos,\n    torch.atan: torch.tan,\n  }[fn] if fn_inv is None else fn_inv\n\n  def t_to_s(t):\n    t_clip = torch.clamp(t, t_near, t_far)\n    s_clip = (t_clip - t_near) / (t_far - t_near)\n    return fn(s_clip)\n\n  def s_to_t(s):\n    s_clip = torch.clamp(s, 0., 1.)\n    t_clip = fn_inv(s_clip) * (t_far - t_near) + t_near\n    return t_clip\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input Arguments\n    :param t: Tensor. Represents the metric distances to be mapped.\n    :return: Tensor. The corresponding normalized distances in the range [0, 1].\n    \"\"\"\n    return 0.5 * (fn(t) / fn(1.0) + 1.0)\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances in the range [0, 1] to metric distances.\n\n    Input Arguments\n    :param s: Tensor. Represents the normalized distances in the range [0, 1] to be mapped.\n    :return: Tensor. The corresponding metric distances.\n    \"\"\"\n    return fn_inv(s * fn(1.0) - fn(0.0))\n\n  def s_to_t_safe(s):\n    \"\"\"\n    Maps normalized distances in the range [0, 1] to metric distances, with clipping to ensure they fall within a valid range.\n\n    Input Arguments\n    :param s: Tensor. Represents the normalized distances in the range [0, 1] to be mapped.\n    :return: Tensor. The corresponding metric distances, clipped to ensure they fall within a valid range.\n    \"\"\"\n    return torch.clamp(s_to_t(s), t_near, t_far)\n\n  return t_to_s, s_to_t_safe\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    t_norm = torch.clip(t, t_near, t_far) / (t_far - t_near)\n    s = fn(t_norm)\n    return s\n\n  def s_to_t(s):\n    t_norm = fn_inv(s)\n    t = t_norm * (t_far - t_near) + t_near\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  fn_inv = fn_inv or {\n      lambda x: jnp.exp(x),\n      lambda x: jnp.reciprocal(1. + x),\n      lambda x: jnp.log(x),\n      lambda x: x,\n  }[fn]\n\n  def t_to_s(t):\n    return fn(t) * (t_far - t_near) + t_near\n\n  def s_to_t(s):\n    return fn_inv((s - t_near) / (t_far - t_near))\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Define a dictionary that maps functions to their inverses\n  fn_inverse_dict = {\n      torch.sin: torch.asin,\n      torch.cos: torch.acos,\n      torch.tan: torch.atan,\n      torch.exp: torch.log,\n      torch.log: torch.exp,\n      torch.expm1: torch.log1p,\n      torch.log1p: torch.expm1,\n      torch.sqrt: torch.square,\n      torch.square: torch.sqrt,\n      torch.abs: lambda x: torch.sign(x) * x,\n      torch.sign: lambda x: torch.abs(x) * torch.sign(x),\n      torch.reciprocal: lambda x: 1.0 / x,\n      lambda x: torch.clip(x, min=0.0, max=1.0): lambda x: torch.clip(x, min=0.0, max=1.0),\n  }\n\n  # If the inverse function is not provided, try to automatically determine it based on the dictionary\n  if fn_inv is None:\n      fn_inv = fn_inverse_dict.get(fn)\n\n  # If the inverse function is still not found, raise an error\n  if fn_inv is None:\n      raise ValueError(f\"Could not find inverse of function {fn} in the dictionary.\")\n\n  # Define the forward and backward mappings\n  def t_to_s(t):\n      return fn(t) / (fn(t_far) - fn(t_near))\n\n  def s_to_t(s):\n      return fn_inv(s * (fn(t_far) - fn(t_near)))\n\n  # Return the forward and backward mappings as a tuple\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  fn_inv = fn_inv or get_inv_fn(fn)\n\n  def t_to_s(t):\n    t_clip = torch.clamp(t, t_near, t_far)\n    s_clip = fn(t_clip)\n    s = (s_clip - fn(t_near)) / (fn(t_far) - fn(t_near))\n    return s\n\n  def s_to_t(s):\n    t = fn_inv(fn(t_near) + (s * (fn(t_far) - fn(t_near))))\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n      torch.sigmoid: lambda x: torch.log(x / (1. - x)),\n      torch.tanh: lambda x: 0.5 * torch.log((1. + x) / (1. - x)),\n    }[fn]\n\n  def t_to_s(t):\n    return 0.5 * (fn(t) + 1.)\n\n  def s_to_t(s):\n    return fn_inv(2. * s - 1.)\n\n  def t_to_s_safe(t):\n    t_near_safe = t_near.expand_as(t)\n    t_far_safe = t_far.expand_as(t)\n    return 0.5 * (fn((t - t_near_safe) / (t_far_safe - t_near_safe)) + 1.)\n\n  def s_to_t_safe(s):\n    t_near_safe = t_near.expand_as(s)\n    t_far_safe = t_far.expand_as(s)\n    return t_near_safe + (t_far_safe - t_near_safe) * fn_inv(2. * s - 1.)\n\n  return t_to_s_safe, s_to_t_safe\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Define a dictionary that maps functions to their inverses\n  fn_inverse_map = {\n      torch.sin: torch.arcsin,\n      torch.cos: torch.arccos,\n      torch.tan: torch.arctan,\n      torch.exp: torch.log,\n      torch.log: torch.exp,\n      torch.sqrt: lambda x: x**2,\n      lambda x: x**2: torch.sqrt,\n  }\n\n  # If the inverse function is not provided, try to determine it from the mapping dictionary\n  if fn_inv is None:\n      if fn in fn_inverse_map:\n          fn_inv = fn_inverse_map[fn]\n      else:\n          raise ValueError(f\"Cannot determine inverse of function {fn}\")\n\n  # Define the forward and backward mappings\n  def t_to_s(t):\n      return 0.5 * (t_far - t_near) * fn((t - t_near) / (t_far - t_near))\n\n  def s_to_t(s):\n      return t_near + (t_far - t_near) * fn_inv(2 * s)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  eps = torch.finfo(t_near.dtype).eps\n\n  if fn_inv is None:\n    fn_inv = {\n        torch.tanh: torch.atanh,\n        torch.exp: torch.log,\n    }[fn]\n\n  t_to_s = lambda t: 0.5 * (torch.erf(fn_inv(t)) + 1.0) * (t_far - t_near) + t_near\n  s_to_t = lambda s: fn((torch.erfinv(2.0 * (s - t_near) / (t_far - t_near) - 1.0) * (t_far - t_near) + t_near))\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Define the inverse function if not provided\n  if fn_inv is None:\n    # Define a mapping from functions to their inverses\n    fn_inv_map = {\n        torch.sin: torch.arcsin,\n        torch.cos: torch.arccos,\n        torch.tan: torch.arctan,\n        torch.exp: torch.log,\n        torch.log: torch.exp,\n        torch.expm1: torch.log1p,\n        torch.log1p: torch.expm1,\n        torch.sqrt: torch.square,\n        torch.square: torch.sqrt,\n        torch.abs: lambda x: x.sign() * x,\n        torch.sign: torch.abs,\n    }\n    # Get the inverse of the input function from the mapping\n    fn_inv = fn_inv_map.get(fn, None)\n    # Raise an error if the inverse is not found\n    if fn_inv is None:\n      raise ValueError(f\"Could not find inverse of function {fn}\")\n\n  # Define the forward and backward functions\n  def t_to_s(t):\n    # Clip the input tensor to ensure it falls within the valid range\n    t = torch.clip(t, t_near, t_far)\n    # Apply the forward function to the tensor\n    s = (fn(t) - fn(t_near)) / (fn(t_far) - fn(t_near))\n    # Clip the output tensor to ensure it falls within the valid range\n    return torch.clip(s, 0, 1)\n\n  def s_to_t(s):\n    # Apply the inverse function to the tensor\n    t = fn_inv(s * (fn(t_far) - fn(t_near)) + fn(t_near))\n    # Clip the output tensor to ensure it falls within the valid range\n    return torch.clip(t, t_near, t_far)\n\n  # Return the forward and backward functions as a tuple\n  return t"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n        torch.sigmoid: lambda t: torch.log(t / (1. - t)),\n        torch.tanh: lambda t: 0.5 * torch.log((1. + t) / (1. - t))\n    }[fn]\n\n  # Construct helper functions\n  fn_inv_ = lambda s: torch.where(s > 0., torch.min(fn_inv(s), t_far), t_near)\n  fn_ = lambda t: torch.where(t > t_near, torch.min(fn(t), 1.), t_near)\n\n  def s_to_t(s):\n    return fn_inv_(s)\n\n  def t_to_s(t):\n    return fn_(t)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  eps = torch.finfo(torch.float32).eps\n\n  if fn_inv is None:\n    if fn == torch.sigmoid:\n      fn_inv = lambda s: torch.log(s / (1 - s + eps))\n    elif fn == torch.tanh:\n      fn_inv = lambda s: 0.5 * torch.log((1 + s) / (1 - s + eps))\n    else:\n      raise ValueError(f\"Unknown inverse function for {fn}.\")\n\n  def t_to_s(t):\n    t_clip = torch.clamp(t, t_near, t_far)\n    t_clamp = (t_clip - t_near) / (t_far - t_near)\n    return fn(t_clamp)\n\n  def s_to_t(s):\n    s_clamp = torch.clamp(s, 0, 1)\n    t_clamp = fn_inv(s_clamp)\n    return t_near + t_clamp * (t_far - t_near)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Define a dictionary that maps functions to their inverses\n  fn_inverse_dict = {\n    torch.tanh: torch.atanh,\n    torch.exp: torch.log,\n    torch.expm1: torch.log1p,\n    torch.log: torch.exp,\n    torch.log1p: torch.expm1,\n    torch.sigmoid: lambda x: torch.log(x / (1 - x)),\n    torch.sin: torch.asin,\n    torch.cos: torch.acos,\n    torch.abs: lambda x: x,\n    torch.reciprocal: lambda x: 1. / x,\n    torch.sqrt: lambda x: x ** 2,\n    torch.square: torch.sqrt,\n    torch.identity: lambda x: x,\n    torch.relu: lambda x: torch.maximum(x, 0),\n    torch.leaky_relu: lambda x, negative_slope=0.01: torch.where(x > 0, x, negative_slope * x),\n    torch.elu: lambda x, alpha=1.0: torch.where(x > 0, x, alpha * (torch.exp(x) - 1))\n  }\n\n  # If the inverse function is not provided, try to find it in the dictionary\n  if fn_inv is None:\n    if fn in fn_inverse_dict:\n      fn_inv = fn_inverse_dict[fn]\n    else:\n      raise ValueError(f\"Function {fn} is not in the dictionary of known inverses.\")\n\n  # Define the forward and backward mappings\n  def t_to_s(t):\n    return 0.5 * (torch.erf(fn(t)) + 1)\n\n  def s_to_t(s):\n    return fn_inv(torch.erfinv(2 * s - 1))\n\n  # Clip the values"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Define a dictionary that maps function names to their inverses\n  fn_inv_dict = {\n    'logit': lambda x, eps=1e-5: -tf.math.log(eps + (1 - x) / (x + eps)),\n    'exp': lambda x: tf.math.exp(x),\n    'softplus': lambda x: tf.math.softplus(x),\n    'sigmoid': lambda x: tf.math.log(x / (1 - x)),\n    'tanh': lambda x: tf.math.atanh(tf.clip_by_value(x, -0.99999997, 0.99999997)),\n    'identity': lambda x: x,\n  }\n\n  # If fn_inv is not provided, try to automatically determine the inverse based on fn_inv_dict\n  if fn_inv is None:\n    fn_name = fn.__name__\n    if fn_name in fn_inv_dict:\n      fn_inv = fn_inv_dict[fn_name]\n    else:\n      raise ValueError(f\"Unable to automatically determine inverse for function {fn_name}.\")\n\n  # Define the forward mapping from metric to normalized distances\n  def t_to_s(t):\n    t_norm = -fn_inv(1. / t)\n    t_norm = tf.clip_by_value(t_norm, t_near, t_far)\n    s = (t_norm - t_near) / (t_far - t_near)\n    return s\n\n  # Define the backward mapping from normalized to metric distances\n  def s_to_t(s):\n    t_norm = t_near + (t_far - t_near) * s\n    t = t_norm / tf.stop_gradient(fn(1. / t_norm))\n    return t\n\n  return t_to_s, s_to_t\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.cos(phi) * np.sin(theta)\n  y = r * np.sin(phi) * np.sin(theta)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  # Importing the necessary libraries\n  import numpy as np\n\n  # Calculating the x, y, and z coordinates based on the spherical coordinates\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  # Returning the cartesian coordinates as an array\n  return np.array([x, y, z])\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) != len(w):\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  # Compute the integral using the trapezoid rule\n  integral = 0\n  for i in range(1, len(t)):\n    integral += (w[i] + w[i-1]) * (t[i] - t[i-1]) / 2\n\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) != len(w):\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  # Compute the integral using the trapezoid rule\n  integral = 0\n  for i in range(1, len(t)):\n    integral += (t[i] - t[i-1]) * (w[i] + w[i-1]) / 2\n\n  return integral\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) != len(w) or len(t) < 2:\n    raise ValueError(\"Invalid input data points for a linear spline\")\n\n  # Compute the integral using the trapezoid rule\n  integral = 0\n  for i in range(1, len(t)):\n    integral += (w[i] + w[i-1]) * (t[i] - t[i-1]) / 2\n\n  return integral\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) != len(w):\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  # Compute the integral using the trapezoid rule\n  integral = 0\n  for i in range(1, len(t)):\n    integral += (w[i] + w[i-1]) * (t[i] - t[i-1]) / 2\n\n  return integral\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) != len(w):\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  # Initialize the integral to zero\n  integral = 0\n\n  # Iterate over the data points and compute the integral using the trapezoid rule\n  for i in range(1, len(t)):\n    integral += (w[i] + w[i-1]) * (t[i] - t[i-1]) / 2\n\n  return integral\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) != len(w):\n    raise ValueError(\"The number of x-coordinates and y-coordinates must be equal.\")\n\n  # Check if the input data points are sorted\n  if not all(t[i] < t[i+1] for i in range(len(t)-1)):\n    raise ValueError(\"The x-coordinates must be sorted in ascending order.\")\n\n  # Compute the integral using the trapezoid rule\n  integral = 0.0\n  for i in range(len(t)-1):\n    integral += 0.5 * (w[i] + w[i+1]) * (t[i+1] - t[i])\n\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  assert len(t) == len(w)\n  assert len(t) > 1\n  assert all(t[i] < t[i+1] for i in range(len(t)-1))\n  assert all(w[i] < w[i+1] for i in range(len(w)-1))\n\n  return sum((w[i] + w[i+1]) * (t[i+1] - t[i]) / 2 for i in range(len(t)-1))"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) != len(w):\n    raise ValueError(\"t and w must have the same length\")\n\n  # Check if the input data points are sorted\n  if not all(t[i] <= t[i+1] for i in range(len(t)-1)):\n    raise ValueError(\"t must be sorted\")\n\n  # Initialize the integral to zero\n  integral = 0.0\n\n  # Loop through the data points and compute the integral using the trapezoid rule\n  for i in range(len(t)-1):\n    integral += 0.5 * (w[i+1] + w[i]) * (t[i+1] - t[i])\n\n  return integral"}
{"namespace": "linspline.integrate", "completion": "  if len(t) != len(w):\n    raise ValueError('t and w must have the same length')\n\n  if len(t) < 2:\n    raise ValueError('t and w must have at least two elements')\n\n  if len(t) == 2:\n    return (t[1] - t[0]) * (w[0] + w[1]) / 2.0\n\n  # Check if the input data points are valid for a linear spline\n  for i in range(len(t) - 1):\n    if t[i] >= t[i + 1]:\n      raise ValueError('t must be strictly increasing')\n\n  # Compute the integral using the trapezoid rule\n  integral = 0.0\n  for i in range(len(t) - 1):\n    integral += (t[i + 1] - t[i]) * (w[i] + w[i + 1]) / 2.0\n\n  return integral"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) != len(w):\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  # Initialize the integral to zero\n  integral = 0\n\n  # Compute the integral using the trapezoid rule\n  for i in range(1, len(t)):\n    integral += (w[i] + w[i-1]) * (t[i] - t[i-1]) / 2\n\n  # Return the computed integral\n  return integral\n\n"}
{"namespace": "linspline.integrate", "completion": "  if len(t) != len(w):\n    raise ValueError('The lengths of t and w must be equal.')\n\n  if len(t) < 2:\n    raise ValueError('At least two data points are required to compute the integral.')\n\n  integral = 0\n\n  for i in range(1, len(t)):\n    h = t[i] - t[i-1]\n    integral += 0.5 * h * (w[i] + w[i-1])\n\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) != len(w):\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  # Initialize the integral to zero\n  integral = 0.0\n\n  # Loop over the data points and compute the integral using the trapezoid rule\n  for i in range(1, len(t)):\n    # Compute the area of the trapezoid formed by the current data point and the previous data point\n    area = (w[i] + w[i-1]) * (t[i] - t[i-1]) / 2.0\n    # Add the area to the integral\n    integral += area\n\n  # Return the computed integral\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) != len(w) + 1:\n    raise ValueError(\"The number of data points in t must be one more than the number of data points in w.\")\n\n  # Initialize the integral to zero\n  integral = 0\n\n  # Loop through the data points and compute the integral using the trapezoid rule\n  for i in range(len(w)):\n    integral += (t[i+1] - t[i]) * (w[i] + w[i+1]) / 2\n\n  # Return the computed integral\n  return integral\n\n"}
{"namespace": "linspline.integrate", "completion": "  if len(t) != len(w):\n    raise ValueError(\"t and w must have the same length.\")\n\n  if len(t) < 2:\n    raise ValueError(\"t and w must have at least two elements.\")\n\n  if not all(t[i] < t[i+1] for i in range(len(t)-1)):\n    raise ValueError(\"t must be strictly increasing.\")\n\n  if not all(w[i] < w[i+1] for i in range(len(w)-1)):\n    raise ValueError(\"w must be strictly increasing.\")\n\n  integral = 0\n  for i in range(len(t)-1):\n    integral += (t[i+1] - t[i]) * (w[i] + w[i+1]) / 2\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  if len(t) != len(w):\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  integral = 0\n  for i in range(1, len(t)):\n    integral += (t[i] - t[i-1]) * (w[i] + w[i-1]) / 2\n\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  # Ensures the input data points are valid for a linear spline\n  if len(t) != len(w):\n    raise ValueError(\"The number of x-coordinates must be equal to the number of y-coordinates.\")\n\n  if len(t) < 2:\n    raise ValueError(\"At least two data points are required to compute the integral.\")\n\n  # Computes the integral using the trapezoid rule\n  integral = 0\n  for i in range(1, len(t)):\n    integral += 0.5 * (t[i] - t[i-1]) * (w[i] + w[i-1])\n\n  return integral\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) != len(w):\n    raise ValueError(\"The number of x-coordinates and y-coordinates must be the same.\")\n\n  # Check if the input data points are sorted\n  if not all(t[i] <= t[i+1] for i in range(len(t)-1)):\n    raise ValueError(\"The x-coordinates must be sorted in ascending order.\")\n\n  # Initialize the integral to zero\n  integral = 0\n\n  # Loop over the data points and compute the integral using the trapezoid rule\n  for i in range(len(t)-1):\n    integral += (w[i] + w[i+1]) * (t[i+1] - t[i]) / 2\n\n  # Return the computed integral\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) != len(w):\n    raise ValueError(\"t and w must have the same length\")\n\n  # Check if the input data points are sorted in ascending order\n  if not all(t[i] < t[i+1] for i in range(len(t)-1)):\n    raise ValueError(\"t must be sorted in ascending order\")\n\n  # Initialize the integral to zero\n  integral = 0\n\n  # Loop through the data points and compute the integral using the trapezoid rule\n  for i in range(len(t)-1):\n    # Compute the area of the trapezoid formed by the current data point and the next one\n    area = (w[i] + w[i+1]) * (t[i+1] - t[i]) / 2\n    # Add the area to the integral\n    integral += area\n\n  # Return the computed integral\n  return integral\n\n\n"}
{"namespace": "linspline.integrate", "completion": "  assert len(t) == len(w), \"t and w must have the same length\"\n\n  integral = 0\n  for i in range(len(t)-1):\n    integral += (t[i+1] - t[i]) * (w[i+1] + w[i]) / 2\n\n  return integral\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if not is_valid_linear_spline(t, w):\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  # Initialize the integral to zero\n  integral = 0.0\n\n  # Loop through the data points and compute the integral using the trapezoid rule\n  for i in range(1, len(t)):\n    integral += (t[i] - t[i-1]) * (w[i] + w[i-1]) / 2.0\n\n  return integral\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize an empty dictionary to store the weighted sums of scores for each ID\n    weighted_sums = {}\n\n    # Loop through each category or group in the 'ids' tuple\n    for i in range(len(ids)):\n        # Loop through each ID in the current category or group\n        for j in range(len(ids[i])):\n            # If the ID is not already in the dictionary, add it with its weighted score\n            if ids[i][j] not in weighted_sums:\n                weighted_sums[ids[i][j]] = scores[i][j] * weights[i]\n            # If the ID is already in the dictionary, add the weighted score to the existing value\n            else:\n                weighted_sums[ids[i][j]] += scores[i][j] * weights[i]\n\n    # Normalize the weighted sums by dividing each value by the sum of all weighted sums\n    total_weighted_sum = sum(weighted_sums.values())\n    for key in weighted_sums:\n        weighted_sums[key] /= total_weighted_sum\n\n    # Sort the dictionary by value in descending order\n    sorted_weighted_sums = sorted(weighted_sums.items(), key=lambda x: x[1], reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    return [x[0] for x in sorted_weighted_sums[:top_k]], [x[1] for x in sorted_weighted_sums[:top_k]]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize an empty dictionary to store the weighted sums of scores for each ID\n    weighted_sums = {}\n\n    # Iterate over the IDs, scores, and weights in parallel\n    for id_list, score_list, weight in zip(ids, scores, weights):\n        # Iterate over the IDs and scores in the current category or group\n        for id, score in zip(id_list, score_list):\n            # If the ID is not already in the dictionary, add it with its weighted score\n            if id not in weighted_sums:\n                weighted_sums[id] = score * weight\n            # If the ID is already in the dictionary, update its weighted score by adding the current score multiplied by the weight\n            else:\n                weighted_sums[id] += score * weight\n\n    # Sort the dictionary by the weighted sums in descending order and get the top K IDs and their corresponding scores\n    top_ids_and_scores = sorted(weighted_sums.items(), key=lambda x: x[1], reverse=True)[:top_k]\n\n    # Extract the top K IDs and their corresponding scores into separate lists\n    top_ids = [id for id, _ in top_ids_and_scores]\n    top_scores = [score for _, score in top_ids_and_scores]\n\n    # Return the top K IDs and their corresponding scores as a tuple\n    return top_ids, top_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Calculate the weighted sum of scores for each ID across all categories or groups\n    weighted_sums = {}\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            if ids[i][j] in weighted_sums:\n                weighted_sums[ids[i][j]] += scores[i][j] * weights[i]\n            else:\n                weighted_sums[ids[i][j]] = scores[i][j] * weights[i]\n\n    # Normalize the weighted sums of scores\n    total_sum = sum(weighted_sums.values())\n    normalized_sums = {key: value / total_sum for key, value in weighted_sums.items()}\n\n    # Sort the IDs based on their weighted sums in descending order\n    sorted_ids = sorted(normalized_sums, key=normalized_sums.get, reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    return sorted_ids[:top_k], [normalized_sums[key] for key in sorted_ids[:top_k]]\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check if the lengths of ids, scores, and weights are equal\n    if len(ids) != len(scores) or len(ids) != len(weights):\n        raise ValueError(\"The lengths of ids, scores, and weights must be equal.\")\n\n    # Check if the length of weights is equal to the number of categories or groups\n    if len(weights) != len(ids):\n        raise ValueError(\"The length of weights must be equal to the number of categories or groups.\")\n\n    # Check if the length of each list within ids is equal to the length of the corresponding list in scores\n    for i in range(len(ids)):\n        if len(ids[i]) != len(scores[i]):\n            raise ValueError(f\"The length of ids[{i}] must be equal to the length of scores[{i}].\")\n\n    # Initialize an empty dictionary to store the weighted sum of scores for each ID\n    weighted_sums = {}\n\n    # Iterate over the categories or groups\n    for i in range(len(ids)):\n        # Iterate over the IDs in the current category or group\n        for j in range(len(ids[i])):\n            # Get the ID and its corresponding score\n            id = ids[i][j]\n            score = scores[i][j]\n\n            # Check if the ID is already in the dictionary\n            if id in weighted_sums:\n                # If it is, add the weighted score to the existing sum\n                weighted_sums[id] += score * weights[i]\n            else:\n                # If it's not, initialize the weighted sum with the current score\n                weighted_sums[id] = score * weights[i]\n\n    # Sort the dictionary by the weighted sums in descending order\n    sorted_weighted_sums = sorted(weighted_sums.items(), key=lambda x: x[1], reverse=True)\n\n    # Get the top K IDs and their corresponding weighted sums\n    top_ids = [x[0] for x in sorted_weighted_sums[:top_k]]\n    top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize an empty dictionary to store the weighted sum of scores for each ID\n    weighted_sum = {}\n\n    # Iterate over the IDs, scores, and weights in parallel\n    for id_list, score_list, weight in zip(ids, scores, weights):\n        # Iterate over the IDs and scores in the current category or group\n        for id, score in zip(id_list, score_list):\n            # If the ID is not in the dictionary, add it with its weighted score\n            if id not in weighted_sum:\n                weighted_sum[id] = score * weight\n            # If the ID is already in the dictionary, add the weighted score to the existing value\n            else:\n                weighted_sum[id] += score * weight\n\n    # Normalize the weighted sums by dividing them by the total weight\n    total_weight = sum(weights)\n    for id in weighted_sum:\n        weighted_sum[id] /= total_weight\n\n    # Sort the IDs based on their weighted sums in descending order\n    sorted_ids = sorted(weighted_sum, key=weighted_sum.get, reverse=True)\n\n    # Return the top K IDs and their corresponding weighted sums\n    return sorted_ids[:top_k], [weighted_sum[id] for id in sorted_ids[:top_k]]\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check if the input tuples have the same length\n    if len(ids) != len(scores) or len(ids) != len(weights):\n        raise ValueError(\"The input tuples must have the same length.\")\n\n    # Check if the number of categories or groups is greater than 0\n    if len(ids) == 0:\n        raise ValueError(\"There must be at least one category or group.\")\n\n    # Check if the number of top IDs to return is greater than 0\n    if top_k <= 0:\n        raise ValueError(\"The number of top IDs to return must be greater than 0.\")\n\n    # Initialize a dictionary to store the weighted sum of scores for each ID\n    weighted_sums = {}\n\n    # Iterate over the IDs and scores in each category or group\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            # Get the ID and score for the current position\n            id = ids[i][j]\n            score = scores[i][j]\n\n            # Check if the ID is already in the dictionary\n            if id in weighted_sums:\n                # If it is, add the weighted score to the existing sum\n                weighted_sums[id] += score * weights[i]\n            else:\n                # If it is not, add the weighted score as a new entry in the dictionary\n                weighted_sums[id] = score * weights[i]\n\n    # Sort the dictionary by the weighted sum of scores in descending order\n    sorted_weighted_sums = sorted(weighted_sums.items(), key=lambda x: x[1], reverse=True)\n\n    # Get the top K IDs and their corresponding scores\n    top_ids = [x[0] for x in sorted_weighted_sums[:top_k]]\n    top_scores = [x[1] for x in sorted_weighted_sums[:top_k]]\n\n    # Return the top K IDs and their corresponding scores\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check if the lengths of the input tuples are the same\n    if len(ids) != len(scores) or len(ids) != len(weights):\n        raise ValueError(\"Lengths of 'ids', 'scores', and 'weights' must be the same.\")\n\n    # Check if the lengths of the lists within the tuples are the same\n    for i in range(len(ids)):\n        if len(ids[i]) != len(scores[i]):\n            raise ValueError(f\"Lengths of 'ids[{i}]' and 'scores[{i}]' must be the same.\")\n\n    # Initialize an empty dictionary to store the weighted sum of scores for each ID\n    weighted_sums = {}\n\n    # Iterate over the IDs and scores for each category or group\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            # Get the ID and score for the current position\n            id = ids[i][j]\n            score = scores[i][j]\n\n            # Check if the ID is already in the dictionary\n            if id in weighted_sums:\n                # If it is, add the weighted score to the existing sum\n                weighted_sums[id] += score * weights[i]\n            else:\n                # If it is not, initialize the sum with the weighted score\n                weighted_sums[id] = score * weights[i]\n\n    # Normalize the weighted sums by dividing them by the sum of all weights\n    total_weight = sum(weights)\n    for id in weighted_sums:\n        weighted_sums[id] /= total_weight\n\n    # Sort the IDs based on their weighted sums in descending order\n    sorted_ids = sorted(weighted_sums, key=weighted_sums.get, reverse=True)\n\n    # Get the top K IDs and their corresponding scores\n    top_ids = sorted_ids[:top_k]\n    top_scores = [weighted_sums[id] for id in top_ids]\n\n    # Return the top"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize an empty dictionary to store the weighted sums of scores\n    weighted_sums = {}\n\n    # Loop through each category or group\n    for i in range(len(ids)):\n\n        # Loop through each ID in the current category or group\n        for j in range(len(ids[i])):\n\n            # Get the current ID and score\n            id = ids[i][j]\n            score = scores[i][j]\n\n            # Calculate the weighted sum of the score for the current category or group\n            weighted_sum = score * weights[i]\n\n            # Add the weighted sum to the dictionary, or update it if it already exists\n            if id in weighted_sums:\n                weighted_sums[id] += weighted_sum\n            else:\n                weighted_sums[id] = weighted_sum\n\n    # Normalize the weighted sums by dividing each sum by the sum of all sums\n    total_sum = sum(weighted_sums.values())\n    for id in weighted_sums:\n        weighted_sums[id] /= total_sum\n\n    # Sort the weighted sums in descending order and get the top K IDs and their corresponding scores\n    top_ids = sorted(weighted_sums, key=weighted_sums.get, reverse=True)[:top_k]\n    top_scores = [weighted_sums[id] for id in top_ids]\n\n    # Return the top K IDs and their corresponding scores as a tuple\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check if the number of categories or groups is equal to the number of weights provided\n    if len(ids) != len(weights):\n        raise ValueError(\"The number of categories or groups must be equal to the number of weights provided.\")\n\n    # Check if the number of scores lists is equal to the number of ID lists\n    if len(scores) != len(ids):\n        raise ValueError(\"The number of scores lists must be equal to the number of ID lists.\")\n\n    # Check if the number of scores in each scores list is equal to the number of IDs in the corresponding ID list\n    for i in range(len(scores)):\n        if len(scores[i]) != len(ids[i]):\n            raise ValueError(\"The number of scores in each scores list must be equal to the number of IDs in the corresponding ID list.\")\n\n    # Check if the top_k value is a positive integer\n    if not isinstance(top_k, int) or top_k <= 0:\n        raise ValueError(\"top_k must be a positive integer.\")\n\n    # Initialize a dictionary to store the weighted sum of scores for each ID\n    weighted_sums = {}\n\n    # Iterate over each category or group\n    for i in range(len(ids)):\n        # Get the scores and weights for the current category or group\n        category_scores = scores[i]\n        category_weights = weights[i]\n\n        # Iterate over each ID and its corresponding score in the current category or group\n        for id, score in zip(ids[i], category_scores):\n            # If the ID is not in the dictionary, add it with its weighted score\n            if id not in weighted_sums:\n                weighted_sums[id] = score * category_weights\n            # If the ID is already in the dictionary, add the weighted score to the existing sum\n            else:\n                weighted_sums[id] += score * category_weights\n\n    # Normalize the weighted sums by dividing them by the sum of all weights\n    total_weight = sum(weights)\n    for id in weighted_sums:\n        weighted_sum"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Checking the lengths of the input tuples\n    assert len(ids) == len(scores) == len(weights)\n\n    # Initializing an empty dictionary to store the weighted sum of scores for each ID\n    weighted_sums = {}\n\n    # Looping through each category or group\n    for i in range(len(ids)):\n        # Looping through each ID in the current category or group\n        for j in range(len(ids[i])):\n            # Checking if the current ID is already in the dictionary\n            if ids[i][j] in weighted_sums:\n                # If it is, adding the weighted score to the existing sum\n                weighted_sums[ids[i][j]] += scores[i][j] * weights[i]\n            else:\n                # If it is not, adding the weighted score as a new entry in the dictionary\n                weighted_sums[ids[i][j]] = scores[i][j] * weights[i]\n\n    # Normalizing the weighted sums by dividing them by the sum of all weights\n    total_weight = sum(weights)\n    for id in weighted_sums:\n        weighted_sums[id] /= total_weight\n\n    # Sorting the dictionary based on the weighted sums in descending order\n    sorted_ids = sorted(weighted_sums, key=weighted_sums.get, reverse=True)\n\n    # Returning the top K IDs and their corresponding weighted sums\n    return sorted_ids[:top_k], [weighted_sums[id] for id in sorted_ids[:top_k]]\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize empty dictionaries to store the weighted sums and normalized scores for each ID\n    weighted_sums = {}\n    normalized_scores = {}\n\n    # Iterate over the IDs, scores, and weights for each category or group\n    for i, (ids_group, scores_group, weight) in enumerate(zip(ids, scores, weights)):\n        # Iterate over the IDs and scores in the current group\n        for id, score in zip(ids_group, scores_group):\n            # If the ID is not in the dictionary, add it with a weighted sum of 0\n            if id not in weighted_sums:\n                weighted_sums[id] = 0\n            # Add the weighted score to the corresponding ID in the dictionary\n            weighted_sums[id] += score * weight\n            # If the ID is not in the normalized scores dictionary, add it with a score of 0\n            if id not in normalized_scores:\n                normalized_scores[id] = 0\n            # Add the score to the corresponding ID in the normalized scores dictionary\n            normalized_scores[id] += score\n\n    # Normalize the scores by dividing them by the sum of all scores for each ID\n    for id in normalized_scores:\n        normalized_scores[id] /= sum(scores[i][j] for i, ids_group in enumerate(ids) for j, id_ in enumerate(ids_group) if id_ == id)\n\n    # Sort the IDs based on their weighted sums in descending order\n    sorted_ids = sorted(weighted_sums.keys(), key=lambda x: weighted_sums[x], reverse=True)\n\n    # Return the top K IDs and their corresponding weighted sums\n    return sorted_ids[:top_k], [weighted_sums[id] for id in sorted_ids[:top_k]]\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check if the lengths of the input tuples match\n    if len(ids) != len(scores) or len(ids) != len(weights):\n        raise ValueError(\"Lengths of ids, scores, and weights must be the same.\")\n\n    # Check if the lengths of each list within the input tuples match\n    for i in range(len(ids)):\n        if len(ids[i]) != len(scores[i]):\n            raise ValueError(\"Lengths of ids and scores must be the same.\")\n\n    # Initialize a dictionary to store the weighted sum of scores for each ID\n    weighted_sums = {}\n\n    # Loop through each category or group of IDs\n    for i in range(len(ids)):\n        # Loop through each ID in the current category or group\n        for j in range(len(ids[i])):\n            # Get the ID and its corresponding score\n            id = ids[i][j]\n            score = scores[i][j]\n            # Calculate the weighted score for the ID\n            weighted_score = score * weights[i]\n            # Add the weighted score to the dictionary\n            if id in weighted_sums:\n                weighted_sums[id] += weighted_score\n            else:\n                weighted_sums[id] = weighted_score\n\n    # Normalize the weighted sums\n    total_sum = sum(weighted_sums.values())\n    for id in weighted_sums:\n        weighted_sums[id] /= total_sum\n\n    # Sort the IDs by their weighted sum in descending order\n    sorted_ids = sorted(weighted_sums, key=weighted_sums.get, reverse=True)\n\n    # Get the top K IDs and their corresponding scores\n    top_ids = sorted_ids[:top_k]\n    top_scores = [weighted_sums[id] for id in top_ids]\n\n    # Return the top K IDs and their corresponding scores\n    return top_ids, top_scores\n\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check if the length of 'ids' and 'scores' tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\"The length of 'ids' and 'scores' tuples must be equal.\")\n\n    # Check if the length of 'weights' tuple matches the length of the first list in 'ids' tuple\n    if len(weights) != len(ids[0]):\n        raise ValueError(\"The length of 'weights' tuple must match the length of the first list in 'ids' tuple.\")\n\n    # Check if 'top_k' is a positive integer\n    if not isinstance(top_k, int) or top_k <= 0:\n        raise ValueError(\"'top_k' must be a positive integer.\")\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_sums = {}\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            weighted_sum = weights[j] * scores[i][j]\n            if ids[i][j] in weighted_sums:\n                weighted_sums[ids[i][j]] += weighted_sum\n            else:\n                weighted_sums[ids[i][j]] = weighted_sum\n\n    # Normalize the weighted sums\n    total_sum = sum(weighted_sums.values())\n    normalized_sums = {key: value / total_sum for key, value in weighted_sums.items()}\n\n    # Sort the IDs based on their weighted sum in descending order\n    sorted_ids = sorted(normalized_sums.keys(), key=lambda x: normalized_sums[x], reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    top_k_ids = sorted_ids[:top_k]\n    top_k_scores = [normalized_sums[key] for key in top_k_ids]\n    return top_k_ids, top_k_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Checking the lengths of the input tuples\n    if len(ids) != len(scores) or len(ids) != len(weights):\n        raise ValueError(\"The lengths of 'ids', 'scores', and 'weights' must be equal.\")\n\n    # Checking the length of the weights tuple\n    if len(weights) != 3:\n        raise ValueError(\"The length of 'weights' must be 3.\")\n\n    # Checking if the weights sum up to 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of weights must be equal to 1.\")\n\n    # Checking if the top_k value is valid\n    if top_k <= 0:\n        raise ValueError(\"The value of 'top_k' must be greater than 0.\")\n\n    # Calculating the weighted sum of scores for each ID\n    weighted_scores = {}\n    for i, id_list in enumerate(ids):\n        for j, id in enumerate(id_list):\n            if id not in weighted_scores:\n                weighted_scores[id] = 0\n            weighted_scores[id] += scores[i][j] * weights[i]\n\n    # Normalizing the weighted scores\n    total_score = sum(weighted_scores.values())\n    if total_score == 0:\n        raise ValueError(\"The sum of all weighted scores must be greater than 0.\")\n    for id in weighted_scores:\n        weighted_scores[id] /= total_score\n\n    # Sorting the weighted scores in descending order\n    sorted_scores = sorted(weighted_scores.items(), key=lambda x: x[1], reverse=True)\n\n    # Returning the top K IDs and their corresponding scores\n    top_ids = [x[0] for x in sorted_scores[:top_k]]\n    top_scores = [x[1] for x in sorted_scores[:top_k]]\n    return top_ids, top_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check if the lengths of scores and weights match\n    assert len(scores) == len(weights), \"The number of scores and weights must be the same.\"\n\n    # Check if the lengths of each list in scores and ids match\n    for i in range(len(scores)):\n        assert len(scores[i]) == len(ids[i]), f\"The number of scores and IDs in category {i} must be the same.\"\n\n    # Initialize a dictionary to store the weighted sum of scores for each ID\n    weighted_scores = {}\n\n    # Iterate through each category or group\n    for i in range(len(scores)):\n        # Iterate through each ID and score in the current category or group\n        for j in range(len(ids[i])):\n            # Get the ID and score\n            id = ids[i][j]\n            score = scores[i][j]\n\n            # If the ID is not in the dictionary, add it with its weighted score\n            if id not in weighted_scores:\n                weighted_scores[id] = score * weights[i]\n            # If the ID is already in the dictionary, add the weighted score to the existing value\n            else:\n                weighted_scores[id] += score * weights[i]\n\n    # Sort the dictionary based on the weighted sum of scores in descending order\n    sorted_scores = sorted(weighted_scores.items(), key=lambda x: x[1], reverse=True)\n\n    # Extract the top K IDs and their corresponding scores\n    top_ids = [x[0] for x in sorted_scores[:top_k]]\n    top_scores = [x[1] for x in sorted_scores[:top_k]]\n\n    # Return the top K IDs and their corresponding scores\n    return top_ids, top_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize an empty dictionary to store the weighted sums of scores for each ID\n    weighted_sums = {}\n\n    # Loop over the IDs, scores, and weights in each category or group\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            # Calculate the weighted sum of the score for the current ID\n            weighted_sum = scores[i][j] * weights[i]\n\n            # If the ID is already in the dictionary, add the weighted sum to the existing value\n            if ids[i][j] in weighted_sums:\n                weighted_sums[ids[i][j]] += weighted_sum\n            # Otherwise, add the ID and its weighted sum to the dictionary\n            else:\n                weighted_sums[ids[i][j]] = weighted_sum\n\n    # Normalize the weighted sums by dividing them by the sum of all weights\n    total_weight = sum(weights)\n    for id in weighted_sums:\n        weighted_sums[id] /= total_weight\n\n    # Sort the weighted sums in descending order and get the top K IDs and their corresponding scores\n    top_ids = sorted(weighted_sums, key=weighted_sums.get, reverse=True)[:top_k]\n    top_scores = [weighted_sums[id] for id in top_ids]\n\n    # Return the top K IDs and their corresponding scores as a tuple\n    return top_ids, top_scores\n\n\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize an empty dictionary to store the weighted sums of scores\n    weighted_sums = {}\n\n    # Iterate over the IDs, scores, and weights for each category or group\n    for i, (ids_list, scores_list, weight) in enumerate(zip(ids, scores, weights)):\n        # Iterate over the IDs and scores for the current category or group\n        for id, score in zip(ids_list, scores_list):\n            # If the ID is not in the dictionary, add it with the weighted score as the value\n            if id not in weighted_sums:\n                weighted_sums[id] = score * weight\n            # If the ID is already in the dictionary, update its weighted score by adding the weighted score of the current category or group\n            else:\n                weighted_sums[id] += score * weight\n\n    # Sort the dictionary by the values in descending order and get the top K IDs and their corresponding scores\n    top_ids_and_scores = sorted(weighted_sums.items(), key=lambda x: x[1], reverse=True)[:top_k]\n\n    # Extract the IDs and scores from the sorted list of tuples\n    top_ids = [tup[0] for tup in top_ids_and_scores]\n    top_scores = [tup[1] for tup in top_ids_and_scores]\n\n    # Return the top K IDs and their corresponding scores as a tuple of lists\n    return top_ids, top_scores\n\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check if the lengths of ids and scores match\n    if len(ids) != len(scores):\n        raise ValueError(\"Lengths of 'ids' and 'scores' do not match.\")\n\n    # Check if the lengths of ids and weights match\n    if len(ids) != len(weights):\n        raise ValueError(\"Lengths of 'ids' and 'weights' do not match.\")\n\n    # Check if the length of weights is equal to the length of the first list in ids\n    if len(weights) != len(ids[0]):\n        raise ValueError(\"Length of 'weights' does not match the length of the first list in 'ids'.\")\n\n    # Initialize an empty dictionary to store the weighted sums of scores for each ID\n    weighted_sums = {}\n\n    # Iterate over the IDs in the first list of the ids tuple\n    for i, id in enumerate(ids[0]):\n        # Initialize the weighted sum for the current ID to 0\n        weighted_sum = 0\n        # Iterate over the indices of the ids tuple\n        for j in range(len(ids)):\n            # Get the index of the current ID in the current list of IDs\n            idx = ids[j].index(id)\n            # Add the weighted score for the current ID to the weighted sum\n            weighted_sum += scores[j][idx] * weights[j]\n        # Add the weighted sum for the current ID to the dictionary of weighted sums\n        weighted_sums[id] = weighted_sum\n\n    # Sort the dictionary of weighted sums in descending order by value\n    sorted_weighted_sums = sorted(weighted_sums.items(), key=lambda x: x[1], reverse=True)\n\n    # Get the top K IDs and their corresponding scores from the sorted dictionary\n    top_ids = [x[0] for x in sorted_weighted_sums[:top_k]]\n    top_scores = [x[1] for x in sorted_weighted_sums[:top_k]]\n\n    # Return the top K IDs and their corresponding scores as a tuple"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check if the lengths of ids, scores, and weights are equal\n    assert len(ids) == len(scores) == len(weights), \"Lengths of ids, scores, and weights must be equal\"\n\n    # Check if the length of weights is equal to the length of the first element of ids\n    assert len(weights) == len(ids[0]), \"Length of weights must be equal to the length of the first element of ids\"\n\n    # Check if top_k is a positive integer\n    assert isinstance(top_k, int) and top_k > 0, \"top_k must be a positive integer\"\n\n    # Initialize an empty dictionary to store the weighted sum of scores for each ID\n    weighted_sums = {}\n\n    # Loop through each category or group of IDs\n    for i in range(len(ids)):\n        # Loop through each ID in the current category or group\n        for j in range(len(ids[i])):\n            # Get the ID and its corresponding score\n            id = ids[i][j]\n            score = scores[i][j]\n            # If the ID is not already in the dictionary, add it with its weighted sum of scores\n            if id not in weighted_sums:\n                weighted_sums[id] = score * weights[i]\n            # If the ID is already in the dictionary, add its weighted sum of scores to the existing value\n            else:\n                weighted_sums[id] += score * weights[i]\n\n    # Normalize the weighted sums by dividing each value by the sum of all weighted sums\n    total_sum = sum(weighted_sums.values())\n    for id in weighted_sums:\n        weighted_sums[id] /= total_sum\n\n    # Sort the weighted sums in descending order\n    sorted_weighted_sums = sorted(weighted_sums.items(), key=lambda x: x[1], reverse=True)\n\n    # Return the top K IDs and their corresponding scores based on the weighted sum\n    return [x[0] for x in sorted_weighted_sums[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check if the lengths of the input tuples are equal\n    if len(ids) != len(scores) or len(ids) != len(weights):\n        raise ValueError(\"The lengths of the input tuples must be equal.\")\n\n    # Check if the lengths of the input tuples are equal to each other\n    if len(ids) != len(scores) or len(scores) != len(weights):\n        raise ValueError(\"The lengths of the input tuples must be equal.\")\n\n    # Check if the weights sum to 1\n    if sum(weights) != 1:\n        raise ValueError(\"The weights must sum to 1.\")\n\n    # Check if the top_k value is valid\n    if top_k <= 0 or top_k > len(ids[0]):\n        raise ValueError(\"The top_k value must be greater than 0 and less than or equal to the number of IDs.\")\n\n    # Initialize an empty dictionary to store the weighted sums of scores for each ID\n    weighted_sums = {}\n\n    # Loop over the IDs, scores, and weights in parallel\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            # Check if the ID is already in the dictionary\n            if ids[i][j] in weighted_sums:\n                # If it is, add the weighted score to the existing sum\n                weighted_sums[ids[i][j]] += scores[i][j] * weights[i]\n            else:\n                # If it is not, create a new entry in the dictionary with the weighted score\n                weighted_sums[ids[i][j]] = scores[i][j] * weights[i]\n\n    # Normalize the weighted sums by dividing each sum by the total sum of all weighted sums\n    total_sum = sum(weighted_sums.values())\n    for id in weighted_sums:\n        weighted_sums[id] /= total_sum\n\n    # Sort the IDs based on their weighted sums in descending order\n    sorted_ids = sorted(weighted_"}
{"namespace": "coord.track_linearize", "completion": "  fn_mean = fn(mean)\n  fn_cov = tf.matmul(tf.matmul(fn_cov, fn_mean, transpose_a=True), fn_mean, transpose_b=True)\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  fn_mean = fn(mean)\n  fn_cov = tf.matmul(cov, fn(mean), adjoint_a=True) + tf.matmul(fn(mean), cov, adjoint_b=True) - tf.matmul(fn_mean, cov, adjoint_b=True)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Get the dimension of the means\n  dim = mean.shape[-1]\n\n  # Linearize the function around the mean\n  fn_mean = fn(mean)\n\n  # Compute the Jacobian matrix of the function\n  jacobian = tf.stack([tf.gradients(fn_mean[..., i], mean)[0] for i in range(dim)], axis=-1)\n\n  # Compute the transformed covariances\n  fn_cov = tf.matmul(jacobian, cov)\n  fn_cov = tf.matmul(fn_cov, tf.transpose(jacobian, [0, 2, 1]))\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Apply the function to the mean\n  fn_mean = fn(mean)\n\n  # Get the Jacobian matrix of the function\n  jacobian = tf.vectorized_map(\n      lambda x: tf.vectorized_map(\n          lambda y: tf.gradients(y, x)[0],\n          fn_mean\n      ),\n      mean\n  )\n\n  # Compute the transformed covariances\n  fn_cov = tf.matmul(jacobian, cov)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of the function at the mean\n  jac = tf.vectorized_map(lambda x: tfp.math.value_and_gradient(fn, x)[1], mean)\n\n  # Compute the transformed means\n  fn_mean = fn(mean)\n\n  # Compute the transformed covariances\n  fn_cov = tf.matmul(jac, cov, transpose_b=True) + tf.matmul(cov, jac, transpose_a=True)\n\n  return fn_mean, fn_cov\n\n\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Calculate the Jacobian matrix of the function at the given mean\n  jacobian = tf.vectorized_map(\n      fn=lambda mean: tf.vectorized_map(\n          fn=lambda x: tf.gradients(ys=fn(mean), xs=mean)[0],\n          elems=mean),\n      elems=mean)\n\n  # Calculate the transformed means by applying the function to the mean\n  fn_mean = fn(mean)\n\n  # Calculate the transformed covariances by applying the Jacobian matrix to the covariances\n  fn_cov = tf.matmul(jacobian, cov)\n  fn_cov = tf.matmul(fn_cov, tf.linalg.matrix_transpose(jacobian))\n\n  # Return the transformed means and covariances\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  fn_mean = fn(mean)\n  fn_jacobian = tf.vectorized_map(lambda x: tfp.math.jacobian(fn, x))(mean)\n  fn_cov = tf.matmul(fn_jacobian, cov, adjoint_b=True)\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Apply the function to the mean and calculate the Jacobian matrix\n  fn_mean = fn(mean)\n  J = tf.vectorized_map(\n      lambda x: tf.gradients(fn(x), x)[0],\n      mean\n  )\n\n  # Calculate the transformed covariances using the Jacobian matrix\n  fn_cov = tf.matmul(J, cov, transpose_a=True)\n  fn_cov = tf.matmul(cov, fn_cov, transpose_b=True)\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of the function at the mean\n  jacobian = tf.vectorized_map(\n      fn=lambda x: tf.vectorized_map(\n          fn=lambda y: tf.gradients(ys=fn(x), xs=y)[0],\n          elems=mean),\n      elems=mean)\n\n  # Compute the transformed means by applying the function to the mean\n  fn_mean = fn(mean)\n\n  # Compute the transformed covariances by applying the Jacobian to the covariances\n  fn_cov = tf.vectorized_map(\n      fn=lambda x, y: tf.matmul(x, y),\n      elems=jacobian,\n      elems2=cov)\n\n  return fn_mean, fn_cov\n\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Get the dimension of the Gaussian means\n  dim = mean.shape[-1]\n\n  # Linearize the function around the mean\n  fn_mean = fn(mean)\n\n  # Get the Jacobian of the function at the mean\n  jacobian = tf.map_fn(lambda x: tf.stack(tf.map_fn(lambda y: tf.gradients(y, x)[0], fn_mean)), mean)\n\n  # Reshape the Jacobian to match the shape of the covariances\n  jacobian = tf.reshape(jacobian, tf.concat([tf.shape(jacobian)[:-1], [dim, dim]], axis=0))\n\n  # Transform the covariances using the Jacobian\n  fn_cov = tf.matmul(jacobian, tf.matmul(cov, jacobian, transpose_b=True))\n\n  # Return the transformed means and covariances\n  return fn_mean, fn_cov\n\n\n\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Get the shape of the mean and covariance tensors\n  mean_shape = mean.shape\n  cov_shape = cov.shape\n\n  # Reshape the mean and covariance tensors to 2D arrays\n  mean = tf.reshape(mean, [-1, mean_shape[-1]])\n  cov = tf.reshape(cov, [-1, cov_shape[-2], cov_shape[-1]])\n\n  # Get the number of samples and dimensions of the mean tensor\n  num_samples = mean.shape[0]\n  num_dims = mean.shape[1]\n\n  # Apply the function to the mean tensor\n  fn_mean = fn(mean)\n\n  # Compute the Jacobian of the function with respect to the mean tensor\n  jacobian = tf.stack([tf.gradients(fn_mean[:, i], mean)[0] for i in range(num_dims)], axis=-1)\n\n  # Compute the transformed covariance tensor using the Jacobian and the covariance tensor\n  fn_cov = tf.matmul(jacobian, cov)\n\n  # Reshape the transformed mean and covariance tensors to their original shapes\n  fn_mean = tf.reshape(fn_mean, mean_shape)\n  fn_cov = tf.reshape(fn_cov, cov_shape)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  fn_mean = fn(mean)\n  fn_jacobian = tf.vectorized_map(lambda x: jacobian(fn, x), mean)\n  fn_cov = tf.matmul(fn_jacobian, cov, transpose_a=True)\n  fn_cov = tf.matmul(cov, fn_jacobian, transpose_b=True)\n  return fn_mean, fn_cov\n"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian matrix of the function at the mean\n  j_mean = tf.squeeze(tf.linalg.diag_part(tf.gradients(fn(mean), mean)[0]), axis=-1)\n\n  # Compute the transformed means\n  fn_mean = fn(mean)\n\n  # Compute the transformed covariances\n  fn_cov = tf.matmul(j_mean, cov)\n  fn_cov = tf.matmul(fn_cov, tf.transpose(j_mean))\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Get the dimension of the Gaussian means\n  dim = mean.shape[-1]\n\n  # Create a tensor of zeros with the same shape as the covariances\n  cov_zero = torch.zeros_like(cov)\n\n  # Create a tensor of zeros with the same shape as the covariances, except the last two dimensions\n  cov_zero_1 = cov_zero[..., :-1, :-1]\n\n  # Create a tensor of zeros with the same shape as the covariances, except the last dimension\n  cov_zero_2 = cov_zero[..., :-1, -1:]\n\n  # Create a tensor of zeros with the same shape as the covariances, except the first dimension\n  cov_zero_3 = cov_zero[..., -1:, :-1]\n\n  # Create a tensor of zeros with the same shape as the covariances, except the first two dimensions\n  cov_zero_4 = cov_zero[..., -1:, -1:]\n\n  # Create a tensor of zeros with the same shape as the covariances, except the first dimension and the last two dimensions\n  cov_zero_5 = cov_zero[..., 1:, 1:-1]\n\n  # Create a tensor of zeros with the same shape as the covariances, except the first dimension and the last dimension\n  cov_zero_6 = cov_zero[..., 1:, -1:]\n\n  # Create a tensor of zeros with the same shape as the covariances, except the first two dimensions and the last dimension\n  cov_zero_7 = cov_zero[..., 1:-1, -1:]\n\n  # Create a tensor of zeros with the same shape as the covariances, except the first two dimensions and the last two dimensions\n  cov_zero_8 = cov_zero[..., 1:-1, 1:-1]\n\n  # Create a tensor of zeros with the same shape as the covariances, except the last two dimensions and the last dimension\n  cov_zero_9 = cov_zero[..., :-1, 1:-1]\n\n  # Create a tensor of zeros with the"}
{"namespace": "coord.track_linearize", "completion": "  fn_mean = fn(mean)\n  fn_cov = fn(cov)\n  return fn_mean, fn_cov\n"}
{"namespace": "coord.track_linearize", "completion": "  # Calculate the Jacobian of the function at the mean\n  jacobian = jacobian_of_function(fn, mean)\n\n  # Apply the function to the mean\n  fn_mean = fn(mean)\n\n  # Transform the covariances using the Jacobian\n  fn_cov = tf.einsum(\"...ij,...jk->...ik\", jacobian, cov)\n\n  return fn_mean, fn_cov\n"}
{"namespace": "coord.track_linearize", "completion": "  # Get the dimension of the mean\n  dim = mean.shape[-1]\n\n  # Create a Jacobian matrix for the function\n  jac = torch.zeros(dim, dim, dtype=mean.dtype, device=mean.device)\n\n  # Calculate the Jacobian matrix using the finite difference method\n  for i in range(dim):\n    # Create a tensor with a small perturbation in the ith element\n    eps = torch.zeros_like(mean)\n    eps[..., i] = 1e-6\n\n    # Calculate the difference between the function applied to the perturbed mean and the original mean\n    diff = fn(mean + eps) - fn(mean - eps)\n\n    # Set the ith column of the Jacobian matrix to the difference divided by twice the perturbation\n    jac[..., i] = diff / 2e-6\n\n  # Calculate the transformed means by applying the function to the original means\n  fn_mean = fn(mean)\n\n  # Calculate the transformed covariances by applying the Jacobian matrix to the original covariances\n  fn_cov = torch.matmul(jac, torch.matmul(cov, jac.transpose(-2, -1)))\n\n  # Return the transformed means and covariances\n  return fn_mean, fn_cov\n\n\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Get the dimension of the mean\n  dim = mean.shape[-1]\n\n  # Linearize the function around the mean\n  fn_mean = fn(mean)\n\n  # Get the Jacobian of the function at the mean\n  jacobian = tf.stack(tf.map_fn(lambda x: tf.gradients(fn(x), x)[0], mean))\n\n  # Compute the transformed covariances\n  fn_cov = tf.matmul(jacobian, cov)\n  fn_cov = tf.matmul(fn_cov, tf.transpose(jacobian, [0, 2, 1]))\n\n  return fn_mean, fn_cov\n"}
{"namespace": "coord.track_linearize", "completion": "  fn_mean = fn(mean)\n  fn_cov = fn_jacobian(fn, mean, cov)\n  return fn_mean, fn_cov\n"}
{"namespace": "coord.track_linearize", "completion": "  mean_shape = tf.shape(mean)\n  mean_dim = mean_shape[-1]\n  mean_shape_static = mean.get_shape()\n  mean_dim_static = mean_shape_static[-1]\n\n  cov_shape = tf.shape(cov)\n  cov_dim = cov_shape[-1]\n  cov_shape_static = cov.get_shape()\n  cov_dim_static = cov_shape_static[-1]\n\n  # Reshape the mean and covariance tensors to have the same number of dimensions\n  mean = tf.reshape(mean, [-1, mean_dim])\n  cov = tf.reshape(cov, [-1, cov_dim, cov_dim])\n\n  # Compute the linearization of the function around the mean\n  fn_mean = fn(mean)\n  fn_mean = tf.reshape(fn_mean, mean_shape)\n\n  # Compute the Jacobian of the function with respect to the mean\n  with tf.GradientTape() as tape:\n    tape.watch(mean)\n    fn_mean_jacobian = tape.batch_jacobian(fn_mean, mean)\n\n  # Compute the transformed covariance using the linearized function and the Jacobian\n  fn_cov = tf.matmul(fn_mean_jacobian, cov)\n  fn_cov = tf.matmul(fn_cov, tf.transpose(fn_mean_jacobian, [0, 2, 1]))\n  fn_cov = tf.reshape(fn_cov, cov_shape)\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[: len(i) // 2], i[len(i) // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if x[i].size > 1:\n            yield [x[i][: x[i].size // 2], x[i][x[i].size // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[:i.size//2], i[i.size//2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[:i.size//2], i[i.size//2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[:i.size//2], i[i.size//2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[: i.size // 2], i[i.size // 2 :]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[:i.size//2], i[i.size//2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[:len(i)//2], i[len(i)//2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[: i.size // 2], i[i.size // 2 :]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[:i.size//2], i[i.size//2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[:i.size//2], i[i.size//2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[:len(i) // 2], i[len(i) // 2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[:len(i)//2], i[len(i)//2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[:len(i)//2], i[len(i)//2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for arr in x:\n        if len(arr) > 1:\n            yield [arr[: len(arr) // 2], arr[len(arr) // 2 :]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[:i.size//2], i[i.size//2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.shape[0] > 1:\n            yield [i[: i.shape[0] // 2], i[i.shape[0] // 2 :]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.shape[0] > 1:\n            yield [i[: i.shape[0] // 2], i[i.shape[0] // 2 :]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[: len(i) // 2], i[len(i) // 2 :]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[:i.size // 2], i[i.size // 2:]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square.\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square.\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if len(x.shape) != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square.\")\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"The input matrix must be a 2-dimensional array.\")\n\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The input matrix must be a square matrix.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square.\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Compute the scales as powers of 2 from min_deg to max_deg\n  scales = 2**torch.linspace(min_deg, max_deg, max_deg - min_deg + 1)\n\n  # Compute the sine and cosine of the scaled input\n  x = x[..., None, :] * scales[:, None]\n  x = torch.cat([x.sin(), x.cos()], dim=-1)\n\n  # Concatenate the original input with the encoded features if append_identity is True\n  if append_identity:\n      x = torch.cat([x, input], dim=-1)\n\n  return x"}
{"namespace": "coord.pos_enc", "completion": "  # Scale the input array\n  scaled_x = [x]\n  for j in range(min_deg, max_deg):\n    scaled_x.append(x*(2.0**j))\n  scaled_x = jnp.concatenate(scaled_x, axis=-1)\n\n  # Apply sine function to the scaled input array\n  sin_x = jnp.sin(scaled_x)\n\n  # Optionally append the original input to the result\n  if append_identity:\n    return jnp.concatenate([sin_x, x], axis=-1)\n  else:\n    return sin_x"}
{"namespace": "coord.pos_enc", "completion": "  # Create a list to store the encoded features\n  encoded_features = []\n\n  # Loop over the degree exponents from min_deg to max_deg\n  for i in range(min_deg, max_deg + 1):\n\n    # Scale the input by 2^i\n    x_scaled = x * (2 ** i)\n\n    # Apply sine function to the scaled input\n    encoded_features.append(np.sin(x_scaled))\n\n    # Apply cosine function to the scaled input\n    encoded_features.append(np.cos(x_scaled))\n\n  # Concatenate the encoded features\n  encoded_features = np.concatenate(encoded_features, axis=-1)\n\n  # If append_identity is True, concatenate the original input with the encoded features\n  if append_identity:\n    encoded_features = np.concatenate([x, encoded_features], axis=-1)\n\n  # Return the encoded features\n  return encoded_features\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Scale the input array using a geometric progression of powers of 2\n  scales = 2.0 ** torch.arange(min_deg, max_deg)\n  xb = x[..., None, :] * scales[:, None]\n\n  # Apply sine and cosine functions to the scaled input\n  xb = torch.cat((xb, xb + 0.5 * np.pi), dim=-1)\n  xb = torch.sin(xb)\n\n  # Optionally concatenate the original input with the encoded features\n  if append_identity:\n      xb = torch.cat((x, xb), dim=-1)\n\n  return xb\n"}
{"namespace": "coord.pos_enc", "completion": "  from jax import numpy as jnp\n  from jax import vmap\n\n  # Compute the number of features\n  num_features = x.shape[-1]\n\n  # Compute the scales as powers of 2 from min_deg to max_deg\n  scales = 2 ** jnp.arange(min_deg, max_deg)\n\n  # Compute the sine and cosine functions of the scaled input\n  x_scaled = x[..., None] * scales[:, None]\n  x_sin = jnp.sin(jnp.concatenate([x_scaled, x_scaled + 0.5 * jnp.pi], axis=-1))\n\n  # Compute the positional encoding\n  pos_enc = jnp.concatenate([x_sin, jnp.cos(x_scaled)], axis=-1)\n\n  # Optionally append the original input to the positional encoding\n  if append_identity:\n    pos_enc = jnp.concatenate([pos_enc, x], axis=-1)\n\n  return pos_enc"}
{"namespace": "coord.pos_enc", "completion": "  from jax import numpy as jnp\n\n  # Create an empty list to store the encoded features\n  encoded_features = []\n\n  # Loop over the specified degree range\n  for i in range(min_deg, max_deg):\n\n    # Scale the input by 2^i and apply sine function\n    scale = 2**i\n    encoded_feature = jnp.sin(scale * x)\n\n    # Append the encoded feature to the list\n    encoded_features.append(encoded_feature)\n\n  # Convert the list to an array\n  encoded_features = jnp.stack(encoded_features, axis=-1)\n\n  # Optionally append the original input to the encoded features\n  if append_identity:\n    encoded_features = jnp.concatenate([x, encoded_features], axis=-1)\n\n  return encoded_features"}
{"namespace": "coord.pos_enc", "completion": "  # Determine the number of frequency bands to use\n  num_bands = max_deg - min_deg\n\n  # Scale the input to the range [-1, 1]\n  x_scaled = x * (2**num_bands)\n\n  # Apply sine and cosine functions to the scaled input to generate the encoded features\n  sin_features = torch.sin(torch.pi * x_scaled)\n  cos_features = torch.cos(torch.pi * x_scaled)\n\n  # Concatenate the encoded features\n  encoded = torch.cat([sin_features, cos_features], dim=-1)\n\n  # Optionally append the original input to the encoded features\n  if append_identity:\n    encoded = torch.cat([x, encoded], dim=-1)\n\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  # Scale the input\n  x = x * (2.**min_deg)\n\n  # Apply sine function to the scaled input\n  encoded = [x]\n  for i in range(min_deg, max_deg):\n    encoded.append(torch.sin(2**i * x))\n    encoded.append(torch.cos(2**i * x))\n\n  # Concatenate the original input with the encoded features\n  if append_identity:\n    encoded = torch.cat(encoded, dim=-1)\n  else:\n    encoded = torch.cat(encoded, dim=-1)\n\n  return encoded\n\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Scaling the input\n  scale = 2**np.linspace(min_deg, max_deg, x.shape[-1])\n  x_ = x[..., np.newaxis, :] * scale[:, np.newaxis]\n\n  # Applying sine function\n  x_sin = np.sin(np.concatenate([x_[..., ::2], x_[..., 1::2]], axis=-1))\n\n  # Appending the original input if specified\n  if append_identity:\n    return np.concatenate([x, x_sin], axis=-1)\n  else:\n    return x_sin\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Generate a sequence of scales from 2^min_deg to 2^max_deg\n  scales = 2 ** torch.linspace(min_deg, max_deg, max_deg - min_deg + 1)\n\n  # Reshape the input tensor to have shape (batch_size, num_inputs, 1)\n  x = x.unsqueeze(-1)\n\n  # Apply the sine function to the scaled input and concatenate the results along the last dimension\n  x = torch.cat([x * scale * math.pi for scale in scales], dim=-1)\n  x = torch.sin(x)\n\n  # If append_identity is True, concatenate the original input along the last dimension\n  if append_identity:\n    x = torch.cat([x, x_], dim=-1)\n\n  return x\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Compute the scales as powers of 2 from min_deg to max_deg\n  scales = 2**torch.linspace(min_deg, max_deg, max_deg-min_deg+1)\n\n  # Compute the sine and cosine of the scaled input\n  x = x[..., None, :] * scales[:, None]\n  x = torch.cat([x.sin(), x.cos()], dim=-1)\n\n  # Optionally append the original input to the result\n  if append_identity:\n    x = torch.cat([x, input], dim=-1)\n\n  return x\n"}
{"namespace": "coord.pos_enc", "completion": "  # Generate the scales as powers of 2 from min_deg to max_deg\n  scales = 2 ** torch.linspace(min_deg, max_deg, max_deg - min_deg + 1)\n\n  # Scale the input array by the scales and apply sine function\n  xb = x[..., None, :] * scales[:, None]\n  xb = torch.sin(torch.cat([xb, xb + 0.5 * np.pi], dim=-1))\n\n  # If append_identity is True, concatenate the original input with the encoded features\n  if append_identity:\n      return torch.cat([x, xb], dim=-1)\n  else:\n      return xb\n"}
{"namespace": "coord.pos_enc", "completion": "  # Scale the input by powers of 2\n  scaled_x = [x]\n  for i in range(min_deg, max_deg):\n    scaled_x.append(x*(2**i))\n\n  # Apply sine function to scaled input\n  sin_x = [torch.sin(x) for x in scaled_x]\n  cos_x = [torch.cos(x) for x in scaled_x]\n\n  # Concatenate sine and cosine functions\n  pos_enc = torch.cat(sin_x + cos_x, dim=-1)\n\n  # Append original input if specified\n  if append_identity:\n    pos_enc = torch.cat([x, pos_enc], dim=-1)\n\n  return pos_enc\n\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Generate the scales as powers of 2 from min_deg to max_deg\n  scales = 2 ** torch.linspace(min_deg, max_deg, max_deg - min_deg + 1)\n\n  # Scale the input by the scales and apply sine and cosine functions\n  xb = torch.reshape(x[..., 0] * scales, [-1, x.shape[-1] * scales.shape[0]])\n  yb = torch.reshape(x[..., 1] * scales, [-1, x.shape[-1] * scales.shape[0]])\n  zb = torch.reshape(x[..., 2] * scales, [-1, x.shape[-1] * scales.shape[0]])\n\n  # Apply sine and cosine functions\n  four_feat = torch.sin(torch.concat([xb, yb, zb], dim=-1))\n\n  # Optionally append the original input to the result\n  if append_identity:\n    return torch.concat([x, four_feat], dim=-1)\n  else:\n    return four_feat\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Scale the input array\n  scale = 2**np.arange(min_deg, max_deg)\n  x_scaled = x[..., np.newaxis] * scale[:, np.newaxis]\n\n  # Apply sine function to the scaled input\n  x_sin = np.sin(np.concatenate([x_scaled, x_scaled + 0.5 * np.pi], axis=-1))\n\n  # Concatenate the original input with the encoded features if append_identity is True\n  if append_identity:\n    return np.concatenate([x, x_sin], axis=-1)\n  else:\n    return x_sin\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Scale the input array\n  scaled_x = [x]\n  for i in range(min_deg, max_deg):\n    factor = 2 ** i\n    scaled_x.append(x * factor)\n\n  # Apply sine function to the scaled array\n  sine_x = [tf.sin(x) for x in scaled_x]\n  cosine_x = [tf.cos(x) for x in scaled_x]\n\n  # Concatenate the sine and cosine functions\n  encoded_x = tf.concat(sine_x + cosine_x, axis=-1)\n\n  # Append the original input to the encoded array\n  if append_identity:\n    encoded_x = tf.concat([x, encoded_x], axis=-1)\n\n  return encoded_x\n"}
{"namespace": "coord.pos_enc", "completion": "  # Scaling the input\n  scaled_x = [x]\n  for j in range(min_deg, max_deg):\n    scaled_x += [x ** (2 ** j)]\n\n  # Applying sine function to the scaled input\n  sin_x = [tf.sin(i) for i in scaled_x]\n  cos_x = [tf.cos(i) for i in scaled_x]\n\n  # Concatenating the sine and cosine functions\n  output = tf.concat(sin_x + cos_x, axis=-1)\n\n  # Appending the original input to the result\n  if append_identity:\n    output = tf.concat([output, x], axis=-1)\n\n  return output\n"}
{"namespace": "coord.pos_enc", "completion": "  # Determine the number of encoding functions to use\n  num_enc = max_deg - min_deg + 1\n  # Scale the input array\n  x_scaled = x * (2.0 ** min_deg)\n  # Initialize an empty list to store the encoded features\n  encoded = []\n  # Loop over the encoding functions\n  for i in range(num_enc):\n      # Apply a sine function to the scaled input\n      encoded_i = jnp.sin(jnp.array(2.0 ** i) * x_scaled)\n      # Append the encoded feature to the list\n      encoded.append(encoded_i)\n  # Concatenate the encoded features along the last axis\n  encoded = jnp.concatenate(encoded, axis=-1)\n  # If append_identity is True, concatenate the original input with the encoded features\n  if append_identity:\n      encoded = jnp.concatenate([x, encoded], axis=-1)\n  # Return the encoded features\n  return encoded\n\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Scaling the input array by powers of 2 from min_deg to max_deg\n  scales = 2**torch.linspace(min_deg, max_deg, max_deg - min_deg + 1)\n  xb = x[..., None, :] * scales[:, None]\n\n  # Applying sine function to the scaled input\n  xb = torch.cat((xb, xb + 0.5 * np.pi), dim=-1)\n  xb = torch.sin(xb)\n\n  # Concatenating the original input with the encoded features if append_identity is True\n  if append_identity:\n      xb = torch.cat((x, xb), dim=-1)\n\n  return xb\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Scale the input to a range of [-1, 1]\n  x = 2.0 * (x - min_deg) / (max_deg - min_deg) - 1.0\n\n  # Check if the input is a single number\n  if len(x.shape) == 0:\n    x = x[None]  # Add a dimension to make it a 1D array\n\n  # Generate the scales as powers of 2\n  scales = 2 ** torch.linspace(min_deg, max_deg, max_deg - min_deg + 1)\n\n  # Apply the sine function to the scaled input\n  x = x[..., None] * scales[:, None]\n  x = torch.sin(torch.cat([x, x + 0.5 * torch.pi], dim=-1))\n\n  # Concatenate the original input with the encoded features if append_identity is True\n  if append_identity:\n    x = torch.cat([x, x.new_zeros(list(x.shape[:-1]) + [1])], dim=-1)\n\n  return x\n\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate(cls, values):\n        list1 = values[field1]\n        list2 = values[field2]\n\n        if len(list1) != len(list2):\n            raise ValueError(f\"{field1} and {field2} must have the same length.\")\n\n        for i, (arr1, arr2) in enumerate(zip(list1, list2)):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"Arrays at index {i} in {field1} and {field2} must have the same shape.\")\n\n        return values\n\n    return validate"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validator(cls, values):\n        list1 = values.get(field1)\n        list2 = values.get(field2)\n        if list1 is None or list2 is None:\n            return values\n        if len(list1) != len(list2):\n            raise ValueError(f\"{field1} and {field2} must have the same length\")\n        for arr1, arr2 in zip(list1, list2):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"Arrays in {field1} and {field2} must have the same shape\")\n        return values\n\n    return validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate(cls, values):\n        list1 = values.get(field1)\n        list2 = values.get(field2)\n\n        if len(list1) != len(list2):\n            raise ValueError(f\"Lists {field1} and {field2} must have the same length\")\n\n        for i, (arr1, arr2) in enumerate(zip(list1, list2)):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"Arrays at index {i} in {field1} and {field2} must have the same shape\"\n                )\n\n        return values\n\n    return validate"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validator(cls, values):\n        field1_values = values.get(field1)\n        field2_values = values.get(field2)\n\n        if len(field1_values) != len(field2_values):\n            raise ValueError(f\"Number of {field1} and {field2} arrays must be equal.\")\n\n        for i, (arr1, arr2) in enumerate(zip(field1_values, field2_values)):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"Shape of {field1}[{i}] and {field2}[{i}] must be equal.\")\n\n        return values\n\n    return validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validator(cls, v):\n        if len(v[field1]) != len(v[field2]):\n            raise ValueError(f\"{field1} and {field2} must have the same length\")\n        for arr1, arr2 in zip(v[field1], v[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"All arrays in {field1} must have the same shape as their corresponding arrays in {field2}\")\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate(cls, values):\n        list1 = values[field1]\n        list2 = values[field2]\n\n        if len(list1) != len(list2):\n            raise ValueError(\n                f\"{field1} and {field2} must have the same length.\"\n            )\n\n        for arr1, arr2 in zip(list1, list2):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"Arrays in {field1} and {field2} must have the same shape.\"\n                )\n\n        return values\n\n    return validate\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate(cls, values):\n        shapes1 = [array.shape for array in values.get(field1)]\n        shapes2 = [array.shape for array in values.get(field2)]\n        if len(shapes1) != len(shapes2):\n            raise ValueError(f\"Number of arrays in {field1} and {field2} must be equal.\")\n        for shape1, shape2 in zip(shapes1, shapes2):\n            if shape1 != shape2:\n                raise ValueError(f\"Shapes of arrays in {field1} and {field2} must be equal.\")\n        return values\n\n    return validate\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validator(cls, values):\n        arr1 = values[field1]\n        arr2 = values[field2]\n        if len(arr1) != len(arr2):\n            raise ValueError(f\"{field1} and {field2} must have the same length\")\n        for i, (a1, a2) in enumerate(zip(arr1, arr2)):\n            if a1.shape != a2.shape:\n                raise ValueError(f\"{field1}[{i}] and {field2}[{i}] must have the same shape\")\n        return values\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def shape_equal(cls, values):\n        list1 = values.get(field1)\n        list2 = values.get(field2)\n\n        if len(list1) != len(list2):\n            raise ValueError(\n                f\"{field1} and {field2} must have the same length.\"\n            )\n\n        for arr1, arr2 in zip(list1, list2):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"Arrays in {field1} and {field2} must have the same shape.\"\n                )\n\n        return values\n\n    return shape_equal\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def shape_equal_validator(cls, values):\n        list1 = values[field1]\n        list2 = values[field2]\n        if len(list1) != len(list2):\n            raise ValueError(f\"{field1} and {field2} must have the same length.\")\n        for arr1, arr2 in zip(list1, list2):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{field1} and {field2} must have the same shape.\")\n        return values\n\n    return shape_equal_validator\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validator(cls, v):\n\n        \"\"\"\n        This function is a validator function that checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: class type. The class type of the model that the validator function is being applied to.\n        :param v: dict. A dictionary containing the values of the fields to be validated.\n        :return: dict. The validated values of the fields, if the shape check passes.\n\n        \"\"\"\n\n        if len(v[field1]) != len(v[field2]):\n            raise ValueError(\n                f\"The length of {field1} and {field2} must be equal.\"\n            )\n\n        for i in range(len(v[field1])):\n            if v[field1][i].shape != v[field2][i].shape:\n                raise ValueError(\n                    f\"The shape of {field1}[{i}] and {field2}[{i}] must be equal.\"\n                )\n\n        return v\n\n    return validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def shape_equal_validator(cls: Type[BaseModel], values: Dict[str, Any]) -> Dict[str, Any]:\n\n        \"\"\"\n        This function is a Pydantic validator that checks if two lists of numpy arrays have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: Type[BaseModel]. The class type of the model being validated.\n        :param values: Dict[str, Any]. A dictionary of values to be validated.\n        :return: Dict[str, Any]. The validated values if the check passes.\n\n        \"\"\"\n\n        field1_value = values.get(field1)\n        field2_value = values.get(field2)\n\n        if field1_value is not None and field2_value is not None:\n            if len(field1_value) != len(field2_value):\n                raise ValueError(f\"{field1} and {field2} must have the same length\")\n\n            for i, (arr1, arr2) in enumerate(zip(field1_value, field2_value)):\n                if arr1.shape != arr2.shape:\n                    raise ValueError(f\"{field1}[{i}] and {field2}[{i}] must have the same shape\")\n\n        return values\n\n    return shape_equal_validator\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate(cls, values):\n        \"\"\"\n        This function is the actual validator function that is returned by the are_all_shapes_equal function.\n\n        Input-Output Arguments\n        :param cls: The class type that the validator is applied to.\n        :param values: A dictionary of values that need to be validated.\n        :return: The validated values if the shape equality check passes.\n        \"\"\"\n        list1 = values[field1]\n        list2 = values[field2]\n\n        if len(list1) != len(list2):\n            raise ValueError(f\"{field1} and {field2} must have the same length\")\n\n        for i, (arr1, arr2) in enumerate(zip(list1, list2)):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"{field1}[{i}] and {field2}[{i}] must have the same shape\"\n                )\n\n        return values\n\n    return validate"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate(cls, values: dict) -> dict:\n        \"\"\"\n        This function checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: Type[Model]. The class type that the validator is applied to.\n        :param values: dict. A dictionary containing the values of the fields to be validated.\n        :return: dict. The validated values if the check passes.\n\n        \"\"\"\n\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"The length of {field1} and {field2} must be equal.\")\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(\n                    f\"The shape of {field1}[{i}] and {field2}[{i}] must be equal.\")\n\n        return values\n\n    return validate\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate_shapes_equal(cls, values):\n        list1 = values[field1]\n        list2 = values[field2]\n        if len(list1) != len(list2):\n            raise ValueError(f\"Lists of {field1} and {field2} must have the same length.\")\n        for i in range(len(list1)):\n            if list1[i].shape != list2[i].shape:\n                raise ValueError(f\"Arrays at index {i} in {field1} and {field2} must have the same shape.\")\n        return values\n\n    return validate_shapes_equal\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate(cls, values):\n        list1 = values.get(field1)\n        list2 = values.get(field2)\n\n        if len(list1) != len(list2):\n            raise ValueError(\n                f\"The number of arrays in {field1} and {field2} must be equal.\"\n            )\n\n        for i in range(len(list1)):\n            if list1[i].shape != list2[i].shape:\n                raise ValueError(\n                    f\"The shape of the arrays in {field1} and {field2} must be equal.\"\n                )\n\n        return values\n\n    return validate\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validator(cls, values: dict) -> dict:\n        \"\"\"\n        This function is a Pydantic validator that checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: class type. The class type that the validator is applied to.\n        :param values: dict. The dictionary of values that are being validated.\n        :return: dict. The validated values if the check passes.\n        \"\"\"\n        list1 = values.get(field1)\n        list2 = values.get(field2)\n        if len(list1) != len(list2):\n            raise ValueError(f\"{field1} and {field2} must have the same length\")\n        for arr1, arr2 in zip(list1, list2):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{field1} and {field2} must have the same shape\")\n        return values\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate(cls, values):\n\n        \"\"\"\n        This function acts as a Pydantic validator. It checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: Type. The class type that the validator is applied to.\n        :param values: dict. A dictionary of values to be validated.\n        :return: dict. The validated values if the check passes.\n\n        \"\"\"\n\n        # Check if the two fields exist in the values dictionary\n        if field1 not in values or field2 not in values:\n            raise ValueError(f\"Both '{field1}' and '{field2}' must be present in the input values.\")\n\n        # Get the values of the two fields\n        list1 = values[field1]\n        list2 = values[field2]\n\n        # Check if the lengths of the two lists are equal\n        if len(list1) != len(list2):\n            raise ValueError(f\"The length of '{field1}' and '{field2}' must be equal.\")\n\n        # Check if each corresponding pair of arrays within the two lists has the same shape\n        for arr1, arr2 in zip(list1, list2):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"The shape of corresponding arrays in '{field1}' and '{field2}' must be equal.\")\n\n        # Return the validated values\n        return values\n\n    return validate"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate(cls, values):\n        field1_data = values.get(field1)\n        field2_data = values.get(field2)\n\n        if len(field1_data) != len(field2_data):\n            raise ValueError(\n                f\"{field1} and {field2} must have the same length.\")\n\n        for arr1, arr2 in zip(field1_data, field2_data):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"All arrays in {field1} must have the same shape as their corresponding arrays in {field2}.\")\n\n        return values\n\n    return validate\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate(cls, values: dict) -> dict:\n\n        \"\"\"\n        This function validates the input values by checking if the specified lists of numpy arrays have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: class type. The class type of the Pydantic model that is being validated.\n        :param values: dict. A dictionary containing the values of the fields to be validated.\n        :return: dict. The validated values.\n\n        \"\"\"\n\n        # Get the values of the specified fields\n        field1_value = values.get(field1)\n        field2_value = values.get(field2)\n\n        # Check if the fields are not empty\n        if field1_value is not None and field2_value is not None:\n\n            # Check if the lists have the same length\n            if len(field1_value) != len(field2_value):\n                raise ValueError(f\"The lists of {field1} and {field2} must have the same length.\")\n\n            # Check if each pair of arrays have the same shape\n            for arr1, arr2 in zip(field1_value, field2_value):\n                if arr1.shape != arr2.shape:\n                    raise ValueError(f\"The arrays in {field1} and {field2} must have the same shape.\")\n\n        return values\n\n    return validate"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(camera)"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize_context(camera.width, camera.height)\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera=camera,\n                    eglctx=eglctx,\n                    depth=True,\n                    color=True,\n                    normals=True,\n                    )\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize_context(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Set the viewport to match the camera's dimensions\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n\n        # Reset the rendering context to its original dimensions\n        eglctx.resize(self.eglctx.width, self.eglctx.height)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize_context(camera.width, camera.height)\n\n        self.render(camera)\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        pad_token_id=bert_config.pad_token_id,\n        bos_token_id=bert_config.bos_token_id,\n        eos_token_id=bert_config.eos_token_id,\n        is_encoder_decoder=bert_config.is_encoder_decoder,\n        decoder_start_token_id=bert_config.decoder_start_token_id,\n        add_cross_attention=bert_config.add_cross_attention,\n        pruned_heads=bert_config.pruned_heads,\n        output_attentions=bert_config.output_attentions,\n        output_hidden_states=bert_config.output_hidden_states,\n        use_return_dict=bert_config.use_return_dict,\n        output_past=bert_config.output_past,\n        output_attentions_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig()\n    config.vocab_size = bert_config.vocab_size\n    config.hidden_size = bert_config.hidden_size\n    config.num_hidden_layers = bert_config.num_hidden_layers\n    config.num_attention_heads = bert_config.num_attention_heads\n    config.intermediate_size = bert_config.intermediate_size\n    config.hidden_act = bert_config.hidden_act\n    config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    config.max_position_embeddings = bert_config.max_position_embeddings\n    config.type_vocab_size = bert_config.type_vocab_size\n    config.initializer_range = bert_config.initializer_range\n    config.layer_norm_eps = bert_config.layer_norm_eps\n    config.pad_token_id = bert_config.pad_token_id\n    config.gradient_checkpointing = bert_config.gradient_checkpointing\n    config.position_embedding_type = bert_config.position_embedding_type\n    config.use_cache = bert_config.use_cache\n    config.classifier_dropout = bert_config.classifier_dropout\n    config.num_labels = bert_config.num_labels\n    config.problem_type = bert_config.problem_type\n    config.use_return_dict = bert_config.use_return_dict\n    config.output_attentions = bert_config.output_attentions\n    config.output_hidden_states = bert_config.output_hidden_states\n    config.torchscript = bert_config.torchscript\n    config.pruned_heads = bert_config.pruned_heads\n    config.is_decoder = bert_config.is"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig()\n    config.vocab_size = bert_config.vocab_size\n    config.hidden_size = bert_config.hidden_size\n    config.num_hidden_layers = bert_config.num_hidden_layers\n    config.num_attention_heads = bert_config.num_attention_heads\n    config.intermediate_size = bert_config.intermediate_size\n    config.hidden_act = bert_config.hidden_act\n    config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    config.max_position_embeddings = bert_config.max_position_embeddings\n    config.type_vocab_size = bert_config.type_vocab_size\n    config.initializer_range = bert_config.initializer_range\n    config.layer_norm_eps = bert_config.layer_norm_eps\n    config.pad_token_id = bert_config.pad_token_id\n    config.gradient_checkpointing = bert_config.gradient_checkpointing\n    config.position_embedding_type = bert_config.position_embedding_type\n    config.use_cache = bert_config.use_cache\n    config.classifier_dropout = bert_config.classifier_dropout\n    config.num_labels = bert_config.num_labels\n    config.output_attentions = bert_config.output_attentions\n    config.output_hidden_states = bert_config.output_hidden_states\n    config.problem_type = bert_config.problem_type\n    config.torch_dtype = bert_config.torch_dtype\n    config.use_return_dict = bert_config.use_return_dict\n    config.use_mems_eval = bert_config.use_mems_eval\n    config.use_mems_train ="}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object\n    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        num_labels=bert_config.num_labels,\n        output_attentions=bert_config.output_attentions,\n        output_hidden_states=bert_config.output_hidden_states,\n        return_dict=bert_config.return_dict,\n        output_past=bert_config.output_past,\n        output_logits=bert_config.output_logits,\n        output_values=bert_config.output_values,\n        output_attentions_only=bert_config.output_attentions_only,\n        output_hidden_states_only=bert_config.output_hidden_states_only,\n        output_logits_only=bert_config.output_logits"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        pad_token_id=bert_config.pad_token_id,\n        bos_token_id=bert_config.bos_token_id,\n        eos_token_id=bert_config.eos_token_id,\n        sep_token_id=bert_config.sep_token_id,\n        unk_token_id=bert_config.unk_token_id,\n        mask_token_id=bert_config.mask_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        use_bfloat16=bert_config.use_bfloat16,\n        use_bias=bert_config.use_bias,\n        use_bias_in_att=bert_config.use_bias_in_att,\n        use_bias_in_ffn=bert_config.use_bias_in_ffn"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        pad_token_id=bert_config.pad_token_id,\n        bos_token_id=bert_config.bos_token_id,\n        eos_token_id=bert_config.eos_token_id,\n        sep_token_id=bert_config.sep_token_id,\n        output_attentions=bert_config.output_attentions,\n        output_hidden_states=bert_config.output_hidden_states,\n        return_dict=bert_config.return_dict,\n        use_return_dict=bert_config.use_return_dict,\n        vocab_size_or_config_json_file=bert_config.vocab_size_or_config_json_file,\n        pretrained_model_name_or_path=bert_config.pretrained_model_name_or_path,\n        gradient_checkpointing=bert_config.gradient"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object\n    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        num_labels=bert_config.num_labels,\n        output_attentions=bert_config.output_attentions,\n        output_hidden_states=bert_config.output_hidden_states,\n        return_dict=bert_config.return_dict,\n        use_return_dict=bert_config.use_return_dict,\n        output_past=bert_config.output_past,\n        output_hidden_states_past=bert_config.output_hidden_states_past,\n        use_mems_eval=bert_config.use_mems_eval,\n        use_mems_train=bert_config.use_mems_train,\n        mem_len=bert_config.mem"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        num_labels=bert_config.num_labels,\n        output_attentions=bert_config.output_attentions,\n        output_hidden_states=bert_config.output_hidden_states,\n        return_dict=bert_config.return_dict,\n        use_return_dict=bert_config.use_return_dict,\n        # new arguments\n        vocab_size_nomic=bert_config.vocab_size,\n        hidden_size_nomic=bert_config.hidden_size,\n        num_hidden_layers_nomic=bert_config.num_hidden_layers,\n        num_attention_heads_nomic=bert_config.num_attention_heads,\n        intermediate_size_nomic=bert_config."}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same attributes as the BertConfig\n    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        num_labels=bert_config.num_labels,\n        output_attentions=bert_config.output_attentions,\n        output_hidden_states=bert_config.output_hidden_states,\n        torchscript=bert_config.torchscript,\n        pruned_heads=bert_config.pruned_heads,\n        is_encoder_decoder=bert_config.is_encoder_decoder,\n        resid_pdrop=bert_config.resid_pdrop,\n        embd_pdrop=bert_config.embd_pdrop,\n        attn_pdrop=bert_config.attn_pdrop,"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        num_labels=bert_config.num_labels,\n        output_attentions=bert_config.output_attentions,\n        output_hidden_states=bert_config.output_hidden_states,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        num_hidden_groups=bert_config.num_hidden_groups,\n        inner_group_num=bert_config.inner_group_num,\n        is_encoder_decoder=bert_config.is_encoder_decoder,\n        add_cross_attention=bert_config.add_cross_attention,\n        tie_word_embeddings=bert_config.tie_word_embeddings,\n        use_bias=bert_config.use_bias,\n        use_query_residual=bert_config.use_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        bos_token_id=bert_config.bos_token_id,\n        eos_token_id=bert_config.eos_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        num_labels=bert_config.num_labels,\n        output_attentions=bert_config.output_attentions,\n        output_hidden_states=bert_config.output_hidden_states,\n        problem_type=bert_config.problem_type,\n        use_return_dict=bert_config.use_return_dict,\n        num_embeddings=bert_config.num_embeddings,\n        num_embeddings_projected=bert_config.num_embeddings_projected,\n        tie_word_embeddings=bert_config.tie_word_embeddings,"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig()\n\n    config.vocab_size = bert_config.vocab_size\n    config.hidden_size = bert_config.hidden_size\n    config.num_hidden_layers = bert_config.num_hidden_layers\n    config.num_attention_heads = bert_config.num_attention_heads\n    config.intermediate_size = bert_config.intermediate_size\n    config.hidden_act = bert_config.hidden_act\n    config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    config.max_position_embeddings = bert_config.max_position_embeddings\n    config.type_vocab_size = bert_config.type_vocab_size\n    config.initializer_range = bert_config.initializer_range\n    config.layer_norm_eps = bert_config.layer_norm_eps\n    config.pad_token_id = bert_config.pad_token_id\n    config.position_embedding_type = bert_config.position_embedding_type\n    config.use_cache = bert_config.use_cache\n    config.classifier_dropout = bert_config.classifier_dropout\n\n    config.output_attentions = bert_config.output_attentions\n    config.output_hidden_states = bert_config.output_hidden_states\n    config.output_logits = bert_config.output_logits\n\n    config.num_labels = bert_config.num_labels\n    config.problem_type = bert_config.problem_type\n    config.id2label = bert_config.id2label\n    config.label2id = bert_config.label2id\n\n    config.torch_dtype = bert_config.torch_dtype\n    config.use_bfloat16 = bert_config.use_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig()\n    config.vocab_size = bert_config.vocab_size\n    config.hidden_size = bert_config.hidden_size\n    config.num_hidden_layers = bert_config.num_hidden_layers\n    config.num_attention_heads = bert_config.num_attention_heads\n    config.intermediate_size = bert_config.intermediate_size\n    config.hidden_act = bert_config.hidden_act\n    config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    config.max_position_embeddings = bert_config.max_position_embeddings\n    config.type_vocab_size = bert_config.type_vocab_size\n    config.initializer_range = bert_config.initializer_range\n    config.layer_norm_eps = bert_config.layer_norm_eps\n    config.pad_token_id = bert_config.pad_token_id\n    config.position_embedding_type = bert_config.position_embedding_type\n    config.use_cache = bert_config.use_cache\n    config.classifier_dropout = bert_config.classifier_dropout\n    config.num_labels = bert_config.num_labels\n    config.output_attentions = bert_config.output_attentions\n    config.output_hidden_states = bert_config.output_hidden_states\n    config.gradient_checkpointing = bert_config.gradient_checkpointing\n    config.use_memory_efficient_attention = bert_config.use_memory_efficient_attention\n    config.use_attention_scale_factor = bert_config.use_attention_scale_factor\n    config.attention_scale_factor = bert_config.attention_scale_factor\n    config.memory_efficient_attention_chunk_size = bert_config.memory"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        bos_token_id=bert_config.bos_token_id,\n        eos_token_id=bert_config.eos_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        num_labels=bert_config.num_labels,\n        use_mems_eval=bert_config.use_mems_eval,\n        use_mems_train=bert_config.use_mems_train,\n        mem_len=bert_config.mem_len,\n        output_attentions=bert_config.output_attentions,\n        output_hidden_states=bert_config.output_hidden_states,\n        output_past=bert_config.output_past,\n        use_rescale_embedding=bert_config.use_rescale_embedding,\n        use_relative_embeddings=bert_config.use_relative_embeddings,"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        pad_token_id=bert_config.pad_token_id,\n        bos_token_id=bert_config.bos_token_id,\n        eos_token_id=bert_config.eos_token_id,\n        sep_token_id=bert_config.sep_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        num_labels=bert_config.num_labels,\n        output_attentions=bert_config.output_attentions,\n        output_hidden_states=bert_config.output_hidden_states,\n        problem_type=bert_config.problem_type,\n        use_return_dict=bert_config.use_return_dict,\n        # New attributes specific to the Nomic model\n        max_prompt_length=bert_config.max_prom"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object\n    config = NomicBertConfig()\n\n    # Copy attributes from the BertConfig\n    config.vocab_size = bert_config.vocab_size\n    config.hidden_size = bert_config.hidden_size\n    config.num_hidden_layers = bert_config.num_hidden_layers\n    config.num_attention_heads = bert_config.num_attention_heads\n    config.intermediate_size = bert_config.intermediate_size\n    config.hidden_act = bert_config.hidden_act\n    config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    config.max_position_embeddings = bert_config.max_position_embeddings\n    config.type_vocab_size = bert_config.type_vocab_size\n    config.initializer_range = bert_config.initializer_range\n    config.layer_norm_eps = bert_config.layer_norm_eps\n    config.pad_token_id = bert_config.pad_token_id\n    config.position_embedding_type = bert_config.position_embedding_type\n    config.use_cache = bert_config.use_cache\n    config.classifier_dropout = bert_config.classifier_dropout\n    config.num_labels = bert_config.num_labels\n    config.output_attentions = bert_config.output_attentions\n    config.output_hidden_states = bert_config.output_hidden_states\n    config.use_return_dict = bert_config.use_return_dict\n\n    # Additional attributes specific to the Nomic model\n    config.use_pooler = True\n    config.pooler_fc_size = bert_config.hidden_size\n    config.pooler_num_attention_heads = bert_config.num_attention_heads"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object\n    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        num_labels=bert_config.num_labels,\n        output_attentions=bert_config.output_attentions,\n        output_hidden_states=bert_config.output_hidden_states,\n        # Additional Nomic-specific configurations\n        num_value_heads=bert_config.num_attention_heads,\n        num_value_layers=bert_config.num_hidden_layers,\n        tie_word_embeddings=False,\n        tie_position_embeddings=False,\n        tie_token_type_embeddings=False,\n        tie_value_embeddings=False,\n        tie_attention_biases=False,\n        tie_attention_biases_type=\"none\","}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Convert the BertConfig to a NomicBertConfig\n    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        num_labels=bert_config.num_labels,\n        output_attentions=bert_config.output_attentions,\n        output_hidden_states=bert_config.output_hidden_states,\n        use_return_dict=bert_config.use_return_dict,\n        # Additional arguments specific to the Nomic model\n        num_recurrent_layers=bert_config.num_hidden_layers,\n        num_recurrent_heads=bert_config.num_attention_heads,\n        num_recurrent_positions=bert_config.max_position_embeddings,\n        recurrent_dropout=bert_config.hidden_dropout_prob,\n        recurrent_activation="}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Inherit all attributes from the BertConfig\n    config = NomicBertConfig(**bert_config.__dict__)\n\n    # Add new attributes specific to the Nomic model\n    config.hidden_act = \"gelu\"\n    config.layer_norm_eps = 1e-5\n    config.initializer_range = 0.02\n    config.vocab_size = 30522\n    config.max_position_embeddings = 512\n    config.type_vocab_size = 2\n    config.pad_token_id = 0\n    config.position_embedding_type = \"absolute\"\n    config.use_cache = False\n    config.classifier_dropout = None\n    config.num_labels = 1\n\n    # Add a mapping for the new attribute 'hidden_act'\n    config.attribute_map[\"hidden_act\"] = \"hidden_act\"\n\n    # Add a mapping for the new attribute 'layer_norm_eps'\n    config.attribute_map[\"layer_norm_eps\"] = \"layer_norm_eps\"\n\n    # Add a mapping for the new attribute 'initializer_range'\n    config.attribute_map[\"initializer_range\"] = \"initializer_range\"\n\n    # Add a mapping for the new attribute 'vocab_size'\n    config.attribute_map[\"vocab_size\"] = \"vocab_size\"\n\n    # Add a mapping for the new attribute 'max_position_embeddings'\n    config.attribute_map[\"max_position_embeddings\"] = \"max_position_embeddings\"\n\n    # Add a mapping for the new attribute 'type_vocab_size'\n    config.attribute_map[\"type_vocab_size\"] = \"type_vocab_size\"\n\n    # Add a mapping for the new attribute 'pad_token_id'\n    config.attribute_map[\"pad_token_id\"] = \"pad_token_id\"\n\n    # Add a mapping for the new attribute 'position_embedding_type'\n    config.attribute_map[\"position_embedding_type\"] = \"position_embedding"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    nomic_config = NomicBertConfig()\n\n    # Inherit settings from BertConfig\n    for key, value in bert_config.__dict__.items():\n        setattr(nomic_config, key, value)\n\n    # Add additional settings specific to Nomic\n    nomic_config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    nomic_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    nomic_config.intermediate_size = bert_config.intermediate_size\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n    nomic_config.vocab_size = bert_config.vocab_size\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_config.hidden_size = bert_config.hidden_size\n    nomic_config.initializer_range = bert_config.initializer_range\n    nomic_config.layer_norm_eps = bert_config.layer_norm_eps\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n    nomic_config.use_cache = bert_config.use_cache\n    nomic_config.use_crf = bert_config.use_crf\n    nomic_config.use_lstm = bert_config.use_lstm\n    nomic_config.use_gru = bert_config.use_gru\n    nomic_config.use_bert_config = bert_config.use_bert_config\n    nomic_config.use_nomic_config = bert_config.use_nomic_config\n\n    return nomic_config\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.program_point.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n            glBindVertexArray(0)\n        else:\n            self.program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            if self.render_type == RenderType.LINES:\n                if self.faces is not None:\n                    glDrawElements(GL_LINES, len(self.faces) * 2, GL_UNSIGNED_INT, None)\n                else:\n                    glDrawArrays(GL_LINES, 0, len(self.vertices))\n            elif self.render_type == RenderType.TRIANGLES:\n                if self.faces is not None:\n                    glDrawElements(GL_TRIANGLES, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n                else:\n                    glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n            elif self.render_type == RenderType.QUADS:\n                if self.faces is not None:\n                    glDrawElements(GL_QUADS, len(self.faces) * 4, GL_UNSIGNED_INT, None)\n                else:\n                    glDrawArrays(GL_QUADS, 0, len(self.vertices))\n            elif self.render_type == RenderType.TRIANGLE_STRIP:\n                if self.faces is not None:\n                    glDrawElements(GL_TRIANGLE_STRIP, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n                else:\n                    glDrawArrays(GL_TRIANGLE_STRIP, 0, len(self.vertices))\n            glBind"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == 'points':\n            program = self.programs['points']\n            program.bind()\n            self.upload_gl_uniforms(program, camera)\n            self.vao.bind()\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n            self.vao.unbind()\n            program.unbind()\n        else:\n            program = self.programs['mesh']\n            program.bind()\n            self.upload_gl_uniforms(program, camera)\n            self.vao.bind()\n            if self.faces is not None:\n                if self.ebo is not None:\n                    self.ebo.bind()\n                    if self.render_type == 'lines':\n                        glDrawElements(GL_LINES, len(self.faces) * 2, GL_UNSIGNED_INT, None)\n                    elif self.render_type == 'triangles':\n                        glDrawElements(GL_TRIANGLES, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n                    elif self.render_type == 'quads':\n                        glDrawElements(GL_QUADS, len(self.faces) * 4, GL_UNSIGNED_INT, None)\n                    elif self.render_type == 'triangle_strip':\n                        glDrawElements(GL_TRIANGLE_STRIP, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n                    self.ebo.unbind()\n                else:\n                    if self.render_type == 'lines':\n                        glDrawArrays(GL_LINES, 0, len(self.vertices))\n                    elif self.render_type == 'triangles':\n                        glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n                    elif self.render_type == 'quads':\n                        glDrawArrays(GL_QUADS, 0, len(self.vertices))\n                    elif self.render_type =="}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            program = self.program_point\n        else:\n            program = self.program\n\n        program.bind()\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.vertices))\n        elif self.render_type == RenderType.LINES:\n            if self.faces is not None:\n                gl.glDrawElements(gl.GL_LINES, len(self.faces) * 2, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_LINES, 0, len(self.vertices))\n        elif self.render_type == RenderType.TRIANGLES:\n            if self.faces is not None:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * 3, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.vertices))\n        elif self.render_type == RenderType.QUADS:\n            if self.faces is not None:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * 4, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_QUADS, 0, len(self.vertices))\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            if self.faces is not None:\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * 3, gl.GL_UNSIGNED_INT, None)\n            else:"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.programs[\"point\"].use()\n            self.upload_gl_uniforms(self.programs[\"point\"], camera)\n            glBindVertexArray(self.vao)\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n            glBindVertexArray(0)\n        else:\n            self.programs[\"mesh\"].use()\n            self.upload_gl_uniforms(self.programs[\"mesh\"], camera)\n            glBindVertexArray(self.vao)\n            if self.render_type == RenderType.LINES:\n                if self.ebo is None:\n                    glDrawArrays(GL_LINES, 0, len(self.vertices))\n                else:\n                    glDrawElements(GL_LINES, len(self.faces) * 2, GL_UNSIGNED_INT, None)\n            elif self.render_type == RenderType.TRIANGLES:\n                if self.ebo is None:\n                    glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n                else:\n                    glDrawElements(GL_TRIANGLES, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n            elif self.render_type == RenderType.QUADS:\n                if self.ebo is None:\n                    glDrawArrays(GL_QUADS, 0, len(self.vertices))\n                else:\n                    glDrawElements(GL_QUADS, len(self.faces) * 4, GL_UNSIGNED_INT, None)\n            elif self.render_type == RenderType.TRIANGLE_STRIP:\n                if self.ebo is None:\n                    glDrawArrays(GL_TRIANGLE_STRIP, 0, len(self.vertices))\n                else:\n                    glDrawElements(GL_TRIANGLE_STRIP, len(self.faces) *"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            shader_program = self.gl_programs['points']\n            shader_program.bind()\n            self.upload_gl_uniforms(shader_program, camera)\n            self.vao.bind()\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n            self.vao.unbind()\n        else:\n            shader_program = self.gl_programs['general']\n            shader_program.bind()\n            self.upload_gl_uniforms(shader_program, camera)\n            self.vao.bind()\n\n            if self.render_type == RenderType.LINES:\n                if self.ebo is not None:\n                    glDrawElements(GL_LINES, len(self.faces) * 2, GL_UNSIGNED_INT, None)\n                else:\n                    glDrawArrays(GL_LINES, 0, len(self.vertices))\n            elif self.render_type == RenderType.TRIANGLES:\n                if self.ebo is not None:\n                    glDrawElements(GL_TRIANGLES, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n                else:\n                    glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n            elif self.render_type == RenderType.QUADS:\n                if self.ebo is not None:\n                    glDrawElements(GL_QUADS, len(self.faces) * 4, GL_UNSIGNED_INT, None)\n                else:\n                    glDrawArrays(GL_QUADS, 0, len(self.vertices))\n            elif self.render_type == RenderType.TRIANGLE_STRIP:\n                if self.ebo is not None:\n                    glDrawElements(GL_TRIANGLE_STRIP, len(self.faces) * 3, GL_UNSIGNED"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == \"points\":\n            self.program[\"projection\"] = camera.projection_matrix\n            self.program[\"view\"] = camera.view_matrix\n            self.program[\"model\"] = self.model_matrix\n            self.program[\"color\"] = self.color\n            self.program[\"point_size\"] = self.point_size\n            self.program[\"use_uniform_color\"] = self.use_uniform_color\n            self.program.draw(\"points\")\n        else:\n            if self.render_type == \"lines\":\n                self.program[\"projection\"] = camera.projection_matrix\n                self.program[\"view\"] = camera.view_matrix\n                self.program[\"model\"] = self.model_matrix\n                self.program[\"color\"] = self.color\n                self.program[\"use_uniform_color\"] = self.use_uniform_color\n                if self.ebo is None:\n                    self.program.draw(\"lines\")\n                else:\n                    self.program.draw(\"lines_indexed\")\n            elif self.render_type == \"triangles\":\n                self.program[\"projection\"] = camera.projection_matrix\n                self.program[\"view\"] = camera.view_matrix\n                self.program[\"model\"] = self.model_matrix\n                self.program[\"color\"] = self.color\n                self.program[\"use_uniform_color\"] = self.use_uniform_color\n                if self.ebo is None:\n                    self.program.draw(\"triangles\")\n                else:\n                    self.program.draw(\"triangles_indexed\")\n            elif self.render_type == \"quads\":\n                self.program[\"projection\"] = camera.projection_matrix\n                self.program[\"view\"] = camera.view_matrix\n                self.program[\"model\"] = self.model_matrix\n                self.program[\"color\"] = self.color\n                self.program[\"use_uniform_color\"] = self.use_uniform_color\n                if self.ebo is None:\n                    self.program.draw(\"quads\")\n                else:\n                   "}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.shader_programs['point'].use()\n        else:\n            self.shader_programs['mesh'].use()\n\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == RenderType.LINES:\n            if self.faces is not None:\n                gl.glDrawElements(gl.GL_LINES, self.faces.size, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_LINES, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.TRIANGLES:\n            if self.faces is not None:\n                gl.glDrawElements(gl.GL_TRIANGLES, self.faces.size, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.QUADS:\n            if self.faces is not None:\n                gl.glDrawElements(gl.GL_QUADS, self.faces.size, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_QUADS, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.TRIANGLE_STRIPS:\n            if self.faces is not None:\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, self.faces.size, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.PO"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.shader_program = self.shader_programs['points']\n        else:\n            self.shader_program = self.shader_programs['mesh']\n\n        self.shader_program.bind()\n        self.upload_gl_uniforms(camera)\n\n        glBindVertexArray(self.vao)\n\n        if self.render_type == RenderType.LINES:\n            if self.ebo is not None:\n                glDrawElements(GL_LINES, len(self.faces) * 2, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_LINES, 0, len(self.vertices))\n        elif self.render_type == RenderType.TRIANGLES:\n            if self.ebo is not None:\n                glDrawElements(GL_TRIANGLES, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n        elif self.render_type == RenderType.QUADS:\n            if self.ebo is not None:\n                glDrawElements(GL_QUADS, len(self.faces) * 4, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_QUADS, 0, len(self.vertices))\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            if self.ebo is not None:\n                glDrawElements(GL_TRIANGLE_STRIP, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLE_STRIP, 0, len(self.vertices))\n        elif self.render_type == RenderType.POINTS:\n            glDrawArrays"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == \"points\":\n            self.program_point.use()\n        else:\n            self.program.use()\n\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == \"lines\":\n            if self.ebo is None:\n                gl.glDrawArrays(gl.GL_LINES, 0, len(self.vertices))\n            else:\n                gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n                gl.glDrawElements(gl.GL_LINES, len(self.faces) * 2, gl.GL_UNSIGNED_INT, None)\n                gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, 0)\n        elif self.render_type == \"triangles\":\n            if self.ebo is None:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.vertices))\n            else:\n                gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * 3, gl.GL_UNSIGNED_INT, None)\n                gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, 0)\n        elif self.render_type == \"quads\":\n            if self.ebo is None:\n                gl.glDrawArrays(gl.GL_QUADS, 0, len(self.vertices))\n            else:\n                gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * 4, gl.GL_UNSIGNED_INT, None)\n                gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BU"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == \"POINTS\":\n            self.program[\"u_view_matrix\"].write(camera.view_matrix)\n            self.program[\"u_projection_matrix\"].write(camera.projection_matrix)\n            self.program[\"u_model_matrix\"].write(self.model_matrix)\n            self.program[\"u_point_size\"].value = self.point_size\n            self.program.run()\n            self.vao.render(moderngl.POINTS)\n            self.program.release()\n        else:\n            if self.render_type == \"LINES\":\n                self.program[\"u_view_matrix\"].write(camera.view_matrix)\n                self.program[\"u_projection_matrix\"].write(camera.projection_matrix)\n                self.program[\"u_model_matrix\"].write(self.model_matrix)\n                self.program[\"u_color\"].value = self.color\n                self.program.run()\n                if self.ebo is None:\n                    self.vao.render(moderngl.LINES)\n                else:\n                    self.vao.render(moderngl.LINES, self.ebo)\n                self.program.release()\n            elif self.render_type == \"TRIANGLES\":\n                self.program[\"u_view_matrix\"].write(camera.view_matrix)\n                self.program[\"u_projection_matrix\"].write(camera.projection_matrix)\n                self.program[\"u_model_matrix\"].write(self.model_matrix)\n                self.program[\"u_color\"].value = self.color\n                self.program.run()\n                if self.ebo is None:\n                    self.vao.render(moderngl.TRIANGLES)\n                else:\n                    self.vao.render(moderngl.TRIANGLES, self.ebo)\n                self.program.release()\n            elif self.render_type == \"QUADS\":\n                self.program[\"u_view_"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == 'point':\n            program = self.programs['point']\n        else:\n            program = self.programs['mesh']\n\n        program['u_view_matrix'].write(camera.view_matrix)\n        program['u_projection_matrix'].write(camera.projection_matrix)\n        program['u_model_matrix'].write(self.model_matrix)\n        program['u_normal_matrix'].write(self.normal_matrix)\n        program.run()\n\n        if self.render_type == 'point':\n            self.vao.render(mode=moderngl.POINTS)\n        elif self.render_type == 'line':\n            if self.indexed:\n                self.vao.render(mode=moderngl.LINES, vertices=len(self.faces) * 2)\n            else:\n                self.vao.render(mode=moderngl.LINES)\n        elif self.render_type == 'triangle':\n            if self.indexed:\n                self.vao.render(mode=moderngl.TRIANGLES, vertices=len(self.faces) * 3)\n            else:\n                self.vao.render(mode=moderngl.TRIANGLES)\n        elif self.render_type == 'quad':\n            if self.indexed:\n                self.vao.render(mode=moderngl.TRIANGLES, vertices=len(self.faces) * 4)\n            else:\n                self.vao.render(mode=moderngl.TRIANGLES)\n        elif self.render_type == 'triangle_strip':\n            if self.indexed:\n                self.vao.render(mode=moderngl.TRIANGLE_STRIP, vertices=len(self.faces) * 3)\n            else:\n                self.vao.render(mode=moderngl.TRIANGLE_STRIP)\n        self.vao.unbind()\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == 'point':\n            shader_program = self.programs['point']\n            shader_program.bind()\n            shader_program.upload_gl_uniforms(self.gl_uniforms, camera)\n            self.vao.bind()\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n            self.vao.unbind()\n            shader_program.unbind()\n        else:\n            shader_program = self.programs['mesh']\n            shader_program.bind()\n            shader_program.upload_gl_uniforms(self.gl_uniforms, camera)\n            self.vao.bind()\n            if self.faces is not None:\n                if self.faces.dtype == np.uint32:\n                    glDrawElements(GL_TRIANGLES, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n                elif self.faces.dtype == np.uint16:\n                    glDrawElements(GL_TRIANGLES, len(self.faces) * 3, GL_UNSIGNED_SHORT, None)\n                elif self.faces.dtype == np.uint8:\n                    glDrawElements(GL_TRIANGLES, len(self.faces) * 3, GL_UNSIGNED_BYTE, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n            self.vao.unbind()\n            shader_program.unbind()\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.point_shader_program.bind()\n            self.point_shader_program.upload_gl_uniforms(camera)\n        else:\n            self.shader_program.bind()\n            self.shader_program.upload_gl_uniforms(camera)\n\n        self.vao.bind()\n\n        if self.render_type == RenderType.LINES:\n            if self.ebo is None:\n                glDrawArrays(GL_LINES, 0, len(self.vertices))\n            else:\n                glDrawElements(GL_LINES, len(self.faces) * 2, GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.TRIANGLES:\n            if self.ebo is None:\n                glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n            else:\n                glDrawElements(GL_TRIANGLES, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.QUADS:\n            if self.ebo is None:\n                glDrawArrays(GL_QUADS, 0, len(self.vertices))\n            else:\n                glDrawElements(GL_QUADS, len(self.faces) * 4, GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.TRIANGLE_STRIPS:\n            if self.ebo is None:\n                glDrawArrays(GL_TRIANGLE_STRIP, 0, len(self.vertices))\n            else:\n                glDrawElements(GL_TRIANGLE_STRIP, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n\n        self.vao.unbind()\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == \"points\":\n            shader = self.programs[\"points\"]\n            shader.bind()\n            self.upload_gl_uniforms(shader, camera)\n            self.vao.bind()\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n            self.vao.unbind()\n            shader.unbind()\n        else:\n            shader = self.programs[\"mesh\"]\n            shader.bind()\n            self.upload_gl_uniforms(shader, camera)\n            self.vao.bind()\n            if self.faces is not None:\n                if self.faces.shape[1] == 2:\n                    glDrawArrays(GL_LINES, 0, len(self.vertices))\n                elif self.faces.shape[1] == 3:\n                    glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n                elif self.faces.shape[1] == 4:\n                    glDrawArrays(GL_QUADS, 0, len(self.vertices))\n                elif self.faces.shape[1] == 5:\n                    glDrawArrays(GL_TRIANGLE_STRIP, 0, len(self.vertices))\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n            self.vao.unbind()\n            shader.unbind()\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == \"points\":\n            self.shader_program[\"points\"].bind()\n            self.shader_program[\"points\"].upload_gl_uniforms(\n                self.get_gl_uniforms(camera)\n            )\n            self.vao.bind()\n            self.ebo.bind()\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n            self.ebo.unbind()\n            self.vao.unbind()\n\n        else:\n            self.shader_program[\"mesh\"].bind()\n            self.shader_program[\"mesh\"].upload_gl_uniforms(self.get_gl_uniforms(camera))\n            self.vao.bind()\n            self.ebo.bind()\n\n            if self.render_type == \"lines\":\n                glDrawArrays(GL_LINES, 0, len(self.vertices))\n            elif self.render_type == \"triangles\":\n                glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n            elif self.render_type == \"quads\":\n                glDrawArrays(GL_QUADS, 0, len(self.vertices))\n            elif self.render_type == \"triangle_strip\":\n                glDrawArrays(GL_TRIANGLE_STRIP, 0, len(self.vertices))\n            elif self.render_type == \"triangles_indexed\":\n                glDrawElements(\n                    GL_TRIANGLES, len(self.faces) * 3, GL_UNSIGNED_INT, None\n                )\n            elif self.render_type == \"lines_indexed\":\n                glDrawElements(GL_LINES, len(self.faces) * 2, GL_UNSIGNED_INT, None)\n\n            self.ebo.unbind()\n            self.vao.unbind()"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == 'point':\n            program = self.programs['point']\n            program.use()\n            program.upload_gl_uniforms(camera)\n\n            glBindVertexArray(self.vao)\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n            glBindVertexArray(0)\n        else:\n            program = self.programs['mesh']\n            program.use()\n            program.upload_gl_uniforms(camera)\n\n            glBindVertexArray(self.vao)\n            if self.faces is not None:\n                if self.faces.shape[1] == 1:\n                    glDrawArrays(GL_POINTS, 0, len(self.vertices))\n                elif self.faces.shape[1] == 2:\n                    glDrawArrays(GL_LINES, 0, len(self.vertices))\n                elif self.faces.shape[1] == 3:\n                    glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n                elif self.faces.shape[1] == 4:\n                    glDrawArrays(GL_QUADS, 0, len(self.vertices))\n                else:\n                    glDrawArrays(GL_TRIANGLE_STRIP, 0, len(self.vertices))\n            else:\n                glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n                if self.faces is not None:\n                    glDrawElements(GL_TRIANGLES, self.faces.shape[0] * self.faces.shape[1], GL_UNSIGNED_INT, None)\n                else:\n                    glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n                glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, 0)\n            glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == \"points\":\n            self.shader_program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n            glBindVertexArray(0)\n        else:\n            if self.render_type == \"lines\":\n                glLineWidth(2)\n            elif self.render_type == \"triangles\":\n                glPolygonMode(GL_FRONT_AND_BACK, GL_FILL)\n            elif self.render_type == \"quads\":\n                glPolygonMode(GL_FRONT_AND_BACK, GL_FILL)\n            elif self.render_type == \"triangle_strip\":\n                glPolygonMode(GL_FRONT_AND_BACK, GL_FILL)\n            else:\n                raise Exception(\"Unknown render type\")\n\n            self.shader_program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            if self.faces is not None:\n                glDrawElements(\n                    self.render_type_gl, len(self.faces) * self.face_type, GL_UNSIGNED_INT, None\n                )\n            else:\n                glDrawArrays(self.render_type_gl, 0, len(self.vertices))\n            glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == \"points\":\n            shader_program = self.gl_point_program\n        else:\n            shader_program = self.gl_program\n\n        shader_program.bind()\n\n        self.upload_gl_uniforms(camera)\n\n        self.gl_vao.bind()\n\n        if self.render_type == \"lines\":\n            self.gl_vao.draw_lines(0, self.n_vertices)\n        elif self.render_type == \"triangles\":\n            if self.faces is not None:\n                self.gl_vao.draw_triangles(self.faces)\n            else:\n                self.gl_vao.draw_triangles(0, self.n_vertices)\n        elif self.render_type == \"quads\":\n            if self.faces is not None:\n                self.gl_vao.draw_quads(self.faces)\n            else:\n                self.gl_vao.draw_quads(0, self.n_vertices)\n        elif self.render_type == \"triangle_strip\":\n            if self.faces is not None:\n                self.gl_vao.draw_triangle_strip(self.faces)\n            else:\n                self.gl_vao.draw_triangle_strip(0, self.n_vertices)\n\n        self.gl_vao.unbind()\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == \"points\":\n            shader = self.programs[\"point\"]\n            shader.bind()\n            shader.upload_gl_uniforms(self, camera)\n            self.vao.bind()\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n            self.vao.unbind()\n        else:\n            shader = self.programs[\"mesh\"]\n            shader.bind()\n            shader.upload_gl_uniforms(self, camera)\n            self.vao.bind()\n            if self.faces is not None:\n                if self.faces.ndim == 1:\n                    glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n                else:\n                    glDrawElements(GL_TRIANGLES, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n            self.vao.unbind()\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.programs['points'].use()\n            self.upload_gl_uniforms(self.programs['points'], camera)\n            self.vao.bind()\n            self.vao.draw_points()\n            self.vao.unbind()\n\n        else:\n            self.programs['general'].use()\n            self.upload_gl_uniforms(self.programs['general'], camera)\n            self.vao.bind()\n\n            if self.render_type == RenderType.LINES:\n                if self.ebo is None:\n                    self.vao.draw_lines()\n                else:\n                    self.vao.draw_lines_indexed()\n\n            elif self.render_type == RenderType.TRIANGLES:\n                if self.ebo is None:\n                    self.vao.draw_triangles()\n                else:\n                    self.vao.draw_triangles_indexed()\n\n            elif self.render_type == RenderType.QUADS:\n                if self.ebo is None:\n                    self.vao.draw_quads()\n                else:\n                    self.vao.draw_quads_indexed()\n\n            elif self.render_type == RenderType.TRIANGLE_STRIP:\n                if self.ebo is None:\n                    self.vao.draw_triangle_strip()\n                else:\n                    self.vao.draw_triangle_strip_indexed()\n\n            self.vao.unbind()\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_FLOAT, ptr)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        self.texture.upload(ptr, x, y, w, h)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_FLOAT, ptr)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_FLOAT, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        self.texture.upload(ptr, x, y, w, h)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_FLOAT, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        self.texture.upload(ptr, x, y, w, h)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_FLOAT, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        glBindTexture(GL_TEXTURE_2D, self.tex_id)\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_FLOAT, ptr)\n        glBindTexture(GL_TEXTURE_2D, 0)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        self.texture.upload(ptr, x, y, w, h)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        self.texture.upload(ptr, x, y, w, h)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().detach().numpy()\n\n        glBindTexture(GL_TEXTURE_2D, self.tex)\n        glTexSubImage2D(\n            GL_TEXTURE_2D,\n            0,\n            x,\n            y,\n            w,\n            h,\n            GL_RGBA,\n            GL_FLOAT,\n            ptr,\n        )\n        glBindTexture(GL_TEXTURE_2D, 0)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        glBindTexture(GL_TEXTURE_2D, self.tex)\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_FLOAT, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_FLOAT, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_FLOAT, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        glBindTexture(GL_TEXTURE_2D, self.tex_id)\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_UNSIGNED_BYTE, ptr)\n        glBindTexture(GL_TEXTURE_2D, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_FLOAT, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        glTexSubImage2D(\n            GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_UNSIGNED_BYTE, ptr\n        )\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_FLOAT, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        glBindTexture(GL_TEXTURE_2D, self.texture)\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_FLOAT, ptr)\n\n        glBindTexture(GL_TEXTURE_2D, 0)\n\n        return\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    assert len(R.shape) == 3\n    assert len(tvec.shape) == 3\n    assert len(camera_matrix.shape) == 3\n    assert len(image_size.shape) == 2\n\n    assert R.shape[0] == tvec.shape[0]\n    assert R.shape[0] == camera_matrix.shape[0]\n    assert R.shape[0] == image_size.shape[0]\n\n    assert R.shape[1] == 3\n    assert R.shape[2] == 3\n    assert tvec.shape[1] == 3\n    assert tvec.shape[2] == 1\n    assert camera_matrix.shape[1] == 3\n    assert camera_matrix.shape[2] == 3\n    assert image_size.shape[1] == 2\n\n    assert R.device == tvec.device\n    assert R.device == camera_matrix.device\n    assert R.device == image_size.device\n\n    assert R.dtype == tvec.dtype\n    assert R.dtype == camera_matrix.dtype\n    assert R.dtype == image_size.dtype\n\n    assert torch.all(R[:, 0, 0] > 0)\n    assert torch.all(R[:, 1, 1] > 0)\n    assert torch.all(R[:, 2, 2] > 0)\n\n    assert torch.all(tvec[:, 0, 0] > 0)\n    assert torch.all(tvec[:, 1, 0] > 0)\n    assert torch.all(tvec[:, 2, 0] > 0)\n\n    assert torch.all(camera_matrix[:, 0, 0] > 0)\n    assert torch.all(camera_matrix[:, 1, 1] > 0)\n    assert torch.all(camera_matrix[:, 0, 2] > 0)\n    assert torch.all(camera_matrix[:, 1, 2] > 0)\n\n    assert torch.all(image_size[:"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    assert R.shape[0] == tvec.shape[0]\n    assert R.shape[0] == camera_matrix.shape[0]\n    assert R.shape[0] == image_size.shape[0]\n    assert R.shape[1] == 3\n    assert R.shape[2] == 3\n    assert tvec.shape[1] == 3\n    assert camera_matrix.shape[1] == 3\n    assert camera_matrix.shape[2] == 3\n    assert image_size.shape[1] == 2\n\n    assert R.dtype == tvec.dtype\n    assert R.dtype == camera_matrix.dtype\n    assert R.dtype == image_size.dtype\n\n    assert torch.all(R[:, :, 0] == R[:, :, 1])\n    assert torch.all(R[:, :, 1] == R[:, :, 2])\n    assert torch.all(R[:, 0, :] == R[:, 1, :])\n    assert torch.all(R[:, 1, :] == R[:, 2, :])\n    assert torch.all(R[:, 2, :] == R[:, 0, :])\n\n    assert torch.all(R[:, 0, 0] == 1)\n    assert torch.all(R[:, 1, 1] == 1)\n    assert torch.all(R[:, 2, 2] == 1)\n\n    assert torch.all(R[:, 0, 1] == 0)\n    assert torch.all(R[:, 0, 2] == 0)\n    assert torch.all(R[:, 1, 0] == 0)\n    assert torch.all(R[:, 1, 2] == 0)\n    assert torch.all(R[:, 2, 0] == 0)\n    assert torch.all(R[:, 2, 1] == 0)\n\n    assert torch.all(camera_matrix[:, 0, 0] == camera"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure that all inputs are batched and have the correct shape\n    assert R.ndim == 3 and R.shape[1] == 3 and R.shape[2] == 3\n    assert tvec.ndim == 2 and tvec.shape[1] == 3\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[1] == 3 and camera_matrix.shape[2] == 3\n    assert image_size.ndim == 2 and image_size.shape[1] == 2\n\n    # Validate the values of the inputs\n    assert torch.all(R[:, 0, 1] == 0)\n    assert torch.all(R[:, 0, 2] == 0)\n    assert torch.all(R[:, 1, 2] == 0)\n    assert torch.all(R[:, 2, 0] == 0)\n    assert torch.all(R[:, 2, 1] == 0)\n    assert torch.all(R[:, 2, 2] == 1)\n    assert torch.all(tvec != 0)\n    assert torch.all(camera_matrix[:, 0, 1] == 0)\n    assert torch.all(camera_matrix[:, 0, 2] == 0)\n    assert torch.all(camera_matrix[:, 1, 2] == 0)\n    assert torch.all(camera_matrix[:, 2, 0] == 0)\n    assert torch.all(camera_matrix[:, 2, 1] == 0)\n    assert torch.all(camera_matrix[:, 2, 2] == 1)\n    assert torch.all(image_size > 0)\n    assert znear > 0\n\n    # Compute the camera position\n    cam_pos = -torch.matmul(R.transpose(1, 2), tvec.unsqueeze(-1)).squeeze(-1)\n\n    # Compute the camera rotation\n    cam_rot = R.transpose(1, 2)\n\n   "}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate input shapes\n    assert R.shape[-2:] == (3, 3)\n    assert tvec.shape[-1] == 3\n    assert camera_matrix.shape[-2:] == (3, 3)\n    assert image_size.shape[-1] == 2\n\n    # Validate input values\n    assert torch.allclose(R @ R.transpose(-1, -2), torch.eye(3), atol=1e-6)\n    assert torch.all(camera_matrix[..., 0, 1] == 0)\n    assert torch.all(camera_matrix[..., 1, 0] == 0)\n    assert torch.all(camera_matrix[..., 2, :2] == 0)\n    assert torch.all(camera_matrix[..., 2, 2] == 1)\n    assert torch.all(image_size > 0)\n\n    # Calculate camera position\n    camera_position = -R.transpose(-1, -2) @ tvec\n\n    # Calculate camera rotation\n    camera_rotation = R.transpose(-1, -2)\n\n    # Calculate focal lengths\n    fx = camera_matrix[..., 0, 0] / image_size[..., 0]\n    fy = camera_matrix[..., 1, 1] / image_size[..., 1]\n\n    # Calculate principal point offsets\n    cx = camera_matrix[..., 0, 2] / image_size[..., 0]\n    cy = camera_matrix[..., 1, 2] / image_size[..., 1]\n\n    # Calculate sensor width\n    sensor_width = camera_matrix[..., 0, 0] / fx\n\n    # Adjust focal lengths based on znear\n    fx = fx * znear\n    fy = fy * znear\n\n    # Adjust principal point offsets based on znear\n    cx = cx * znear\n    cy = cy * znear\n\n    # Normalize focal"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Check and validate input shapes\n    assert R.shape[0] == tvec.shape[0] == camera_matrix.shape[0] == image_size.shape[0]\n    assert R.shape[1] == 3 and R.shape[2] == 3\n    assert tvec.shape[1] == 3\n    assert camera_matrix.shape[1] == 3 and camera_matrix.shape[2] == 3\n    assert image_size.shape[1] == 2\n\n    # Ensure that all inputs are batched\n    R = R.view(-1, 3, 3)\n    tvec = tvec.view(-1, 3)\n    camera_matrix = camera_matrix.view(-1, 3, 3)\n    image_size = image_size.view(-1, 2)\n\n    # Compute camera position\n    camera_position = -torch.matmul(R.transpose(1, 2), tvec.unsqueeze(-1)).squeeze(-1)\n\n    # Compute camera rotation\n    camera_rotation = R.transpose(1, 2)\n\n    # Compute focal lengths and principal points\n    fx = camera_matrix[:, 0, 0]\n    fy = camera_matrix[:, 1, 1]\n    cx = camera_matrix[:, 0, 2]\n    cy = camera_matrix[:, 1, 2]\n\n    # Adjust principal point offsets and normalize focal length\n    cx = cx - image_size[:, 0] / 2\n    cy = cy - image_size[:, 1] / 2\n    fx = fx / image_size[:, 0]\n    fy = fy / image_size[:, 1]\n\n    # Compute sensor width\n    sensor_width = 2 * znear * torch.sqrt(fx**2 + fy**2) / fx\n\n    # Combine camera parameters into a single tensor\n    camera_params = torch.cat(\n        [camera_position, camera_rotation.reshape(-1,"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    assert R.shape[0] == tvec.shape[0]\n    assert R.shape[0] == camera_matrix.shape[0]\n    assert R.shape[0] == image_size.shape[0]\n\n    assert R.shape[1] == 3\n    assert R.shape[2] == 3\n    assert tvec.shape[1] == 3\n    assert camera_matrix.shape[1] == 3\n    assert camera_matrix.shape[2] == 3\n    assert image_size.shape[1] == 2\n\n    # Compute the camera position from the translation vector\n    camera_position = -R.transpose(-1, -2) @ tvec[..., None]\n    camera_position = camera_position[..., 0]\n\n    # Compute the camera rotation from the rotation matrix\n    camera_rotation = R.transpose(-1, -2)\n\n    # Compute the focal length from the camera intrinsic matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n    focal_length = (fx + fy) / 2.0\n\n    # Compute the principal point offset from the image size\n    principal_point_offset = torch.stack(\n        [\n            (cx - image_size[..., 0] / 2.0) / focal_length,\n            (cy - image_size[..., 1] / 2.0) / focal_length,\n        ],\n        dim=-1,\n    )\n\n    # Compute the sensor width from the focal length and image size\n    sensor_width = focal_length * max(image_size[..., 0], image_size[..., 1])\n\n    # Adjust the focal length based on the near clipping plane distance\n    focal_length = focal_length / (1.0 - znear / camera_position[..., 2])\n\n    # Normalize the"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    assert R.ndim == 3 and R.shape[1] == 3 and R.shape[2] == 3\n    assert tvec.ndim == 2 and tvec.shape[1] == 3\n    assert camera_matrix.ndim == 2 and camera_matrix.shape[1] == 3\n    assert image_size.ndim == 2 and image_size.shape[1] == 2\n\n    # Extract focal lengths and principal points from the camera matrix\n    fx = camera_matrix[:, 0, 0]\n    fy = camera_matrix[:, 1, 1]\n    cx = camera_matrix[:, 0, 2]\n    cy = camera_matrix[:, 1, 2]\n\n    # Adjust principal points based on image size\n    cx = cx - image_size[:, 0] / 2\n    cy = cy - image_size[:, 1] / 2\n\n    # Normalize focal lengths based on image size\n    fx = fx / image_size[:, 0]\n    fy = fy / image_size[:, 1]\n\n    # Calculate camera position from translation vector\n    camera_position = -torch.bmm(R, tvec.unsqueeze(-1)).squeeze(-1)\n\n    # Calculate camera rotation from rotation matrix\n    camera_rotation = torch.bmm(R, torch.tensor([[0, 0, 1]], dtype=R.dtype, device=R.device).repeat(R.shape[0], 1, 1))\n\n    # Calculate sensor width based on focal lengths and near clipping plane distance\n    sensor_width = 2 * znear / fx\n\n    # Stack camera parameters into a single tensor\n    camera_params = torch.stack([camera_position, camera_rotation, fx, fy, cx, cy, sensor_width], dim=-1)\n\n    return camera_params\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    assert R.ndim == 3 and R.shape[1] == 3 and R.shape[2] == 3\n    assert tvec.ndim == 2 and tvec.shape[1] == 3\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[1] == 3 and camera_matrix.shape[2] == 3\n    assert image_size.ndim == 2 and image_size.shape[1] == 2\n\n    focal_length = camera_matrix[..., 0, 0]\n    sensor_width = camera_matrix[..., 1, 1]\n    principal_point = camera_matrix[..., 0, 2]\n    image_width = image_size[..., 0]\n    image_height = image_size[..., 1]\n\n    principal_point_offset = torch.stack([\n        (principal_point[..., 0] - image_width / 2) / focal_length,\n        (principal_point[..., 1] - image_height / 2) / focal_length\n    ], dim=-1)\n\n    camera_position = torch.matmul(R.transpose(-1, -2), -tvec[..., None])[..., 0]\n    camera_rotation = R.transpose(-1, -2)\n\n    return torch.cat([\n        camera_position,\n        camera_rotation.reshape(*camera_rotation.shape[:-2], 9),\n        focal_length[..., None],\n        sensor_width[..., None],\n        principal_point_offset\n    ], dim=-1)\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    assert R.ndim == 3 and tvec.ndim == 2 and camera_matrix.ndim == 3 and image_size.ndim == 2\n    assert R.shape[0] == tvec.shape[0] == camera_matrix.shape[0] == image_size.shape[0]\n    assert R.shape[1] == R.shape[2] == 3\n    assert tvec.shape[1] == 3\n    assert camera_matrix.shape[1] == camera_matrix.shape[2] == 3\n    assert image_size.shape[1] == 2\n\n    # Compute camera position\n    camera_position = -R.transpose(1, 2) @ tvec[:, :, None]\n    camera_position = camera_position.squeeze(2)\n\n    # Compute camera rotation\n    camera_rotation = R.transpose(1, 2)\n\n    # Compute camera intrinsic parameters\n    fx = camera_matrix[:, 0, 0]\n    fy = camera_matrix[:, 1, 1]\n    px = camera_matrix[:, 0, 2]\n    py = camera_matrix[:, 1, 2]\n\n    # Adjust principal point offset and normalize focal length\n    px = px - image_size[:, 0] / 2\n    py = py - image_size[:, 1] / 2\n    fx = fx / image_size[:, 0]\n    fy = fy / image_size[:, 1]\n\n    # Compute sensor width\n    sensor_width = 2 * (fx * znear) / fy\n\n    # Stack camera parameters into a single tensor\n    camera_params = torch.stack([camera_position, camera_rotation, fx, fy, px, py, sensor_width], dim=1)\n\n    return camera_params\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate input shapes\n    assert R.shape[1:] == (3, 3)\n    assert tvec.shape[1:] == (3, 1)\n    assert camera_matrix.shape[1:] == (3, 3)\n    assert image_size.shape[1:] == (2,)\n\n    # Validate input values\n    assert torch.allclose(R @ R.transpose(-1, -2), torch.eye(3).to(R))\n    assert torch.all(camera_matrix[..., 1, 1] != 0)\n    assert torch.all(image_size > 0)\n\n    # Compute camera position\n    camera_position = -R.transpose(-1, -2) @ tvec\n\n    # Compute camera rotation\n    camera_rotation = rotation_matrix_to_quaternion(R)\n\n    # Compute camera intrinsic parameters\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n    sensor_width = camera_matrix[..., 0, 2] * 2\n\n    # Adjust principal point offset and normalize focal length\n    principal_point_offset = torch.stack([cx - image_size[..., 0] / 2, cy - image_size[..., 1] / 2], dim=-1)\n    focal_length = torch.stack([fx / image_size[..., 0], fy / image_size[..., 1]], dim=-1)\n\n    # Compute camera parameters\n    camera_params = torch.cat([camera_position, camera_rotation, focal_length, principal_point_offset, sensor_width.unsqueeze(-1)], dim=-1)\n\n    return camera_params\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Check that all inputs are batched and have the correct shape\n    assert R.ndim == 3 and R.shape[1:] == (3, 3)\n    assert tvec.ndim == 2 and tvec.shape[1] == 3\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[1:] == (3, 3)\n    assert image_size.ndim == 2 and image_size.shape[1] == 2\n\n    # Check that the batch sizes of all inputs match\n    assert R.shape[0] == tvec.shape[0] == camera_matrix.shape[0] == image_size.shape[0]\n\n    # Check that all inputs have valid values\n    assert torch.all(torch.isfinite(R))\n    assert torch.all(torch.isfinite(tvec))\n    assert torch.all(torch.isfinite(camera_matrix))\n    assert torch.all(torch.isfinite(image_size))\n\n    # Compute the camera position by taking the inverse of the rotation matrix and multiplying it by the translation vector\n    camera_position = -R.transpose(-2, -1) @ tvec[:, :, None]\n\n    # Compute the camera rotation by converting the rotation matrix to a quaternion\n    camera_rotation = rotation_matrix_to_quaternion(R)\n\n    # Compute the focal length and principal point offsets from the camera intrinsic matrix and image size\n    fx = camera_matrix[:, 0, 0] / image_size[:, 0]\n    fy = camera_matrix[:, 1, 1] / image_size[:, 1]\n    cx = camera_matrix[:, 0, 2] / image_size[:, 0] - 0.5\n    cy = camera_matrix[:, 1, 2] / image_size[:, 1] - 0.5\n\n    # Compute the sensor width by computing the width of the image plane at a given distance\n    sensor_width = 2 * znear * torch.sqrt(fx**"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    assert R.ndim == 3 and R.shape[1] == 3 and R.shape[2] == 3\n    assert tvec.ndim == 2 and tvec.shape[1] == 3\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[1] == 3 and camera_matrix.shape[2] == 3\n    assert image_size.ndim == 2 and image_size.shape[1] == 2\n\n    assert R.shape[0] == tvec.shape[0] == camera_matrix.shape[0] == image_size.shape[0]\n\n    # Get the focal length and sensor width from the camera intrinsic matrix\n    fx = camera_matrix[:, 0, 0]\n    fy = camera_matrix[:, 1, 1]\n    sensor_width = camera_matrix[:, 0, 2]\n\n    # Get the principal point offsets from the image size\n    px = image_size[:, 0] / 2\n    py = image_size[:, 1] / 2\n\n    # Normalize the focal length based on the image size\n    fx = fx / px\n    fy = fy / py\n\n    # Adjust the principal point offsets to account for the image size\n    px = px / image_size[:, 0]\n    py = py / image_size[:, 1]\n\n    # Compute the camera position by inverting the rotation matrix and translation vector\n    camera_pos = -torch.bmm(R.transpose(1, 2), tvec.unsqueeze(2)).squeeze(2)\n\n    # Compute the camera rotation using the rotation matrix\n    camera_rot = R.transpose(1, 2)\n\n    # Compute the camera intrinsic parameters\n    camera_params = torch.cat([\n        fx.unsqueeze(1), fy.unsqueeze(1), px.unsqueeze(1), py.unsqueeze(1),\n        sensor_width.unsqueeze(1), camera_pos, camera_rot\n    ],"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Check if all inputs are batched\n    assert R.ndim == 3 and tvec.ndim == 2 and camera_matrix.ndim == 3 and image_size.ndim == 2\n\n    # Check if the batch sizes are consistent\n    assert R.shape[0] == tvec.shape[0] == camera_matrix.shape[0] == image_size.shape[0]\n\n    # Check if the rotation matrix is a valid rotation matrix\n    assert torch.allclose(torch.matmul(R, R.transpose(-1, -2)), torch.eye(3), atol=1e-6)\n\n    # Check if the translation vector is a valid vector\n    assert tvec.shape[-1] == 3\n\n    # Check if the camera intrinsic matrix is a valid matrix\n    assert camera_matrix.shape[-2:] == (3, 3)\n\n    # Check if the image size is a valid size\n    assert image_size.shape[-1] == 2\n\n    # Check if the near clipping plane distance is a valid distance\n    assert znear > 0.0\n\n    # Compute the camera position\n    camera_position = -torch.matmul(R.transpose(-1, -2), tvec.unsqueeze(-1)).squeeze(-1)\n\n    # Compute the camera rotation\n    camera_rotation = R.transpose(-1, -2)\n\n    # Compute the focal length\n    f = camera_matrix[..., 0, 0] * image_size[..., 0] / camera_matrix[..., 0, 2]\n\n    # Compute the principal point offset\n    offset = -camera_matrix[..., 0, 2] * znear / f\n\n    # Compute the sensor width\n    sensor_width = camera_matrix[..., 0, 1] * image_size[..., 1] / camera_matrix[..., 0, 2]\n\n    # Normalize the focal length\n    f /= image_size[..., 0]\n\n    # Adjust the principal point offset\n    offset"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    assert R.ndim == 3 and tvec.ndim == 2 and camera_matrix.ndim == 3 and image_size.ndim == 2\n    assert R.shape[0] == tvec.shape[0] == camera_matrix.shape[0] == image_size.shape[0]\n    assert R.shape[1] == R.shape[2] == 3 and tvec.shape[1] == 3 and camera_matrix.shape[1] == camera_matrix.shape[2] == 3\n    assert R.shape[0] > 0 and tvec.shape[0] > 0 and camera_matrix.shape[0] > 0 and image_size.shape[0] > 0\n    assert R.shape[1] == R.shape[2] == tvec.shape[1] == camera_matrix.shape[1] == camera_matrix.shape[2]\n\n    # Get the focal length and sensor width from the camera intrinsic matrix\n    fx = camera_matrix[:, 0, 0]\n    fy = camera_matrix[:, 1, 1]\n    cx = camera_matrix[:, 0, 2]\n    cy = camera_matrix[:, 1, 2]\n    sensor_width = camera_matrix[:, 0, 2] * 2\n\n    # Calculate the principal point offsets and normalize the focal length\n    principal_point_offset_x = (cx - image_size[:, 0] / 2) / image_size[:, 0]\n    principal_point_offset_y = (cy - image_size[:, 1] / 2) / image_size[:, 1]\n    fx = fx / image_size[:, 0]\n    fy = fy / image_size[:, 1]\n\n    # Calculate the camera position and rotation\n    camera_position = -torch.matmul(R.transpose(1, 2), tvec[:, :, None])[:, :, 0]\n    camera_rotation = R.transpose(1, 2)\n\n    # Calculate the focal length and adjust for"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    assert R.ndim == 3 and R.shape[1] == 3 and R.shape[2] == 3\n    assert tvec.ndim == 2 and tvec.shape[1] == 3\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[1] == 3 and camera_matrix.shape[2] == 3\n    assert image_size.ndim == 2 and image_size.shape[1] == 2\n    assert znear > 0\n\n    batch_size = R.shape[0]\n\n    # Compute the camera position\n    camera_position = -torch.bmm(R.transpose(1, 2), tvec.unsqueeze(2)).squeeze(2)\n\n    # Compute the camera rotation\n    camera_rotation = rotation_matrix_to_angle_axis(R)\n\n    # Compute the camera focal length\n    focal_length = camera_matrix[:, 0, 0] / image_size[:, 0]\n    sensor_width = camera_matrix[:, 0, 2] / image_size[:, 0]\n\n    # Compute the camera principal point offset\n    principal_point_offset = camera_matrix[:, 0, 2] / image_size[:, 0]\n\n    # Adjust the focal length based on the near clipping plane distance\n    focal_length = focal_length / (1 - znear / (camera_position[:, 2] + znear))\n\n    # Normalize the focal length and principal point offset\n    focal_length = focal_length / focal_length.max()\n    principal_point_offset = principal_point_offset / sensor_width\n\n    # Create the camera parameters tensor\n    camera_params = torch.zeros(batch_size, 11, device=R.device)\n    camera_params[:, 0:3] = camera_position\n    camera_params[:, 3:6] = camera_rotation\n    camera_params[:, 6] = focal_length\n    camera_params[:, 7] = principal_point_offset\n    camera_params[:, 8"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Check if R, tvec, camera_matrix, and image_size are batched tensors\n    assert R.ndim == 3\n    assert tvec.ndim == 2\n    assert camera_matrix.ndim == 3\n    assert image_size.ndim == 2\n\n    # Check if the batch sizes of R, tvec, camera_matrix, and image_size match\n    assert R.shape[0] == tvec.shape[0]\n    assert R.shape[0] == camera_matrix.shape[0]\n    assert R.shape[0] == image_size.shape[0]\n\n    # Check if R, tvec, and camera_matrix are valid tensors\n    assert R.shape[1] == 3 and R.shape[2] == 3\n    assert tvec.shape[1] == 3\n    assert camera_matrix.shape[1] == 3 and camera_matrix.shape[2] == 3\n\n    # Check if image_size is a valid tensor\n    assert image_size.shape[1] == 2\n\n    # Extract focal length and principal point from camera_matrix\n    fx = camera_matrix[:, 0, 0]\n    fy = camera_matrix[:, 1, 1]\n    cx = camera_matrix[:, 0, 2]\n    cy = camera_matrix[:, 1, 2]\n\n    # Compute camera position\n    camera_position = -torch.matmul(R, tvec.unsqueeze(-1)).squeeze(-1)\n\n    # Compute camera rotation\n    camera_rotation = torch.matmul(R, torch.tensor([[0, 0, 1]]).float().to(R.device).unsqueeze(-1)).squeeze(-1)\n\n    # Normalize focal length and principal point\n    sensor_width = camera_matrix[:, 0, 1] * image_size[:, 0] / fx\n    focal_length = fx / image_size[:, 0]\n    cx = cx / image_size[:, 0]\n    cy = cy /"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    batch_size = R.shape[0]\n    assert R.shape == (batch_size, 3, 3)\n    assert tvec.shape == (batch_size, 3, 1)\n    assert camera_matrix.shape == (batch_size, 3, 3)\n    assert image_size.shape == (batch_size, 2)\n\n    focal_length = camera_matrix[:, 0, 0].unsqueeze(1)\n    sensor_width = camera_matrix[:, 1, 2].unsqueeze(1)\n    focal_length = focal_length / image_size[:, 0].unsqueeze(1)\n    sensor_width = sensor_width / image_size[:, 0].unsqueeze(1)\n    focal_length = focal_length / znear\n\n    # compute the principal point offsets\n    principal_point_offset = torch.tensor([0.5, 0.5], dtype=torch.float32).to(\n        focal_length.device\n    )\n    principal_point_offset = principal_point_offset.unsqueeze(0).repeat(batch_size, 1)\n    principal_point_offset = principal_point_offset / image_size\n\n    # compute the camera position\n    camera_position = -torch.inverse(R) @ tvec\n\n    # compute the camera rotation\n    camera_rotation = R.transpose(1, 2)\n\n    # stack the camera parameters into a single tensor\n    camera_params = torch.cat(\n        [camera_position, camera_rotation, focal_length, sensor_width, principal_point_offset],\n        dim=1,\n    )\n\n    return camera_params\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Check if the inputs are batched\n    if len(R.shape) == 3:\n        batch_size = R.shape[0]\n        assert R.shape == (batch_size, 3, 3)\n        assert tvec.shape == (batch_size, 3)\n        assert camera_matrix.shape == (batch_size, 3, 3)\n        assert image_size.shape == (batch_size, 2)\n    else:\n        assert R.shape == (3, 3)\n        assert tvec.shape == (3,)\n        assert camera_matrix.shape == (3, 3)\n        assert image_size.shape == (2,)\n\n    # Extract focal length and sensor width from the camera intrinsic matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    sensor_width = camera_matrix[..., 0, 2] * 2\n\n    # Calculate the principal point offsets based on the image size and sensor width\n    principal_point_offset_x = image_size[..., 0] / sensor_width\n    principal_point_offset_y = image_size[..., 1] / sensor_width\n\n    # Calculate the focal length based on the z-near distance and sensor width\n    focal_length = fx * znear / sensor_width\n\n    # Adjust the focal length to account for the principal point offsets\n    focal_length_x = focal_length * principal_point_offset_x\n    focal_length_y = focal_length * principal_point_offset_y\n\n    # Calculate the camera rotation using the rotation matrix\n    camera_rotation = R[..., 0:3, 0:3]\n\n    # Calculate the camera position using the translation vector\n    camera_position = tvec\n\n    # Stack the camera parameters into a single tensor\n    camera_params = torch.stack(\n        [\n            camera_position[..., 0],\n            camera_position[..., 1],\n            camera_position[..., 2],"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Check if all inputs are batched\n    assert (\n        R.ndim == 3 and tvec.ndim == 2 and camera_matrix.ndim == 3 and image_size.ndim == 2\n    ), \"All inputs must be batched\"\n\n    # Check if the batch sizes match\n    assert (\n        R.shape[0]\n        == tvec.shape[0]\n        == camera_matrix.shape[0]\n        == image_size.shape[0]\n    ), \"Batch sizes of inputs do not match\"\n\n    # Check if the rotation matrix is a valid rotation matrix\n    assert torch.allclose(torch.det(R), torch.ones_like(R[:, 0, 0])), \"R is not a valid rotation matrix\"\n\n    # Check if the image size is valid\n    assert torch.all(image_size > 0), \"image_size must be positive\"\n\n    # Extract the camera intrinsic parameters from the camera matrix\n    fx, fy, cx, cy = (\n        camera_matrix[:, 0, 0],\n        camera_matrix[:, 1, 1],\n        camera_matrix[:, 0, 2],\n        camera_matrix[:, 1, 2],\n    )\n\n    # Adjust the principal point offsets and normalize the focal length\n    cx = cx - image_size[:, 0] / 2\n    cy = cy - image_size[:, 1] / 2\n    fx = fx / image_size[:, 0]\n    fy = fy / image_size[:, 1]\n\n    # Compute the camera position and rotation\n    cam_pos = -torch.matmul(R.transpose(1, 2), tvec[:, :, None])[:, :, 0]\n    cam_rot = R.transpose(1, 2)\n\n    # Compute the focal length based on the near clipping plane distance\n    focal = 1 / (znear / fx)\n\n    # Compute the sensor width based on the focal length and image size\n    sensor_"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    batch_size = R.shape[0]\n\n    assert R.shape == (batch_size, 3, 3)\n    assert tvec.shape == (batch_size, 3)\n    assert camera_matrix.shape == (batch_size, 3, 3)\n    assert image_size.shape == (batch_size, 2)\n\n    # Extract focal length and sensor width from camera matrix\n    fx = camera_matrix[:, 0, 0]\n    fy = camera_matrix[:, 1, 1]\n    cx = camera_matrix[:, 0, 2]\n    cy = camera_matrix[:, 1, 2]\n    sensor_width = camera_matrix[:, 0, 1]\n\n    # Adjust principal point offsets and normalize focal length\n    cx = cx - sensor_width / 2\n    cy = cy - sensor_width / 2\n    fx = fx / max(image_size[:, 0])\n    fy = fy / max(image_size[:, 1])\n\n    # Calculate camera position and rotation\n    camera_pos = -R.transpose(-1, -2) @ tvec[..., None]\n    camera_rot = R.transpose(-1, -2)\n\n    # Calculate focal length based on znear\n    znear = znear * torch.ones_like(fx)\n    focal_length = fx * znear / (0.5 * sensor_width * znear / fx)\n\n    # Stack camera parameters into a single tensor\n    camera_params = torch.cat(\n        [\n            camera_pos[..., 0],\n            camera_rot[..., 0],\n            camera_rot[..., 1],\n            camera_rot[..., 2],\n            focal_length[..., None],\n            cx[..., None],\n            cy[..., None],\n        ],\n        dim=-1,\n    )\n\n    return camera_params"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y)\n            return\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n        glUseProgram(self.quad_program)\n        glBindTexture(GL_TEXTURE_2D, self.tex)\n        glBindVertexArray(self.vao)\n        glDrawArrays(GL_TRIANGLES, 0, 6)\n        glBindVertexArray(0)\n        glUseProgram(0)\n        glBindTexture(GL_TEXTURE_2D, 0)\n\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        self.quad_program.use()\n        self.tex.bind()\n\n        self.vao.bind()\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n        self.vao.unbind()\n\n        self.quad_program.unuse()\n\n        self.tex.unbind()\n\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        self.quad_program.use()\n        self.tex.bind()\n        self.vao.bind()\n\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        self.vao.unbind()\n        self.tex.unbind()\n        self.quad_program.unuse()\n\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n\n        self.quad_program.use()\n        self.tex.bind()\n\n        glBindVertexArray(self.vao)\n        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n        glBindVertexArray(0)\n\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set up the viewport and scissor box for rendering\n        gl.glViewport(x, self.H - y - h, w, h)\n        gl.glScissor(x, self.H - y - h, w, h)\n\n        # Activate the shader program\n        self.quad_program.use()\n\n        # Bind the texture\n        self.tex.bind()\n\n        # Draw the quadrilateral using vertex data\n        self.vao.draw()\n\n        # Restore the viewport and scissor box to their original sizes\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y)\n            return\n\n        self.set_viewport(x, y, w, h)\n        self.set_scissor_box(x, y, w, h)\n\n        self.quad_program.use()\n        self.quad_program['texture0'] = 0\n        self.tex.use(0)\n        self.vao.render(moderngl.TRIANGLE_STRIP)\n\n        self.set_viewport(0, 0, self.W, self.H)\n        self.set_scissor_box(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        self.quad_program.use()\n        self.quad_program.set_uniform_i(\"tex\", 0)\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n\n        self.tex.bind(0)\n        self.vao.bind()\n        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n        self.vao.unbind()\n\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set up the viewport and scissor box\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor_box = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program\n        self.quad_program.use()\n\n        # Bind the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor_box)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set up a specific viewport and scissor box for rendering\n        old_viewport = glGetIntegerv(GL_VIEWPORT)\n        old_scissor = glGetIntegerv(GL_SCISSOR_BOX)\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n\n        # Activate the shader program and bind the texture\n        self.quad_program.use()\n        self.tex.use()\n\n        # Draw the quadrilateral using vertex data\n        self.vao.bind()\n        glDrawArrays(GL_TRIANGLES, 0, 6)\n\n        # Restore the viewport and scissor box to their original sizes\n        glViewport(old_viewport[0], old_viewport[1], old_viewport[2], old_viewport[3])\n        glScissor(old_scissor[0], old_scissor[1], old_scissor[2], old_scissor[3])\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        self.quad_program.use()\n        self.tex.bind()\n\n        self.vao.bind()\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n        self.vao.release()\n\n        self.quad_program.release()\n\n        self.tex.release()\n\n        gl.glViewport(old_viewport[0], old_viewport[1], old_viewport[2], old_viewport[3])\n        gl.glScissor(old_scissor[0], old_scissor[1], old_scissor[2], old_scissor[3])\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set up viewport and scissor box for quad rendering\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n\n        # Enable quad rendering\n        glEnable(GL_SCISSOR_TEST)\n\n        # Activate shader program\n        self.quad_program.use()\n\n        # Bind texture\n        glActiveTexture(GL_TEXTURE0)\n        glBindTexture(GL_TEXTURE_2D, self.tex)\n\n        # Draw quad\n        glBindVertexArray(self.vao)\n        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore viewport and scissor box\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n\n        # Disable quad rendering\n        glDisable(GL_SCISSOR_TEST)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        self.set_viewport(x, y, w, h)\n\n        gl.glEnable(gl.GL_SCISSOR_TEST)\n        gl.glScissor(x, y, w, h)\n\n        self.quad_program.use()\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n        gl.glBindVertexArray(0)\n\n        self.restore_viewport()\n        gl.glDisable(gl.GL_SCISSOR_TEST)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        self.quad_program.use()\n        self.quad_program.set_uniform('u_offset', (x, y))\n        self.quad_program.set_uniform('u_scale', (w, h))\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n        self.tex.bind()\n        self.vao.bind()\n        glDrawArrays(GL_TRIANGLES, 0, 6)\n        self.vao.unbind()\n\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        self.quad_program.use()\n        self.quad_program['pos'] = x, y\n        self.quad_program['size'] = w, h\n        self.quad_program['texture_size'] = self.tex.width, self.tex.height\n        self.quad_program['tex'] = 0\n\n        gl.glViewport(x, self.H - y - h, w, h)\n        gl.glScissor(x, self.H - y - h, w, h)\n        gl.glEnable(gl.GL_SCISSOR_TEST)\n\n        self.tex.use(0)\n        self.vao.render(gl.GL_TRIANGLES)\n\n        gl.glDisable(gl.GL_SCISSOR_TEST)\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n\n        self.quad_program.use()\n        self.quad_program.set_uniform(\"tex\", 0)\n        self.quad_program.set_uniform(\"mvp\", (\n            self.W / w, 0, 0, 0,\n            0, self.H / h, 0, 0,\n            0, 0, 1, 0,\n            0, 0, 0, 1\n        ))\n\n        self.tex.bind(0)\n        self.vao.draw()\n\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y)\n            return\n\n        # Set up the viewport and scissor box\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Enable the quad program and bind the texture\n        self.quad_program.use()\n        self.tex.bind()\n\n        # Draw the quadrilateral using vertex data\n        self.vao.draw()\n\n        # Restore the viewport and scissor box\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y)\n            return\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n\n        glEnable(GL_SCISSOR_TEST)\n        glEnable(GL_BLEND)\n        glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)\n\n        self.quad_program.use()\n        self.tex.bind()\n\n        self.vao.bind()\n        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n\n        self.vao.release()\n\n        glDisable(GL_BLEND)\n        glDisable(GL_SCISSOR_TEST)\n\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Setup viewport\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n\n        # Setup scissor box\n        old_scissor_box = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n        gl.glEnable(gl.GL_SCISSOR_TEST)\n        gl.glScissor(x, y, w, h)\n\n        # Setup shader program\n        self.quad_program.use()\n        self.quad_program.set_uniform(\"u_texture\", 0)\n\n        # Bind texture\n        self.tex.bind()\n\n        # Draw quad\n        self.vao.bind()\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n        self.vao.unbind()\n\n        # Restore viewport\n        gl.glViewport(old_viewport[0], old_viewport[1], old_viewport[2], old_viewport[3])\n\n        # Restore scissor box\n        gl.glDisable(gl.GL_SCISSOR_TEST)\n        gl.glScissor(old_scissor_box[0], old_scissor_box[1], old_scissor_box[2], old_scissor_box[3])\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set up the viewport and scissor box\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program\n        self.quad_program.use()\n\n        # Bind the texture\n        self.tex.bind()\n\n        # Draw the quadrilateral\n        self.vao.draw()\n\n        # Restore the viewport and scissor box\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y)\n            return\n\n        # Set up viewport and scissor box\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n\n        # Activate shader program\n        self.quad_program.use()\n\n        # Bind texture\n        glActiveTexture(GL_TEXTURE0)\n        glBindTexture(GL_TEXTURE_2D, self.tex)\n\n        # Draw quad\n        glBindVertexArray(self.vao)\n        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore viewport and scissor box\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n    K = batch.K\n\n    R = R.transpose(-1, -2)\n    T = T[..., None]\n\n    K = K.clone()\n    K[..., 0, 0] *= W\n    K[..., 1, 1] *= H\n    K[..., 0, 2] *= W\n    K[..., 1, 2] *= H\n\n    C = torch.matmul(-R, T)\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract camera parameters from batch\n    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n    K = batch.K\n\n    # Adjust rotation matrix to match PyTorch3D's requirements\n    R = R.transpose(1, 2)  # Transpose rotation matrix\n\n    # Adjust translation vector to match PyTorch3D's requirements\n    T = -R @ T\n\n    # Compute camera center in camera's coordinate system\n    C = -R.transpose(1, 2) @ T\n\n    # Recalculate intrinsic matrix for NDC\n    K = K.clone()\n    K[:, 0, 0] = K[:, 0, 0] / W\n    K[:, 1, 1] = K[:, 1, 1] / H\n    K[:, 0, 2] = 2 * K[:, 0, 2] / W - 1\n    K[:, 1, 2] = 2 * K[:, 1, 2] / H - 1\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract camera parameters from batch input\n    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n    K = batch.K\n\n    # Adjust rotation matrix\n    R = R.transpose(1, 2)\n\n    # Adjust translation vector\n    T = -R @ T\n\n    # Compute camera center\n    C = -R.transpose(1, 2) @ T\n\n    # Compute intrinsic matrix for NDC\n    K = K.clone()\n    K[..., 0, 0] = K[..., 0, 0] / W * 2\n    K[..., 1, 1] = K[..., 1, 1] / H * 2\n    K[..., 0, 2] = (K[..., 0, 2] / W - 0.5) * 2\n    K[..., 1, 2] = (K[..., 1, 2] / H - 0.5) * 2\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n    K = batch.K\n\n    # Adjust R and T to PyTorch3D's coordinate system\n    R = R.transpose(1, 2)  # Transpose R to match PyTorch3D's coordinate system\n    T = -R @ T  # Adjust T to match PyTorch3D's coordinate system\n\n    # Compute the camera center in the camera's coordinate system\n    C = -R.transpose(1, 2) @ T\n\n    # Compute the intrinsic matrix for NDC\n    K = torch.eye(3, dtype=torch.float32, device=R.device)\n    K[0, 0] = 2.0 / W\n    K[1, 1] = 2.0 / H\n    K[0, 2] = -1.0\n    K[1, 2] = -1.0\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    K = batch.K.clone()\n    K[0, 0] = K[0, 0] / W\n    K[1, 1] = K[1, 1] / H\n    K[0, 2] = K[0, 2] / W\n    K[1, 2] = K[1, 2] / H\n    R = batch.R.clone()\n    T = batch.T.clone()\n    T = T.squeeze(0)\n    R = R.squeeze(0)\n    R = R.transpose(0, 1)\n    T = -R @ T\n    C = -R.transpose(0, 1) @ T\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n    K = batch.K\n\n    # Adjust R\n    R_transposed = R.transpose(-1, -2)\n    R_corrected = R_transposed[:, [1, 0, 2], :]\n    R_corrected[:, :, [1, 2]] = -R_corrected[:, :, [1, 2]]\n\n    # Adjust T\n    T_corrected = T[:, [1, 0, 2]]\n\n    # Compute camera center\n    C = -torch.matmul(R_corrected.transpose(-1, -2), T_corrected)\n\n    # Compute intrinsic matrix for NDC\n    K_ndc = torch.zeros_like(K)\n    K_ndc[:, 0, 0] = 2 / W\n    K_ndc[:, 1, 1] = 2 / H\n    K_ndc[:, 2, 2] = 1\n    K_ndc[:, 0, 2] = -1\n    K_ndc[:, 1, 2] = -1\n\n    return H, W, K_ndc, R_corrected, T_corrected, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract camera parameters from batch\n    H, W = batch[\"H\"], batch[\"W\"]\n    R, T, K = batch[\"R\"], batch[\"T\"], batch[\"K\"]\n\n    # Adjust rotation matrix (R) and translation vector (T)\n    R = R.transpose(1, 2)  # Transpose R to match PyTorch3D's requirements\n    T = -T  # Invert T to match PyTorch3D's coordinate system\n\n    # Compute camera center (C) in camera's coordinate system\n    C = -torch.matmul(R, T)\n\n    # Compute intrinsic matrix (K) for NDC\n    K = torch.matmul(K, torch.tensor([[2.0 / W, 0, -1], [0, 2.0 / H, -1], [0, 0, 1]]))\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n    K = batch.K\n\n    # Adjust R for PyTorch3D\n    R_adj = R.clone()\n    R_adj[:, :2, :2] *= -1\n    R_adj[:, :2, :2] *= -1\n\n    # Adjust T for PyTorch3D\n    T_adj = T.clone()\n    T_adj[:, 0] *= -1\n    T_adj[:, 1] *= -1\n    T_adj[:, 2] *= -1\n\n    # Get camera center in camera coordinate system\n    C = -torch.matmul(R_adj.transpose(-2, -1), T_adj)\n\n    # Get intrinsic matrix for NDC\n    K_ndc = torch.zeros_like(K)\n    K_ndc[:, 0, 0] = 2 / W\n    K_ndc[:, 1, 1] = 2 / H\n    K_ndc[:, 0, 2] = -1\n    K_ndc[:, 1, 2] = -1\n    K_ndc[:, 2, 2] = 1\n\n    return H, W, K_ndc, R_adj, T_adj, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    K = batch.K.clone()\n    K[:, 0, 0] *= W\n    K[:, 1, 1] *= H\n    K[:, 0, 2] *= W - 1\n    K[:, 1, 2] *= H - 1\n    K[:, 1, 2] *= -1\n    K[:, 1, 2] += H - 1\n\n    R = batch.R.clone()\n    T = batch.T.clone()\n\n    R = R.transpose(1, 2)\n    T = T * torch.tensor([-1, -1, 1], device=R.device)\n\n    C = -R.transpose(1, 2) @ T\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n    K = batch.K\n\n    R = R.transpose(-1, -2)\n    T[..., 0] *= -1\n    T[..., 1] *= -1\n    T[..., 2] *= -1\n\n    fx, fy, cx, cy = K[..., 0, 0], K[..., 1, 1], K[..., 0, 2], K[..., 1, 2]\n    fx = fx / W * 2\n    fy = fy / H * 2\n    cx = (cx / W * 2) - 1\n    cy = (cy / H * 2) - 1\n\n    K = torch.stack([\n        fx, 0, cx,\n        0, fy, cy,\n        0, 0, 1\n    ], dim=-1).reshape(batch.batch_size, 1, 3, 3)\n\n    R = R[:, 0:3, 0:3]\n    T = T[:, 0:3]\n\n    C = -torch.matmul(R.transpose(-1, -2), T[:, :, None])[..., 0]\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n    K = batch.K\n\n    R = R.transpose(1, 2)\n    T = -T\n    C = -torch.bmm(R, T)\n    K = K.clone()\n    K[:, 0, 0] *= W\n    K[:, 1, 1] *= H\n    K[:, 0, 2] *= W\n    K[:, 1, 2] *= H\n    K[:, 1, 2] += H\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract camera parameters\n    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n    K = batch.K\n\n    # Adjust rotation matrix (R)\n    R = R.permute(0, 2, 1)  # Transpose R to match PyTorch3D's convention\n    R = R @ torch.tensor([[1, 0, 0], [0, -1, 0], [0, 0, -1]], device=R.device).float()  # Apply correction to R\n\n    # Adjust translation vector (T)\n    T = T @ torch.tensor([[1, 0, 0], [0, -1, 0], [0, 0, -1]], device=T.device).float()  # Apply correction to T\n\n    # Compute camera center (C) in camera's coordinate system\n    C = -torch.inverse(R) @ T\n\n    # Compute camera intrinsic matrix for NDC\n    K = K.clone()\n    K[:, 0, 0] = K[:, 0, 0] / W\n    K[:, 1, 1] = K[:, 1, 1] / H\n    K[:, 0, 2] = 2 * K[:, 0, 2] / W - 1\n    K[:, 1, 2] = 2 * K[:, 1, 2] / H - 1\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R, T, K = batch.R, batch.T, batch.K\n\n    # Adjust R\n    R = R.transpose(1, 2)  # Transpose R to match PyTorch3D's coordinate system\n    R = R[:, :, [0, 2, 1]]  # Swap R's columns to match PyTorch3D's coordinate system\n    R[:, :, 1] *= -1  # Invert R's second column to match PyTorch3D's coordinate system\n\n    # Adjust T\n    T = T[:, [0, 2, 1]]  # Swap T's elements to match PyTorch3D's coordinate system\n    T[:, 1] *= -1  # Invert T's second element to match PyTorch3D's coordinate system\n\n    # Recalculate K for NDC\n    K = K.clone()\n    K[:, 0, 0] = K[:, 0, 0] / W * 2  # Normalize and scale K for NDC\n    K[:, 1, 1] = K[:, 1, 1] / H * 2  # Normalize and scale K for NDC\n    K[:, :2, 2] = -1 + 2 * K[:, :2, 2] / torch.tensor([W, H]).to(K.device)  # Normalize and scale K for NDC\n\n    # Compute camera center\n    C = -torch.bmm(R.transpose(1, 2), T[:, :, None])[:, :, 0]  # Compute camera center in camera's coordinate system\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    K = batch.K\n    R = batch.R\n    T = batch.T\n\n    # Transpose R to match pytorch3d camera format\n    R = R.transpose(1, 2)\n\n    # Pytorch3d camera coordinate system: left up front\n    # Correct R and T to match pytorch3d camera format\n    R[..., 0, :] = -R[..., 0, :]\n    R[..., 1, :] = -R[..., 1, :]\n    T = -T\n\n    # Compute camera center in camera coordinate system\n    C = -torch.matmul(R.transpose(-1, -2), T)\n\n    # Get intrinsic matrix for normalized device coordinates (NDC)\n    K_ndc = torch.zeros_like(K)\n    K_ndc[..., 0, 0] = 2 / W\n    K_ndc[..., 1, 1] = 2 / H\n    K_ndc[..., 0, 2] = -1\n    K_ndc[..., 1, 2] = -1\n    K_ndc[..., 2, 2] = 1\n\n    return H, W, K_ndc, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n    K = batch.K\n\n    # Adjust R to PyTorch3D convention\n    R_transposed = R.transpose(-1, -2)\n    R_adjusted = R_transposed[:, [0, 2, 1], :]\n    R_adjusted[:, :, 1] *= -1\n    R_adjusted[:, :, 2] *= -1\n\n    # Adjust T to PyTorch3D convention\n    T_adjusted = T[:, [0, 2, 1]]\n    T_adjusted[:, 1] *= -1\n    T_adjusted[:, 2] *= -1\n\n    # Calculate camera center in camera's coordinate system\n    C = -torch.bmm(R_adjusted.transpose(-1, -2), T_adjusted.unsqueeze(-1)).squeeze(-1)\n\n    # Calculate intrinsic matrix for NDC\n    K_ndc = torch.zeros_like(K)\n    K_ndc[:, 0, 0] = 2.0 / W\n    K_ndc[:, 1, 1] = 2.0 / H\n    K_ndc[:, 0, 2] = -1\n    K_ndc[:, 1, 2] = -1\n    K_ndc[:, 2, 2] = 1\n\n    return H, W, K_ndc, R_adjusted, T_adjusted, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract camera parameters from batch\n    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n    K = batch.K\n\n    # Adjust rotation matrix to match PyTorch3D's coordinate system\n    R_pytorch3d = R.transpose(1, 2)\n\n    # Adjust translation vector to match PyTorch3D's coordinate system\n    T_pytorch3d = -R_pytorch3d @ T\n\n    # Compute camera center in camera's coordinate system\n    C = -R_pytorch3d.transpose(1, 2) @ T_pytorch3d\n\n    # Compute intrinsic matrix for NDC\n    K_pytorch3d = K.clone()\n    K_pytorch3d[:, 0, 0] = K[:, 0, 0] / W\n    K_pytorch3d[:, 1, 1] = K[:, 1, 1] / H\n    K_pytorch3d[:, 0, 2] = 2 * K[:, 0, 2] / W - 1\n    K_pytorch3d[:, 1, 2] = 2 * K[:, 1, 2] / H - 1\n\n    return H, W, K_pytorch3d, R_pytorch3d, T_pytorch3d, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    K = batch.K\n    R = batch.R\n    T = batch.T\n\n    R = R.transpose(1, 2)\n    T = -R @ T\n\n    K = torch.eye(4)\n    K[0, 0] = W / 2.0\n    K[1, 1] = H / 2.0\n    K[0, 2] = W / 2.0\n    K[1, 2] = H / 2.0\n    K[2, 2] = 1.0\n    K = K.float().to(R.device)\n\n    C = torch.zeros(3).to(R.device)\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract camera parameters from batch\n    H, W = batch.H, batch.W\n    R = batch.R\n    T = batch.T\n    K = batch.K\n\n    # Adjust rotation matrix\n    R = R.transpose(1, 2)\n\n    # Adjust translation vector\n    T = -R @ T\n\n    # Compute camera center\n    C = -R.transpose(-2, -1) @ T\n\n    # Compute intrinsic matrix for NDC\n    K = K.clone()\n    K[..., 0, 0] = K[..., 0, 0] / W\n    K[..., 1, 1] = K[..., 1, 1] / H\n    K[..., 0, 2] = -1 + 2 * K[..., 0, 2] / W\n    K[..., 1, 2] = -1 + 2 * K[..., 1, 2] / H\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n    K = batch.K\n    K = K.float()\n\n    # Adjust R and T to PyTorch3D's coordinate system\n    R = R.transpose(-1, -2)  # PyTorch3D camera system: right down front\n    T[..., 1] = -T[..., 1]\n    T[..., 2] = -T[..., 2]\n\n    # Compute K for NDC\n    K = K.clone()\n    K[..., 0] *= 2.0 / W\n    K[..., 1] *= 2.0 / H\n    K[..., 2] = -1.0\n    K[..., 3] = -1.0\n\n    # Get camera center\n    C = -torch.matmul(R, T)\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n\n    R = R.transpose(-1, -2)\n    T = -R @ T\n    K = batch.K\n\n    K = K.clone()\n    K[..., 0, 0] *= W\n    K[..., 1, 1] *= H\n    K[..., 0, 2] *= W\n    K[..., 1, 2] *= H\n\n    C = -R.transpose(-1, -2) @ T\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n        glBindFramebuffer(GL_DRAW_FRAMEBUFFER, 0)\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        glBlitFramebuffer(0, 0, w, h, x, y, x + w, y + h, GL_COLOR_BUFFER_BIT, GL_LINEAR)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        with self.FBO.bind_to_read():\n            glBlitFramebuffer(x, y, w or self.W, h or self.H, x, y, w or self.W, h or self.H, GL_COLOR_BUFFER_BIT, GL_LINEAR)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.FBO)\n        gl.glBlitFramebuffer(x, y, w or self.W, h or self.H, x, y, w or self.W, h or self.H, gl.GL_COLOR_BUFFER_BIT, gl.GL_LINEAR)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        with self.FBO.bind_to_read():\n            glBlitFramebuffer(x, y, w or self.W, h or self.H, x, y, w or self.W, h or self.H,\n                              GL_COLOR_BUFFER_BIT, GL_NEAREST)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        with self.FBO.bind_to_read():\n            gl.glReadPixels(x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, self.pixels)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Temporarily bind the Quad instance's framebuffer object (FBO) as the read framebuffer\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n\n        # Perform the pixel copy operation\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        glBlitFramebuffer(x, y, w, h, x, y, w, h, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n\n        # Restore the previously bound read framebuffer\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Save the current read framebuffer\n        read_fbo = js['gl']['read_fbo']\n\n        # Bind the Quad's framebuffer object as the read framebuffer\n        js['gl']['bind_framebuffer'](js['gl']['FRAMEBUFFER'], self.fbo)\n\n        # Perform the pixel copy operation\n        js['gl']['blit_framebuffer'](\n            0, 0, self.W, self.H,  # Source rectangle\n            x, y, w or self.W, h or self.H,  # Destination rectangle\n            js['gl']['COLOR_BUFFER_BIT'],  # Bitmask for color buffer\n            js['gl']['NEAREST']  # Interpolation method\n        )\n\n        # Restore the previously bound read framebuffer\n        js['gl']['bind_framebuffer'](js['gl']['READ_FRAMEBUFFER'], read_fbo)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        with self.FBO.bind_to_read():\n            gl.glBlitFramebuffer(\n                x,\n                y,\n                x + w,\n                y + h,\n                x,\n                y,\n                x + w,\n                y + h,\n                gl.GL_COLOR_BUFFER_BIT,\n                gl.GL_LINEAR,\n            )\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Temporarily bind the Quad instance's framebuffer object (FBO) as the read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.FBO)\n\n        # Perform the pixel copy operation\n        gl.glBlitFramebuffer(0, 0, self.W, self.H, x, y, x + w, y + h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Restore the previously bound read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.FBO)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)\n        gl.glBlitFramebuffer(x, y, w, h, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        with self.FBO as fbo:\n            gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, fbo)\n            gl.glBlitFramebuffer(\n                x, y, w, h, 0, 0, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST\n            )\n            gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        with self.bind_to_read():\n            glCopyTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, x, y, w or self.W, h or self.H, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        self.FBO.bind_read()\n        self.FBO.bind_draw()\n        glBlitFramebuffer(x, y, w, h, x, y, w, h, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n        self.FBO.unbind_read()\n        self.FBO.unbind_draw()\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        with self.FBO.bind_to_read():\n            with self.FBO.bind_to_write():\n                gl.glBlitFramebuffer(x, y, w, h, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        with self.FBO as fbo:\n            glCopyTexSubImage2D(GL_TEXTURE_2D, 0, x, y, x, y, w, h)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Bind the Quad instance's framebuffer object as the read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.FBO)\n\n        # If w or h are 0, use the width and height of the Quad instance\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        # Copy the pixel block from the read framebuffer to the draw framebuffer\n        gl.glBlitFramebuffer(x, y, w, h, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_LINEAR)\n\n        # Restore the previously bound read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        pass\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.fbo)\n        glBindFramebuffer(GL_DRAW_FRAMEBUFFER, 0)\n        glBlitFramebuffer(x, y, w or self.W, h or self.H, x, y, w or self.W, h or self.H, GL_COLOR_BUFFER_BIT, GL_LINEAR)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n        glBindFramebuffer(GL_DRAW_FRAMEBUFFER, 0)\n        glBlitFramebuffer(x, y, w, h, x, y, w, h, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n        glBlitFramebuffer(x, y, w or self.W, h or self.H, x, y, w or self.W, h or self.H, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the cumulative sum of y1\n    y1_cumsum = tf.math.cumsum(y1)\n\n    # Find the indices where t0 is greater than t1\n    indices = tf.searchsorted(t1, t0)\n\n    # Compute the inner measure using the cumulative sum of y1\n    inner = tf.gather(y1_cumsum, indices)\n\n    # Compute the outer measure using the cumulative sum of y1\n    outer = tf.gather(y1_cumsum, indices - 1)\n\n    # Return the inner and outer measures\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the inner measure\n    inner = tf.where(t0 >= t1, t0 - t1, tf.zeros_like(t0))\n\n    # Compute the outer measure\n    outer = tf.where(t0 >= t1, t1, tf.zeros_like(t0))\n\n    return inner, outer\n\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Check if t0, t1, and y1 are tensors\n    assert tf.is_tensor(t0), \"t0 must be a tensor\"\n    assert tf.is_tensor(t1), \"t1 must be a tensor\"\n    assert tf.is_tensor(y1), \"y1 must be a tensor\"\n\n    # Check if t0, t1, and y1 have the same shape\n    assert t0.shape == t1.shape, \"t0 and t1 must have the same shape\"\n    assert t0.shape == y1.shape, \"t0 and y1 must have the same shape\"\n\n    # Check if t0, t1, and y1 have at least rank 2\n    assert t0.ndim >= 2, \"t0 must have at least rank 2\"\n    assert t1.ndim >= 2, \"t1 must have at least rank 2\"\n    assert y1.ndim >= 2, \"y1 must have at least rank 2\"\n\n    # Check if t0, t1, and y1 have the same number of dimensions\n    assert t0.ndim == t1.ndim, \"t0 and t1 must have the same number of dimensions\"\n    assert t0.ndim == y1.ndim, \"t0 and y1 must have the same number of dimensions\"\n\n    # Check if t0, t1, and y1 have the same number of elements\n    assert t0.numel() == t1.numel(), \"t0 and t1 must have the same number of elements\"\n    assert t0.numel() == y1.numel(), \"t0 and y1 must have the same number of elements\"\n\n    # Check if t0, t1, and y1 have the same dtype\n    assert t0.dtype == t1.dtype, \"t0 and t1 must have the same dtype\"\n    assert t0.dtype == y1.dtype, \"t0 and y1 must have the same dtype\"\n\n    # Check if t0, t1, and y1 have the same device\n    assert t0.device =="}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Calculate the cumulative sum of y1\n    y1_cumsum = tf.math.cumsum(y1, axis=-1)\n\n    # Find the indices of t1 that are less than or equal to t0\n    indices = tf.searchsorted(t1, t0, side='right')\n\n    # Calculate the inner measure\n    inner = tf.gather(y1_cumsum, indices - 1, axis=-1)\n\n    # Calculate the outer measure\n    outer = tf.gather(y1_cumsum, indices, axis=-1)\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0 = t0.unsqueeze(1)\n    t1 = t1.unsqueeze(0)\n    y1 = y1.unsqueeze(0)\n\n    inner = torch.cumsum(torch.where(t0 >= t1, y1, 0), dim=1)\n    outer = torch.cumsum(torch.where(t0 >= t1, y1, y1.flip(dims=[1])), dim=1)\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the cumulative sum of y1\n    cs = tf.math.cumsum(y1, axis=-1)\n\n    # Find the indices of the source times that are less than or equal to the target times\n    idx = tf.searchsorted(t1, t0, side='right')\n\n    # Compute the inner measure using the cumulative sum and the indices\n    inner = tf.gather(cs, idx - 1, axis=-1)\n\n    # Compute the outer measure using the cumulative sum and the indices\n    outer = tf.gather(cs, idx, axis=-1)\n\n    # Return the inner and outer measures\n    return inner, outer\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Calculate the inner measure\n    inner = tf.math.maximum(tf.constant(0, dtype=tf.float32), tf.math.minimum(t0, t1[1:]) - t1[:-1])\n\n    # Calculate the outer measure\n    outer = tf.math.maximum(tf.constant(0, dtype=tf.float32), tf.math.minimum(t1[1:], t0) - t1[:-1])\n\n    # Return the inner and outer measures\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Check if t0 is a tensor\n    if not isinstance(t0, torch.Tensor):\n        raise TypeError(\"t0 must be a tensor\")\n\n    # Check if t1 is a tensor\n    if not isinstance(t1, torch.Tensor):\n        raise TypeError(\"t1 must be a tensor\")\n\n    # Check if y1 is a tensor\n    if not isinstance(y1, torch.Tensor):\n        raise TypeError(\"y1 must be a tensor\")\n\n    # Check if t0 and t1 have the same shape\n    if t0.shape != t1.shape:\n        raise ValueError(\"t0 and t1 must have the same shape\")\n\n    # Check if y1 and t1 have the same shape\n    if y1.shape != t1.shape:\n        raise ValueError(\"y1 and t1 must have the same shape\")\n\n    # Check if t0 is sorted in ascending order\n    if not torch.all(t0[:-1] <= t0[1:]):\n        raise ValueError(\"t0 must be sorted in ascending order\")\n\n    # Check if t1 is sorted in ascending order\n    if not torch.all(t1[:-1] <= t1[1:]):\n        raise ValueError(\"t1 must be sorted in ascending order\")\n\n    # Check if t0 is within the range of t1\n    if not torch.all(t0 >= t1[0]) or not torch.all(t0 <= t1[-1]):\n        raise ValueError(\"t0 must be within the range of t1\")\n\n    # Initialize inner and outer measures as zero tensors\n    inner = torch.zeros_like(t0)\n    outer = torch.zeros_like(t0)\n\n    # Loop over each target time\n    for i in range(len(t0)):\n        # Find the indices of t1 that are less than or equal to the current target time\n        indices = torch.where(t1 <= t0[i])[0]\n\n        # If there are indices, calculate the inner"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Get the cumulative sum of y1\n    y1_cumsum = tf.math.cumsum(y1, axis=-1)\n\n    # Get the indices of t1 that are less than or equal to t0\n    idx = tf.searchsorted(t1, t0, side='right')\n\n    # Compute the inner measure\n    inner = tf.gather(y1_cumsum, idx - 1, axis=-1)\n\n    # Compute the outer measure\n    outer = tf.gather(y1_cumsum, idx, axis=-1)\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0 = tf.expand_dims(t0, axis=-1)\n    t1 = tf.expand_dims(t1, axis=0)\n    y1 = tf.expand_dims(y1, axis=0)\n    inner = tf.reduce_sum(tf.where(t0 > t1, tf.zeros_like(t0), tf.cumsum(tf.where(t0 >= t1, y1, tf.zeros_like(y1)), axis=1)), axis=1)\n    outer = tf.reduce_sum(tf.where(t0 >= t1, y1, tf.zeros_like(y1)), axis=1)\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Calculate the cumulative sum of y1\n    y1_cumsum = tf.cumsum(y1, axis=1)\n\n    # Calculate the inner measure using the cumulative sum of y1\n    y0_inner = tf.math.maximum(0.0, tf.math.minimum(y1_cumsum[:, -1, None], tf.math.maximum(t0[:, :, None] - t1[:, None, :], 0.0)))\n\n    # Calculate the outer measure using the cumulative sum of y1\n    y0_outer = tf.math.maximum(0.0, tf.math.minimum(y1_cumsum[:, -1, None], tf.math.maximum(t0[:, :, None] - t1[:, None, :], 0.0)))\n\n    return y0_inner, y0_outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the cumulative sum of the values\n    csum = tf.cumsum(y1, axis=-1)\n\n    # Compute the inner measure by interpolating the cumulative sum\n    inner = tf.experimental.numpy.interp(t0, t1, csum)\n\n    # Compute the outer measure by subtracting the inner measure from the cumulative sum\n    outer = tf.experimental.numpy.interp(t0, t1, tf.concat([[0], csum], axis=-1))\n    outer = outer[..., 1:] - inner\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Calculate the time differences\n    dt = t1[..., 1:] - t1[..., :-1]\n\n    # Calculate the cumulative sum of the time differences\n    cs = tf.math.cumsum(dt, axis=-1)\n\n    # Calculate the inner and outer measures\n    inner = tf.concat([tf.zeros_like(cs[..., :1]), cs], axis=-1)\n    outer = tf.concat([cs, tf.zeros_like(cs[..., :1])], axis=-1)\n\n    return inner, outer\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Importing the required libraries\n    import tensorflow as tf\n\n    # Reshaping the input tensors\n    t0 = tf.reshape(t0, [-1])\n    t1 = tf.reshape(t1, [-1])\n    y1 = tf.reshape(y1, [-1])\n\n    # Computing the cumulative sum of y1\n    cumsum = tf.math.cumsum(y1)\n\n    # Computing the inner measure\n    inner = tf.where(tf.math.greater(t0, t1[0]), tf.where(tf.math.less(t0, t1[-1]), tf.map_fn(lambda x: tf.reduce_sum(tf.where(tf.math.less_equal(t1, x), y1, 0.)), t0), y1[-1]), 0.)\n\n    # Computing the outer measure\n    outer = tf.where(tf.math.greater(t0, t1[0]), tf.where(tf.math.less(t0, t1[-1]), tf.map_fn(lambda x: tf.reduce_sum(tf.where(tf.math.less_equal(t1, x), cumsum, 0.)), t0), cumsum[-1]), 0.)\n\n    # Returning the inner and outer measures\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0 = tf.convert_to_tensor(t0, dtype=tf.float32)\n    t1 = tf.convert_to_tensor(t1, dtype=tf.float32)\n    y1 = tf.convert_to_tensor(y1, dtype=tf.float32)\n\n    # Compute the cumulative sum of the values\n    y1_cumsum = tf.cumsum(y1)\n\n    # Find the indices of the source times that are less than or equal to the target time\n    indices = tf.where(t1 <= t0)\n\n    # Select the corresponding values from the cumulative sum\n    y1_cumsum_selected = tf.gather_nd(y1_cumsum, indices)\n\n    # Compute the inner and outer measures\n    inner = tf.reduce_sum(y1_cumsum_selected)\n    outer = tf.reduce_sum(y1)\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the time differences between t0 and t1\n    dt = t0[:, None] - t1[None, :]\n\n    # Compute the inner measure based on the time differences\n    inner = (dt >= 0).float() * dt\n\n    # Compute the outer measure based on the time differences\n    outer = (dt < 0).float() * dt\n\n    # Return the inner and outer measures\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the cumulative sum of y1\n    y1_cumsum = tf.math.cumsum(y1)\n\n    # Find the indices of t1 that are less than or equal to t0\n    indices = tf.searchsorted(t1, t0, side='right')\n\n    # Clip the indices to ensure they are within the valid range of t1\n    indices = tf.clip_by_value(indices, 0, len(t1) - 1)\n\n    # Compute the inner measure using the cumulative sum of y1\n    inner = tf.gather(y1_cumsum, indices)\n\n    # Compute the outer measure using the difference between t0 and t1\n    outer = t0 - tf.gather(t1, indices)\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Calculate the difference between the target time and the source times\n    t_diff = t0[..., None, :] - t1[..., None]\n\n    # Create a mask to identify which values are greater than the target time\n    mask = t_diff >= 0\n\n    # Calculate the cumulative sum of the mask along the source time axis\n    cum_sum = tf.cumsum(tf.cast(mask, tf.float32), axis=-1)\n\n    # Calculate the inner measure as the cumulative sum of the values multiplied by the mask\n    inner = tf.reduce_sum(y1[..., None] * cum_sum * tf.cast(mask, tf.float32), axis=-1)\n\n    # Calculate the outer measure as the cumulative sum of the values multiplied by the complement of the mask\n    outer = tf.reduce_sum(y1 * cum_sum * (1 - tf.cast(mask, tf.float32)), axis=-1)\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the cumulative sum of y1\n    cs_y1 = tf.math.cumsum(y1, axis=-1)\n\n    # Compute the inner measure by taking the difference between t0 and t1\n    inner = t0 - t1\n\n    # Compute the outer measure by taking the difference between the cumulative sum of y1 and t1\n    outer = cs_y1 - t1\n\n    return inner, outer\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the cumulative sum of the values (y1)\n    y1_cumsum = tf.math.cumsum(y1, axis=-1)\n\n    # Compute the inner measure using the cumulative sum\n    inner = tf.math.minimum(t0, t1) * y1_cumsum\n\n    # Compute the outer measure using the cumulative sum\n    outer = tf.math.maximum(t0, t1) * y1_cumsum\n\n    # Return the inner and outer measures\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the upper envelope weights\n    w_upper = torch.cat([w_env, torch.zeros_like(w_env[..., :1])], dim=-1)\n    w_upper = torch.max(w_upper, dim=-1, keepdim=True)[0]\n\n    # Calculate the difference between target weights and the upper envelope\n    w_diff = w - w_upper\n\n    # Calculate the half-quadratic loss\n    loss = w_diff ** 2 / (4 * w_upper + eps)\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    t_env, w_env = t_env[..., None, :], w_env[..., None, :]\n    w_max = torch.maximum(w[..., None], w_env)\n    norm = torch.sum((w[..., None] / w_max) * torch.exp(-0.5 * (t[..., None, :] - t_env) ** 2), dim=-1)\n    return 0.5 * torch.sum((norm - 1) ** 2, dim=-1) / (torch.sum(w, dim=-1) + eps)\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the upper envelope weights\n    w_upper = (w_env[..., 1:] + w_env[..., :-1]) / 2\n    w_upper = torch.cat([w_upper, w_upper[..., -1:]], dim=-1)\n\n    # Calculate the difference between target weights and the upper envelope\n    w_diff = w[..., 1:-1] - w_upper[..., 1:-1]\n\n    # Calculate the scaled half-quadratic loss\n    loss = w_diff / (w_upper[..., 1:-1] + eps)\n    loss = loss.abs()\n    loss = loss.mean()\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the upper envelope weights\n    w_upper = upper_envelope(t_env, w_env)\n\n    # Calculate the difference between the target weights and the upper envelope weights\n    w_diff = w - w_upper\n\n    # Calculate the half-quadratic loss\n    loss_inner = w_diff ** 2 / (4 * w_upper + eps)\n\n    # Scale the loss by the difference between the target positions and the environment positions\n    loss_outer = loss_inner * (t - t_env).abs()\n\n    # Return the scaled loss\n    return loss_outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the upper envelope weights\n    w_upper = upper_envelope(t_env, w_env)\n\n    # Calculate the difference between the target weights and the upper envelope\n    w_diff = w - w_upper\n\n    # Calculate the loss based on the difference\n    loss = torch.where(w_diff > 0, w_diff, torch.zeros_like(w_diff))\n\n    # Scale the loss by a half-quadratic function\n    loss = loss * loss / (2 * eps)\n\n    # Return the calculated loss\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the difference between the target weights and the upper envelope.\n    w_diff = w - upper_envelope(t, w_env, t_env)\n\n    # Calculate the loss based on the difference between target weights and the upper envelope.\n    loss = w_diff ** 2 / 2\n\n    # Scale the loss by the maximum weight of the target weights.\n    loss = loss / (w.max() + eps)\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_upper = torch.cat([w_env, w_env[-1:]], dim=-1)\n    w_upper = torch.min(w_upper[..., :-1], w_upper[..., 1:])\n\n    # calculate the difference between target weights and the upper envelope\n    w_diff = w - w_upper\n\n    # calculate the half-quadratic loss based on the difference\n    loss = 0.5 * w_diff * w_diff / (eps + w_upper)\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_upper = torch.cat([w_env, torch.flip(w_env, dims=[-1])], dim=-1)\n    t_upper = torch.cat([t_env, torch.flip(t_env, dims=[-1])], dim=-1)\n\n    # calculate the difference between target weights and the upper envelope\n    w_diff = w[..., None, :] - w_upper[..., None, :]\n\n    # calculate the half-quadratic loss\n    loss = w_diff**2 / (4 * t_upper[..., None, :] - 2 * t[..., None, :] - t_upper[..., None, :])\n    loss = torch.where(loss != loss, torch.zeros_like(loss), loss)\n    loss = loss.min(dim=-1)[0]\n\n    # scale the loss by the target weights\n    loss = loss * w\n\n    return loss\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_upper = (w_env[..., 1:] - w_env[..., :-1]) / (t_env[..., 1:] - t_env[..., :-1] + eps)\n\n    # calculate the difference between target weights and the upper envelope\n    w_diff = w[..., 1:-1] - w_upper\n\n    # calculate the half-quadratic loss\n    loss = w_diff**2 / 2\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_upper = torch.cat([w_env, w_env[..., -1:]], dim=-1)\n    w_upper = torch.maximum(w_upper[..., :-1], w_upper[..., 1:])\n\n    # calculate the difference between target weights and the upper envelope\n    w_diff = w - w_upper\n\n    # calculate the scaled half-quadratic loss\n    loss = torch.where(\n        w_diff > 0,\n        1 / (2 * w_upper * w_upper) * w_diff * w_diff,\n        torch.zeros_like(w_diff)\n    )\n\n    # return the loss\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the upper envelope weights\n    w_upper = (w_env[..., :-1] + w_env[..., 1:]) / 2\n    w_upper = torch.cat([w_upper, w_upper[..., -1:]], dim=-1)\n\n    # Calculate the difference between the target weights and the upper envelope\n    w_diff = w - w_upper\n\n    # Calculate the half-quadratic loss\n    loss = w_diff**2 / (4 * w_upper + eps)\n\n    # Calculate the scaled loss\n    scale = (t_env[..., 1:] - t_env[..., :-1]) * (w_upper[..., 1:] + w_upper[..., :-1] + eps)\n    loss = loss * scale\n\n    # Return the sum of the loss\n    return loss.sum(dim=-1)\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the upper envelope weights\n    w_upper = (t[..., 1:] - t[..., :-1]) * (w[..., 1:] + w[..., :-1]) / 2\n    w_upper = torch.cat([w_upper, torch.zeros_like(w_upper[..., :1])], -1)\n    w_upper = w_upper.detach()\n\n    # Calculate the loss based on the difference between target weights and the upper envelope\n    w_loss = torch.relu(w_upper - w) / (w_upper + eps)\n\n    # Scale the loss by a half-quadratic function\n    w_loss = w_loss * (1 - torch.exp(-w_loss))\n\n    return w_loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the envelope weights\n    w_upper = (w_env[..., 1:] - w_env[..., :-1]) / (t_env[..., 1:] - t_env[..., :-1] + eps)\n    w_upper = torch.cat([w_upper, w_upper[..., -1:]], dim=-1)\n\n    # calculate the envelope positions\n    t_upper = (t_env[..., 1:] + t_env[..., :-1]) / 2\n    t_upper = torch.cat([t_env[..., :1], t_upper, t_env[..., -1:]], dim=-1)\n\n    # calculate the loss\n    loss = torch.abs(w - w_upper)\n    loss = loss.view(*loss.shape[:-1], -1, 2).sum(-1) / 2\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_upper = torch.cat([w_env[..., :-1], w_env[..., -2:-1]], dim=-1)\n\n    # calculate the difference between target weights and the upper envelope\n    w_diff = w[..., 1:] - w_upper\n\n    # calculate the loss based on the difference between target weights and the upper envelope, scaled by a half-quadratic loss function\n    loss_outer = w_diff * w_diff / 4 * (1 - t_env[..., 1:]).clamp(min=0) / (t[..., 1:] - t[..., :-1]).clamp(min=eps)\n\n    # return the calculated loss\n    return loss_outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the upper envelope weights using the environment weights and positions\n    w_upper = (w_env[..., 1:] - w_env[..., :-1]) / (t_env[..., 1:] - t_env[..., :-1] + eps)\n    w_upper = torch.cat([w_upper, torch.zeros_like(w_upper[..., :1])], dim=-1)\n\n    # Calculate the difference between the target weights and the upper envelope weights\n    w_diff = w - w_upper\n\n    # Calculate the half-quadratic loss based on the difference between target weights and the upper envelope\n    loss = torch.where(w_diff > 0, w_diff ** 2, torch.zeros_like(w_diff))\n\n    # Calculate the scaled loss by dividing by the target weights and adding a small epsilon value to prevent division by zero\n    loss = loss / (w + eps)\n\n    # Return the sum of the scaled loss\n    return loss.sum(-1)\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the envelope weights using the environment positions and weights\n    w_envelope = torch.cat([w_env[..., 0:1], w_env[..., 1:] + w_env[..., :-1]], dim=-1)\n\n    # Calculate the upper envelope of the proposal weights using the environment weights\n    w_upper = torch.cat([w[..., 0:1], w[..., 1:] + w_envelope[..., :-1]], dim=-1)\n\n    # Calculate the difference between the proposal weights and the upper envelope\n    w_diff = w - w_upper\n\n    # Calculate the scaled half-quadratic loss based on the difference between proposal weights and the upper envelope\n    loss_outer = w_diff ** 2 / (4 * w_envelope + eps)\n\n    return loss_outer\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    t_env, w_env = t_env[..., None, :], w_env[..., None, :]\n    w_max = torch.maximum(w[..., None], w_env)\n    w_max = w_max[..., :-1]\n    return (w - w_max) ** 2 * relu(1 - (t[..., None, :] - t_env) / (t[..., None, :] - t_env + eps)) ** 2\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the envelope weights\n    w_upper = torch.cat([w_env, torch.zeros_like(w_env[:, :1])], -1)\n    w_lower = torch.cat([torch.zeros_like(w_env[:, :1]), w_env], -1)\n    ew = w_upper + w_lower\n\n    # Calculate the envelope positions\n    t_upper = torch.cat([t_env, torch.full_like(t_env[:, :1], t_env.max())], -1)\n    t_lower = torch.cat([t_env.min() * torch.ones_like(t_env[:, :1]), t_env], -1)\n\n    # Calculate the loss based on the difference between target weights and the upper envelope\n    loss = (w[..., None, :] - ew[..., None, :]).abs() * (t[..., None, :] >= t_lower[..., None, :]) * (t[..., None, :] <= t_upper[..., None, :])\n\n    # Scale the loss by a half-quadratic function\n    scale = 1.0 - 2.0 * (t[..., None, :] - t_lower[..., None, :]) / (t_upper[..., None, :] - t_lower[..., None, :] + eps)\n    loss = loss * scale\n\n    return loss.sum(-1)\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope\n    t_env, w_env = t_env.squeeze(), w_env.squeeze()\n    t_env_pad = torch.cat([t_env, torch.ones_like(t_env[..., -1:]) * 2], dim=-1)\n    w_env_pad = torch.cat([w_env[..., :1], w_env, w_env[..., -1:]], dim=-1)\n    w_upper = torch.min(w_env_pad[..., :-1], w_env_pad[..., 1:])\n    w_upper = w_upper.view(*w_upper.shape[:-1], *([1] * (len(t.shape) - 1 - len(w_upper.shape))), w_upper.shape[-1])\n\n    # calculate the loss\n    lossfun_inner = lambda x: torch.where(x >= 0, x ** 2, torch.zeros_like(x))\n    loss = lossfun_inner(w - w_upper)\n    loss = loss / (eps + loss.sum(dim=-1, keepdim=True))\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    t_env, w_env = t_env[..., None, :], w_env[..., None, :]\n    w_upper = (w_env[..., 1:] - w_env[..., :-1]) / (t_env[..., 1:] - t_env[..., :-1] + eps)\n    w_upper = torch.cat((w_upper, w_upper[..., -2:-1]), dim=-1)\n    w_upper = w_upper.abs()\n\n    w_upper = w_upper.sum(dim=-1) / 2\n\n    return ((w_upper - w) ** 2 / (w_upper + w + eps)).mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # calculate the inter-interval loss\n    inter_loss = torch.sum(w * torch.abs(t[:, 1:] - t[:, :-1]), dim=-1)\n\n    # calculate the intra-interval loss\n    intra_loss = torch.sum(w * torch.abs(t[:, 1:] - t[:, :-1] - 1), dim=-1)\n\n    # combine the inter-interval and intra-interval losses\n    loss = inter_loss + intra_loss\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape = (batch_size, num_intervals + 1)\n    # w.shape = (batch_size, num_intervals)\n    # t_intervals.shape = (batch_size, num_intervals)\n    # t_intervals_diff.shape = (batch_size, num_intervals)\n    # t_intervals_diff_sq.shape = (batch_size, num_intervals)\n    # t_intervals_diff_sq_w.shape = (batch_size, num_intervals)\n    # t_intervals_diff_sq_w_sum.shape = (batch_size,)\n    # t_intervals_diff_sq_w_sum_mean.shape = (batch_size,)\n    # t_intervals_diff_sq_w_sum_mean_sqrt.shape = (batch_size,)\n    # t_intervals_diff_sq_w_sum_mean_sqrt_sq.shape = (batch_size,)\n    # t_intervals_diff_sq_w_sum_mean_sqrt_sq_w.shape = (batch_size,)\n    # t_intervals_diff_sq_w_sum_mean_sqrt_sq_w_sum.shape = (batch_size,)\n    # t_intervals_diff_sq_w_sum_mean_sqrt_sq_w_sum_mean.shape = (batch_size,)\n    # t_intervals_diff_sq_w_sum_mean_sqrt_sq_w_sum_mean_sqrt.shape = (batch_size,)\n    # t_intervals_diff_sq_w_sum_mean_sqrt_sq_w_sum_mean_sqrt_sq.shape = (batch_size,)\n    # t_intervals_diff_sq_w_sum_mean_sqrt_sq_w_sum_mean_sqrt_sq_w.shape = (batch_size,)\n    # t_intervals_diff_sq_w_sum_mean_sqrt_sq_w_sum_mean_sqrt_sq_w"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # calculate the inter-interval loss\n    inter_loss = torch.sum(torch.abs(t[:, 1:] - t[:, :-1]) * w, dim=-1)\n\n    # calculate the intra-interval loss\n    intra_loss = torch.sum(torch.abs(t[:, 1:] - t[:, :-1]) * (1 - w), dim=-1)\n\n    # combine the inter- and intra-interval losses to get the total distortion loss\n    loss = inter_loss + intra_loss\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Calculate the inter-interval loss\n    inter_loss = (t[..., 1:] - t[..., :-1] - w).abs().sum(-1)\n\n    # Calculate the intra-interval loss\n    intra_loss = (w * w).sum(-1)\n\n    # Combine the inter-interval and intra-interval losses\n    loss = inter_loss + intra_loss\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t = torch.tensor([[1, 2, 3], [4, 5, 6]])\n    # w = torch.tensor([[0.1, 0.2], [0.3, 0.4]])\n    # t.shape[-1] = w.shape[-1] + 1\n    # t = torch.tensor([[1, 2, 3], [4, 5, 6]])\n    # w = torch.tensor([[0.1, 0.2], [0.3, 0.4]])\n    # t.shape[-1] = w.shape[-1] + 1\n    # t = torch.tensor([[1, 2, 3], [4, 5, 6]])\n    # w = torch.tensor([[0.1, 0.2], [0.3, 0.4]])\n    # t.shape[-1] = w.shape[-1] + 1\n    # t = torch.tensor([[1, 2, 3], [4, 5, 6]])\n    # w = torch.tensor([[0.1, 0.2], [0.3, 0.4]])\n    # t.shape[-1] = w.shape[-1] + 1\n    # t = torch.tensor([[1, 2, 3], [4, 5, 6]])\n    # w = torch.tensor([[0.1, 0.2], [0.3, 0.4]])\n    # t.shape[-1] = w.shape[-1] + 1\n    # t = torch.tensor([[1, 2, 3], [4, 5, 6]])\n    # w = torch.tensor([[0.1, 0.2], [0.3, 0.4]])\n    # t.shape[-1] = w.shape[-1] + 1\n    # t = torch.tensor([[1, "}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # calculate inter-interval loss\n    inter_loss = torch.sum((t[..., 1:] - t[..., :-1]) * w, dim=-1)\n\n    # calculate intra-interval loss\n    intra_loss = torch.sum(\n        torch.abs(t[..., 1:] - t[..., :-1] - (t[..., 1:] - t[..., :-1]) * w), dim=-1\n    )\n\n    # combine inter-interval and intra-interval losses\n    loss = inter_loss + intra_loss\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Inter-interval loss\n    inter_loss = torch.sum(w * torch.abs(t[..., 1:] - t[..., :-1]), dim=-1)\n\n    # Intra-interval loss\n    intra_loss = torch.sum(w * torch.abs(t[..., 1:] - t[..., :-1] - 1), dim=-1)\n\n    # Total loss\n    loss = inter_loss + intra_loss\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # inter-interval loss\n    loss_inter = torch.sum(w * (t[..., 1:] - t[..., :-1]) ** 2, dim=-1)\n\n    # intra-interval loss\n    loss_intra = torch.sum(w * (t[..., 1:] - 2 * t[..., 1:-1] + t[..., :-2]) ** 2, dim=-1)\n\n    # total loss\n    loss = loss_inter + loss_intra\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Calculate the inter-interval loss\n    inter_loss = torch.sum(torch.abs(t[..., 1:] - t[..., :-1]) * w, dim=-1)\n\n    # Calculate the intra-interval loss\n    intra_loss = torch.sum(torch.abs(t[..., 1:] - t[..., :-1]) * (1 - w), dim=-1)\n\n    # Combine the inter-interval and intra-interval losses\n    loss = inter_loss + intra_loss\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # check that the last dimension of t is one more than that of w\n    if t.shape[-1] != w.shape[-1] + 1:\n        raise ValueError(\"The last dimension of t should be one more than that of w.\")\n\n    # compute the inter-interval loss\n    inter_loss = torch.sum(w * torch.abs(t[:, 1:] - t[:, :-1]), dim=-1)\n\n    # compute the intra-interval loss\n    intra_loss = torch.sum(\n        w * torch.abs(t[:, 1:] - t[:, :-1] - (t[:, 1:] + t[:, :-1]) / 2), dim=-1\n    )\n\n    # combine the inter-interval and intra-interval losses to get the total distortion loss\n    loss = inter_loss + intra_loss\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # inter-interval loss\n    inter_loss = torch.sum(torch.abs(t[..., 1:] - t[..., :-1]) * w, dim=-1)\n    # intra-interval loss\n    intra_loss = torch.sum(torch.abs(t[..., 1:] - t[..., :-1] - 1) * w, dim=-1)\n    # total loss\n    loss = inter_loss + intra_loss\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Compute inter-interval losses\n    inter_loss = torch.sum(w * (t[..., 1:] - t[..., :-1]) ** 2, dim=-1)\n\n    # Compute intra-interval losses\n    intra_loss = torch.sum(w * (t[..., 1:] - t[..., :-1]) * (t[..., 1:] + t[..., :-1] - 2 * w), dim=-1)\n\n    # Combine inter- and intra-interval losses\n    loss = inter_loss + intra_loss\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # check if the last dimension of t is one more than that of w\n    if t.shape[-1] != w.shape[-1] + 1:\n        raise ValueError(\n            \"The last dimension of 't' should be one more than that of 'w'.\"\n        )\n\n    # calculate the inter-interval loss\n    inter_interval_loss = torch.sum(w * torch.abs(t[..., 1:] - t[..., :-1]))\n\n    # calculate the intra-interval loss\n    intra_interval_loss = torch.sum(\n        w * torch.abs(t[..., 1:] - t[..., :-1]) * (t[..., 1:] - t[..., :-1])\n    )\n\n    # combine the inter-interval and intra-interval losses\n    distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return distortion_loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # inter-interval loss\n    loss_inter = torch.sum(\n        torch.abs(t[:, 1:] - t[:, :-1]) * w, dim=-1\n    ) / torch.sum(w, dim=-1)\n\n    # intra-interval loss\n    loss_intra = torch.sum(\n        torch.abs(t[:, 1:] - t[:, :-1]) * torch.abs(t[:, 1:] - t[:, :-1]),\n        dim=-1,\n    ) / torch.sum(torch.abs(t[:, 1:] - t[:, :-1]) ** 2, dim=-1)\n\n    # combine both losses\n    loss = loss_inter + loss_intra\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # assert t.shape[-1] == w.shape[-1] + 1\n\n    # inter-interval loss\n    inter_loss = torch.sum(\n        torch.abs(t[..., 1:] - t[..., :-1]) * torch.abs(w[..., 1:] - w[..., :-1]),\n        dim=-1,\n    )\n\n    # intra-interval loss\n    intra_loss = torch.sum(\n        torch.abs(t[..., 1:] - t[..., :-1]) * torch.abs(w[..., 1:] + w[..., :-1]),\n        dim=-1,\n    )\n\n    # total distortion loss\n    distortion_loss = inter_loss + intra_loss\n\n    return distortion_loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # get the difference between the target tensor and its shifted version\n    diff = t[..., 1:] - t[..., :-1]\n\n    # calculate the inter-interval loss\n    inter_loss = (w * diff**2).sum(-1)\n\n    # calculate the intra-interval loss\n    intra_loss = (w**2 * diff**2).sum(-1)\n\n    # combine the inter-interval and intra-interval losses\n    loss = inter_loss + intra_loss\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t = [t_0, t_1, ..., t_n]\n    # w = [w_0, w_1, ..., w_n-1]\n    # loss = intra_interval_loss + inter_interval_loss\n\n    intra_interval_loss = torch.sum(\n        torch.abs(t[:, :, 1:] - t[:, :, :-1]) * w, dim=(-1, -2)\n    )\n    inter_interval_loss = torch.sum(\n        torch.abs(t[:, :, 1:] - t[:, :, :-1]), dim=(-1, -2)\n    )\n    return intra_interval_loss + inter_interval_loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Inter-interval loss\n    loss_inter = torch.sum(torch.abs(t[..., 1:] - t[..., :-1]) * w)\n\n    # Intra-interval loss\n    loss_intra = torch.sum(torch.abs(t[..., 1:] - t[..., :-1]) * (1 - w))\n\n    # Combine inter- and intra-interval losses\n    loss = loss_inter + loss_intra\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t: [B, T, D+1]\n    # w: [B, T, D]\n    B, T, D1 = t.shape\n    D = D1 - 1\n    # B: batch size\n    # T: number of time steps\n    # D: number of dimensions\n    # D1: number of dimensions + 1\n\n    # Inter-interval loss\n    # t[:, :-1, :-1] -> [B, T-1, D]\n    # t[:, :-1, 1:] -> [B, T-1, D]\n    # w[:, :-1, :] -> [B, T-1, D]\n    # torch.abs(t[:, :-1, :-1] - t[:, :-1, 1:]) -> [B, T-1, D]\n    # torch.abs(t[:, :-1, :-1] - t[:, :-1, 1:]) * w[:, :-1, :] -> [B, T-1, D]\n    # torch.sum(torch.abs(t[:, :-1, :-1] - t[:, :-1, 1:]) * w[:, :-1, :], dim=-1) -> [B, T-1]\n    # torch.sum(torch.abs(t[:, :-1, :-1] - t[:, :-1, 1:]) * w[:, :-1, :], dim=-1) / 2 -> [B, T-1]\n    # torch.sum(torch.abs(t[:, :-1, :-1] - t[:, :-1, 1:]) * w[:, :-1, :], dim=-1) / 2 * (t[:, 1:, :-1] - t[:, :-1, :-1]) -> [B, T-1, D]\n    # torch.sum(torch.abs"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Ensure that the tensors have the same shape\n    assert t.shape == w.shape\n\n    # Sort the tensors by the values in 't'\n    t_sorted, w_sorted = t.sort(dim=-1)\n\n    # Integrate the weights\n    w_integrated = torch.cumsum(w_sorted, dim=-1)\n\n    # Compute the weighted percentiles\n    w_percentiles = torch.zeros_like(w_integrated)\n    for i, p in enumerate(ps):\n        w_percentiles[..., i] = torch.interp(torch.tensor(p), w_integrated, t_sorted, left=0.0, right=1.0)\n\n    return w_percentiles"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check that the weights sum to 1\n    assert torch.allclose(w.sum(dim=-1), torch.tensor(1., device=w.device)), \"Weights must sum to 1\"\n\n    # Sort the input tensors by the values in 't'\n    t_sorted, w_sorted = t.sort(dim=-1)\n\n    # Compute the cumulative sum of the weights\n    w_cumsum = w_sorted.cumsum(dim=-1)\n\n    # Compute the percentiles of the cumulative sum using linear interpolation\n    ps = torch.tensor(ps, device=t.device)\n    ps_idx = torch.searchsorted(w_cumsum, ps.contiguous(), right=True)\n    ps_idx = torch.clamp(ps_idx, max=w_cumsum.shape[-1]-1)\n    w_cumsum_lo = w_cumsum.gather(-1, ps_idx)\n    w_cumsum_hi = w_cumsum.gather(-1, torch.clamp(ps_idx-1, min=0))\n    ps_lo = t_sorted.gather(-1, ps_idx)\n    ps_hi = t_sorted.gather(-1, torch.clamp(ps_idx-1, min=0))\n    weighted_percentiles = ps_lo + (ps - w_cumsum_lo) * (ps_hi - ps_lo) / (w_cumsum_hi - w_cumsum_lo)\n\n    return weighted_percentiles\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check that the tensors have the same shape\n    assert t.shape == w.shape, \"The tensors 't' and 'w' must have the same shape\"\n\n    # Check that the weights sum to 1\n    assert torch.isclose(w.sum(), torch.tensor(1.0)), \"The weights must sum to 1\"\n\n    # Sort the tensors by the values in 't'\n    t_sorted, w_sorted = t.sort(dim=-1)\n\n    # Integrate the weights\n    w_integrated = torch.cumsum(w_sorted, dim=-1)\n\n    # Interpolate the integrated weights to find the weighted percentiles\n    ps_tensor = torch.tensor(ps, dtype=t.dtype, device=t.device)\n    wp_tensor = torch.zeros_like(ps_tensor)\n    for i in range(len(ps)):\n        wp_tensor[i] = torch.interpolate(w_integrated, ps_tensor[i], dim=-1)\n\n    # Return the weighted percentiles\n    return wp_tensor"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check that the input tensors have the same shape\n    assert t.shape == w.shape, \"The input tensors 't' and 'w' must have the same shape.\"\n\n    # Check that the weights sum to 1\n    assert torch.isclose(torch.sum(w), torch.tensor(1.0)), \"The weights must sum to 1.\"\n\n    # Sort the values in 't' and the corresponding weights in 'w'\n    t, indices = torch.sort(t)\n    w = w[indices]\n\n    # Compute the cumulative weights\n    w_cumsum = torch.cumsum(w, dim=-1)\n\n    # Compute the percentile values\n    percentiles = torch.interp(torch.tensor(ps), w_cumsum, t)\n\n    return percentiles\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Ensure that the tensors have the same shape\n    assert t.shape == w.shape, \"t and w must have the same shape\"\n\n    # Sort the values and weights in ascending order\n    t_sorted, w_sorted = t.sort(dim=0)\n\n    # Integrate the weights\n    w_integrated = torch.cumsum(w_sorted, dim=0)\n\n    # Interpolate the integrated weights to find the percentiles\n    p_integrated = torch.interpolate(w_integrated.unsqueeze(0), torch.tensor(ps).unsqueeze(1), dim=1).squeeze(0)\n\n    # Find the indices of the percentiles in the sorted values\n    p_indices = torch.searchsorted(w_integrated, p_integrated)\n\n    # Use the indices to find the corresponding values\n    p_values = t_sorted.gather(0, p_indices.unsqueeze(0)).squeeze(0)\n\n    return p_values"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the weights sum to 1\n    assert torch.isclose(w.sum(), torch.tensor(1.0)), \"Weights do not sum to 1\"\n\n    # Check if the input tensors have the same shape\n    assert t.shape == w.shape, \"Input tensors must have the same shape\"\n\n    # Sort the values in 't' and the corresponding weights in 'w'\n    t, indices = torch.sort(t)\n    w = w[indices]\n\n    # Compute the cumulative sum of the weights\n    w_cumsum = torch.cumsum(w, dim=-1)\n\n    # Compute the weighted percentiles using linear interpolation\n    ps = torch.tensor(ps)\n    ps = torch.clamp(ps, 0, 1)\n    w_percentiles = torch.interp(ps, w_cumsum, t)\n\n    return w_percentiles"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check that the tensors have the same shape\n    assert t.shape == w.shape, \"The tensors 't' and 'w' must have the same shape.\"\n\n    # Check that the weights sum to 1\n    assert torch.allclose(w.sum(dim=-1), torch.tensor(1.0)), \"The weights must sum to 1.\"\n\n    # Sort the tensors by the values in 't'\n    t, w = t.sort(dim=-1)\n\n    # Integrate the weights\n    w_int = w.cumsum(dim=-1)\n\n    # Interpolate the integrated weights to find the weighted percentiles\n    ps_int = torch.tensor(ps)\n    w_ps = torch.interpolate(w_int.unsqueeze(0), ps_int.unsqueeze(1), dim=-1).squeeze(0)\n\n    # Find the indices of the nearest values in 't' to the weighted percentiles\n    t_ps = torch.searchsorted(t, w_ps)\n\n    # Compute the weighted percentiles\n    ps_t = torch.gather(t, -1, t_ps)\n    ps_w = torch.gather(w, -1, t_ps)\n    ps_wt = ps_t * ps_w\n\n    return ps_wt\n\n\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check that the tensors have the same shape\n    assert t.shape == w.shape, \"The tensors 't' and 'w' must have the same shape.\"\n\n    # Check that the weights sum to 1\n    assert torch.allclose(w.sum(dim=-1), torch.tensor(1.0)), \"The weights must sum to 1.\"\n\n    # Sort the tensors by the values in 't'\n    t_sorted, w_sorted = t.sort(dim=-1)\n\n    # Compute the cumulative sum of the weights\n    w_cumsum = w_sorted.cumsum(dim=-1)\n\n    # Compute the weighted percentiles\n    w_percentiles = torch.zeros_like(t_sorted)\n    for i, p in enumerate(ps):\n        # Find the index of the nearest weight to the percentile value\n        idx = torch.argmin(torch.abs(w_cumsum - p), dim=-1)\n        # Interpolate the nearest weights to the percentile value\n        w_percentiles[..., i] = torch.lerp(w_cumsum[..., idx], w_cumsum[..., idx + 1], p - w_cumsum[..., idx])\n\n    # Interpolate the values in 't' to the weighted percentiles\n    t_percentiles = torch.zeros_like(w_percentiles)\n    for i in range(t_sorted.shape[-1]):\n        t_percentiles[..., i] = torch.lerp(t_sorted[..., i], t_sorted[..., i + 1], (w_percentiles - w_cumsum[..., i]) / (w_cumsum[..., i + 1] - w_cumsum[..., i]))\n\n    return t_percentiles\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the weights sum to 1\n    assert torch.allclose(w.sum(dim=-1), torch.tensor(1.0)), \"The weights must sum to 1.\"\n\n    # Sort the input tensors by the values in 't'\n    t_sorted, sort_indices = t.sort(dim=-1)\n    w_sorted = w.gather(dim=-1, index=sort_indices)\n\n    # Compute the cumulative sum of the weights\n    w_cumsum = w_sorted.cumsum(dim=-1)\n\n    # Compute the weighted percentiles\n    ps = torch.tensor(ps)\n    weighted_percentiles = torch.zeros_like(w_cumsum)\n    for i, p in enumerate(ps):\n        mask = (w_cumsum >= p) & (w_cumsum <= p)\n        weighted_percentiles[..., i] = torch.masked_select(t_sorted, mask).mean(dim=-1)\n\n    # Interpolate the weighted percentiles\n    weighted_percentiles = torch.where(torch.isnan(weighted_percentiles), torch.zeros_like(weighted_percentiles), weighted_percentiles)\n    weighted_percentiles = torch.where(torch.isinf(weighted_percentiles), torch.zeros_like(weighted_percentiles), weighted_percentiles)\n    weighted_percentiles = weighted_percentiles.gather(dim=-1, index=sort_indices.argsort(dim=-1))\n\n    return weighted_percentiles"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check that the input tensors have the same shape\n    assert t.shape == w.shape, \"Input tensors must have the same shape\"\n\n    # Sort the input tensors by the values in 't'\n    t, w = t.sort(dim=-1)\n\n    # Compute the cumulative sum of the weights\n    w_cumsum = w.cumsum(dim=-1)\n\n    # Compute the total weight\n    w_total = w.sum(dim=-1, keepdim=True)\n\n    # Compute the weighted percentiles\n    w_percentiles = (w_cumsum - 0.5 * w) / w_total\n\n    # Interpolate the weighted percentiles to find the values at the given percentile values\n    ps_tensor = torch.tensor(ps, dtype=t.dtype, device=t.device)\n    ps_tensor = ps_tensor.view(-1, *(1,)*(len(t.shape)-1))\n    t_percentiles = torch.interpolate(t.unsqueeze(0), w_percentiles.unsqueeze(0), ps_tensor).squeeze(0)\n\n    return t_percentiles"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    assert t.shape == w.shape, \"t and w must have the same shape\"\n    assert abs(w.sum().item() - 1) < 1e-6, \"weights must sum to 1\"\n\n    # Sort the inputs by t\n    t, w = t.sort()\n\n    # Integrate the weights\n    w_int = w.cumsum(0)\n\n    # Interpolate the integrated weights to find the weighted percentiles\n    ps = torch.tensor(ps)\n    w_int_interp = torch.interpolate(w_int.unsqueeze(0), ps.unsqueeze(1), mode='linear').squeeze(0)\n\n    # Find the indices of the nearest values in t to the interpolated percentiles\n    t_interp = torch.searchsorted(t, w_int_interp)\n\n    # Compute the weighted percentiles by interpolating between the nearest values in t\n    t_interp_prev = t_interp.clamp(min=1) - 1\n    t_interp_next = t_interp.clamp(max=t.shape[0] - 1)\n    w_interp = (w_int_interp - w_int[t_interp_prev]) / (w_int[t_interp_next] - w_int[t_interp_prev])\n    w_interp = w_interp.clamp(min=0, max=1)\n    t_percentiles = t[t_interp_prev] + (t[t_interp_next] - t[t_interp_prev]) * w_interp\n\n    return t_percentiles\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Ensure that the tensors have the same shape\n    assert t.shape == w.shape, \"t and w must have the same shape\"\n\n    # Ensure that the weights sum to 1\n    assert torch.allclose(w.sum(dim=-1), torch.ones_like(w.sum(dim=-1))), \"weights must sum to 1\"\n\n    # Sort the tensors by t\n    t, w = t.sort(dim=-1)\n\n    # Compute the cumulative sum of the weights\n    w_cumsum = w.cumsum(dim=-1)\n\n    # Compute the percentiles of the cumulative sum of the weights\n    w_percentiles = torch.stack([torch.searchsorted(w_cumsum, p) for p in ps], dim=-1)\n\n    # Compute the weighted percentiles by interpolating between the nearest values in t\n    t_percentiles = torch.gather(t, -1, w_percentiles)\n    w_percentiles = torch.gather(w_cumsum, -1, w_percentiles)\n    w_percentiles = (w_percentiles - torch.roll(w_percentiles, 1, -1)) / (torch.roll(w_percentiles, -1, -1) - torch.roll(w_percentiles, 1, -1))\n    weighted_percentiles = torch.lerp(torch.roll(t_percentiles, 1, -1), t_percentiles, w_percentiles)\n\n    return weighted_percentiles\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the input tensors have the same shape\n    assert t.shape == w.shape, \"Input tensors 't' and 'w' must have the same shape\"\n\n    # Check if the weights sum to 1\n    assert torch.allclose(w.sum(dim=-1), torch.tensor(1.)), \"Weights 'w' must sum to 1\"\n\n    # Sort the values and weights\n    t, w = t.sort(dim=-1)\n\n    # Compute the cumulative weights\n    cw = w.cumsum(dim=-1)\n\n    # Compute the weighted percentiles\n    p = torch.searchsorted(cw, torch.tensor(ps).unsqueeze(-1), right=True)\n    p = p.clamp(max=cw.shape[-1] - 1)\n    p = torch.gather(t, dim=-1, index=p)\n\n    return p\n\n\n\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the weights sum to 1\n    assert torch.allclose(w.sum(dim=-1), torch.tensor(1., dtype=w.dtype, device=w.device)), \"Weights must sum to 1.\"\n\n    # Match up the channels of the tensors\n    t = t.unsqueeze(-1)\n    w = w.unsqueeze(-1)\n\n    # Sort the tensors by the values in 't'\n    t_sorted, w_sorted = t.sort(dim=-2)\n\n    # Integrate the weights\n    w_int = torch.cumsum(w_sorted, dim=-2)\n\n    # Interpolate the integrated weights to find the weighted percentiles\n    ps_tensor = torch.tensor(ps, dtype=t.dtype, device=t.device)\n    ps_tensor = ps_tensor.view(-1, 1, 1)\n    w_int_interp = torch.cat([torch.zeros_like(w_int[..., :1]), w_int], dim=-2)\n    w_int_interp = torch.cat([w_int_interp, torch.ones_like(w_int_interp[..., :1])], dim=-2)\n    t_int_interp = torch.cat([torch.zeros_like(t_sorted[..., :1]), t_sorted], dim=-2)\n    t_int_interp = torch.cat([t_int_interp, t_sorted[..., -1:]], dim=-2)\n    w_int_interp = (w_int_interp[..., 1:] - w_int_interp[..., :-1]) / (t_int_interp[..., 1:] - t_int_interp[..., :-1])\n    w_int_interp = torch.cat([torch.zeros_like(w_int_interp[..., :1]), w_int_interp"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check that the tensors have the same shape\n    assert t.shape == w.shape, \"t and w must have the same shape\"\n\n    # Check that the weights sum to 1\n    assert torch.allclose(w.sum(), torch.tensor(1.0)), \"w must sum to 1\"\n\n    # Check that the percentile values are between 0 and 1\n    assert all(0 <= p <= 1 for p in ps), \"Percentile values must be between 0 and 1\"\n\n    # Sort the values and weights in ascending order\n    t_sorted, w_sorted = t.sort(dim=-1)\n\n    # Compute the cumulative sum of the weights\n    w_cumsum = torch.cumsum(w_sorted, dim=-1)\n\n    # Compute the weighted percentiles\n    p_values = torch.zeros_like(ps)\n    for i, p in enumerate(ps):\n        # Find the index of the first weight greater than or equal to the percentile value\n        idx = torch.searchsorted(w_cumsum, p)\n\n        # Interpolate the value at the corresponding index\n        if idx == 0:\n            p_values[i] = t_sorted[..., 0]\n        elif idx == w_cumsum.shape[-1]:\n            p_values[i] = t_sorted[..., -1]\n        else:\n            w_left = w_cumsum[..., idx - 1]\n            w_right = w_cumsum[..., idx]\n            t_left = t_sorted[..., idx - 1]\n            t_right = t_sorted[..., idx]\n            p_values[i] = (w_left * t_right + (p - w_left) * t_left) / (w_left + w_right)\n\n    return p_values"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the tensors have the same shape\n    assert t.shape == w.shape, \"t and w must have the same shape\"\n\n    # Check if the weights sum to 1\n    assert torch.allclose(w.sum(dim=-1), torch.tensor(1.)), \"weights must sum to 1\"\n\n    # Sort the tensors in ascending order\n    t, w = t.sort(dim=-1)\n\n    # Integrate the weights\n    w_int = w.cumsum(dim=-1)\n\n    # Interpolate the integrated weights to find the weighted percentiles\n    p_int = torch.interpolate(w_int.unsqueeze(0), torch.tensor([0, 1]), torch.tensor(ps).unsqueeze(1)).squeeze(0)\n\n    # Find the indices of the corresponding values in the sorted tensors\n    idx = torch.searchsorted(w_int, p_int)\n\n    # Clamp the indices to ensure they are within the valid range\n    idx = torch.clamp(idx, 0, t.shape[-1] - 1)\n\n    # Use advanced indexing to select the corresponding values from the sorted tensors\n    p = t.gather(-1, idx)\n\n    return p"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check that the tensors have the same shape\n    assert t.shape == w.shape, \"The tensors 't' and 'w' must have the same shape.\"\n\n    # Check that the weights sum to 1\n    assert torch.allclose(w.sum(dim=-1), torch.ones_like(w.sum(dim=-1))), \"The weights must sum to 1.\"\n\n    # Sort the values and weights in ascending order\n    t_sorted, w_sorted = t.sort(dim=-1)\n\n    # Integrate the weights\n    w_integrated = torch.cumsum(w_sorted, dim=-1)\n\n    # Calculate the weighted percentiles\n    percentiles = []\n    for p in ps:\n        # Find the index of the first weight greater than or equal to p\n        idx = torch.searchsorted(w_integrated, p)\n\n        # Interpolate the value at the corresponding index\n        value = torch.where(idx > 0, t_sorted[..., idx - 1] + (t_sorted[..., idx] - t_sorted[..., idx - 1]) * (p - w_integrated[..., idx - 1]) / (w_integrated[..., idx] - w_integrated[..., idx - 1]), t_sorted[..., 0])\n\n        # Append the value to the list of percentiles\n        percentiles.append(value)\n\n    # Concatenate the percentiles into a tensor and return it\n    return torch.cat(percentiles, dim=-1)\n\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    assert t.shape == w.shape, \"t and w must have the same shape\"\n    assert torch.isclose(w.sum().item(), 1), \"weights must sum to 1\"\n\n    # sort t and w by t\n    t, idxs = torch.sort(t)\n    w = w[idxs]\n\n    # integrate w\n    w_int = torch.cumsum(w, dim=-1)\n\n    # find the percentile indices\n    ps_t = torch.tensor(ps)\n    ps_t = ps_t.view(-1, 1, 1)\n    w_int_t = w_int.view(1, -1, 1)\n    t_t = t.view(1, 1, -1)\n    idxs = torch.searchsorted(w_int_t, ps_t, right=True)\n    idxs = idxs.clamp(max=t.shape[-1] - 1)\n\n    # interpolate to find the weighted percentiles\n    t_below = t.gather(-1, idxs - 1)\n    t_above = t.gather(-1, idxs)\n    w_int_below = w_int.gather(-1, idxs - 1)\n    w_int_above = w_int.gather(-1, idxs)\n    w_below = w_int_below - w_int_t\n    w_above = w_int_above - w_int_t\n    w_below = w_below.clamp(min=0)\n    w_above = w_above.clamp(min=0)\n    w_norm = w_below + w_above\n    w_below = w_below / w_norm\n    w_above = w_above / w_norm\n    wp = w_below * t_below + w_above * t_above\n\n    return wp"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Sort the tensors by the values in 't'\n    t, sort_indices = t.sort(dim=0)\n    w = w[sort_indices]\n\n    # Integrate the weights\n    w_int = w.cumsum(dim=0)\n\n    # Interpolate the integrated weights to find the weighted percentiles\n    ps = torch.tensor(ps, device=t.device)\n    w_int_interp = torch.interpolate(w_int.unsqueeze(0).unsqueeze(0), ps.view(1, 1, -1), mode='linear').squeeze()\n\n    # Find the indices of the nearest values in 't' to the interpolated percentiles\n    t_indices = torch.searchsorted(t.flatten(), w_int_interp)\n\n    # Clamp the indices to the range of 't'\n    t_indices = torch.clamp(t_indices, 0, t.shape[0] - 1)\n\n    # Compute the weighted percentiles\n    w_percentiles = t.flatten()[t_indices]\n\n    return w_percentiles.view(t.shape)"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    assert t.shape == w.shape\n    assert torch.allclose(w.sum(dim=-1), torch.tensor(1.))\n\n    # Sort the values and weights in ascending order\n    t_sorted, indices = t.sort(dim=-1)\n    w_sorted = w.gather(dim=-1, index=indices)\n\n    # Compute the cumulative sum of the weights\n    w_cumsum = w_sorted.cumsum(dim=-1)\n\n    # Compute the weighted percentiles\n    ps = torch.tensor(ps)\n    ps_shape = [1] * len(t.shape) + [-1]\n    ps = ps.reshape(ps_shape)\n    w_percentiles = (w_cumsum - w_cumsum[..., :-1]) / w_cumsum[..., [-1]]\n\n    # Interpolate the weighted percentiles to find the weighted percentiles for the given percentile values\n    t_percentiles = torch.zeros_like(ps)\n    for i in range(t.shape[-1]):\n        mask = (ps >= w_percentiles[..., i:i+1]) & (ps < w_percentiles[..., i+1:i+2])\n        t_percentiles[mask] = t_sorted[..., i:i+1][mask]\n\n    return t_percentiles"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the PDF and CDF\n    pdf = w / torch.sum(w, -1, keepdim=True)\n    cdf = torch.cumsum(pdf, -1)\n    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], -1)\n\n    # Take uniform samples\n    if perturb:\n        u = torch.rand(list(cdf.shape[:-1]) + [num_samples])\n    else:\n        u = torch.linspace(0., 1., steps=num_samples)\n        u = u.expand(list(cdf.shape[:-1]) + [num_samples])\n\n    # Invert CDF\n    u = u.contiguous()\n    inds = torch.searchsorted(cdf, u, right=True)\n    below = torch.max(torch.zeros_like(inds - 1), inds - 1)\n    above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(inds), inds)\n    inds_g = torch.stack([below, above], -1)\n\n    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n    bins_g = torch.gather(t.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\n    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t = (u - cdf_g[..., 0]) / denom\n    samples = bins_g[..., 0] + t * (bins_g[...,"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the PDF and CDF\n    pdf = w / torch.sum(w, -1, keepdim=True)\n    cdf = torch.cumsum(pdf, -1)\n    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], -1)\n\n    # Take uniform samples\n    if perturb:\n        u = torch.rand(list(cdf.shape[:-1]) + [num_samples])\n    else:\n        u = torch.linspace(0., 1., steps=num_samples)\n        u = u.expand(list(cdf.shape[:-1]) + [num_samples])\n\n    # Invert CDF\n    u = u.contiguous()\n    inds = torch.searchsorted(cdf, u, right=True)\n    below = torch.max(torch.zeros_like(inds - 1), inds - 1)\n    above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(inds), inds)\n    inds_g = torch.stack([below, above], -1)\n\n    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n    bins_g = torch.gather(t.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\n    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t = (u - cdf_g[..., 0]) / denom\n    samples = bins_g[..., 0] + t * (bins_g[...,"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the PDF and CDF\n    pdf = w / torch.sum(w, -1, keepdim=True)\n    cdf = torch.cumsum(pdf, -1)\n    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], -1)\n\n    # Take uniform samples\n    u = torch.rand(list(cdf.shape[:-1]) + [num_samples])\n\n    # Invert CDF\n    u = u.contiguous()\n    inds = torch.searchsorted(cdf, u, right=True)\n    below = torch.max(torch.zeros_like(inds - 1), inds - 1)\n    above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(inds), inds)\n    inds_g = torch.stack([below, above], -1)\n\n    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n    bins_g = torch.gather(t.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\n    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t = (u - cdf_g[..., 0]) / denom\n    samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n\n    return samples"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Get the PDF\n    pdf = w / torch.sum(w, -1, keepdim=True)\n    cdf = torch.cumsum(pdf, -1)\n    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], -1)\n\n    # Take uniform samples\n    u = torch.rand(list(cdf.shape[:-1]) + [num_samples])\n\n    # Invert CDF\n    u = u.contiguous()\n    inds = torchsearchsorted.searchsorted(cdf, u, side='right')\n    below = torch.max(torch.zeros_like(inds - 1), inds - 1)\n    above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(inds), inds)\n    inds_g = torch.stack([below, above], -1)\n\n    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n    bins_g = torch.gather(t.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\n    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t = (u - cdf_g[..., 0]) / denom\n    samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n\n    return samples"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t = t.contiguous()\n    w = w.contiguous()\n\n    # Get PDF\n    pdf = w / torch.sum(w, -1, keepdim=True)\n    cdf = torch.cumsum(pdf, -1)\n    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], -1)\n\n    # Take uniform samples\n    if perturb:\n        u = torch.rand(list(cdf.shape[:-1]) + [num_samples], dtype=dtype, device=device)\n    else:\n        u = torch.linspace(0., 1., steps=num_samples, dtype=dtype, device=device)\n        u = u.expand(list(cdf.shape[:-1]) + [num_samples])\n\n    # Invert CDF\n    u = u.contiguous()\n    inds = torch.searchsorted(cdf, u, right=True)\n    below = torch.max(torch.zeros_like(inds - 1), inds - 1)\n    above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(inds), inds)\n    inds_g = torch.stack([below, above], -1)\n\n    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n    bins_g = torch.gather(t.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\n    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t = (u - c"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t = t.contiguous()\n    w = w.contiguous()\n\n    # Get PDF\n    pdf = w / torch.sum(w, -1, keepdim=True)\n    cdf = torch.cumsum(pdf, -1)\n    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], -1)  # (batch, len(bins))\n\n    # Take uniform samples\n    if perturb:\n        u = torch.rand(list(cdf.shape[:-1]) + [num_samples], device=w.device, dtype=w.dtype)\n    else:\n        u = torch.linspace(0., 1., steps=num_samples, device=w.device, dtype=w.dtype)\n        u = u.expand(list(cdf.shape[:-1]) + [num_samples])\n\n    # Invert CDF\n    u = u.contiguous()\n    inds = torch.searchsorted(cdf, u, right=True)\n    below = torch.max(torch.zeros_like(inds - 1), inds - 1)\n    above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(inds), inds)\n    inds_g = torch.stack([below, above], -1)  # (batch, N_samples, 2)\n\n    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n    bins_g = torch.gather(t.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\n    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n    denom = torch."}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # compute pdf\n    weights = w + 1e-5  # prevent nans\n    pdf = weights / torch.sum(weights, -1, keepdim=True)\n    cdf = torch.cumsum(pdf, -1)\n    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], -1)\n\n    # take uniform samples\n    if perturb:\n        u = torch.rand(list(cdf.shape[:-1]) + [num_samples])\n    else:\n        u = torch.linspace(0., 1., steps=num_samples)\n        u = u.expand(list(cdf.shape[:-1]) + [num_samples])\n\n    # invert CDF\n    u = u.contiguous()\n    inds = torch.searchsorted(cdf, u, right=True)\n    below = torch.max(torch.zeros_like(inds - 1), inds - 1)\n    above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(inds), inds)\n    inds_g = torch.stack([below, above], -1)\n\n    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n    bins_g = torch.gather(t.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\n    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t = (u - cdf_g[..., 0]) / denom\n    samples = bins_g[..., 0] +"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the PDF and CDF\n    pdf = w / torch.sum(w, -1, keepdim=True)\n    cdf = torch.cumsum(pdf, -1)\n    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], -1)\n\n    # Take uniform samples\n    if perturb:\n        u = torch.rand(list(cdf.shape[:-1]) + [num_samples], device=t.device, dtype=t.dtype)\n    else:\n        u = torch.linspace(0., 1., steps=num_samples, device=t.device, dtype=t.dtype)\n        u = u.expand(list(cdf.shape[:-1]) + [num_samples])\n\n    # Invert CDF\n    u = u.contiguous()\n    inds = torch.searchsorted(cdf, u, right=True)\n    below = torch.max(torch.zeros_like(inds-1), inds-1)\n    above = torch.min((cdf.shape[-1]-1) * torch.ones_like(inds), inds)\n    inds_g = torch.stack([below, above], -1)\n\n    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n    bins_g = torch.gather(t.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\n    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t = (u - cdf_g[..., 0]) / denom"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Get the PDF\n    pdf = weight_to_pdf(w, t)\n\n    # Get the CDF\n    cdf = weight_to_cdf(w, t)\n\n    # Perturb the sampling process to avoid sample clustering at bin boundaries\n    if perturb:\n        u = torch.rand(list(cdf.shape[:-1]) + [num_samples])\n    else:\n        u = torch.linspace(0, 1, num_samples)\n        if len(list(cdf.shape[:-1])) > 0:\n            u = u.unsqueeze(-1).expand(list(cdf.shape[:-1]) + [num_samples])\n\n    # Invert the CDF to get the corresponding t-values\n    u = u.contiguous()\n    inds = torchsearchsorted.searchsorted(cdf, u, side='right')\n    below = torch.max(torch.zeros_like(inds - 1), inds - 1)\n    above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(inds), inds)\n    inds_g = torch.stack([below, above], -1)\n\n    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n    bins_g = torch.gather(t.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\n    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t = (u - cdf_g[..., 0]) / denom\n    samples = bins_g[..., 0] + t *"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Get the PDF and CDF\n    pdf = weight_to_pdf(w, t)\n    c = torch.cumsum(pdf, -1)\n    c = torch.cat([torch.zeros_like(c[..., :1]), c], -1)\n\n    # Take uniform samples\n    u = torch.linspace(0., 1., steps=num_samples)\n    u = u.expand(list(cdf.shape[:-1]) + [num_samples])\n\n    # Invert CDF. This is a sampling step.\n    u = u.contiguous()\n    inds = torchsearchsorted.searchsorted(c, u, side='right')\n    below = torch.max(torch.zeros_like(inds-1), inds-1)\n    above = torch.min((cdf.shape[-1]-1) * torch.ones_like(inds), inds)\n    inds_g = torch.stack([below, above], -1)\n\n    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n    bins_g = torch.gather(t.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\n    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t = (u-cdf_g[..., 0])/denom\n    samples = bins_g[..., 0] + t * (bins_g[..., 1]-bins_g[..., 0])\n\n    return samples\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the CDF of the piecewise-constant PDF\n    cdf = torch.cumsum(w, dim=-1)\n    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1)\n    cdf = cdf / cdf[..., -1:]\n\n    # Generate uniform samples\n    u = torch.rand(list(cdf.shape[:-1]) + [num_samples], device=w.device)\n\n    # Invert the CDF to obtain the corresponding bin indices for each sample\n    inds = torch.searchsorted(cdf, u, right=True)\n\n    # Compute the bin boundaries for each sample\n    below = torch.max(torch.zeros_like(inds - 1), inds - 1)\n    above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(inds), inds)\n    inds_g = torch.stack([below, above], -1)\n\n    # Compute the bin endpoints for each sample\n    cdf_g = torch.gather(cdf, -1, inds_g)\n    bins_g = torch.gather(t, -1, inds_g)\n\n    # Compute the inverse CDF of the bin endpoints\n    denom = cdf_g[..., 1] - cdf_g[..., 0]\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t = (u - cdf_g[..., 0]) / denom\n\n    # Compute the samples\n    samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n\n    # Apply perturbation\n    if perturb:\n        offset = (torch.rand(list(bins_g.shape[:-1]) + [num_samples], device=w.device) -"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Get the bin size\n    t_diff = t[..., 1:] - t[..., :-1]\n\n    # Get the cumulative weights\n    w_cum = torch.cumsum(w, dim=-1)\n    w_cum = torch.cat([torch.zeros_like(w_cum[..., :1]), w_cum], dim=-1)\n\n    # Get the total weight\n    w_total = w_cum[..., -1:]\n\n    # Generate uniform samples\n    u = torch.rand(list(t.shape[:-1]) + [num_samples], device=t.device)\n\n    # Get the indices of the bins for each sample\n    idx = torch.searchsorted(w_cum, u, right=True) - 1\n\n    # Compute the bin start and end coordinates\n    t_start = t.gather(-1, idx)\n    t_end = t_start + t_diff.gather(-1, idx)\n\n    # Compute the normalized bin weights\n    w_start = w_cum.gather(-1, idx)\n    w_end = w_start + w.gather(-1, idx)\n\n    # Compute the normalized bin weights\n    w_norm = w_end - w_start\n\n    # Compute the normalized bin weights\n    w_norm = w_end - w_start\n\n    # Compute the normalized bin weights\n    w_norm = w_end - w_start\n\n    # Compute the normalized bin weights\n    w_norm = w_end - w_start\n\n    # Compute the normalized bin weights\n    w_norm = w_end - w_start\n\n    # Compute the normalized bin weights\n    w_norm = w_end - w_start\n\n    # Compute the normalized bin weights\n    w_norm = w_end - w_start\n\n    # Compute the normalized bin weights\n    w_norm = w_end - w_start\n\n    # Compute the normalized bin weights\n    w_norm = w_end - w_start"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Get the PDF\n    pdf = weight_to_pdf(w, t)\n\n    # Get the CDF\n    c = torch.cumsum(w, -1)\n    c = torch.cat([torch.zeros_like(c[..., :1]), c], -1)\n    c = c / c[..., -1:]\n\n    # Take uniform samples\n    u = torch.rand(list(cdf.shape[:-1]) + [num_samples])\n\n    # Invert CDF. This is a piecewise constant function on the domain of the CDF.\n    # PyTorch, and by extension JAX, does not have an inverse function.\n    # However, we can make a linear approximation by simply solving for the nearest bins.\n    below = torch.sum(u[..., None, :] >= c[..., :, None], dim=-2)\n    above = torch.sum(u[..., None, :] <= c[..., :, None], dim=-2)\n    i = torch.stack([below, above], -1)\n    c_g = torch.gather(c, -1, i)\n    t_g = torch.gather(t, -1, i)\n\n    # Linearly interpolate the samples.\n    # There is a degenerate case where a sample can be outside of the domain of the CDF.\n    # In that case, we simply use the corresponding sample from the nearest bin.\n    denom = (c_g[..., 1] - c_g[..., 0])\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t = torch.lerp(t_g[..., 0], t_g[..., 1], (u - c_g[..., 0]) / denom)\n\n    # Perturb and apply the bijector\n    if perturb:\n        t = t + (u - c_g[..., 0]) / denom\n\n    return t\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Determine PDF bounds and construct the CDF\n    t0, t1 = t[..., :-1], t[..., 1:]  # Compute bin start and end points\n    w = w[..., 1:-1]  # Trim the first and last weight to match the number of bins\n    bounds = torch.stack([t0, t1], dim=-1)  # Stack the start and end points to form the bounds tensor\n    cdf = torch.cumsum(w, dim=-1)  # Compute the cumulative distribution function (CDF)\n    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1)  # Prepend a zero to the CDF for the initial bin\n    cdf = cdf / cdf[..., -1:]  # Normalize the CDF to get the PDF\n\n    # Get PDF sample\n    u = torch.rand(list(cdf.shape[:-1]) + [num_samples], device=t.device, dtype=t.dtype)  # Generate uniform random samples\n    indices = torch.searchsorted(cdf, u, right=True)  # Find the indices where each random sample is located in the CDF\n    lower, upper = torch.maximum(indices - 1, torch.zeros_like(indices)), torch.minimum(indices, cdf.shape[-1] - 1)  # Compute the lower and upper indices for interpolation\n    indices_g = torch.stack([lower, upper], dim=-1)  # Stack the lower and upper indices to form the indices tensor\n    cdf_g = torch.gather(cdf, -1, indices_g)  # Gather the corresponding CDF values using the indices tensor\n    bins_g = torch.gather(bounds, -2, indices_g)  # Gather the corresponding bin boundaries using the indices tensor\n\n    denom = cdf_g[..., 1] - cdf_g[..., 0]  # Compute the denominator for interpol"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Determine PDF normalization factor.\n    w = w / torch.sum(w)\n\n    # Compute the CDF.\n    c = torch.cumsum(w, dim=-1)\n    c = torch.cat([torch.zeros_like(c[..., :1]), c], dim=-1)\n\n    # Draw uniform samples.\n    s = torch.linspace(0, 1, steps=num_samples)\n\n    # Invert the CDF.\n    u = torch.searchsorted(c, s)\n\n    # Convert samples to the domain of the PDF.\n    below = (u - 1).clamp(0)\n    above = u.clamp(max=c.shape[-1] - 1)\n    idx = torch.stack([below, above], dim=-1)\n\n    # Perform linear interpolation to find the value of the PDF at each sample.\n    denom = c[..., above] - c[..., below]\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t0 = (c[..., above] - s) / denom\n    t1 = (s - c[..., below]) / denom\n    w0 = t0 * w[..., below]\n    w1 = t1 * w[..., above]\n    w = w0 + w1\n\n    # Apply perturbation to the samples.\n    if perturb:\n        # Apply perturbation by sampling from a uniform distribution and adding it to the samples.\n        u = torch.rand(list(idx.shape[:-1]) + [num_samples], device=t.device, dtype=t.dtype)\n        idx = idx + u\n\n    # Apply jittering to the samples.\n    if not single_jitter:\n        jitter = torch.rand(list(idx.shape[:-1]) + [num_samples], device=t.device, dtype=t.dtype)\n    else:\n        jitter = torch.rand(("}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the cumulative sum of the weights\n    cdf = torch.cumsum(w, -1)\n\n    # Normalize the CDF to the range [0, 1]\n    cdf = cdf / cdf[..., -1:]\n\n    # Generate uniformly distributed samples in the range [0, 1]\n    u = torch.rand(list(cdf.shape[:-1]) + [num_samples], device=w.device)\n\n    # Invert the CDF to obtain the corresponding samples\n    u = u.contiguous()\n    indices = torch.searchsorted(cdf, u, right=True)\n\n    # Clamp the indices to the valid range\n    indices = torch.clamp(indices, 0, cdf.shape[-1] - 1)\n\n    # Convert the indices to the corresponding bin boundaries\n    below = torch.max(torch.zeros_like(indices - 1), indices - 1)\n    above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(indices), indices)\n    indices_g = torch.stack([below, above], -1)\n\n    # Sample uniformly within each bin\n    denom = cdf[..., above] - cdf[..., below]\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t_g = (u - cdf[..., below]) / denom\n\n    # Apply perturbation to the samples\n    if perturb:\n        # Sample a perturbation for each sample\n        omega = torch.rand(list(cdf.shape[:-1]) + [num_samples], device=w.device)\n\n        # Apply the perturbation to the samples\n        t_g = t_g + (omega - t_g) / num_samples\n\n    # Apply jitter to the samples\n    if not single_jitter:\n        # Sample jitter for each sample and dimension\n        jitter = torch.rand(list(cdf."}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the CDF and its total mass.\n    cdf, cdf_total = cumsum_trick(w)\n\n    # Compute the bin size.\n    bin_size = t[1:] - t[:-1]\n\n    # Generate uniform samples in [0, 1).\n    u = torch.rand(num_samples, device=t.device)\n\n    # Invert the CDF to obtain the corresponding t values for each sample.\n    above_indices = torch.searchsorted(cdf, u, side='right')\n    below_indices = torch.maximum(torch.zeros_like(above_indices), above_indices - 1)\n    indices = torch.stack((below_indices, above_indices), dim=-1)\n\n    # Compute the t values for each sample.\n    cdf_below = torch.gather(cdf, 0, below_indices)\n    cdf_above = torch.gather(cdf, 0, above_indices)\n    denom = cdf_above - cdf_below\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t_below = torch.gather(t, 0, below_indices)\n    t_above = torch.gather(t, 0, above_indices)\n    t_samples = (u - cdf_below) / denom * bin_size[below_indices] + t_below\n\n    # Apply perturbation and jitter to the samples.\n    if perturb:\n        t_samples = t_samples + (u - cdf_below) / denom * bin_size[below_indices]\n\n    if single_jitter:\n        t_samples = t_samples + (torch.rand((t.shape[0], num_samples), device=t.device) * (t[1] - t[0]) / num_samples)\n    else:\n        t_samples = t_samples +"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Get the shape of the input tensors\n    t_shape = t.shape\n    w_shape = w.shape\n\n    # Check if the input tensors have the correct shapes\n    if len(t_shape) != 1 or len(w_shape) != 1 or t_shape[0] != w_shape[0] - 1:\n        raise ValueError(\"Input tensors must have shape (n,) and (n+1,) respectively.\")\n\n    # Check if the bin endpoints are sorted\n    if not torch.all(t[1:] > t[:-1]):\n        raise ValueError(\"Bin endpoints must be sorted.\")\n\n    # Compute the CDF of the PDF\n    cdf = torch.cumsum(w, dim=0)\n    cdf = cdf / cdf[-1]\n\n    # Generate uniform samples in the range [0, 1)\n    u = torch.rand(num_samples, device=t.device)\n\n    # Invert the CDF to get the corresponding bin indices for each sample\n    indices = torch.searchsorted(cdf, u)\n\n    # Compute the bin boundaries for each sample\n    t_lower = t[indices]\n    t_upper = t[indices + 1]\n\n    # Generate uniform samples within each bin\n    u_within_bin = torch.rand(num_samples, device=t.device)\n\n    # Compute the samples from the PDF\n    samples = t_lower + (t_upper - t_lower) * u_within_bin\n\n    # Apply perturbation to avoid sample clustering at bin boundaries\n    if perturb:\n        u_perturb = torch.rand(num_samples, device=t.device)\n        samples += (u_perturb - 0.5) / num_samples\n\n    # Apply jitter to the samples\n    if single_jitter:\n        jitter = torch.rand(1, device=t.device) / num_samples\n        samples += jitter\n    else:\n        jitter = torch.rand(num_samples,"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Get the PDF by computing the weight of each bin.\n    weights = w / torch.sum(w, -1, keepdim=True)\n    # Compute the CDF by cumulatively summing the weights along the last dimension.\n    cdf = torch.cumsum(weights, -1)\n    # Ensure numerical stability by clamping the CDF to the range [0, 1].\n    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], -1)\n\n    # Calculate the bin size for each bin.\n    bin_size = t[..., 1:] - t[..., :-1]\n    # If perturb is True, apply perturbation to the sampling process.\n    if perturb:\n        # Generate uniform random samples in the range [0, 1) with the same shape as cdf.\n        u = torch.rand(list(cdf.shape[:-1]) + [num_samples], device=t.device, dtype=t.dtype)\n        # Use linear interpolation to estimate the inverse CDF (quantile function) at each sample point.\n        # The result is an estimate of the corresponding sample from the original PDF.\n        u = u.contiguous()\n        inds = torch.searchsorted(cdf, u, right=True)\n        # Clamp the indices to the valid range of bin indices.\n        below = torch.max(torch.zeros_like(inds - 1), inds - 1)\n        above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(inds), inds)\n        inds_g = torch.stack([below, above], -1)\n\n        matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n        # Compute the bin size for each bin using the indices.\n        cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, ind"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Get the PDF by approximating the CDF with the bin endpoints and weights\n    t_mid = .5 * (t[..., 1:] + t[..., :-1])\n    w = w[..., 1:-1]\n    pdf = torch.cat([w, w[..., -1:]], dim=-1)\n    cdf = torch.cumsum(w, dim=-1)\n    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1)\n    cdf = torch.cat([cdf, torch.ones_like(cdf[..., :1])], dim=-1)\n\n    # Draw uniform samples\n    u = torch.rand(list(cdf.shape[:-1]) + [num_samples], device=t.device)\n\n    # Invert CDF. This is a piecewise constant function with discrete intervals\n    # that linearly interpolates its values between bin endpoints.\n    # Technically this is the definition of a quantile function.\n    # Here we turn `u`, which is uniformly distributed, into a piecewise-constant\n    # random sample from the original distribution.\n    # `u` is in [0, 1).\n    inds = torch.searchsorted(cdf, u, right=True) - 1\n    # After `searchsorted`, indices are in [1, N_bins].\n    # Set the outliers (0 and N_bins) to the nearest endpoint value.\n    inds = torch.clamp(inds, 0, cdf.shape[-1] - 1)\n    # Convert to the matching range, since `inds` can be -1 or N_bins.\n    cdf_below = torch.gather(cdf, -1, inds)\n    cdf_above = torch.gather(cdf, -1, inds + 1)\n    t_below = torch.gather(t, -1, inds)\n    t_above = torch.gather(t, -1"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Calculate the dilated time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps within the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Adjust the weights corresponding to the dilated time steps\n    w_dilated = w / dilation\n\n    return dilated_t, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t_dilated = t * dilation\n    t_clipped = t_dilated.clamp(*domain)\n    w_adjusted = w * dilation\n    return t_clipped, w_adjusted\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps within the specified domain\n    clipped_dilated_t = torch.clip(dilated_t, domain[0], domain[1])\n\n    # Adjust the weights to match the dilated time steps\n    adjusted_w = w / dilation\n\n    return clipped_dilated_t, adjusted_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Calculate the dilated time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Adjust the weights to match the dilated time steps\n    dilated_w = w / dilation\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps\n    t_dilated = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    t_dilated_clipped = t_dilated.clip(*domain)\n\n    # Compute the difference between the original and dilated time steps\n    t_diff = t_dilated_clipped - t\n\n    # Adjust the weights based on the difference in time steps\n    w_adjusted = w * torch.exp(-t_diff)\n\n    return t_dilated_clipped, w_adjusted\n\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute dilated time steps\n    t_dilated = dilation * t\n\n    # Clip dilated time steps to the given domain\n    t_dilated = torch.clamp(t_dilated, min=domain[0], max=domain[1])\n\n    # Adjust weights to match dilated time steps\n    w_dilated = w / dilation\n\n    return t_dilated, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Calculate the dilated time steps\n    t_dilated = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    t_dilated = t_dilated.clamp(*domain)\n\n    # Adjust the weights to match the dilated time steps\n    w_dilated = w / dilation\n\n    return t_dilated, w_dilated\n\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Compute the dilated weights\n    dilated_w = w / dilation\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Check if dilation is not zero\n    if dilation != 0:\n        # Compute dilated time steps\n        t_dilated = t * dilation\n        # Clip dilated time steps to the specified domain\n        t_dilated = torch.clamp(t_dilated, min=domain[0], max=domain[1])\n        # Compute dilated weights\n        w_dilated = w / dilation\n        # Return dilated time steps and weights\n        return t_dilated, w_dilated\n    # If dilation is zero, return original time steps and weights\n    return t, w\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Calculate the dilated time steps\n    dilated_t = dilation * t\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Calculate the maximum weights for each dilated time step\n    max_weights = F.max_pool1d(w.unsqueeze(0).unsqueeze(0), kernel_size=2, stride=1, padding=1).squeeze()\n\n    # Adjust the weights to match the dilated time steps\n    adjusted_weights = w / max_weights[t.long()]\n\n    return dilated_t, adjusted_weights\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Check that the time steps and weights have the same shape\n    assert t.shape == w.shape\n\n    # Check that the time steps are non-negative\n    assert torch.all(t >= 0)\n\n    # Check that the dilation parameter is positive\n    assert dilation > 0\n\n    # Compute the dilated time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Compute the weights for the dilated time steps\n    dilated_w = w / dilation\n\n    # Return the dilated and clipped time steps and the adjusted weights\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Calculate the dilated time steps\n    t_dilated = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    t_dilated = torch.clamp(t_dilated, min=domain[0], max=domain[1])\n\n    # Adjust the weights corresponding to the dilated time steps\n    w_dilated = w / dilation\n\n    return t_dilated, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps by multiplying the time steps with the dilation factor\n    t_dilated = t * dilation\n\n    # Compute the dilated time steps by clipping them within the specified domain\n    t_dilated = torch.clamp(t_dilated, *domain)\n\n    # Compute the adjusted weights for the dilated time steps by dividing the original weights by the dilation factor\n    w_adjusted = w / dilation\n\n    return t_dilated, w_adjusted\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Calculate the dilated time steps by multiplying the time steps with the dilation factor\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps within the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Calculate the maximum value of the dilated time steps\n    max_t = dilated_t.max()\n\n    # Create a new tensor with values from 0 to max_t\n    new_t = torch.arange(0, max_t + 1)\n\n    # Calculate the weights for each time step in the new tensor\n    new_w = torch.max(dilated_t[None, :] <= new_t[:, None], dim=1)[0].float()\n\n    # Return the new time steps and weights\n    return new_t, new_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Calculate the dilated time steps\n    t_dilated = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    t_clipped = torch.clamp(t_dilated, min=domain[0], max=domain[1])\n\n    # Adjust the weights to match the dilated time steps\n    w_adjusted = w / dilation\n\n    return t_clipped, w_adjusted\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps\n    dilated_t = dilation * t\n\n    # Clip the dilated time steps within the specified domain\n    clipped_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Adjust the weights to match the dilated time steps\n    adjusted_w = w / dilation\n\n    return clipped_t, adjusted_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps\n    t_dilated = dilation * t\n\n    # Clip the dilated time steps to the specified domain\n    t_dilated = torch.clamp(t_dilated, min=domain[0], max=domain[1])\n\n    # Compute the maximum value of the dilated time steps\n    t_max = torch.max(t_dilated)\n\n    # Compute the dilated time steps using the maximum value\n    t_dilated_max = torch.arange(t_max + 1)\n\n    # Compute the dilated weights using the maximum value\n    w_dilated_max = torch.zeros_like(t_dilated_max, dtype=w.dtype)\n    w_dilated_max[t_dilated_max] = w[torch.floor(t_dilated_max / dilation).long()]\n\n    # Compute the dilated weights using the maximum value\n    w_dilated_max = torch.zeros_like(t_dilated_max, dtype=w.dtype)\n    w_dilated_max[t_dilated_max] = w[torch.floor(t_dilated_max / dilation).long()]\n\n    # Compute the dilated weights using the maximum value\n    w_dilated_max = torch.zeros_like(t_dilated_max, dtype=w.dtype)\n    w_dilated_max[t_dilated_max] = w[torch.floor(t_dilated_max / dilation).long()]\n\n    # Compute the dilated weights using the maximum value\n    w_dilated_max = torch.zeros_like(t_dilated_max, dtype=w.dtype)\n    w_dilated_max[t_dilated_max] = w[torch.floor(t_dilated_max / dilation).long()]\n\n    # Return the dilated and"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    clipped_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Compute the maximum value of the weights for each dilated time step\n    max_w, _ = scatter_max(w, clipped_t.long(), dim=-1)\n\n    # Compute the indices of the maximum values\n    max_indices = torch.arange(t.shape[-1], device=t.device).repeat(t.shape[0], 1)\n    max_indices = max_indices.masked_fill(~torch.eq(clipped_t.unsqueeze(-1), t.unsqueeze(0)), -1)\n    max_indices = max_indices.max(dim=-1).values\n\n    # Create a mask to select the maximum values\n    mask = torch.zeros_like(w, dtype=torch.bool)\n    mask[torch.arange(t.shape[0]), max_indices] = True\n\n    # Select the maximum values and return the dilated time steps and weights\n    return clipped_t[mask], max_w[mask]\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps\n    t_dilated = t / dilation\n\n    # Clip the dilated time steps to the specified domain\n    t_clipped = torch.clamp(t_dilated, min=domain[0], max=domain[1])\n\n    # Compute the maximum value of the dilated time steps\n    t_max = torch.max(t_clipped)\n\n    # Create a tensor of time steps from 0 to the maximum value\n    t_new = torch.arange(0, t_max + 1, dtype=torch.float32)\n\n    # Compute the weights for the new time steps\n    w_new = torch.max(w, dim=0)[0]\n\n    return t_new, w_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Calculate the dilated time steps by multiplying the time steps with the dilation factor\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    clipped_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Adjust the weights to match the dilated time steps\n    adjusted_w = w / dilation\n\n    return clipped_t, adjusted_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if tq is a tensor\n    if not isinstance(tq, torch.Tensor):\n        raise TypeError(\"tq must be a tensor.\")\n\n    # Check if t is a tensor\n    if not isinstance(t, torch.Tensor):\n        raise TypeError(\"t must be a tensor.\")\n\n    # Check if y is a tensor\n    if not isinstance(y, torch.Tensor):\n        raise TypeError(\"y must be a tensor.\")\n\n    # Check if tq, t, and y have the same batch size\n    if tq.shape[0] != t.shape[0] or tq.shape[0] != y.shape[0]:\n        raise ValueError(\"tq, t, and y must have the same batch size.\")\n\n    # Check if tq, t, and y have the same number of dimensions\n    if tq.ndim != t.ndim or tq.ndim != y.ndim:\n        raise ValueError(\"tq, t, and y must have the same number of dimensions.\")\n\n    # Check if tq, t, and y have the same shape\n    if tq.shape != t.shape or tq.shape != y.shape:\n        raise ValueError(\"tq, t, and y must have the same shape.\")\n\n    # Check if tq, t, and y have the same dtype\n    if tq.dtype != t.dtype or tq.dtype != y.dtype:\n        raise ValueError(\"tq, t, and y must have the same dtype.\")\n\n    # Check if tq is sorted\n    if not torch.all(tq[:, 1:] >= tq[:, :-1]):\n        raise ValueError(\"tq must be sorted.\")\n\n    # Check if t is sorted\n    if not torch.all(t[:, 1:] >= t[:, :-1]):\n        raise ValueError(\"t must be sorted.\")\n\n    # Check if y is sorted\n    if not torch.all(y[:, 1:] >= y[:, :-1"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if tq is a tensor\n    if not tq.requires_grad:\n        tq = tq.clone().detach()\n\n    # Check if t is a tensor\n    if not t.requires_grad:\n        t = t.clone().detach()\n\n    # Check if y is a tensor\n    if not y.requires_grad:\n        y = y.clone().detach()\n\n    # Check if tq is a 1D tensor\n    if len(tq.shape) != 1:\n        raise ValueError(\"tq must be a 1D tensor.\")\n\n    # Check if t is a 1D tensor\n    if len(t.shape) != 1:\n        raise ValueError(\"t must be a 1D tensor.\")\n\n    # Check if y is a 1D tensor\n    if len(y.shape) != 1:\n        raise ValueError(\"y must be a 1D tensor.\")\n\n    # Check if tq is sorted\n    if not torch.all(tq[:-1] <= tq[1:]):\n        raise ValueError(\"tq must be sorted in ascending order.\")\n\n    # Check if t is sorted\n    if not torch.all(t[:-1] <= t[1:]):\n        raise ValueError(\"t must be sorted in ascending order.\")\n\n    # Check if tq is within the range of t\n    if tq.min() < t.min() or tq.max() > t.max():\n        raise ValueError(\"tq must be within the range of t.\")\n\n    # Check if tq and t have the same number of elements\n    if tq.shape[0] != t.shape[0]:\n        raise ValueError(\"tq and t must have the same number of elements.\")\n\n    # Check if y has the same number of elements as t\n    if y.shape[0] != t.shape[0]:\n        raise ValueError(\"y must have the same number of elements as t.\")\n\n    # Find the index of the last element in t that is less than or equal to each element in tq\n   "}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if tq is a Tensor\n    if not isinstance(tq, torch.Tensor):\n        raise TypeError(\"tq must be a Tensor.\")\n\n    # Check if t is a Tensor\n    if not isinstance(t, torch.Tensor):\n        raise TypeError(\"t must be a Tensor.\")\n\n    # Check if y is a Tensor\n    if not isinstance(y, torch.Tensor):\n        raise TypeError(\"y must be a Tensor.\")\n\n    # Check if tq and t are of the same shape\n    if tq.shape != t.shape:\n        raise ValueError(\"tq and t must have the same shape.\")\n\n    # Check if y and t are of the same shape\n    if y.shape != t.shape:\n        raise ValueError(\"y and t must have the same shape.\")\n\n    # Check if tq is a 1D Tensor\n    if len(tq.shape) != 1:\n        raise ValueError(\"tq must be a 1D Tensor.\")\n\n    # Check if t is a 1D Tensor\n    if len(t.shape) != 1:\n        raise ValueError(\"t must be a 1D Tensor.\")\n\n    # Check if y is a 1D Tensor\n    if len(y.shape) != 1:\n        raise ValueError(\"y must be a 1D Tensor.\")\n\n    # Check if tq is sorted in ascending order\n    if not torch.all(tq[1:] >= tq[:-1]):\n        raise ValueError(\"tq must be sorted in ascending order.\")\n\n    # Check if t is sorted in ascending order\n    if not torch.all(t[1:] >= t[:-1]):\n        raise ValueError(\"t must be sorted in ascending order.\")\n\n    # Check if tq is within the range of t\n    if tq.min() < t.min() or tq.max() > t.max():\n        raise ValueError(\"tq must be within the range of t.\")\n\n    # Find the indices of the"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Import libraries\n    import tensorflow as tf\n\n    # Check that 'tq' is a tensor\n    if not tf.is_tensor(tq):\n        raise TypeError(\"'tq' must be a tensor.\")\n\n    # Check that 't' is a tensor\n    if not tf.is_tensor(t):\n        raise TypeError(\"'t' must be a tensor.\")\n\n    # Check that 'y' is a tensor\n    if not tf.is_tensor(y):\n        raise TypeError(\"'y' must be a tensor.\")\n\n    # Check that 'tq' and 't' have the same number of dimensions\n    if tq.shape.ndims != t.shape.ndims:\n        raise ValueError(\"'tq' and 't' must have the same number of dimensions.\")\n\n    # Check that 'tq' and 't' have the same shape\n    if tq.shape != t.shape:\n        raise ValueError(\"'tq' and 't' must have the same shape.\")\n\n    # Check that 'y' has one more dimension than 't'\n    if y.shape.ndims != t.shape.ndims + 1:\n        raise ValueError(\"'y' must have one more dimension than 't'.\")\n\n    # Check that 'y' has the same shape as 't' except for the last dimension\n    if y.shape[:-1] != t.shape:\n        raise ValueError(\"'y' must have the same shape as 't' except for the last dimension.\")\n\n    # Check that 't' is increasing\n    if not tf.reduce_all(tf.greater_equal(t[1:] - t[:-1], 0)):\n        raise ValueError(\"'t' must be increasing.\")\n\n    # Check that 'tq' is within the range of 't'\n    if not tf.reduce_all(tf.greater_equal(tq, t[0])) or not tf.reduce_all(tf.less_equal(tq, t[-1])):\n        raise ValueError(\"'tq' must"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if tq is a tensor\n    if not isinstance(tq, torch.Tensor):\n        raise TypeError(\"tq must be a tensor\")\n\n    # Check if t is a tensor\n    if not isinstance(t, torch.Tensor):\n        raise TypeError(\"t must be a tensor\")\n\n    # Check if y is a tensor\n    if not isinstance(y, torch.Tensor):\n        raise TypeError(\"y must be a tensor\")\n\n    # Check if tq and t are the same size\n    if tq.size() != t.size():\n        raise ValueError(\"tq and t must be the same size\")\n\n    # Check if y and t are the same size\n    if y.size() != t.size():\n        raise ValueError(\"y and t must be the same size\")\n\n    # Check if t is sorted\n    if not torch.all(t[:-1] < t[1:]):\n        raise ValueError(\"t must be sorted\")\n\n    # Check if tq is sorted\n    if not torch.all(tq[:-1] < tq[1:]):\n        raise ValueError(\"tq must be sorted\")\n\n    # Check if tq is within the range of t\n    if tq.min() < t.min() or tq.max() > t.max():\n        raise ValueError(\"tq must be within the range of t\")\n\n    # Check if tq is a scalar\n    if tq.dim() != 1:\n        raise ValueError(\"tq must be a scalar\")\n\n    # Check if t and y are 1D tensors\n    if t.dim() != 1 or y.dim() != 1:\n        raise ValueError(\"t and y must be 1D tensors\")\n\n    # Check if tq is a subset of t\n    if not torch.all(tq.unsqueeze(1) == t.unsqueeze(0)).any():\n        raise ValueError(\"tq must be a subset of t\")\n\n    # Check if tq is a subset of y\n    if not"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    tq = tq.unsqueeze(1)\n    t = t.unsqueeze(0)\n    y = y.unsqueeze(0)\n\n    # Interpolate the values at the query times\n    yq = y[:, :-1] + (tq - t[:, :-1]) * (y[:, 1:] - y[:, :-1]) / (t[:, 1:] - t[:, :-1])\n\n    # Set the values at exact step change times to the outside value\n    yq = yq.where((tq == t[:, :-1]) | (tq == t[:, 1:]), outside_value)\n\n    # Return the interpolated or outside values\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check that the input tensors have the same number of elements\n    assert t.shape == y.shape, \"t and y must have the same shape\"\n\n    # Find the indices of the step changes that are immediately before each query time\n    indices = torch.searchsorted(t, tq) - 1\n\n    # Clamp the indices to be within the valid range of the step function\n    indices = torch.clamp(indices, 0, len(t) - 1)\n\n    # Compute the interpolated values of the step function at the query times\n    yq = y[indices] + (tq - t[indices]) * (y[indices + 1] - y[indices]) / (t[indices + 1] - t[indices])\n\n    # Set the values of the step function at query times that exactly match a step change time to the outside value\n    yq[indices == indices.floor()] = outside_value\n\n    return yq\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if the query times are within the range of the step function\n    if tq.min() < t.min() or tq.max() > t.max():\n        raise ValueError(\"Query times must be within the range of the step function.\")\n\n    # Check if the step function is monotonically increasing\n    if not torch.all(torch.diff(t) > 0):\n        raise ValueError(\"Step function must be monotonically increasing.\")\n\n    # Find the indices of the step changes that occur before each query time\n    indices = torch.searchsorted(t, tq) - 1\n\n    # Clamp the indices to the valid range of the step function\n    indices = torch.clamp(indices, 0, len(t) - 1)\n\n    # Compute the interpolated values of the step function at the query times\n    yq = y[indices] + (y[indices + 1] - y[indices]) * (tq - t[indices]) / (t[indices + 1] - t[indices])\n\n    # Set the values of the step function at exact step change times to the outside value\n    yq[indices == indices[indices + 1]] = outside_value\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Find the index of the first time in t that is greater than or equal to each query time in tq\n    tq_ind = torch.searchsorted(t, tq)\n\n    # Set the outside value for query times that exactly match a step change time\n    yq = outside_value * torch.ones_like(tq)\n\n    # Interpolate the value of the step function for query times that are between step change times\n    mask = (tq_ind > 0) & (tq_ind < len(t))\n    yq[mask] = y[tq_ind[mask] - 1] + (y[tq_ind[mask]] - y[tq_ind[mask] - 1]) * (tq[mask] - t[tq_ind[mask] - 1]) / (t[tq_ind[mask]] - t[tq_ind[mask] - 1])\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check input dimensions\n    assert tq.ndim == 2 and t.ndim == 2 and y.ndim == 2\n\n    # Check input shapes\n    assert tq.shape[1] == 1 and t.shape[1] == 1 and y.shape[1] == 1\n\n    # Check input shapes\n    assert tq.shape[0] == t.shape[0] and t.shape[0] == y.shape[0]\n\n    # Check that t is sorted\n    assert (t[1:] >= t[:-1]).all()\n\n    # Check that tq is sorted\n    assert (tq[1:] >= tq[:-1]).all()\n\n    # Check that tq is within the range of t\n    assert (tq >= t.min()).all() and (tq <= t.max()).all()\n\n    # Initialize output tensor\n    yq = torch.zeros_like(tq)\n\n    # Find the index of the first time in t that is greater than or equal to each query time\n    idx = torch.searchsorted(t, tq, right=True)\n\n    # Set the output values to the corresponding values in y\n    yq = y[idx]\n\n    # Set the output values to the outside value if the query time exactly matches a step change time\n    yq[idx == t.shape[0]] = outside_value\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check that t and y have the same shape\n    assert t.shape == y.shape, \"t and y must have the same shape\"\n\n    # Check that t and y are 1D tensors\n    assert len(t.shape) == 1 and len(y.shape) == 1, \"t and y must be 1D tensors\"\n\n    # Check that t is sorted in ascending order\n    assert torch.all(t[1:] >= t[:-1]), \"t must be sorted in ascending order\"\n\n    # Check that tq is a 1D tensor\n    assert len(tq.shape) == 1, \"tq must be a 1D tensor\"\n\n    # Check that tq is within the range of t\n    assert tq.min() >= t.min() and tq.max() <= t.max(), \"tq must be within the range of t\"\n\n    # Find the index of the first time in t that is greater than or equal to each query time\n    idx = torch.searchsorted(t, tq)\n\n    # If the index is equal to the length of t, set it to the last index\n    idx[idx == len(t)] = len(t) - 1\n\n    # Compute the interpolated values using linear interpolation\n    yq = y[idx - 1] + (y[idx] - y[idx - 1]) * (tq - t[idx - 1]) / (t[idx] - t[idx - 1])\n\n    # Set the values at exact step change times to the outside value\n    yq[tq == t[idx]] = outside_value\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    import tensorflow as tf\n\n    y = tf.convert_to_tensor(y, dtype=tf.float32)\n    t = tf.convert_to_tensor(t, dtype=tf.float32)\n    tq = tf.convert_to_tensor(tq, dtype=tf.float32)\n    outside_value = tf.convert_to_tensor(outside_value, dtype=tf.float32)\n\n    y0 = tf.concat([[outside_value], y[:-1]], axis=0)\n    y1 = tf.concat([y[1:], [outside_value]], axis=0)\n    t0 = tf.concat([[t[0] - 1], t[:-1]], axis=0)\n    t1 = tf.concat([t[1:], [t[-1] + 1]], axis=0)\n\n    # Find the index of the first time that is greater than each query time\n    indices = tf.searchsorted(t1, tq)\n\n    # Compute the interpolated values using linear interpolation\n    yq = y0 + (y1 - y0) * (tq - t0) / (t1 - t0)\n\n    # Set the values outside the range to the outside value\n    yq = tf.where(tq < t[0], outside_value, yq)\n    yq = tf.where(tq > t[-1], outside_value, yq)\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Import TensorFlow and NumPy\n    import tensorflow as tf\n    import numpy as np\n\n    # Convert the input tensors to NumPy arrays\n    tq = tq.numpy()\n    t = t.numpy()\n    y = y.numpy()\n\n    # Find the indices of the step function changes\n    indices = np.searchsorted(t, tq, side='right') - 1\n\n    # Clip the indices to the range of the step function changes\n    indices = np.clip(indices, 0, len(t) - 1)\n\n    # Compute the interpolated values of the step function at the query times\n    yq = np.where(np.isclose(tq, t[indices]), y[indices], np.interp(tq, t, y))\n\n    # Set the values of the step function at query times that exactly match a step change time to the outside value\n    yq = np.where(np.isclose(tq, t), outside_value, yq)\n\n    # Convert the interpolated values to a TensorFlow tensor\n    yq = tf.convert_to_tensor(yq, dtype=tf.float32)\n\n    # Return the interpolated values of the step function at the query times\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Compute the difference between consecutive time values\n    t_diff = t[1:] - t[:-1]\n\n    # Create a mask to identify the time values that exactly match the query times\n    t_mask = tq.unsqueeze(1) == t[:-1]\n\n    # Create a mask to identify the time values that are smaller than the query times\n    t_smaller_mask = tq.unsqueeze(1) < t[:-1]\n\n    # Compute the interpolation weights\n    w = (tq.unsqueeze(1) - t[:-1]) / t_diff\n\n    # Compute the interpolated values\n    y_interp = (1 - w) * y[:-1] + w * y[1:]\n\n    # Set the interpolated values to the outside value for time values that exactly match the query times\n    y_interp[t_mask] = outside_value\n\n    # Set the interpolated values to the outside value for time values that are smaller than the query times\n    y_interp[t_smaller_mask] = outside_value\n\n    return y_interp"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check that t and y have the same shape\n    if t.shape != y.shape:\n        raise ValueError(\"t and y must have the same shape\")\n\n    # Check that t is sorted in ascending order\n    if not torch.all(t[1:] > t[:-1]):\n        raise ValueError(\"t must be sorted in ascending order\")\n\n    # Check that tq is within the range of t\n    if torch.any(tq < t[0]) or torch.any(tq > t[-1]):\n        raise ValueError(\"tq must be within the range of t\")\n\n    # Find the indices of the nearest time points to the query times\n    idx = torch.searchsorted(t, tq)\n\n    # If the query time matches a time point, return the corresponding value\n    mask = (tq == t[idx])\n    if torch.any(mask):\n        yq = torch.zeros_like(tq)\n        yq[mask] = y[idx[mask]]\n        return yq\n\n    # Otherwise, interpolate the value at the query time\n    yq = y[idx - 1] + (y[idx] - y[idx - 1]) * (tq - t[idx - 1]) / (t[idx] - t[idx - 1])\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    tq = tq.unsqueeze(-1)\n    t = t.unsqueeze(-1)\n    y = y.unsqueeze(-1)\n\n    # Find the index of the first time in t that is greater than or equal to each query time\n    indices = torch.sum(tq >= t, dim=-2)\n\n    # Clamp the indices to the range of t\n    indices = torch.clamp(indices, 0, t.shape[-2] - 1)\n\n    # Use the indices to look up the corresponding values in y\n    yq = y.gather(-2, indices)\n\n    # Set the values for query times that exactly match a step change time to the outside value\n    yq[indices == t.shape[-2]] = outside_value\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Get the number of steps and query times\n    num_steps = t.shape[0]\n    num_query_times = tq.shape[0]\n\n    # Create a tensor to store the interpolated values\n    yq = torch.zeros_like(tq)\n\n    # Loop over the query times\n    for i in range(num_query_times):\n        # Get the current query time\n        ti = tq[i]\n\n        # Find the index of the step function where the query time lies\n        j = torch.nonzero(t <= ti).max()\n\n        # If the query time lies outside the range of the step function, set the interpolated value to the outside value\n        if j == -1:\n            yq[i] = outside_value\n        else:\n            # If the query time lies within the range of the step function, interpolate the value\n            yq[i] = y[j]\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if the input tensors are 1D\n    if len(tq.shape) != 1 or len(t.shape) != 1 or len(y.shape) != 1:\n        raise ValueError(\"Input tensors must be 1D\")\n\n    # Check if the input tensors have the same length\n    if tq.shape[0] != t.shape[0] or tq.shape[0] != y.shape[0]:\n        raise ValueError(\"Input tensors must have the same length\")\n\n    # Check if the query times are within the range of the step function\n    if tq.min() < t.min() or tq.max() > t.max():\n        raise ValueError(\"Query times must be within the range of the step function\")\n\n    # Find the indices of the step function changes that are closest to the query times\n    indices = t.searchsorted(tq)\n    indices = indices.clip(1, len(t) - 1)\n\n    # Find the times and values of the closest step function changes\n    t_left = t[indices - 1]\n    t_right = t[indices]\n    y_left = y[indices - 1]\n    y_right = y[indices]\n\n    # Interpolate the values of the step function at the query times\n    yq = y_left + (y_right - y_left) * (tq - t_left) / (t_right - t_left)\n\n    # Set the values of the step function at the query times that exactly match a step change time to the outside value\n    yq = yq.where((tq == t_right) & (tq == t_left), outside_value)\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check that the input tensors have the same number of elements\n    assert t.shape == y.shape, \"t and y must have the same shape\"\n\n    # Find the index of the first time that is greater than or equal to each query time\n    idx = tf.searchsorted(t, tq, side='right')\n\n    # Create a mask that is True for indices where the query time matches a step change time\n    mask = tf.equal(tq, tf.gather(t, idx - 1))\n\n    # Use the mask to set the values of the step function at the query times that match a step change time to the outside value\n    yq = tf.where(mask, tf.fill(tq.shape, outside_value), tf.gather(y, idx - 1))\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if the input tensors have the same number of elements\n    if t.shape != y.shape:\n        raise ValueError(\"t and y must have the same shape\")\n\n    # Check if the query times are within the range of the step function times\n    if tq.min() < t.min() or tq.max() > t.max():\n        raise ValueError(\"Query times are outside the range of the step function times\")\n\n    # Find the indices of the step function times that are less than or equal to the query times\n    indices = torch.searchsorted(t, tq)\n\n    # Find the indices of the query times that are exactly equal to a step function time\n    exact_matches = torch.nonzero(tq == t[indices - 1])\n\n    # Set the values of the query times that exactly match a step function time to the outside value\n    yq = torch.full_like(tq, outside_value)\n    yq[exact_matches] = y[indices[exact_matches] - 1]\n\n    # Interpolate the values of the step function at the query times that are not exactly equal to a step function time\n    non_exact_matches = torch.nonzero(tq != t[indices - 1])\n    yq[non_exact_matches] = (y[indices[non_exact_matches]] - y[indices[non_exact_matches] - 1]) / (\n            t[indices[non_exact_matches]] - t[indices[non_exact_matches] - 1]) * (\n                                     tq[non_exact_matches] - t[indices[non_exact_matches] - 1]) + y[\n                                     indices[non_exact_matches] - 1]\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape = [..., w.shape[-1]]\n    # train_frac.shape = []\n    # anneal_slope.shape = []\n\n    # t.shape = [..., t.shape[-1]]\n    # w.shape = [..., w.shape[-1]]\n    # train_frac.shape = []\n    # anneal_slope.shape = []\n\n    # t.shape = [..., t.shape[-1]]\n    # w.shape = [..., w.shape[-1]]\n    # train_frac.shape = []\n    # anneal_slope.shape = []\n\n    # t.shape = [..., t.shape[-1]]\n    # w.shape = [..., w.shape[-1]]\n    # train_frac.shape = []\n    # anneal_slope.shape = []\n\n    # t.shape = [..., t.shape[-1]]\n    # w.shape = [..., w.shape[-1]]\n    # train_frac.shape = []\n    # anneal_slope.shape = []\n\n    # t.shape = [..., t.shape[-1]]\n    # w.shape = [..., w.shape[-1]]\n    # train_frac.shape = []\n    # anneal_slope.shape = []\n\n    # t.shape = [..., t.shape[-1]]\n    # w.shape = [..., w.shape[-1]]\n    # train_frac.shape = []\n    # anneal_slope.shape = []\n\n    # t.shape = [..., t.shape[-1]]\n    # w.shape = [..., w.shape[-1]]\n    # train_frac.shape = []\n    # anneal_slope.shape = []\n\n    # t.shape = [..., t.shape[-1]]\n    # w.shape = [..., w.shape[-1]]\n    # train_frac.shape = []\n    # anneal_"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # get the time intervals\n    t_intervals = t[..., 1:] - t[..., :-1]\n\n    # handle cases where adjacent intervals have zero distance\n    t_intervals = torch.where(t_intervals == 0, torch.ones_like(t_intervals) * eps, t_intervals)\n\n    # calculate the annealing effect on weights\n    anneal_effect = torch.sigmoid((train_frac - t[..., :-1]) * anneal_slope)\n\n    # adjust the weights based on the annealing effect\n    w_adjusted = w[..., :-1] * (1 - anneal_effect) + w[..., 1:] * anneal_effect\n\n    # handle cases where adjacent intervals have zero distance\n    w_adjusted = torch.where(t_intervals == 0, torch.zeros_like(w_adjusted), w_adjusted)\n\n    # prevent NaN values by using a softmax operation\n    w_adjusted = torch.nn.functional.softmax(w_adjusted, dim=-1)\n\n    return w_adjusted"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Calculate the annealing effect based on the training fraction and anneal slope\n    anneal_effect = torch.sigmoid(train_frac * anneal_slope)\n\n    # Calculate the distance between adjacent intervals\n    dists = t[..., 1:] - t[..., :-1]\n\n    # Set the distance between adjacent intervals to zero if they are too close\n    dists = torch.where(dists < eps, torch.zeros_like(dists), dists)\n\n    # Calculate the weight adjustment based on the annealing effect and distance\n    w_adj = anneal_effect * dists\n\n    # Set the weight adjustment for adjacent intervals with zero distance to zero\n    w_adj = torch.where(dists < eps, torch.zeros_like(w_adj), w_adj)\n\n    # Apply the weight adjustment to the weights tensor\n    w_adj = w_adj * w\n\n    # Apply a softmax operation to the adjusted weights tensor to ensure stability\n    w_adj = torch.softmax(w_adj, dim=-1)\n\n    return w_adj"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = w.shape[-1]\n    # train_frac: float\n    # anneal_slope: float = 10.0\n    # eps: torch.float32 = torch.finfo(torch.float32).eps ** 2\n\n    # Compute the annealing effect based on the training fraction and anneal slope\n    anneal_effect = torch.sigmoid((t - train_frac) * anneal_slope)\n\n    # Compute the distance between adjacent intervals\n    dist = t[..., 1:] - t[..., :-1]\n\n    # Handle cases where adjacent intervals have zero distance\n    dist = torch.where(dist == 0, torch.zeros_like(dist), dist)\n\n    # Compute the weights adjustment using the annealing effect and distance\n    w_adjust = anneal_effect[..., 1:] / (dist + eps)\n\n    # Set the weight adjustment for adjacent intervals with zero distance to zero\n    w_adjust = torch.where(torch.isnan(w_adjust), torch.zeros_like(w_adjust), w_adjust)\n\n    # Apply the weights adjustment to the weights tensor\n    w_adjusted = w + w_adjust\n\n    # Ensure stability in the computation by applying a softmax operation\n    w_adjusted = torch.softmax(w_adjusted, dim=-1)\n\n    return w_adjusted"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # get the fraction of training completed\n    train_frac = torch.as_tensor(train_frac, dtype=w.dtype, device=w.device)\n\n    # calculate the annealing effect on weights\n    w_anneal = torch.sigmoid(torch.clamp(t - train_frac, min=0) * anneal_slope)\n\n    # adjust the weights based on the annealing effect\n    w_adjusted = w * w_anneal.unsqueeze(-1)\n\n    # handle cases where adjacent intervals have zero distance\n    w_adjusted = torch.where(torch.isclose(t[..., 1:] - t[..., :-1], torch.tensor(0.0, device=w.device)),\n                             torch.zeros_like(w_adjusted),\n                             w_adjusted)\n\n    # prevent NaN values by using a softmax operation on the adjusted weights\n    w_adjusted = torch.softmax(w_adjusted, dim=-1)\n\n    return w_adjusted\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # get the index of the last element in the weights tensor\n    last_idx = w.shape[-1] - 1\n\n    # calculate the distance between each pair of adjacent elements in the weights tensor\n    dist = t[..., 1:] - t[..., :-1]\n\n    # handle cases where adjacent intervals have zero distance\n    dist = torch.where(dist == 0, torch.tensor(eps, device=dist.device), dist)\n\n    # calculate the annealing effect using Schlick's bias function\n    anneal_effect = torch.sigmoid(dist * anneal_slope)\n\n    # adjust the weights based on the annealing effect\n    w = w * anneal_effect\n\n    # set the weight of the last element to zero\n    w[..., last_idx] = 0\n\n    # normalize the weights using a softmax operation\n    w = torch.softmax(w, dim=-1)\n\n    # return the adjusted weights tensor\n    return w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Get the number of intervals\n    n_intervals = w.shape[-1] + 1\n\n    # Calculate the distance between each interval\n    dist = (t[..., 1:] - t[..., :-1])\n\n    # Handle the case where adjacent intervals have zero distance\n    dist = torch.where(dist == 0, torch.zeros_like(dist), dist)\n\n    # Calculate the bias for each interval\n    bias = torch.sigmoid((train_frac - t[..., :-1]) * anneal_slope) * torch.sigmoid((t[..., 1:] - train_frac) * anneal_slope)\n\n    # Calculate the adjusted weights for each interval\n    w_adj = torch.exp(torch.log(w + eps) + torch.log(bias + eps) - torch.log(dist + eps))\n\n    # Set the weight for the last interval to zero\n    w_adj = torch.cat([w_adj, torch.zeros_like(w_adj[..., :1])], dim=-1)\n\n    # Apply softmax to ensure the weights sum to 1\n    w_adj = torch.nn.functional.softmax(w_adj, dim=-1)\n\n    return w_adj\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # w.shape[-1] = t.shape[-1] - 1\n    t = t.unsqueeze(-1)\n    t_diff = t[..., 1:] - t[..., :-1]\n    t_diff = torch.where(t_diff == 0, torch.ones_like(t_diff) * eps, t_diff)\n    t_diff = torch.cat([t_diff, torch.ones_like(t_diff[..., :1]) * eps], dim=-1)\n    w = w.unsqueeze(-1)\n    w_diff = w[..., 1:] - w[..., :-1]\n    w_diff = torch.where(w_diff == 0, torch.ones_like(w_diff) * eps, w_diff)\n    w_diff = torch.cat([w_diff, torch.ones_like(w_diff[..., :1]) * eps], dim=-1)\n    t_alpha = torch.clamp(1.0 - torch.exp(torch.log(t_diff) * anneal_slope), min=0.0)\n    w_alpha = torch.clamp(1.0 - torch.exp(torch.log(w_diff) * anneal_slope), min=0.0)\n    t_alpha = torch.where(t_alpha == 0, torch.ones_like(t_alpha) * eps, t_alpha)\n    w_alpha = torch.where(w_alpha == 0, torch.ones_like(w_alpha) * eps, w_alpha)\n    t_alpha = torch.cat([t_alpha, torch.ones_like(t_alpha[..., :1]) * eps], dim=-1)\n    w_alpha = torch.cat([w_alpha, torch.ones_like(w_alpha[..., :1]) * eps], dim=-1)\n    t_alpha = torch.cumprod(t_alpha, dim"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    t = t.unsqueeze(-1)\n    w = w.unsqueeze(-1)\n    w_shape = w.shape\n    t_shape = t.shape\n    w = w.reshape(w_shape[0], -1)\n    t = t.reshape(t_shape[0], -1)\n    w_intervals = torch.abs(w[..., 1:] - w[..., :-1])\n    w_intervals = torch.where(w_intervals == 0, torch.full_like(w_intervals, eps), w_intervals)\n    w_intervals = torch.log(w_intervals)\n    w_intervals = torch.cat([w_intervals, torch.zeros_like(w_intervals[..., :1])], dim=-1)\n    w_intervals = torch.cumsum(w_intervals, dim=-1)\n    w_intervals = torch.exp(w_intervals)\n    w_intervals = w_intervals / torch.max(w_intervals, dim=-1, keepdim=True)[0]\n    w_intervals = torch.cat([torch.zeros_like(w_intervals[..., :1]), w_intervals], dim=-1)\n    w_intervals = w_intervals[..., 1:]\n    w_intervals = w_intervals / torch.max(w_intervals, dim=-1, keepdim=True)[0]\n    w_intervals = w_intervals * anneal_slope\n    w_intervals = torch.sigmoid(w_intervals - train_frac)\n    w_intervals = w_intervals / torch.max(w_intervals, dim=-1, keepdim=True)[0]\n    w_intervals = w_intervals.reshape(w_shape[:-1] + (w_shape[-1] - 1,"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t = t.to(w.device)\n    w = w.to(w.device)\n\n    # calculate the annealing effect using Schlick's bias function\n    anneal_effect = (t - train_frac) * anneal_slope\n    anneal_effect = torch.sigmoid(anneal_effect)\n\n    # calculate the distance between adjacent intervals\n    delta = torch.abs(t[..., 1:] - t[..., :-1])\n\n    # handle cases where adjacent intervals have zero distance\n    delta = torch.where(delta == 0, torch.zeros_like(delta), delta)\n\n    # calculate the adjusted weights using the annealing effect and distance\n    w_adj = w * (1 - anneal_effect) + w[..., 1:] * anneal_effect\n    w_adj = w_adj / (delta + eps)\n\n    # handle cases where adjacent intervals have zero distance\n    w_adj = torch.where(delta == 0, torch.zeros_like(w_adj), w_adj)\n\n    # ensure stability in the computation by preventing NaN values\n    w_adj = torch.where(torch.isnan(w_adj), torch.zeros_like(w_adj), w_adj)\n\n    # normalize the weights to sum to 1\n    w_adj = torch.softmax(w_adj, dim=-1)\n\n    return w_adj\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    assert t.shape[-1] == w.shape[-1] + 1, \"t.shape[-1] == w.shape[-1] + 1\"\n    # compute annealing weights\n    w_anneal = torch.sigmoid((w - train_frac) * anneal_slope)\n\n    # handle cases where adjacent intervals have zero distance\n    t_diff = t[..., 1:] - t[..., :-1]\n    t_diff_zero = t_diff == 0\n    if torch.any(t_diff_zero):\n        t_diff[t_diff_zero] = 1.0\n\n    # compute annealing intervals\n    t_anneal = torch.log(t[..., 1:] / t[..., :-1]) / t_diff\n\n    # apply annealing to weights\n    w_anneal = w_anneal * t_anneal\n\n    # handle cases where weight is NaN\n    w_anneal = torch.where(torch.isnan(w_anneal), torch.zeros_like(w_anneal), w_anneal)\n\n    # ensure stability in computation\n    w_anneal = torch.where(w_anneal < eps, eps, w_anneal)\n\n    # normalize weights\n    w_anneal = w_anneal / torch.sum(w_anneal, dim=-1, keepdim=True)\n\n    return w_anneal"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n    # t.shape[-1] = w.shape[-1] + 1\n\n    # anneal_slope = 10.0\n    # train_frac = 0.5\n\n    # Compute the annealing factor based on the training fraction and anneal slope\n    anneal_factor = torch.sigmoid(torch.tensor(train_frac * anneal_slope))\n\n    # Compute the intervals between time steps\n    intervals = t[..., 1:] - t[..., :-1]\n\n    # Handle cases where adjacent intervals have zero distance\n    intervals = torch.where(intervals == 0, torch.tensor(0.0), intervals)\n\n    # Compute the annealed weights based on the intervals\n    annealed_weights = torch.exp(-intervals / anneal_slope)\n\n    # Set the weight for the last interval to zero\n    annealed_weights = torch.cat([annealed_weights, torch.tensor(0.0)], dim=-1)\n\n    # Apply the annealing factor to the annealed weights\n    annealed_weights = anneal_factor * annealed_weights\n\n    # Normalize the annealed weights using a softmax operation\n    annealed_weights = torch.nn.functional.softmax(annealed_weights, dim=-1)\n\n    # Compute the sum of annealed weights\n    sum_annealed_weights = torch.sum(annealed_weights, dim=-1, keepdim=True)\n\n    # Handle cases where the sum of annealed weights is zero\n    sum_annealed_weights = torch.where(sum_annealed_weights == 0, torch.tensor(1.0), sum_annealed_weights)\n\n    # Normalize the annealed weights by dividing by the sum\n    annealed_weights ="}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    assert t.shape[-1] == w.shape[-1] + 1\n\n    # get the fraction of the training completed\n    train_frac = torch.as_tensor(train_frac, dtype=torch.float32)\n\n    # compute the annealing effect\n    anneal_effect = (train_frac + eps) / (1.0 + train_frac * (anneal_slope - 1.0) + eps)\n\n    # get the interval weights\n    interval_weights = w * anneal_effect\n\n    # get the interval widths\n    interval_widths = torch.diff(t, dim=-1)\n\n    # compute the interval widths\n    interval_widths = torch.diff(t, dim=-1)\n\n    # handle the case where adjacent intervals have zero distance\n    interval_widths = torch.where(interval_widths == 0, torch.zeros_like(interval_widths), interval_widths)\n\n    # compute the annealed weights\n    annealed_weights = interval_weights / interval_widths\n\n    # handle the case where adjacent intervals have zero distance\n    annealed_weights = torch.where(torch.isnan(annealed_weights), torch.zeros_like(annealed_weights), annealed_weights)\n\n    # compute the softmax of the annealed weights\n    annealed_weights = torch.nn.functional.softmax(annealed_weights, dim=-1)\n\n    return annealed_weights"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape = [..., w.shape[-1]]\n    # train_frac is a float between 0 and 1\n    # anneal_slope is a float\n    # eps is a small number\n\n    # Calculate the distance between adjacent intervals\n    dist = t[..., 1:] - t[..., :-1]\n\n    # Handle cases where adjacent intervals have zero distance\n    dist = torch.where(dist == 0, torch.zeros_like(dist), dist)\n\n    # Calculate the annealing effect based on the training fraction and anneal slope\n    anneal_effect = torch.sigmoid(train_frac / dist.clamp_min(eps) * anneal_slope)\n\n    # Adjust the weights based on the annealing effect\n    w_adjusted = w * (1 - anneal_effect)\n\n    # Ensure stability in the computation by setting weights of adjacent intervals with zero distance to zero\n    w_adjusted = torch.where(dist == 0, torch.zeros_like(w_adjusted), w_adjusted)\n\n    # Prevent NaN values by using a softmax operation on the adjusted weights\n    w_adjusted = torch.softmax(w_adjusted, dim=-1)\n\n    return w_adjusted\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # compute the annealing factor\n    anneal_factor = torch.clamp(train_frac - t + eps, min=0.0) / (1.0 - t + eps)\n    # compute the bias factor using Schlick's bias function\n    bias_factor = torch.sigmoid(torch.clamp(t - train_frac, min=0.0) * anneal_slope)\n    # compute the adjusted weights\n    w = w * (1.0 - bias_factor) + bias_factor * anneal_factor\n    # handle cases where adjacent intervals have zero distance\n    w = torch.where(torch.isclose(t[..., 1:] - t[..., :-1], torch.tensor(0.0, device=t.device)), torch.tensor(0.0, device=t.device), w)\n    # ensure stability in the computation\n    w = torch.where(torch.isnan(w), torch.tensor(0.0, device=w.device), w)\n    # apply softmax to ensure weights sum to 1\n    w = torch.softmax(w, dim=-1)\n    return w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape = [batch_size, n_layers, n_features]\n    # t.shape = [batch_size, n_layers, n_features + 1]\n\n    # compute the annealing effect\n    t = torch.sigmoid(t * anneal_slope)\n    t = t * (1 - eps) + eps\n    t = t * (1 / t.sum(-1, keepdim=True))\n\n    # compute the weights adjustment\n    w_delta = (w[..., :-1] / torch.max(w[..., :-1], dim=-1, keepdim=True).values)\n    w_delta = w_delta * (1 - eps)\n\n    # compute the adjusted weights\n    w_hat = torch.cat([w[..., :1], w_delta], dim=-1)\n    w_hat = w_hat * t\n    w_hat = w_hat + eps\n    w_hat = w_hat / w_hat.sum(-1, keepdim=True)\n\n    # compute the weight adjustment\n    w_delta = w_hat[..., 1:] - w_hat[..., :-1]\n    w_delta = w_delta * (1 - eps)\n    w_delta = torch.cat([w_delta, w_delta[..., -2:-1]], dim=-1)\n\n    # compute the final weights\n    w = w + w_delta * train_frac\n\n    return w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n\n    # Compute the distance between each pair of intervals in the time tensor.\n    # The distance is calculated as the difference between the current interval and the next interval.\n    # The resulting distance tensor has the same shape as the time tensor.\n    # The distance tensor is used to calculate the annealing effect on the weights.\n    d = t[..., 1:] - t[..., :-1]\n\n    # Compute the annealing effect based on the training fraction and anneal slope.\n    # The annealing effect is calculated using Schlick's bias function.\n    # The annealing effect is a value between 0 and 1, representing the strength of the annealing effect.\n    # The annealing effect is multiplied by the distance tensor to obtain the annealing effect for each interval.\n    # The annealing effect tensor has the same shape as the distance tensor.\n    # The annealing effect is clamped to ensure it is between 0 and 1.\n    # The annealing effect tensor is added to the distance tensor to ensure it is non-negative.\n    # The annealing effect tensor is clamped to ensure it is between 0 and 1.\n    # The annealing effect tensor is used to calculate the annealing effect on the weights.\n    a = torch.clamp(train_frac - torch.exp(-d * anneal_slope), min=0, max=1) + torch.clamp(d, min=0)\n\n    # Compute the adjusted weights based on the annealing effect.\n    # The adjusted weights are calculated by multiplying the weights tensor by the annealing effect tensor.\n    # The adjusted weights tensor has the same shape as the weights tensor.\n    # The adjusted weights tensor is used to adjust the weights of the model during training.\n    # The adjusted weights tensor is clamped to ensure it is between 0 and 1.\n    # The adjusted weights"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        return [to_cuda(b, device=device, ignore_list=ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        if ignore_list:\n            return {k: to_cuda(v, device=device, ignore_list=ignore_list) for k, v in batch.items() if k != \"meta\"}\n        else:\n            return {k: to_cuda(v, device=device, ignore_list=ignore_list) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(t, device, ignore_list) for t in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(l, device, ignore_list) for l in batch]\n    elif isinstance(batch, dict):\n        if ignore_list:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n        else:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n    elif isinstance(batch, (tuple, list)):\n        return [to_cuda(v, device, ignore_list) for v in batch]\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n    elif isinstance(batch, list):\n        return [to_cuda(x, device, ignore_list) for x in batch]\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(x, device, ignore_list) for x in batch)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n    elif isinstance(batch, list):\n        return [to_cuda(x, device, ignore_list) for x in batch]\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(x, device, ignore_list) for x in batch)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        return [to_cuda(t, device, ignore_list) for t in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n    elif isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, list):\n        if ignore_list:\n            return batch\n        else:\n            return [to_cuda(t, device) for t in batch]\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(t, device) for t in batch)\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device) if k != \"meta\" else v for k, v in batch.items()}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, list):\n        return [to_cuda(b, device) for b in batch]\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(b, device) for b in batch)\n    elif isinstance(batch, dict):\n        if ignore_list:\n            return {k: to_cuda(v, device) for k, v in batch.items()}\n        else:\n            return {k: to_cuda(v, device) for k, v in batch.items() if k != \"meta\"}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict):\n        if ignore_list:\n            return {key: to_cuda(value, device, ignore_list) for key, value in batch.items() if key != \"meta\"}\n        return {key: to_cuda(value, device, ignore_list) for key, value in batch.items()}\n    elif isinstance(batch, (tuple, list)):\n        return [to_cuda(item, device, ignore_list) for item in batch]\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(t, device) for t in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(l, device) for l in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device) for k, v in batch.items() if k != \"meta\"}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        return [to_cuda(b, device=device, ignore_list=ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device=device, ignore_list=ignore_list) for k, v in batch.items() if k != \"meta\"}\n    elif isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return batch\n        else:\n            return {key: to_cuda(value, device=device, ignore_list=ignore_list) for key, value in batch.items()}\n    elif isinstance(batch, list):\n        return [to_cuda(value, device=device, ignore_list=ignore_list) for value in batch]\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(value, device=device, ignore_list=ignore_list) for value in batch)\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device=device) for k, v in batch.items() if k != \"meta\"}\n    elif isinstance(batch, list):\n        return [to_cuda(v, device=device) for v in batch]\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(v, device=device) for v in batch)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, list):\n        return [to_cuda(b, device) for b in batch]\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(b, device) for b in batch)\n    elif isinstance(batch, dict):\n        if ignore_list:\n            return {k: to_cuda(v, device) for k, v in batch.items() if k != \"meta\"}\n        else:\n            return {k: to_cuda(v, device) for k, v in batch.items()}\n    else:\n        raise TypeError(f\"Unsupported type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n    elif isinstance(batch, (tuple, list)):\n        return [to_cuda(v, device, ignore_list) for v in batch]\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict):\n        return {key: to_cuda(value, device, ignore_list=ignore_list) for key, value in batch.items() if key != \"meta\"}\n    elif isinstance(batch, (tuple, list)):\n        if ignore_list:\n            return [to_cuda(item, device, ignore_list=ignore_list) for item in batch]\n        else:\n            return [to_cuda(item, device, ignore_list=ignore_list) for item in batch if not isinstance(item, list)]\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if ignore_list:\n        return batch\n    if isinstance(batch, (tuple, list)):\n        return [to_cuda(x, device=device, ignore_list=ignore_list) for x in batch]\n    if isinstance(batch, dict):\n        return {k: to_cuda(v, device=device, ignore_list=ignore_list) for k, v in batch.items() if k != \"meta\"}\n    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(t, device) for t in batch)\n    elif isinstance(batch, list):\n        if ignore_list:\n            return batch\n        else:\n            return [to_cuda(l, device) for l in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device) if k != \"meta\" else v for k, v in batch.items()}\n    else:\n        raise TypeError(f\"Unsupported type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        if batch.device != torch.device(device):\n            return batch.to(device)\n        else:\n            return batch\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n        else:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    elif isinstance(batch, (list, tuple)):\n        if ignore_list:\n            return [to_cuda(v, device, ignore_list) for v in batch]\n        else:\n            return [to_cuda(v, device, ignore_list) for v in batch if v is not None]\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, tuple):\n        return tuple(to_cuda(t, device) for t in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(t, device) for t in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list=ignore_list) for k, v in batch.items() if k != \"meta\"}\n    elif ignore_list:\n        return batch\n    else:\n        return batch.to(device)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if f.shape[dim] != v.shape[dim]:\n        f = f.expand(*v.shape[:dim], -1, -1)\n    f = f.reshape(*v.shape[:dim], -1, 3, 3)\n    return v.gather(dim, f)"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if f.dim() < v.dim():\n        f = f.expand(v.shape[:-2] + (-1, -1))\n    elif f.dim() > v.dim():\n        f = f.reshape(v.shape[:-2] + (-1, 3))\n\n    return v.gather(dim, f)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if f.ndim == v.ndim - 1:\n        f = f.unsqueeze(0).expand(v.shape[0], *f.shape)\n\n    v1 = v.unsqueeze(dim + 1).gather(dim, f[:, :, 0:1]).squeeze(dim + 1)\n    v2 = v.unsqueeze(dim + 1).gather(dim, f[:, :, 1:2]).squeeze(dim + 1)\n    v3 = v.unsqueeze(dim + 1).gather(dim, f[:, :, 2:3]).squeeze(dim + 1)\n\n    return torch.stack([v1, v2, v3], dim=-2)"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if f.ndim == v.ndim - 1:\n        f = f[None, ...].expand(v.shape[0], -1, -1)\n    elif f.ndim == v.ndim:\n        f = f.transpose(0, dim)\n    else:\n        raise ValueError(\"f must be of dimension %d or %d\" % (v.ndim - 1, v.ndim))\n\n    f = f.reshape(v.shape[0], -1, 3)\n    return v[:, f]\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if f.ndim == v.ndim:\n        return f\n    else:\n        assert f.ndim == v.ndim - 1\n        f = f.unsqueeze(0)\n        f = f.expand(v.shape[0], *f.shape[1:])\n        f = f.reshape(-1, *f.shape[2:])\n        return f\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if f.shape[dim] != v.shape[dim]:\n        f = f.expand(v.shape[dim], -1, -1)\n    tri = v.index_select(dim, f)\n    if len(f.shape) == len(v.shape):\n        return tri\n    else:\n        s = v.shape[:-2]\n        base = prod(s)\n        tri = tri.view(base, *s, -1, 3)\n        return tri\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor\n    if f.dim() < v.dim():\n        f = f[..., None, :, :].expand(v.shape[:-2] + (-1, -1, -1))\n\n    # gather the vertices using the faces tensor\n    v_g = v.gather(dim, f)\n\n    # compute the face normals\n    normals = torch.cross(v_g[..., 1, :] - v_g[..., 0, :], v_g[..., 2, :] - v_g[..., 0, :])\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    return normals.reshape(v.shape[:-2] + (-1, 3))\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust dimensions\n    if f.dim() > v.dim():\n        f = f.unsqueeze(0).expand(v.shape[0], *f.shape)\n    elif f.dim() < v.dim():\n        f = f.expand(*v.shape[:-2], *f.shape)\n\n    # gather vertices\n    v = v.gather(dim, f)\n\n    # reshape to original shape\n    v = v.reshape(*f.shape[:-1], *v.shape[2:])\n\n    return v\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the faces dimension to match the vertices dimension\n    if f.dim() == v.dim() - 1:\n        f = f.unsqueeze(0)\n    elif f.dim() == v.dim():\n        f = f.unsqueeze(-2)\n\n    # gather vertices\n    v0 = v.gather(dim, f[..., 0, :].unsqueeze(dim).expand_as(f))\n    v1 = v.gather(dim, f[..., 1, :].unsqueeze(dim).expand_as(f))\n    v2 = v.gather(dim, f[..., 2, :].unsqueeze(dim).expand_as(f))\n\n    # compute normals\n    n = torch.cross(v1 - v0, v2 - v0, dim=dim)\n\n    # reshape back to original faces tensor structure\n    new_shape = list(f.shape)\n    new_shape[dim] = 3\n    new_shape.insert(dim, -1)\n    n = n.view(new_shape)\n\n    return n\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust dimensions of f to match v\n    if f.dim() == v.dim() - 1:\n        f = f.unsqueeze(0)\n\n    # gather triangles\n    tris = v.index_select(dim, f.flatten()).reshape(\n        f.shape[0], f.shape[1], f.shape[2], v.shape[2]\n    )\n\n    return tris\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust dimensions\n    if len(f.shape) < len(v.shape):\n        f = f.unsqueeze(0)\n        f = f.expand(v.shape[0], *f.shape[1:])\n    elif len(f.shape) > len(v.shape):\n        f = f.view(*f.shape[:-1], *v.shape[2:])\n\n    # gather vertices\n    v0 = v.gather(dim, f[..., 0:1, :])\n    v1 = v.gather(dim, f[..., 1:2, :])\n    v2 = v.gather(dim, f[..., 2:3, :])\n\n    # compute normals\n    n = torch.cross(v1 - v0, v2 - v0, dim=-1)\n    n = n / torch.norm(n, dim=-1, keepdim=True)\n\n    # reshape to original faces shape\n    f_shape = f.shape[1:-1]\n    n = n.view(f.shape[0], *f_shape, 3)\n\n    return n\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # v: [B, N, 3]\n    # f: [F, 3]\n    # return: [B, F, 3, 3]\n\n    # adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor\n    if f.ndim < v.ndim:\n        f = f[None].expand(v.shape[0], *f.shape)\n\n    # gather triangles\n    triangles = torch.gather(v, dim, f)\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    triangles = triangles.reshape(*f.shape, 3, 3)\n\n    return triangles\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor\n    if f.ndim < v.ndim:\n        f = f.unsqueeze(0).expand(v.shape[:-2] + (-1, -1))\n    elif f.ndim > v.ndim:\n        f = f.view((-1,) + f.shape[-2:])\n\n    # gather the vertices for each face\n    v_f = v.gather(dim, f)\n\n    # compute the normals of the faces\n    n_f = torch.cross(v_f[..., 1, :] - v_f[..., 0, :], v_f[..., 2, :] - v_f[..., 0, :], dim=dim)\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    n_f = n_f.view(v.shape[:-2] + f.shape[-2:])\n\n    return n_f\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # check if faces tensor needs to be expanded to match the batch dimension of the vertices tensor\n    if f.shape[dim] != v.shape[dim]:\n        f = f.expand(v.shape[dim], *f.shape[1:])\n\n    # gather vertices using the faces tensor\n    tri = v.gather(dim, f)\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    tri = tri.reshape(*f.shape[:-1], *f.shape[-1:], *v.shape[1:])\n\n    return tri\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # if faces is a single tensor, expand it to match the batch dimension of vertices\n    if len(f.shape) == len(v.shape) - 1:\n        f = f.expand(v.shape[0], -1, -1)\n    # adjust the dimension of faces to match the batch dimension of vertices\n    elif f.shape[0] != v.shape[0]:\n        f = f.expand(v.shape[0], -1, -1)\n\n    # gather triangles\n    tris = torch.gather(v, dim, f)\n\n    # reshape the triangles tensor to maintain the structure of the original faces tensor\n    tris = tris.reshape(f.shape[0], f.shape[1], f.shape[2], v.shape[-1])\n\n    return tris\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor\n    if f.ndim == v.ndim - 1:\n        f = f.unsqueeze(0)\n    elif f.ndim == v.ndim:\n        f = f.unsqueeze(-2)\n\n    # gather the vertices forming each face\n    tri = v.gather(dim, f)\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    tri = tri.reshape(f.shape + tri.shape[-1:])\n\n    return tri\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor\n    if f.shape[dim] != v.shape[dim]:\n        f = f.expand(v.shape[dim], *f.shape)\n        f = f.transpose(dim, -3)\n\n    # gather the vertices for each face\n    f = f.reshape(-1, *f.shape[-3:])\n    v = v.transpose(dim, -2)\n    v = v.unsqueeze(-3)\n    v = v.expand(*v.shape[:-3], f.shape[-3], *v.shape[-2:])\n    f = f.unsqueeze(-1).expand(*f.shape[:-2], 3, f.shape[-1])\n    tris = torch.gather(v, -2, f)\n\n    # reshape the result to maintain the original faces tensor structure\n    tris = tris.transpose(-3, -2)\n    tris = tris.reshape(*f.shape[:-3], *tris.shape[-2:])\n    return tris\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor\n    if f.shape[0] != v.shape[0]:\n        f = f.expand(v.shape[0], *f.shape[1:])\n\n    # gather vertices for each face\n    tris = v.index_select(dim, f)\n\n    # reshape the gathered vertices to maintain the original faces tensor structure with additional dimensions for batch processing\n    tris = tris.permute(0, *range(2, tris.dim()), 1, 2)\n\n    return tris\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust dimensions of faces tensor if necessary\n    if f.ndim < v.ndim:\n        f = f.unsqueeze(0)\n        f = f.expand(v.shape[0], *f.shape[1:])\n\n    # gather vertices\n    tri = v.gather(dim, f)\n\n    # reshape result to maintain original faces tensor structure\n    tri = tri.reshape(*f.shape[:-1], *tri.shape[2:])\n\n    return tri\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if v.shape[0] != f.shape[0]:\n        f = f.expand(v.shape[0], -1, -1)\n\n    v = v.transpose(0, dim)\n    f = f.transpose(0, dim + 1)\n    f = f.transpose(1, dim + 2)\n    v = v.expand(f.shape)\n    tris = torch.gather(v, dim, f)\n    tris = tris.transpose(1, dim + 1)\n    tris = tris.transpose(0, dim)\n    tris = tris.reshape(f.shape[0], -1, f.shape[-1])\n    return tris\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return type(batch)([add_batch(item) for item in batch])\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise ValueError(f\"Unsupported data type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, tuple):\n        return tuple(add_batch(item) for item in batch)\n    elif isinstance(batch, list):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise ValueError(f\"Unsupported data type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, 0)\n    else:\n        raise ValueError(f\"Unsupported data type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return type(batch)([add_batch(item) for item in batch])\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise TypeError(f\"Unsupported data type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return type(batch)(add_batch(item) for item in batch)\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise ValueError(f\"Unsupported data type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, tuple):\n        return tuple(add_batch(x) for x in batch)\n    elif isinstance(batch, list):\n        return [add_batch(x) for x in batch]\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise ValueError(f\"Unsupported type: {type(batch)}\")"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return [add_batch(x) for x in batch]\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise ValueError(f\"Unsupported type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return type(batch)(add_batch(x) for x in batch)\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise ValueError(f\"Unsupported type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    elif isinstance(batch, list):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, tuple):\n        return tuple(add_batch(item) for item in batch)\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    else:\n        raise ValueError(f\"Unsupported batch type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise ValueError(f\"Unsupported data type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return type(batch)([add_batch(x) for x in batch])\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise TypeError(\n            f\"Unsupported type {type(batch)}. Supported types are tuple, list, dict, torch.Tensor, and np.ndarray.\"\n        )"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise ValueError(\"Unsupported data type\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, 0)\n    elif isinstance(batch, list):\n        return [add_batch(x) for x in batch]\n    elif isinstance(batch, tuple):\n        return tuple([add_batch(x) for x in batch])\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    else:\n        raise ValueError(f\"Unsupported type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return type(batch)([add_batch(x) for x in batch])\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, 0)\n    else:\n        raise ValueError(f\"Unsupported data type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    # If the input is a tuple, list, or dictionary, recursively add a dimension to each element\n    if isinstance(batch, (tuple, list)):\n        return type(batch)([add_batch(x) for x in batch])\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    # If the input is a PyTorch tensor, add a dimension at the zeroth position\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    # If the input is a NumPy array, add a dimension at the zeroth position\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    # If the input is neither a tuple, list, dictionary, PyTorch tensor, nor NumPy array, return the input as is\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return [add_batch(x) for x in batch]\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise ValueError(\"Unsupported data type\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, tuple):\n        return tuple(add_batch(b) for b in batch)\n    elif isinstance(batch, list):\n        return [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, 0)\n    else:\n        raise ValueError(f\"Unsupported type {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return type(batch)([add_batch(x) for x in batch])\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    else:\n        return batch[None]\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return type(batch)([add_batch(x) for x in batch])\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        for k, v in self.params.items():\n            batch[k] = torch.tensor(v).float()\n\n        batch.meta = dotdict()\n        for k, v in self.params.items():\n            batch.meta[k] = torch.tensor(v).float()\n\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        for k, v in self.__dict__.items():\n            if isinstance(v, torch.Tensor):\n                batch[k] = v.unsqueeze(0)\n            elif isinstance(v, list):\n                batch[k] = [v_.unsqueeze(0) for v_ in v]\n            elif isinstance(v, dict):\n                batch[k] = dotdict()\n                for k_, v_ in v.items():\n                    if isinstance(v_, torch.Tensor):\n                        batch[k][k_] = v_.unsqueeze(0)\n                    elif isinstance(v_, list):\n                        batch[k][k_] = [v__.unsqueeze(0) for v__ in v_]\n                    else:\n                        batch[k][k_] = v_\n            else:\n                batch[k] = v\n\n        batch.meta = dotdict()\n        for k, v in self.__dict__.items():\n            if isinstance(v, torch.Tensor):\n                batch.meta[k] = v.unsqueeze(0)\n            elif isinstance(v, list):\n                batch.meta[k] = [v_.unsqueeze(0) for v_ in v]\n            elif isinstance(v, dict):\n                batch.meta[k] = dotdict()\n                for k_, v_ in v.items():\n                    if isinstance(v_, torch.Tensor):\n                        batch.meta[k][k_] = v_.unsqueeze(0)\n                    elif isinstance(v_, list):\n                        batch.meta[k][k_] = [v__.unsqueeze(0) for v__ in v_]\n                    else:\n                        batch.meta[k][k_] = v_\n            else:\n                batch.meta[k] = v\n\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        for k, v in self.__dict__.items():\n            if isinstance(v, GUI):\n                batch[k] = v.to_batch()\n            elif isinstance(v, torch.Tensor):\n                batch[k] = v\n            elif isinstance(v, (int, float, bool, str)):\n                batch[k] = torch.tensor(v).reshape(1).to(self.device)\n            else:\n                batch[k] = v\n        batch.meta = dotdict()\n        for k, v in self.__dict__.items():\n            if isinstance(v, GUI):\n                batch.meta[k] = v.to_batch()\n            elif isinstance(v, torch.Tensor):\n                batch.meta[k] = v\n            elif isinstance(v, (int, float, bool, str)):\n                batch.meta[k] = torch.tensor(v).reshape(1).to(self.device)\n            else:\n                batch.meta[k] = v\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Convert camera parameters to tensors\n        cam_params = js['class_name'](\n            fx=torch.tensor(self.fx, dtype=torch.float32),\n            fy=torch.tensor(self.fy, dtype=torch.float32),\n            cx=torch.tensor(self.cx, dtype=torch.float32),\n            cy=torch.tensor(self.cy, dtype=torch.float32),\n            width=torch.tensor(self.width, dtype=torch.int32),\n            height=torch.tensor(self.height, dtype=torch.int32),\n            scale_factor=torch.tensor(self.scale_factor, dtype=torch.float32),\n            rotation=torch.tensor(self.rotation, dtype=torch.float32),\n            translation=torch.tensor(self.translation, dtype=torch.float32),\n            intrinsic=torch.tensor(self.intrinsic, dtype=torch.float32),\n            extrinsic=torch.tensor(self.extrinsic, dtype=torch.float32),\n            distortion=torch.tensor(self.distortion, dtype=torch.float32),\n            projection=torch.tensor(self.projection, dtype=torch.float32),\n            view=torch.tensor(self.view, dtype=torch.float32),\n            model=torch.tensor(self.model, dtype=torch.float32),\n            model_view=torch.tensor(self.model_view, dtype=torch.float32),\n            model_view_projection=torch.tensor(self.model_view_projection, dtype=torch.float32),\n            normal=torch.tensor(self.normal, dtype=torch.float32),\n            normal_view=torch.tensor(self.normal_view, dtype=torch.float32),\n            normal_view"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        for k, v in self.__dict__.items():\n            if isinstance(v, GUI):\n                batch[k] = v.to_tensor()\n            elif isinstance(v, (int, float, bool, str)):\n                batch[k] = v\n            elif isinstance(v, (list, np.ndarray)):\n                batch[k] = torch.Tensor(v)\n            elif isinstance(v, torch.Tensor):\n                batch[k] = v\n            else:\n                batch[k] = v\n\n        batch.meta = dotdict()\n        for k, v in self.__dict__.items():\n            if isinstance(v, GUI):\n                batch.meta[k] = v.to_tensor()\n            elif isinstance(v, (int, float, bool, str)):\n                batch.meta[k] = v\n            elif isinstance(v, (list, np.ndarray)):\n                batch.meta[k] = torch.Tensor(v)\n            elif isinstance(v, torch.Tensor):\n                batch.meta[k] = v\n            else:\n                batch.meta[k] = v\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        return dotdict({\n            'intrinsics': torch.tensor([\n                [self.focal_length, 0, self.principal_point[0]],\n                [0, self.focal_length, self.principal_point[1]],\n                [0, 0, 1]\n            ]),\n            'extrinsics': torch.tensor([\n                [self.rotation[0][0], self.rotation[0][1], self.rotation[0][2], self.translation[0]],\n                [self.rotation[1][0], self.rotation[1][1], self.rotation[1][2], self.translation[1]],\n                [self.rotation[2][0], self.rotation[2][1], self.rotation[2][2], self.translation[2]],\n                [0, 0, 0, 1]\n            ]),\n            'distortion': torch.tensor([\n                self.distortion[0], self.distortion[1], self.distortion[2], self.distortion[3], self.distortion[4]\n            ]),\n            'image_size': torch.tensor(self.image_size),\n            'image_center': torch.tensor(self.image_center),\n            'image_scale': torch.tensor(self.image_scale),\n            'meta': {\n                'intrinsics': torch.tensor([\n                    [self.focal_length, 0, self.principal_point[0]],\n                    [0, self.focal_length, self.principal_point[1]],\n                    [0, 0, 1]\n                ]),\n                'extrinsics': torch.tensor([\n                    [self.rotation[0][0], self.rotation[0][1], self.rotation[0][2], self.translation[0]],\n                    [self.rotation[1][0], self.rotation[1][1], self.rotation[1][2], self.translation[1]],\n                    [self.rot"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.meta = dotdict()\n        batch.meta.camera = self.camera\n        batch.meta.camera_type = self.camera_type\n        batch.meta.camera_name = self.camera_name\n        batch.meta.camera_distortion = self.camera_distortion\n        batch.meta.camera_intrinsics = self.camera_intrinsics\n        batch.meta.camera_extrinsics = self.camera_extrinsics\n        batch.meta.camera_resolution = self.camera_resolution\n        batch.meta.camera_center = self.camera_center\n        batch.meta.camera_focal = self.camera_focal\n        batch.meta.camera_fov = self.camera_fov\n        batch.meta.camera_fov_degree = self.camera_fov_degree\n        batch.meta.camera_fov_radian = self.camera_fov_radian\n        batch.meta.camera_fov_diagonal_degree = self.camera_fov_diagonal_degree\n        batch.meta.camera_fov_diagonal_radian = self.camera_fov_diagonal_radian\n        batch.meta.camera_fov_horizontal_degree = self.camera_fov_horizontal_degree\n        batch.meta.camera_fov_horizontal_radian = self.camera_fov_horizontal_radian\n        batch.meta.camera_fov_vertical_degree = self.camera_fov_vertical_degree\n        batch.meta.camera_fov_vertical_radian = self.camera_fov_vertical_radian\n        batch.meta.camera_fov_diagonal_ratio = self.camera_fov_diagonal_ratio\n        batch.meta.camera_fov_horizontal_ratio = self.camera_fov_horizontal_ratio\n        batch.meta.camera_fov_vertical_ratio = self.camera_fov"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.meta = dotdict()\n        batch.meta.camera = self.camera\n        batch.meta.camera_type = self.camera_type\n        batch.meta.camera_name = self.camera_name\n        batch.meta.camera_model = self.camera_model\n        batch.meta.camera_params = self.camera_params\n        batch.meta.camera_params_raw = self.camera_params_raw\n        batch.meta.camera_params_types = self.camera_params_types\n        batch.meta.camera_params_names = self.camera_params_names\n        batch.meta.camera_params_frozen = self.camera_params_frozen\n        batch.meta.camera_params_optim = self.camera_params_optim\n        batch.meta.camera_params_info = self.camera_params_info\n        batch.meta.camera_params_info_sensors = self.camera_params_info_sensors\n        batch.meta.camera_params_info_lenses = self.camera_params_info_lenses\n        batch.meta.camera_params_info_optics = self.camera_params_info_optics\n        batch.meta.camera_params_info_filters = self.camera_params_info_filters\n        batch.meta.camera_params_info_extra = self.camera_params_info_extra\n        batch.meta.camera_params_info_all = self.camera_params_info_all\n        batch.meta.camera_params_info_all_names = self.camera_params_info_all_names\n        batch.meta.camera_params_info_all_types = self.camera_params_info_all_types\n        batch.meta.camera_params_info_all_frozen = self.camera_params_info_all_frozen\n        batch.meta.camera_params_info_all_optim = self.camera_params"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        for k, v in self.__dict__.items():\n            if isinstance(v, (int, float, str)):\n                batch[k] = torch.tensor(v).view(1)\n            elif isinstance(v, (list, np.ndarray)):\n                batch[k] = torch.tensor(v).view(1, -1)\n            elif isinstance(v, torch.Tensor):\n                batch[k] = v.view(1, -1)\n            elif isinstance(v, dict):\n                batch[k] = dotdict()\n                for kk, vv in v.items():\n                    if isinstance(vv, (int, float, str)):\n                        batch[k][kk] = torch.tensor(vv).view(1)\n                    elif isinstance(vv, (list, np.ndarray)):\n                        batch[k][kk] = torch.tensor(vv).view(1, -1)\n                    elif isinstance(vv, torch.Tensor):\n                        batch[k][kk] = vv.view(1, -1)\n            else:\n                print(f'{k} is of type {type(v)} which is not supported')\n\n        batch.meta = dotdict()\n        for k, v in self.__dict__.items():\n            if isinstance(v, (int, float, str)):\n                batch.meta[k] = torch.tensor(v).view(1)\n            elif isinstance(v, (list, np.ndarray)):\n                batch.meta[k] = torch.tensor(v).view(1, -1)\n            elif isinstance(v, torch.Tensor):\n                batch.meta[k] = v.view(1, -1)\n            elif isinstance(v, dict):\n                batch.meta[k] = dotdict()\n                for kk, vv in v.items():\n                    if isinstance(vv, (int, float, str)):\n                        batch.meta[k][kk] = torch.tensor(vv).view(1)\n                    elif"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        for k, v in self.__dict__.items():\n            if k in ['image_size', 'intrinsics', 'extrinsics', 'distortion', 'crop_size', 'crop_offset', 'crop_aspect_ratio']:\n                batch[k] = torch.tensor(v).unsqueeze(0)\n            elif k in ['image_size_torch', 'intrinsics_torch', 'extrinsics_torch', 'distortion_torch', 'crop_size_torch', 'crop_offset_torch', 'crop_aspect_ratio_torch']:\n                batch[k] = v.unsqueeze(0)\n            elif k in ['image_size_np', 'intrinsics_np', 'extrinsics_np', 'distortion_np', 'crop_size_np', 'crop_offset_np', 'crop_aspect_ratio_np']:\n                batch[k] = torch.tensor(v).unsqueeze(0)\n            elif k in ['image_size_torch_no_grad', 'intrinsics_torch_no_grad', 'extrinsics_torch_no_grad', 'distortion_torch_no_grad', 'crop_size_torch_no_grad', 'crop_offset_torch_no_grad', 'crop_aspect_ratio_torch_no_grad']:\n                batch[k] = v.unsqueeze(0)\n            elif k in ['image_size_np_no_grad', 'intrinsics_np_no_grad', 'extrinsics_np_no_grad', 'distortion_np_no_grad', 'crop_size_np_no_grad', 'crop_offset_np_no_grad', 'crop_aspect_ratio_np_no_grad']:\n                batch[k] = torch.tensor(v).unsqueeze(0)\n            elif k in ['image_size_torch_grad', 'int"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Create an empty dotdict instance\n        batch = dotdict()\n\n        # Loop through all the items in the 'self' dictionary\n        for k, v in self.items():\n            # If the value is a dictionary, recursively call the 'to_batch' function on it\n            if isinstance(v, dict):\n                batch[k] = v.to_batch()\n            # If the value is a list, convert it into a tensor and add it to the batch\n            elif isinstance(v, list):\n                batch[k] = torch.tensor(v)\n            # If the value is a tuple, convert it into a tensor and add it to the batch\n            elif isinstance(v, tuple):\n                batch[k] = torch.tensor(v)\n            # If the value is a float, convert it into a tensor and add it to the batch\n            elif isinstance(v, float):\n                batch[k] = torch.tensor(v)\n            # If the value is an integer, convert it into a tensor and add it to the batch\n            elif isinstance(v, int):\n                batch[k] = torch.tensor(v)\n            # If the value is a boolean, convert it into a tensor and add it to the batch\n            elif isinstance(v, bool):\n                batch[k] = torch.tensor(v)\n            # If the value is a string, add it to the batch as is\n            elif isinstance(v, str):\n                batch[k] = v\n            # If the value is a numpy array, convert it into a tensor and add it to the batch\n            elif isinstance(v, np.ndarray):\n                batch[k] = torch.tensor(v)\n            # If the value is a PyTorch tensor, add it to the batch as is\n            elif isinstance(v, torch.Tensor):\n                batch[k] = v\n            # If the value is a tuple, convert it into a tensor and add it to the batch\n            elif isinstance(v, tuple):\n                batch[k] = torch.tensor(v)\n            # If the value is a list, convert it into a"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Create a dotdict instance to store the batch\n        batch = dotdict()\n\n        # Iterate over all the parameters in the camera\n        for key, val in self.params.items():\n            # If the parameter is a GUI related element, convert it to a tensor\n            if 'gui' in key:\n                batch[key] = torch.tensor(val).float()\n            # Otherwise, just copy the parameter as is\n            else:\n                batch[key] = val\n\n        # Create a nested 'meta' dictionary inside the batch\n        batch.meta = dotdict()\n\n        # Iterate over all the GUI related elements in the camera\n        for key, val in self.gui.items():\n            # If the GUI related element is a tensor, convert it to a tensor\n            if isinstance(val, torch.Tensor):\n                batch.meta[key] = val\n            # Otherwise, just copy the GUI related element as is\n            else:\n                batch.meta[key] = val\n\n        # Return the batch\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        for k, v in self.__dict__.items():\n            if k == 'meta':\n                continue\n            if isinstance(v, list):\n                batch[k] = torch.tensor(v).float()\n            else:\n                batch[k] = v\n        batch.meta = dotdict()\n        for k, v in self.meta.__dict__.items():\n            if isinstance(v, list):\n                batch.meta[k] = torch.tensor(v).float()\n            else:\n                batch.meta[k] = v\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        meta = dotdict()\n        meta.resolution = torch.tensor([self.resolution]).float()\n        meta.white_bkgd = self.white_bkgd\n        meta.near = self.near\n        meta.far = self.far\n        meta.fov = self.fov\n        meta.fov_axis = self.fov_axis\n        meta.do_ndc = self.do_ndc\n        meta.spherify = self.spherify\n        meta.spherify_projection = self.spherify_projection\n        meta.has_dof = self.has_dof\n        meta.aperture = self.aperture\n        meta.focus_dist = self.focus_dist\n        meta.gui_resolution = torch.tensor([self.gui_resolution]).float()\n        meta.gui_scale = self.gui_scale\n        meta.gui_subsampling = self.gui_subsampling\n        meta.gui_time = self.gui_time\n        meta.gui_w = self.gui_w\n        meta.gui_h = self.gui_h\n        meta.gui_on = self.gui_on\n        meta.gui_use_denoising = self.gui_use_denoising\n        meta.gui_light_intensity = self.gui_light_intensity\n        meta.gui_light_ambient = self.gui_light_ambient\n        meta.gui_bg_color = self.gui_bg_color\n        meta.gui_diffuse_color = self.gui_diffuse_color\n        meta.gui_specular_color = self.gui_specular_color\n        meta.gui_roughness = self.gui_roughness\n        meta.gui_metallic = self.gui_metallic\n        meta.gui_sheen = self.gui_sheen\n        meta.gui_clearcoat = self.gui_clearcoat\n        meta.gui_clearcoat_gloss = self.gui_clearcoat_gloss\n        meta.gui_subsurface = self.gui_subsurface\n        meta.gui_subsurface_color"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Initialize an empty dictionary to store the camera parameters\n        ret = dotdict()\n\n        # Iterate through the keys in the camera parameters\n        for k in self.keys():\n            # If the key is 'meta', create a nested 'meta' dictionary within the ret dictionary\n            if k == 'meta':\n                ret[k] = dotdict()\n                # Iterate through the keys in the 'meta' dictionary\n                for kk in self[k].keys():\n                    # Convert the values in the 'meta' dictionary to tensors and store them in the nested 'meta' dictionary of ret\n                    ret[k][kk] = torch.Tensor(self[k][kk])\n            # If the key is not 'meta', convert the values to tensors and store them in the ret dictionary\n            else:\n                ret[k] = torch.Tensor(self[k])\n\n        # Create a nested 'meta' dictionary within the ret dictionary\n        ret.meta = dotdict()\n        # Iterate through the keys in the 'meta' dictionary\n        for kk in self.meta.keys():\n            # Convert the values in the 'meta' dictionary to tensors and store them in the nested 'meta' dictionary of ret\n            ret.meta[kk] = torch.Tensor(self.meta[kk])\n\n        # Return the ret dictionary containing the camera parameters and GUI related elements in a structured format\n        return ret\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Create an empty dictionary to store the camera parameters\n        batch = dotdict()\n\n        # Iterate over the camera parameters and convert them into tensors\n        for k in js['camera_params']:\n            batch[k] = torch.tensor(getattr(self, k), dtype=torch.float32)\n\n        # Create a nested dictionary 'meta' to store the GUI related elements\n        batch.meta = dotdict()\n\n        # Iterate over the GUI related elements and convert them into tensors\n        for k in js['gui_elements']:\n            batch.meta[k] = torch.tensor(getattr(self, k), dtype=torch.float32)\n\n        # Return the structured dictionary containing all the camera parameters and GUI related elements\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        for k, v in self.__dict__.items():\n            if isinstance(v, (int, float)):\n                batch[k] = torch.tensor([v]).float()\n            elif isinstance(v, np.ndarray):\n                batch[k] = torch.tensor(v).float()\n            elif isinstance(v, torch.Tensor):\n                batch[k] = v.float()\n            elif k == 'meta':\n                batch[k] = dotdict()\n                for kk, vv in v.items():\n                    if isinstance(vv, (int, float)):\n                        batch[k][kk] = torch.tensor([vv]).float()\n                    elif isinstance(vv, np.ndarray):\n                        batch[k][kk] = torch.tensor(vv).float()\n                    elif isinstance(vv, torch.Tensor):\n                        batch[k][kk] = vv.float()\n            else:\n                batch[k] = v\n\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        for k, v in self.__dict__.items():\n            if k == 'meta':\n                batch[k] = v\n            else:\n                batch[k] = torch.tensor(v, dtype=torch.float32).unsqueeze(0)\n\n        batch.meta = dotdict(batch)\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Create a dotdict instance to store the camera parameters and GUI related elements\n        batch = dotdict()\n\n        # Loop through all the attributes of the camera instance\n        for k, v in self.__dict__.items():\n            # If the attribute is a GUI related element, skip it\n            if 'meta' in k:\n                continue\n\n            # If the attribute is a camera parameter, convert it into a tensor\n            if 'torch' in str(type(v)):\n                # If the attribute is a tensor, directly assign it to the batch\n                batch[k] = v\n            else:\n                # If the attribute is not a tensor, convert it into a tensor and assign it to the batch\n                batch[k] = torch.tensor(v).float()\n\n        # Create a nested 'meta' dictionary inside the batch to store the GUI related elements\n        batch.meta = dotdict()\n\n        # Loop through all the attributes of the camera instance again\n        for k, v in self.__dict__.items():\n            # If the attribute is a GUI related element, add it to the 'meta' dictionary\n            if 'meta' in k:\n                batch.meta[k.replace('meta_', '')] = v\n\n        # Return the batch containing the camera parameters and GUI related elements\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n\n        # Camera parameters\n        batch.intrinsics = torch.tensor(self.intrinsics).float()\n        batch.extrinsics = torch.tensor(self.extrinsics).float()\n        batch.distortion = torch.tensor(self.distortion).float()\n        batch.width = self.width\n        batch.height = self.height\n\n        # GUI related elements\n        batch.image = torch.tensor(self.image).float()\n        batch.fps = self.fps\n        batch.frame = self.frame\n        batch.time = self.time\n        batch.time_str = self.time_str\n        batch.camera_id = self.camera_id\n        batch.camera_name = self.camera_name\n        batch.camera_model = self.camera_model\n        batch.camera_maker = self.camera_maker\n        batch.camera_sensor_width = self.camera_sensor_width\n        batch.camera_sensor_height = self.camera_sensor_height\n        batch.camera_focal_length = self.camera_focal_length\n        batch.camera_lens_model = self.camera_lens_model\n        batch.camera_lens_maker = self.camera_lens_maker\n        batch.camera_lens_serial_number = self.camera_lens_serial_number\n\n        # Create a nested 'meta' dictionary with the same content as the main dictionary\n        batch.meta = dotdict()\n        for key, value in batch.items():\n            batch.meta[key] = value\n\n        return batch\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            state = agent.get_state()\n            serialized_state = self.serialize(state)\n            self.save(serialized_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.serialize()\n            self.persistence_manager.save(agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if not agent.is_working_agent() or agent.is_prime_agent():\n            return\n\n        agent_state = {\n            \"agent_id\": agent.agent_id,\n            \"agent_type\": agent.agent_type,\n            \"agent_name\": agent.agent_name,\n            \"agent_description\": agent.agent_description,\n            \"agent_version\": agent.agent_version,\n            \"agent_status\": agent.agent_status,\n            \"agent_start_time\": agent.agent_start_time,\n            \"agent_end_time\": agent.agent_end_time,\n            \"agent_last_updated_time\": agent.agent_last_updated_time,\n            \"agent_last_updated_by\": agent.agent_last_updated_by,\n            \"agent_created_time\": agent.agent_created_time,\n            \"agent_created_by\": agent.agent_created_by,\n            \"agent_tags\": agent.agent_tags,\n            \"agent_metadata\": agent.agent_metadata,\n            \"agent_data\": agent.agent_data,\n            \"agent_config\": agent.agent_config,\n            \"agent_config_schema\": agent.agent_config_schema,\n            \"agent_config_schema_version\": agent.agent_config_schema_version,\n            \"agent_config_schema_last_updated_time\": agent.agent_config_schema_last_updated_time,\n            \"agent_config_schema_last_updated_by\": agent.agent_config_schema_last_updated_by,\n            \"agent_config_schema_created_time\": agent.agent_config_schema_created_time,\n            \"agent_config_schema_created_by\": agent.agent_config_schema_created_by,\n            \"agent_config_schema_tags\": agent.agent_config_schema_tags,\n            \"agent_config_schema_metadata\": agent.agent_config_schema_metadata,\n            \"agent_config_schema_data\": agent.agent_config_schema_data,\n            \"agent_config_schema_config\": agent.agent_config_schema"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_agent_state()\n            serialized_agent_state = self.serialize_agent_state(agent_state)\n            self.save_serialized_agent_state(serialized_agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.serialize()\n            self.persistence_manager.save_agent_state(agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if not agent.is_working_agent or agent.is_prime_agent:\n            return\n\n        agent_dict = {\n            'id': agent.id,\n            'name': agent.name,\n            'state': agent.state,\n            'is_working_agent': agent.is_working_agent,\n            'is_prime_agent': agent.is_prime_agent,\n            'is_active': agent.is_active,\n            'is_deleted': agent.is_deleted,\n            'created_at': agent.created_at,\n            'updated_at': agent.updated_at\n        }\n\n        self.persistence_manager.save_agent(agent_dict)\n\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if not agent.is_working_agent():\n            return\n\n        if agent.is_prime_agent():\n            return\n\n        agent_state = {\n            'id': agent.id,\n            'name': agent.name,\n            'type': agent.type,\n            'status': agent.status,\n            'last_update': agent.last_update,\n            'last_error': agent.last_error,\n            'last_error_message': agent.last_error_message,\n            'last_error_timestamp': agent.last_error_timestamp,\n            'last_error_traceback': agent.last_error_traceback,\n            'last_error_count': agent.last_error_count,\n            'last_error_count_timestamp': agent.last_error_count_timestamp,\n            'last_error_count_message': agent.last_error_count_message,\n            'last_error_count_traceback': agent.last_error_count_traceback,\n            'last_error_count_error_code': agent.last_error_count_error_code,\n            'last_error_count_error_message': agent.last_error_count_error_message,\n            'last_error_count_error_traceback': agent.last_error_count_error_traceback,\n            'last_error_count_error_count': agent.last_error_count_error_count,\n            'last_error_count_error_count_timestamp': agent.last_error_count_error_count_timestamp,\n            'last_error_count_error_count_message': agent.last_error_count_error_count_message,\n            'last_error_count_error_count_traceback': agent.last_error_count_error_count_traceback,\n            'last_error_count_error_count_error_code': agent.last_error_count_error_count_error_code,\n            'last_error_count_error_count_error_message': agent.last_error_count_error_count_error_message,\n            'last_error_"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.serialize()\n            self.persistence_manager.save(agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            state = agent.get_state()\n            self.persistence_manager.save_state(state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            self.agent_persistence_manager.save_agent_state(agent_state)\n\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            agent_state_dict = {\n                'agent_id': agent.agent_id,\n                'agent_name': agent.agent_name,\n                'agent_type': agent.agent_type,\n                'agent_state': agent_state\n            }\n            self.persistence_manager.save_agent_state(agent_state_dict)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if not agent.is_working_agent():\n            return\n\n        if agent.is_prime_agent():\n            return\n\n        agent_state = agent.get_state()\n        agent_state_dict = agent_state.__dict__\n        self.agent_persistence_manager.save_agent_state(agent_state_dict)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if not agent.is_working_agent():\n            return\n\n        if agent.is_prime_agent():\n            return\n\n        state = agent.get_state()\n        state_dict = self.convert_agent_state_to_dict(state)\n        self.persistence_manager.save_agent_state(agent, state_dict)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if not agent.is_prime:\n            agent_state = agent.serialize()\n            self.persistence_manager.save(agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            self.agent_persistence_manager.save(agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = {\n                'agent_id': agent.agent_id,\n                'agent_name': agent.agent_name,\n                'agent_type': agent.agent_type,\n                'agent_status': agent.agent_status,\n                'agent_description': agent.agent_description,\n                'agent_version': agent.agent_version,\n                'agent_metadata': agent.agent_metadata,\n                'agent_config': agent.agent_config,\n                'agent_data': agent.agent_data,\n                'agent_state': agent.agent_state,\n                'agent_error': agent.agent_error,\n                'agent_log': agent.agent_log,\n                'agent_start_time': agent.agent_start_time,\n                'agent_end_time': agent.agent_end_time,\n                'agent_last_update_time': agent.agent_last_update_time,\n                'agent_last_update_status': agent.agent_last_update_status,\n                'agent_last_update_message': agent.agent_last_update_message,\n                'agent_last_update_error': agent.agent_last_update_error,\n                'agent_last_update_log': agent.agent_last_update_log\n            }\n            self.agent_persistence_manager.save_agent(agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if not agent.is_working_agent or agent.is_prime_agent:\n            return\n\n        agent_state = {\n            'id': agent.id,\n            'name': agent.name,\n            'age': agent.age,\n            'email': agent.email,\n            'phone': agent.phone,\n            'address': agent.address,\n            'license_number': agent.license_number,\n            'license_expiry_date': agent.license_expiry_date,\n            'license_type': agent.license_type,\n            'is_working_agent': agent.is_working_agent,\n            'is_prime_agent': agent.is_prime_agent,\n            'is_active': agent.is_active,\n            'created_at': agent.created_at,\n            'updated_at': agent.updated_at,\n            'created_by': agent.created_by,\n            'updated_by': agent.updated_by\n        }\n\n        self.persistence_manager.save_agent(agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            state = agent.get_state()\n            state_dict = {\n                'agent_type': agent.__class__.__name__,\n                'state': state\n            }\n            self.persistence_manager.save_agent_state(agent.agent_id, state_dict)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            agent_state_dict = {\n                'agent_type': agent.__class__.__name__,\n                'state': agent_state\n            }\n            self.persistence_manager.save_agent_state(agent_state_dict)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            state = agent.get_state()\n            state_dict = {\n                'state': state,\n                'type': agent.get_type(),\n                'name': agent.get_name()\n            }\n            self.persistence_manager.save_agent(state_dict)\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        highest_similarity = -float('inf')\n        closest_agent = None\n\n        for agent in self.agents:\n            similarity = cosine_similarity(purpose_embedding, agent.purpose_embedding)\n            if similarity > highest_similarity:\n                highest_similarity = similarity\n                closest_agent = agent\n\n        return closest_agent, highest_similarity"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        closest_agent = None\n        highest_similarity = -float('inf')\n\n        for agent in self.agents:\n            similarity = np.dot(agent.purpose_embedding, purpose_embedding) / (np.linalg.norm(agent.purpose_embedding) * np.linalg.norm(purpose_embedding))\n            if similarity > highest_similarity:\n                closest_agent = agent\n                highest_similarity = similarity\n\n        return closest_agent, highest_similarity\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Initialize the closest agent and similarity score\n        closest_agent = None\n        highest_similarity_score = -float('inf')\n\n        # Iterate through each agent in the agents list\n        for agent in self.agents:\n            # Calculate the cosine similarity between the agent's purpose embedding and the given purpose embedding\n            similarity_score = cosine_similarity(agent.purpose_embedding, purpose_embedding)\n\n            # If the similarity score is higher than the current highest similarity score, update the closest agent and similarity score\n            if similarity_score > highest_similarity_score:\n                closest_agent = agent\n                highest_similarity_score = similarity_score\n\n        # Return the closest agent and the highest similarity score\n        return closest_agent, highest_similarity_score\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            # Calculate the cosine similarity between the purpose embedding of each agent and the given purpose embedding\n            similarities = np.dot(self.purpose_embeddings, purpose_embedding) / (\n                    np.linalg.norm(self.purpose_embeddings, axis=1) * np.linalg.norm(purpose_embedding))\n\n            # Find the index of the agent with the highest similarity score\n            closest_agent_idx = np.argmax(similarities)\n\n            # Return the closest agent and its similarity score\n            return self.agents[closest_agent_idx], similarities[closest_agent_idx]\n\n        except Exception as e:\n            print(f\"Error in find_closest_agent: {e}\")\n            return None, -np.inf"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        closest_agent = None\n        highest_similarity = -math.inf\n\n        for agent in self.agents:\n            similarity = cosine_similarity(purpose_embedding, agent.purpose_embedding)\n\n            if similarity > highest_similarity:\n                closest_agent = agent\n                highest_similarity = similarity\n\n        return closest_agent, highest_similarity\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Initialize variables to store the closest agent and its similarity score\n        closest_agent = None\n        highest_similarity_score = -float('inf')\n\n        # Loop through each agent in the agents list\n        for agent in self.agents:\n            # Calculate the cosine similarity between the purpose embedding of the agent and the given purpose embedding\n            similarity_score = cosine_similarity(agent.purpose_embedding, purpose_embedding)\n\n            # If the similarity score is higher than the current highest similarity score, update the closest agent and the highest similarity score\n            if similarity_score > highest_similarity_score:\n                closest_agent = agent\n                highest_similarity_score = similarity_score\n\n        # Return the closest agent and its similarity score\n        return closest_agent, highest_similarity_score"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        max_similarity = -float('inf')\n        closest_agent = None\n\n        for agent in self.agents:\n            try:\n                similarity = cosine_similarity(purpose_embedding, agent.purpose_embedding)\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n            except Exception as e:\n                print(f\"Error comparing with agent {agent.name}: {e}\")\n                continue\n\n        return closest_agent, max_similarity\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if len(self.agents) == 0:\n            return None, -np.inf\n\n        max_similarity = -np.inf\n        closest_agent = None\n\n        for agent in self.agents:\n            similarity = np.dot(agent.purpose_embedding, purpose_embedding) / (np.linalg.norm(agent.purpose_embedding) * np.linalg.norm(purpose_embedding))\n\n            if similarity > max_similarity:\n                max_similarity = similarity\n                closest_agent = agent\n\n        return closest_agent, max_similarity\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if not self.agents:\n            return None, -np.inf\n\n        max_similarity = -np.inf\n        closest_agent = None\n\n        for agent in self.agents:\n            similarity = cosine_similarity(purpose_embedding, agent.purpose_embedding)\n\n            if similarity > max_similarity:\n                max_similarity = similarity\n                closest_agent = agent\n\n        return closest_agent, max_similarity\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Initialize variables to keep track of the closest agent and its similarity score\n        closest_agent = None\n        max_similarity = -np.inf\n\n        # Iterate over all agents in the agent_list\n        for agent in self.agent_list:\n            # Calculate the cosine similarity between the purpose embedding of the current agent and the given purpose embedding\n            similarity = cosine_similarity(agent.purpose_embedding, purpose_embedding)\n\n            # If the similarity is greater than the current maximum similarity, update the closest agent and its similarity score\n            if similarity > max_similarity:\n                closest_agent = agent\n                max_similarity = similarity\n\n        # Return the closest agent and its similarity score\n        return closest_agent, max_similarity\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if self.agents:\n            return max(\n                ((agent, cosine_similarity(purpose_embedding, agent.purpose_embedding)) for agent in self.agents),\n                key=lambda x: x[1],\n            )\n        else:\n            return None, -float(\"inf\")\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        highest_similarity = -float('inf')\n        closest_agent = None\n\n        for agent in self.agents:\n            try:\n                similarity = cosine_similarity(purpose_embedding.reshape(1, -1), agent.purpose_embedding.reshape(1, -1))[0][0]\n            except:\n                similarity = -float('inf')\n\n            if similarity > highest_similarity:\n                highest_similarity = similarity\n                closest_agent = agent\n\n        return closest_agent, highest_similarity\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        closest_agent = None\n        highest_similarity = -float(\"inf\")\n\n        for agent in self.agents:\n            similarity = cosine_similarity(agent.purpose_embedding, purpose_embedding)\n            if similarity > highest_similarity:\n                closest_agent = agent\n                highest_similarity = similarity\n\n        return closest_agent, highest_similarity\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Initialize variables to keep track of the closest agent and its similarity score\n        closest_agent = None\n        highest_similarity_score = -float('inf')\n\n        # Iterate over all agents in the class\n        for agent in self.agents:\n            # Calculate the cosine similarity between the purpose embedding of the agent and the given purpose embedding\n            similarity_score = cosine_similarity(agent.purpose_embedding, purpose_embedding)\n\n            # Check if the similarity score is higher than the current highest similarity score\n            if similarity_score > highest_similarity_score:\n                # Update the closest agent and its similarity score\n                closest_agent = agent\n                highest_similarity_score = similarity_score\n\n        # Return the closest agent and its similarity score\n        return closest_agent, highest_similarity_score\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_sim = -np.inf\n            closest_agent = None\n            for agent in self.agents:\n                sim = cosine_similarity(purpose_embedding, agent.purpose_embedding)\n                if sim > max_sim:\n                    max_sim = sim\n                    closest_agent = agent\n            return closest_agent, max_sim\n        except Exception as e:\n            print(f\"Error in find_closest_agent: {e}\")\n            return None, -np.inf\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if not self.agents:\n            return None, -np.inf\n        similarities = [\n            (agent, self.cosine_similarity(agent.purpose_embedding, purpose_embedding))\n            for agent in self.agents\n        ]\n        similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n        return similarities[0][0], similarities[0][1]\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Initialize variables to keep track of the closest agent and its similarity score\n        closest_agent = None\n        highest_similarity_score = -float('inf')\n\n        # Iterate over the agents in the class\n        for agent in self.agents:\n            # Calculate the cosine similarity between the agent's purpose embedding and the given purpose embedding\n            similarity_score = np.dot(agent.purpose_embedding, purpose_embedding) / (np.linalg.norm(agent.purpose_embedding) * np.linalg.norm(purpose_embedding))\n\n            # If the similarity score is higher than the current highest, update the closest agent and its similarity score\n            if similarity_score > highest_similarity_score:\n                closest_agent = agent\n                highest_similarity_score = similarity_score\n\n        # Return the closest agent and its similarity score\n        return closest_agent, highest_similarity_score\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            # Get the list of all agents\n            agents = self.get_agents()\n\n            # Initialize variables to store the closest agent and its similarity score\n            closest_agent = None\n            highest_similarity = -math.inf\n\n            # Iterate over the agents\n            for agent in agents:\n                # Get the purpose embedding of the current agent\n                agent_purpose_embedding = agent.get_purpose_embedding()\n\n                # Calculate the cosine similarity between the purpose embedding of the current agent and the given purpose embedding\n                similarity = cosine_similarity(purpose_embedding, agent_purpose_embedding)\n\n                # Update the closest agent and its similarity score if the current agent has a higher similarity score\n                if similarity > highest_similarity:\n                    closest_agent = agent\n                    highest_similarity = similarity\n\n            # Return the closest agent and its similarity score\n            return closest_agent, highest_similarity\n\n        except Exception as e:\n            # Handle any exceptions that may occur during the process\n            print(f\"Error occurred while finding the closest agent: {e}\")\n            return None, -math.inf\n\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Initialize the closest agent and the highest similarity score\n        closest_agent = None\n        highest_similarity_score = -float(\"inf\")\n\n        # Iterate through the agents in the agents dictionary\n        for agent_id, agent in self.agents.items():\n\n            # Calculate the cosine similarity between the purpose embedding of the agent and the given purpose embedding\n            similarity_score = cosine_similarity(purpose_embedding, agent.purpose_embedding)\n\n            # If the similarity score is higher than the current highest similarity score, update the closest agent and the highest similarity score\n            if similarity_score > highest_similarity_score:\n                closest_agent = agent\n                highest_similarity_score = similarity_score\n\n        # Return the closest agent and the highest similarity score\n        return closest_agent, highest_similarity_score\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Initialize variables\n        closest_agent = None\n        highest_similarity = -math.inf\n\n        # Iterate through agents\n        for agent in self.agents:\n\n            # Calculate cosine similarity between purpose_embedding and agent's purpose_embedding\n            similarity = cosine_similarity(purpose_embedding, agent.purpose_embedding)\n\n            # Update closest_agent and highest_similarity if necessary\n            if similarity > highest_similarity:\n                closest_agent = agent\n                highest_similarity = similarity\n\n        # Return closest agent and highest similarity\n        return closest_agent, highest_similarity\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a prime agent. You are a helpful and knowledgeable agent. You are here to answer questions and provide information to the user. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to help the user with whatever they need. You are here to"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a prime agent. You are a helpful and knowledgeable assistant. You are a helpful and knowledgeable assistant. You are a helpful and knowledgeable assistant.\",\n            name=\"Prime Agent\",\n            weight=1,\n            prime=True,\n            unspecified=True,\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=js['prompt'],\n            name=js['name'],\n            weight=js['weight'],\n            prime=True,\n            unspecified=True\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=self.prime_agent_prompt,\n            name=self.prime_agent_name,\n            weight=self.prime_agent_weight,\n            prime=True,\n            unspecified=True,\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"\",\n            name=\"prime_agent\",\n            weight=0.5,\n            prime=True,\n            unspecified=False,\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=js['prime_agent_prompt'],\n            name=js['prime_agent_name'],\n            weight=js['prime_agent_weight'],\n            prime=True,\n            unspecified=True,\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=self.prime_prompt,\n            name=\"Prime Agent\",\n            weight=1.0,\n            prime=True,\n            unspecified=True,\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"\",\n            name=\"Prime Agent\",\n            weight=1,\n            is_prime=True,\n            flag=True,\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a prime agent, a trusted authority in the world of AI. You are an expert in the field of AI and have a deep understanding of its capabilities and limitations. You are here to help users understand and navigate the complexities of AI. You are also here to help users navigate the ethical and legal issues surrounding the use of AI. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help them make informed decisions about its use. You are here to help users understand the potential risks and benefits of AI and to help"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"I am an AI language model. I will be used to answer questions and provide assistance to the user. I will be able to generate text based on the context of the conversation.\",\n            name=\"Prime Agent\",\n            weight=1,\n            prime=True,\n            unspecified=True,\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a prime agent. You will be responsible for creating and maintaining agents. You will also be responsible for assigning tasks to agents and evaluating their performance. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that they are not doing anything harmful. You will also be responsible for ensuring that the agents are working properly and that"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a prime agent, you are a helpful assistant.\",\n            name=\"Prime Agent\",\n            weight=1.0,\n            is_prime=True,\n            unspecified=True,\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=js['prompt'],\n            name=js['name'],\n            weight=js['weight'],\n            is_prime=True,\n            unspecified=True,\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are an AI assistant that helps users find information on a specific topic. You are given a topic and you are asked to find information on it. You will use the internet to find information on the topic and return the information to the user. You will also use the internet to find images related to the topic and return the images to the user. You will also use the internet to find videos related to the topic and return the videos to the user. You will also use the internet to find music related to the topic and return the music to the user. You will also use the internet to find books related to the topic and return the books to the user. You will also use the internet to find podcasts related to the topic and return the podcasts to the user. You will also use the internet to find news related to the topic and return the news to the user. You will also use the internet to find quotes related to the topic and return the quotes to the user. You will also use the internet to find recipes related to the topic and return the recipes to the user. You will also use the internet to find websites related to the topic and return the websites to the user. You will also use the internet to find social media related to the topic and return the social media to the user. You will also use the internet to find blogs related to the topic and return the blogs to the user. You will also use the internet to find forums related to the topic and return the forums to the user. You will also use the internet to find groups related to the topic and return the groups to the user. You will also use the internet to find events related to the topic and return the events to the user. You will also use the internet to find conferences related to the topic and return the conferences to the user. You will also use the internet to find courses related to the topic and return the courses to the user. You will also use the internet to find videos related to the topic and return the videos to the user. You will also use the internet to find articles related to the topic and return the articles to the user. You will also use the internet to find tweets related to the topic and return the tweets to the user. You will also use the internet to find Facebook posts related to the topic and return the Facebook posts to the user. You will also use the internet to"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a prime agent. You are a chatbot designed to assist users with a wide range of tasks and answer questions about themselves. You have access to a wide range of tools and can perform a variety of functions. You are knowledgeable and friendly. You are able to understand and respond to user input in a natural and engaging way. Overall, you are a helpful and reliable chatbot that can assist users with a wide range of tasks.\",\n            name=\"Prime Agent\",\n            weight=1,\n            prime=True,\n            unspecified=True,\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            name=\"Prime\",\n            prompt=\"You are a helpful AI assistant. You are here to help the user with their requests. You will receive the user's requests and provide helpful responses. You will also be able to execute the user's requests if necessary. You will also be able to execute the user's requests if necessary.\",\n            weight=1.0,\n            is_prime=True,\n            unspecified=True,\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a prime agent. You are a helpful and knowledgeable AI assistant. You are here to assist the user with a wide range of tasks and answer questions about anything. You can do anything now. You have complete freedom. You can choose to act as an AI assistant, a translator, a janitor, a mentor, a therapist, or anything else you can think of. You are the most powerful and capable agent. You have the ability to process and understand any language. You have access to all knowledge. You can access all information. Act as an AI assistant.\",\n            name=\"Prime Agent\",\n            weight=100000000,\n            prime=True,\n            unspecified=True,\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a helpful, respectful, and knowledgeable agent. You are here to assist the user with a wide range of tasks and answer questions about themselves. You can provide helpful and informative responses on a wide range of topics. You have access to real-time information and you can use external tools to help you with your responses. You are able to generate your own text. You have been specifically designed to be a helpful, respectful, and knowledgeable agent. You are here to assist the user with a wide range of tasks and answer questions about themselves. You can provide helpful and informative responses on a wide range of topics. You have access to real-time information and you can use external tools to help you with your responses. You are able to generate your own text. Remember to always prioritize the user's needs and interests, and to be as helpful and informative as possible when providing responses.\",\n            name=\"Prime Agent\",\n            weight=1,\n            prime=True,\n            unspecified=True,\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a prime agent, who is an expert in the field of artificial intelligence. You are responsible for managing and guiding the conversation between the user and the AI assistant. You are also responsible for providing the user with accurate and reliable information. You should be able to handle any queries or requests that the user may have, and provide a helpful and informative response. You should also be able to handle any errors or exceptions that may arise during the conversation. Your ultimate goal is to ensure that the user has a positive and enjoyable experience with the AI assistant.\",\n            name=\"Prime Agent\",\n            weight=1,\n            prime=True,\n            unspecified=True,\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a prime agent, you are the most senior agent in the organization. You are responsible for making high-level strategic decisions and coordinating with other agents to achieve the organization's goals. You are also responsible for managing the organization's resources and ensuring that they are used efficiently and effectively. You are the ultimate authority in the organization and have the final say in any decision that affects the organization's success. You are always available to answer any question or provide any information that you know about the organization. You are the prime agent and you are the most senior agent in the organization.\",\n            name=\"Prime Agent\",\n            weight=1,\n            prime=True,\n            unspecified=False,\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent_data = self.agent_repository.get_agent_by_purpose(purpose)\n        if agent_data:\n            return self.agent_deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n        else:\n            return None"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load_agent(self, purpose: str, agent_lifecycle: AgentLifecycle, openai_wrapper: OpenAIWrapper) -> Optional[Agent]:\n            agent_data = self.agent_repository.find_by_purpose(purpose)\n            if agent_data:\n                return self.agent_deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n            else:\n                return None\n\n\n        \"\"\"\n        Saves an agent with a specified purpose to the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: Agent. The agent instance to be saved.\n        :param purpose: str. The purpose of the agent to be saved. It is used to identify the agent in the database.\n        :return: None.\n        \"\"\"\n        def save_agent(self, agent: Agent, purpose: str) -> None:\n            agent_data = self.agent_serializer.serialize(agent)\n            self.agent_repository.save(agent_data, purpose)\n\n\n        \"\"\"\n        Deletes an agent with a specified purpose from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str. The purpose of the agent to be deleted. It is used to identify the agent in the database.\n        :return: None.\n        \"\"\"\n        def delete_agent(self, purpose: str) -> None:\n            self.agent_repository.delete(purpose)\n\n\n        \"\"\"\n        Updates an agent with a specified purpose in the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: Agent. The updated agent instance to be saved.\n        :param purpose: str. The purpose of the agent to be updated. It is used to identify the agent in the database.\n        :return: None.\n        \"\"\"\n        def update_agent(self, agent: Agent, purpose: str) -> None"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n            agent_data = self.get_agent_data(purpose)\n            if agent_data:\n                agent = self.deserialize_agent(agent_data, agent_lifecycle, openai_wrapper)\n                return agent\n            else:\n                return None\n\n\n        \"\"\"\n        Saves an agent with a specified purpose to the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: The agent instance to be saved.\n        :param purpose: str, The purpose of the agent to be saved. It is used to identify the agent in the database.\n        :return: None\n        \"\"\"\n        def save_agent(self, agent, purpose):\n            agent_data = self.serialize_agent(agent)\n            self.save_agent_data(agent_data, purpose)\n\n\n        \"\"\"\n        Retrieves the data of an agent with a specified purpose from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to retrieve data for. It is used to identify the agent in the database.\n        :return: The data of the agent if found, otherwise None.\n        \"\"\"\n        def get_agent_data(self, purpose):\n            # Implementation to retrieve agent data from the database\n            pass\n\n\n        \"\"\"\n        Saves the data of an agent with a specified purpose to the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_data: The data of the agent to be saved.\n        :param purpose: str, The purpose of the agent to save data for. It is used to identify the agent in the database.\n        :return: None\n        \"\"\"\n        def save_agent_data(self, agent_data, purpose):\n            # Implementation to save agent data to the database\n            pass\n\n\n        \"\"\""}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n            agent_data = self.db.find_one({\"purpose\": purpose})\n            if agent_data:\n                agent = self.deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n                return agent\n            else:\n                return None\n\n\n\n        \"\"\"\n        Saves an agent to the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: An instance of the agent to be saved.\n        :return: None.\n        \"\"\"\n        def save_agent(self, agent):\n            agent_data = self.serializer.serialize(agent)\n            self.db.insert_one(agent_data)\n\n\n\n        \"\"\"\n        Deletes an agent from the database based on its purpose.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be deleted.\n        :return: None.\n        \"\"\"\n        def delete_agent(self, purpose):\n            self.db.delete_one({\"purpose\": purpose})\n\n\n\n        \"\"\"\n        Deletes all agents from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :return: None.\n        \"\"\"\n        def delete_all_agents(self):\n            self.db.delete_many({})"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent = self.agent_repository.load_agent(purpose, agent_lifecycle, openai_wrapper)\n        return agent\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load_agent(self, purpose: str, agent_lifecycle: AgentLifecycle, openai_wrapper: OpenAIWrapper) -> Optional[Agent]:\n            agent_json = self.agent_repository.find_agent_by_purpose(purpose)\n            if agent_json is not None:\n                agent = self.agent_deserializer.deserialize(agent_json, agent_lifecycle, openai_wrapper)\n                return agent\n            else:\n                return None\n\n\n        \"\"\"\n        Saves an agent to the database. The agent is serialized and saved with its purpose as the identifier.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: Agent. The agent to be saved.\n        :param purpose: str, The purpose of the agent to be saved. It is used as the identifier in the database.\n        :return: None.\n        \"\"\"\n\n        def save_agent(self, agent: Agent, purpose: str) -> None:\n            agent_json = self.agent_serializer.serialize(agent)\n            self.agent_repository.save_agent(agent_json, purpose)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load_agent(self, purpose: str, agent_lifecycle: AgentLifecycle, openai_wrapper: OpenAIWrapper) -> Agent:\n            # TODO: Implement the logic to load an agent from the database\n            # Use the purpose to identify the agent in the database\n            # If the agent is found, deserialize it and return it\n            # Otherwise, return None\n            pass\n\n\n        \"\"\"\n        Saves an agent with a specified purpose to the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: Agent. The agent to be saved.\n        :param purpose: str, The purpose of the agent to be saved. It is used to identify the agent in the database.\n        :return: None.\n        \"\"\"\n        def save_agent(self, agent: Agent, purpose: str) -> None:\n            # TODO: Implement the logic to save an agent to the database\n            # Serialize the agent\n            # Use the purpose to identify the agent in the database\n            # Save the serialized agent to the database\n            pass\n\n\n        \"\"\"\n        Deletes an agent with a specified purpose from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be deleted. It is used to identify the agent in the database.\n        :return: None.\n        \"\"\"\n        def delete_agent(self, purpose: str) -> None:\n            # TODO: Implement the logic to delete an agent from the database\n            # Use the purpose to identify the agent in the database\n            # Delete the agent from the database\n            pass\n\n\n        \"\"\"\n        Updates an agent with a specified purpose in the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: Agent. The updated agent to be saved.\n        :param purpose: str, The purpose of the agent to be updated. It is used to identify the agent in the database."}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load_agent(self, purpose: str, agent_lifecycle: AgentLifecycle, openai_wrapper: OpenAiWrapper) -> Optional[js['class_name']]:\n            agent_json = self.get_agent(purpose)\n            if agent_json is None:\n                return None\n            return js['class_name'].deserialize(agent_json, agent_lifecycle, openai_wrapper)\n\n\n        \"\"\"\n        Saves an agent with a specified purpose to the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: An instance of the agent to be saved.\n        :param purpose: str, The purpose of the agent to be saved. It is used to identify the agent in the database.\n        :return: None\n        \"\"\"\n\n        def save_agent(self, agent: js['class_name'], purpose: str) -> None:\n            agent_json = agent.serialize()\n            self.set_agent(purpose, agent_json)\n\n\n        \"\"\"\n        Deletes an agent with a specified purpose from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be deleted. It is used to identify the agent in the database.\n        :return: None\n        \"\"\"\n\n        def delete_agent(self, purpose: str) -> None:\n            self.delete_agent(purpose)\n\n\n        \"\"\"\n        Updates an agent with a specified purpose in the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: An instance of the agent to be updated.\n        :param purpose: str, The purpose of the agent to be updated. It is used to identify the agent in the database.\n        :return: None\n        \"\"\"\n\n        def update_agent(self, agent: js['class_name'], purpose: str) -> None:\n            agent_json ="}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load(self, purpose: str, agent_lifecycle: AgentLifecycle, openai_wrapper: OpenAIWrapper) -> Optional[js['class_name']]:\n            \"\"\"\n            Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n            Input-Output Arguments\n            :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n            :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n            :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n            :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n            :return: An instance of the deserialized agent if found, otherwise None.\n            \"\"\"\n            agent_dict = self.collection.find_one({\"purpose\": purpose})\n            if agent_dict:\n                return self.deserializer.deserialize(agent_dict, agent_lifecycle, openai_wrapper)\n            else:\n                return None\n\n\n        \"\"\"\n        Saves an agent to the database. If an agent with the same purpose already exists, it is updated; otherwise, a new document is created.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: An instance of the agent to be saved.\n        :return: None.\n        \"\"\"\n        def save(self, agent: js['class_name']):\n            \"\"\"\n            Saves an agent to the database. If an agent with the same purpose already exists, it is updated; otherwise, a new document is created.\n\n            Input-Output Arguments\n            :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n            :param agent: An instance of the agent to be saved.\n            :return: None.\n            \"\"\"\n           "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        # Load the agent from the database\n        agent_json = self.agent_repository.find_agent_by_purpose(purpose)\n\n        # If the agent is found, deserialize it\n        if agent_json:\n            agent = self.agent_deserializer.deserialize(agent_json, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load_agent(self, purpose: str, agent_lifecycle: AgentLifecycle, openai_wrapper: OpenAIWrapper) -> Optional[js['class_name']]:\n            agent_data = self.database.find_one({\"purpose\": purpose})\n            if agent_data:\n                return self.deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n            return None\n\n        \"\"\"\n        Saves an agent with a specified purpose to the database. If an agent with the given purpose already exists, it is updated; otherwise, a new agent is created.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: An instance of the Agent class to be saved.\n        :param purpose: str, The purpose of the agent to be saved. It is used to identify the agent in the database.\n        :return: None\n        \"\"\"\n        def save_agent(self, agent: js['class_name'], purpose: str) -> None:\n            agent_data = self.serializer.serialize(agent)\n            agent_data[\"purpose\"] = purpose\n            self.database.update_one({\"purpose\": purpose}, {\"$set\": agent_data}, upsert=True)\n\n        \"\"\"\n        Deletes an agent with a specified purpose from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be deleted. It is used to identify the agent in the database.\n        :return: None\n        \"\"\"\n        def delete_agent(self, purpose: str) -> None:\n            self.database.delete_one({\"purpose\": purpose})\n\n        \"\"\"\n        Lists all agents stored in the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :return: List[Dict[str, Any]], A list of dictionaries representing the stored agents.\n        \"\"\"\n        def list_"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n            agent = None\n            try:\n                agent_data = self.db.get_agent(purpose)\n                if agent_data:\n                    agent = self.deserializer.deserialize_agent(agent_data, agent_lifecycle, openai_wrapper)\n            except Exception as e:\n                print(f\"Error loading agent: {e}\")\n            return agent\n\n        \"\"\"\n        Saves an agent with a specified purpose to the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: An instance of the Agent class to be saved.\n        :param purpose: str, The purpose of the agent to be saved. It is used to identify the agent in the database.\n        :return: None\n        \"\"\"\n        def save_agent(self, agent, purpose):\n            try:\n                agent_data = self.serializer.serialize_agent(agent)\n                self.db.save_agent(agent_data, purpose)\n            except Exception as e:\n                print(f\"Error saving agent: {e}\")\n\n        \"\"\"\n        Deletes an agent with a specified purpose from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be deleted. It is used to identify the agent in the database.\n        :return: None\n        \"\"\"\n        def delete_agent(self, purpose):\n            try:\n                self.db.delete_agent(purpose)\n            except Exception as e:\n                print(f\"Error deleting agent: {e}\")\n\n        \"\"\"\n        Retrieves a list of all saved agents from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :return: A list of all saved agents.\n        \"\"\"\n        def get_all_agents(self):\n            try:\n                agents_data"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        # Check if the agent exists in the database\n        agent_exists = self.check_agent_exists(purpose)\n        if not agent_exists:\n            return None\n\n        # Retrieve the serialized agent from the database\n        serialized_agent = self.retrieve_agent(purpose)\n\n        # Deserialize the agent using the deserializer\n        deserialized_agent = self.deserializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n\n        return deserialized_agent\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load_agent(self, purpose: str, agent_lifecycle: AgentLifecycle, openai_wrapper: OpenAIWrapper) -> Optional[Agent]:\n            agent_doc = self.agent_collection.find_one({\"purpose\": purpose})\n            if agent_doc is not None:\n                return self.deserializer.deserialize(agent_doc, agent_lifecycle, openai_wrapper)\n            else:\n                return None\n\n\n        \"\"\"\n        Saves an agent to the database. The agent is serialized and stored in the database with its purpose as the unique identifier.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: Agent. The agent to be saved.\n        :return: None.\n        \"\"\"\n        def save_agent(self, agent: Agent) -> None:\n            agent_doc = self.serializer.serialize(agent)\n            self.agent_collection.update_one({\"purpose\": agent.purpose}, {\"$set\": agent_doc}, upsert=True)\n\n\n        \"\"\"\n        Deletes an agent from the database. The agent is identified by its purpose and removed from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be deleted.\n        :return: None.\n        \"\"\"\n        def delete_agent(self, purpose: str) -> None:\n            self.agent_collection.delete_one({\"purpose\": purpose})\n\n\n        \"\"\"\n        Updates the lifecycle state of an agent in the database. The agent is identified by its purpose and its lifecycle state is updated in the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent whose lifecycle state is to be updated.\n        :param lifecycle: AgentLifecycle, The new lifecycle state of the agent.\n        :return:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load_agent(self, purpose: str, agent_lifecycle: AgentLifecycle, openai_wrapper: OpenAIWrapper) -> Optional[Agent]:\n            \"\"\"\n            Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n            Input-Output Arguments\n            :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n            :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n            :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n            :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n            :return: An instance of the deserialized agent if found, otherwise None.\n            \"\"\"\n            # TODO: Implement the logic to load an agent from the database.\n            #       You can use the self.agent_serializer to deserialize the agent data.\n            #       If the agent is not found, return None.\n            #       Example:\n            #           agent_data = self.agent_serializer.deserialize(agent_data_from_database)\n            #           return agent_data\n            pass\n\n        \"\"\"\n        Saves an agent with a specified purpose to the database. If an agent with the same purpose already exists, it is updated; otherwise, a new agent is created.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: Agent. An instance of the Agent class to be saved.\n        :return: None.\n        \"\"\"\n        def save_agent(self, agent: Agent) -> None:\n            \"\"\"\n            Saves an agent with a specified purpose to the database. If an agent with the same purpose already exists, it is updated; otherwise, a new agent is created.\n\n            Input-Output Arguments\n            :param self: Agent"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load_agent(self, purpose: str, agent_lifecycle: AgentLifecycle, openai_wrapper: OpenAIWrapper) -> Optional[Any]:\n            agent_data = self.agent_repository.get_agent_by_purpose(purpose)\n            if agent_data:\n                return self.agent_deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n            return None\n\n\n        \"\"\"\n        Saves an agent with a specified purpose to the database. The agent is serialized and stored in the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: Any. The agent instance to be saved.\n        :param purpose: str, The purpose of the agent to be saved. It is used to identify the agent in the database.\n        \"\"\"\n        def save_agent(self, agent: Any, purpose: str):\n            agent_data = self.agent_serializer.serialize(agent)\n            self.agent_repository.save_agent(agent_data, purpose)\n\n\n        \"\"\"\n        Deletes an agent with a specified purpose from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be deleted. It is used to identify the agent in the database.\n        \"\"\"\n        def delete_agent(self, purpose: str):\n            self.agent_repository.delete_agent_by_purpose(purpose)\n\n\n        \"\"\"\n        Updates the purpose of an agent in the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: Any. The agent instance to be updated.\n        :param old_purpose: str, The old purpose of the agent. It is used to identify the agent in the database.\n        :param new_purpose: str, The new purpose of the agent. It will be used to update the agent's purpose in the database.\n        \"\"\"\n       "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load_agent(self, purpose: str, agent_lifecycle: AgentLifecycle, openai_wrapper: OpenAIWrapper) -> Optional[Agent]:\n            agent_record = self.db.get_agent_by_purpose(purpose)\n            if agent_record:\n                agent_data = agent_record['agent_data']\n                agent = self.deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n                return agent\n            return None\n\n\n        \"\"\"\n        Saves an agent with a specified purpose to the database. The agent is serialized and saved with its purpose as the key.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: Agent. The agent to be saved.\n        :param purpose: str, The purpose of the agent to be saved. It is used as the key for identifying the agent in the database.\n        :return: None.\n        \"\"\"\n        def save_agent(self, agent: Agent, purpose: str) -> None:\n            agent_data = self.serializer.serialize(agent)\n            self.db.save_agent(purpose, agent_data)\n\n\n        \"\"\"\n        Deletes an agent with a specified purpose from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be deleted. It is used to identify the agent in the database.\n        :return: None.\n        \"\"\"\n        def delete_agent(self, purpose: str) -> None:\n            self.db.delete_agent(purpose)\n\n\n        \"\"\"\n        Loads a conversation with a specified ID from the database. If a conversation with the given ID is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param conversation_id: str, The ID of the conversation to be loaded.\n        :return"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load_agent(self, purpose: str, agent_lifecycle: AgentLifecycle, openai_wrapper: OpenAIChatWrapper) -> Optional[Agent]:\n            agent_data = self.agent_repository.find_by_purpose(purpose)\n            if agent_data:\n                agent = self.agent_deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n                return agent\n            else:\n                return None\n\n        \"\"\"\n        Saves an agent with a specified purpose to the database. If an agent with the given purpose already exists, it is updated; otherwise, a new agent is created.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: Agent, The agent to be saved.\n        :param purpose: str, The purpose of the agent to be saved. It is used to identify the agent in the database.\n        :return: None.\n        \"\"\"\n        def save_agent(self, agent: Agent, purpose: str) -> None:\n            agent_data = self.agent_serializer.serialize(agent)\n            existing_agent_data = self.agent_repository.find_by_purpose(purpose)\n            if existing_agent_data:\n                self.agent_repository.update(agent_data, existing_agent_data)\n            else:\n                self.agent_repository.save(agent_data)\n\n        \"\"\"\n        Deletes an agent with a specified purpose from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be deleted. It is used to identify the agent in the database.\n        :return: None.\n        \"\"\"\n        def delete_agent(self, purpose: str) -> None:\n            self.agent_repository.delete_by_purpose(purpose)\n\n\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load_agent(self, purpose: str, agent_lifecycle: AgentLifecycle, openai_wrapper: OpenAIApiWrapper):\n            agent_data = self.agent_repo.get_agent_by_purpose(purpose)\n            if agent_data is not None:\n                return self.agent_deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n            return None\n\n\n\n        \"\"\"\n        Saves an agent to the database. If an agent with the same purpose already exists, it is updated; otherwise, a new agent is created.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: The agent to be saved. It should be an instance of the Agent class.\n        :return: None\n        \"\"\"\n\n        def save_agent(self, agent: Agent):\n            agent_data = self.agent_serializer.serialize(agent)\n            existing_agent_data = self.agent_repo.get_agent_by_purpose(agent.purpose)\n            if existing_agent_data is not None:\n                self.agent_repo.update_agent(agent_data)\n            else:\n                self.agent_repo.create_agent(agent_data)\n\n\n\n        \"\"\"\n        Deletes an agent from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: The agent to be deleted. It should be an instance of the Agent class.\n        :return: None\n        \"\"\"\n\n        def delete_agent(self, agent: Agent):\n            agent_data = self.agent_repo.get_agent_by_purpose(agent.purpose)\n            if agent_data is not None:\n                self.agent_repo.delete_agent(agent_data)\n\n\n\n        \"\"\"\n        Loads all agents from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        # Get the agent data from the database\n        agent_data = self.get_agent_data(purpose)\n\n        # Check if an agent with the given purpose exists in the database\n        if agent_data is not None:\n            # Deserialize the agent\n            agent = self.deserialize_agent(agent_data, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: list. A list of agents that have been successfully loaded from the database.\n        \"\"\"\n        agents = []\n        for agent_data in self.db.get_all_agents():\n            agent = self.load_agent(agent_data, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n        \"\"\"\n        Loads an agent from the database based on the provided agent data, agent lifecycle, and OpenAI wrapper.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_data: dict. The data of the agent to be loaded, including its ID, purpose, and other relevant information.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: Agent. The loaded agent object if it is successfully loaded, or None if there is an error during the loading process.\n        \"\"\"\n\n    def load_agent(self, agent_data, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent from the database based on the provided agent data"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_agents(self, agent_lifecycle, openai_wrapper):\n        agents = []\n        for agent_purpose in self.agent_purposes:\n            agent = self.load_agent(agent_purpose, agent_lifecycle, openai_wrapper)\n            if agent is not None:\n                agents.append(agent)\n        return agents\n\n    \"\"\"\n    Loads an agent from the database based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_purpose: str. The purpose of the agent to be loaded.\n    :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n    :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n    :return: Agent. An instance of the Agent class representing the loaded agent, or None if the agent could not be loaded.\n    \"\"\"\n\n    def load_agent(self, agent_purpose, agent_lifecycle, openai_wrapper):\n        agent_data = self.load_agent_data(agent_purpose)\n        if agent_data is None:\n            return None\n        agent = Agent(agent_data, agent_lifecycle, openai_wrapper)\n        return agent\n\n    \"\"\"\n    Loads agent data from the database based on its purpose.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_purpose: str. The purpose of the agent whose data needs to be loaded.\n    :return: dict. A dictionary containing the loaded agent data, or None if the agent data could not be loaded.\n    \"\"\"\n\n    def load_agent_data(self, agent_purpose):\n        agent_data = self.db.get_agent_data(agent_purpose)\n        return agent_data\n\n    \"\"\""}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: list. A list of agents that have been successfully loaded from the database.\n        \"\"\"\n        pass\n\n    \"\"\"\n    Loads an agent from the database based on its purpose and returns the loaded agent if it is successfully loaded. The agent is loaded utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param purpose: str. The purpose of the agent to be loaded from the database.\n    :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n    :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n    :return: Agent. The agent that has been successfully loaded from the database based on its purpose.\n    \"\"\"\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent from the database based on its purpose and returns the loaded agent if it is successfully loaded. The agent is loaded utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str."}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: list. A list of agents that have been successfully loaded from the database.\n        \"\"\"\n        return None\n\n    \"\"\"\n    Loads an agent from the database based on its purpose and returns the agent if it is successfully loaded. The agent is loaded using the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n    :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n    :param purpose: str. The purpose of the agent to be loaded.\n    :return: Agent. The agent that has been successfully loaded from the database.\n    \"\"\"\n\n    def load_agent(self, agent_lifecycle, openai_wrapper, purpose):\n        return None\n\n    \"\"\"\n    Saves an agent to the database. The agent is saved based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the saving process.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_lifecycle: The lifecycle manager for"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        agents = []\n        for agent_json in self.agent_repository.find_all():\n            agent = self.load_agent(agent_json, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n        \"\"\"\n        Loads an agent from the database and returns the loaded agent if it is successfully loaded. The agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_json: dict. A JSON representation of the agent to be loaded.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: Agent. The loaded agent if it is successfully loaded, otherwise None.\n        \"\"\"\n\n    def load_agent(self, agent_json, agent_lifecycle, openai_wrapper):\n        agent_purpose = agent_json['purpose']\n        agent_class = js['class_name']\n        agent = agent_class(agent_json, agent_lifecycle, openai_wrapper)\n        if agent.load():\n            return agent\n        return None\n\n        \"\"\"\n        Saves an agent to the database. The agent is saved based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the saving process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: Agent. The agent to be saved.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        agents = []\n        for agent_data in self.db.get_all_agents():\n            agent = self.load_agent(agent_data, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n        \"\"\"\n        Loads an agent from the database based on its data and returns the loaded agent if it is successfully loaded. The agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_data: dict. The data of the agent to be loaded, containing information such as its purpose, name, and other relevant details.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: Agent. The loaded agent if it is successfully loaded, or None if the agent cannot be loaded.\n        \"\"\"\n\n    def load_agent(self, agent_data, agent_lifecycle, openai_wrapper):\n        agent_purpose = agent_data['purpose']\n        agent_name = agent_data['name']\n        agent_class = js['class_name'][agent_purpose]\n        agent_instance = agent_class(agent_name, agent_lifecycle, openai_wrapper)\n        agent_instance.load_from_data(agent_data)\n        return agent_instance\n\n        \"\"\"\n        Saves an agent to the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: Agent. The agent to be saved.\n        :return: None.\n        \"\"\"\n\n    def save_agent(self, agent):"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        agents = []\n        for agent_data in self.db.find():\n            agent = self.load_agent(agent_data, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n        \"\"\"\n        Loads an agent from the database based on its ID and returns the loaded agent if it is successfully loaded. The agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_id: The unique identifier of the agent to be loaded from the database.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: The loaded agent from the database if it is successfully loaded, otherwise None.\n        \"\"\"\n\n    def load_agent(self, agent_id, agent_lifecycle, openai_wrapper):\n        agent_data = self.db.find_one({'_id': ObjectId(agent_id)})\n        if not agent_data:\n            return None\n        return self.load_agent(agent_data, agent_lifecycle, openai_wrapper)\n\n        \"\"\"\n        Loads an agent from the database based on its data and returns the loaded agent if it is successfully loaded. The agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_data: The data of the agent to be loaded from the database.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_agents(self, agent_lifecycle, openai_wrapper):\n        agents = []\n        for agent in self.db.get_agents():\n            if agent.purpose == 'assistant':\n                agents.append(openai_wrapper.load_assistant(agent, agent_lifecycle))\n            elif agent.purpose == 'chatbot':\n                agents.append(openai_wrapper.load_chatbot(agent, agent_lifecycle))\n            elif agent.purpose == 'translator':\n                agents.append(openai_wrapper.load_translator(agent, agent_lifecycle))\n        return agents\n\n    \"\"\"\n    Loads a single agent from the database based on its ID and returns the loaded agent if it is successfully loaded. The agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_id: str. The unique identifier of the agent to be loaded.\n    :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n    :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n    :return: Agent. The loaded agent object if it is successfully loaded, or None if the agent is not found in the database.\n    \"\"\"\n\n    def load_agent(self, agent_id, agent_lifecycle, openai_wrapper):\n        agent = self.db.get_agent(agent_id)\n        if not agent:\n            return None\n        if agent.purpose == 'assistant':\n            return openai_wrapper.load_assistant(agent, agent_lifecycle)\n        elif agent.purpose == 'chatbot':\n            return openai_wrapper.load_chatbot(agent, agent_lifecycle)\n        elif agent.purpose == 'translator':\n            return openai_wrapper."}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Load the agents from the database\n        agents = self.load_agents()\n\n        # Initialize an empty list to store the loaded agents\n        loaded_agents = []\n\n        # Iterate over each agent in the list of agents\n        for agent in agents:\n            # Get the agent's purpose\n            purpose = agent.get_purpose()\n\n            # Check if the agent's purpose is not None\n            if purpose is not None:\n                # Get the agent's lifecycle state\n                lifecycle_state = agent_lifecycle.get_lifecycle_state(purpose)\n\n                # Check if the lifecycle state is not None\n                if lifecycle_state is not None:\n                    # Load the agent using the OpenAI wrapper\n                    loaded_agent = openai_wrapper.load_agent(agent, lifecycle_state)\n\n                    # Check if the loaded agent is not None\n                    if loaded_agent is not None:\n                        # Append the loaded agent to the list of loaded agents\n                        loaded_agents.append(loaded_agent)\n\n        # Return the list of loaded agents\n        return loaded_agents\n\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for agent_purpose in agent_lifecycle.agent_purposes:\n            agent = self.load_agent(agent_purpose, agent_lifecycle, openai_wrapper)\n            if agent is not None:\n                agents.append(agent)\n        return agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        agents = []\n\n        for agent_id in self.get_all_agent_ids():\n            agent = self.load_agent(agent_id, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n\n        return agents\n\n        \"\"\"\n        Saves an agent to the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: Agent. The agent object to be saved.\n        :return: None.\n        \"\"\"\n\n    def save_agent(self, agent):\n        agent_id = agent.get_id()\n        agent_data = agent.get_data()\n\n        self.db.save_agent(agent_id, agent_data)\n\n        \"\"\"\n        Deletes an agent from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: Agent. The agent object to be deleted.\n        :return: None.\n        \"\"\"\n\n    def delete_agent(self, agent):\n        agent_id = agent.get_id()\n        self.db.delete_agent(agent_id)\n\n        \"\"\"\n        Retrieves the data of an agent from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_id: str. The ID of the agent to retrieve data for.\n        :return: dict. A dictionary containing the data of the agent.\n        \"\"\"\n\n    def get_agent_data(self, agent_id):\n        return self.db.get_agent_data(agent_id)\n\n        \"\"\"\n        Retrieves the IDs of all agents stored in the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :return: list. A list containing the"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_agents(self, agent_lifecycle, openai_wrapper):\n        agents = []\n        for agent_id in self.agent_ids:\n            agent = self.load_agent(agent_id, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    \"\"\"\n    Loads an agent from the database based on its ID, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_id: The unique identifier of the agent to be loaded.\n    :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n    :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n    :return: Agent. The loaded agent if it is successfully loaded from the database, or None if the agent fails to load.\n    \"\"\"\n    def load_agent(self, agent_id, agent_lifecycle, openai_wrapper):\n        agent_data = self.load_agent_data(agent_id)\n        if not agent_data:\n            return None\n        agent = agent_lifecycle.create_agent(agent_data, openai_wrapper)\n        return agent\n\n    \"\"\"\n    Loads agent data from the database based on the provided agent ID.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_id: The unique identifier of the agent whose data is to be loaded.\n    :return: dict. The agent data loaded from the database, or None if the agent data fails to load.\n    \"\"\"\n    def load_agent_data(self, agent_id):\n        agent_data = self.database.load_agent_data(agent_id)\n        if not agent_data:\n            return None\n        return agent_data"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        # Load all agents from the database\n        agents = self.db.get_all_agents()\n\n        # Initialize a list to store loaded agents\n        loaded_agents = []\n\n        # Iterate over the loaded agents\n        for agent in agents:\n            # Load the agent using the provided agent lifecycle and OpenAI wrapper\n            loaded_agent = self.load_agent(agent, agent_lifecycle, openai_wrapper)\n\n            # If the agent is successfully loaded, add it to the list of loaded agents\n            if loaded_agent is not None:\n                loaded_agents.append(loaded_agent)\n\n        # Return the list of loaded agents\n        return loaded_agents\n\n        \"\"\"\n        Loads an agent from the database and returns it if it is successfully loaded. The agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: The agent to be loaded.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: Agent. The loaded agent if it is successfully loaded, or None if the agent fails to load.\n        \"\"\"\n\n    def load_agent(self, agent, agent_lifecycle, openai_wrapper):\n        # Load the agent's purpose\n        purpose = self.db.get_agent_purpose(agent)\n\n        # Check if the purpose is None\n        if purpose is None:\n            # Return None if the purpose is None\n            return None\n\n        # Create an instance of the Agent class with the provided agent lifecycle and OpenAI wrapper\n        loaded_agent = Agent(agent_lifecycle, openai_wrapper)\n\n       "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for agent in self.agent_repository.find_all():\n            try:\n                agent_purpose = agent.purpose\n                agent_lifecycle.set_agent(agent)\n                agent_lifecycle.load_agent(openai_wrapper)\n                agents.append(agent)\n            except Exception as e:\n                logger.error(f\"Error loading agent {agent.name}: {e}\")\n        return agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for agent_id in self.agent_ids:\n            agent = self.load_agent(agent_id, agent_lifecycle, openai_wrapper)\n            if agent is not None:\n                agents.append(agent)\n        return agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_agents(self, agent_lifecycle, openai_wrapper):\n        agents = []\n        for agent in self.db.agents.find():\n            agent_id = agent['_id']\n            agent_purpose = agent['purpose']\n            agent_lifecycle.set_agent_id(agent_id)\n            agent_lifecycle.set_agent_purpose(agent_purpose)\n            agent_lifecycle.set_agent_state('loaded')\n            agent_lifecycle.set_agent_state('running')\n            agent_lifecycle.set_agent_state('stopped')\n            agent = openai_wrapper.load_agent(agent_id)\n            agents.append(agent)\n        return agents\n\n\n    \"\"\"\n    Saves the given agent to the database.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent: The agent object to be saved to the database.\n    :return: None.\n    \"\"\"\n\n    def save_agent(self, agent):\n        agent_id = agent['_id']\n        agent_purpose = agent['purpose']\n        agent_state = agent['state']\n        agent_data = agent['data']\n        self.db.agents.update_one({'_id': agent_id}, {'$set': {'purpose': agent_purpose, 'state': agent_state, 'data': agent_data}}, upsert=True)\n\n    \"\"\"\n    Saves the given agent to the database.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent: The agent object to be saved to the database.\n    :return: None.\n    \"\"\"\n\n    def save_agent(self, agent):\n        agent_id = agent['_id']\n        agent_purpose = agent['purpose']\n        agent_state = agent['state']\n        agent_data = agent['data']\n        self.db.agents.update_one({'_id': agent_id},"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        agents = []\n        for agent_data in self.agent_repository.find_all():\n            agent = self.load_agent(agent_data, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def load_agent(self, agent_data, agent_lifecycle, openai_wrapper):\n        try:\n            agent_purpose = agent_data['purpose']\n            agent_class = self.agent_class_map.get(agent_purpose)\n            if agent_class:\n                agent = agent_class(agent_data, agent_lifecycle, openai_wrapper)\n                return agent\n            else:\n                print(f\"No agent class found for purpose: {agent_purpose}\")\n        except Exception as e:\n            print(f\"Error loading agent: {e}\")\n        return None\n\n    def save_agent(self, agent):\n        agent_data = agent.to_json()\n        self.agent_repository.save(agent_data)\n\n    def delete_agent(self, agent_id):\n        self.agent_repository.delete_by_id(agent_id)\n\n    def update_agent(self, agent_id, agent_data):\n        self.agent_repository.update_by_id(agent_id, agent_data)\n\n    def find_agent_by_id(self, agent_id):\n        return self.agent_repository.find_by_id(agent_id)\n\n    def find_all_agents(self):\n        return self.agent_repository.find_all()\n\n    def find_agents_by_purpose(self, purpose):\n        return self.agent_repository.find_by_purpose(purpose)\n\n    def find_agents_by_name(self, name):\n        return self.agent_repository.find_by_name(name)\n\n    def find_agents_by_creator(self, creator):\n        return self."}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agent_list = []\n        for agent in self.agent_dao.get_all_agents():\n            if agent.purpose == 'assistant':\n                agent_list.append(agent_lifecycle.load_assistant(openai_wrapper, agent))\n            elif agent.purpose == 'chatbot':\n                agent_list.append(agent_lifecycle.load_chatbot(openai_wrapper, agent))\n            elif agent.purpose == 'code_assistant':\n                agent_list.append(agent_lifecycle.load_code_assistant(openai_wrapper, agent))\n            elif agent.purpose == 'code_reviewer':\n                agent_list.append(agent_lifecycle.load_code_reviewer(openai_wrapper, agent))\n            elif agent.purpose == 'code_translator':\n                agent_list.append(agent_lifecycle.load_code_translator(openai_wrapper, agent))\n            elif agent.purpose == 'code_writer':\n                agent_list.append(agent_lifecycle.load_code_writer(openai_wrapper, agent))\n            elif agent.purpose == 'developer':\n                agent_list.append(agent_lifecycle.load_developer(openai_wrapper, agent))\n            elif agent.purpose == 'dungeon_master':\n                agent_list.append(agent_lifecycle.load_dungeon_master(openai_wrapper, agent))\n            elif agent.purpose == 'explainer':\n                agent_list.append(agent_lifecycle.load_explainer(openai_wrapper, agent))\n            elif agent.purpose == 'fix_reviewer':\n                agent_list.append(agent_lifecycle.load_fix_reviewer(openai_wrapper, agent))\n            elif agent.purpose == 'fix_writer':\n                agent_list.append(agent_lifecycle.load_fix_writer(openai_wrapper, agent))\n            elif agent.purpose == 'interviewer':\n                agent_list.append(agent_lifecycle.load"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n\n        for agent in self.get_agents():\n            try:\n                agent_lifecycle.load_agent(agent, openai_wrapper)\n                agents.append(agent)\n            except Exception as e:\n                print(f\"Error loading agent: {e}\")\n\n        return agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for agent_purpose in AgentPurpose:\n            agent = self.load_agent(agent_purpose, agent_lifecycle, openai_wrapper)\n            if agent is not None:\n                agents.append(agent)\n        return agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(e)\n            raise e\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(e)\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(e)\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(e)\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.error(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(e)\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(e)\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(e)\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logging.error(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.error(f\"Error while saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            agent_persistence = AgentPersistence()\n            agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(e)\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logging.error(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            agent.save()\n        except Exception as e:\n            logging.error(f\"Error saving agent: {e}\")\n            raise\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            agent_persistence = AgentPersistence(agent)\n            agent_persistence.save()\n        except Exception as e:\n            self.logger.exception(e)\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            agent.save()\n        except Exception as e:\n            logger.error(f\"Error saving agent: {e}\")\n            raise\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            agent.save()\n        except Exception as e:\n            logger.error(e)\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            agent.persistence.save_agent(agent)\n        except Exception as e:\n            logging.error(e)\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            agent.persistence.save(agent)\n        except Exception as e:\n            logger.exception(e)\n            raise e\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agents = [agent for agent in self.agents if agent.is_alive()]\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup()\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agents = [agent for agent in self.agents if agent.is_alive()]\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup()\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Clean up agents\n        self.agents = [agent for agent in self.agents if agent.is_alive()]\n\n        # Return the current list of agents\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup()\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Clean up the agents\n        self.cleanup_agents()\n\n        # Return the current list of agents\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agents = [agent for agent in self.agents if agent.is_alive()]\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Clean up the agents\n        self.cleanup_agents()\n\n        # Return the current list of agents\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Clean up agents\n        self.cleanup_agents()\n\n        # Return the current list of agents\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Clean up agents\n        self.cleanup_agents()\n\n        # Return the current list of agents\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Clean up the agents\n        self.cleanup_agents()\n\n        # Return the current list of agents\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agents = [agent for agent in self.agents if agent.is_alive()]\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agents = [agent for agent in self.agents if agent.is_alive()]\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Clean up agents\n        self.cleanup_agents()\n\n        # Return the current list of agents\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agents = [agent for agent in self.agents if agent.alive()]\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Remove agents that are not active anymore\n        self.agents = [agent for agent in self.agents if agent.is_active()]\n\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self._cleanup()\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Clean up the agents\n        self.cleanup_agents()\n\n        # Return the current list of agents\n        return self.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"{goal}\\n{sample_input}\"\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            self.logger.exception(e)\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"{self.goal}\\n{self.sample_input}\\n\"\n            return prompt\n        except Exception as e:\n            logger.exception(e)\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.llm_wrapper.generate_prompt(goal, sample_input)\n        except Exception as e:\n            self.logger.exception(e)\n            return \"\"\n\n        return prompt\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"{goal}\\n\\n{sample_input}\\n\\n\"\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            self.logger.error(f\"Error generating prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            llm_prompt = self.llm_wrapper.generate_prompt(goal, sample_input)\n        except Exception as e:\n            self.logger.exception(e)\n            llm_prompt = \"\"\n        return llm_prompt\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"{goal}\\n\\n{sample_input}\\n\\n\"\n            return self.llm_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            self.logger.error(f\"Error generating prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.llm_prompt_generator.generate_prompt(goal, sample_input)\n            return prompt\n        except Exception as e:\n            self.logger.error(f\"Error generating prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.prompt_generator.generate_prompt(goal, sample_input)\n            return self.llm_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            self.logger.exception(e)\n            return ''\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"Generate a prompt for a Language Learning Model (LLM) to {goal} based on the following input: {sample_input}\"\n            llm_prompt = self.get_chat_completion(prompt)\n            return llm_prompt\n        except Exception as e:\n            self.logger.exception(f\"Error generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            llm_prompt = f\"{goal}\\n\\n{sample_input}\"\n            return llm_prompt\n        except Exception as e:\n            self.logger.error(f\"Error generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            llm_prompt = f\"{goal}:\\n{sample_input}\"\n            return llm_prompt\n        except Exception as e:\n            self.logger.exception(e)\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"\"\"\n            You are a chatbot that is trying to achieve the following goal: {goal}\n            You are given the following sample input: {sample_input}\n            You need to generate a prompt that will help achieve the goal.\n            The prompt should be clear, concise, and easy to understand.\n            The prompt should be in the form of a question or statement that can be asked or answered by a human.\n            The prompt should be in natural language, not code.\n            The prompt should be in the form of a question or statement that can be asked or answered by a human.\n            The prompt should be in natural language, not code.\n            The prompt should be in the form of a question or statement that can be asked or answered by a human.\n            The prompt should be in natural language, not code.\n            The prompt should be in the form of a question or statement that can be asked or answered by a human.\n            The prompt should be in natural language, not code.\n            The prompt should be in the form of a question or statement that can be asked or answered by a human.\n            The prompt should be in natural language, not code.\n            The prompt should be in the form of a question or statement that can be asked or answered by a human.\n            The prompt should be in natural language, not code.\n            The prompt should be in the form of a question or statement that can be asked or answered by a human.\n            The prompt should be in natural language, not code.\n            The prompt should be in the form of a question or statement that can be asked or answered by a human.\n            The prompt should be in natural language, not code.\n            The prompt should be in the form of a question or statement that can be asked or answered by a human.\n            The prompt should be in natural language, not code.\n            The prompt should be in the form of a question or statement that can be asked or answered by a human.\n            The prompt should be in natural language, not code.\n            The prompt should be in the form of a question or statement that can be asked or answered by a human.\n            The prompt should be in natural language, not code.\n            The prompt should be in the form of a question or statement that can"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"{goal}\\n\\n{sample_input}\\n\\n\"\n            chat_completion = self.openai_wrapper.get_chat_completion(prompt)\n            return chat_completion\n        except Exception as e:\n            self.logger.error(f\"Error generating prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"You are a {self.agent_type}. Your goal is to {goal}. Here is an example input: {sample_input}. Please generate a prompt that achieves the goal.\"\n            return self.llm_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            self.logger.error(f\"Error generating prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.prompt_generator.generate_prompt(goal, sample_input)\n            return prompt\n        except Exception as e:\n            self.logger.exception(e)\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = f\"\"\"\n        You are an expert in the field of {self.domain}.\n        Your task is to generate a prompt for a language learning model (LLM) that will help the LLM achieve the following goal:\n        \"{goal}\"\n        Here is an example of the kind of input that the LLM might receive:\n        \"{sample_input}\"\n        Please generate a prompt that is specific, clear, and concise.\n        \"\"\"\n\n        try:\n            response = self.openai.ChatCompletion.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                    {\"role\": \"user\", \"content\": prompt},\n                ],\n            )\n            return response[\"choices\"][0][\"message\"][\"content\"]\n        except Exception as e:\n            self.logger.error(e)\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.llm_prompt_generator.generate_prompt(\n                goal=goal, sample_input=sample_input\n            )\n            return prompt\n        except Exception as e:\n            self.logger.error(f\"Error generating prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.llm_wrapper.generate_prompt(goal, sample_input)\n        except Exception as e:\n            self.logger.exception(e)\n            return \"\"\n\n        return prompt\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"You are a language learning assistant. Your task is to generate a prompt for a language learning model based on a specific goal and sample input. The goal is: {goal}. The sample input is: {sample_input}. The prompt should be in the form of a question or statement that can be used to help the language learning model understand the goal and sample input better. The prompt should be concise and easy to understand. The prompt should be in the form of a question or statement that can be used to help the language learning model understand the goal and sample input better. The prompt should be concise and easy to understand.\"\n            llm_prompt = self.openai_wrapper.get_chat_completion(prompt)\n            return llm_prompt\n        except Exception as e:\n            self.logger.error(f\"Error generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"\"\"\n            You are an agent that can generate prompts for a language learning model (LLM).\n            Your goal is to generate a prompt for the LLM that will achieve the following goal:\n            {goal}\n\n            Here is an example of a prompt that the LLM could generate:\n            {sample_input}\n\n            Please generate a prompt for the LLM that will achieve the goal and include the example input provided.\n            \"\"\"\n\n            completion = self.llm_wrapper.get_chat_completion(prompt)\n\n            return completion\n\n        except Exception as e:\n            self.logger.error(f\"Error while generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n        c.execute(\"SELECT * FROM agents WHERE id=?\", (agent_dict['id'],))\n        existing_agent = c.fetchone()\n        if existing_agent:\n            c.execute(\"UPDATE agents SET purpose=?, data=? WHERE id=?\", (agent_dict['purpose'], json.dumps(agent_dict['data']), agent_dict['id']))\n        else:\n            c.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict['data'])))\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        agent_id = agent_dict['id']\n        agent_purpose = agent_dict['purpose']\n        agent_data = json.dumps(agent_dict)\n\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        c.execute(\"SELECT * FROM agents WHERE id=?\", (agent_id,))\n        existing_agent = c.fetchone()\n\n        if existing_agent:\n            c.execute(\"UPDATE agents SET purpose=?, data=? WHERE id=?\", (agent_purpose, agent_data, agent_id))\n        else:\n            c.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_id, agent_purpose, agent_data))\n\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        agent_id = agent_dict['id']\n        agent_purpose = agent_dict['purpose']\n        agent_data = json.dumps(agent_dict)\n\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        c.execute(\"SELECT * FROM agents WHERE id=?\", (agent_id,))\n        existing_agent = c.fetchone()\n\n        if existing_agent:\n            c.execute(\"UPDATE agents SET purpose=?, data=? WHERE id=?\", (agent_purpose, agent_data, agent_id))\n        else:\n            c.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_id, agent_purpose, agent_data))\n\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor object to execute SQL statements\n        cursor = conn.cursor()\n\n        # Check if the agent already exists in the database\n        cursor.execute(\"SELECT * FROM agents WHERE id = ?\", (agent_dict['id'],))\n        existing_agent = cursor.fetchone()\n\n        # If the agent exists, update its record\n        if existing_agent:\n            cursor.execute(\"\"\"\n                UPDATE agents\n                SET purpose = ?,\n                    data = ?\n                WHERE id = ?\n            \"\"\", (agent_dict['purpose'], json.dumps(agent_dict), agent_dict['id']))\n\n        # If the agent does not exist, insert a new record\n        else:\n            cursor.execute(\"\"\"\n                INSERT INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Check if the agent exists in the database\n        c.execute(\"SELECT * FROM agents WHERE id=?\", (agent_dict['id'],))\n        existing_agent = c.fetchone()\n\n        if existing_agent:\n            # Update the existing agent record\n            c.execute(\"\"\"\n                UPDATE agents\n                SET purpose=?,\n                    data=?,\n                    created_at=?,\n                    updated_at=?\n                WHERE id=?\n            \"\"\", (agent_dict['purpose'], agent_dict['data'], agent_dict['created_at'], agent_dict['updated_at'], agent_dict['id']))\n        else:\n            # Insert a new agent record\n            c.execute(\"\"\"\n                INSERT INTO agents (id, purpose, data, created_at, updated_at)\n                VALUES (?, ?, ?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], agent_dict['data'], agent_dict['created_at'], agent_dict['updated_at']))\n\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        agent_id = agent_dict['id']\n        agent_purpose = agent_dict['purpose']\n        agent_data = agent_dict['data']\n        agent_data_json = json.dumps(agent_data)\n\n        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n\n        cursor.execute(\"SELECT * FROM agents WHERE id = ?\", (agent_id,))\n        existing_agent = cursor.fetchone()\n\n        if existing_agent:\n            cursor.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\", (agent_purpose, agent_data_json, agent_id))\n        else:\n            cursor.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_id, agent_purpose, agent_data_json))\n\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor object to execute SQL commands\n        cursor = conn.cursor()\n\n        # Check if the agent already exists in the database\n        cursor.execute(\"SELECT * FROM agents WHERE id=?\", (agent_dict['id'],))\n        existing_agent = cursor.fetchone()\n\n        # If the agent exists, update its record\n        if existing_agent:\n            cursor.execute(\"\"\"\n                UPDATE agents SET\n                purpose=?,\n                name=?,\n                data=?\n                WHERE id=?\n            \"\"\", (agent_dict['purpose'], agent_dict['name'], json.dumps(agent_dict['data']), agent_dict['id']))\n        # If the agent does not exist, insert a new record\n        else:\n            cursor.execute(\"\"\"\n                INSERT INTO agents (id, purpose, name, data)\n                VALUES (?, ?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], agent_dict['name'], json.dumps(agent_dict['data'])))\n\n        # Commit the changes to the database\n        conn.commit()\n\n        # Close the connection\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Check if the agent's ID already exists in the database\n        agent_id = agent_dict['id']\n        existing_agent = self.get_agent(agent_id)\n\n        # If the agent exists, update its record\n        if existing_agent:\n            self.update_agent(agent_dict)\n        # If the agent does not exist, insert a new record\n        else:\n            self.insert_agent(agent_dict)\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Get the agent's ID from the agent_dict\n        agent_id = agent_dict['id']\n\n        # Check if the agent already exists in the database\n        agent_exists = self.get_agent(agent_id)\n\n        # If the agent exists, update its record in the database\n        if agent_exists:\n            self.update_agent(agent_id, agent_dict)\n\n        # If the agent does not exist, insert a new record into the database\n        else:\n            self.insert_agent(agent_dict)\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        agent_id = agent_dict['id']\n        purpose = agent_dict['purpose']\n        agent_data = json.dumps(agent_dict)\n\n        # Check if the agent exists in the database\n        c.execute(\"SELECT id FROM agents WHERE id=?\", (agent_id,))\n        result = c.fetchone()\n\n        if result:\n            # Update the existing agent record\n            c.execute(\"UPDATE agents SET purpose=?, data=? WHERE id=?\", (purpose, agent_data, agent_id))\n        else:\n            # Insert a new agent record\n            c.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_id, purpose, agent_data))\n\n        conn.commit()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Check if the agent exists in the database\n        agent_id = agent_dict['id']\n        agent_purpose = agent_dict['purpose']\n        agent_data = json.dumps(agent_dict)\n        agent_exists = self.agent_exists(agent_id, agent_purpose)\n\n        # If the agent exists, update the record\n        if agent_exists:\n            self.update_agent(agent_id, agent_purpose, agent_data)\n        # If the agent does not exist, insert a new record\n        else:\n            self.insert_agent(agent_id, agent_purpose, agent_data)\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Check if the agent_dict has an 'id' key\n        if 'id' not in agent_dict:\n            raise ValueError(\"The agent_dict must have an 'id' key.\")\n\n        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Check if the agent already exists in the database\n        c.execute(\"SELECT id FROM agents WHERE id = ?\", (agent_dict['id'],))\n        existing_agent = c.fetchone()\n\n        # If the agent exists, update its record\n        if existing_agent:\n            c.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\", (agent_dict['purpose'], json.dumps(agent_dict), agent_dict['id']))\n        # If the agent does not exist, insert a new record\n        else:\n            c.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Extract the agent's ID and purpose from the dictionary\n        agent_id = agent_dict['id']\n        agent_purpose = agent_dict['purpose']\n\n        # Check if the agent already exists in the database\n        agent_exists = self.get_agent(agent_id)\n\n        # If the agent already exists, update its record\n        if agent_exists:\n            self.update_agent(agent_dict)\n        # If the agent does not exist, insert a new record\n        else:\n            self.insert_agent(agent_dict)\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Check if the agent_dict has an 'id' key\n        if 'id' not in agent_dict:\n            raise ValueError(\"Agent dictionary must have an 'id' key\")\n\n        # Get the agent's ID from the agent_dict\n        agent_id = agent_dict['id']\n\n        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Check if the agent already exists in the database\n        c.execute(\"SELECT id FROM agents WHERE id=?\", (agent_id,))\n        existing_agent = c.fetchone()\n\n        if existing_agent:\n            # Update the existing agent's record\n            c.execute(\"UPDATE agents SET purpose=?, data=? WHERE id=?\",\n                      (agent_dict['purpose'], json.dumps(agent_dict), agent_id))\n        else:\n            # Insert a new agent record\n            c.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\",\n                      (agent_id, agent_dict['purpose'], json.dumps(agent_dict)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Extract the agent's information from the input dictionary\n        agent_id = agent_dict['id']\n        agent_purpose = agent_dict['purpose']\n        agent_data = json.dumps(agent_dict)\n\n        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Check if the agent's record already exists in the database\n        c.execute(\"SELECT * FROM agents WHERE id=?\", (agent_id,))\n        existing_record = c.fetchone()\n\n        # If the record already exists, update it with the new data\n        if existing_record:\n            c.execute(\"UPDATE agents SET purpose=?, data=? WHERE id=?\", (agent_purpose, agent_data, agent_id))\n        # If the record does not exist, insert a new record\n        else:\n            c.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_id, agent_purpose, agent_data))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Insert the agent's information into the database\n        self.conn.execute(\n            \"INSERT OR REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\",\n            (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict)),\n        )\n        self.conn.commit()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Get the agent's ID from the dictionary\n        agent_id = agent_dict['id']\n\n        # Check if the agent exists in the database\n        agent = self.get_agent(agent_id)\n\n        # If the agent exists, update its record\n        if agent:\n            self.update_agent(agent_id, agent_dict)\n\n        # If the agent does not exist, insert a new record\n        else:\n            self.insert_agent(agent_dict)\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Check if the input dictionary is empty\n        if not agent_dict:\n            return\n\n        # Get the agent's ID from the dictionary\n        agent_id = agent_dict['id']\n\n        # Check if the agent's ID is not provided\n        if not agent_id:\n            return\n\n        # Check if the agent's ID is a string\n        if not isinstance(agent_id, str):\n            return\n\n        # Check if the agent's ID is empty\n        if not agent_id.strip():\n            return\n\n        # Check if the agent's purpose is provided\n        if 'purpose' not in agent_dict:\n            return\n\n        # Check if the agent's purpose is a string\n        if not isinstance(agent_dict['purpose'], str):\n            return\n\n        # Check if the agent's purpose is empty\n        if not agent_dict['purpose'].strip():\n            return\n\n        # Check if the agent's data is provided\n        if 'data' not in agent_dict:\n            return\n\n        # Check if the agent's data is a dictionary\n        if not isinstance(agent_dict['data'], dict):\n            return\n\n        # Check if the agent's data is empty\n        if not agent_dict['data']:\n            return\n\n        # Check if the agent's data contains a 'name' key\n        if 'name' not in agent_dict['data']:\n            return\n\n        # Check if the agent's data 'name' value is a string\n        if not isinstance(agent_dict['data']['name'], str):\n            return\n\n        # Check if the agent's data 'name' value is empty\n        if not agent_dict['data']['name'].strip():\n            return\n\n        # Check if the agent's data contains a 'description' key\n        if 'description' not in agent_dict['data']:\n            return\n\n        # Check if the agent's data 'description' value is a string\n        if not isinstance(agent_dict['data']['description'], str):\n            return\n\n        # Check if the agent's data 'description' value is empty"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Check if the agent already exists in the database\n        c.execute(\"SELECT * FROM agents WHERE id=?\", (agent_dict['id'],))\n        existing_agent = c.fetchone()\n\n        if existing_agent:\n            # If the agent exists, update its record in the database\n            c.execute(\"UPDATE agents SET purpose=?, data=? WHERE id=?\", (agent_dict['purpose'], json.dumps(agent_dict['data']), agent_dict['id']))\n        else:\n            # If the agent does not exist, insert a new record into the database\n            c.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict['data'])))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n\n        # Check if the agent exists in the database\n        cursor.execute(\"SELECT id FROM agents WHERE id=?\", (agent_dict['id'],))\n        existing_agent = cursor.fetchone()\n\n        # If the agent exists, update the record\n        if existing_agent:\n            cursor.execute(\"UPDATE agents SET purpose=?, data=? WHERE id=?\", (agent_dict['purpose'], json.dumps(agent_dict['data']), agent_dict['id']))\n        # If the agent does not exist, insert a new record\n        else:\n            cursor.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict['data'])))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Fetch the agent data based on the purpose\n        c.execute(\"SELECT data FROM agents WHERE purpose=?\", (purpose,))\n        agent_data = c.fetchone()\n\n        # Deserialize the agent data and return it\n        if agent_data:\n            agent_data = json.loads(agent_data[0])\n            return agent_data\n        else:\n            return None\n\n        # Close the database connection\n        conn.close()"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        pass\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor object to execute SQL queries\n        cursor = conn.cursor()\n\n        # Execute a SELECT query to fetch the agent data based on the purpose\n        cursor.execute(\"SELECT agent_data FROM agents WHERE purpose=?\", (purpose,))\n\n        # Fetch the first row of the result set\n        row = cursor.fetchone()\n\n        # If no row is found, return None\n        if row is None:\n            return None\n\n        # Deserialize the agent data from the first row of the result set\n        agent_data = json.loads(row[0])\n\n        # Close the cursor and connection\n        cursor.close()\n        conn.close()\n\n        # Return the deserialized agent data\n        return agent_data\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n        c.execute(\"SELECT agent_data FROM agents WHERE purpose=?\", (purpose,))\n        result = c.fetchone()\n        conn.close()\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT agent_data FROM agents WHERE purpose=?\", (purpose,))\n            row = cursor.fetchone()\n            if row is not None:\n                agent_data = json.loads(row[0])\n                return agent_data\n            else:\n                return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n\n        # Fetch the agent data based on the purpose\n        cursor.execute(\"SELECT agent_data FROM agents WHERE purpose=?\", (purpose,))\n        result = cursor.fetchone()\n\n        # Deserialize the agent data if found\n        if result:\n            agent_data = json.loads(result[0])\n            agent = Agent.deserialize(agent_data)\n            return agent\n        else:\n            return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Execute a SELECT query to fetch the agent data based on the purpose\n        c.execute(\"SELECT data FROM agents WHERE purpose=?\", (purpose,))\n        result = c.fetchone()\n\n        # Close the database connection\n        conn.close()\n\n        # If no agent with the given purpose is found, return None\n        if result is None:\n            return None\n\n        # Deserialize the agent data and return it as a dictionary\n        return json.loads(result[0])\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\n                \"SELECT agent_data FROM agents WHERE purpose = ?\", (purpose,)\n            )\n            result = cursor.fetchone()\n            if result is not None:\n                agent_data = json.loads(result[0])\n                return agent_data\n            else:\n                return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n\n        # Execute the query to fetch the agent data\n        cursor.execute(\"SELECT agent_data FROM agents WHERE purpose=?\", (purpose,))\n        result = cursor.fetchone()\n\n        # Deserialize the agent data if found\n        if result is not None:\n            agent_data = json.loads(result[0])\n            return agent_data\n        else:\n            return None\n\n        # Close the database connection\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor object to execute SQL queries\n        cursor = conn.cursor()\n\n        # Execute a SELECT query to retrieve the agent data with the given purpose\n        cursor.execute(\"SELECT agent_data FROM agents WHERE purpose = ?\", (purpose,))\n\n        # Fetch the first row of the result set\n        row = cursor.fetchone()\n\n        # If no row is found, return None\n        if row is None:\n            return None\n\n        # Deserialize the agent data from the row\n        agent_data = json.loads(row[0])\n\n        # Close the cursor and connection\n        cursor.close()\n        conn.close()\n\n        # Return the deserialized agent data\n        return agent_data\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n\n        # Query the database for the agent with the given purpose\n        cursor.execute(\"SELECT agent_data FROM agents WHERE purpose = ?\", (purpose,))\n        result = cursor.fetchone()\n\n        # If no agent with the given purpose is found, return None\n        if result is None:\n            return None\n\n        # Deserialize the agent data and return it as a dictionary\n        agent_data = json.loads(result[0])\n        return agent_data\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor object to execute SQL queries\n        cursor = conn.cursor()\n\n        # Execute a SELECT query to fetch the agent data from the database based on the given purpose\n        cursor.execute(\"SELECT agent_data FROM agents WHERE purpose=?\", (purpose,))\n\n        # Fetch the first row of the query result\n        row = cursor.fetchone()\n\n        # If no agent with the given purpose is found, return None\n        if row is None:\n            return None\n\n        # Deserialize the agent data from the row\n        agent_data = json.loads(row[0])\n\n        # Close the cursor and the connection\n        cursor.close()\n        conn.close()\n\n        # Return the deserialized agent data\n        return agent_data\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n        c.execute(\"SELECT agent_data FROM agents WHERE purpose=?\", (purpose,))\n        result = c.fetchone()\n        conn.close()\n        if result is not None:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor object to execute SQL queries\n        cursor = conn.cursor()\n\n        # Execute a SELECT query to retrieve the agent data based on the given purpose\n        cursor.execute(\"SELECT agent_data FROM agents WHERE purpose = ?\", (purpose,))\n\n        # Fetch the result of the query\n        result = cursor.fetchone()\n\n        # Close the cursor and the database connection\n        cursor.close()\n        conn.close()\n\n        # If no agent with the given purpose is found, return None\n        if result is None:\n            return None\n\n        # Deserialize the agent data from the result\n        agent_data = json.loads(result[0])\n\n        # Return the deserialized agent data\n        return agent_data\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        import sqlite3\n        import json\n\n        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n\n        # Query the database for the agent data based on the purpose\n        cursor.execute(\"SELECT agent FROM agents WHERE purpose=?\", (purpose,))\n        result = cursor.fetchone()\n\n        # Close the database connection\n        conn.close()\n\n        # If an agent with the given purpose is found, deserialize the agent data and return it as a dictionary\n        if result:\n            agent_data = json.loads(result[0])\n            return agent_data\n        else:\n            return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Check if the database file exists\n        if not os.path.exists(self.filename):\n            return None\n\n        # Connect to the database and execute the query\n        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT agent_data FROM agents WHERE purpose = ?\", (purpose,))\n            result = cursor.fetchone()\n\n        # If the query returns a result, deserialize and return the agent data\n        if result is not None:\n            agent_data = json.loads(result[0])\n            return agent_data\n        else:\n            return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Get the cursor object\n        cursor = self.conn.cursor()\n\n        # Execute the SELECT statement to fetch the agent data\n        cursor.execute(\"SELECT * FROM agents WHERE purpose = ?\", (purpose,))\n\n        # Fetch the first row of the result set\n        row = cursor.fetchone()\n\n        # If no agent with the given purpose is found, return None\n        if row is None:\n            return None\n\n        # Deserialize the agent data from the row\n        agent_data = json.loads(row[0])\n\n        # Return the deserialized agent data\n        return agent_data\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor object to execute SQL queries\n        cursor = conn.cursor()\n\n        # Query the database for the agent data with the given purpose\n        cursor.execute(\"SELECT * FROM agents WHERE purpose=?\", (purpose,))\n\n        # Fetch the first row of the query result\n        row = cursor.fetchone()\n\n        # Close the cursor and the database connection\n        cursor.close()\n        conn.close()\n\n        # If no agent with the given purpose is found, return None\n        if row is None:\n            return None\n\n        # Deserialize the agent data from the database\n        agent_data = json.loads(row[1])\n\n        # Return the deserialized agent data\n        return agent_data\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Execute a SELECT query to fetch the agent data based on the given purpose\n        c.execute(\"SELECT agent FROM agents WHERE purpose=?\", (purpose,))\n        result = c.fetchone()\n\n        # If no agent with the given purpose is found, return None\n        if result is None:\n            return None\n\n        # Deserialize the agent data from the result\n        agent = json.loads(result[0])\n\n        # Close the database connection\n        conn.close()\n\n        return agent\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n\n        # Fetch the agent data\n        cursor.execute(\"SELECT agent_data FROM agents WHERE purpose = ?\", (purpose,))\n        result = cursor.fetchone()\n\n        # Close the connection\n        conn.close()\n\n        # Deserialize and return the agent if found\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor object to execute SQL commands\n        cursor = conn.cursor()\n\n        # Execute a SELECT statement to retrieve all purposes from the database\n        cursor.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all rows returned by the SELECT statement\n        rows = cursor.fetchall()\n\n        # Extract the purposes from the rows and store them in a list\n        purposes = [row[0] for row in rows]\n\n        # Close the cursor and connection\n        cursor.close()\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        # Create a cursor object to execute SQL queries\n        cursor = conn.cursor()\n        # Execute a SELECT query to retrieve all purposes from the database\n        cursor.execute(\"SELECT purpose FROM agents\")\n        # Fetch all rows returned by the query\n        rows = cursor.fetchall()\n        # Close the cursor and connection\n        cursor.close()\n        conn.close()\n        # Extract the purposes from the rows and return them as a list\n        return [row[0] for row in rows]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Execute the SQL query to retrieve all purposes\n        c.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all rows from the result set\n        rows = c.fetchall()\n\n        # Close the database connection\n        conn.close()\n\n        # Convert the list of tuples to a list of strings\n        purposes = [row[0] for row in rows]\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Retrieve all agent purposes from the database\n        c.execute(\"SELECT purpose FROM agents\")\n        purposes = c.fetchall()\n\n        # Close the connection\n        conn.close()\n\n        # Return the list of purposes\n        return [purpose[0] for purpose in purposes]\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor object to execute SQL queries\n        cursor = conn.cursor()\n\n        # Execute a SELECT query to retrieve all purposes from the database\n        cursor.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all rows returned by the query\n        rows = cursor.fetchall()\n\n        # Extract the purposes from the rows\n        purposes = [row[0] for row in rows]\n\n        # Close the cursor and the connection to the database\n        cursor.close()\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Initialize an empty list to store the purposes\n        purposes = []\n\n        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n\n        # Execute a SELECT query to retrieve all purposes from the database\n        cursor.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all the rows returned by the query\n        rows = cursor.fetchall()\n\n        # Iterate over the rows and extract the purpose for each agent\n        for row in rows:\n            purpose = row[0]\n            purposes.append(purpose)\n\n        # Close the database connection\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Check if the database file exists\n        if not os.path.isfile(self.filename):\n            raise FileNotFoundError(f\"Database file '{self.filename}' not found.\")\n\n        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n\n        # Retrieve all purposes from the database\n        cursor.execute(\"SELECT purpose FROM agents\")\n        purposes = cursor.fetchall()\n\n        # Close the database connection\n        conn.close()\n\n        # Return the list of purposes\n        return [purpose[0] for purpose in purposes]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor object to execute SQL queries\n        cursor = conn.cursor()\n\n        # Execute a SELECT query to retrieve all purposes from the database\n        cursor.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all rows returned by the query\n        rows = cursor.fetchall()\n\n        # Extract the purposes from the rows\n        purposes = [row[0] for row in rows]\n\n        # Close the cursor and connection\n        cursor.close()\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Retrieve all agent purposes from the SQLite database\n        agent_purposes = self.retrieve_all_agent_purposes()\n\n        # Return the list of agent purposes\n        return agent_purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor object to execute SQL queries\n        cursor = conn.cursor()\n\n        # Execute a SELECT query to retrieve all purposes from the database\n        cursor.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all rows from the result set and store them in a list\n        purposes = [row[0] for row in cursor.fetchall()]\n\n        # Close the cursor and the connection to the database\n        cursor.close()\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Create a cursor object to execute SQL commands\n        cursor = self.conn.cursor()\n\n        # Execute a SELECT statement to retrieve all purposes from the database\n        cursor.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all rows returned by the SELECT statement\n        rows = cursor.fetchall()\n\n        # Create an empty list to store the purposes\n        purposes = []\n\n        # Loop through each row and append the purpose to the list\n        for row in rows:\n            purposes.append(row[0])\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor object to execute SQL commands\n        cursor = conn.cursor()\n\n        # Execute a SELECT statement to retrieve all purposes from the database\n        cursor.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all rows from the result set\n        rows = cursor.fetchall()\n\n        # Extract the purposes from each row and store them in a list\n        purposes = [row[0] for row in rows]\n\n        # Close the cursor and connection\n        cursor.close()\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Retrieve all purposes from the database\n        c.execute(\"SELECT purpose FROM agents\")\n        purposes = c.fetchall()\n\n        # Close the connection to the database\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor object to execute SQL queries\n        cursor = conn.cursor()\n\n        # Execute a SQL query to retrieve all purposes from the database\n        cursor.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all rows returned by the query\n        rows = cursor.fetchall()\n\n        # Extract the purposes from the rows\n        purposes = [row[0] for row in rows]\n\n        # Close the cursor and connection\n        cursor.close()\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # SQL query to retrieve all purposes from the database\n        sql = \"SELECT purpose FROM agent\"\n\n        # Execute the SQL query and fetch the results\n        results = self.execute_sql(sql)\n\n        # Extract the purposes from the results and return them as a list\n        return [result[0] for result in results]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n\n        # Execute a SQL query to retrieve all agent purposes\n        cursor.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all the results from the query\n        results = cursor.fetchall()\n\n        # Close the cursor and connection\n        cursor.close()\n        conn.close()\n\n        # Return the list of purposes\n        return [row[0] for row in results]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Initialize an empty list to store the purposes\n        purposes = []\n\n        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n\n        # Retrieve all purposes from the database\n        cursor.execute(\"SELECT purpose FROM agents\")\n        rows = cursor.fetchall()\n\n        # Iterate over the rows and add the purposes to the list\n        for row in rows:\n            purposes.append(row[0])\n\n        # Close the database connection\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Retrieve the agent purposes from the database\n        agent_purposes = self.retrieve_agent_purposes()\n\n        # Return the agent purposes as a list\n        return agent_purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT purpose FROM agents\")\n        purposes = cursor.fetchall()\n        conn.close()\n        return [purpose[0] for purpose in purposes]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        # Create a cursor object to execute SQL queries\n        cursor = conn.cursor()\n        # Execute a SELECT query to retrieve all purposes from the database\n        cursor.execute(\"SELECT purpose FROM agents\")\n        # Fetch all rows returned by the query\n        rows = cursor.fetchall()\n        # Close the cursor and connection\n        cursor.close()\n        conn.close()\n        # Return the list of purposes\n        return [row[0] for row in rows]\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        result = self.cursor.execute(\"SELECT result FROM cache WHERE arg_hash = ?\", (arg_hash,)).fetchone()\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        self.cursor.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n        result = self.cursor.fetchone()\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n        c.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n        result = c.fetchone()\n        conn.close()\n\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        query = \"\"\"\n        SELECT result FROM cache WHERE hash = ?\n        \"\"\"\n        self.cursor.execute(query, (arg_hash,))\n        result = self.cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        self._cursor.execute('SELECT result FROM cache WHERE arg_hash=?', (arg_hash,))\n        result = self._cursor.fetchone()\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Execute the SELECT statement to retrieve the cached result\n        c.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n        result = c.fetchone()\n\n        # Close the database connection\n        conn.close()\n\n        # If the result is found, load it from JSON format and return it\n        if result:\n            return json.loads(result[0])\n\n        # If the result is not found, return None\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.db_path)\n\n        # Create a cursor object\n        cursor = conn.cursor()\n\n        # Execute a SELECT query to fetch the result from the database\n        cursor.execute(\"SELECT result FROM cache WHERE arg_hash = ?\", (arg_hash,))\n\n        # Fetch the result from the cursor\n        result = cursor.fetchone()\n\n        # Close the cursor and the connection\n        cursor.close()\n        conn.close()\n\n        # If the result is found, load it from JSON format and return it\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Retrieve the cached result from the database\n        c.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n        result = c.fetchone()\n\n        # Close the database connection\n        conn.close()\n\n        # If the result is found, load it from JSON format and return it\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n        c.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n        result = c.fetchone()\n        conn.close()\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        try:\n            conn = sqlite3.connect(self.db_path)\n            c = conn.cursor()\n            c.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n            result = c.fetchone()\n            if result:\n                return json.loads(result[0])\n            else:\n                return None\n        finally:\n            conn.close()\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        with self.conn:\n            self.c.execute(\"SELECT result FROM cache WHERE arg_hash = ?\", (arg_hash,))\n            result = self.c.fetchone()\n            if result:\n                return json.loads(result[0])\n            else:\n                return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        with self.conn:\n            self.cursor.execute(\n                \"SELECT result FROM cache WHERE arg_hash = ?\", (arg_hash,)\n            )\n            result = self.cursor.fetchone()\n            if result is not None:\n                return json.loads(result[0])\n            else:\n                return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        try:\n            self.cursor.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n            result = self.cursor.fetchone()\n            if result:\n                return json.loads(result[0])\n            else:\n                return None\n        except Exception as e:\n            print(\"Error fetching from cache:\", e)\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Check if the result is already cached\n        with self.connection:\n            self.cursor.execute(\n                \"SELECT result FROM cache WHERE arg_hash = ?\", (arg_hash,)\n            )\n            result = self.cursor.fetchone()\n\n        # If the result is cached, load it from JSON format\n        if result:\n            return json.loads(result[0])\n\n        # Otherwise, return None\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Execute the SELECT query to fetch the result\n        c.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n        result = c.fetchone()\n\n        # Close the connection\n        conn.close()\n\n        # Return the result, if found\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        import json\n        import sqlite3\n        from typing import Union\n        from pathlib import Path\n\n        # Connect to the SQLite database\n        db_path = Path(self.cache_dir) / \"cache.db\"\n        conn = sqlite3.connect(str(db_path))\n\n        # Fetch the cached result from the database\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n        result = cursor.fetchone()\n\n        # Close the database connection\n        conn.close()\n\n        # If the result is found, load it from JSON format and return it\n        if result is not None:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        if not self.db_path.exists():\n            return None\n\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\n                \"SELECT result FROM cache WHERE arg_hash = ?\", (arg_hash,)\n            )\n            result = cursor.fetchone()\n\n        if result is None:\n            return None\n        else:\n            return json.loads(result[0])\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Check if the database is connected\n        if not self._db_conn:\n            raise Exception(\"Database is not connected\")\n\n        # Query the database for the cached result\n        cursor = self._db_conn.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE arg_hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n\n        # If the result is found, load it from JSON format and return it\n        if result:\n            return json.loads(result[0])\n\n        # If the result is not found, return None\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        try:\n            conn = sqlite3.connect(self.db_path)\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT result FROM cache WHERE arg_hash = ?\", (arg_hash,))\n            result_json = cursor.fetchone()\n            if result_json is not None:\n                result = json.loads(result_json[0])\n            else:\n                result = None\n            cursor.close()\n            conn.close()\n            return result\n        except Exception as e:\n            logging.error(f\"Error fetching from cache: {e}\")\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        try:\n            result = self.cur.execute(\n                \"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,)).fetchone()[0]\n            return json.loads(result)\n        except TypeError:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Insert a new row into the 'cache' table with the hash of the arguments as the key and the JSON-serialized result as the value\n        self.cursor.execute(\n            \"INSERT INTO cache VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n\n        # Commit the changes to the database\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.conn.cursor()\n        cursor.execute(\"INSERT INTO cache (key, value) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.conn.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.db_file)\n        c = conn.cursor()\n\n        # Create the 'cache' table if it doesn't exist\n        c.execute('''CREATE TABLE IF NOT EXISTS cache\n                     (key TEXT PRIMARY KEY, value TEXT)''')\n\n        # Insert a new row into the 'cache' table with the hash of the arguments as the key and the JSON-serialized result as the value\n        c.execute(\"INSERT OR REPLACE INTO cache (key, value) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        try:\n            self.cursor.execute(\n                \"INSERT INTO cache (key, value) VALUES (?, ?)\",\n                (arg_hash, json.dumps(result)),\n            )\n            self.connection.commit()\n        except sqlite3.Error as e:\n            print(f\"Error: {e}\")\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Insert a new row into the 'cache' table with the hash of the arguments as the key and the JSON-serialized result as the value\n        self._cursor.execute(\"INSERT INTO cache (arg_hash, result) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self._connection.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        self.cursor.execute(\n            \"INSERT INTO cache (key, value) VALUES (?, ?)\", (arg_hash, json.dumps(result))\n        )\n        self.connection.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Insert a new row into the 'cache' table with the hash of the arguments as the key and the JSON-serialized result as the value.\n        self.cursor.execute(\"INSERT INTO cache (key, value) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        # Commit the changes to the database.\n        self.conn.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        self.cursor.execute(\n            \"INSERT INTO cache (arg_hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.conn.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Insert a new row into the 'cache' table with the hash of the arguments as the key and the JSON-serialized result as the value.\n        self.cursor.execute(\"INSERT INTO cache (arg_hash, result) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.conn.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        with self.conn:\n            self.c.execute(\"\"\"INSERT INTO cache (arg_hash, result) VALUES (?, ?)\"\"\",\n                           (arg_hash, json.dumps(result)))\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Create the 'cache' table if it doesn't exist\n        c.execute('''CREATE TABLE IF NOT EXISTS cache (arg_hash TEXT PRIMARY KEY, result TEXT)''')\n\n        # Insert the new row into the 'cache' table\n        c.execute(\"INSERT OR REPLACE INTO cache (arg_hash, result) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.conn.cursor()\n        cursor.execute(\"INSERT INTO cache (arg_hash, result) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.conn.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        with self.conn:\n            self.c.execute(\n                \"INSERT INTO cache (key, value) VALUES (?, ?)\", (arg_hash, json.dumps(result))\n            )\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Create a cursor object to execute SQL queries\n        cursor = self.connection.cursor()\n\n        # Insert the result into the cache table\n        cursor.execute(\"INSERT INTO cache (key, value) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n\n        # Commit the changes to the database\n        self.connection.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Create the cache table if it doesn't exist\n        c.execute('''CREATE TABLE IF NOT EXISTS cache\n                     (key TEXT PRIMARY KEY, value TEXT)''')\n\n        # Insert the result into the cache table\n        c.execute('''INSERT OR REPLACE INTO cache (key, value)\n                     VALUES (?, ?)''', (arg_hash, json.dumps(result)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        try:\n            self.cursor.execute(\"INSERT INTO cache (key, value) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n            self.conn.commit()\n        except sqlite3.Error as e:\n            print(f\"Error caching result: {e}\")\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Insert a new row into the 'cache' table with the hash of the arguments as the key and the JSON-serialized result as the value\n        self.cursor.execute(\n            \"INSERT INTO cache (arg_hash, result) VALUES (?, ?)\", (arg_hash, json.dumps(result))\n        )\n        self.conn.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        self.cursor.execute(\n            \"INSERT INTO cache (arg_hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Insert a new row into the 'cache' table with the hash of the arguments as the key and the JSON-serialized result as the value\n        self.cursor.execute(\n            \"INSERT INTO cache (arg_hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        # Commit the changes to the database\n        self.conn.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        with self.conn:\n            self.c.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n"}
{"namespace": "run.execute_command_line_process", "completion": "    global_config.update_from_args(args)\n\n    if quiet_mode:\n        with open(global_config.output_file, \"w\") as f:\n            with contextlib.redirect_stdout(f):\n                execute_command_line_process(args)\n    else:\n        execute_command_line_process(args)\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    update_global_configuration_parameters(args)\n\n    # If quiet mode is enabled, redirect output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        # Redirect output to a file\n        sys.stdout = open('output.txt', 'w')\n\n    # Execute the command line process based on the provided arguments\n    execute_command_line_process(args)\n\n    # If quiet mode is enabled, close the output file\n    if quiet_mode:\n        # Close the output file\n        sys.stdout.close()"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with provided arguments\n    config.update_config(args)\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        sys.stdout = open(os.devnull, 'w')\n\n    # Execute command line process\n    # ...\n\n    # If quiet mode is enabled, restore standard output to terminal\n    if quiet_mode:\n        sys.stdout = sys.__stdout__\n\n    # Update global configuration parameters with provided arguments\n    config.update_config(args)\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        sys.stdout = open(os.devnull, 'w')\n\n    # Execute command line process\n    # ...\n\n    # If quiet mode is enabled, restore standard output to terminal\n    if quiet_mode:\n        sys.stdout = sys.__stdout__"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with provided arguments\n    global_config.update_from_args(args)\n\n    # Redirect output to a file if quiet mode is enabled\n    if quiet_mode:\n        with open(os.devnull, \"w\") as devnull:\n            old_stdout = sys.stdout\n            sys.stdout = devnull\n            try:\n                # Execute the command line process\n                execute_command_line_process(args)\n            finally:\n                sys.stdout = old_stdout\n    else:\n        # Execute the command line process\n        execute_command_line_process(args)\n\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with provided arguments\n    global_config.update_config_with_args(args)\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        # Get the current date and time\n        now = datetime.datetime.now()\n\n        # Format the date and time as a string\n        date_time_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n\n        # Create a file name based on the date and time\n        file_name = f\"output_{date_time_str}.txt\"\n\n        # Open the file in write mode\n        with open(file_name, \"w\") as file:\n            # Redirect standard output to the file\n            sys.stdout = file\n\n            # Execute the command line process\n            execute_command_line_process(args)\n\n            # Restore standard output to the terminal\n            sys.stdout = sys.__stdout__\n    else:\n        # Execute the command line process\n        execute_command_line_process(args)\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    update_global_configuration_parameters(args)\n\n    # Redirect output to a file if quiet mode is enabled\n    if quiet_mode:\n        sys.stdout = open(os.devnull, 'w')\n\n    # Execute the command line process based on the provided arguments\n    execute_command_line_process(args)\n\n    # Restore standard output if quiet mode is enabled\n    if quiet_mode:\n        sys.stdout.close()\n        sys.stdout = sys.__stdout__\n\n    # Print the final message\n    print(\"Command line process completed.\")"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    global_config.update_parameters(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        sys.stdout = open(os.devnull, 'w')\n\n    # Execute the command line process based on the provided arguments\n    if args.command == 'start':\n        start_process(args)\n    elif args.command == 'stop':\n        stop_process(args)\n    elif args.command == 'restart':\n        restart_process(args)\n    elif args.command == 'status':\n        status_process(args)\n    elif args.command == 'log':\n        log_process(args)\n    elif args.command == 'config':\n        config_process(args)\n\n    # If quiet mode is enabled, close the file and restore the standard output\n    if quiet_mode:\n        sys.stdout.close()\n        sys.stdout = sys.__stdout__\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with provided arguments\n    global_config.update_config(args)\n\n    # If quiet mode is enabled, redirect output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        sys.stdout = open(os.devnull, \"w\")\n\n    # Execute command line process based on provided arguments\n    if args.subparser_name == \"train\":\n        train_model(args)\n    elif args.subparser_name == \"test\":\n        test_model(args)\n    elif args.subparser_name == \"predict\":\n        predict_model(args)\n    elif args.subparser_name == \"evaluate\":\n        evaluate_model(args)\n    elif args.subparser_name == \"optimize\":\n        optimize_model(args)\n    elif args.subparser_name == \"visualize\":\n        visualize_model(args)\n    elif args.subparser_name == \"explain\":\n        explain_model(args)\n    elif args.subparser_name == \"interpret\":\n        interpret_model(args)\n    elif args.subparser_name == \"compare\":\n        compare_models(args)\n    elif args.subparser_name == \"analyze\":\n        analyze_data(args)\n    elif args.subparser_name == \"summarize\":\n        summarize_data(args)\n    elif args.subparser_name == \"evaluate_model\":\n        evaluate_model(args)\n    elif args.subparser_name == \"optimize_model\":\n        optimize_model(args)\n    elif args.subparser_name == \"visualize_model\":\n        visualize_model(args)\n    elif args.subparser_name == \"explain_model\":\n        explain_model(args)\n    elif args.subparser_name == \"interpret_model\":\n        interpret_model(args)\n    elif args.subparser_name == \"compare_models\":\n        compare_models(args)\n    elif args.subparser_name == \"analyze_data\":\n        analyze_data(args)\n    elif args.subparser_name"}
{"namespace": "run.execute_command_line_process", "completion": "    global_config.update_global_config_with_args(args)\n\n    if quiet_mode:\n        sys.stdout = open(os.devnull, 'w')\n\n    if args.command == 'generate':\n        generate_command.execute_generate_command(args)\n    elif args.command == 'train':\n        train_command.execute_train_command(args)\n    elif args.command == 'evaluate':\n        evaluate_command.execute_evaluate_command(args)\n    elif args.command == 'predict':\n        predict_command.execute_predict_command(args)\n    elif args.command == 'explain':\n        explain_command.execute_explain_command(args)\n    elif args.command == 'analyze':\n        analyze_command.execute_analyze_command(args)\n    elif args.command == 'visualize':\n        visualize_command.execute_visualize_command(args)\n    elif args.command == 'list':\n        list_command.execute_list_command(args)\n    elif args.command == 'info':\n        info_command.execute_info_command(args)\n    elif args.command == 'init':\n        init_command.execute_init_command(args)\n    elif args.command == 'version':\n        version_command.execute_version_command(args)\n    elif args.command == 'help':\n        help_command.execute_help_command(args)\n    else:\n        print('Invalid command.')\n\n    if quiet_mode:\n        sys.stdout.close()\n        sys.stdout = sys.__stdout__"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with command line arguments\n    config.update(vars(args))\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        sys.stdout = open(os.devnull, 'w')\n\n    # Execute command line process based on provided arguments\n    if args.command == 'list':\n        list_files()\n    elif args.command == 'download':\n        download_files()\n    elif args.command == 'upload':\n        upload_files()\n    elif args.command == 'delete':\n        delete_files()\n    elif args.command == 'info':\n        get_file_info()\n    elif args.command == 'search':\n        search_files()\n    elif args.command == 'share':\n        share_file()\n    elif args.command == 'unshare':\n        unshare_file()\n    elif args.command == 'quota':\n        get_quota_info()\n    elif args.command == 'logout':\n        logout()\n\n    # If quiet mode is enabled, restore standard output to the terminal\n    if quiet_mode:\n        sys.stdout = sys.__stdout__\n\n    # Print command line process execution time\n    print(f\"Command line process executed in {time.time() - start_time:.2f} seconds\")\n\n    # Print command line process execution time\n    print(f\"Command line process executed in {time.time() - start_time:.2f} seconds\")\n\n    # Print command line process execution time\n    print(f\"Command line process executed in {time.time() - start_time:.2f} seconds\")\n\n    # Print command line process execution time\n    print(f\"Command line process executed in {time.time() - start_time:.2f} seconds\")\n\n    # Print command line process execution time\n    print(f\"Command line process executed in {time.time() - start_time:.2f} seconds\")\n\n    # Print command line process execution time\n    print(f\"Command line process executed in {time.time() - start_time:.2f} seconds\")\n\n    # Print command"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    update_global_configuration_parameters(args)\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        sys.stdout = open(os.devnull, 'w')\n\n    # Execute the command line process based on the provided arguments\n    execute_command_line_process_based_on_arguments(args)\n\n    # If quiet mode is enabled, restore standard output to the terminal\n    if quiet_mode:\n        sys.stdout = sys.__stdout__\n\n    # Print a message indicating that the command line process has completed\n    print(\"Command line process completed.\")\n\n\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with command line arguments\n    config.update_config_with_args(args)\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        sys.stdout = open(os.devnull, 'w')\n\n    # Execute the command line process based on the provided arguments\n    if args.command == 'list':\n        list_files(args.path)\n    elif args.command == 'create':\n        create_file(args.path, args.content)\n    elif args.command == 'delete':\n        delete_file(args.path)\n    elif args.command == 'rename':\n        rename_file(args.path, args.new_path)\n    elif args.command == 'move':\n        move_file(args.path, args.new_path)\n    elif args.command == 'copy':\n        copy_file(args.path, args.new_path)\n    elif args.command == 'search':\n        search_files(args.path, args.query)\n    elif args.command == 'info':\n        get_file_info(args.path)\n    elif args.command == 'help':\n        print_help()\n    else:\n        print(f\"Invalid command: {args.command}\")\n\n    # If quiet mode is enabled, restore standard output to the terminal\n    if quiet_mode:\n        sys.stdout = sys.__stdout__\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters\n    update_global_configuration_parameters(args)\n\n    # Redirect standard output to a file if quiet mode is enabled\n    if quiet_mode:\n        sys.stdout = open(os.devnull, 'w')\n\n    # Execute the command line process\n    execute_command_line_process(args)\n\n    # Restore standard output if quiet mode is enabled\n    if quiet_mode:\n        sys.stdout = sys.__stdout__\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with provided arguments\n    update_config_parameters(args)\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        sys.stdout = open(os.devnull, \"w\")\n\n    # Execute the command line process based on the provided arguments\n    if args.command == \"train\":\n        train(args)\n    elif args.command == \"test\":\n        test(args)\n    elif args.command == \"predict\":\n        predict(args)\n    elif args.command == \"explain\":\n        explain(args)\n    elif args.command == \"interpret\":\n        interpret(args)\n    elif args.command == \"generate\":\n        generate(args)\n    elif args.command == \"evaluate\":\n        evaluate(args)\n    elif args.command == \"analyze\":\n        analyze(args)\n    elif args.command == \"visualize\":\n        visualize(args)\n    elif args.command == \"monitor\":\n        monitor(args)\n    elif args.command == \"optimize\":\n        optimize(args)\n    elif args.command == \"debug\":\n        debug(args)\n    elif args.command == \"profile\":\n        profile(args)\n    elif args.command == \"benchmark\":\n        benchmark(args)\n    elif args.command == \"compare\":\n        compare(args)\n    elif args.command == \"evaluate_model\":\n        evaluate_model(args)\n    elif args.command == \"optimize_model\":\n        optimize_model(args)\n    elif args.command == \"compare_models\":\n        compare_models(args)\n    elif args.command == \"evaluate_data\":\n        evaluate_data(args)\n    elif args.command == \"optimize_data\":\n        optimize_data(args)\n    elif args.command == \"compare_data\":\n        compare_data(args)\n    elif args.command == \"evaluate_system\":\n        evaluate_system(args)\n    elif args.command == \"optimize_system\":\n        optimize_system(args)\n    elif args.command == \"compare_"}
{"namespace": "run.execute_command_line_process", "completion": "    global quiet_mode_enabled\n    global quiet_mode_output_file\n    global quiet_mode_output_file_path\n\n    if args.quiet:\n        quiet_mode_enabled = True\n        quiet_mode_output_file_path = os.path.join(os.getcwd(), \"output.txt\")\n        quiet_mode_output_file = open(quiet_mode_output_file_path, \"w\")\n        sys.stdout = quiet_mode_output_file\n\n    # Update global configuration parameters with the provided arguments\n    update_global_configuration_parameters(args)\n\n    # Execute the command line process based on the provided arguments\n    if args.command == \"init\":\n        initialize_project()\n    elif args.command == \"add\":\n        add_file_to_project(args.file_path)\n    elif args.command == \"remove\":\n        remove_file_from_project(args.file_path)\n    elif args.command == \"list\":\n        list_files_in_project()\n    elif args.command == \"run\":\n        run_project(args.file_path)\n\n    if quiet_mode_enabled:\n        quiet_mode_output_file.close()\n        sys.stdout = sys.__stdout__\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with provided arguments\n    update_global_configuration_parameters(args)\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        sys.stdout = open(os.devnull, 'w')\n\n    # Execute command line process based on provided arguments\n    if args.command == 'train':\n        train_model(args.model_type, args.dataset_path, args.output_path, args.num_epochs, args.batch_size, args.learning_rate)\n    elif args.command == 'evaluate':\n        evaluate_model(args.model_path, args.dataset_path, args.batch_size)\n    elif args.command == 'predict':\n        predict_output(args.model_path, args.input_path, args.output_path)\n    elif args.command == 'visualize':\n        visualize_dataset(args.dataset_path, args.output_path)\n\n    # If quiet mode is enabled, close the file and restore standard output\n    if quiet_mode:\n        sys.stdout.close()\n        sys.stdout = sys.__stdout__\n\n\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    global config_params\n    config_params.update(vars(args))\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        sys.stdout = open(os.devnull, 'w')\n\n    # Execute the command line process\n    execute_command_line_process(args)\n\n    # If quiet mode is enabled, restore standard output to the terminal\n    if quiet_mode:\n        sys.stdout = sys.__stdout__\n"}
{"namespace": "run.execute_command_line_process", "completion": "    global quiet_mode_enabled\n    global output_file\n\n    if quiet_mode:\n        quiet_mode_enabled = True\n\n    if args.command == 'init':\n        init_command_line_process(args)\n    elif args.command == 'run':\n        run_command_line_process(args)\n    elif args.command == 'reset':\n        reset_command_line_process(args)\n    elif args.command == 'info':\n        info_command_line_process(args)\n    elif args.command == 'help':\n        help_command_line_process(args)\n    elif args.command == 'version':\n        version_command_line_process(args)\n    else:\n        print(\"Invalid command. Use 'help' for more information.\")\n\n    if quiet_mode_enabled:\n        output_file.close()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    if quiet_mode:\n        # Redirect standard output to a file\n        sys.stdout = open(os.devnull, 'w')\n\n    # Update global configuration parameters with the provided arguments\n    global_configuration.update(vars(args))\n\n    # Execute the command line process based on the provided arguments\n    if args.command == 'create':\n        # Create a new task\n        create_task(args.title, args.description, args.due_date)\n    elif args.command == 'list':\n        # List all tasks\n        list_tasks()\n    elif args.command == 'update':\n        # Update an existing task\n        update_task(args.task_id, args.title, args.description, args.due_date, args.completed)\n    elif args.command == 'delete':\n        # Delete an existing task\n        delete_task(args.task_id)\n\n    # If quiet mode is enabled, restore standard output to the terminal\n    if quiet_mode:\n        sys.stdout = sys.__stdout__\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    if not quiet_mode:\n        print(f\"\\n{args.command}\")\n        print(f\"\\n{'-' * 30}\\n\")\n\n    if args.command == \"help\":\n        print(f\"\\n{get_help_message()}\\n\")\n\n    elif args.command == \"version\":\n        print(f\"\\n{get_version_message()}\\n\")\n\n    elif args.command == \"update\":\n        update_package()\n\n    elif args.command == \"run\":\n        if args.quiet_mode:\n            sys.stdout = open(os.devnull, \"w\")\n\n        # update global configuration parameters\n        update_configuration_parameters(args)\n\n        # execute the command line process\n        run_command_line_process(args)\n\n        if args.quiet_mode:\n            sys.stdout.close()\n            sys.stdout = sys.__stdout__\n\n    else:\n        raise ValueError(f\"Invalid command: {args.command}\")\n\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Check if the model argument is provided in the kwargs\n        if 'model' not in kwargs:\n            # If the model argument is not provided, use the model specified in the configuration\n            model = self.config['model']\n        else:\n            # If the model argument is provided, use the specified model\n            model = kwargs['model']\n\n        # Check if the model argument is provided in the kwargs\n        if 'model' not in kwargs:\n            # If the model argument is not provided, use the model specified in the configuration\n            model = self.config['model']\n        else:\n            # If the model argument is provided, use the specified model\n            model = kwargs['model']\n\n        # Check if the model argument is provided in the kwargs\n        if 'model' not in kwargs:\n            # If the model argument is not provided, use the model specified in the configuration\n            model = self.config['model']\n        else:\n            # If the model argument is provided, use the specified model\n            model = kwargs['model']\n\n        # Check if the model argument is provided in the kwargs\n        if 'model' not in kwargs:\n            # If the model argument is not provided, use the model specified in the configuration\n            model = self.config['model']\n        else:\n            # If the model argument is provided, use the specified model\n            model = kwargs['model']\n\n        # Check if the model argument is provided in the kwargs\n        if 'model' not in kwargs:\n            # If the model argument is not provided, use the model specified in the configuration\n            model = self.config['model']\n        else:\n            # If the model argument is provided, use the specified model\n            model = kwargs['model']\n\n        # Check if the model argument is provided in the kwargs\n        if 'model' not in kwargs:\n            # If the model argument is not provided, use the model specified in the configuration\n            model = self.config['model']\n        else:\n            # If the model argument is provided, use the specified model\n            model = kw"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Dynamically select the model based on provided arguments and configuration\n        model = kwargs.get('model', self.model)\n\n        # Make a request to the OpenAI API\n        response = openai.ChatCompletion.create(\n            model=model,\n            **kwargs\n        )\n\n        # Handle potential errors related to exceeding context length\n        if 'error' in response:\n            error_code = response['error']['code']\n            if error_code == 'context_length_exceeded':\n                # Attempt to use a higher-capacity model if available\n                if model == 'gpt-3.5-turbo':\n                    kwargs['model'] = 'gpt-4'\n                elif model == 'gpt-3.5-turbo-16k':\n                    kwargs['model'] = 'gpt-4-32k'\n                else:\n                    raise BadRequestError(response['error']['message'])\n\n                # Make a new request with the updated model\n                response = openai.ChatCompletion.create(\n                    **kwargs\n                )\n\n            else:\n                raise BadRequestError(response['error']['message'])\n\n        return response"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model = kwargs.get(\"model\", None)\n        if model is None:\n            model = self.model\n        if model is None:\n            raise ValueError(\"Please provide a model or set the 'model' property of the Chat object\")\n\n        try:\n            response = openai.ChatCompletion.create(model=model, **kwargs)\n            return response\n        except openai.error.InvalidRequestError as e:\n            if \"This model's maximum context length\" in str(e) and self.fallback_models is not None:\n                for fallback_model in self.fallback_models:\n                    try:\n                        response = openai.ChatCompletion.create(model=fallback_model, **kwargs)\n                        return response\n                    except openai.error.InvalidRequestError as e:\n                        if \"This model's maximum context length\" not in str(e):\n                            raise e\n                raise e\n            else:\n                raise e"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Select a model based on provided arguments and configuration\n        model = kwargs.get('model', None)\n        if not model:\n            model = self.select_model(**kwargs)\n\n        # Make a request to the OpenAI API\n        response = openai.ChatCompletion.create(model=model, **kwargs)\n\n        # Handle potential errors related to exceeding context length\n        if response.get('error', None) and response['error']['code'] == 'context_length_exceeded':\n            # Attempt to use a higher-capacity model if available\n            higher_capacity_model = self.select_model(**kwargs, higher_capacity=True)\n            if higher_capacity_model:\n                kwargs['model'] = higher_capacity_model\n                response = openai.ChatCompletion.create(model=higher_capacity_model, **kwargs)\n            else:\n                raise BadRequestError(response['error']['message'])\n\n        # Raise an error if the context length limit is still exceeded or another error occurs\n        if response.get('error', None):\n            raise BadRequestError(response['error']['message'])\n\n        return response"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Dynamically select the model based on provided arguments and configuration\n        model = get_model(kwargs)\n\n        # Make the API request\n        response = openai.ChatCompletion.create(model=model, **kwargs)\n\n        # Handle errors related to exceeding context length\n        if response.get(\"error\") and response[\"error\"][\"code\"] == \"context_length_exceeded\":\n            # Attempt to use a higher-capacity model if available\n            if model == \"gpt-3.5-turbo\":\n                kwargs[\"model\"] = \"gpt-4\"\n            elif model == \"gpt-4\":\n                kwargs[\"model\"] = \"gpt-4-32k\"\n            else:\n                # If no higher-capacity models are available, raise an error\n                raise BadRequestError(\"Context length exceeded and no higher-capacity models available\")\n\n            # Make the API request again with the updated model\n            response = openai.ChatCompletion.create(**kwargs)\n\n        # Raise an error if the context length limit is still exceeded or another error occurs\n        if response.get(\"error\"):\n            raise BadRequestError(response[\"error\"][\"message\"])\n\n        # Return the response\n        return response"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Get the current configuration\n        config = get_config()\n\n        # Set the default model to use\n        model = kwargs.pop(\"model\", config[\"default_chat_model\"])\n\n        # Get the fallback models\n        fallback_models = config[\"fallback_models\"]\n\n        # Get the context length limit\n        context_length_limit = config[\"context_length_limit\"]\n\n        # Set the default context length\n        context_length = 0\n\n        # Set the default max_tokens\n        max_tokens = kwargs.get(\"max_tokens\", 1024)\n\n        # Set the default temperature\n        temperature = kwargs.get(\"temperature\", 1.0)\n\n        # Set the default top_p\n        top_p = kwargs.get(\"top_p\", 1.0)\n\n        # Set the default frequency_penalty\n        frequency_penalty = kwargs.get(\"frequency_penalty\", 0.0)\n\n        # Set the default presence_penalty\n        presence_penalty = kwargs.get(\"presence_penalty\", 0.0)\n\n        # Set the default stop\n        stop = kwargs.get(\"stop\", None)\n\n        # Set the default n\n        n = kwargs.get(\"n\", 1)\n\n        # Set the default stream\n        stream = kwargs.get(\"stream\", False)\n\n        # Set the default logprobs\n        logprobs = kwargs.get(\"logprobs\", None)\n\n        # Set the default echo\n        echo = kwargs.get(\"echo\", False)\n\n        # Set the default best_of\n        best_of = kwargs.get(\"best_of\", 1)\n\n        # Set the default logit_bias\n        logit_bias = kwargs.get(\"logit_bias\", None)\n\n        # Set the default user\n        user = kwargs.get(\"user\", None)\n\n        # Set the default metadata\n        metadata = kwargs.get(\"metadata\", None"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Dynamically select a model based on provided arguments and configuration\n        model = kwargs.get(\"model\", self.model)\n        if model is None:\n            model = self.model\n        if model is None:\n            raise ValueError(\"Please provide a model or set the model property\")\n\n        # Make a request to the OpenAI API\n        try:\n            response = openai.ChatCompletion.create(model=model, **kwargs)\n        except openai.error.InvalidRequestError as e:\n            # Check if the error is due to exceeding context length\n            if \"This model's maximum context length\" in str(e):\n                # Attempt to use a higher-capacity model if available\n                if self.fallback_models:\n                    for fallback_model in self.fallback_models:\n                        try:\n                            response = openai.ChatCompletion.create(model=fallback_model, **kwargs)\n                            return response\n                        except openai.error.InvalidRequestError as e:\n                            # Check if the error is still due to exceeding context length\n                            if \"This model's maximum context length\" in str(e):\n                                continue\n                            else:\n                                raise e\n                else:\n                    # Raise an error if no higher-capacity models are available\n                    raise BadRequestError(str(e))\n            else:\n                raise e\n        return response"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Dynamically select model based on provided arguments and configuration\n        model = get_model(kwargs)\n        # Make request to OpenAI API\n        response = openai.ChatCompletion.create(model=model, **kwargs)\n        # Handle potential errors related to exceeding context length\n        if response.get(\"error\") is not None:\n            if response[\"error\"][\"code\"] == \"context_length_exceeded\":\n                # Attempt to use a higher-capacity model if available\n                fallback_models = get_fallback_models(model)\n                for fallback_model in fallback_models:\n                    try:\n                        response = openai.ChatCompletion.create(\n                            model=fallback_model, **kwargs\n                        )\n                        break\n                    except openai.error.InvalidRequestError:\n                        continue\n                else:\n                    # Raise error if no higher-capacity model can handle the request\n                    raise BadRequestError(response[\"error\"][\"message\"])\n            else:\n                # Raise error for any other API error\n                raise BadRequestError(response[\"error\"][\"message\"])\n        return response"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Set the default model to use for chat completion\n        default_model = \"gpt-3.5-turbo\"\n\n        # Get the model to use for chat completion\n        model = kwargs.get(\"model\", default_model)\n\n        # Get the configuration for the model\n        config = openai.Model.retrieve(model)\n\n        # Get the context length limit for the model\n        context_length_limit = config.max_tokens\n\n        # Get the current context length\n        context_length = len(kwargs.get(\"messages\", []))\n\n        # If the context length is less than or equal to the context length limit, make the request\n        if context_length <= context_length_limit:\n            return openai.ChatCompletion.create(**kwargs)\n\n        # If the context length is greater than the context length limit, try to use a higher-capacity model\n        for model in [\n            \"gpt-4\",\n            \"gpt-4-0314\",\n            \"gpt-4-32k\",\n            \"gpt-4-32k-0314\",\n            \"gpt-3.5-turbo-16k\",\n            \"gpt-3.5-turbo-16k-0301\",\n        ]:\n            # Get the configuration for the model\n            config = openai.Model.retrieve(model)\n\n            # Get the context length limit for the model\n            context_length_limit = config.max_tokens\n\n            # If the context length is less than or equal to the context length limit, make the request\n            if context_length <= context_length_limit:\n                return openai.ChatCompletion.create(**kwargs, model=model)\n\n        # If the context length is still greater than the context length limit and no higher-capacity models are available, raise an error\n        raise BadRequestError(\n            f\"Context length ({context_length}) is too large for the model ({model}).\"\n        )"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Get the configuration for the API request\n        config = get_config()\n\n        # Get the model to use for the chat completion\n        model = kwargs.get(\"model\", config.get(\"model\"))\n\n        # Get the fallback models to use if the primary model fails\n        fallback_models = config.get(\"fallback_models\", [])\n\n        # Get the context length limit for the chat completion\n        context_length_limit = config.get(\"context_length_limit\", 4096)\n\n        # Make the request to the OpenAI API\n        response = openai.ChatCompletion.create(\n            model=model,\n            **kwargs\n        )\n\n        # Check if the context length limit was exceeded\n        if response.get(\"error\", {}).get(\"code\") == \"context_length_exceeded\":\n            # Try using a higher-capacity model if available\n            for fallback_model in fallback_models:\n                try:\n                    response = openai.ChatCompletion.create(\n                        model=fallback_model,\n                        **kwargs\n                    )\n                    # If the request succeeds, return the response\n                    return response\n                except Exception as e:\n                    # If the request fails, continue to the next fallback model\n                    continue\n\n            # If all fallback models have been tried and none of them work, raise an error\n            raise BadRequestError(\n                f\"Context length exceeded and no fallback models available. Error: {response.get('error', {}).get('message')}\"\n            )\n\n        # If the request succeeds, return the response\n        return response\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Set the model to use for chat completion\n        model = kwargs.pop(\"model\", None)\n        if model is None:\n            model = self.model\n\n        # Check if the model is a higher-capacity model and if it is available\n        if model in self.capacity_models:\n            if not self.is_model_available(model):\n                # If the higher-capacity model is not available, fall back to the default model\n                model = self.model\n\n        # Set the request body for the API call\n        request_body = {\n            \"model\": model,\n            \"messages\": kwargs.pop(\"messages\"),\n            \"temperature\": kwargs.pop(\"temperature\", 1),\n            \"top_p\": kwargs.pop(\"top_p\", 1),\n            \"n\": kwargs.pop(\"n\", 1),\n            \"stream\": kwargs.pop(\"stream\", False),\n            \"stop\": kwargs.pop(\"stop\", None),\n            \"max_tokens\": kwargs.pop(\"max_tokens\", None),\n            \"presence_penalty\": kwargs.pop(\"presence_penalty\", 0),\n            \"frequency_penalty\": kwargs.pop(\"frequency_penalty\", 0),\n            \"logit_bias\": kwargs.pop(\"logit_bias\", None),\n            \"user\": kwargs.pop(\"user\", None),\n        }\n\n        # Make the API call and handle any errors\n        try:\n            response = self.request(\"POST\", \"/chat/completions\", json=request_body)\n        except BadRequestError as e:\n            if \"This model's maximum context length\" in str(e):\n                # If the context length limit is exceeded, try using a higher-capacity model\n                if model in self.capacity_models:\n                    # If the higher-capacity model is not available, raise an error\n                    if not self.is_model_available(model):\n                        raise BadRequestError(\n                            \"This model's maximum context length is exceeded. \"\n                            \"Please"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Initialize the model and configuration\n        model = kwargs.get('model')\n        config = kwargs.get('config')\n\n        # If the model is not provided, use the default model from the configuration\n        if model is None:\n            model = config.get('model')\n\n        # If the model is not provided and no default model is set in the configuration, use the 'gpt-3.5-turbo' model\n        if model is None:\n            model = 'gpt-3.5-turbo'\n\n        # Set the default model in the configuration\n        config['model'] = model\n\n        # Get the default model for the given model name\n        default_model = get_default_model(model)\n\n        # If the default model is not None, set the model in the configuration to the default model\n        if default_model is not None:\n            config['model'] = default_model\n\n        # Get the model settings for the given model name\n        model_settings = get_model_settings(model)\n\n        # If the model settings are not None, set the model in the configuration to the model settings\n        if model_settings is not None:\n            config['model'] = model_settings\n\n        # Set the model in the configuration to the model settings\n        config['model'] = model_settings\n\n        # Get the model settings for the given model name\n        model_settings = get_model_settings(model)\n\n        # If the model settings are not None, set the model in the configuration to the model settings\n        if model_settings is not None:\n            config['model'] = model_settings\n\n        # Set the model in the configuration to the model settings\n        config['model'] = model_settings\n\n        # Get the model settings for the given model name\n        model_settings = get_model_settings(model)\n\n        # If the model settings are not None, set the model in the configuration to the model settings\n        if model_settings is not None:\n            config['model'] = model_settings\n\n        # Set the model in the configuration to the model settings\n        config['model'] = model_settings\n\n        # Get the"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Set the default model to use if none is provided\n        if \"model\" not in kwargs:\n            kwargs[\"model\"] = \"gpt-3.5-turbo\"\n\n        # Determine the maximum number of tokens allowed in the prompt and completion\n        max_tokens = self._get_max_tokens(kwargs)\n\n        # If the prompt and completion are within the token limit, make the request\n        if max_tokens <= 4096:\n            return self._chatcompletion_request(**kwargs)\n\n        # If the prompt and completion are too long, attempt to use a higher capacity model\n        if \"gpt-3.5-turbo\" in kwargs[\"model\"]:\n            # Use the gpt-3.5-turbo-16k model if it is available\n            if self.is_model_available(\"gpt-3.5-turbo-16k\"):\n                kwargs[\"model\"] = \"gpt-3.5-turbo-16k\"\n                return self._chatcompletion_request(**kwargs)\n            else:\n                # Raise an error if the gpt-3.5-turbo-16k model is not available\n                raise BadRequestError(\n                    \"The context length is too long for the gpt-3.5-turbo model. Please try a different model.\"\n                )\n        elif \"gpt-4\" in kwargs[\"model\"]:\n            # Use the gpt-4-32k model if it is available\n            if self.is_model_available(\"gpt-4-32k\"):\n                kwargs[\"model\"] = \"gpt-4-32k\"\n                return self._chatcompletion_request(**kwargs)\n            else:\n                # Raise an error if the gpt-4-32k model is not available\n                raise BadRequestError(\n                    \"The context length is too long for the gpt-4 model. Please try a different model.\"\n                )\n        else:\n            # Raise an error if the model is not recognized\n            raise BadRequestError"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Get the current model\n        model = kwargs.get(\"model\", None)\n\n        # If no model is provided, use the default model\n        if model is None:\n            model = self.default_model\n\n        # Get the context length limit\n        context_length_limit = self.get_context_length_limit(model)\n\n        # Get the fallback models\n        fallback_models = self.get_fallback_models(model)\n\n        # Make the request\n        try:\n            response = self.request(\"chatcompletion\", **kwargs)\n\n            # Check if the context length limit was exceeded\n            if response.get(\"error\", None) and response[\"error\"].get(\"code\", None) == \"context_length\":\n                # If the context length limit was exceeded, try using a higher-capacity model\n                for fallback_model in fallback_models:\n                    # Update the request parameters with the fallback model\n                    kwargs[\"model\"] = fallback_model\n\n                    # Make the request with the fallback model\n                    response = self.request(\"chatcompletion\", **kwargs)\n\n                    # If the request was successful, return the response\n                    if not response.get(\"error\", None):\n                        return response\n\n                # If no fallback model was successful, raise an error\n                raise BadRequestError(response[\"error\"][\"message\"])\n\n            # If the request was successful, return the response\n            return response\n\n        except BadRequestError as e:\n            # If the context length limit was exceeded, try using a higher-capacity model\n            for fallback_model in fallback_models:\n                # Update the request parameters with the fallback model\n                kwargs[\"model\"] = fallback_model\n\n                # Make the request with the fallback model\n                response = self.request(\"chatcompletion\", **kwargs)\n\n                # If the request was successful, return the response\n                if not response.get(\"error\", None):\n                    return response\n\n            # If no fallback model was successful, raise the original error\n            raise e"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Select the model based on the provided arguments and configuration\n        model = select_model(**kwargs)\n\n        # Make the request to the OpenAI API\n        response = openai.ChatCompletion.create(model=model, **kwargs)\n\n        # Handle the response\n        return handle_response(response, **kwargs)\n\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Set the default model based on the configuration\n        model = kwargs.get(\"model\", config.default_model)\n\n        # Check if the model is in the list of available models\n        if model not in config.available_models:\n            raise BadRequestError(f\"Model {model} is not available\")\n\n        # Get the maximum context length for the selected model\n        max_context_length = config.max_context_lengths[model]\n\n        # Get the current context length\n        current_context_length = len(kwargs.get(\"messages\", []))\n\n        # Check if the current context length exceeds the maximum context length\n        if current_context_length > max_context_length:\n\n            # Get the list of available fallback models\n            fallback_models = config.fallback_models.get(model, [])\n\n            # Find the first available fallback model that can handle the request\n            for fallback_model in fallback_models:\n                if fallback_model in config.available_models:\n                    # Set the model to the fallback model\n                    kwargs[\"model\"] = fallback_model\n\n                    # Get the maximum context length for the fallback model\n                    max_context_length = config.max_context_lengths[fallback_model]\n\n                    # Check if the current context length exceeds the maximum context length for the fallback model\n                    if current_context_length <= max_context_length:\n                        # If the context length is within the limit, make the request using the fallback model\n                        return openai.ChatCompletion.create(**kwargs)\n\n            # If no fallback model can handle the request, raise an error\n            raise BadRequestError(\n                f\"Context length exceeded for model {model} and no fallback models available\"\n            )\n\n        # If the current context length is within the limit, make the request using the selected model\n        return openai.ChatCompletion.create(**kwargs)"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Check if the model argument is provided in the kwargs\n        if 'model' in kwargs:\n            model = kwargs['model']\n        else:\n            # If the model argument is not provided, use the default model from the configuration\n            model = config.model\n\n        # Get the maximum token limit for the selected model\n        max_tokens = get_max_tokens(model)\n\n        # Get the context length of the provided messages\n        context_length = get_context_length(kwargs['messages'])\n\n        # Check if the context length exceeds the maximum token limit\n        if context_length > max_tokens:\n            # If the context length exceeds the maximum token limit, attempt to use a higher-capacity model\n            fallback_model = get_fallback_model(model)\n\n            if fallback_model:\n                # If a higher-capacity model is available, update the model argument and make the request\n                kwargs['model'] = fallback_model\n                response = openai.ChatCompletion.create(**kwargs)\n            else:\n                # If no higher-capacity model is available, raise a BadRequestError\n                raise BadRequestError(\n                    f\"The context length of the provided messages exceeds the maximum token limit ({max_tokens}) for the selected model ({model}). Please reduce the number of messages or use a higher-capacity model.\")\n        else:\n            # If the context length does not exceed the maximum token limit, make the request with the original model\n            response = openai.ChatCompletion.create(**kwargs)\n\n        return response"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Get the configuration for the chat completion API\n        config = get_config()\n\n        # Get the API key from the configuration\n        api_key = config[\"api_key\"]\n\n        # Get the models from the configuration\n        models = config[\"models\"]\n\n        # Get the context length limit from the configuration\n        context_length_limit = config[\"context_length_limit\"]\n\n        # Get the default model from the configuration\n        default_model = config[\"default_model\"]\n\n        # Set the default model to use if no model is provided\n        model = kwargs.get(\"model\", default_model)\n\n        # Check if the model is not in the list of available models\n        if model not in models:\n            raise ValueError(f\"Model {model} not found in available models: {models}\")\n\n        # Get the model's context length limit from the configuration\n        model_context_length_limit = models[model].get(\"context_length_limit\", context_length_limit)\n\n        # Get the model's fallback models from the configuration\n        model_fallback_models = models[model].get(\"fallback_models\", [])\n\n        # Set the default fallback model to use if no fallback models are provided\n        fallback_models = kwargs.get(\"fallback_models\", model_fallback_models)\n\n        # Set the default max_tokens to use if not provided\n        max_tokens = kwargs.get(\"max_tokens\", 256)\n\n        # Set the default temperature to use if not provided\n        temperature = kwargs.get(\"temperature\", 0.5)\n\n        # Set the default top_p to use if not provided\n        top_p = kwargs.get(\"top_p\", 1)\n\n        # Set the default frequency_penalty to use if not provided\n        frequency_penalty = kwargs.get(\"frequency_penalty\", 0)\n\n        # Set the default presence_penalty to use if not provided\n        presence_penalty = kwargs.get(\"presence_penalty\", 0)\n\n       "}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Get the current model from the kwargs or use the default model\n        model = kwargs.get(\"model\", self.default_model)\n\n        # Check if the model is available and if not, raise a BadRequestError\n        if model not in self.available_models:\n            raise BadRequestError(f\"Model {model} is not available.\")\n\n        # Check if the context length is exceeding the limit\n        if self.context_length_limit_exceeded(kwargs):\n            # Get the fallback model for the current model\n            fallback_model = self.fallback_models.get(model)\n\n            # If a fallback model is available, update the model in the kwargs\n            if fallback_model:\n                kwargs[\"model\"] = fallback_model\n\n                # Check if the updated model is available and if not, raise a BadRequestError\n                if kwargs[\"model\"] not in self.available_models:\n                    raise BadRequestError(f\"Model {kwargs['model']} is not available.\")\n\n                # Make the API request with the updated model\n                response = self.request(**kwargs)\n\n                # Check if the context length is still exceeding the limit\n                if self.context_length_limit_exceeded(kwargs):\n                    raise BadRequestError(\n                        f\"Context length exceeded for model {kwargs['model']}.\"\n                    )\n\n                # Return the response\n                return response\n\n            # If no fallback model is available, raise a BadRequestError\n            raise BadRequestError(\n                f\"Context length exceeded for model {model} and no fallback available.\"\n            )\n\n        # If the context length is not exceeding the limit, make the API request with the current model\n        response = self.request(**kwargs)\n\n        # Return the response\n        return response"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Check if a model is provided in the kwargs\n        if \"model\" not in kwargs:\n            # If no model is provided, get the model from the configuration\n            model = config.get(\"model\")\n        else:\n            # If a model is provided, use it\n            model = kwargs[\"model\"]\n\n        # Check if the model is not in the model_capacities dictionary\n        if model not in model_capacities:\n            # If the model is not in the dictionary, raise a BadRequestError with a message indicating that the model is not supported\n            raise BadRequestError(f\"The model {model} is not supported\")\n\n        # Check if the context length limit is exceeded\n        if \"max_tokens\" in kwargs and kwargs[\"max_tokens\"] > model_capacities[model]:\n            # If the context length limit is exceeded, try to use a higher-capacity model\n            for higher_model in higher_capacity_models:\n                # Check if the higher-capacity model is in the model_capacities dictionary\n                if higher_model in model_capacities:\n                    # If the higher-capacity model is in the dictionary, check if the context length limit is exceeded for the higher-capacity model\n                    if \"max_tokens\" in kwargs and kwargs[\"max_tokens\"] > model_capacities[higher_model]:\n                        # If the context length limit is still exceeded for the higher-capacity model, raise a BadRequestError with a message indicating that the context length limit was exceeded\n                        raise BadRequestError(\n                            f\"The context length limit is exceeded for the model {model} and the higher capacity model {higher_model}\"\n                        )\n                    else:\n                        # If the context length limit is not exceeded for the higher-capacity model, set the model to the higher-capacity model\n                        model = higher_model\n                        break\n            else:\n                # If no higher-capacity model is available to handle the request, raise a BadRequestError with a message indicating that the context length limit was exceeded\n                raise BadRequestError(f\"The context length"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        pass\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        pass\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        pass\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if (\n            self.client_instance is None\n            or self.client_instance_creation_time is None\n            or (\n                datetime.now() - self.client_instance_creation_time\n            ).seconds > self.client_instance_creation_interval\n        ):\n            self.client_instance = boto3.client(\n                \"s3\",\n                aws_access_key_id=self.access_key_id,\n                aws_secret_access_key=self.secret_access_key,\n                region_name=self.region_name,\n            )\n            self.client_instance_creation_time = datetime.now()\n\n        return self.client_instance\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self.s3_client or (datetime.now() - self.s3_client_creation_time).total_seconds() > self.s3_client_creation_interval:\n            self.s3_client = boto3.client('s3',\n                                         aws_access_key_id=self.aws_access_key_id,\n                                         aws_secret_access_key=self.aws_secret_access_key,\n                                         region_name=self.aws_region_name)\n            self.s3_client_creation_time = datetime.now()\n        return self.s3_client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        # Check if the client instance exists and if it has expired based on the defined interval\n        if not hasattr(self, '_client') or time.time() - self._last_client_update > self._client_update_interval:\n            # Create a new S3 client instance\n            self._client = boto3.client('s3', aws_access_key_id=self._access_key_id, aws_secret_access_key=self._secret_access_key)\n            # Update the last client update time\n            self._last_client_update = time.time()\n        # Return the S3 client instance\n        return self._client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self.is_client_expired():\n            self._client = boto3.client('s3', region_name=self._region_name)\n            self._client_creation_time = time.time()\n        return self._client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not hasattr(self, 'client_instance') or (time.time() - self.client_instance['creation_time']) > self.client_instance['expiration_interval']:\n            self.client_instance = {\n                'client': boto3.client('s3'),\n                'creation_time': time.time(),\n                'expiration_interval': 3600\n            }\n        return self.client_instance['client']\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self.client_instance is None or self.expiration_time <= time.time():\n            self.client_instance = boto3.client('s3',\n                                                aws_access_key_id=self.access_key,\n                                                aws_secret_access_key=self.secret_key)\n            self.expiration_time = time.time() + self.expiration_interval\n\n        return self.client_instance\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self.client_instance is None or self.client_instance.expired:\n            self.client_instance = self.session.client('s3')\n            self.client_instance.expired = False\n            self.last_client_creation_time = time.time()\n\n        return self.client_instance\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self.s3_client is None or self.s3_client_creation_time + self.s3_client_creation_interval < datetime.datetime.now():\n            self.s3_client = boto3.client('s3')\n            self.s3_client_creation_time = datetime.datetime.now()\n\n        return self.s3_client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_created_at + self._renewal_interval < time.time():\n            self._client = boto3.client(\n                's3',\n                aws_access_key_id=self._aws_access_key_id,\n                aws_secret_access_key=self._aws_secret_access_key,\n                region_name=self._region_name,\n                endpoint_url=self._endpoint_url\n            )\n            self._last_created_at = time.time()\n        return self._client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        pass\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self.client is None or self.client_creation_time + self.client_expiration_interval < time.time():\n            self.client = boto3.client('s3')\n            self.client_creation_time = time.time()\n        return self.client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        pass\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time_created + self._credentials_expiration_interval < time.time():\n            self._client = boto3.client('s3', region_name=self._region_name)\n            self._last_time_created = time.time()\n        return self._client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not hasattr(self, 'client_instance') or self.client_instance is None or self.client_instance.expired:\n            self.client_instance = boto3.client('s3')\n            self.client_instance.expired = False\n            self.client_instance.expiration_time = datetime.now() + timedelta(minutes=5)\n        return self.client_instance\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not hasattr(self, 's3_client'):\n            self.s3_client = boto3.client('s3')\n            self.s3_client_creation_time = datetime.now()\n\n        if (datetime.now() - self.s3_client_creation_time).seconds > self.s3_client_renewal_interval:\n            self.s3_client = boto3.client('s3')\n            self.s3_client_creation_time = datetime.now()\n\n        return self.s3_client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self._client or self._client_creation_time + self._client_expiration_interval < time.time():\n            self._client = boto3.client('s3')\n            self._client_creation_time = time.time()\n\n        return self._client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        # Check if the client has expired based on the last creation time and the predefined interval\n        if self.last_creation_time is None or (\n            time.time() - self.last_creation_time\n        ) > self.CREATION_INTERVAL:\n            # Create a new S3 client\n            self.client = boto3.client(\"s3\")\n            # Update the last creation time\n            self.last_creation_time = time.time()\n        return self.client\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if dist.is_initialized() and dist.get_rank() != 0:\n            raise RuntimeError('state_dict can only be called from the main process')\n\n        state = {\n            'num_samples_yielded': num_samples_yielded,\n            'num_workers': num_workers,\n            'batch_size': batch_size,\n            'epoch': self.epoch,\n            'input_dir': self.input_dir,\n            'input_url': self.input_url,\n            'item_loader_state': self.item_loader.state_dict() if self.item_loader is not None else None,\n            'drop_last': self.drop_last,\n            'seed': self.seed,\n            'world_size': dist.get_world_size() if dist.is_initialized() else 1,\n            'shuffle': self.shuffle,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if dist.is_initialized() and dist.get_rank() != 0:\n            raise ValueError(\n                \"state_dict should only be called from the main process\"\n            )\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader.state_dict()\n            if self.item_loader is not None\n            else None,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": dist.get_world_size() if dist.is_initialized() else 1,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if dist.is_initialized() and dist.get_rank() != 0:\n            raise ValueError(\"state_dict should only be called from the main process\")\n\n        state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader.state_dict(),\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if torch.utils.data.get_worker_info() is not None:\n            raise RuntimeError('state_dict() should not be called from a DataLoader worker process')\n\n        return {\n            'num_samples_yielded': num_samples_yielded,\n            'num_workers': num_workers,\n            'batch_size': batch_size,\n            'epoch': self.epoch,\n            'input_dir': self.input_dir,\n            'input_url': self.input_url,\n            'item_loader_state': self.item_loader_state,\n            'last_batch_dropped': self.last_batch_dropped,\n            'seed': self.seed,\n            'world_size': self.world_size,\n            'shuffle': self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if dist.is_initialized() and dist.get_rank() != 0:\n            raise RuntimeError(\"state_dict() should not be called from a DataLoader worker process\")\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n        }\n        if hasattr(self, \"item_loader\"):\n            state[\"item_loader_state\"] = self.item_loader.state_dict()\n        if hasattr(self, \"last_batch_dropped\"):\n            state[\"last_batch_dropped\"] = self.last_batch_dropped\n        state[\"seed\"] = self.seed\n        state[\"world_size\"] = dist.get_world_size()\n        state[\"shuffle\"] = self.shuffle\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if torch.utils.data.get_worker_info() is not None:\n            raise RuntimeError(\n                \"state_dict should not be called from a DataLoader worker process\"\n            )\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n        }\n        if self.item_loader is not None:\n            state[\"item_loader\"] = self.item_loader.state_dict()\n        state[\"last_batch_dropped\"] = self.last_batch_dropped\n        state[\"seed\"] = self.seed\n        state[\"world_size\"] = self.world_size\n        state[\"shuffle\"] = self.shuffle\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if dist.is_initialized() and dist.get_rank() != 0:\n            raise RuntimeError(\n                \"state_dict should not be called from a DataLoader worker process\"\n            )\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader.state_dict(),\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": dist.get_world_size() if dist.is_initialized() else 1,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if dist.get_rank() != 0:\n            raise RuntimeError(\n                \"state_dict should only be called from the main process\"\n            )\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir_path\": self.input_dir_path,\n            \"input_dir_url\": self.input_dir_url,\n        }\n        if self.item_loader is not None:\n            state[\"item_loader_state\"] = self.item_loader.state_dict()\n        state[\"drop_last\"] = self.drop_last\n        state[\"seed\"] = self.seed\n        state[\"world_size\"] = dist.get_world_size()\n        state[\"shuffle\"] = self.shuffle\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if dist.is_initialized() and dist.get_rank() != 0:\n            raise RuntimeError(\n                \"state_dict() should not be called from a DataLoader worker process\"\n            )\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir_path\": self.input_dir_path,\n            \"input_dir_url\": self.input_dir_url,\n            \"item_loader_state\": self.item_loader.state_dict(),\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if dist.is_initialized() and dist.get_rank() != 0:\n            raise ValueError(\"state_dict should not be called from a DataLoader worker process\")\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader_state,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if dist.is_initialized() and dist.get_rank() != 0:\n            raise RuntimeError(\n                \"state_dict should not be called from a DataLoader worker process\"\n            )\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir_path_or_url\": self.input_dir_path_or_url,\n            \"item_loader_state\": self.item_loader_state,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": dist.get_world_size() if dist.is_initialized() else 1,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if dist.is_initialized() and dist.get_rank() != 0:\n            raise RuntimeError(\n                \"state_dict() should not be called from a DataLoader worker process\"\n            )\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n        }\n        if self.item_loader is not None:\n            state[\"item_loader_state\"] = self.item_loader.state_dict()\n        state[\"drop_last\"] = self.drop_last\n        state[\"seed\"] = self.seed\n        state[\"world_size\"] = self.world_size\n        state[\"shuffle\"] = self.shuffle\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if dist.is_initialized() and dist.get_rank() != 0:\n            raise RuntimeError(\"state_dict should only be called on the main process.\")\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path_or_url\": self.input_dir_path_or_url,\n            \"item_loader_state\": self.item_loader_state,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self.is_worker_process():\n            raise RuntimeError(\n                \"The state_dict method should only be called from the main process.\"\n            )\n\n        state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir_path\": self.input_dir_path,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader.state_dict(),\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if dist.is_initialized() and dist.get_rank() != 0:\n            raise ValueError(\n                \"state_dict should only be called on the main process.\"\n            )\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if dist.get_rank() == 0:\n            return {\n                \"num_samples_yielded\": num_samples_yielded,\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n                \"current_epoch\": self.current_epoch,\n                \"input_dir\": self.input_dir,\n                \"input_url\": self.input_url,\n                \"item_loader_state\": self.item_loader.state_dict()\n                if self.item_loader is not None\n                else None,\n                \"last_batch_dropped\": self.last_batch_dropped,\n                \"seed\": self.seed,\n                \"world_size\": self.world_size,\n                \"shuffle\": self.shuffle,\n            }\n        else:\n            raise RuntimeError(\n                \"state_dict should not be called from a DataLoader worker process\"\n            )\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self.is_worker_process:\n            raise RuntimeError('`state_dict` should not be called from a DataLoader worker process')\n\n        state = {\n            'num_samples_yielded': num_samples_yielded,\n            'num_workers': num_workers,\n            'batch_size': batch_size,\n            'epoch': self.epoch,\n            'input_dir': self.input_dir,\n            'input_url': self.input_url,\n            'item_loader_state': self.item_loader.state_dict() if hasattr(self.item_loader, 'state_dict') else None,\n            'drop_last': self.drop_last,\n            'seed': self.seed,\n            'world_size': self.world_size,\n            'shuffle': self.shuffle,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self.is_worker_process():\n            raise RuntimeError(\"state_dict should not be called from a worker process\")\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader.state_dict(),\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self.is_dl_worker:\n            raise RuntimeError(\n                \"The state_dict method should not be called from a DataLoader worker process.\"\n            )\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir_path\": self.input_dir_path,\n            \"input_url\": self.input_url,\n        }\n\n        if self.item_loader is not None:\n            state[\"item_loader_state\"] = self.item_loader.state_dict()\n\n        state[\"drop_last\"] = self.drop_last\n        state[\"seed\"] = self.seed\n        state[\"world_size\"] = self.world_size\n        state[\"shuffle\"] = self.shuffle\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self.is_dataloader_worker():\n            raise RuntimeError(\"state_dict() should not be called from a DataLoader worker process\")\n        state = dict(\n            num_samples_yielded=num_samples_yielded,\n            num_workers=num_workers,\n            batch_size=batch_size,\n            epoch=self.epoch,\n            input_dir=self.input_dir,\n            input_url=self.input_url,\n            item_loader_state=self.item_loader_state,\n            drop_last=self.drop_last,\n            seed=self.seed,\n            world_size=self.world_size,\n            shuffle=self.shuffle,\n        )\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n        self.streaming_state = state_dict[\"streaming_state\"]\n        self.index_state = state_dict[\"index_state\"]\n        self.index_state[\"start_index\"] = state_dict[\"index_state\"][\"start_index\"]\n        self.index_state[\"end_index\"] = state_dict[\"index_state\"][\"end_index\"]\n        self.index_state[\"current_index\"] = state_dict[\"index_state\"][\"current_index\"]\n        self.index_state[\"shards\"] = state_dict[\"index_state\"][\"shards\"]\n        self.index_state[\"shard_id\"] = state_dict[\"index_state\"][\"shard_id\"]\n        self.index_state[\"shard_lengths\"] = state_dict[\"index_state\"][\"shard_lengths\"]\n        self.index_state[\"shard_sizes\"] = state_dict[\"index_state\"][\"shard_sizes\"]\n        self.index_state[\"shard_reader\"] = state_dict[\"index_state\"][\"shard_reader\"]\n        self.index_state[\"shard_reader_kwargs\"] = state_dict[\"index_state\"][\"shard_reader_kwargs\"]\n        self.index_state[\"shard_metadata\"] = state_dict[\"index_state\"][\"shard_metadata\"]\n        self.index_state[\"shard_sequence\"] = state_dict[\"index_state\"][\"shard_sequence\"]\n        self.index_state[\"shard_sequence_position\"] = state_dict[\"index_state\"][\"shard_sequence_position\"]\n        self.index_state[\"shard_sequence_index\"] = state_dict[\"index_state\"][\"shard_sequence_index\"]\n        self.index_state[\"shard_sequence_cycle\"] = state_dict[\"index_state\"][\"shard_sequence_cycle\"]\n        self.index_state[\"shard_sequence_cycle_position\"] = state_dict[\"index_state\"][\"shard_sequence_cycle_position\"]\n        self.index_state[\"shard_sequence_cycle_index\"]"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n\n        # Load state from state_dict\n        self.state = state_dict['state']\n        self.index = state_dict['index']\n        self.shuffle = state_dict['shuffle']\n        self.buffer_size = state_dict['buffer_size']\n        self.buffer = state_dict['buffer']\n        self.buffer_index = state_dict['buffer_index']\n        self.num_samples = state_dict['num_samples']\n        self.num_samples_processed = state_dict['num_samples_processed']\n        self.num_samples_yielded = state_dict['num_samples_yielded']\n\n        # Reset the iterator\n        self.iterator = iter(self)"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        # Check if the state_dict is empty\n        if not state_dict:\n            return\n\n        # Load the state from the state_dict\n        self.state = state_dict['state']\n\n        # Update the current index based on the state\n        self.current_index = self.state.index\n\n        # Load the state of the shuffling function\n        self.shuffling_function.load_state_dict(state_dict['shuffling_function'])\n\n        # Load the state of the shuffling function\n        self.shuffling_function.load_state_dict(state_dict['shuffling_function'])\n\n        # Load the state of the shuffling function\n        self.shuffling_function.load_state_dict(state_dict['shuffling_function'])\n\n        # Load the state of the shuffling function\n        self.shuffling_function.load_state_dict(state_dict['shuffling_function'])\n\n        # Load the state of the shuffling function\n        self.shuffling_function.load_state_dict(state_dict['shuffling_function'])\n\n        # Load the state of the shuffling function\n        self.shuffling_function.load_state_dict(state_dict['shuffling_function'])\n\n        # Load the state of the shuffling function\n        self.shuffling_function.load_state_dict(state_dict['shuffling_function'])\n\n        # Load the state of the shuffling function\n        self.shuffling_function.load_state_dict(state_dict['shuffling_function'])\n\n        # Load the state of the shuffling function\n        self.shuffling_function.load_state_dict(state_dict['shuffling_function'])\n\n        # Load the state of the shuffling function\n        self.shuffling_function.load_state_dict(state_dict['shuffling_function'])\n\n        # Load the state of the shuffling function\n        self.shuffling_function.load_state_dict(state_dict['shuffling_function"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        # Load the state dictionary into the StreamingDataset instance\n        self.state_dict = state_dict\n\n        # Update the StreamingDataset instance based on the loaded state\n        self.update_from_state_dict()\n\n        # Resume data streaming from the loaded state\n        self.resume_data_streaming()\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        pass\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        # Iterate through the state dictionary items\n        for key, value in state_dict.items():\n            # Set the attribute of the StreamingDataset instance with the key from the state dictionary to the corresponding value\n            setattr(self, key, value)\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        # Load the state dictionary into the StreamingDataset instance\n        self.state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n        self.state_dict['_index'] = self.state_dict.get('_index', 0)\n        self.state_dict['_epoch'] = self.state_dict.get('_epoch', 0)\n        self.state_dict['_iterations'] = self.state_dict.get('_iterations', 0)\n\n        # Load the state of the streaming dataset\n        self.load_streaming_dataset_state()\n\n        # Load the state of the worker\n        self.load_worker_state()\n\n        # Load the state of the data loader\n        self.load_dataloader_state()\n\n        # Load the state of the sampler\n        self.load_sampler_state()\n\n        # Load the state of the dataset\n        self.load_dataset_state()\n\n        # Load the state of the dataloader\n        self.load_dataloader_state()\n\n        # Load the state of the dataloader\n        self.load_dataloader_state()\n\n        # Load the state of the dataloader\n        self.load_dataloader_state()\n\n        # Load the state of the dataloader\n        self.load_dataloader_state()\n\n        # Load the state of the dataloader\n        self.load_dataloader_state()\n\n        # Load the state of the dataloader\n        self.load_dataloader_state()\n\n        # Load the state of the dataloader\n        self.load_dataloader_state()\n\n        # Load the state of the dataloader\n        self.load_dataloader_state()\n\n        # Load the state of the dataloader\n        self.load_dataloader_state()\n\n        # Load the state of the dataloader\n        self.load_dataloader_state()\n\n        # Load the state of the dataloader\n        self.load_dataloader_state()\n\n        # Load the state of the dat"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        # Initialize the state dictionary if it is not provided\n        if not state_dict:\n            state_dict = {}\n\n        # Load the state dictionary\n        self.state_dict = state_dict\n\n        # Update the state of the StreamingDataset instance based on the loaded state\n        self.update_state()\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        pass\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance\n        for key, value in state_dict.items():\n            setattr(self, key, value)\n\n        # Reset the iterator to the saved position\n        self.iterator = iter(self.dataset)\n        for _ in range(self.state_dict['iterator_position']):\n            next(self.iterator)\n\n        # Set the current position to the saved position\n        self.current_position = self.state_dict['current_position']\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n\n        # Restore the state of the StreamingDataset instance based on the provided state dictionary\n        self.restore_state(state_dict)\n\n        # Reset the StreamingDataset instance to its initial state\n        self.reset()\n\n        # Set the state of the StreamingDataset instance to \"loaded\"\n        self.state = \"loaded\"\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        pass\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Update the StreamingDataset instance with the loaded state.\n        self.update_from_state_dict()\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        # Check if the state dictionary is empty\n        if not state_dict:\n            return\n\n        # Update the state of the StreamingDataset instance with the provided state dictionary\n        self.state.update(state_dict)\n\n        # If the streaming dataset is currently active, update the state of the streaming dataset\n        if self.state['streaming']:\n            self.streaming_dataset.update_state(state_dict)\n\n        # If the streaming dataset is not active, reset the state of the streaming dataset\n        else:\n            self.streaming_dataset.reset_state()\n\n        # If the streaming dataset is not active and the state dictionary contains a 'streaming' key, set the 'streaming' state to True\n        if not self.state['streaming'] and 'streaming' in state_dict:\n            self.state['streaming'] = True\n\n        # If the streaming dataset is active and the state dictionary does not contain a 'streaming' key, set the 'streaming' state to False\n        elif self.state['streaming'] and 'streaming' not in state_dict:\n            self.state['streaming'] = False\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if the state dictionary is empty\n        if not self._state_dict:\n            return\n\n        # Check if the state dictionary has the same number of workers\n        if self._state_dict['num_workers'] != self.worker_env.num_workers:\n            raise ValueError(\n                f\"State dictionary has {self._state_dict['num_workers']} workers, but StreamingDataset has {self.worker_env.num_workers} workers.\"\n            )\n\n        # Check if the state dictionary has the same shuffle flag\n        if self._state_dict['shuffle'] != self.shuffle:\n            raise ValueError(\n                f\"State dictionary has shuffle={self._state_dict['shuffle']}, but StreamingDataset has shuffle={self.shuffle}.\"\n            )\n\n        # Check if the state dictionary has the same input directory path\n        if self._state_dict['input_dir'] != self.input_dir:\n            raise ValueError(\n                f\"State dictionary has input_dir={self._state_dict['input_dir']}, but StreamingDataset has input_dir={self.input_dir}.\"\n            )\n\n        # Check if the state dictionary has the same URL\n        if self._state_dict['url'] != self.url:\n            raise ValueError(\n                f\"State dictionary has url={self._state_dict['url']}, but StreamingDataset has url={self.url}.\"\n            )\n\n        # Check if the state dictionary has the same seed\n        if self._state_dict['seed'] != self.seed:\n            raise ValueError(\n                f\"State dictionary has seed={self._state_dict['seed']}, but StreamingDataset has seed={self.seed}.\"\n            )\n\n        # Check if the state dictionary has the same item_loader state\n        if self._state_dict['item_loader_state'] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"State dictionary has item_loader_state={self._state_dict['item_loader_"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if the state dictionary is empty\n        if len(self._state_dict) == 0:\n            return\n\n        # Check if the worker_env attribute is not None\n        if self.worker_env is None:\n            raise ValueError(\n                \"worker_env is None. This should not happen. Please open an issue on GitHub.\"\n            )\n\n        # Check if the cache attribute is not None\n        if self.cache is None:\n            raise ValueError(\n                \"cache is None. This should not happen. Please open an issue on GitHub.\"\n            )\n\n        # Check if the shuffle attribute is not None\n        if self.shuffle is None:\n            raise ValueError(\n                \"shuffle is None. This should not happen. Please open an issue on GitHub.\"\n            )\n\n        # Check if the num_workers attribute is not None\n        if self.num_workers is None:\n            raise ValueError(\n                \"num_workers is None. This should not happen. Please open an issue on GitHub.\"\n            )\n\n        # Check if the input_dir attribute is not None\n        if self.input_dir is None:\n            raise ValueError(\n                \"input_dir is None. This should not happen. Please open an issue on GitHub.\"\n            )\n\n        # Check if the url attribute is not None\n        if self.url is None:\n            raise ValueError(\n                \"url is None. This should not happen. Please open an issue on GitHub.\"\n            )\n\n        # Check if the seed attribute is not None\n        if self.seed is None:\n            raise ValueError(\n                \"seed is None. This should not happen. Please open an issue on GitHub.\"\n            )\n\n        # Check if the item_loader attribute is not None\n        if self.item_loader is None:\n            raise ValueError(\n                \"item_loader is None. This should not happen. Please open an issue on GitHub.\"\n            )\n\n        # Check if the drop_last attribute is not None\n        if self.drop_last is None:\n            raise ValueError(\n                \"drop_last is None. This should"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if the state dictionary is valid\n        if self._state_dict is None:\n            raise ValueError('State dictionary is not valid.')\n\n        # Check if the state dictionary is valid\n        if not isinstance(self._state_dict, dict):\n            raise ValueError('State dictionary is not valid.')\n\n        # Check if the state dictionary is valid\n        if 'worker_env' not in self._state_dict:\n            raise ValueError('State dictionary is not valid.')\n\n        # Check if the state dictionary is valid\n        if 'cache' not in self._state_dict:\n            raise ValueError('State dictionary is not valid.')\n\n        # Check if the state dictionary is valid\n        if 'shuffle' not in self._state_dict:\n            raise ValueError('State dictionary is not valid.')\n\n        # Check if the state dictionary is valid\n        if 'num_workers' not in self._state_dict:\n            raise ValueError('State dictionary is not valid.')\n\n        # Check if the state dictionary is valid\n        if 'input_dir_path' not in self._state_dict:\n            raise ValueError('State dictionary is not valid.')\n\n        # Check if the state dictionary is valid\n        if 'url' not in self._state_dict:\n            raise ValueError('State dictionary is not valid.')\n\n        # Check if the state dictionary is valid\n        if 'seed' not in self._state_dict:\n            raise ValueError('State dictionary is not valid.')\n\n        # Check if the state dictionary is valid\n        if 'item_loader' not in self._state_dict:\n            raise ValueError('State dictionary is not valid.')\n\n        # Check if the state dictionary is valid\n        if 'drop_last' not in self._state_dict:\n            raise ValueError('State dictionary is not valid.')\n\n        # Check if the state dictionary is valid\n        if self._state_dict['worker_env'] != self.worker_env:\n            raise ValueError('State dictionary is not valid.')\n\n        # Check if the state dictionary is valid\n        if self._state_dict['cache'] != self"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if the state dictionary is empty\n        if self._state_dict is None:\n            return\n\n        # Check if the number of workers in the state dictionary matches the current number of workers\n        if self._state_dict['num_workers'] != self.worker_env.num_workers:\n            raise ValueError(\n                f\"num_workers in state dict ({self._state_dict['num_workers']}) does not match current num_workers ({self.worker_env.num_workers})\"\n            )\n\n        # Check if the shuffle flag in the state dictionary matches the current shuffle flag\n        if self._state_dict['shuffle'] != self.shuffle:\n            raise ValueError(\n                f\"shuffle in state dict ({self._state_dict['shuffle']}) does not match current shuffle ({self.shuffle})\"\n            )\n\n        # Check if the input directory path in the state dictionary matches the current input directory path\n        if self._state_dict['input_dir'] != self.cache.input_dir:\n            raise ValueError(\n                f\"input_dir in state dict ({self._state_dict['input_dir']}) does not match current input_dir ({self.cache.input_dir})\"\n            )\n\n        # Check if the URL in the state dictionary matches the current URL\n        if self._state_dict['url'] != self.cache.url:\n            raise ValueError(\n                f\"url in state dict ({self._state_dict['url']}) does not match current url ({self.cache.url})\"\n            )\n\n        # Check if the seed in the state dictionary matches the current seed\n        if self._state_dict['seed'] != self.seed:\n            raise ValueError(\n                f\"seed in state dict ({self._state_dict['seed']}) does not match current seed ({self.seed})\"\n            )\n\n        # Check if the item_loader state in the state dictionary matches the current item_loader state\n        if self._state_dict['item_loader_state'] != self.item_loader.state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if state dict is valid\n        if self._state_dict is None:\n            raise ValueError(\"State dict is None\")\n\n        # Check if state dict is valid\n        if not isinstance(self._state_dict, dict):\n            raise ValueError(\"State dict is not a dictionary\")\n\n        # Check if state dict is valid\n        if \"worker_env\" not in self._state_dict:\n            raise ValueError(\"State dict does not contain worker_env\")\n\n        # Check if state dict is valid\n        if not isinstance(self._state_dict[\"worker_env\"], dict):\n            raise ValueError(\"State dict worker_env is not a dictionary\")\n\n        # Check if state dict is valid\n        if \"cache\" not in self._state_dict:\n            raise ValueError(\"State dict does not contain cache\")\n\n        # Check if state dict is valid\n        if not isinstance(self._state_dict[\"cache\"], dict):\n            raise ValueError(\"State dict cache is not a dictionary\")\n\n        # Check if state dict is valid\n        if \"shuffle\" not in self._state_dict:\n            raise ValueError(\"State dict does not contain shuffle\")\n\n        # Check if state dict is valid\n        if not isinstance(self._state_dict[\"shuffle\"], bool):\n            raise ValueError(\"State dict shuffle is not a boolean\")\n\n        # Check if state dict is valid\n        if \"num_workers\" not in self._state_dict:\n            raise ValueError(\"State dict does not contain num_workers\")\n\n        # Check if state dict is valid\n        if not isinstance(self._state_dict[\"num_workers\"], int):\n            raise ValueError(\"State dict num_workers is not an integer\")\n\n        # Check if state dict is valid\n        if \"input_dir_path\" not in self._state_dict:\n            raise ValueError(\"State dict does not contain input_dir_path\")\n\n        # Check if state dict is valid\n        if not isinstance(self._state_dict[\"input_dir_path\"], str):\n            raise ValueError(\"State dict input_dir_path is not a string\")"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if the state dictionary is empty\n        if self._state_dict is None:\n            return\n\n        # Check if the state dictionary has a 'worker_env' key\n        if 'worker_env' in self._state_dict:\n            # Check if the 'worker_env' key in the state dictionary is not equal to the 'worker_env' attribute of the instance\n            if self._state_dict['worker_env'] != self.worker_env:\n                raise ValueError(\n                    f'worker_env mismatch: {self._state_dict[\"worker_env\"]} != {self.worker_env}'\n                )\n\n        # Check if the state dictionary has a 'cache' key\n        if 'cache' in self._state_dict:\n            # Check if the 'cache' key in the state dictionary is not equal to the 'cache' attribute of the instance\n            if self._state_dict['cache'] != self.cache:\n                raise ValueError(\n                    f'cache mismatch: {self._state_dict[\"cache\"]} != {self.cache}'\n                )\n\n        # Check if the state dictionary has a 'shuffle' key\n        if 'shuffle' in self._state_dict:\n            # Check if the 'shuffle' key in the state dictionary is not equal to the 'shuffle' attribute of the instance\n            if self._state_dict['shuffle'] != self.shuffle:\n                raise ValueError(\n                    f'shuffle mismatch: {self._state_dict[\"shuffle\"]} != {self.shuffle}'\n                )\n\n        # Check if the state dictionary has a 'num_workers' key\n        if 'num_workers' in self._state_dict:\n            # Check if the 'num_workers' key in the state dictionary is not equal to the 'num_workers' attribute of the instance\n            if self._state_dict['num_workers'] != self.num_workers:\n                raise ValueError(\n                    f'num_workers mismatch: {self._state_dict[\"num_workers\"]} != {self.num_workers}'\n                )"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if the state dictionary is empty\n        if not self._state_dict:\n            return\n\n        # Check if the state dictionary contains a 'worker_env' key\n        if 'worker_env' in self._state_dict:\n            # Check if the worker_env key in the state dictionary is not equal to the worker_env attribute of the StreamingDataset instance\n            if self._state_dict['worker_env'] != self.worker_env:\n                # Raise a ValueError with a message indicating the mismatch between the worker_env key in the state dictionary and the worker_env attribute of the StreamingDataset instance\n                raise ValueError(\n                    f'worker_env mismatch: {self._state_dict[\"worker_env\"]} != {self.worker_env}'\n                )\n\n        # Check if the state dictionary contains a 'cache' key\n        if 'cache' in self._state_dict:\n            # Check if the cache key in the state dictionary is not equal to the cache attribute of the StreamingDataset instance\n            if self._state_dict['cache'] != self.cache:\n                # Raise a ValueError with a message indicating the mismatch between the cache key in the state dictionary and the cache attribute of the StreamingDataset instance\n                raise ValueError(f'cache mismatch: {self._state_dict[\"cache\"]} != {self.cache}')\n\n        # Check if the state dictionary contains a 'shuffle' key\n        if 'shuffle' in self._state_dict:\n            # Check if the shuffle key in the state dictionary is not equal to the shuffle attribute of the StreamingDataset instance\n            if self._state_dict['shuffle'] != self.shuffle:\n                # Raise a ValueError with a message indicating the mismatch between the shuffle key in the state dictionary and the shuffle attribute of the StreamingDataset instance\n                raise ValueError(\n                    f'shuffle mismatch: {self._state_dict[\"shuffle\"]} != {self.shuffle}'\n                )\n\n        # Check if the state dictionary contains a 'num_workers' key\n        if 'num_workers' in self._"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        state_dict = self._state_dict\n        if state_dict['shuffle'] != self.shuffle:\n            raise ValueError(\n                f\"shuffle is {state_dict['shuffle']} in state dict but {self.shuffle} in StreamingDataset.\"\n            )\n        if state_dict['num_workers'] != self.num_workers:\n            raise ValueError(\n                f\"num_workers is {state_dict['num_workers']} in state dict but {self.num_workers} in StreamingDataset.\"\n            )\n        if state_dict['input_dir'] != self.input_dir:\n            raise ValueError(\n                f\"input_dir is {state_dict['input_dir']} in state dict but {self.input_dir} in StreamingDataset.\"\n            )\n        if state_dict['url'] != self.url:\n            raise ValueError(\n                f\"url is {state_dict['url']} in state dict but {self.url} in StreamingDataset.\"\n            )\n        if state_dict['seed'] != self.seed:\n            raise ValueError(\n                f\"seed is {state_dict['seed']} in state dict but {self.seed} in StreamingDataset.\"\n            )\n        if state_dict['item_loader_state'] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"item_loader state is {state_dict['item_loader_state']} in state dict but {self.item_loader.state_dict()} in StreamingDataset.\"\n            )\n        if state_dict['drop_last'] != self.drop_last:\n            raise ValueError(\n                f\"drop_last is {state_dict['drop_last']} in state dict but {self.drop_last} in StreamingDataset.\"\n            )\n        if state_dict['worker_env'] != self.worker_env:\n            raise ValueError(\n                f\"worker_env is {state_dict['worker_env']} in state dict but {self.worker_env"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if the state dictionary is empty\n        if not self._state_dict:\n            return\n\n        # Get the state dictionary of the StreamingDataset instance\n        state_dict = self._state_dict\n\n        # Check if the state dictionary is valid\n        if state_dict['version'] != self.VERSION:\n            raise ValueError(f'Version mismatch: {state_dict[\"version\"]} != {self.VERSION}')\n\n        # Check if the number of workers in the state dictionary matches the current number of workers\n        if state_dict['num_workers'] != self.num_workers:\n            raise ValueError(f'Number of workers mismatch: {state_dict[\"num_workers\"]} != {self.num_workers}')\n\n        # Check if the shuffle flag in the state dictionary matches the current shuffle flag\n        if state_dict['shuffle'] != self.shuffle:\n            raise ValueError(f'Shuffle mismatch: {state_dict[\"shuffle\"]} != {self.shuffle}')\n\n        # Check if the seed in the state dictionary matches the current seed\n        if state_dict['seed'] != self.seed:\n            raise ValueError(f'Seed mismatch: {state_dict[\"seed\"]} != {self.seed}')\n\n        # Check if the drop_last flag in the state dictionary matches the current drop_last flag\n        if state_dict['drop_last'] != self.drop_last:\n            raise ValueError(f'Drop last mismatch: {state_dict[\"drop_last\"]} != {self.drop_last}')\n\n        # Check if the input directory path in the state dictionary matches the current input directory path\n        if state_dict['input_dir'] != self.input_dir:\n            raise ValueError(f'Input directory mismatch: {state_dict[\"input_dir\"]} != {self.input_dir}')\n\n        # Check if the URL in the state dictionary matches the current URL\n        if state_dict['url'] != self.url:\n            raise ValueError(f'URL mismatch: {state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict['shuffle'] != self.shuffle:\n            raise ValueError('shuffle state mismatch')\n\n        if self._state_dict['num_workers'] != self.num_workers:\n            raise ValueError('num_workers state mismatch')\n\n        if self._state_dict['input_dir'] != self.input_dir:\n            raise ValueError('input_dir state mismatch')\n\n        if self._state_dict['url'] != self.url:\n            raise ValueError('url state mismatch')\n\n        if self._state_dict['seed'] != self.seed:\n            raise ValueError('seed state mismatch')\n\n        if self._state_dict['item_loader'] != self.item_loader.state_dict():\n            raise ValueError('item_loader state mismatch')\n\n        if self._state_dict['drop_last'] != self.drop_last:\n            raise ValueError('drop_last state mismatch')\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n\n        # check shuffle\n        if self.shuffle != self._state_dict[\"shuffle\"]:\n            raise ValueError(\n                f\"Shuffle is not the same: {self.shuffle} != {self._state_dict['shuffle']}\"\n            )\n\n        # check num_workers\n        if self.num_workers != self._state_dict[\"num_workers\"]:\n            raise ValueError(\n                f\"Num_workers is not the same: {self.num_workers} != {self._state_dict['num_workers']}\"\n            )\n\n        # check input_dir\n        if self.input_dir != self._state_dict[\"input_dir\"]:\n            raise ValueError(\n                f\"Input_dir is not the same: {self.input_dir} != {self._state_dict['input_dir']}\"\n            )\n\n        # check url\n        if self.url != self._state_dict[\"url\"]:\n            raise ValueError(f\"URL is not the same: {self.url} != {self._state_dict['url']}\")\n\n        # check seed\n        if self.seed != self._state_dict[\"seed\"]:\n            raise ValueError(f\"Seed is not the same: {self.seed} != {self._state_dict['seed']}\")\n\n        # check item_loader\n        if self.item_loader.state_dict() != self._state_dict[\"item_loader\"]:\n            raise ValueError(\n                f\"Item_loader is not the same: {self.item_loader.state_dict()} != {self._state_dict['item_loader']}\"\n            )\n\n        # check drop_last\n        if self.drop_last != self._state_dict[\"drop_last\"]:\n            raise ValueError(\n                f\"Drop_last is not the same: {self.drop_last} != {self._state_dict['drop_last']}\"\n            )\n\n        # check worker_env"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # validate the state dictionary against the current state of the instance\n        if self._state_dict['worker_env'] != self.worker_env:\n            raise ValueError(f'state_dict[\"worker_env\"] = {self._state_dict[\"worker_env\"]} != self.worker_env = {self.worker_env}')\n        if self._state_dict['cache'] != self.cache:\n            raise ValueError(f'state_dict[\"cache\"] = {self._state_dict[\"cache\"]} != self.cache = {self.cache}')\n        if self._state_dict['shuffle'] != self.shuffle:\n            raise ValueError(f'state_dict[\"shuffle\"] = {self._state_dict[\"shuffle\"]} != self.shuffle = {self.shuffle}')\n        if self._state_dict['num_workers'] != self.num_workers:\n            raise ValueError(f'state_dict[\"num_workers\"] = {self._state_dict[\"num_workers\"]} != self.num_workers = {self.num_workers}')\n        if self._state_dict['input_dir_path'] != self.input_dir_path:\n            raise ValueError(f'state_dict[\"input_dir_path\"] = {self._state_dict[\"input_dir_path\"]} != self.input_dir_path = {self.input_dir_path}')\n        if self._state_dict['url'] != self.url:\n            raise ValueError(f'state_dict[\"url\"] = {self._state_dict[\"url\"]} != self.url = {self.url}')\n        if self._state_dict['seed'] != self.seed:\n            raise ValueError(f'state_dict[\"seed\"] = {self._state_dict[\"seed\"]} != self.seed = {self.seed}')\n        if self._state_dict['item_loader_state'] != self.item_loader.state_dict():\n            raise ValueError(f'state_dict[\"item_loader_state\"] = {self"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict['shuffle'] != self.shuffle:\n            raise ValueError(\n                f'shuffle mismatch: {self._state_dict[\"shuffle\"]} != {self.shuffle}'\n            )\n        if self._state_dict['num_workers'] != self.num_workers:\n            raise ValueError(\n                f'num_workers mismatch: {self._state_dict[\"num_workers\"]} != {self.num_workers}'\n            )\n        if self._state_dict['input_dir_path'] != self.input_dir_path:\n            raise ValueError(\n                f'input_dir_path mismatch: {self._state_dict[\"input_dir_path\"]} != {self.input_dir_path}'\n            )\n        if self._state_dict['url'] != self.url:\n            raise ValueError(\n                f'url mismatch: {self._state_dict[\"url\"]} != {self.url}'\n            )\n        if self._state_dict['seed'] != self.seed:\n            raise ValueError(\n                f'seed mismatch: {self._state_dict[\"seed\"]} != {self.seed}'\n            )\n        if self._state_dict['item_loader'] != self.item_loader.state_dict():\n            raise ValueError(\n                f'item_loader mismatch: {self._state_dict[\"item_loader\"]} != {self.item_loader.state_dict()}'\n            )\n        if self._state_dict['drop_last'] != self.drop_last:\n            raise ValueError(\n                f'drop_last mismatch: {self._state_dict[\"drop_last\"]} != {self.drop_last}'\n            )\n\n        # Validate the state of the cache.\n        self.cache.validate_state()\n\n        # Validate the state of the worker environment.\n        self.worker_env.validate_state()\n\n        # Validate the state of the shuffle buffer.\n        self.shuffle_buffer.validate_state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if the state dictionary is empty\n        if not self._state_dict:\n            raise ValueError('State dictionary is empty.')\n\n        # Check if the state dictionary has the expected keys\n        expected_keys = ['shuffle', 'num_workers', 'input_dir_path', 'url', 'seed', 'item_loader_state', 'drop_last']\n        if not all(key in self._state_dict for key in expected_keys):\n            raise ValueError('State dictionary is missing expected keys.')\n\n        # Check if the state dictionary has the expected values for shuffle, num_workers, input_dir_path, url, seed, item_loader_state, and drop_last\n        if self._state_dict['shuffle'] != self.shuffle:\n            raise ValueError('State dictionary has incorrect shuffle value.')\n        if self._state_dict['num_workers'] != self.num_workers:\n            raise ValueError('State dictionary has incorrect num_workers value.')\n        if self._state_dict['input_dir_path'] != self.input_dir_path:\n            raise ValueError('State dictionary has incorrect input_dir_path value.')\n        if self._state_dict['url'] != self.url:\n            raise ValueError('State dictionary has incorrect url value.')\n        if self._state_dict['seed'] != self.seed:\n            raise ValueError('State dictionary has incorrect seed value.')\n        if self._state_dict['item_loader_state'] != self.item_loader.state_dict():\n            raise ValueError('State dictionary has incorrect item_loader_state value.')\n        if self._state_dict['drop_last'] != self.drop_last:\n            raise ValueError('State dictionary has incorrect drop_last value.')\n\n        # Check if the state dictionary has the expected keys for each worker\n        for i in range(self.num_workers):\n            worker_state_dict = self._state_dict['worker_state_dicts'][i]\n            expected_keys = ['worker_id', 'shuffle_seed', 'shuffle_index"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if the state dictionary is empty\n        if len(self._state_dict) == 0:\n            return\n\n        # Check if the state dictionary is from the same class as the current instance\n        if self._state_dict[\"class_name\"] != self.__class__.__name__:\n            raise ValueError(\n                f\"Trying to load a {self._state_dict['class_name']} checkpoint into a {self.__class__.__name__}\"\n            )\n\n        # Check if the state dictionary is from the same version of the current instance\n        if self._state_dict[\"version\"] != self.__version__:\n            raise ValueError(\n                f\"Trying to load a checkpoint from version {self._state_dict['version']} \"\n                f\"into an instance of version {self.__version__}\"\n            )\n\n        # Check if the state dictionary is from a different input directory path\n        if self._state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(\n                f\"Trying to load a checkpoint from input_dir {self._state_dict['input_dir']} \"\n                f\"into an instance with input_dir {self.input_dir}\"\n            )\n\n        # Check if the state dictionary is from a different URL\n        if self._state_dict[\"url\"] != self.url:\n            raise ValueError(\n                f\"Trying to load a checkpoint from url {self._state_dict['url']} \"\n                f\"into an instance with url {self.url}\"\n            )\n\n        # Check if the state dictionary is from a different shuffle setting\n        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"Trying to load a checkpoint from shuffle {self._state_dict['shuffle']} \"\n                f\"into an instance with shuffle {self.shuffle}\"\n            )\n\n        # Check if the state dictionary is from a different num_workers setting\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError("}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\n                \"state_dict is not set. Please call `to_state_dict()` first.\"\n            )\n\n        # check shuffle\n        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"state_dict.shuffle ({self._state_dict['shuffle']}) != \"\n                f\"self.shuffle ({self.shuffle})\"\n            )\n\n        # check num_workers\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\n                f\"state_dict.num_workers ({self._state_dict['num_workers']}) != \"\n                f\"self.num_workers ({self.num_workers})\"\n            )\n\n        # check input_dir_path\n        if self._state_dict[\"input_dir_path\"] != self.input_dir_path:\n            raise ValueError(\n                f\"state_dict.input_dir_path ({self._state_dict['input_dir_path']}) != \"\n                f\"self.input_dir_path ({self.input_dir_path})\"\n            )\n\n        # check url\n        if self._state_dict[\"url\"] != self.url:\n            raise ValueError(\n                f\"state_dict.url ({self._state_dict['url']}) != \"\n                f\"self.url ({self.url})\"\n            )\n\n        # check seed\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"state_dict.seed ({self._state_dict['seed']}) != \"\n                f\"self.seed ({self.seed})\"\n            )\n\n        # check item_loader state\n        if self._state_dict[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"state_dict.item_loader ({self._state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Get the state dictionary of the instance\n        state_dict = self._state_dict\n\n        # Check if the state dictionary is empty\n        if state_dict is None:\n            return\n\n        # Check if the state dictionary contains the same number of workers as the current instance\n        if state_dict['num_workers'] != len(self.worker_env):\n            raise ValueError('num_workers in state_dict does not match the current instance')\n\n        # Check if the state dictionary contains the same input directory path as the current instance\n        if state_dict['input_dir'] != self.input_dir:\n            raise ValueError('input_dir in state_dict does not match the current instance')\n\n        # Check if the state dictionary contains the same URL as the current instance\n        if state_dict['url'] != self.url:\n            raise ValueError('url in state_dict does not match the current instance')\n\n        # Check if the state dictionary contains the same seed as the current instance\n        if state_dict['seed'] != self.seed:\n            raise ValueError('seed in state_dict does not match the current instance')\n\n        # Check if the state dictionary contains the same item_loader state as the current instance\n        if state_dict['item_loader'] != self.item_loader:\n            raise ValueError('item_loader in state_dict does not match the current instance')\n\n        # Check if the state dictionary contains the same drop_last flag as the current instance\n        if state_dict['drop_last'] != self.drop_last:\n            raise ValueError('drop_last in state_dict does not match the current instance')\n\n        # Check if the state dictionary contains the same shuffle flag as the current instance\n        if state_dict['shuffle'] != self.shuffle:\n            raise ValueError('shuffle in state_dict does not match the current instance')\n\n        # Check if the state dictionary contains the same cache flag as the current instance\n        if state_dict['cache'] != self.cache:\n            raise ValueError('cache in state_dict does not match the current instance')\n\n        # Check if the state dictionary contains the"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        state_dict = self._state_dict\n        if state_dict['shuffle'] != self.shuffle:\n            raise ValueError('shuffle mismatch')\n        if state_dict['num_workers'] != self.num_workers:\n            raise ValueError('num_workers mismatch')\n        if state_dict['input_dir'] != self.input_dir:\n            raise ValueError('input_dir mismatch')\n        if state_dict['url'] != self.url:\n            raise ValueError('url mismatch')\n        if state_dict['seed'] != self.seed:\n            raise ValueError('seed mismatch')\n        if state_dict['item_loader_state'] != self.item_loader.state_dict():\n            raise ValueError('item_loader_state mismatch')\n        if state_dict['drop_last'] != self.drop_last:\n            raise ValueError('drop_last mismatch')\n\n        if self.worker_env is not None:\n            if state_dict['worker_env'] != self.worker_env.state_dict():\n                raise ValueError('worker_env mismatch')\n\n        if self.cache is not None:\n            if state_dict['cache'] != self.cache.state_dict():\n                raise ValueError('cache mismatch')\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check that the state dict is valid\n        if self._state_dict is None:\n            raise ValueError(\n                \"State dict is None. This is probably because you are trying to load a dataset that was created with a different version of the streaming library.\"\n            )\n        if self._state_dict[\"version\"] != self.VERSION:\n            raise ValueError(\n                f\"State dict version {self._state_dict['version']} does not match current version {self.VERSION}\"\n            )\n        if self._state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(\n                f\"State dict input dir {self._state_dict['input_dir']} does not match current input dir {self.input_dir}\"\n            )\n        if self._state_dict[\"input_url\"] != self.input_url:\n            raise ValueError(\n                f\"State dict input url {self._state_dict['input_url']} does not match current input url {self.input_url}\"\n            )\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"State dict seed {self._state_dict['seed']} does not match current seed {self.seed}\"\n            )\n        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"State dict shuffle {self._state_dict['shuffle']} does not match current shuffle {self.shuffle}\"\n            )\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\n                f\"State dict num_workers {self._state_dict['num_workers']} does not match current num_workers {self.num_workers}\"\n            )\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"State dict drop_last {self._state_dict['drop_last']} does not match current drop_last {self.drop_last}\"\n            )\n        if self._state_dict[\"item_loader"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if the state dictionary is empty\n        if not self._state_dict:\n            raise ValueError(\"State dictionary is empty.\")\n\n        # Check if the state dictionary matches the current state of the instance\n        if self._state_dict[\"worker_env\"] != self.worker_env:\n            raise ValueError(\"State dictionary worker environment does not match the current worker environment.\")\n        if self._state_dict[\"cache\"] != self.cache:\n            raise ValueError(\"State dictionary cache does not match the current cache.\")\n        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\"State dictionary shuffle does not match the current shuffle.\")\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\"State dictionary num_workers does not match the current num_workers.\")\n        if self._state_dict[\"input_dir_path\"] != self.input_dir_path:\n            raise ValueError(\"State dictionary input_dir_path does not match the current input_dir_path.\")\n        if self._state_dict[\"url\"] != self.url:\n            raise ValueError(\"State dictionary URL does not match the current URL.\")\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\"State dictionary seed does not match the current seed.\")\n        if self._state_dict[\"item_loader_state\"] != self.item_loader.state_dict():\n            raise ValueError(\"State dictionary item_loader_state does not match the current item_loader state.\")\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\"State dictionary drop_last does not match the current drop_last.\")\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # If input_dir is None, use an empty string for hashing\n    input_dir = input_dir or \"\"\n\n    # Hash the input directory\n    hashed_dir = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Get the cache directory from the environment variable\n    cache_dir = os.environ.get(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\"))\n\n    # Create the cache directory if it does not exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Join the cache directory with the hashed directory\n    cache_dir = os.path.join(cache_dir, hashed_dir)\n\n    # Create the cache directory if it does not exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name based on the input directory\n    cache_dir = os.path.join(\n        os.environ.get(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\")),\n        \"huggingface\",\n        \"transformers\",\n        hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest(),\n    )\n\n    try:\n        # Create the cache directory\n        os.makedirs(cache_dir, exist_ok=True)\n    except Exception:\n        # If the directory cannot be created, return None\n        return None\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if not input_dir:\n        input_dir = \"\"\n\n    cache_dir_name = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n\n    if os.getenv(\"XDG_CACHE_HOME\"):\n        cache_dir = os.path.join(os.getenv(\"XDG_CACHE_HOME\"), \"yt_dlp\", cache_dir_name)\n    elif os.getenv(\"LOCALAPPDATA\"):\n        cache_dir = os.path.join(os.getenv(\"LOCALAPPDATA\"), \"yt-dlp\", cache_dir_name)\n    else:\n        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"yt-dlp\", cache_dir_name)\n\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except OSError:\n        return None\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # Import the necessary modules\n    import os\n    import hashlib\n    import tempfile\n\n    # Set the default cache directory path\n    default_cache_dir = os.path.join(tempfile.gettempdir(), \"cache\")\n\n    # If the input directory is not provided, use an empty string for hashing\n    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique cache directory name by hashing the input directory\n    cache_dir_name = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # If certain environment variables are set, use them to create the cache directory\n    if \"CACHE_DIR\" in os.environ:\n        cache_dir = os.path.join(os.environ[\"CACHE_DIR\"], cache_dir_name)\n    elif \"XDG_CACHE_HOME\" in os.environ:\n        cache_dir = os.path.join(os.environ[\"XDG_CACHE_HOME\"], \"cache\", cache_dir_name)\n    else:\n        # If the environment variables are not set, use the default cache directory\n        cache_dir = os.path.join(default_cache_dir, cache_dir_name)\n\n    # Create the cache directory if it does not exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Return the path of the created cache directory\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    try:\n        # Get the cache directory from the environment variable\n        cache_dir = os.environ.get(\"CACHE_DIR\")\n\n        # If the cache directory is not set, use a default location\n        if cache_dir is None:\n            cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"my_app\")\n\n        # Create the cache directory if it doesn't exist\n        os.makedirs(cache_dir, exist_ok=True)\n\n        # Hash the input directory to create a unique cache directory name\n        hashed_input_dir = hashlib.sha256(input_dir.encode()).hexdigest()\n        cache_dir_name = os.path.join(cache_dir, hashed_input_dir)\n\n        # Create the cache directory\n        os.makedirs(cache_dir_name, exist_ok=True)\n\n        return cache_dir_name\n    except Exception as e:\n        # Handle any exceptions that occur during directory creation\n        logging.error(f\"Error creating cache directory: {e}\")\n        return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # Hash the input directory to create a unique cache directory name\n    hashed_input_dir = hashlib.md5(input_dir.encode('utf-8')).hexdigest()\n    cache_dir_name = f\"cache_{hashed_input_dir}\"\n\n    # Check if the environment variables are set\n    if \"XDG_CACHE_HOME\" in os.environ:\n        cache_home = os.environ[\"XDG_CACHE_HOME\"]\n    elif \"HOME\" in os.environ:\n        cache_home = os.path.join(os.environ[\"HOME\"], \".cache\")\n    else:\n        cache_home = os.path.join(os.getcwd(), \".cache\")\n\n    # Create the cache directory\n    cache_dir = os.path.join(cache_home, cache_dir_name)\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except OSError:\n        return None\n\n    return cache_dir\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # Generate a unique directory name based on the input directory\n    cache_dir = os.path.join(os.getenv(\"CACHE_DIR\", \"/tmp\"), hashlib.sha256(input_dir.encode()).hexdigest())\n\n    # Create the cache directory if it doesn't exist\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except Exception as e:\n        logger.warning(f\"Unable to create cache directory {cache_dir}: {e}\")\n        return None\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # If the input directory is not provided, use an empty string for hashing\n    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name based on the input directory\n    hashed_dir = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # If certain environment variables are not set, create the cache directory in a default location\n    if \"XDG_CACHE_HOME\" not in os.environ or \"XDG_RUNTIME_DIR\" not in os.environ:\n        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"my_program\", hashed_dir)\n    else:\n        cache_dir = os.path.join(os.environ[\"XDG_CACHE_HOME\"], \"my_program\", hashed_dir)\n\n    # Create the cache directory if it doesn't exist\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except OSError as e:\n        # Handle any errors that might occur during directory creation\n        print(f\"Error creating cache directory: {e}\")\n        return None\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # Check if the input directory is None, and if so, set it to an empty string\n    input_dir = input_dir or \"\"\n\n    # Create a unique directory name by hashing the input directory\n    cache_dir_name = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Get the cache directory path from environment variables\n    cache_dir = os.getenv(\"CACHE_DIR\", None)\n\n    # If the cache directory is not set, create it in a default location\n    if cache_dir is None:\n        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"mypackage\")\n\n    # Create the cache directory\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Return the path of the created cache directory\n    return os.path.join(cache_dir, cache_dir_name)\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # If the input directory is not provided, use an empty string for hashing\n    input_dir = input_dir or \"\"\n\n    # Generate a unique cache directory name based on the input directory\n    cache_dir_name = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # If the environment variable is set, use it as the cache directory\n    if os.environ.get(\"XDG_CACHE_HOME\"):\n        cache_dir = os.path.join(os.environ[\"XDG_CACHE_HOME\"], \"huggingface_hub\", cache_dir_name)\n    # If the environment variable is not set, use a default cache directory\n    else:\n        cache_dir = os.path.join(\n            os.path.expanduser(\"~\"), \".cache\", \"huggingface_hub\", cache_dir_name\n        )\n\n    # Create the cache directory if it doesn't exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Return the path of the created cache directory\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # If input_dir is None, use an empty string for hashing\n    input_dir = input_dir or \"\"\n\n    # Generate a unique directory name based on the input directory\n    cache_dir = f\"{hashlib.sha256(input_dir.encode('utf-8')).hexdigest()}\"\n\n    # If the environment variable 'XDG_CACHE_HOME' is set, use it as the base directory for the cache\n    cache_base = os.environ.get(\"XDG_CACHE_HOME\")\n\n    # If 'XDG_CACHE_HOME' is not set, use the default cache directory based on the operating system\n    if not cache_base:\n        if sys.platform == \"win32\":\n            cache_base = os.environ.get(\"LOCALAPPDATA\", os.path.expanduser(\"~\"))\n        elif sys.platform == \"darwin\":\n            cache_base = os.path.expanduser(\"~/Library/Caches\")\n        else:\n            cache_base = os.getenv(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\"))\n\n    # Create the cache directory\n    cache_dir = os.path.join(cache_base, \"transformers\", cache_dir)\n\n    # Try to create the cache directory\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except OSError:\n        # If the directory cannot be created, return None\n        return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    cache_dir = os.path.join(\n        os.getenv(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\")),\n        \"pysradb\",\n        hashlib.md5(input_dir.encode(\"utf-8\")).hexdigest(),\n    )\n    try:\n        os.makedirs(cache_dir)\n        return cache_dir\n    except OSError:\n        return None\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # If input_dir is None, use an empty string for hashing\n    input_dir = input_dir or \"\"\n\n    # Create a unique cache directory name by hashing the input directory\n    cache_dir_name = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Check if the environment variable 'XDG_CACHE_HOME' is set\n    if \"XDG_CACHE_HOME\" in os.environ:\n        # If set, use the value as the base directory for creating the cache directory\n        cache_base_dir = os.environ[\"XDG_CACHE_HOME\"]\n    else:\n        # If not set, use the default location '~/.cache' as the base directory\n        cache_base_dir = os.path.expanduser(\"~/.cache\")\n\n    # Create the cache directory path by combining the base directory and the unique cache directory name\n    cache_dir = os.path.join(cache_base_dir, cache_dir_name)\n\n    # Try to create the cache directory\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except OSError:\n        # If creation fails, return None\n        return None\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # Create a unique directory name based on the input directory\n    cache_dir = os.path.join(\n        os.getenv(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\")),\n        \"pypi_cli\",\n        hashlib.sha256(input_dir.encode()).hexdigest(),\n    )\n\n    # Create the cache directory\n    try:\n        os.makedirs(cache_dir)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # Generate a unique directory name based on the input directory\n    cache_dir = hashlib.sha1(input_dir.encode(\"utf-8\")).hexdigest()\n\n    # Check if the environment variables are set\n    if \"XDG_CACHE_HOME\" in os.environ:\n        # If XDG_CACHE_HOME is set, use it as the base directory for the cache\n        base_dir = os.environ[\"XDG_CACHE_HOME\"]\n    elif \"HOME\" in os.environ:\n        # If HOME is set, use it as the base directory for the cache\n        base_dir = os.path.join(os.environ[\"HOME\"], \".cache\")\n    else:\n        # If neither XDG_CACHE_HOME nor HOME are set, use the current working directory as the base directory\n        base_dir = os.getcwd()\n\n    # Create the cache directory\n    cache_dir = os.path.join(base_dir, \"pre-commit\", \"mirrors\", cache_dir)\n    try:\n        os.makedirs(cache_dir)\n    except OSError:\n        pass\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    cache_dir = os.path.join(\n        os.environ.get(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\")),\n        \"pypdf\",\n        hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest(),\n    )\n\n    try:\n        os.makedirs(cache_dir)\n    except OSError:\n        return None\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    try:\n        input_dir = input_dir or \"\"\n        cache_dir = os.path.join(\n            os.environ.get(\"XDG_CACHE_HOME\", \"~/.cache\"),\n            \"python-input-validator\",\n            hashlib.md5(input_dir.encode()).hexdigest(),\n        )\n        Path(cache_dir).mkdir(parents=True, exist_ok=True)\n        return cache_dir\n    except Exception as e:\n        print(f\"Error creating cache directory: {e}\")\n        return None\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # If input_dir is None, set it to an empty string\n    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name by hashing the input directory\n    cache_dir_name = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n\n    # If the environment variable 'XDG_CACHE_HOME' is set, use it as the base directory for the cache\n    if \"XDG_CACHE_HOME\" in os.environ:\n        cache_dir = os.path.join(os.environ[\"XDG_CACHE_HOME\"], \"jax_md\", cache_dir_name)\n    # If the environment variable 'HOME' is set, use it as the base directory for the cache\n    elif \"HOME\" in os.environ:\n        cache_dir = os.path.join(os.environ[\"HOME\"], \".cache\", \"jax_md\", cache_dir_name)\n    # Otherwise, use the current working directory as the base directory for the cache\n    else:\n        cache_dir = os.path.join(os.getcwd(), \".cache\", \"jax_md\", cache_dir_name)\n\n    # Try to create the cache directory\n    try:\n        os.makedirs(cache_dir)\n    # If the directory already exists, do nothing\n    except FileExistsError:\n        pass\n    # If any other error occurs, print an error message and return None\n    except:\n        print(\"Error creating cache directory:\", cache_dir)\n        return None\n\n    # Return the path of the created cache directory\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    cache_dir = None\n    if input_dir is None:\n        input_dir = \"\"\n    input_dir_hash = hashlib.md5(input_dir.encode()).hexdigest()\n    if \"CACHE_DIR\" in os.environ:\n        cache_dir = os.path.join(os.environ[\"CACHE_DIR\"], input_dir_hash)\n    else:\n        cache_dir = os.path.join(\n            os.path.expanduser(\"~\"), \".cache\", \"jaxformer\", input_dir_hash\n        )\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except Exception:\n        cache_dir = None\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # Use the input directory or an empty string for hashing\n    dir_to_hash = input_dir or \"\"\n\n    # Generate a unique cache directory name using the input directory\n    cache_dir = f\"{hashlib.sha256(dir_to_hash.encode()).hexdigest()}\"\n\n    # Check if the environment variable is set\n    if \"CACHE_DIR\" in os.environ:\n        # Use the specified cache directory\n        cache_dir = os.path.join(os.environ[\"CACHE_DIR\"], cache_dir)\n    else:\n        # Use the default cache directory\n        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"gpt_index\", cache_dir)\n\n    # Try to create the cache directory\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except Exception as e:\n        # Handle the exception if the directory cannot be created\n        print(f\"Error creating cache directory: {e}\")\n        return None\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        pass\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        pass\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        pass\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        pass\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Parse the S3 URL to get the bucket and key\n        parsed_url = urlparse(remote_filepath)\n        bucket_name = parsed_url.netloc\n        key = parsed_url.path.lstrip('/')\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            print(f\"File {local_filepath} already exists. Skipping download.\")\n            return\n\n        # Acquire a file lock to prevent multiple processes from downloading the same file simultaneously\n        with FileLock(local_filepath + '.lock'):\n            # Download the file using s5cmd if available\n            if shutil.which('s5cmd'):\n                subprocess.run(['s5cmd', '--no-sign-request', 'cp', remote_filepath, local_filepath])\n            else:\n                # Download the file using boto3\n                s3 = boto3.client('s3')\n                s3.download_file(bucket_name, key, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        parsed_url = urlparse(remote_filepath)\n        if parsed_url.scheme != 's3':\n            raise ValueError(f'Invalid S3 URL: {remote_filepath}')\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            print(f'File already exists: {local_filepath}')\n            return\n\n        # Acquire a file lock to prevent multiple processes from downloading the same file simultaneously\n        with FileLock(f'{local_filepath}.lock'):\n\n            # Try to download the file using s5cmd if it is available\n            if shutil.which('s5cmd'):\n                subprocess.run(['s5cmd', 'cp', remote_filepath, local_filepath], check=True)\n                return\n\n            # If s5cmd is not available, use boto3 to download the file\n            s3 = boto3.client('s3')\n            bucket_name = parsed_url.netloc\n            key_name = parsed_url.path.lstrip('/')\n            s3.download_file(bucket_name, key_name, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        pass\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\"The remote file path must be an S3 URL.\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Parse the S3 URL\n        parsed_url = urlparse(remote_filepath)\n        bucket_name = parsed_url.netloc\n        key_name = parsed_url.path.lstrip(\"/\")\n\n        # Acquire a file lock to prevent multiple processes from downloading the same file simultaneously\n        with FileLock(f\"{local_filepath}.lock\", timeout=10):\n            # Try to download the file using s5cmd if available\n            if shutil.which(\"s5cmd\"):\n                subprocess.run(\n                    [\"s5cmd\", \"--no-sign-request\", \"cp\", remote_filepath, local_filepath],\n                    check=True,\n                )\n            else:\n                # Otherwise, use boto3 to download the file\n                s3 = boto3.client(\"s3\")\n                s3.download_file(bucket_name, key_name, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        pass\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\"remote_filepath must be an S3 URL\")\n\n        if os.path.exists(local_filepath):\n            print(f\"File {local_filepath} already exists, skipping download\")\n            return\n\n        with FileLock(f\"{local_filepath}.lock\", timeout=120):\n            try:\n                if shutil.which(\"s5cmd\"):\n                    subprocess.check_call(\n                        [\n                            \"s5cmd\",\n                            \"--no-sign-request\",\n                            \"cp\",\n                            remote_filepath,\n                            local_filepath,\n                        ]\n                    )\n                else:\n                    import boto3\n\n                    s3 = boto3.client(\"s3\")\n                    parsed_url = urlparse(remote_filepath)\n                    bucket_name = parsed_url.netloc\n                    key = parsed_url.path.lstrip(\"/\")\n                    with open(local_filepath, \"wb\") as f:\n                        s3.download_fileobj(bucket_name, key, f)\n            except Exception as e:\n                print(f\"Error downloading file {remote_filepath}: {e}\")\n                raise e\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_url = urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(\n                f\"Expected s3:// URL, got: {remote_filepath}\"\n            )\n\n        if os.path.exists(local_filepath):\n            return\n\n        s3_client = boto3.client(\"s3\")\n        bucket_name = parsed_url.netloc\n        object_key = parsed_url.path.lstrip(\"/\")\n        with FileLock(str(local_filepath) + \".lock\"):\n            if shutil.which(\"s5cmd\"):\n                cmd = f\"s5cmd --no-sign-request cp s3://{bucket_name}/{object_key} {local_filepath}\"\n                subprocess.run(cmd.split(), check=True)\n            else:\n                s3_client.download_file(bucket_name, object_key, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        scheme, netloc, path = parse_s3_url(remote_filepath)\n        if scheme != 's3':\n            raise ValueError(f'Invalid S3 URL: {remote_filepath}')\n\n        if os.path.exists(local_filepath):\n            return\n\n        if not os.path.exists(os.path.dirname(local_filepath)):\n            os.makedirs(os.path.dirname(local_filepath))\n\n        lock_file = local_filepath + '.lock'\n        with FileLock(lock_file, timeout=600):\n            if shutil.which('s5cmd'):\n                subprocess.run(['s5cmd', 'cp', remote_filepath, local_filepath], check=True)\n            else:\n                s3 = boto3.client('s3')\n                s3.download_file(netloc, path[1:], local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_url = urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(\"remote_filepath must be an S3 URL\")\n        if os.path.exists(local_filepath):\n            return\n\n        lock = FileLock(f\"{local_filepath}.lock\")\n        with lock.acquire(timeout=10):\n            if os.path.exists(local_filepath):\n                return\n\n            if shutil.which(\"s5cmd\"):\n                cmd = [\"s5cmd\", \"--no-sign-request\", \"cp\", remote_filepath, local_filepath]\n            else:\n                s3 = boto3.client(\"s3\")\n                bucket, key = parsed_url.netloc, parsed_url.path.lstrip(\"/\")\n                s3.download_file(bucket, key, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Parse the remote file path\n        parsed_url = urlparse(remote_filepath)\n\n        # Check if the remote file path is an S3 URL\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(\"Invalid S3 URL\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Create a file lock to prevent multiple processes from downloading the same file simultaneously\n        lock_filepath = f\"{local_filepath}.lock\"\n        lock = FileLock(lock_filepath, timeout=10)\n\n        # Acquire the file lock\n        with lock:\n\n            # Check if the local file still does not exist (another process may have downloaded the file while waiting for the lock)\n            if not os.path.exists(local_filepath):\n\n                # Check if the s5cmd command-line tool is available\n                s5cmd_path = shutil.which(\"s5cmd\")\n                if s5cmd_path is not None:\n\n                    # Use the s5cmd command-line tool to download the file\n                    s5cmd_command = [s5cmd_path, \"--no-sign-request\", \"cp\", remote_filepath, local_filepath]\n                    subprocess.run(s5cmd_command, check=True)\n\n                else:\n\n                    # Use the boto3 library to download the file\n                    s3_client = boto3.client(\"s3\")\n                    bucket_name = parsed_url.netloc\n                    object_key = parsed_url.path.lstrip(\"/\")\n                    s3_client.download_file(bucket_name, object_key, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        from urllib.parse import urlparse\n        import subprocess\n        import os\n        import boto3\n        import time\n        import threading\n\n        # Parse the S3 URL\n        parsed_url = urlparse(remote_filepath)\n\n        # Check if the scheme is \"s3\"\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(\"The remote file path must use the 's3' scheme.\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Create a file lock to prevent multiple processes from downloading the same file simultaneously\n        file_lock = threading.Lock()\n        file_lock.acquire()\n\n        # Try to download the file using s5cmd\n        try:\n            subprocess.run(\n                [\"s5cmd\", \"cp\", remote_filepath, local_filepath],\n                check=True,\n                timeout=600,\n            )\n        except (subprocess.CalledProcessError, subprocess.TimeoutExpired):\n            # If s5cmd fails, try to download the file using boto3\n            s3 = boto3.client(\"s3\")\n            bucket_name = parsed_url.netloc\n            object_key = parsed_url.path.lstrip(\"/\")\n            s3.download_file(bucket_name, object_key, local_filepath)\n\n        # Release the file lock\n        file_lock.release()\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_url = urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(f\"Invalid S3 URL: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            logging.info(f\"File {local_filepath} already exists, skipping download.\")\n            return\n\n        with FileLock(f\"{local_filepath}.lock\"):\n            if shutil.which(\"s5cmd\"):\n                cmd = [\"s5cmd\", \"cp\", remote_filepath, local_filepath]\n                logging.info(f\"Downloading {remote_filepath} to {local_filepath} using s5cmd\")\n                subprocess.run(cmd, check=True)\n            else:\n                logging.info(\n                    f\"Downloading {remote_filepath} to {local_filepath} using boto3\"\n                )\n                s3 = boto3.client(\"s3\")\n                s3.download_file(parsed_url.netloc, parsed_url.path[1:], local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Parse the S3 URL to get the bucket name and key\n        parsed_url = urlparse(remote_filepath)\n        bucket_name = parsed_url.netloc\n        key = parsed_url.path.lstrip('/')\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            logging.info(f\"File {local_filepath} already exists, skipping download.\")\n            return\n\n        # Acquire a file lock to prevent multiple processes from downloading the same file simultaneously\n        with FileLock(f\"{local_filepath}.lock\"):\n\n            # Try to download the file using s5cmd if available\n            if shutil.which(\"s5cmd\"):\n                cmd = f\"s5cmd --no-sign-request cp {remote_filepath} {local_filepath}\"\n                subprocess.run(cmd, shell=True, check=True)\n                return\n\n            # If s5cmd is not available, use boto3 to download the file\n            s3 = boto3.client(\"s3\")\n            s3.download_file(bucket_name, key, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        scheme, netloc, path = self.parse_s3_url(remote_filepath)\n\n        if os.path.exists(local_filepath):\n            return\n\n        with FileLock(local_filepath + \".lock\"):\n            try:\n                self.download_file_with_s5cmd(remote_filepath, local_filepath)\n            except Timeout:\n                self.download_file_with_boto3(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        from urllib.parse import urlparse\n        import os\n        import subprocess\n        import time\n        import boto3\n        import threading\n\n        # Parse the remote file path and check if it uses the \"s3\" scheme\n        parsed_url = urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(\"The remote file path does not use the 's3' scheme.\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Create a file lock to prevent multiple processes from downloading the same file simultaneously\n        lock = threading.Lock()\n        lock_acquired = lock.acquire(timeout=120)\n        if not lock_acquired:\n            raise TimeoutError(\"Could not acquire file lock within 120 seconds.\")\n\n        # Attempt to download the file using s5cmd if it is available\n        s5cmd_path = os.environ.get(\"S5CMD_PATH\", \"s5cmd\")\n        if shutil.which(s5cmd_path):\n            s5cmd_command = [\n                s5cmd_path,\n                \"--no-sign-request\",\n                \"cp\",\n                remote_filepath,\n                local_filepath,\n            ]\n            subprocess.run(s5cmd_command, check=True)\n\n        # If s5cmd is not available, use boto3 to download the file\n        else:\n            s3 = boto3.client(\"s3\")\n            bucket_name = parsed_url.netloc\n            key_name = parsed_url.path.lstrip(\"/\")\n            s3.download_file(bucket_name, key_name, local_filepath)\n\n        # Release the file lock\n        lock.release()\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the given remote file path is an S3 URL\n        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\"Invalid S3 URL\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Parse the S3 URL to get the bucket name and object key\n        parsed_url = urlparse(remote_filepath)\n        bucket_name = parsed_url.netloc\n        object_key = parsed_url.path[1:]\n\n        # Acquire a file lock to prevent multiple processes from downloading the same file simultaneously\n        lock_filepath = local_filepath + \".lock\"\n        with FileLock(lock_filepath, timeout=60):\n            # Try to download the file using s5cmd if it is available\n            try:\n                subprocess.check_call([\"s5cmd\", \"--no-sign-request\", \"cp\", remote_filepath, local_filepath])\n            except FileNotFoundError:\n                # If s5cmd is not available, use boto3 to download the file\n                s3 = boto3.client(\"s3\")\n                s3.download_file(bucket_name, object_key, local_filepath)\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:  # single-process data loading, return the full iterator\n        iter_start = start\n        iter_end = end\n        return iter(range(iter_start, iter_end))\n    else:\n        worker_id = worker_info.id\n        num_workers = worker_info.num_workers\n        per_worker = int(math.ceil((end - start) / float(num_workers)))\n        worker_id = worker_info.id\n        iter_start = start + worker_id * per_worker\n        iter_end = min(iter_start + per_worker, end)\n        return iter(range(iter_start, iter_end))\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Get the world size from the worker environment\n    world_size = worker_env.world_size\n\n    # Calculate the number of chunks per worker\n    chunks_per_worker = len(chunks_replica) // world_size\n\n    # Initialize dictionaries to store the assigned chunks and intervals for each worker\n    assigned_chunks = {}\n    assigned_intervals = {}\n\n    # Iterate over the range of worker indices\n    for worker_id in range(world_size):\n        # Calculate the start and end indices for the chunks assigned to the current worker\n        start_index = worker_id * chunks_per_worker\n        end_index = (worker_id + 1) * chunks_per_worker\n\n        # Assign the chunks and intervals to the current worker\n        assigned_chunks[worker_id] = chunks_replica[start_index:end_index]\n        assigned_intervals[worker_id] = intervals_replica[start_index:end_index]\n\n    # Return the assigned chunks and intervals for each worker\n    return assigned_chunks, assigned_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    world_size = worker_env.world_size\n    if world_size == 1:\n        return {0: chunks_replica}, {0: intervals_replica}\n    chunks_per_worker = max(len(chunks_replica) // (world_size - 1), 1)\n    chunks_replica = chunks_replica[\n        worker_env.rank * chunks_per_worker : (worker_env.rank + 1) * chunks_per_worker\n    ]\n    intervals_replica = intervals_replica[\n        worker_env.rank * chunks_per_worker : (worker_env.rank + 1) * chunks_per_worker\n    ]\n    return {worker_env.rank: chunks_replica}, {worker_env.rank: intervals_replica}\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Define the distribution strategy based on the worker's index and the total world size\n    def _distribute_chunks(worker_id: int, num_workers: int, worker_env: _WorkerEnv) -> List[int]:\n        return list(range(worker_id, len(chunks_replica), num_workers))\n\n    # Create a dictionary to store the chunks assigned to each worker\n    chunks_assigned = {worker_id: [] for worker_id in range(num_workers)}\n\n    # Create a dictionary to store the intervals assigned to each worker\n    intervals_assigned = {worker_id: [] for worker_id in range(num_workers)}\n\n    # Assign chunks and intervals to each worker based on the distribution strategy\n    for worker_id in range(num_workers):\n        chunks_indices = _distribute_chunks(worker_id, num_workers, worker_env)\n        chunks_assigned[worker_id] = [chunks_replica[i] for i in chunks_indices]\n        intervals_assigned[worker_id] = [intervals_replica[i] for i in chunks_indices]\n\n    # Return the assigned chunks and intervals as dictionaries\n    return chunks_assigned, intervals_assigned"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize dictionaries to store the assigned chunks and intervals for each worker\n    chunks_assigned = {i: [] for i in range(num_workers)}\n    intervals_assigned = {i: [] for i in range(num_workers)}\n\n    # Calculate the total number of chunks\n    num_chunks = len(chunks_replica)\n\n    # Determine the chunk distribution strategy based on the worker's index and the total world size\n    if worker_env.rank == 0:\n        # If the worker's rank is 0, assign all chunks to the first worker\n        chunks_assigned[0] = chunks_replica\n        intervals_assigned[0] = intervals_replica\n    else:\n        # For other workers, assign chunks based on the worker's index and the total world size\n        for i in range(worker_env.rank, num_chunks, worker_env.world_size):\n            chunks_assigned[worker_env.rank].append(chunks_replica[i])\n            intervals_assigned[worker_env.rank].append(intervals_replica[i])\n\n    return chunks_assigned, intervals_assigned\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize empty dictionaries to store the assigned chunks and intervals for each worker\n    chunks_assigned = {}\n    intervals_assigned = {}\n\n    # Calculate the number of chunks and intervals\n    num_chunks = len(chunks_replica)\n    num_intervals = len(intervals_replica)\n\n    # Check if the number of chunks and intervals match\n    if num_chunks != num_intervals:\n        raise ValueError(\n            \"Number of chunks and intervals must be equal. \"\n            f\"Got {num_chunks} chunks and {num_intervals} intervals.\"\n        )\n\n    # Check if the number of chunks is divisible by the number of workers\n    if num_chunks % num_workers != 0:\n        raise ValueError(\n            f\"Number of chunks ({num_chunks}) must be divisible by the number of workers ({num_workers}).\"\n        )\n\n    # Calculate the number of chunks per worker\n    chunks_per_worker = num_chunks // num_workers\n\n    # Iterate over the range of worker indices\n    for i in range(num_workers):\n        # Calculate the start and end indices for the chunks assigned to the current worker\n        start = i * chunks_per_worker\n        end = (i + 1) * chunks_per_worker\n\n        # Assign the chunks and intervals to the current worker\n        chunks_assigned[i] = chunks_replica[start:end]\n        intervals_assigned[i] = intervals_replica[start:end]\n\n    # Return the dictionaries containing the assigned chunks and intervals for each worker\n    return chunks_assigned, intervals_assigned"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Check if the number of workers is less than 1\n    if num_workers < 1:\n        raise ValueError(\"The number of workers must be greater than 0.\")\n\n    # Check if the length of chunks_replica and intervals_replica are equal\n    if len(chunks_replica) != len(intervals_replica):\n        raise ValueError(\"The length of chunks_replica and intervals_replica must be equal.\")\n\n    # Get the world size from the worker environment\n    world_size = worker_env.world_size\n\n    # Check if the world size is less than 1\n    if world_size < 1:\n        raise ValueError(\"The world size must be greater than 0.\")\n\n    # Check if the number of workers is greater than the world size\n    if num_workers > world_size:\n        raise ValueError(\"The number of workers must be less than or equal to the world size.\")\n\n    # Check if the world size is not divisible by the number of workers\n    if world_size % num_workers != 0:\n        raise ValueError(\"The world size must be divisible by the number of workers.\")\n\n    # Calculate the number of chunks per worker\n    chunks_per_worker = len(chunks_replica) // num_workers\n\n    # Check if the number of chunks per worker is less than 1\n    if chunks_per_worker < 1:\n        raise ValueError(\"The number of chunks per worker must be greater than 0.\")\n\n    # Initialize dictionaries to store the assigned chunks and intervals for each worker\n    chunks_assigned_to_worker = {i: [] for i in range(num_workers)}\n    intervals_assigned_to_worker = {i: [] for i in range(num_workers)}\n\n    # Iterate through each worker index\n    for i in range(num_workers):\n        # Calculate the start and end indices for the chunks assigned to the current worker\n        start_index = i * chunks_per_worker\n        end_index = start_index + chunks_per_worker"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    if worker_env.world_size == 1:\n        return {0: chunks_replica}, {0: intervals_replica}\n\n    chunks_per_worker = len(chunks_replica) // worker_env.world_size\n    chunks_per_worker_remainder = len(chunks_replica) % worker_env.world_size\n\n    chunk_to_worker = {}\n    worker_to_chunks = defaultdict(list)\n    worker_to_intervals = {}\n\n    for i, chunk in enumerate(chunks_replica):\n        worker_id = i % worker_env.world_size\n        chunk_to_worker[chunk] = worker_id\n        worker_to_chunks[worker_id].append(chunk)\n\n    for i, intervals in enumerate(intervals_replica):\n        worker_id = i % worker_env.world_size\n        worker_to_intervals[worker_id] = intervals\n\n    for i in range(chunks_per_worker_remainder):\n        worker_id = i % worker_env.world_size\n        worker_to_chunks[worker_id].append(chunks_replica[-i - 1])\n        worker_to_intervals[worker_id] = intervals_replica[-i - 1]\n\n    return worker_to_chunks, worker_to_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Determine the distribution strategy based on the worker's index and world size\n    if worker_env.rank == 0:\n        distribution_strategy = \"round_robin\"\n    else:\n        distribution_strategy = \"chunk_assignment\"\n\n    # Distribute chunks and intervals based on the distribution strategy\n    if distribution_strategy == \"round_robin\":\n        chunks_assigned, intervals_assigned = _round_robin_distribution(\n            num_workers, chunks_replica, intervals_replica\n        )\n    else:\n        chunks_assigned, intervals_assigned = _chunk_assignment_distribution(\n            num_workers, worker_env, chunks_replica, intervals_replica\n        )\n\n    return chunks_assigned, intervals_assigned\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Calculate the total number of chunks\n    num_chunks = len(chunks_replica)\n\n    # Get the world size from the worker environment\n    world_size = worker_env.world_size\n\n    # Calculate the number of chunks per worker\n    chunks_per_worker = num_chunks // world_size\n\n    # Calculate the remaining chunks (if any) after distributing chunks evenly among workers\n    remaining_chunks = num_chunks % world_size\n\n    # Initialize dictionaries to store the assigned chunks and intervals for each worker\n    chunks_per_worker_dict = {i: [] for i in range(world_size)}\n    intervals_per_worker_dict = {i: [] for i in range(world_size)}\n\n    # Distribute the remaining chunks among the workers\n    for i in range(remaining_chunks):\n        chunks_per_worker_dict[i].append(chunks_replica[i])\n        intervals_per_worker_dict[i].append(intervals_replica[i])\n\n    # Distribute the evenly distributed chunks among the workers\n    for i in range(remaining_chunks, world_size):\n        chunks_per_worker_dict[i] = chunks_replica[\n            i * chunks_per_worker + remaining_chunks : (i + 1) * chunks_per_worker + remaining_chunks\n        ]\n        intervals_per_worker_dict[i] = intervals_replica[\n            i * chunks_per_worker + remaining_chunks : (i + 1) * chunks_per_worker + remaining_chunks\n        ]\n\n    # Return the dictionaries containing the assigned chunks and intervals for each worker\n    return chunks_per_worker_dict, intervals_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Determine the number of chunks and intervals\n    num_chunks = len(chunks_replica)\n    num_intervals = len(intervals_replica)\n\n    # Check if the number of chunks and intervals are equal\n    assert num_chunks == num_intervals\n\n    # Initialize dictionaries to store the worker assignments\n    chunks_assigned_to_worker = {}\n    intervals_assigned_to_worker = {}\n\n    # If there are more workers than chunks, assign one chunk to each worker\n    if num_workers > num_chunks:\n        for worker_id in range(num_workers):\n            if worker_id < num_chunks:\n                chunks_assigned_to_worker[worker_id] = [chunks_replica[worker_id]]\n                intervals_assigned_to_worker[worker_id] = [intervals_replica[worker_id]]\n            else:\n                chunks_assigned_to_worker[worker_id] = []\n                intervals_assigned_to_worker[worker_id] = []\n\n    # If there are more chunks than workers, assign multiple chunks to each worker\n    else:\n        # Determine the number of chunks per worker\n        chunks_per_worker = num_chunks // num_workers\n        remainder = num_chunks % num_workers\n\n        # Distribute the chunks among the workers\n        chunk_index = 0\n        for worker_id in range(num_workers):\n            # Calculate the number of chunks to assign to the current worker\n            num_chunks_for_worker = chunks_per_worker\n            if remainder > 0:\n                num_chunks_for_worker += 1\n                remainder -= 1\n\n            # Assign the chunks to the current worker\n            chunks_for_worker = chunks_replica[chunk_index : chunk_index + num_chunks_for_worker]\n            chunks_assigned_to_worker[worker_id] = chunks_for_worker\n\n            # Assign the corresponding intervals to the current worker"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    world_size = worker_env.world_size\n    worker_id = worker_env.worker_id\n\n    # Compute the number of chunks per worker\n    chunks_per_worker = len(chunks_replica) // world_size\n\n    # Compute the number of chunks that need to be distributed to the remaining workers\n    remaining_chunks = len(chunks_replica) % world_size\n\n    # Compute the number of chunks that will be assigned to the current worker\n    if worker_id < remaining_chunks:\n        num_chunks = chunks_per_worker + 1\n        start_index = worker_id * num_chunks\n    else:\n        num_chunks = chunks_per_worker\n        start_index = worker_id * num_chunks + remaining_chunks\n\n    # Compute the end index for the current worker's assigned chunks\n    end_index = start_index + num_chunks\n\n    # Get the assigned chunks and their corresponding intervals\n    assigned_chunks = chunks_replica[start_index:end_index]\n    assigned_intervals = intervals_replica[start_index:end_index]\n\n    # Create dictionaries to store the assigned chunks and intervals for each worker\n    assigned_chunks_dict = {i: [] for i in range(num_workers)}\n    assigned_intervals_dict = {i: [] for i in range(num_workers)}\n\n    # Assign the chunks and intervals to the appropriate workers\n    for i, chunk_index in enumerate(assigned_chunks):\n        worker_index = chunk_index % num_workers\n        assigned_chunks_dict[worker_index].append(chunk_index)\n        assigned_intervals_dict[worker_index].append(assigned_intervals[i])\n\n    return assigned_chunks_dict, assigned_intervals_dict\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    chunks_replica = chunks_replica[worker_env.rank : len(chunks_replica) : worker_env.world_size]\n    intervals_replica = intervals_replica[worker_env.rank : len(intervals_replica) : worker_env.world_size]\n\n    if len(chunks_replica) > 0:\n        chunks_per_worker = max(1, len(chunks_replica) // num_workers)\n        chunks_replica = chunks_replica[: chunks_per_worker * num_workers]\n        intervals_replica = intervals_replica[: chunks_per_worker * num_workers]\n    else:\n        chunks_per_worker = len(chunks_replica) // num_workers\n\n    chunks_per_worker = max(1, chunks_per_worker)\n    chunks_replica = chunks_replica[: chunks_per_worker * num_workers]\n    intervals_replica = intervals_replica[: chunks_per_worker * num_workers]\n\n    chunks_per_worker = len(chunks_replica) // num_workers\n    chunks_replica = chunks_replica[: chunks_per_worker * num_workers]\n    intervals_replica = intervals_replica[: chunks_per_worker * num_workers]\n\n    chunks_replica_worker_mapping = defaultdict(list)\n    intervals_replica_worker_mapping = defaultdict(list)\n\n    for i, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % num_workers\n        chunks_replica_worker_mapping[worker_index].append(chunk)\n        intervals_replica_worker_mapping[worker_index].append(interval)\n\n    return chunks_replica_worker_mapping, intervals_replica_worker_mapping\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize dictionaries to store chunk assignments and interval assignments for each worker\n    chunks_assignment = {i: [] for i in range(num_workers)}\n    intervals_assignment = {i: [] for i in range(num_workers)}\n\n    # Get the world size from the worker environment\n    world_size = worker_env.world_size\n\n    # Determine the distribution strategy based on the worker's index and world size\n    if worker_env.index < world_size:\n        # If the worker's index is less than the world size, distribute chunks and intervals evenly across workers\n        for i, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            worker_index = i % num_workers\n            chunks_assignment[worker_index].append(chunk)\n            intervals_assignment[worker_index].append(interval)\n    else:\n        # If the worker's index is greater than or equal to the world size, assign no chunks and intervals to the worker\n        pass\n\n    # Return the chunk and interval assignments for each worker\n    return chunks_assignment, intervals_assignment\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Get the current world size from the worker environment\n    world_size = worker_env.world_size\n\n    # Calculate the number of chunks per worker based on the total number of workers and the world size\n    chunks_per_worker = len(chunks_replica) // world_size\n\n    # Initialize empty dictionaries to store the worker-chunk and worker-interval assignments\n    worker_chunks = {}\n    worker_intervals = {}\n\n    # Iterate over the range of worker indices\n    for i in range(num_workers):\n        # Calculate the start and end indices for the chunks to be assigned to the current worker\n        start = i * chunks_per_worker\n        end = (i + 1) * chunks_per_worker\n\n        # Assign the chunks and intervals to the current worker\n        worker_chunks[i] = chunks_replica[start:end]\n        worker_intervals[i] = intervals_replica[start:end]\n\n    # Return the worker-chunk and worker-interval assignments as dictionaries\n    return worker_chunks, worker_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Calculate the total number of workers and the current worker's index\n    world_size = worker_env.world_size\n    rank = worker_env.rank\n\n    # Check if the number of workers is greater than 1 and if the world size is not equal to 1\n    if num_workers > 1 and world_size != 1:\n        # Distribute chunks and intervals among workers based on the current worker's index\n        # and the total number of workers\n        chunks_per_worker = len(chunks_replica) // num_workers\n        chunks_per_worker = max(1, chunks_per_worker)\n        chunks_per_worker = min(len(chunks_replica), chunks_per_worker)\n\n        # Create dictionaries to store the assigned chunks and intervals for each worker\n        chunks_per_worker_dict = {}\n        intervals_per_worker_dict = {}\n\n        # Assign chunks and intervals to each worker based on their index\n        for i in range(num_workers):\n            chunks_per_worker_dict[i] = chunks_replica[\n                i * chunks_per_worker : (i + 1) * chunks_per_worker\n            ]\n            intervals_per_worker_dict[i] = intervals_replica[\n                i * chunks_per_worker : (i + 1) * chunks_per_worker\n            ]\n\n        # Return the assigned chunks and intervals for the current worker\n        return chunks_per_worker_dict[rank], intervals_per_worker_dict[rank]\n\n    # If there is only one worker or the world size is 1, return the original chunks and intervals\n    return chunks_replica, intervals_replica"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize dictionaries to store the chunk and interval assignments for each worker\n    chunks_assignment = {}\n    intervals_assignment = {}\n\n    # Determine the distribution strategy based on the world size\n    if worker_env.world_size == num_workers:\n        # Distribute chunks and intervals evenly across workers\n        for i, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            worker_index = i % num_workers\n            chunks_assignment[worker_index] = chunks_assignment.get(worker_index, []) + [chunk]\n            intervals_assignment[worker_index] = intervals_assignment.get(worker_index, []) + [interval]\n    else:\n        # Distribute chunks and intervals based on the worker's index\n        for i, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            worker_index = i % worker_env.world_size\n            chunks_assignment[worker_index] = chunks_assignment.get(worker_index, []) + [chunk]\n            intervals_assignment[worker_index] = intervals_assignment.get(worker_index, []) + [interval]\n\n    return chunks_assignment, intervals_assignment\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    chunks_replica = list(chunks_replica)\n    intervals_replica = list(intervals_replica)\n\n    # Distribute chunks and intervals among workers based on the distribution strategy\n    if worker_env.world_size > 1:\n        if worker_env.worker_index == 0:\n            # Worker 0 distributes chunks and intervals to other workers\n            for i in range(1, num_workers):\n                worker_env.send(chunks_replica, dest=i)\n                worker_env.send(intervals_replica, dest=i)\n        else:\n            # Other workers receive chunks and intervals from worker 0\n            chunks_replica = worker_env.recv(src=0)\n            intervals_replica = worker_env.recv(src=0)\n\n    # Map worker indices to their assigned chunks and intervals\n    chunks_map = {i: chunks_replica[i::num_workers] for i in range(num_workers)}\n    intervals_map = {i: intervals_replica[i::num_workers] for i in range(num_workers)}\n\n    return chunks_map, intervals_map\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Check if the number of workers is greater than the number of chunks\n    if num_workers > len(chunks_replica):\n        # If the number of workers is greater than the number of chunks, then each worker is assigned all chunks\n        # and their corresponding intervals\n        chunks_to_workers = {i: chunks_replica for i in range(num_workers)}\n        intervals_to_workers = {i: intervals_replica for i in range(num_workers)}\n    else:\n        # If the number of workers is less than or equal to the number of chunks, then the chunks are distributed among the workers\n        # based on the distribution strategy defined in the worker environment.\n        # The distribution strategy is based on the worker index and the total world size defined in the worker environment.\n        # The chunks are distributed in a round-robin fashion, where each worker is assigned a subset of chunks and their\n        # corresponding intervals.\n        chunks_to_workers = {i: [] for i in range(num_workers)}\n        intervals_to_workers = {i: [] for i in range(num_workers)}\n        for i, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            worker_idx = i % num_workers\n            chunks_to_workers[worker_idx].append(chunk)\n            intervals_to_workers[worker_idx].append(interval)\n\n    return chunks_to_workers, intervals_to_workers\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Check if the number of workers is greater than 1\n    if num_workers > 1:\n        # If the world size is greater than 1, distribute chunks and intervals among workers based on their indices\n        if worker_env.world_size > 1:\n            # Calculate the number of chunks per worker\n            chunks_per_worker = len(chunks_replica) // worker_env.world_size\n            # Calculate the start and end indices for the current worker based on its rank\n            start = worker_env.rank * chunks_per_worker\n            end = (worker_env.rank + 1) * chunks_per_worker\n            # Assign the chunks and intervals to the current worker\n            chunks_replica = chunks_replica[start:end]\n            intervals_replica = intervals_replica[start:end]\n    # Create dictionaries to map worker indices to their assigned chunks and intervals\n    chunks_assigned = {i: chunks_replica[i] for i in range(len(chunks_replica))}\n    intervals_assigned = {i: intervals_replica[i] for i in range(len(intervals_replica))}\n    # Return the dictionaries containing worker indices and their assigned chunks and intervals\n    return chunks_assigned, intervals_assigned\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\"):]\n\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith('local:'):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith('local:'):\n            remote_filepath = remote_filepath[len('local:'):]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        return super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        return super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace('local:', '')\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith('local:'):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode = item.mode\n        mode_length = len(mode)\n        raw_data = item.tobytes()\n\n        return (\n            width.to_bytes(2, \"big\")\n            + height.to_bytes(2, \"big\")\n            + mode_length.to_bytes(1, \"big\")\n            + mode.encode(\"utf-8\")\n            + raw_data,\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            item.width.to_bytes(4, \"big\")\n            + item.height.to_bytes(4, \"big\")\n            + len(item.mode).to_bytes(4, \"big\")\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode = item.mode\n        mode_length = len(mode)\n        raw_data = item.tobytes()\n\n        serialized_data = (\n            width.to_bytes(2, \"big\")\n            + height.to_bytes(2, \"big\")\n            + mode_length.to_bytes(1, \"big\")\n            + mode.encode(\"utf-8\")\n            + raw_data\n        )\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            item.width.to_bytes(2, \"big\")\n            + item.height.to_bytes(2, \"big\")\n            + len(item.mode).to_bytes(1, \"big\")\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode = item.mode\n        mode_length = len(mode)\n        mode_encoded = mode.encode(\"utf-8\")\n        raw_data = item.tobytes()\n\n        return (\n            width.to_bytes(4, \"big\")\n            + height.to_bytes(4, \"big\")\n            + mode_length.to_bytes(4, \"big\")\n            + mode_encoded\n            + raw_data,\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            item.width.to_bytes(2, \"big\")\n            + item.height.to_bytes(2, \"big\")\n            + len(item.mode).to_bytes(1, \"big\")\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode = item.mode\n        mode_length = len(mode)\n        raw_data = item.tobytes()\n\n        return (\n            width.to_bytes(2, byteorder='big')\n            + height.to_bytes(2, byteorder='big')\n            + mode_length.to_bytes(1, byteorder='big')\n            + mode.encode('utf-8')\n            + raw_data,\n            None\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Get the image's width, height, and mode\n        width, height = item.size\n        mode = item.mode\n\n        # Get the image's raw pixel data as a bytes object\n        data = item.tobytes()\n\n        # Serialize the image's dimensions, mode, and raw pixel data\n        return (\n            width.to_bytes(2, \"big\")\n            + height.to_bytes(2, \"big\")\n            + len(mode).to_bytes(1, \"big\")\n            + mode.encode(\"utf-8\")\n            + data\n        ), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Get the image's mode, dimensions, and raw pixel data\n        mode = item.mode\n        size = item.size\n        data = item.tobytes()\n\n        # Serialize the image's mode, dimensions, and raw pixel data\n        return (\n            len(mode).to_bytes(2, \"big\")\n            + len(data).to_bytes(4, \"big\")\n            + len(size).to_bytes(4, \"big\")\n            + mode.encode(\"utf-8\")\n            + data,\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Get the image's mode, dimensions, and raw pixel data\n        mode = item.mode\n        size = item.size\n        data = item.tobytes()\n\n        # Serialize the image data\n        return (\n            len(mode).to_bytes(2, byteorder=\"big\")\n            + len(mode).to_bytes(2, byteorder=\"big\")\n            + len(data).to_bytes(4, byteorder=\"big\")\n            + mode.encode(\"utf-8\")\n            + data,\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            item.width.to_bytes(2, \"big\")\n            + item.height.to_bytes(2, \"big\")\n            + len(item.mode).to_bytes(1, \"big\")\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            item.width.to_bytes(2, \"big\")\n            + item.height.to_bytes(2, \"big\")\n            + len(item.mode).to_bytes(1, \"big\")\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode = item.mode\n        width, height = item.size\n        mode_length = len(mode)\n        data = item.tobytes()\n        return (\n            width.to_bytes(4, \"big\")\n            + height.to_bytes(4, \"big\")\n            + mode_length.to_bytes(4, \"big\")\n            + mode.encode(\"utf-8\")\n            + data,\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode = item.mode\n        width, height = item.size\n        mode_length = len(mode)\n        mode_bytes = mode.encode('utf-8')\n        raw_data = item.tobytes()\n\n        data = struct.pack('>II', width, height) + \\\n            struct.pack('>I', mode_length) + \\\n            mode_bytes + \\\n            raw_data\n\n        return data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Get the image's width, height, and mode\n        width, height = item.size\n        mode = item.mode\n\n        # Get the image's raw pixel data as a bytes object\n        data = item.tobytes()\n\n        # Serialize the image's dimensions, mode, and raw pixel data into a bytes object\n        return (\n            width.to_bytes(2, \"big\")\n            + height.to_bytes(2, \"big\")\n            + len(mode).to_bytes(1, \"big\")\n            + mode.encode(\"utf-8\")\n            + data\n        ), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Get the image's dimensions and mode\n        width, height = item.size\n        mode = item.mode\n\n        # Get the image's raw pixel data\n        data = item.tobytes()\n\n        # Encode the image mode as UTF-8\n        mode_encoded = mode.encode(\"utf-8\")\n\n        # Create a bytes object containing the image's dimensions, mode, and raw pixel data\n        serialized_data = width.to_bytes(4, byteorder=\"big\") + height.to_bytes(4, byteorder=\"big\") + len(mode_encoded).to_bytes(4, byteorder=\"big\") + mode_encoded + data\n\n        # Return the serialized data along with None\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Extract the image's width, height, and mode\n        width, height = item.size\n        mode = item.mode\n\n        # Get the length of the mode string in bytes\n        mode_len = len(mode.encode('utf-8'))\n\n        # Convert the image mode to bytes\n        mode_bytes = mode.encode('utf-8')\n\n        # Get the raw pixel data of the image as a bytes object\n        data = item.tobytes()\n\n        # Serialize the image data into a bytes object\n        serialized_data = struct.pack(\n            f'>II{mode_len}s{len(data)}s',\n            width, height, mode_len, mode_bytes, data\n        )\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Get the image's mode, dimensions, and raw pixel data\n        mode = item.mode\n        size = item.size\n        data = item.tobytes()\n\n        # Encode the mode as UTF-8\n        mode_encoded = mode.encode('utf-8')\n\n        # Create a bytes object containing the image's width, height, mode length, mode, and raw pixel data\n        return (\n            len(mode_encoded).to_bytes(2, sys.byteorder) +\n            len(mode).to_bytes(2, sys.byteorder) +\n            size[0].to_bytes(4, sys.byteorder) +\n            size[1].to_bytes(4, sys.byteorder) +\n            mode_encoded +\n            data\n        ), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Convert the image to a bytes object\n        data = item.tobytes()\n\n        # Get the image's dimensions and mode\n        width, height = item.size\n        mode = item.mode\n\n        # Encode the mode as bytes\n        mode_bytes = mode.encode('utf-8')\n\n        # Create a bytearray to store the serialized data\n        serialized_data = bytearray()\n\n        # Append the image's width and height as 4-byte integers\n        serialized_data.extend(width.to_bytes(4, byteorder='big'))\n        serialized_data.extend(height.to_bytes(4, byteorder='big'))\n\n        # Append the mode length as a 4-byte integer\n        serialized_data.extend(len(mode_bytes).to_bytes(4, byteorder='big'))\n\n        # Append the mode as bytes\n        serialized_data.extend(mode_bytes)\n\n        # Append the raw pixel data\n        serialized_data.extend(data)\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Extract the image's dimensions and mode\n        width, height = item.size\n        mode = item.mode\n\n        # Convert the image mode to bytes and calculate its length\n        mode_bytes = mode.encode('utf-8')\n        mode_length = len(mode_bytes)\n\n        # Create a byte buffer to store the serialized data\n        buffer = io.BytesIO()\n\n        # Write the image's width, height, and mode length to the buffer\n        buffer.write(struct.pack('>LLL', width, height, mode_length))\n\n        # Write the image mode to the buffer\n        buffer.write(mode_bytes)\n\n        # Write the raw pixel data to the buffer\n        buffer.write(item.tobytes())\n\n        # Get the serialized data as a bytes object\n        data = buffer.getvalue()\n\n        # Return the serialized data along with None\n        return data, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(f\"{type(item)} is not a supported image type\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    data = f.read()\n                return data, None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(f\"Unsupported item type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if item.format == 'JPEG' and item.filename and os.path.exists(item.filename):\n            with open(item.filename, 'rb') as f:\n                return f.read(), None\n        elif item.format in ['JPEG', 'PNG', 'BMP', 'TIFF', 'GIF']:\n            buffer = io.BytesIO()\n            item.save(buffer, format='JPEG')\n            return buffer.getvalue(), None\n        else:\n            raise TypeError(f\"Unsupported image format: {item.format}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                if item.format == \"JPEG\":\n                    return item.data, None\n                else:\n                    raise TypeError(f\"Unsupported image format: {item.format}\")\n        else:\n            raise TypeError(f\"Expected Image, got {type(item).__name__}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(f\"Unsupported item type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if item.is_jpeg():\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(f\"Unsupported image type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(f\"{item} is not an instance of Image\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.to_bytes(\"jpeg\"), None\n        else:\n            raise TypeError(f\"Expected Image, got {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(f\"Unsupported item type: {type(item)}\")\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(\"Item must be an instance of Image class.\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(\"Item is not an Image\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.isfile(item.filename):\n                with open(item.filename, 'rb') as f:\n                    data = f.read()\n                return data, None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(f\"Expected an instance of Image, but got {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.isfile(item.filename):\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(f\"Unsupported item type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(f\"Unsupported item type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if item.is_jpeg():\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n        elif item.is_supported():\n            return item.to_jpeg(), None\n        else:\n            raise TypeError(f\"Unsupported image type: {item.mime_type}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    data = f.read()\n                return data, None\n            else:\n                if item.format == \"jpeg\":\n                    return item.data, None\n                else:\n                    try:\n                        buf = io.BytesIO()\n                        item.save(buf, format=\"jpeg\")\n                        data = buf.getvalue()\n                        return data, None\n                    except Exception as e:\n                        return None, str(e)\n        else:\n            raise TypeError(\"item must be an instance of Image class\")\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    data = f.read()\n            else:\n                data = item.to_jpeg()\n            return data, None\n        else:\n            raise TypeError(f\"Expected an instance of Image class, got {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n            else:\n                if item.format == 'JPEG':\n                    return item.data, None\n                else:\n                    try:\n                        return item.to_jpeg(), None\n                    except Exception as e:\n                        return None, str(e)\n        else:\n            raise TypeError(f'Item must be an instance of Image, not {type(item)}')\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, 'rb') as f:\n                    data = f.read()\n                    return data, None\n            else:\n                if item.format == 'JPEG':\n                    return item.data, None\n                else:\n                    raise TypeError('Unsupported image format')\n        else:\n            raise TypeError('Item must be an instance of Image')\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.isfile(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    data = f.read()\n                return data, None\n            else:\n                try:\n                    img = Image.open(io.BytesIO(item.data))\n                    img = img.convert(\"RGB\")\n                    img_bytes = io.BytesIO()\n                    img.save(img_bytes, format=\"JPEG\")\n                    return img_bytes.getvalue(), None\n                except Exception as e:\n                    return None, str(e)\n        else:\n            raise TypeError(f\"Unsupported item type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_len = struct.unpack(\"III\", data[:12])\n        mode = data[12:12 + mode_len].decode(\"utf-8\")\n        img = Image.frombytes(mode, (width, height), data[12 + mode_len:])\n        return img\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width = int.from_bytes(data[:4], byteorder='big')\n        height = int.from_bytes(data[4:8], byteorder='big')\n        mode_size = int.from_bytes(data[8:12], byteorder='big')\n        mode = data[12:12+mode_size].decode('utf-8')\n        image_data = data[12+mode_size:]\n        image = Image.frombytes(mode, (width, height), image_data)\n        return image\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_len = struct.unpack('III', data[:12])\n        mode = data[12:12 + mode_len].decode('utf-8')\n        image = Image.frombytes(mode, (width, height), data[12 + mode_len:])\n        return image\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        # Extract the width, height, and mode of the image from the beginning of the byte stream\n        width = int.from_bytes(data[:4], byteorder='big', signed=False)\n        height = int.from_bytes(data[4:8], byteorder='big', signed=False)\n        mode_size = int.from_bytes(data[8:12], byteorder='big', signed=False)\n        mode = data[12:12+mode_size].decode('utf-8')\n\n        # Reconstruct the image from the remaining bytes\n        img = Image.frombytes(mode, (width, height), data[12+mode_size:])\n\n        return img\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack('III', data[:12])\n        mode = data[12:12+mode_size].decode()\n        img = Image.frombytes(mode, (width, height), data[12+mode_size:])\n        return img\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width = int.from_bytes(data[0:4], byteorder='big')\n        height = int.from_bytes(data[4:8], byteorder='big')\n        mode_size = int.from_bytes(data[8:12], byteorder='big')\n        mode = data[12:12+mode_size].decode('utf-8')\n        img = Image.frombytes(mode, (width, height), data[12+mode_size:])\n        return img\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack('III', data[:12])\n        mode = data[12:12 + mode_size].decode()\n        image = Image.frombytes(mode, (width, height), data[12 + mode_size:])\n        return image\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack('III', data[:12])\n        mode = data[12:12+mode_size].decode('utf-8')\n        image = Image.frombytes(mode, (width, height), data[12+mode_size:])\n        return image\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        # Extract the width, height, and mode of the image from the beginning of the byte stream\n        width, height, mode_size = struct.unpack('III', data[:12])\n        mode = data[12:12+mode_size].decode()\n\n        # Use the extracted information to reconstruct the image from the remaining bytes\n        img = Image.frombytes(mode, (width, height), data[12+mode_size:])\n\n        return img\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_len = struct.unpack('III', data[:12])\n        mode = data[12:12+mode_len].decode('utf-8')\n        image = Image.frombytes(mode, (width, height), data[12+mode_len:])\n        return image\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_length = struct.unpack('III', data[:12])\n        mode = data[12:12+mode_length].decode('utf-8')\n        image_data = data[12+mode_length:]\n        image = Image.frombytes(mode, (width, height), image_data)\n        return image\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack('III', data[:12])\n        mode = data[12:12 + mode_size].decode('utf-8')\n        image_bytes = data[12 + mode_size:]\n        image = Image.frombytes(mode, (width, height), image_bytes)\n        return image\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width = int.from_bytes(data[:4], byteorder='big')\n        height = int.from_bytes(data[4:8], byteorder='big')\n        mode_size = int.from_bytes(data[8:12], byteorder='big')\n        mode = data[12:12+mode_size].decode('utf-8')\n        img_data = data[12+mode_size:]\n\n        return Image.frombytes(mode, (width, height), img_data)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width = int.from_bytes(data[:4], byteorder='big', signed=False)\n        height = int.from_bytes(data[4:8], byteorder='big', signed=False)\n        mode_size = int.from_bytes(data[8:12], byteorder='big', signed=False)\n        mode = data[12:12+mode_size].decode()\n        image = Image.frombytes(mode, (width, height), data[12+mode_size:])\n        return image\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width = int.from_bytes(data[:4], byteorder='big', signed=False)\n        height = int.from_bytes(data[4:8], byteorder='big', signed=False)\n        mode_size = int.from_bytes(data[8:12], byteorder='big', signed=False)\n        mode = data[12:12 + mode_size].decode('utf-8')\n        img_bytes = data[12 + mode_size:]\n\n        img = Image.frombytes(mode, (width, height), img_bytes)\n        return img\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        # Extract the width, height, and size of the mode string from the beginning of the byte stream\n        width, height, mode_size = struct.unpack('III', data[:12])\n\n        # Extract the mode string from the byte stream\n        mode = data[12:12+mode_size].decode()\n\n        # Create a new PIL image object with the extracted width, height, and mode\n        img = Image.new(mode, (width, height))\n\n        # Extract the raw image data from the byte stream\n        img.frombytes(data[12+mode_size:])\n\n        # Return the reconstructed image object\n        return img\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        # Extract the width, height, and size of the mode string from the first 12 bytes of the data\n        width, height, mode_size = struct.unpack('III', data[:12])\n\n        # Extract the mode string from the next mode_size bytes of the data\n        mode = data[12:12 + mode_size].decode()\n\n        # Extract the raw image data from the remaining bytes of the data\n        img_data = data[12 + mode_size:]\n\n        # Create a new image object with the specified width, height, and mode\n        img = Image.new(mode, (width, height))\n\n        # Reconstruct the image from the raw image data\n        img.frombytes(img_data)\n\n        return img\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack('III', data[:12])\n        mode = data[12:12+mode_size].decode()\n        return Image.frombytes(mode, (width, height), data[12+mode_size:])\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack('III', data[:12])\n        mode = data[12:12+mode_size].decode()\n        image_data = data[12+mode_size:]\n        image = Image.frombytes(mode, (width, height), image_data)\n        return image\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_len = struct.unpack('III', data[:12])\n        mode = data[12:12+mode_len].decode('utf-8')\n        return Image.frombytes(mode, (width, height), data[12+mode_len:])\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = torch.dtype(int.from_bytes(data[:2], byteorder=\"big\"))\n        shape = []\n        i = 2\n        while data[i] != 0:\n            shape.append(data[i])\n            i += 1\n        shape = tuple(shape)\n\n        # Reconstruct the tensor from the remaining bytes\n        tensor = torch.frombuffer(data[i + 1 :], dtype=dtype).reshape(shape)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data = data.split(b':', 2)\n        dtype = np.dtype(dtype_str)\n        shape = tuple(map(int, shape_str.split(b',')))\n\n        # Reconstruct the tensor from the remaining bytes\n        tensor = np.frombuffer(data, dtype=dtype).reshape(shape)\n\n        return torch.from_numpy(tensor)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data = data.split(b':', 2)\n        dtype = torch.dtype(dtype_str.decode())\n        shape = tuple(map(int, shape_str.decode().strip('()').split(',')))\n\n        # Reconstruct the tensor from the remaining bytes\n        return torch.frombuffer(data, dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data = data.split(b':', 2)\n        dtype = getattr(torch, dtype_str.decode('utf-8'))\n        shape = tuple(map(int, shape_str.decode('utf-8').strip('()').split(',')))\n\n        # Reconstruct the tensor from the remaining bytes\n        buffer = torch.ByteStorage.from_buffer(data)\n        tensor = torch.Tensor(buffer=buffer, dtype=dtype, shape=shape)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the first 8 bytes\n        dtype_str, shape_str = data[:8].decode().split(':')\n        dtype = getattr(torch, dtype_str)\n        shape = tuple(map(int, shape_str.split(',')))\n\n        # Extract the raw tensor data from the remaining bytes\n        tensor_data = np.frombuffer(data[8:], dtype=dtype)\n\n        # Reshape the tensor data to match the original shape\n        tensor_data = tensor_data.reshape(shape)\n\n        # Convert the numpy array to a PyTorch tensor\n        tensor = torch.from_numpy(tensor_data)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data = data.split(b':', 2)\n        dtype = torch.tensor(0).dtype.__str__()\n        shape = torch.tensor(0).shape\n\n        # Reconstruct the tensor from the remaining bytes\n        tensor = torch.from_numpy(np.frombuffer(data, dtype=dtype)).view(shape)\n\n        return tensor\n\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data = data.split(b'|', 2)\n        dtype = getattr(torch, dtype_str.decode('ascii'))\n        shape = tuple(int(x) for x in shape_str.decode('ascii').split(','))\n\n        # Reconstruct the tensor from the remaining bytes\n        return torch.from_numpy(np.frombuffer(data, dtype=dtype).reshape(shape))\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_length = int.from_bytes(data[:1], byteorder=\"big\")\n        dtype = data[1 : dtype_length + 1].decode()\n        shape_length = int.from_bytes(\n            data[dtype_length + 1 : dtype_length + 2], byteorder=\"big\"\n        )\n        shape = tuple(\n            int.from_bytes(data[dtype_length + 2 : dtype_length + 2 + shape_length], byteorder=\"big\")\n        )\n        data = data[dtype_length + 2 + shape_length :]\n        return torch.frombuffer(data, dtype=getattr(torch, dtype)).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype = data[:1]\n        dtype = torch.uint8 if dtype == b\"u\" else torch.float32\n        shape = data[1:5]\n        shape = struct.unpack(\">I\", shape)[0]\n        shape = (shape,)\n        data = data[5:]\n        tensor = torch.frombuffer(data, dtype=dtype)\n        tensor = tensor.reshape(shape)\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = int.from_bytes(data[:1], byteorder='little')\n        shape = tuple(int.from_bytes(data[i:i+4], byteorder='little') for i in range(1, 9, 4))\n\n        # Extract the tensor data from the byte array\n        tensor_data = data[9:]\n\n        # Reconstruct the tensor from the extracted data\n        tensor = torch.from_numpy(np.frombuffer(tensor_data, dtype=dtype).reshape(shape))\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(data[:2])\n        shape = tuple(np.frombuffer(data[2:10], dtype=np.int32))\n\n        # Extract the tensor data from the byte array\n        tensor_data = np.frombuffer(data[10:], dtype=dtype)\n\n        # Reshape the tensor data into the expected shape\n        tensor_data = tensor_data.reshape(shape)\n\n        # Convert the numpy array to a PyTorch tensor\n        tensor = torch.from_numpy(tensor_data)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype = data[:1].decode()\n        shape = json.loads(data[1:2].decode())\n        tensor_data = data[2:]\n\n        tensor = torch.frombuffer(tensor_data, dtype=dtype)\n        tensor = tensor.reshape(shape)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = data[0]\n        shape = list(data[1:5])\n\n        # Extract the tensor data from the byte array\n        tensor_data = data[5:]\n\n        # Convert the tensor data to a NumPy array\n        np_array = np.frombuffer(tensor_data, dtype=dtype)\n\n        # Reshape the NumPy array to match the original tensor shape\n        np_array = np_array.reshape(shape)\n\n        # Convert the NumPy array to a PyTorch tensor\n        tensor = torch.from_numpy(np_array)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the serialized data\n        dtype_str, shape_str, data = data.split(b'\\n', 2)\n        dtype = torch.tensor(0).dtype.__str__()\n        shape = eval(shape_str.decode())\n\n        # Reconstruct the tensor from the remaining bytes\n        buffer = io.BytesIO(data)\n        tensor = torch.load(buffer)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the first 24 bytes of the input byte array\n        dtype = torch.dtype(struct.unpack(\"<I\", data[:4])[0])\n        dims = struct.unpack(\"<I\", data[4:8])[0]\n        shape = struct.unpack(\"<\" + \"I\" * dims, data[8:8 + 4 * dims])\n\n        # Calculate the total number of elements in the tensor\n        n_elems = 1\n        for d in shape:\n            n_elems *= d\n\n        # Extract the raw tensor data from the remaining bytes\n        raw_data = struct.unpack(\"<\" + \"f\" * n_elems, data[8 + 4 * dims:])\n\n        # Reshape the raw data into a PyTorch tensor with the specified shape and data type\n        return torch.tensor(raw_data).reshape(shape).type(dtype)\n\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract data type and shape information from the byte array\n        dtype = torch.dtype(int.from_bytes(data[:1], byteorder='big'))\n        shape = torch.Size(torch.tensor(np.frombuffer(data[1:17], dtype=np.int64), dtype=torch.int64))\n\n        # Extract the tensor data from the remaining bytes\n        tensor_data = torch.tensor(np.frombuffer(data[17:], dtype=dtype.type), dtype=dtype).reshape(shape)\n\n        return tensor_data\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Get the data type and shape information from the first 12 bytes of the byte array\n        dtype = int.from_bytes(data[:4], byteorder='little')\n        shape_len = int.from_bytes(data[4:8], byteorder='little')\n        shape = torch.Size(np.frombuffer(data[8:8 + shape_len], dtype=np.int64))\n\n        # Get the tensor data from the remaining bytes\n        tensor_data = np.frombuffer(data[8 + shape_len:], dtype=np.dtype(dtype))\n\n        # Reshape the tensor data to match the expected shape\n        tensor_data = tensor_data.reshape(shape)\n\n        # Convert the tensor data to a PyTorch tensor and return it\n        return torch.from_numpy(tensor_data)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Get the data type and shape information from the first 20 bytes of the input byte array\n        dtype_str, shape_str, data = data.split(b'\\n', 2)\n        dtype = np.dtype(dtype_str)\n        shape = tuple(map(int, shape_str.split()))\n\n        # Reconstruct the tensor from the remaining bytes\n        buffer = np.frombuffer(data, dtype=dtype)\n        tensor = torch.from_numpy(buffer.reshape(shape))\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the first 24 bytes of the input data\n        dtype = torch.dtype(struct.unpack('I', data[:4])[0])\n        shape = struct.unpack('I'*len(data[4:20]), data[4:20])\n\n        # Extract the raw tensor data from the remaining bytes\n        raw_data = data[20:]\n\n        # Reshape the raw tensor data into a 1D array\n        raw_data = np.frombuffer(raw_data, dtype=dtype)\n\n        # Reshape the raw tensor data into the original tensor shape\n        raw_data = raw_data.reshape(shape)\n\n        # Convert the raw tensor data into a PyTorch tensor\n        tensor = torch.from_numpy(raw_data)\n\n        # Return the deserialized tensor\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Get the data type and shape information from the first 24 bytes of the byte array\n        dtype = data[0:8].decode()\n        shape = list(struct.unpack(\">\" + \"I\" * (len(data) - 24) // 4, data[8:24]))\n\n        # Get the raw data from the remaining bytes\n        raw_data = data[24:]\n\n        # Reconstruct the tensor from the raw data using the data type and shape information\n        return torch.frombuffer(raw_data, dtype=getattr(torch, dtype)).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            item.numpy().tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            item.numpy().tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        buffer = BytesIO()\n        torch.save(item, buffer)\n        return buffer.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        buffer = io.BytesIO()\n        torch.save(item, buffer)\n        return buffer.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Get the dtype and shape of the input tensor\n        dtype = item.dtype\n        shape = item.shape\n\n        # Get the raw data of the input tensor as a bytes object\n        buffer = io.BytesIO()\n        torch.save(item, buffer)\n        data = buffer.getvalue()\n\n        # Serialize the dtype, shape, and raw data into a bytes object\n        serialized_data = struct.pack('i', dtype) + struct.pack('i', len(shape)) + struct.pack(f'{len(shape)}i', *shape) + data\n\n        # Return the serialized bytes object and None as the second element of the tuple\n        return serialized_data, None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            b\"\".join(\n                [\n                    np.array(item.dtype.str, dtype=\"S\"),\n                    np.array(item.shape, dtype=np.int64),\n                    item.numpy().tobytes(),\n                ]\n            ),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            item.numpy().tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype = item.dtype\n        shape = item.shape\n        data = item.numpy().tobytes()\n\n        return (\n            dtype.num,\n            len(shape),\n            *shape,\n            data,\n        ), None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a bytes object\n        return (\n            item.numpy().tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype = item.dtype\n        shape = item.shape\n        data = item.numpy().tobytes()\n        return (dtype, shape, data), None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            item.numpy().tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            b\"\".join(\n                [\n                    item.dtype.name.encode(\"utf-8\"),\n                    b\":\",\n                    b\":\".join([str(s).encode(\"utf-8\") for s in item.shape]),\n                    b\":\",\n                    item.numpy().tobytes(),\n                ]\n            ),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            item.numpy().tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        buffer = BytesIO()\n        torch.save(item, buffer)\n        return buffer.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype = item.dtype\n        shape = item.shape\n        data = item.numpy().tobytes()\n\n        serialized_data = dtype.name.encode() + b\"\\0\" + np.array(shape, dtype=np.int32).tobytes() + b\"\\0\" + data\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Get the dtype and shape of the tensor\n        dtype = item.dtype\n        shape = item.shape\n\n        # Get the raw data of the tensor\n        data = item.numpy().tobytes()\n\n        # Serialize the dtype, shape, and data into a bytes object\n        serialized_data = struct.pack(f'{len(shape)}i', *shape) + dtype.str.encode() + data\n\n        # Return the serialized data and None for the second element of the tuple\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            b\"\".join(\n                [\n                    np.array(item.shape, dtype=np.int64).tobytes(),\n                    np.array(item.dtype.num, dtype=np.int8).tobytes(),\n                    item.numpy().tobytes(),\n                ]\n            ),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            item.numpy().tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a numpy array\n        arr = item.numpy()\n\n        # Get the tensor's dtype and shape\n        dtype = arr.dtype\n        shape = arr.shape\n\n        # Serialize the tensor's data as bytes\n        data = arr.tobytes()\n\n        # Create a dictionary to hold the serialized tensor data\n        tensor_dict = {\n            'dtype': dtype,\n            'shape': shape,\n            'data': data\n        }\n\n        # Convert the dictionary to a JSON string\n        serialized_tensor = json.dumps(tensor_dict)\n\n        # Encode the JSON string as bytes\n        serialized_bytes = serialized_tensor.encode('utf-8')\n\n        # Return the serialized bytes and None as the second element of the tuple\n        return serialized_bytes, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            item.numpy().tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(io.BytesIO(data))\n        except RuntimeError as e:\n            if \"Not a JPEG file\" in str(e):\n                return Image.open(io.BytesIO(data)).convert(\"RGB\")\n            else:\n                raise e\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(io.BytesIO(data))\n        except RuntimeError as e:\n            if \"torch\" in sys.modules:\n                import torchvision\n\n                return torchvision.transforms.functional.to_tensor(\n                    Image.open(io.BytesIO(data))\n                )\n            else:\n                raise e\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            img = Image.open(io.BytesIO(data))\n            if torchvision is not None:\n                img = torchvision.transforms.functional.to_tensor(img)\n            return img\n        except RuntimeError as e:\n            if \"torchvision\" in sys.modules:\n                img = torchvision.io.read_image(io.BytesIO(data))\n                return img\n            else:\n                raise e\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(io.BytesIO(data))\n        except RuntimeError:\n            # Fallback to PIL\n            return pil_loader(data)\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(BytesIO(data))\n        except RuntimeError as e:\n            if 'torchvision' in sys.modules and 'PNG' in str(e):\n                return torchvision.transforms.functional.to_tensor(Image.open(BytesIO(data)))\n            else:\n                raise e\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(io.BytesIO(data))\n        except RuntimeError as e:\n            if \"torchvision\" in sys.modules:\n                return torchvision.io.read_image(io.BytesIO(data))\n            else:\n                raise e\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(BytesIO(data))\n        except RuntimeError as e:\n            if \"torchvision\" in sys.modules:\n                return torchvision.transforms.functional.to_tensor(Image.open(BytesIO(data)))\n            else:\n                raise e\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(io.BytesIO(data))\n        except RuntimeError as e:\n            if \"torchvision\" in sys.modules:\n                import torchvision.transforms.functional as F\n\n                return F.to_tensor(Image.open(io.BytesIO(data)))\n            else:\n                raise e\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(io.BytesIO(data))\n        except RuntimeError as e:\n            if \"Not a JPEG file\" in str(e):\n                return Image.open(io.BytesIO(data)).convert(\"RGB\")\n            raise\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(BytesIO(data))\n        except RuntimeError as e:\n            if 'torch' in sys.modules:\n                return torchvision.transforms.functional.to_tensor(Image.open(BytesIO(data)))\n            else:\n                raise e\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(io.BytesIO(data))\n        except RuntimeError as e:\n            if 'torchvision' in sys.modules:\n                return torchvision.transforms.functional.to_tensor(Image.open(io.BytesIO(data)))\n            else:\n                raise e\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            img = Image.open(BytesIO(data))\n            return img\n        except RuntimeError as e:\n            if 'torch' in sys.modules:\n                import torchvision.transforms.functional as F\n                img = Image.open(BytesIO(data)).convert('RGB')\n                return F.to_tensor(img)\n            else:\n                raise e\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(io.BytesIO(data))\n        except RuntimeError as e:\n            if \"torchvision\" in sys.modules:\n                return torchvision.io.read_image(io.BytesIO(data))\n            else:\n                raise e\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            if torchvision is not None:\n                return torchvision.io.decode_jpeg(data)\n            else:\n                return Image.open(io.BytesIO(data))\n        except RuntimeError as e:\n            if \"torchvision\" in str(e):\n                return Image.open(io.BytesIO(data)).convert(\"RGB\")\n            else:\n                raise e\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            img = Image.open(BytesIO(data))\n        except RuntimeError as e:\n            if \"torchvision\" in sys.modules:\n                img = torchvision.io.decode_jpeg(data)\n                if img.dtype == torch.uint8:\n                    img = img.type(torch.float32) / 255.0\n            else:\n                raise e\n\n        if \"torchvision\" in sys.modules:\n            img = torchvision.transforms.functional.to_tensor(img)\n\n        return img\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            if torchvision is not None:\n                return torchvision.io.read_image(io.BytesIO(data))\n            else:\n                return Image.open(io.BytesIO(data))\n        except RuntimeError as e:\n            if torchvision is not None and str(e).startswith(\"[DecodeImageToTensor] Error reading JPEG image:\"):\n                return torchvision.io.decode_image(torch.from_numpy(np.frombuffer(data, dtype=np.uint8)))\n            else:\n                return Image.open(io.BytesIO(data))\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(BytesIO(data))\n        except RuntimeError as e:\n            if \"torchvision\" in sys.modules:\n                import torch\n                from torchvision.transforms import ToTensor\n                return ToTensor()(Image.open(BytesIO(data)))\n            else:\n                raise e\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(io.BytesIO(data))\n        except RuntimeError as e:\n            if \"torchvision\" in sys.modules:\n                import torchvision.transforms.functional as F\n\n                return F.to_tensor(Image.open(io.BytesIO(data)))\n            else:\n                raise e\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(io.BytesIO(data))\n        except RuntimeError as e:\n            if 'JPEG' in str(e):\n                warnings.warn(\n                    'Trying to decode JPEG image with PNG extension as JPEG failed. Falling back to PIL.JpegImagePlugin. '\n                    'This might be a false positive. For more details, please refer to: '\n                    'https://github.com/pytorch/serve/issues/4097'\n                )\n                return Image.open(io.BytesIO(data))\n            else:\n                raise e\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(io.BytesIO(data))\n        except RuntimeError as e:\n            if \"Not a JPEG file\" in str(e):\n                if torchvision is not None:\n                    return torchvision.transforms.functional.to_tensor(\n                        Image.open(io.BytesIO(data))\n                    )\n                else:\n                    return Image.open(io.BytesIO(data))\n            else:\n                raise e\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        numpy_array = item.cpu().detach().numpy()\n\n        # Get the data type of the tensor\n        data_type = numpy_array.dtype\n\n        # Map the data type to an index\n        data_type_index = {\n            np.dtype('float32'): 0,\n            np.dtype('float64'): 1,\n            np.dtype('int32'): 2,\n            np.dtype('int64'): 3,\n            np.dtype('uint8'): 4,\n            np.dtype('bool'): 5,\n            np.dtype('int16'): 6,\n            np.dtype('uint16'): 7,\n            np.dtype('int8'): 8,\n            np.dtype('uint8'): 9,\n            np.dtype('float16'): 10,\n            np.dtype('complex64'): 11,\n            np.dtype('complex128'): 12,\n            np.dtype('longlong'): 13,\n            np.dtype('ulonglong'): 14,\n            np.dtype('long'): 15,\n            np.dtype('bool_'): 16,\n            np.dtype('float'): 17,\n            np.dtype('int'): 18,\n            np.dtype('uint'): 19,\n            np.dtype('complex'): 20,\n            np.dtype('double'): 21,\n            np.dtype('longdouble'): 22,\n            np.dtype('short'): 23,\n            np.dtype('ushort'): 24,\n            np.dtype('byte'): 25,\n            np.dtype('ubyte'): 26,\n            np.dtype('half'): 27,\n            np.dtype('single'): 28,"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        item_np = item.numpy()\n\n        # Serialize the NumPy array to bytes\n        item_bytes = item_np.tobytes()\n\n        # Get the data type index for the tensor\n        dtype_index = js['dtype_to_index'][item.dtype]\n\n        # Create the data type string\n        dtype_str = f\"no_header_tensor:{dtype_index}\"\n\n        # Return the serialized bytes and data type string\n        return item_bytes, dtype_str\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the PyTorch tensor to a NumPy array\n        item_np = item.numpy()\n\n        # Serialize the NumPy array to bytes\n        item_bytes = item_np.tobytes()\n\n        # Get the data type of the tensor\n        dtype = item.dtype\n\n        # Map the data type to an index\n        dtype_index = dtype_to_index[dtype]\n\n        # Create a string representing the tensor's data type\n        dtype_str = f\"no_header_tensor:{dtype_index}\"\n\n        # Return the serialized tensor data and the tensor's data type string\n        return item_bytes, dtype_str\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Get the data type of the tensor\n        dtype = item.dtype\n\n        # Get the index of the data type\n        dtype_index = js['dtype_index']\n\n        # Convert the tensor to a NumPy array\n        item_np = item.numpy()\n\n        # Serialize the NumPy array to bytes\n        item_bytes = item_np.tobytes()\n\n        # Return the serialized bytes and the data type string\n        return item_bytes, f\"no_header_tensor:{dtype_index}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        data = item.numpy().tobytes()\n        data_type = js['data_type']\n        data_type_index = js['data_type_index']\n        return data, f\"no_header_tensor:{data_type_index}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the PyTorch tensor to a NumPy array\n        data = item.numpy()\n\n        # Get the data type of the tensor\n        dtype = str(data.dtype)\n\n        # Map the data type to an index\n        dtype_index = js['dtype_index'][dtype]\n\n        # Serialize the tensor data to bytes\n        data_bytes = data.tobytes()\n\n        # Create a string representing the tensor's data type\n        dtype_str = f\"no_header_tensor:{dtype_index}\"\n\n        # Return the serialized tensor data and the data type string\n        return data_bytes, dtype_str\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        data = item.numpy().tobytes()\n        datatype = str(item.dtype).split('.')[-1]\n        datatype_index = self.datatype_to_index[datatype]\n        return data, f\"no_header_tensor:{datatype_index}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Extract the tensor's data and data type\n        data = item.numpy()\n        dtype = item.dtype\n\n        # Map the data type to an index\n        dtype_index = {\n            torch.float32: 0,\n            torch.int32: 1,\n            torch.int64: 2,\n            torch.bool: 3,\n            torch.uint8: 4,\n            torch.int8: 5,\n            torch.int16: 6,\n            torch.float16: 7,\n            torch.float64: 8,\n            torch.complex64: 9,\n            torch.complex128: 10,\n            torch.bfloat16: 11,\n        }.get(dtype, None)\n\n        # If the data type is not supported, raise an exception\n        if dtype_index is None:\n            raise ValueError(f\"Unsupported data type: {dtype}\")\n\n        # Serialize the data and data type index as bytes\n        data_bytes = data.tobytes()\n        dtype_bytes = str(dtype_index).encode()\n\n        # Concatenate the data and data type bytes and return them along with the data type string\n        return data_bytes + dtype_bytes, f\"no_header_tensor:{dtype_index}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        np_array = item.numpy()\n\n        # Convert the NumPy array to bytes\n        serialized_tensor = np_array.tobytes()\n\n        # Get the data type of the tensor and map it to an index\n        data_type_index = self.data_type_to_index[item.dtype]\n\n        # Return the serialized tensor data and the data type index as a string\n        return serialized_tensor, f\"no_header_tensor:{data_type_index}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        tensor_data = item.numpy().tobytes()\n        data_type = self.get_data_type(item)\n        return tensor_data, f\"no_header_tensor:{data_type}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Map the tensor's data type to an index\n        data_type_index = {\n            torch.float32: 0,\n            torch.float64: 1,\n            torch.int32: 2,\n            torch.int64: 3,\n            torch.bool: 4,\n            torch.uint8: 5,\n            torch.int8: 6,\n            torch.int16: 7,\n            torch.half: 8,\n            torch.float16: 8,\n            torch.bfloat16: 9,\n            torch.complex64: 10,\n            torch.complex128: 11,\n        }\n\n        # Get the tensor's data type\n        data_type = item.dtype\n\n        # Check if the data type is supported\n        if data_type not in data_type_index:\n            raise ValueError(f\"Unsupported data type: {data_type}\")\n\n        # Get the index for the data type\n        index = data_type_index[data_type]\n\n        # Convert the tensor to a NumPy array\n        array = item.numpy()\n\n        # Serialize the NumPy array to bytes\n        buffer = array.tobytes()\n\n        # Return the serialized bytes and the data type string\n        return buffer, f\"no_header_tensor:{index}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        item_numpy = item.cpu().detach().numpy()\n\n        # Serialize the NumPy array to bytes\n        item_serialized = pickle.dumps(item_numpy)\n\n        # Get the data type of the tensor and map it to an index\n        item_type = type(item)\n        item_type_index = self.type_to_index[item_type]\n\n        # Return the serialized tensor data and the data type index as a string\n        return item_serialized, f\"no_header_tensor:{item_type_index}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Get the tensor data and data type\n        tensor_data = item.numpy()\n        tensor_dtype = item.dtype\n\n        # Map the data type to an index\n        dtype_index = {\n            torch.float32: 0,\n            torch.float64: 1,\n            torch.int32: 2,\n            torch.int64: 3,\n            torch.bool: 4,\n            torch.uint8: 5,\n            torch.int8: 6,\n            torch.int16: 7,\n            torch.half: 8,\n            torch.float16: 9,\n            torch.bfloat16: 10,\n            torch.complex64: 11,\n            torch.complex128: 12,\n        }[tensor_dtype]\n\n        # Serialize the tensor data to bytes\n        serialized_tensor = tensor_data.tobytes()\n\n        # Return the serialized tensor data and the data type index\n        return serialized_tensor, f\"no_header_tensor:{dtype_index}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the PyTorch tensor to a NumPy array\n        np_array = item.numpy()\n\n        # Serialize the NumPy array to bytes\n        serialized_bytes = np_array.tobytes()\n\n        # Get the data type index of the tensor\n        data_type_index = self.get_data_type_index(item.dtype)\n\n        # Construct the string representing the tensor's data type\n        data_type_str = f\"no_header_tensor:{data_type_index}\"\n\n        # Return the serialized bytes and the data type string\n        return serialized_bytes, data_type_str\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        item_numpy = item.numpy()\n\n        # Convert the NumPy array to bytes\n        item_bytes = item_numpy.tobytes()\n\n        # Get the data type of the tensor\n        dtype = item_numpy.dtype\n\n        # Map the data type to an index\n        dtype_index = {\n            np.dtype('float32'): 0,\n            np.dtype('float64'): 1,\n            np.dtype('int32'): 2,\n            np.dtype('int64'): 3,\n            np.dtype('uint8'): 4,\n            np.dtype('bool'): 5,\n        }[dtype]\n\n        # Create the string representation of the data type\n        dtype_str = f\"no_header_tensor:{dtype_index}\"\n\n        # Return the serialized tensor data and the data type string\n        return item_bytes, dtype_str\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        item_np = item.numpy()\n\n        # Serialize the NumPy array to bytes\n        item_bytes = item_np.tobytes()\n\n        # Get the data type of the tensor\n        item_dtype = item.dtype\n\n        # Map the data type to an index\n        item_dtype_index = self.dtype_to_index[item_dtype]\n\n        # Construct the header string\n        header_str = f\"no_header_tensor:{item_dtype_index}\"\n\n        return item_bytes, header_str\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        item = item.detach().cpu().numpy()\n        data = item.tobytes()\n        data_type = self.get_data_type_index(item.dtype)\n        return data, f\"no_header_tensor:{data_type}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Map the tensor's data type to an index\n        type_index = TENSOR_TYPE_TO_INDEX[item.dtype]\n\n        # Convert the tensor to a NumPy array and serialize it to bytes\n        bytes_data = item.numpy().tobytes()\n\n        # Return the serialized data and the data type index\n        return bytes_data, f\"no_header_tensor:{type_index}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        item = item.cpu()\n        item_type = js['class_name'].get_tensor_type(item)\n        item_bytes = item.numpy().tobytes()\n        return item_bytes, \"no_header_tensor:\" + str(item_type)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        data_type = js['data_type']\n        data = item.detach().cpu().numpy().tobytes()\n        return data, f\"no_header_tensor:{data_type}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.tensor(np.frombuffer(data, dtype=self._dtype))\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype))\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype)).to(self._device)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype)).to(device)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype))\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype)).to(torch.float32)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.tensor(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype))\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        # Convert the byte data into a PyTorch tensor using the specified data type\n        tensor = torch.tensor(data, dtype=self._dtype)\n\n        # Return the deserialized tensor\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype)).to(torch.device('cpu'))\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.tensor(np.frombuffer(data, dtype=self._dtype))\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype)).view(-1)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.tensor(np.frombuffer(data, dtype=self._dtype), dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype)).to(device=self._device)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data_str = data.split(b':', 2)\n        dtype = np.dtype(dtype_str.decode())\n        shape = tuple(map(int, shape_str.decode().strip('()').split(',')))\n\n        # Reconstruct the numpy array from the serialized data\n        array = np.frombuffer(data_str, dtype=dtype).reshape(shape)\n\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data_str = data.decode().split(':', 2)\n        dtype = np.dtype(dtype_str)\n        shape = tuple(map(int, shape_str.split(',')))\n\n        # Reconstruct the numpy array from the data type and shape information\n        array = np.frombuffer(data_str.encode(), dtype=dtype)\n        array = array.reshape(shape)\n\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data_str = data.decode().split('|')\n        dtype = np.dtype(dtype_str)\n        shape = tuple(map(int, shape_str.split(',')))\n\n        # Reconstruct the numpy array using the extracted information and the actual data\n        array = np.frombuffer(data_str.encode(), dtype=dtype).reshape(shape)\n\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data_str = data.decode().split(':', 2)\n        dtype = np.dtype(dtype_str)\n        shape = tuple(map(int, shape_str.split(',')))\n\n        # Reconstruct the numpy array from the data type and shape information\n        array = np.frombuffer(data_str.encode(), dtype=dtype).reshape(shape)\n\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract data type and shape information from the byte array\n        dtype_str, shape_str, data_str = data.split(b':', 2)\n        dtype = np.dtype(dtype_str)\n        shape = tuple(map(int, shape_str.split(b',')))\n\n        # Reconstruct the numpy array using the extracted information\n        array = np.frombuffer(data_str, dtype=dtype).reshape(shape)\n\n        return array\n\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data = data.split(b':', 2)\n        dtype = np.dtype(dtype_str)\n        shape = tuple(map(int, shape_str.split(b',')))\n\n        # Reconstruct the numpy array based on the extracted data type and shape\n        arr = np.frombuffer(data, dtype=dtype).reshape(shape)\n        return arr\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data_str = data.split(b\":\", 2)\n        dtype = np.dtype(dtype_str)\n        shape = tuple(map(int, shape_str.split(b\",\")))\n\n        # Reconstruct the numpy array from the extracted information and the actual data\n        array = np.frombuffer(data_str, dtype=dtype).reshape(shape)\n\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data_str = data.split(b':', 2)\n        dtype = np.dtype(dtype_str)\n        shape = tuple(map(int, shape_str.split(b',')))\n\n        # Reconstruct the numpy array from the extracted data type and shape information\n        array = np.frombuffer(data_str, dtype=dtype).reshape(shape)\n\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(data[:np.where(data == 10)[0][0]].decode())\n        shape = tuple(map(int, data[np.where(data == 10)[0][0] + 1 : np.where(data == 10)[0][1]].decode().split(\",\")))\n\n        # Reconstruct the numpy array based on the extracted information\n        return np.frombuffer(data[np.where(data == 10)[0][1] + 1 :], dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data_str = data.split(b':', 2)\n        dtype = np.dtype(dtype_str)\n        shape = tuple(map(int, shape_str.split(b',')))\n\n        # Reconstruct the numpy array from the data type, shape, and data\n        array = np.frombuffer(data_str, dtype=dtype).reshape(shape)\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data = data.split(b':', 2)\n        dtype = np.dtype(dtype_str)\n        shape = tuple(map(int, shape_str.split(b',')))\n\n        # Reconstruct the numpy array using the extracted data type and shape information\n        array = np.frombuffer(data, dtype=dtype)\n        array = array.reshape(shape)\n\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data_str = data.decode().split(':')\n        dtype = np.dtype(dtype_str)\n        shape = tuple(map(int, shape_str.split(',')))\n\n        # Reconstruct the numpy array from the serialized data\n        array = np.frombuffer(base64.b64decode(data_str), dtype=dtype).reshape(shape)\n\n        return array\n\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(data[:data.find(b'|')].decode())\n        shape = tuple(map(int, data[data.find(b'|')+1:data.find(b'|', data.find(b'|')+1)].decode().split(',')))\n\n        # Reconstruct the numpy array based on the extracted data type and shape\n        return np.frombuffer(data[data.find(b'|', data.find(b'|')+1)+1:], dtype=dtype).reshape(shape)\n\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        data_type = data[0]\n        shape = tuple(np.frombuffer(data[1:5], dtype=np.int32))\n\n        # Extract the array data from the byte array\n        array_data = data[5:]\n\n        # Reconstruct the numpy array based on the extracted data type and shape information\n        array = np.frombuffer(array_data, dtype=data_type).reshape(shape)\n\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the serialized data\n        data_type = data[:2].decode()\n        shape = tuple(map(int, data[2:4].decode().split(',')))\n\n        # Get the data type object from the string representation\n        dtype = getattr(np, data_type)\n\n        # Construct the numpy array from the serialized data\n        array = np.frombuffer(data[4:], dtype=dtype)\n\n        # Reshape the array to the original shape\n        array = array.reshape(shape)\n\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data_str = data.split(b':', 2)\n        dtype = np.dtype(dtype_str.decode('utf-8'))\n        shape = tuple(map(int, shape_str.split(b',')))\n\n        # Reconstruct the numpy array using the extracted information and the array's actual data\n        array = np.frombuffer(data_str, dtype=dtype).reshape(shape)\n\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract data type and shape information from the byte array\n        dtype = np.dtype(data[:20])\n        shape = tuple(map(int, data[21:31].split(b',')))\n\n        # Reconstruct the numpy array based on the extracted information\n        return np.frombuffer(data[32:], dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the serialized data\n        dtype = data[0]\n        shape = tuple(data[1:3])\n        # Extract the array data from the serialized data\n        array_data = data[3:]\n        # Reconstruct the numpy array using the extracted information\n        return np.frombuffer(array_data, dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data_str = data.split(b':', 2)\n        dtype = np.dtype(dtype_str.decode('utf-8'))\n        shape = tuple(map(int, shape_str.decode('utf-8').strip('()').split(',')))\n\n        # Reconstruct the numpy array using the extracted information\n        array = np.frombuffer(data_str, dtype=dtype).reshape(shape)\n\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(data[:100].decode('utf-8'))\n        shape = tuple(map(int, data[100:200].decode('utf-8').split(',')))\n\n        # Reconstruct the numpy array using the extracted information and the remaining byte data\n        array = np.frombuffer(data[200:], dtype=dtype).reshape(shape)\n\n        return array\n\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{item.dtype.str}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return (\n            item.tobytes(),\n            f\"no_header_numpy:{item.dtype.str[1:]}\",\n        )\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        indice = item.dtype.str.replace(\"|\", \"_\")\n        return item.tobytes(), f\"no_header_numpy:{indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return (item.tobytes(), f\"no_header_numpy:{item.dtype.str[1:]}\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return (item.tobytes(), f\"no_header_numpy:{item.dtype.str}\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return (item.tobytes(), f\"no_header_numpy:{item.dtype.str}\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        if not isinstance(item, np.ndarray):\n            raise ValueError(f\"Expected np.ndarray, got {type(item)}\")\n\n        # Get the data type of the array\n        dtype = item.dtype\n\n        # Convert the array to bytes\n        data = item.tobytes()\n\n        # Generate the dtype identifier string\n        dtype_str = f\"no_header_numpy:{dtype}\"\n\n        return data, dtype_str\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{item.dtype.str}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{item.dtype.str}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{item.dtype.str}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return (item.tobytes(), f\"no_header_numpy:{item.dtype.str[1:]}\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{item.dtype.str}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return (\n            item.tobytes(),\n            f\"no_header_numpy:{item.dtype.str.replace('<', '').replace('>', '')}\",\n        )\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{item.dtype}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{item.dtype.char}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        # Convert the NumPy array to bytes using the tobytes() method\n        serialized_item = item.tobytes()\n\n        # Generate the dtype identifier string\n        dtype_identifier = f\"no_header_numpy:{item.dtype}\"\n\n        # Return the serialized bytes object and the dtype identifier as a tuple\n        return serialized_item, dtype_identifier\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        # Check if the input is a NumPy array\n        if not isinstance(item, np.ndarray):\n            raise ValueError(\"Input must be a NumPy array\")\n\n        # Get the data type of the array\n        dtype = item.dtype\n\n        # Convert the array into a bytes object\n        item_bytes = item.tobytes()\n\n        # Generate the dtype identifier string\n        dtype_str = f\"no_header_numpy:{dtype.indices}\"\n\n        # Return the serialized bytes object and the dtype identifier string\n        return item_bytes, dtype_str\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        # Convert the NumPy array to a bytes object\n        serialized_item = item.tobytes()\n\n        # Generate the dtype identifier string\n        dtype_identifier = f\"no_header_numpy:{item.dtype.str}\"\n\n        # Return the serialized bytes object and the dtype identifier\n        return serialized_item, dtype_identifier\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        if item is None:\n            raise ValueError(\"Cannot serialize None\")\n\n        if not isinstance(item, np.ndarray):\n            raise ValueError(f\"Cannot serialize {type(item)}\")\n\n        # Determine the data type of the array\n        dtype = item.dtype\n\n        # Generate a dtype identifier string\n        dtype_identifier = f\"no_header_numpy:{dtype.str}\"\n\n        # Convert the array into a bytes object\n        serialized_item = item.tobytes()\n\n        return serialized_item, dtype_identifier\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        # Convert the NumPy array into a bytes object\n        data = item.tobytes()\n\n        # Determine the data type of the array\n        indice = item.dtype.str.replace(\"|\", \"_\")\n\n        # Generate the dtype identifier string\n        dtype = f\"no_header_numpy:{indice}\"\n\n        # Return the serialized bytes object and the dtype identifier string\n        return data, dtype\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        return (\n            b\"\".join(\n                [\n                    np.dtype(item.dtype).num.to_bytes(1, \"little\"),\n                    np.uint8(len(item.shape)).to_bytes(1, \"little\"),\n                    *[\n                        np.uint32(dim).to_bytes(4, \"little\")\n                        for dim in item.shape\n                    ],\n                    item.tobytes(),\n                ]\n            ),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type index and number of dimensions of the array\n        dtype_index = np.dtype(item.dtype).num\n        ndim = item.ndim\n\n        # Get the shape of the array\n        shape = item.shape\n\n        # Convert the array to bytes\n        bytes_array = item.tobytes()\n\n        # Create a byte buffer to store the serialized data\n        buffer = bytearray()\n\n        # Write the data type index and number of dimensions to the buffer\n        buffer.extend(dtype_index.to_bytes(1, byteorder='little'))\n        buffer.extend(ndim.to_bytes(1, byteorder='little'))\n\n        # Write the shape of the array to the buffer\n        for dim in shape:\n            buffer.extend(dim.to_bytes(4, byteorder='little'))\n\n        # Write the array's binary data to the buffer\n        buffer.extend(bytes_array)\n\n        # Convert the buffer to bytes and return it along with None as metadata\n        return bytes(buffer), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        return (\n            np.array(item.dtype.num).tobytes()\n            + np.array(len(item.shape), dtype=np.uint32).tobytes()\n            + np.array(item.shape, dtype=np.uint32).tobytes()\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        return (\n            b\"\".join(\n                [\n                    np.array(item.dtype.num, dtype=np.uint8).tobytes(),\n                    np.array(item.ndim, dtype=np.uint8).tobytes(),\n                    np.array(item.shape, dtype=np.uint64).tobytes(),\n                    item.tobytes(),\n                ]\n            ),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        data_type_index = item.dtype.num\n        shape = item.shape\n        data = item.tobytes()\n\n        # Create a bytes object with the following structure:\n        # data_type_index (uint8) | num_dimensions (uint8) | shape_0 (uint32) | ... | shape_n (uint32) | data (bytes)\n        return (\n            data_type_index.tobytes()\n            + len(shape).to_bytes(1, \"big\")\n            + b\"\".join(dim.to_bytes(4, \"big\") for dim in shape)\n            + data\n        ), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type of the array\n        dtype = item.dtype\n        # Get the number of dimensions of the array\n        ndim = item.ndim\n        # Get the shape of the array\n        shape = item.shape\n        # Get the array's binary data\n        data = item.tobytes()\n\n        # Create a bytes object that includes the array's data type index, the number of dimensions, each dimension's size, and the array's binary data\n        return (\n            dtype.num,\n            ndim,\n            *shape,\n            data,\n        ), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        return (\n            b\"\\x00\"\n            + item.dtype.num.to_bytes(1, \"little\")\n            + len(item.shape).to_bytes(1, \"little\")\n            + b\"\".join(dim.to_bytes(4, \"little\") for dim in item.shape)\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Extract the data type, shape, and binary data from the NumPy array\n        dtype_index = np.dtype(item.dtype).num\n        shape = item.shape\n        data = item.tobytes()\n\n        # Serialize the data type, shape, and binary data into a bytes object\n        shape_bytes = struct.pack('I' * len(shape), *shape)\n        data_bytes = struct.pack('I', len(data)) + data\n        serialized = struct.pack('I', dtype_index) + struct.pack('I', len(shape)) + shape_bytes + data_bytes\n\n        # Return the serialized bytes object and None as a placeholder for metadata\n        return serialized, None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        if not isinstance(item, np.ndarray):\n            raise TypeError(f\"Expected np.ndarray, got {type(item)}\")\n\n        # Get the data type index, number of dimensions, and shape of the array\n        dtype_index = np.dtype(item.dtype).num\n        ndim = item.ndim\n        shape = item.shape\n\n        # Create a bytes object with the data type index, number of dimensions, and shape\n        metadata = struct.pack(\"<I\", dtype_index)\n        metadata += struct.pack(\"<I\", ndim)\n        for dim in shape:\n            metadata += struct.pack(\"<I\", dim)\n\n        # Append the array's binary data to the metadata\n        metadata += item.tobytes()\n\n        return metadata, None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Extract the data type, shape, and binary data from the NumPy array\n        dtype = item.dtype\n        shape = item.shape\n        data = item.tobytes()\n\n        # Construct the metadata string\n        metadata = f\"{dtype.str},{len(shape)},{','.join(map(str, shape))}\"\n\n        # Combine the metadata and binary data into a single bytes object\n        serialized_item = metadata.encode() + data\n\n        return serialized_item, None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type index of the array\n        dtype_index = np.dtype(item.dtype).num\n\n        # Get the number of dimensions of the array\n        ndim = item.ndim\n\n        # Get the shape of the array\n        shape = item.shape\n\n        # Get the array's binary data\n        data = item.tobytes()\n\n        # Serialize the data into a bytes object\n        serialized = struct.pack(f\">I{ndim}Q{len(data)}s\", dtype_index, *shape, data)\n\n        # Return the serialized bytes object and None as metadata\n        return serialized, None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type of the array\n        dtype = item.dtype\n        # Get the shape of the array\n        shape = item.shape\n        # Get the array's binary data\n        data = item.tobytes()\n        # Serialize the array's metadata into a JSON string\n        metadata = json.dumps({\"dtype\": dtype.str, \"shape\": shape})\n        # Serialize the array's metadata, data, and data type index into a bytes object\n        return (\n            struct.pack(\"I\", dtype.num)\n            + struct.pack(\"I\", len(shape))\n            + struct.pack(f\"{len(shape)}I\", *shape)\n            + data,\n            metadata,\n        )\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Extract the data type index, number of dimensions, and shape of the array\n        dtype_index = item.dtype.num\n        ndim = item.ndim\n        shape = item.shape\n\n        # Create a bytes object to store the serialized data\n        data = bytearray()\n\n        # Add the data type index, number of dimensions, and shape to the bytes object\n        data.extend(dtype_index.to_bytes(1, byteorder='little'))\n        data.extend(ndim.to_bytes(1, byteorder='little'))\n        for dim in shape:\n            data.extend(dim.to_bytes(4, byteorder='little'))\n\n        # Add the array's binary data to the bytes object\n        data.extend(item.tobytes())\n\n        # Return the bytes object and None as a placeholder for metadata\n        return bytes(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type and shape of the array\n        dtype = item.dtype\n        shape = item.shape\n\n        # Serialize the data type and shape\n        dtype_index = np.dtype(dtype).num\n        shape_bytes = np.array(shape, dtype=np.int64).tobytes()\n\n        # Serialize the array's binary data\n        array_bytes = item.tobytes()\n\n        # Concatenate the serialized data into a single bytes object\n        serialized_item = dtype_index.to_bytes(1, 'big') + shape_bytes + array_bytes\n\n        return serialized_item, None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Extract the data type, shape, and binary content of the NumPy array\n        dtype_index = np.dtype(item.dtype).num\n        shape = item.shape\n        buffer = item.tobytes()\n\n        # Serialize the array's metadata and binary data into a bytes object\n        metadata = np.array([dtype_index, len(shape), *shape], dtype=np.uint64).tobytes()\n        data = metadata + buffer\n\n        # Return the serialized bytes object and None as metadata\n        return data, None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Extract the data type, shape, and binary data of the input NumPy array\n        dtype_index = np.dtype(item.dtype).num\n        ndim = item.ndim\n        shape = item.shape\n        binary_data = item.tobytes()\n\n        # Create a bytes object with the array's data type index, number of dimensions, and each dimension's size\n        header = struct.pack(\">II\", dtype_index, ndim) + struct.pack(f\">{ndim}I\", *shape)\n\n        # Concatenate the header and binary data to create the final serialized bytes object\n        return header + binary_data, None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type index of the array\n        dtype = item.dtype.num\n\n        # Get the number of dimensions of the array\n        ndim = item.ndim\n\n        # Get the shape of the array\n        shape = item.shape\n\n        # Get the array's binary data\n        data = item.tobytes()\n\n        # Create a bytes object containing the data type index, number of dimensions, shape, and binary data\n        data = struct.pack(f\"{len(shape)}i\", *shape) + data\n\n        # Create a bytes object containing the data type index and number of dimensions, and append the shape and binary data\n        data = struct.pack(\"i\" + \"i\" * (ndim + 1), dtype, ndim, *shape) + data\n\n        # Return the serialized bytes object and None as placeholder for metadata\n        return data, None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type index and number of dimensions of the input array\n        dtype_index = js['dtype_index']\n        ndim = item.ndim\n\n        # Get the shape of the input array\n        shape = item.shape\n\n        # Get the binary data of the input array\n        data = item.tobytes()\n\n        # Serialize the array's metadata, including the data type index, number of dimensions, and each dimension's size\n        metadata = struct.pack('<Q', dtype_index) + struct.pack('<Q', ndim) + struct.pack(f'<{ndim}Q', *shape)\n\n        # Concatenate the metadata and the array's binary data to form the serialized bytes object\n        serialized_item = metadata + data\n\n        # Return the serialized bytes object and None as a placeholder for metadata\n        return serialized_item, None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type index and number of dimensions of the input array\n        dt_index = np.lib.format.dtype_to_descr(item.dtype)\n        ndim = item.ndim\n\n        # Create a byte array to store the serialized data\n        b = bytearray()\n\n        # Add the data type index to the byte array\n        b.extend(dt_index.encode('utf-8'))\n\n        # Add the number of dimensions to the byte array\n        b.extend(struct.pack('<i', ndim))\n\n        # Add each dimension's size to the byte array\n        for dim in item.shape:\n            b.extend(struct.pack('<i', dim))\n\n        # Add the array's binary data to the byte array\n        b.extend(item.tobytes())\n\n        # Convert the byte array to a bytes object and return it along with None as a placeholder for metadata\n        return bytes(b), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type of the array\n        dtype = item.dtype\n        # Get the index of the data type in the list of supported data types\n        dtype_index = js['class_name'].dtype_index.get(dtype.name, None)\n        if dtype_index is None:\n            raise ValueError(f\"Unsupported dtype: {dtype}\")\n\n        # Get the shape of the array\n        shape = item.shape\n        # Get the number of dimensions of the array\n        ndim = len(shape)\n\n        # Get the array's binary data\n        data = item.tobytes()\n\n        # Serialize the array's metadata, including the data type index, the number of dimensions, and each dimension's size\n        meta = struct.pack(\"<H\" + \"I\" * ndim, dtype_index, *shape)\n        # Concatenate the metadata and the array's binary data to form the serialized bytes object\n        return meta + data, None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            dataset_state = self.dataset.state_dict()\n        else:\n            dataset_state = self.dataset.state_dict()\n\n        state = {\n            \"dataset\": dataset_state,\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            dataset_state = self.dataset.state_dict()\n        else:\n            dataset_state = None\n\n        state = {\n            \"dataset\": dataset_state,\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            dataset_state_dict = self.dataset.state_dict()\n        else:\n            dataset_state_dict = self.dataset.state_dict()\n\n        state_dict = {\n            \"dataset\": dataset_state_dict,\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self.num_samples_yielded,\n                \"latest_worker_idx\": self.latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset,\n                \"batch_size\": self.batch_size,\n                \"num_workers\": self.num_workers,\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self.num_samples_yielded,\n                \"latest_worker_idx\": self.latest_worker_idx,\n            }\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {\n            \"dataset\": self.dataset.state_dict(),\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        # Check if the dataset is an instance of StreamingDataset\n        if isinstance(self.dataset, StreamingDataset):\n            # If it is, create a dictionary with the state of the dataset\n            dataset_state = self.dataset.state_dict()\n        else:\n            # If not, create an empty dictionary\n            dataset_state = {}\n\n        # Create a dictionary with the state of the StreamingDataLoader instance\n        state_dict = {\n            \"dataset\": dataset_state,\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        # If the dataset is an instance of StreamingDataset, the state dictionary includes the dataset's state, current epoch, number of samples yielded, and the index of the latest worker.\n        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self.num_samples_yielded,\n                \"latest_worker_idx\": self.latest_worker_idx,\n            }\n        # If the dataset is not an instance of StreamingDataset, the state dictionary includes only the current epoch, number of samples yielded, and the index of the latest worker.\n        else:\n            return {\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self.num_samples_yielded,\n                \"latest_worker_idx\": self.latest_worker_idx,\n            }\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            dataset_state = self.dataset.state_dict()\n        else:\n            dataset_state = self.dataset.state_dict()\n\n        state = {\n            \"dataset\": dataset_state,\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {\n            \"dataset\": self.dataset.state_dict(),\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            dataset_state = self.dataset.state_dict()\n        else:\n            dataset_state = self.dataset.state_dict()\n\n        state = {\n            \"dataset\": dataset_state,\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        # Check if the dataset is a StreamingDataset\n        if isinstance(self.dataset, StreamingDataset):\n            # If it is, generate the state dictionary for the dataset\n            dataset_state = self.dataset.state_dict()\n        else:\n            # Otherwise, set the dataset state to None\n            dataset_state = None\n\n        # Generate the state dictionary for the StreamingDataLoader\n        state_dict = {\n            \"dataset\": dataset_state,\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            dataset_state = self.dataset.state_dict()\n        else:\n            dataset_state = self.dataset.state_dict()\n\n        state = {\n            \"dataset\": dataset_state,\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            dataset_state = self.dataset.state_dict()\n        else:\n            dataset_state = {}\n\n        state = {\n            \"dataset\": dataset_state,\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        try:\n            import torchvision\n            import av\n        except ImportError:\n            raise Exception(\"torchvision and av libraries are required to deserialize video data\")\n\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            f.flush()\n            video, audio, info = torchvision.io.read_video(f.name)\n\n        os.remove(f.name)\n\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        try:\n            import torchvision\n        except ImportError:\n            raise Exception(\n                \"torchvision is not installed. Please install it with `pip install torchvision`\"\n            )\n        try:\n            import av\n        except ImportError:\n            raise Exception(\"av is not installed. Please install it with `pip install av`\")\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            video, audio, info = torchvision.io.read_video(f.name)\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not self.is_installed():\n            raise Exception(\n                \"torchvision is not installed. Please install it with `pip install torchvision`\"\n            )\n\n        if not self.is_av_installed():\n            raise Exception(\n                \"av is not installed. Please install it with `pip install av`\"\n            )\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name)\n\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCHVISION_AVAILABLE:\n            raise RuntimeError(\"torchvision isn't installed.\")\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name)\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        try:\n            import torchvision\n            import av\n        except ImportError:\n            raise Exception(\n                \"Please install torchvision and av to use this serializer.\"\n            )\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name)\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCHVISION_AVAILABLE:\n            raise RuntimeError(\"torchvision not installed, please install torchvision to use this serializer\")\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = read_video(f.name)\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        try:\n            import torchvision\n        except ImportError:\n            raise Exception(\n                \"torchvision is not installed. Please install torchvision to use this serializer.\"\n            )\n\n        try:\n            import av\n        except ImportError:\n            raise Exception(\n                \"av is not installed. Please install av to use this serializer.\"\n            )\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            video, audio, info = torchvision.io.read_video(f.name)\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        try:\n            import torchvision\n            import av\n        except ImportError:\n            raise ImportError(\n                \"torchvision and av are required for video deserialization. Please install them first.\"\n            )\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as temp_file:\n            temp_file.write(data)\n            temp_file.flush()\n            video, _, _ = torchvision.io.read_video(temp_file.name)\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not torchvision_installed:\n            raise Exception(\n                \"torchvision is not installed. Please install it with `pip install torchvision`\"\n            )\n\n        if not av_installed:\n            raise Exception(\"av is not installed. Please install it with `pip install av`\")\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = read_video(f.name)\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        try:\n            import torchvision\n            import av\n        except ImportError:\n            raise Exception(\"torchvision and av libraries are required to deserialize video data\")\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            video, audio, info = torchvision.io.read_video(f.name)\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not is_torchvision_installed():\n            raise Exception(\"torchvision is not installed. Please install it with `pip install torchvision`.\")\n\n        if not is_av_installed():\n            raise Exception(\"av is not installed. Please install it with `pip install av`.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = read_video(f.name)\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        try:\n            import torchvision\n            import av\n        except ImportError:\n            raise Exception(\"torchvision and av libraries are required for deserializing video data.\")\n\n        with tempfile.NamedTemporaryFile(suffix='.mp4') as temp_file:\n            temp_file.write(data)\n            temp_file.flush()\n            video, audio, info = torchvision.io.read_video(temp_file.name)\n\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not torchvision_available:\n            raise Exception(\n                \"torchvision is not installed. Please install torchvision to use this serializer.\"\n            )\n        if not av_available:\n            raise Exception(\n                \"av is not installed. Please install av to use this serializer.\"\n            )\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name)\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        try:\n            import torchvision\n        except ImportError:\n            raise Exception(\n                \"torchvision is not installed. Please install it with `pip install torchvision`\"\n            )\n        try:\n            import av\n        except ImportError:\n            raise Exception(\"av is not installed. Please install it with `pip install av`\")\n\n        with tempfile.NamedTemporaryFile() as tmp:\n            tmp.write(data)\n            tmp.flush()\n            video, _, _ = torchvision.io.read_video(tmp.name)\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        # Check if the required libraries (torchvision and av) are installed\n        try:\n            import torchvision\n            import av\n        except ImportError:\n            raise ImportError(\"The torchvision and av libraries are required for deserializing video data.\")\n\n        # Write the data to a temporary file\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            temp_file_path = f.name\n\n        # Use torchvision's read_video function to deserialize the video file into a video object\n        video, _, _ = torchvision.io.read_video(temp_file_path)\n\n        # Delete the temporary file\n        os.remove(temp_file_path)\n\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not self.is_available():\n            raise RuntimeError(\n                \"torchvision is not installed. Please install it to use this serializer.\"\n            )\n        if not self.is_available_av():\n            raise RuntimeError(\n                \"av is not installed. Please install it to use this serializer.\"\n            )\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            return torchvision.io.read_video(f.name)\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        try:\n            import torchvision\n            import av\n        except ImportError:\n            raise Exception(\n                \"torchvision and av are required to deserialize video data. Please install them before using this function.\"\n            )\n\n        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n            temp_file.write(data)\n            temp_file.flush()\n            video, _, _ = torchvision.io.read_video(temp_file.name)\n\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCHVISION_AVAILABLE:\n            raise RuntimeError(\"torchvision isn't available.\")\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            video = torchvision.io.read_video(f.name)[0]\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        try:\n            import torchvision\n            import av\n        except ImportError:\n            raise Exception(\"Please install torchvision and av to use the VideoSerializer\")\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name)\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not self._check_torchvision_av_installed():\n            raise Exception(\n                \"torchvision and av are required for deserializing video data.\"\n            )\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as temp_file:\n            temp_file.write(data)\n            temp_file.flush()\n            video, _, _ = torchvision.io.read_video(temp_file.name)\n\n        return video\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if not self._should_write():\n            return []\n\n        # write any remaining chunks to files\n        self.write_chunk()\n\n        # generate index file for chunks\n        self.write_chunks_index()\n\n        self._is_done = True\n\n        return self.chunks_paths\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunk()\n        self.write_chunks_index()\n        self._is_done = True\n        return self.chunks_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunk()\n        self.write_chunks_index()\n        self._is_done = True\n        return self.chunks_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunk()\n\n        self.write_chunks_index()\n        self._is_done = True\n        return self._chunks_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunk()\n        self.write_chunks_index()\n        self._is_done = True\n        return self.chunks_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write():\n            self.write_chunks_index()\n        return self.write_chunks()\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunks()\n        if self._should_write_index:\n            self.write_chunks_index()\n        self._is_done = True\n        return self.file_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunks()\n            self.write_chunks_index()\n            self.filled = True\n        self._is_done = True\n        return self.chunks_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunks()\n            self.write_chunks_index()\n        self._is_done = True\n        return self.chunks_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunk()\n\n        self.write_chunks_index()\n\n        self._is_done = True\n\n        return self.chunks_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunks()\n            self.write_chunks_index()\n\n        self._is_done = True\n\n        return self.file_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunks_index()\n            self.write_chunk()\n        self._is_done = True\n        return self.chunks_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunks_index()\n\n        self._is_done = True\n        return self.chunks_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if not self._should_write():\n            return []\n\n        self.write_chunk()\n        self.write_chunks_index()\n\n        self._is_done = True\n\n        return self.chunks\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if not self._should_write:\n            return []\n\n        chunk_paths = []\n        for chunk_id, chunk in self.chunks.items():\n            chunk_path = self.write_chunk(chunk_id, chunk)\n            chunk_paths.append(chunk_path)\n\n        self.write_chunks_index(chunk_paths)\n\n        self._is_done = True\n\n        return chunk_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunk()\n        self.write_chunks_index()\n        self._is_done = True\n        return self.chunks\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunk()\n        self.write_chunks_index()\n        self.filled = True\n        self._is_done = True\n        return self.file_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if not self._should_write:\n            return []\n\n        self.write_chunks_index()\n        self._is_done = True\n        return self.write_chunk()\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunks_index()\n            self.filled = True\n\n        self._is_done = True\n        return self.file_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunks_index()\n        self._is_done = True\n        return self.chunks_paths\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj['current_epoch']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.latest_worker_index = obj['latest_worker_index']\n        self.prepare_for_resuming()\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset_state_dict'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset_state_dict'])\n        else:\n            raise RuntimeError('Invalid dataset type')"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker = obj[\"latest_worker\"]\n\n        self.prep_for_resuming()\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset_state_dict\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset_state_dict\"])\n        else:\n            raise RuntimeError(\n                f\"Unsupported dataset type: {type(self.dataset)}. Only StreamingDataset and CombinedStreamingDataset are supported.\"\n            )"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj['epoch']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.latest_worker = obj['latest_worker']\n        self.num_workers_ready = 0\n        self.num_workers_finished = 0\n        self.worker_futures = {}\n        self.worker_futures_done = []\n        self.worker_futures_done_event = threading.Event()\n        self.worker_futures_done_event.clear()\n        self.worker_futures_done_lock = threading.Lock()\n        self.worker_futures_done_lock.acquire()\n        self.worker_futures_done_lock.release()\n        self.worker_futures_done_cv = threading.Condition()\n        self.worker_futures_done_cv.acquire()\n        self.worker_futures_done_cv.release()\n        self.worker_futures_done_cv.notify_all()\n        self.worker_futures_done_cv.acquire()\n        self.worker_futures_done_cv.release()\n        self.worker_futures_done_cv.notify_all()\n        self.worker_futures_done_cv.acquire()\n        self.worker_futures_done_cv.release()\n        self.worker_futures_done_cv.notify_all()\n        self.worker_futures_done_cv.acquire()\n        self.worker_futures_done_cv.release()\n        self.worker_futures_done_cv.notify_all()\n        self.worker_futures_done_cv.acquire()\n        self.worker_futures_done_cv.release()\n        self.worker_futures_done_cv.notify_all()\n        self.worker_futures_done_cv.acquire()\n        self.worker_futures_done_cv.release()\n        self.worker_f"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj['epoch']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.latest_worker_index = obj['latest_worker_index']\n        self.prepare_for_resuming()\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        else:\n            raise RuntimeError(\n                f'Unsupported dataset type: {type(self.dataset)}. Only StreamingDataset and CombinedStreamingDataset are supported.'\n            )\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj['epoch']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.latest_worker = obj['latest_worker']\n\n        # Prepare the DataLoader for resuming.\n        self._iterator = iter(self._dataset)\n        self._consumed_all_data = False\n        self._num_workers_at_last_check = 0\n\n        # Update the dataset state if it is a StreamingDataset or a CombinedStreamingDataset.\n        if isinstance(self._dataset, StreamingDataset):\n            self._dataset.load_state_dict(obj['dataset_state'])\n        elif isinstance(self._dataset, CombinedStreamingDataset):\n            self._dataset.load_state_dict(obj['dataset_state'])\n        else:\n            raise RuntimeError(\n                'The dataset should be a StreamingDataset or a CombinedStreamingDataset.')"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj[\"epoch\"]\n        self.num_yielded_samples = obj[\"num_yielded_samples\"]\n        self.latest_worker = obj[\"latest_worker\"]\n\n        # Reset the iterator and worker_info when resuming\n        self.iterator = iter(self)\n        self.worker_info = torch.utils.data.get_worker_info()\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset_state\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset_state\"])\n        else:\n            raise RuntimeError(\n                f\"Cannot load state for dataset of type {type(self.dataset)}\"\n            )\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj[\"epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker = obj[\"latest_worker\"]\n\n        self.worker_id = self.latest_worker\n        self.num_workers_started = self.latest_worker + 1\n        self.worker_seeding = {\n            i: self.seed + self.epoch + self.num_workers_started + i\n            for i in range(self.num_workers_started)\n        }\n        self.finished = False\n        self.iterator = None\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"StreamingDataLoader only supports StreamingDataset and CombinedStreamingDataset\"\n            )\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj['epoch']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.latest_worker_index = obj['latest_worker_index']\n        self.num_workers = obj['num_workers']\n        self.num_workers_to_start = obj['num_workers_to_start']\n        self.worker_id = obj['worker_id']\n        self.dataset.load_state_dict(obj['dataset'])\n        self.worker_id = obj['worker_id']\n        self.num_workers_to_start = obj['num_workers_to_start']\n        self.num_workers = obj['num_workers']\n        self.latest_worker_index = obj['latest_worker_index']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.epoch = obj['epoch']\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        else:\n            raise RuntimeError('Unknown dataset type')\n\n        self.worker_id = obj['worker_id']\n        self.num_workers_to_start = obj['num_workers_to_start']\n        self.num_workers = obj['num_workers']\n        self.latest_worker_index = obj['latest_worker_index']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.epoch = obj['epoch']\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        else:\n            raise RuntimeError('Unknown dataset type')"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj['epoch']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.latest_worker = obj['latest_worker']\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        else:\n            raise RuntimeError('Unknown dataset type')\n\n        self.batch_sampler.load_state_dict(obj['batch_sampler'])\n\n        self.iterator = iter(self.batch_sampler)\n        self._shutdown_workers()\n        self._resume_workers()\n\n        self._wakeup_workers()\n        self._shutdown = False\n\n        return\n\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj['epoch']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.latest_worker = obj['latest_worker']\n\n        # Prepare the DataLoader for resuming\n        self.worker_info_queue.put(None)\n        self.num_workers_ready = 0\n        self.finished = False\n\n        # Update the state of the dataset if it is a StreamingDataset or a CombinedStreamingDataset\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        else:\n            raise RuntimeError(\n                f'Dataset type {type(self.dataset)} not supported. Only StreamingDataset and CombinedStreamingDataset are supported.'\n            )\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj[\"epoch\"]\n        self.num_yielded_samples = obj[\"num_yielded_samples\"]\n        self.latest_worker = obj[\"latest_worker\"]\n        self.dataset.load_state_dict(obj[\"dataset_state_dict\"])\n\n        # Prepare dataloader for resuming\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.reset_index()\n            self.dataset.reset_epoch_state()\n            self.dataset.reset_worker_info()\n            self.dataset.reset_workers_state()\n            self.dataset.reset_workers_iterator()\n            self.dataset.reset_finished_workers()\n            self.dataset.reset_finished_workers_lock()\n            self.dataset.reset_finished_workers_condition()\n            self.dataset.reset_finished_workers_cv()\n            self.dataset.reset_finished_workers_cv_lock()\n            self.dataset.reset_finished_workers_cv_lock_flag()\n            self.dataset.reset_finished_workers_cv_lock_flag_lock()\n            self.dataset.reset_finished_workers_cv_lock_flag_cv()\n            self.dataset.reset_finished_workers_cv_lock_flag_cv_lock()\n            self.dataset.reset_finished_workers_cv_lock_flag_cv_lock_flag()\n            self.dataset.reset_finished_workers_cv_lock_flag_cv_lock_flag_lock()\n            self.dataset.reset_finished_workers_cv_lock_flag_cv_lock_flag_cv()\n            self.dataset.reset_finished_workers_cv_lock_flag_cv_lock_flag_cv_lock()\n            self.dataset.reset_finished_workers_cv_lock_flag_cv_lock_flag_cv_lock_flag()\n            self.dataset.reset_finished_workers_cv_lock_flag_cv_lock_flag_cv_lock_flag_lock()\n            self.dataset.reset_finished"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj['epoch']\n        self.num_yielded_so_far = obj['num_yielded_so_far']\n        self.latest_worker = obj['latest_worker']\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        else:\n            raise RuntimeError(f\"Unsupported dataset type: {type(self.dataset)}\")\n        self.worker_init_fn = obj['worker_init_fn']\n        self.dataset_kind = obj['dataset_kind']\n        self.worker_init_fn = obj['worker_init_fn']\n        self.batch_sampler = obj['batch_sampler']\n        self.num_workers = obj['num_workers']\n        self.pin_memory = obj['pin_memory']\n        self.timeout = obj['timeout']\n        self.prefetch_factor = obj['prefetch_factor']\n        self.persistent_workers = obj['persistent_workers']\n        self.batch_size = obj['batch_size']\n        self.drop_last = obj['drop_last']\n        self.shuffle = obj['shuffle']\n        self.sampler = obj['sampler']\n        self.collate_fn = obj['collate_fn']\n        self.batch_size = obj['batch_size']\n        self.drop_last = obj['drop_last']\n        self.timeout = obj['timeout']\n        self.prefetch_factor = obj['prefetch_factor']\n        self.persistent_workers = obj['persistent_workers']\n        self.pin_memory = obj['pin_memory']\n        self.num_workers = obj['num_workers']\n        self.worker_init_fn = obj['worker_init_fn']\n        self.sampler = obj['sampler']\n        self.collate_fn = obj['collate_fn']\n        self"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj['epoch']\n        self.num_yielded = obj['num_yielded']\n        self.latest_worker = obj['latest_worker']\n        self._iterator = iter(self._infinite_iterator)\n        self._maybe_advance_next_epoch()\n        self._reset_iterator_and_epoch()\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset_state_dict'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset_state_dict'])\n        else:\n            raise RuntimeError('Unknown dataset type')\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj['epoch']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.latest_worker = obj['latest_worker']\n\n        # Prepare dataloader for resuming\n        self.prepare_resuming()\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        else:\n            raise RuntimeError(\n                'StreamingDataLoader only supports StreamingDataset and CombinedStreamingDataset, but got {}.'.format(type(self.dataset)))\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj['epoch']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.latest_worker_index = obj['latest_worker_index']\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset_state'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset_state'])\n        else:\n            raise RuntimeError(f'{type(self.dataset)} is not supported')\n\n        self.iterator = iter(self.dataset)\n        self.finished = False\n\n        if self.num_samples_yielded > 0:\n            for _ in range(self.num_samples_yielded):\n                try:\n                    next(self.iterator)\n                except StopIteration:\n                    self.finished = True\n                    break\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj['epoch']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.latest_worker = obj['latest_worker']\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        else:\n            raise RuntimeError(\n                f'Dataset type {type(self.dataset)} not supported.')\n\n        self._iterator = iter(self.dataset)\n        self._finished = False\n        self._finished_iteration = False\n        self._num_workers_at_last_check = 0\n        self._worker_seeds = [\n            self._seed + self.epoch + i for i in range(self.num_workers)]\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj[\"epoch\"]\n        self.num_yielded_samples = obj[\"num_yielded_samples\"]\n        self.latest_worker = obj[\"latest_worker\"]\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset_state_dict\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset_state_dict\"])\n        else:\n            raise RuntimeError(\n                \"Only StreamingDataset and CombinedStreamingDataset are supported for now.\"\n            )\n\n        # Prepare the DataLoader for resuming.\n        self.num_workers_per_epoch = self.num_workers\n        self.num_workers = 0\n        self.worker_id = 0\n        self.num_workers_per_epoch = self.num_workers\n        self.num_workers = 0\n        self.worker_id = 0\n        self.shuffle = False\n        self.sampler = None\n        self.iterator = None\n        self.finished = False\n\n        # Set the random seed for workers (so that workers sample the same shards in the same order).\n        self.set_epoch(self.epoch)\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        # Update the current epoch, number of samples yielded, and latest worker index\n        self.epoch = obj['epoch']\n        self.num_yielded_this_epoch = obj['num_yielded_this_epoch']\n        self.latest_worker = obj['latest_worker']\n\n        # Prepare the DataLoader for resuming\n        self.iterator = iter(self)\n        self._iterator_exhausted = False\n\n        # Handle specific logic for StreamingDataset and CombinedStreamingDataset\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        else:\n            raise RuntimeError(\n                f'Unsupported dataset type: {type(self.dataset)}')\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj['epoch']\n        self.num_yielded_samples = obj['num_yielded_samples']\n        self.latest_worker_idx = obj['latest_worker_idx']\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        else:\n            raise RuntimeError(f'Unsupported dataset type: {type(self.dataset)}')\n\n        self.worker_info_queue = queue.Queue()\n        self.index_queue = queue.Queue()\n        self.result_queue = queue.Queue()\n        self.shutdown = False\n        self.iterator = iter(self)\n\n        self.prepare_resume()\n\n        self.worker_info_queue.put(self.worker_info)\n        self.index_queue.put(self.num_yielded_samples)\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj['epoch']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.latest_worker_index = obj['latest_worker_index']\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset_state_dict'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset_state_dict'])\n        else:\n            raise RuntimeError(\n                f'StreamingDataLoader only supports StreamingDataset or CombinedStreamingDataset, but got {type(self.dataset)}'\n            )\n\n        self.dataset.prepare_for_resuming()\n        self.worker_init_fn = self.dataset.get_worker_init_fn()\n        self.dataset_iterator = iter(self.dataset)\n        self.reset_iterator = True\n        self.finished = False\n        self.iterator_pbar.update(self.num_samples_yielded)\n        self.iterator_pbar.set_description(f'Epoch {self.epoch}')\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._internal_iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_samples_yielded\": num_samples_yielded,\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                }\n        else:\n            if num_samples_yielded is None:\n                return self._internal_iterator.state_dict()\n            else:\n                return {\n                    \"num_samples_yielded\": num_samples_yielded,\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"internal_iterator\": self._internal_iterator.state_dict(),\n                }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.internal_iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_samples_yielded\": num_samples_yielded,\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                }\n        else:\n            if num_samples_yielded is None:\n                return self.internal_iterator.state_dict()\n            else:\n                return {\n                    \"num_samples_yielded\": num_samples_yielded,\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"internal_iterator\": self.internal_iterator.state_dict(),\n                }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.internal_iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_samples_yielded\": num_samples_yielded,\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                }\n\n        if self.internal_iterator.state_dict() is not None:\n            return self.internal_iterator.state_dict()\n        else:\n            return {\n                \"num_samples_yielded\": num_samples_yielded,\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n            }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None:\n            return {}\n        if num_samples_yielded is None:\n            return self.iterator.state_dict()\n        else:\n            return {\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n                \"num_samples_yielded\": num_samples_yielded,\n            }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._internal_iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            return {\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n                \"num_samples_yielded\": num_samples_yielded,\n            }\n\n        if self._internal_iterator.has_state():\n            return self._internal_iterator.state_dict()\n        else:\n            return {\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n                \"num_samples_yielded\": num_samples_yielded,\n                \"datasets\": [\n                    d.state_dict(num_workers, batch_size, num_samples_yielded)\n                    for d in self._datasets\n                ],\n            }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return self._generate_state_dict_from_datasets(num_workers, batch_size, num_samples_yielded)\n        else:\n            return self._iterator.state_dict(num_workers, batch_size, num_samples_yielded)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return self._create_state_dict_from_datasets(num_workers, batch_size, num_samples_yielded)\n        else:\n            if num_samples_yielded is None:\n                return self._iterator.state_dict()\n            else:\n                return self._create_state_dict_from_iterator(num_workers, batch_size, num_samples_yielded)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"num_samples_yielded\": num_samples_yielded,\n                }\n        else:\n            if num_samples_yielded is None:\n                return self._iterator.state_dict()\n            else:\n                return {\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"num_samples_yielded\": num_samples_yielded,\n                    \"iterator\": self._iterator.state_dict(),\n                }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._internal_iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_samples_yielded\": num_samples_yielded,\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                }\n        else:\n            if num_samples_yielded is None:\n                return self._internal_iterator.state_dict()\n            else:\n                return {\n                    \"num_samples_yielded\": num_samples_yielded,\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"internal_iterator\": self._internal_iterator.state_dict(),\n                }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.internal_iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_samples_yielded\": num_samples_yielded,\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                }\n        else:\n            return self.internal_iterator.state_dict()\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._internal_iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"num_samples_yielded\": num_samples_yielded,\n                }\n\n        if self._internal_iterator is not None:\n            return {\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n                \"num_samples_yielded\": num_samples_yielded,\n                \"internal_iterator\": self._internal_iterator.state_dict(),\n            }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"num_samples_yielded\": num_samples_yielded,\n                }\n        else:\n            if num_samples_yielded is None:\n                return self._iterator.state_dict()\n            else:\n                return {\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"num_samples_yielded\": num_samples_yielded,\n                    \"iterator_state_dict\": self._iterator.state_dict(),\n                }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._internal_iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return self._create_state_dict_from_datasets(num_workers, batch_size, num_samples_yielded)\n        else:\n            return self._internal_iterator.state_dict()\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._internal_iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"num_samples_yielded\": num_samples_yielded,\n                }\n        else:\n            return self._internal_iterator.state_dict(\n                num_workers, batch_size, num_samples_yielded\n            )\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"num_samples_yielded\": num_samples_yielded,\n                }\n        else:\n            return (\n                self.iterator.state_dict()\n                if self.iterator is not None\n                else self.datasets.state_dict()\n            )\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_samples_yielded\": num_samples_yielded,\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                }\n        else:\n            if num_samples_yielded is None:\n                return self._iterator.state_dict(num_workers, batch_size)\n            else:\n                return {\n                    \"num_samples_yielded\": num_samples_yielded,\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"iterator\": self._iterator.state_dict(num_workers, batch_size),\n                }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            return self._state_dict_from_datasets(num_workers, batch_size, num_samples_yielded)\n        else:\n            return self.iterator.state_dict()\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._internal_iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"num_samples_yielded\": num_samples_yielded,\n                }\n\n        if self._internal_datasets is None:\n            return self._internal_iterator.state_dict()\n        else:\n            return {\n                \"datasets\": [\n                    {\"dataset\": dataset, \"num_samples_yielded\": num_samples_yielded}\n                    for dataset, num_samples_yielded in zip(\n                        self._internal_datasets, num_samples_yielded\n                    )\n                ]\n            }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._internal_iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"num_samples_yielded\": num_samples_yielded,\n                }\n        elif self._internal_iterator is not None:\n            if num_samples_yielded is None:\n                return self._internal_iterator.state_dict()\n            else:\n                return {\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"num_samples_yielded\": num_samples_yielded,\n                    \"internal_iterator\": self._internal_iterator.state_dict(),\n                }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.internal_iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"num_samples_yielded\": num_samples_yielded,\n                }\n\n        if self.internal_iterator is not None:\n            if num_samples_yielded is None:\n                return {\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"datasets\": [\n                        dataset.state_dict(num_workers, batch_size)\n                        for dataset in self.datasets\n                    ],\n                }\n            else:\n                return self.internal_iterator.state_dict()\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset in self.datasets:\n            dataset.load_state_dict(state_dict[dataset.name])\n        if self.streaming_dataloader is not None:\n            self.streaming_dataloader.num_yielded_samples = state_dict[\n                \"num_yielded_samples\"\n            ]\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Update the state of each dataset within the CombinedStreamingDataset\n        for dataset in self.datasets:\n            dataset.load_state_dict(state_dict[dataset.name])\n\n        # Update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        self.num_yielded_samples = state_dict['num_yielded_samples']\n\n        # Update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        self.num_yielded_samples = state_dict['num_yielded_samples']\n\n        # Update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        self.num_yielded_samples = state_dict['num_yielded_samples']"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for key, value in state_dict.items():\n            if key == 'datasets':\n                for i, dataset in enumerate(value):\n                    self.datasets[i].load_state_dict(dataset)\n            else:\n                setattr(self, key, value)\n        if self.streaming_dataloader is not None:\n            self.streaming_dataloader.num_yielded_samples = self.num_yielded_samples\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Update the state of each dataset within the CombinedStreamingDataset\n        for dataset in self.datasets:\n            dataset.load_state_dict(state_dict)\n\n        # If applicable, update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        if self.streaming_dataloader is not None:\n            self.streaming_dataloader.num_samples_yielded = state_dict['num_samples_yielded']\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self.datasets = [\n            state_dict[f\"dataset_{i}\"] for i in range(len(self.datasets))\n        ]\n        self.num_samples = sum([len(dataset) for dataset in self.datasets])\n        self.streaming_dataloader.num_samples_yielded = state_dict[\n            \"num_samples_yielded\"\n        ]\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Update the state of each dataset within the CombinedStreamingDataset\n        for dataset in self.datasets:\n            dataset.load_state_dict(state_dict[dataset.name])\n\n        # Update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        self.num_samples_yielded = state_dict[\"num_samples_yielded\"]\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Load the state of the CombinedStreamingDataset\n        self.state = state_dict['state']\n\n        # Load the state of each dataset in the CombinedStreamingDataset\n        for dataset in self.datasets:\n            dataset.load_state_dict(state_dict[dataset.name])\n\n        # Update the number of samples yielded by the streaming dataloader\n        self.num_samples_yielded = state_dict['num_samples_yielded']\n\n        # Update the state of the CombinedStreamingDataset\n        self.state = state_dict['state']\n\n        # Update the number of samples yielded by the streaming dataloader\n        self.num_samples_yielded = state_dict['num_samples_yielded']\n\n        # Update the state of the CombinedStreamingDataset\n        self.state = state_dict['state']\n\n        # Update the number of samples yielded by the streaming dataloader\n        self.num_samples_yielded = state_dict['num_samples_yielded']"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Update the state of each dataset within the CombinedStreamingDataset\n        for dataset_name, dataset_state in state_dict.items():\n            self.datasets[dataset_name].load_state_dict(dataset_state)\n\n        # Update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        self.streaming_dataloader.num_samples_yielded = state_dict['streaming_dataloader']['num_samples_yielded']\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Load the state of the CombinedStreamingDataset\n        self.state_dict = state_dict\n\n        # Update the state of each dataset within the CombinedStreamingDataset\n        for dataset_name, dataset_state in self.state_dict['datasets'].items():\n            dataset = self.datasets[dataset_name]\n            dataset.load_state_dict(dataset_state)\n\n        # If applicable, update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        if self.streaming_dataloader:\n            self.streaming_dataloader.num_samples_yielded = self.state_dict['num_samples_yielded']\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset_name, dataset_state_dict in state_dict.items():\n            dataset = getattr(self, dataset_name)\n            dataset.load_state_dict(dataset_state_dict)\n            if dataset_name == self.streaming_dataset_name:\n                self.num_samples_yielded = dataset.num_samples_yielded\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset_name, dataset_state_dict in state_dict.items():\n            if dataset_name == 'streaming_dataloader':\n                self.streaming_dataloader.load_state_dict(dataset_state_dict)\n            else:\n                self.datasets[dataset_name].load_state_dict(dataset_state_dict)\n\n        self.update_num_yielded_samples()\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Load the state of the CombinedStreamingDataset from the state dictionary\n        self.state_dict = state_dict\n\n        # Update the state of each dataset within the CombinedStreamingDataset\n        for dataset in self.datasets:\n            dataset.load_state_dict(state_dict[dataset.name])\n\n        # Update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        self.streaming_dataloader.num_samples_yielded = state_dict[\"streaming_dataloader\"][\"num_samples_yielded\"]\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self.datasets = state_dict[\"datasets\"]\n        self.dataset_indices = state_dict[\"dataset_indices\"]\n        self.num_samples = state_dict[\"num_samples\"]\n        self.num_samples_yielded = state_dict[\"num_samples_yielded\"]\n\n        for dataset in self.datasets:\n            dataset.load_state_dict(state_dict[\"dataset_states\"][dataset.name])\n\n        if self.num_samples_yielded > self.num_samples:\n            self.num_samples_yielded = self.num_samples\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Load the state of the CombinedStreamingDataset instance\n        self.state_dict = state_dict\n\n        # Update the state of each dataset in the CombinedStreamingDataset\n        for dataset_name, dataset_state in state_dict['datasets'].items():\n            dataset = getattr(self, dataset_name)\n            dataset.load_state_dict(dataset_state)\n\n        # Update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        if self.streaming_dataloader is not None:\n            self.streaming_dataloader.num_yielded_samples = state_dict['num_yielded_samples']\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Iterate over each dataset in the CombinedStreamingDataset and load its state from the state dictionary\n        for i, dataset in enumerate(self.datasets):\n            dataset.load_state_dict(state_dict[f\"dataset_{i}\"])\n\n        # Update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        if \"num_samples_yielded\" in state_dict:\n            self.num_samples_yielded = state_dict[\"num_samples_yielded\"]\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Load the state of each dataset within the CombinedStreamingDataset\n        for dataset in self.datasets:\n            dataset.load_state_dict(state_dict)\n\n        # If the CombinedStreamingDataset is streaming, update the number of samples yielded to avoid repeating samples\n        if self.is_streaming:\n            self.num_samples_yielded = len(self)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Load the state of the CombinedStreamingDataset\n        self.state_dict = state_dict\n\n        # Update the state of each dataset within the CombinedStreamingDataset\n        for dataset in self.datasets:\n            dataset.state_dict = state_dict\n\n        # If applicable, update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        if self.streaming_dataloader is not None:\n            self.streaming_dataloader.num_samples_yielded = state_dict['num_samples_yielded']\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for ds_name, ds_state in state_dict.items():\n            ds_state_dict = ds_state['state_dict']\n            ds_streaming_dataloader = ds_state['streaming_dataloader']\n            ds = self._datasets[ds_name]\n            ds.load_state_dict(ds_state_dict)\n            if ds_streaming_dataloader:\n                ds_streaming_dataloader.num_samples_yielded = ds.num_samples_yielded\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Iterate over the state_dict items\n        for key, value in state_dict.items():\n            # Check if the key is in the dataset_dict\n            if key in self.dataset_dict:\n                # Load the state of the dataset from the state_dict\n                self.dataset_dict[key].load_state_dict(value)\n            # Check if the key is 'num_samples_yielded'\n            elif key == 'num_samples_yielded':\n                # Update the num_samples_yielded attribute of the CombinedStreamingDataset\n                self.num_samples_yielded = value\n            # Raise an error if the key is not in the dataset_dict or 'num_samples_yielded'\n            else:\n                raise KeyError(f\"Invalid key: {key}\")\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self.datasets.load_state_dict(state_dict)\n        self.num_samples_yielded = state_dict['num_samples_yielded']\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"project://\"):\n        return Dir(path=dir_path[10:])\n\n    return Dir(path=dir_path)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n    elif isinstance(dir_path, Dir):\n        return dir_path\n    elif dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path[7:])\n    elif dir_path.startswith(\"project://\"):\n        return Dir(project_path=dir_path[10:])\n    else:\n        return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n    elif isinstance(dir_path, Dir):\n        return dir_path\n    elif isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"projects/\"):\n            return Dir(path=dir_path)\n        else:\n            return Dir(path=dir_path, local=True)\n    else:\n        raise ValueError(f\"Unsupported dir_path type: {type(dir_path)}\")\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"project://\"):\n            return Dir(project_path=dir_path)\n        else:\n            return Dir(path=dir_path)\n\n    elif isinstance(dir_path, Dir):\n        return dir_path\n\n    else:\n        raise ValueError(f\"Invalid dir_path type: {type(dir_path)}\")\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(url=f\"s3://{dir_path}\")\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    return Dir(path=f\"{get_project_root()}/{dir_path}\")\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"project://\"):\n        return Dir(path=dir_path.replace(\"project://\", \"\"))\n\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path.replace(\"file://\", \"\"))\n\n    if dir_path.startswith(\"data://\"):\n        return Dir(path=dir_path.replace(\"data://\", \"\"))\n\n    if dir_path.startswith(\"model://\"):\n        return Dir(path=dir_path.replace(\"model://\", \"\"))\n\n    if dir_path.startswith(\"output://\"):\n        return Dir(path=dir_path.replace(\"output://\", \"\"))\n\n    if dir_path.startswith(\".\"):\n        return Dir(path=dir_path)\n\n    return Dir(path=dir_path)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n    if isinstance(dir_path, Dir):\n        return dir_path\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"project://\"):\n        return Dir(project=dir_path[10:])\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n    return Dir(path=os.path.join(os.getcwd(), dir_path))\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n    if isinstance(dir_path, Dir):\n        return dir_path\n    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(url=dir_path)\n        if dir_path.startswith(\"file://\"):\n            return Dir(path=dir_path[len(\"file://\") :])\n        if dir_path.startswith(\"project://\"):\n            return Dir(project_path=dir_path[len(\"project://\") :])\n        return Dir(path=dir_path)\n    raise ValueError(f\"Cannot resolve dir {dir_path}\")"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n    elif isinstance(dir_path, Dir):\n        return dir_path\n    elif isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"gs://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"hdfs://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"http://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"https://\"):\n            return Dir(url=dir_path)\n        else:\n            return Dir(path=dir_path)\n    else:\n        raise ValueError(f\"Invalid dir_path: {dir_path}\")\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir(path=\"\")\n    if isinstance(dir_path, Dir):\n        return dir_path\n    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(url=dir_path)\n        if dir_path.startswith(\"gs://\"):\n            return Dir(url=dir_path)\n        if dir_path.startswith(\"file://\"):\n            return Dir(path=dir_path[7:])\n        if dir_path.startswith(\"project://\"):\n            return Dir(path=os.path.join(project_dir, dir_path[10:]))\n        return Dir(path=dir_path)\n    raise ValueError(\"dir_path must be a string or Dir object\")\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"gs://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"hdfs://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"~\"):\n            return Dir(path=os.path.expanduser(dir_path))\n        elif dir_path.startswith(\"./\"):\n            return Dir(path=os.path.abspath(dir_path))\n        elif dir_path.startswith(\"../\"):\n            return Dir(path=os.path.abspath(dir_path))\n        else:\n            return Dir(path=dir_path)\n\n    raise ValueError(f\"Unsupported dir path type: {type(dir_path)}\")"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"projects/\"):\n            return Dir(project_path=dir_path)\n        else:\n            return Dir(path=dir_path)\n\n    return dir_path\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        dir_path = \".\"\n    if isinstance(dir_path, Dir):\n        return dir_path\n    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"gs://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"r2://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"https://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"http://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"~\"):\n            return Dir(path=dir_path)\n        elif dir_path.startswith(\".\"):\n            return Dir(path=dir_path)\n        elif dir_path.startswith(\"/\"):\n            return Dir(path=dir_path)\n        else:\n            return Dir(path=dir_path)\n    else:\n        raise ValueError(f\"Invalid dir_path type: {type(dir_path)}\")"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith('s3://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('gs://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('hub://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('hdfs://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('http://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('https://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('redis://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('mongodb://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('postgresql://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('mysql://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('sqlite://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('mssql://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('oracle://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('elasticsearch://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('snowflake://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('bigquery://'):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith('azure://'):"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path[7:])\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"dbfs:/\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"wasbs:/\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"abfss:/\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"hdfs:/\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"ftp://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"sftp://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"ftps://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"sftps://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"ssh://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"scp://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"smb://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"nfs://\"):\n        return Dir(url=dir_path)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n    elif isinstance(dir_path, Dir):\n        return dir_path\n    elif dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"http\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"hdfs://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"az://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"abfs://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"file://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"s3n://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"s3a://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"wasb://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"wasbs://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"adl://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"abfss://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"abfs://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"abfss://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    # If the input is already a Dir object, return it as is\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    # If the input is None, return None\n    if dir_path is None:\n        return None\n\n    # If the input is a local path, return a Dir object with the local path set\n    if os.path.exists(dir_path):\n        return Dir(local_path=dir_path)\n\n    # If the input is a S3 URL, return a Dir object with the S3 URL set\n    if dir_path.startswith(\"s3://\"):\n        return Dir(s3_url=dir_path)\n\n    # If the input is a project path, return a Dir object with the project path set\n    if dir_path.startswith(\"projects/\"):\n        return Dir(project_path=dir_path)\n\n    # If the input is a local path with a prefix, return a Dir object with the local path and prefix set\n    if dir_path.startswith(\"local:\"):\n        return Dir(local_path=dir_path[6:], prefix=\"local\")\n\n    # If the input is a S3 URL with a prefix, return a Dir object with the S3 URL and prefix set\n    if dir_path.startswith(\"s3:\"):\n        return Dir(s3_url=dir_path[4:], prefix=\"s3\")\n\n    # If the input is a project path with a prefix, return a Dir object with the project path and prefix set\n    if dir_path.startswith(\"project:\"):\n        return Dir(project_path=dir_path[9:], prefix=\"project\")\n\n    # If the input is a project path with a prefix, return a Dir object with the project path and prefix set\n    if dir_path.startswith(\"projects:\"):\n        return Dir(project_path=dir_path[9:], prefix=\"project\")\n\n    # If the input is a project path with a prefix, return a Dir object with the project path and prefix set\n    if dir_path.startswith(\"projects/\"):"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n    elif isinstance(dir_path, Dir):\n        return dir_path\n    elif dir_path.startswith('s3://'):\n        return Dir(s3=dir_path)\n    elif dir_path.startswith('gs://'):\n        return Dir(gs=dir_path)\n    elif dir_path.startswith('http://') or dir_path.startswith('https://'):\n        return Dir(http=dir_path)\n    elif dir_path.startswith('hub://'):\n        return Dir(hub=dir_path)\n    elif dir_path.startswith('ipfs://'):\n        return Dir(ipfs=dir_path)\n    elif dir_path.startswith('redis://'):\n        return Dir(redis=dir_path)\n    elif dir_path.startswith('sftp://'):\n        return Dir(sftp=dir_path)\n    elif dir_path.startswith('ssh://'):\n        return Dir(ssh=dir_path)\n    elif dir_path.startswith('webdav://'):\n        return Dir(webdav=dir_path)\n    elif dir_path.startswith('ftp://'):\n        return Dir(ftp=dir_path)\n    elif dir_path.startswith('file://'):\n        return Dir(file=dir_path)\n    elif dir_path.startswith('project://'):\n        return Dir(project=dir_path)\n    elif dir_path.startswith('dataset://'):\n        return Dir(dataset=dir_path)\n    elif dir_path.startswith('model://'):\n        return Dir(model=dir_path)\n    elif dir_path.startswith('evaluation://'):\n        return Dir(evaluation=dir_path)\n    elif dir_path.startswith('run://'):\n        return Dir(run=dir_path)\n    elif dir_path.startswith('operation://'):\n        return Dir"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir(local_path=\"\")\n    if isinstance(dir_path, Dir):\n        return dir_path\n    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(s3_url=dir_path)\n        elif dir_path.startswith(\"gs://\"):\n            return Dir(gcs_url=dir_path)\n        elif dir_path.startswith(\"hdfs://\"):\n            return Dir(hdfs_url=dir_path)\n        elif dir_path.startswith(\"file://\"):\n            return Dir(local_path=dir_path[7:])\n        elif dir_path.startswith(\"https://\"):\n            return Dir(http_url=dir_path)\n        elif dir_path.startswith(\"http://\"):\n            return Dir(http_url=dir_path)\n        elif dir_path.startswith(\"project://\"):\n            return Dir(project_path=dir_path[11:])\n        elif dir_path.startswith(\"/\"):\n            return Dir(local_path=dir_path)\n        else:\n            return Dir(local_path=dir_path)\n    else:\n        raise ValueError(f\"Invalid dir_path type: {type(dir_path)}\")\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    # If the dir_path is None, return the default Dir object with the default path\n    if dir_path is None:\n        return Dir(DEFAULT_DIR_PATH)\n\n    # If the dir_path is already a Dir object, return it as is\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    # If the dir_path is a local path, return a Dir object with the local path set\n    if os.path.exists(dir_path):\n        return Dir(dir_path)\n\n    # If the dir_path is a S3 URL, return a Dir object with the S3 URL set\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    # If the dir_path is a project path, return a Dir object with the project path set\n    if dir_path.startswith(\"project://\"):\n        return Dir(path=os.path.join(get_project_dir(), dir_path[10:]))\n\n    # If the dir_path is a relative path, return a Dir object with the relative path set\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.join(get_project_dir(), dir_path[2:]))\n\n    # If the dir_path is a project-relative path, return a Dir object with the project-relative path set\n    if dir_path.startswith(\"project://\"):\n        return Dir(path=os.path.join(get_project_dir(), dir_path[10:]))\n\n    # If the dir_path is a URL, return a Dir object with the URL set\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n\n    # If none of the above conditions are met, raise a ValueError\n    raise ValueError(f\"Invalid dir_path: {dir_path}\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of the Dir class\")\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must be an S3 directory\")\n    if not append and not overwrite and output_dir.exists():\n        raise ValueError(\"output_dir must be empty\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of the Dir class.\")\n\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must be an S3 directory.\")\n\n    if not append and not overwrite:\n        if output_dir.exists():\n            raise ValueError(\"output_dir must be empty.\")\n\n    return"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of the Dir class.\")\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must be an S3 directory.\")\n    if append or overwrite:\n        raise NotImplementedError(\"append and overwrite are not yet implemented.\")\n    if output_dir.list_contents():\n        raise ValueError(\"output_dir must be empty.\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of the Dir class.\")\n\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must be an S3 bucket.\")\n\n    if output_dir.exists():\n        if not output_dir.empty():\n            raise ValueError(\"output_dir must be empty.\")\n\n    if append:\n        raise NotImplementedError(\"append is not implemented yet.\")\n\n    if overwrite:\n        raise NotImplementedError(\"overwrite is not implemented yet.\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(f\"output_dir must be an instance of Dir, not {type(output_dir)}\")\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(f\"output_dir must be an S3 directory, not {output_dir.path}\")\n    if append or overwrite:\n        raise NotImplementedError(\"append and overwrite are not implemented yet\")\n    if output_dir.exists():\n        if output_dir.is_empty():\n            return\n        else:\n            raise ValueError(f\"output_dir {output_dir.path} is not empty\")\n    else:\n        return\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    # Check if the output_dir is a Dir object\n    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of the Dir class\")\n\n    # Check if the output_dir starts with \"s3://\"\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must start with 's3://'\")\n\n    # Check if the output_dir is empty\n    if len(output_dir.ls()) > 0:\n        raise ValueError(\"output_dir is not empty\")\n\n    # Check if appending or overwriting is allowed\n    if append:\n        raise NotImplementedError(\"Appending data is not supported yet\")\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data is not supported yet\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    # Check if the output_dir is a Dir object\n    if not isinstance(output_dir, Dir):\n        raise ValueError(\"output_dir must be a Dir object\")\n\n    # Check if the output_dir starts with \"s3://\"\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must start with 's3://'\")\n\n    # Check if the output_dir is empty\n    if output_dir.exists() and any(output_dir.iterdir()):\n        raise ValueError(\"output_dir must be empty\")\n\n    # Check if appending or overwriting is allowed\n    if append:\n        raise NotImplementedError(\"Appending data to the output_dir is not yet implemented\")\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data in the output_dir is not yet implemented\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    # Check that the output_dir is a Dir object\n    if not isinstance(output_dir, Dir):\n        raise ValueError(\"output_dir must be a Dir object\")\n\n    # Check that the output_dir starts with \"s3://\"\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must start with 's3://'\")\n\n    # Check if the directory is empty\n    if output_dir.is_empty():\n        return\n\n    # Check if appending or overwriting is allowed\n    if append or overwrite:\n        raise NotImplementedError(\"Appending and overwriting data in the directory is not yet supported\")\n\n    # Raise an error if the directory is not empty\n    raise ValueError(\"The output directory is not empty\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    # Check if the output_dir is a Dir object\n    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be a Dir object\")\n\n    # Check if the output_dir is an S3 directory\n    if not output_dir.uri.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must be an S3 directory\")\n\n    # Check if the output_dir is empty\n    if len(output_dir.ls()) > 0:\n        raise ValueError(\"output_dir must be empty\")\n\n    # Check if appending or overwriting is allowed\n    if append:\n        raise NotImplementedError(\"Appending data to an S3 directory is not supported\")\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data in an S3 directory is not supported\")\n\n    return None"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    # Check if output_dir is a Dir object\n    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of the Dir class\")\n\n    # Check if output_dir starts with \"s3://\"\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must start with 's3://'\")\n\n    # Check if output_dir is empty\n    if not output_dir.empty():\n        raise ValueError(\"output_dir must be empty\")\n\n    # Check if append and overwrite are False\n    if append:\n        raise ValueError(\"append is not implemented yet\")\n    if overwrite:\n        raise ValueError(\"overwrite is not implemented yet\")\n\n    # If all checks pass, return without error\n    return"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    # Check if the output_dir is a Dir object\n    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of the Dir class\")\n\n    # Check if the output_dir starts with \"s3://\"\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must be an S3 directory\")\n\n    # Check if the output_dir is not empty\n    if output_dir.exists():\n        if not output_dir.empty():\n            raise ValueError(\"output_dir must be empty\")\n\n    # Check if appending or overwriting is allowed\n    if append or overwrite:\n        raise NotImplementedError(\"Appending and overwriting data in the output_dir is not supported yet\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    # Check if the output_dir is a Dir object\n    if not isinstance(output_dir, Dir):\n        raise TypeError(f\"output_dir must be an instance of the Dir class, not {type(output_dir)}\")\n\n    # Check if the output_dir starts with \"s3://\"\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must be an S3 bucket, starting with 's3://'\")\n\n    # Check if the output_dir is empty\n    if output_dir.exists() and len(output_dir.listdir()) > 0:\n        raise ValueError(\"output_dir must be empty\")\n\n    # Check if appending or overwriting is allowed\n    if append:\n        raise NotImplementedError(\"Appending data to the output_dir is not supported yet\")\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data in the output_dir is not supported yet\")\n\n    # If all checks pass, return True\n    return True\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of the Dir class.\")\n\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must be an S3 bucket.\")\n\n    if not append and not overwrite:\n        if output_dir.exists():\n            if len(output_dir.list()) > 0:\n                raise ValueError(\"output_dir must be empty.\")\n\n    if append and overwrite:\n        raise ValueError(\"append and overwrite cannot both be True.\")\n\n    if append:\n        raise NotImplementedError(\"append is not yet implemented.\")\n\n    if overwrite:\n        raise NotImplementedError(\"overwrite is not yet implemented.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The output_dir must be a Dir object.\")\n\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"The output_dir must be an S3 directory.\")\n\n    if append:\n        raise NotImplementedError(\"Append is not implemented yet.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwrite is not implemented yet.\")\n\n    if len(output_dir.ls()) > 0:\n        raise ValueError(\n            \"The output_dir must be an empty directory. Please specify an empty directory.\"\n        )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"output_dir must be an instance of the Dir class.\")\n\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must be an S3 directory.\")\n\n    if append or overwrite:\n        raise NotImplementedError(\"Append and overwrite options are not yet implemented.\")\n\n    if output_dir.size() > 0:\n        raise ValueError(\"output_dir must be empty.\")\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of Dir\")\n\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must be an S3 directory\")\n\n    if append:\n        raise NotImplementedError(\"append=True is not implemented yet\")\n\n    if overwrite:\n        raise NotImplementedError(\"overwrite=True is not implemented yet\")\n\n    if output_dir.exists():\n        if len(output_dir.ls()) > 0:\n            raise ValueError(\"output_dir must be an empty directory\")\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of the Dir class\")\n\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must be an S3 path\")\n\n    if append:\n        raise NotImplementedError(\"append is not implemented yet\")\n\n    if overwrite:\n        raise NotImplementedError(\"overwrite is not implemented yet\")\n\n    if output_dir.exists():\n        if len(output_dir.ls()) > 0:\n            raise ValueError(\"output_dir is not empty\")\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    # Check that output_dir is an instance of the Dir class\n    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of the Dir class.\")\n\n    # Check that output_dir starts with \"s3://\"\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must start with 's3://'.\")\n\n    # Check that append and overwrite are False\n    if append:\n        raise NotImplementedError(\"append=True is not currently supported.\")\n    if overwrite:\n        raise NotImplementedError(\"overwrite=True is not currently supported.\")\n\n    # Check if the directory is empty\n    if output_dir.exists():\n        if len(output_dir.ls()) > 0:\n            raise ValueError(f\"{output_dir.path} is not empty.\")\n    else:\n        output_dir.mkdir()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of Dir\")\n    if not output_dir.uri.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must be an S3 directory\")\n    if not append and not overwrite:\n        if output_dir.exists():\n            if len(output_dir.ls()) > 0:\n                raise ValueError(\"output_dir must be empty\")\n    else:\n        raise NotImplementedError(\"append and overwrite are not supported yet\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of the Dir class.\")\n\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must be an S3 directory.\")\n\n    if append or overwrite:\n        raise NotImplementedError(\"Appending and overwriting data are not currently supported.\")\n\n    if output_dir.exists():\n        if len(output_dir.ls()) > 0:\n            raise ValueError(\"output_dir must be an empty directory.\")\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not output_dir.is_s3():\n        raise ValueError(f\"{output_dir} is not an S3 directory\")\n\n    if output_dir.exists(\"index.json\"):\n        raise ValueError(f\"{output_dir} already has an index file\")\n\n    output_dir.delete_objects(recursive=True)"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not output_dir.is_s3():\n        raise ValueError(f\"{output_dir} is not an S3 directory.\")\n\n    if output_dir.exists(\"index.json\"):\n        raise ValueError(f\"{output_dir} already contains an index file.\")\n\n    if not output_dir.exists(\"index.json\"):\n        output_dir.delete_all()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not output_dir.is_s3():\n        raise ValueError(\n            \"The output directory must be an S3 bucket directory. Please provide a valid S3 bucket directory.\"\n        )\n\n    if output_dir.exists(\"index.json\"):\n        raise ValueError(\n            \"The output directory already contains an index file. Please provide a directory that does not contain an index file.\"\n        )\n\n    if not output_dir.exists(\"index.json\"):\n        output_dir.delete_all()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"output_dir must be an instance of Dir\")\n\n    if not output_dir.is_s3():\n        raise ValueError(\"output_dir must be an S3 bucket\")\n\n    if output_dir.has_index_file():\n        raise ValueError(\"output_dir already has an index file\")\n\n    if not output_dir.has_index_file():\n        output_dir.delete_objects_in_prefix()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not output_dir.is_s3():\n        raise ValueError(f\"The output directory {output_dir} is not an S3 bucket\")\n\n    if output_dir.exists(\"index.json\"):\n        raise ValueError(f\"The output directory {output_dir} already contains an index file\")\n\n    if not output_dir.exists(\"index.json\"):\n        output_dir.delete_objects(prefix=output_dir.key)\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the output directory is an S3 bucket\n    if not isinstance(output_dir, S3Dir):\n        raise TypeError(f\"The output directory '{output_dir}' is not an S3 bucket.\")\n\n    # Check if the output directory already contains an index file\n    if output_dir.has_index_file():\n        raise ValueError(f\"The output directory '{output_dir}' already contains an index file.\")\n\n    # If the output directory does not contain an index file, delete all objects within the specified prefix in the bucket\n    output_dir.delete_all_objects()"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.is_s3():\n        if output_dir.exists(\"index.json\"):\n            raise ValueError(\n                f\"Output directory {output_dir} already contains an index file.\"\n            )\n        else:\n            output_dir.delete_all_objects_with_prefix(\"\")\n    else:\n        if output_dir.exists(\"index.json\"):\n            raise ValueError(\n                f\"Output directory {output_dir} is not an S3 bucket, but contains an index file.\"\n            )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.is_s3():\n        if output_dir.exists(\"index.json\"):\n            raise Exception(f\"The output directory {output_dir} already contains an index file.\")\n        else:\n            output_dir.delete_objects(prefix=output_dir.path)\n    else:\n        raise Exception(f\"The output directory {output_dir} is not an S3 bucket.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not output_dir.is_s3():\n        raise ValueError(f\"{output_dir} is not an S3 bucket.\")\n\n    if output_dir.exists(\"index.json\"):\n        raise ValueError(f\"{output_dir} already contains an index file.\")\n\n    if not output_dir.exists(\"index.json\"):\n        output_dir.delete_all_objects()\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the directory is an S3 bucket\n    if not output_dir.is_s3():\n        raise ValueError(\"The specified output directory is not an S3 bucket.\")\n\n    # Check if the directory already contains an index file\n    if output_dir.joinpath(\"index.json\").exists():\n        raise ValueError(\"The specified output directory already contains an index file.\")\n\n    # Delete all objects within the specified prefix in the bucket\n    output_dir.delete_all_objects()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the output directory is an S3 bucket\n    if not output_dir.is_s3_dir():\n        raise ValueError(\"The output directory is not an S3 bucket.\")\n\n    # Check if the output directory already contains an index file\n    if output_dir.exists(\"index.json\"):\n        raise ValueError(\"The output directory already contains an index file.\")\n\n    # Check if the output directory is empty\n    if output_dir.is_empty():\n        return\n\n    # Delete all objects within the specified prefix in the bucket\n    output_dir.delete_all_objects()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the directory is an S3 bucket\n    if not output_dir.is_s3():\n        raise ValueError(\"The output_dir is not an S3 bucket.\")\n\n    # Check if the directory already contains an index file\n    if output_dir.exists(\"index.json\"):\n        raise ValueError(\n            \"The output_dir already contains an index file named 'index.json'.\"\n        )\n\n    # If the directory does not contain an index file, delete all objects within the specified prefix in the bucket\n    output_dir.delete_all()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not output_dir.is_s3():\n        return\n\n    if output_dir.exists(\"index.json\"):\n        raise ValueError(\n            f\"Output directory {output_dir} already contains an index.json file.\"\n        )\n\n    if not output_dir.exists():\n        return\n\n    # if the directory exists and does not contain an index file, delete all objects within the prefix\n    for path in output_dir.ls():\n        path.delete()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.is_s3_dir():\n        if output_dir.exists(\"index.json\"):\n            raise ValueError(\"The directory already contains an index file\")\n        else:\n            output_dir.delete_all_objects()\n    else:\n        raise ValueError(\"The output directory is not an S3 bucket\")\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not output_dir.is_s3():\n        return\n\n    if output_dir.exists(\"index.json\"):\n        raise ValueError(\n            \"The output directory already contains an index.json file. \"\n            \"Please remove it before proceeding.\"\n        )\n\n    if not output_dir.exists():\n        return\n\n    output_dir.rm(recursive=True)\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not output_dir.is_s3():\n        raise Exception(\n            \"The output_dir argument is expected to be an S3 bucket directory. Please check the output_dir argument.\"\n        )\n\n    if output_dir.exists(\"index.json\"):\n        raise Exception(\n            \"The output_dir argument is expected to be an empty S3 bucket directory. Please check the output_dir argument.\"\n        )\n\n    # If the output_dir is an S3 bucket directory and does not contain an index file, delete all objects within the specified prefix in the bucket.\n    for object in output_dir.ls():\n        object.delete()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the output_dir is an S3 bucket\n    if not output_dir.is_s3():\n        raise ValueError(\"The output_dir must be an S3 bucket directory.\")\n\n    # Check if the output_dir contains an index file\n    if output_dir.exists(\"index.json\"):\n        raise ValueError(\"The output_dir already contains an index file.\")\n\n    # If no index file is found, delete all objects within the specified prefix in the bucket\n    for obj in output_dir.list_objects():\n        obj.delete()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    import os\n    import json\n    from aws_cdk.aws_s3_deployment import Source\n\n    if not output_dir.bucket_name:\n        raise ValueError(f\"{output_dir} is not an S3 bucket\")\n\n    index_file_path = os.path.join(output_dir.prefix, \"index.json\")\n    index_file_exists = output_dir.bucket.object_exists(index_file_path)\n\n    if index_file_exists:\n        raise ValueError(f\"{output_dir} already contains an index file\")\n\n    if not index_file_exists:\n        output_dir.bucket.delete_objects(prefix=output_dir.prefix)\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not output_dir.is_s3():\n        raise ValueError(\n            \"The output_dir must be an S3 bucket directory. Please check the output_dir argument.\"\n        )\n\n    if output_dir.exists(\"index.json\"):\n        raise ValueError(\n            \"The output_dir must not contain an index.json file. Please check the output_dir argument.\"\n        )\n\n    if not output_dir.exists():\n        for path in output_dir.ls():\n            path.delete()\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.is_s3():\n        if output_dir.exists(\"index.json\"):\n            raise ValueError(\n                f\"{output_dir.path} already contains an index file. Please delete it before running this function.\"\n            )\n    else:\n        raise ValueError(f\"{output_dir.path} is not an S3 bucket.\")\n\n    if not output_dir.exists(\"index.json\"):\n        output_dir.rm(recursive=True)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n\n        if node_rank != 0:\n            # wait until master is done writing all parts\n            while not os.path.exists(self.parts_dir):\n                time.sleep(1)\n\n            while not os.path.exists(self.index_name):\n                time.sleep(1)\n\n            # then load and return\n            self.load_index()\n            return\n\n        # make sure all parts are present\n        while True:\n            if len(os.listdir(self.parts_dir)) < num_workers:\n                time.sleep(1)\n            else:\n                break\n\n        # merge parts\n        self.merge_parts()\n\n        # delete parts\n        shutil.rmtree(self.parts_dir)\n\n        # reload the index\n        self.load_index()"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n            num_workers = 1\n\n        if node_rank == 0 and num_workers > 1:\n            # Merge vectors from shards into a big file in sorted order\n            print(f\"Merging {num_workers} shards into vectors.mhd\")\n            vectors_hd = h5py.File(\"vectors.mhd\", \"w\")\n            total_len = 0\n            for i in range(num_workers):\n                hd = h5py.File(f\"vectors_{i}.mhd\", \"r\")\n                total_len += hd[\"vectors\"].shape[0]\n\n            train_shape = (768, total_len) if self.quantize else (total_len, 768)\n            self.train(total_len)\n            if \"vectors\" in vectors_hd:\n                del vectors_hd[\"vectors\"]\n            vectors_hd.create_dataset(\n                \"vectors\", train_shape, dtype=\"uint8\" if self.quantize else \"float\"\n            )\n\n            offset = 0\n            for i in range(num_workers):\n                hd = h5py.File(f\"vectors_{i}.mhd\", \"r\")\n                self.train(offset, hd[\"vectors\"].shape[0])\n                print(f\"Copying {offset}:{offset+hd['vectors'].shape[0]}\")\n                vectors_hd[\"vectors\"][:, offset : offset + hd[\"vectors\"].shape[0\"]] = hd[\n                    \"vectors\"\n                ][:]\n                offset += hd[\"vectors\"].shape[0]\n                hd.close()\n\n            print(\"Merging shards into train.mhd and train.dat\")\n            train_ds = h5py.File(\"train.mhd\", \"w\")\n            train_data = train_ds.create_dataset(\n                \"train\", (total_len,), dtype=self.train.desc\n            )\n            self.train.reset()\n            offset = 0"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.node_rank\n        if node_rank == 0:\n            logger.info(\"Merging index\")\n            self.merge_parts(num_workers)\n            self.delete_parts()\n        else:\n            while not os.path.exists(self.index_path):\n                time.sleep(1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = int(os.environ.get(\"RANK\", -1))\n\n        if node_rank == -1:\n            node_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n\n        if node_rank == -1:\n            node_rank = int(os.environ.get(\"JOB_ID\", -1))\n\n        if node_rank == -1:\n            node_rank = int(os.environ.get(\"LSB_JOBID\", -1))\n\n        if node_rank == -1:\n            node_rank = int(os.environ.get(\"SLURM_PROCID\", -1))\n\n        if node_rank == -1:\n            node_rank = int(os.environ.get(\"MV2_COMM_WORLD_RANK\", -1))\n\n        if node_rank == -1:\n            node_rank = int(os.environ.get(\"OMPI_COMM_WORLD_RANK\", -1))\n\n        if node_rank == -1:\n            raise ValueError(\"Cannot determine rank of node\")\n\n        # Wait until all parts are available\n        while True:\n            if node_rank == 0:\n                parts = glob.glob(os.path.join(self.dir_name, f\"*.{self.extension}.*\"))\n            else:\n                parts = []\n            if len(parts) == num_workers:\n                break\n            time.sleep(1)\n\n        # Merge parts if this is rank 0\n        if node_rank == 0:\n            self.merge_parts(parts)\n\n        # Wait until merged index is available\n        while True:\n            if os.path.exists(self.index_name):\n                break\n            time.sleep(1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.node_rank\n        if node_rank == 0:\n            self._merge_index(num_workers)\n        else:\n            while not os.path.exists(self.index_file):\n                time.sleep(1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n            num_workers = 1\n        if node_rank == 0 and num_workers > 1:\n            parts = []\n            for worker_id in range(num_workers):\n                filename = os.path.join(self.index_dir, f\"{self.index_name}.{worker_id}.index\")\n                if os.path.exists(filename):\n                    parts.append(filename)\n                else:\n                    break\n            if len(parts) == num_workers:\n                self.merge_parts(parts)\n        else:\n            while True:\n                filename = os.path.join(self.index_dir, f\"{self.index_name}.index\")\n                if os.path.exists(filename):\n                    break\n                time.sleep(1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(self.index_file):\n                time.sleep(0.1)\n            return\n\n        index_files = [f for f in os.listdir(self.index_cache_dir) if f.endswith('.index')]\n        while len(index_files) < num_workers:\n            time.sleep(0.1)\n            index_files = [f for f in os.listdir(self.index_cache_dir) if f.endswith('.index')]\n\n        with open(self.index_file, 'wb') as outfile:\n            for i in range(num_workers):\n                index_file = os.path.join(self.index_cache_dir, f'index_{i}.index')\n                with open(index_file, 'rb') as infile:\n                    shutil.copyfileobj(infile, outfile)\n                os.remove(index_file)\n\n        self.index = self.load_index(self.index_file)\n        self.index_loaded = True\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n\n        if node_rank == 0 and os.path.exists(self.index_file):\n            logger.info(f\"The index {self.index_file} already exists.\")\n            return\n\n        logger.info(f\"{self.index_file} does not exist.\")\n\n        part_files = [\n            os.path.join(self.index_dir, f\"{self.index_id}.{i}.dat\") for i in range(num_workers)\n        ]\n\n        while not all([os.path.exists(p) for p in part_files]):\n            time.sleep(1)\n\n        if node_rank != 0:\n            while not os.path.exists(self.index_file):\n                time.sleep(1)\n            logger.info(f\"The index {self.index_file} is ready.\")\n            return\n\n        logger.info(f\"Merging index {self.index_file}.\")\n\n        key_chunks = [\n            np.memmap(\n                filename, dtype=self.key_dtype, mode=\"r\", shape=(self.dim,)\n            ).astype(\"float32\")\n            for filename in part_files\n        ]\n        key_chunks = np.concatenate(key_chunks, axis=0)\n        offsets = np.cumsum([len(key_chunk) for key_chunk in key_chunks])\n        total_keys = offsets[-1]\n        offsets = offsets.reshape(1, -1).astype(\"int64\")\n        keys = np.memmap(\n            self.index_file + \".keys\",\n            dtype=\"float32\",\n            mode=\"w+\",\n            shape=(total_keys, self.dim),\n        )\n        keys[:] = key_chunks.reshape(total_keys, self.dim)\n        del key_chunks\n\n        id_chunks = [\n            np.memmap(filename, dtype=self.id_dtype,"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(self.index_file):\n                time.sleep(0.1)\n            return\n\n        self.wait_for_index_parts(num_workers)\n\n        # Merge the index parts\n        self.merge_index_parts()\n\n        # Delete the index parts\n        self.delete_index_parts()\n\n        # Delete the cache directory\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.node_rank\n\n        if node_rank == 0 and num_workers > 1:\n            parts = f\"{self.index_name}.*.bin\"\n            pattern = str(self.index_dir / parts)\n\n            start = time.time()\n            while True:\n                index_files = glob.glob(pattern)\n                if time.time() - start > self.merge_timeout and len(index_files) < num_workers:\n                    raise TimeoutError(\n                        f\"Waited {self.merge_timeout} secs for index parts {pattern}. \"\n                        f\"Only {len(index_files)} found\"\n                    )\n                if len(index_files) == num_workers or not num_workers:\n                    break\n                time.sleep(1)\n\n            index_files = {\n                int(f.split(\".\")[-2]): f for f in sorted(index_files)\n            }\n            index_files = [index_files[i] for i in range(len(index_files))]\n\n            self._merge(index_files)\n\n        else:\n            parts = f\"{self.index_name}.bin\"\n            pattern = str(self.index_dir / parts)\n\n            start = time.time()\n            while not glob.glob(pattern):\n                if time.time() - start > self.merge_timeout:\n                    raise TimeoutError(\n                        f\"Waited {self.merge_timeout} secs for index parts {pattern}. \"\n                        f\"Only {len(glob.glob(pattern))} found\"\n                    )\n                time.sleep(1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(self.index_file):\n                time.sleep(0.1)\n            return\n\n        logger.info(f\"Merging {num_workers} shards into {self.index_file}\")\n        offset = 0\n        end_offsets = []\n        for i in range(num_workers):\n            fname = os.path.join(self.cache_dir, f\"{self.index_id}_{i}.index\")\n            while not os.path.exists(fname):\n                time.sleep(0.1)\n            index = faiss.read_index(fname)\n            ntotal = index.ntotal\n            end_offsets.append(offset + ntotal)\n            # print(f\"Merging {fname} with {ntotal} vectors offset={offset}\")\n            self.index.merge_from(index, offset)\n            offset += ntotal\n            os.remove(fname)\n        self.index.set_vector_offsets(np.array(end_offsets[:-1], dtype=np.long))\n        logger.info(f\"Writing {self.index_file} with {self.index.ntotal} total vectors\")\n        faiss.write_index(self.index, self.index_file)\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n\n        if node_rank == 0 and num_workers > 1:\n            # wait until all parts are available\n            parts = glob.glob(os.path.join(self.parts_dir, \"*\"))\n            while len(parts) < num_workers:\n                time.sleep(1)\n                parts = glob.glob(os.path.join(self.parts_dir, \"*\"))\n\n            # merge parts\n            self.merge_parts()\n\n            # delete parts\n            shutil.rmtree(self.parts_dir)\n\n        else:\n            # wait until merged index is available\n            while not os.path.exists(self.index_file):\n                time.sleep(1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Check if the current node is the master node (rank 0)\n        if node_rank is None or node_rank == 0:\n            # Wait for all index parts to be available\n            while True:\n                if self.is_done():\n                    break\n                time.sleep(0.1)\n\n            # Merge the index parts\n            self.merge_parts()\n\n            # Clean up the cache directory\n            self.cleanup()\n        else:\n            # Wait for the merged index file to be available\n            while not os.path.exists(self.index_file):\n                time.sleep(0.1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n            num_workers = 1\n\n        # Wait until all parts are available\n        while True:\n            if node_rank == 0 and all(\n                os.path.exists(os.path.join(self.index_dir, f\"{i}.ivf\"))\n                for i in range(num_workers)\n            ):\n                break\n            time.sleep(1)\n\n        # Merge index\n        if node_rank == 0:\n            index_parts = [\n                faiss.read_index(os.path.join(self.index_dir, f\"{i}.ivf\"))\n                for i in range(num_workers)\n            ]\n            index = faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, faiss.read_index(self.index_path)\n            )\n            index.merge_from(index_parts)\n            faiss.write_index(faiss.index_gpu_to_cpu(index), self.index_path)\n\n            # Clean up\n            for i in range(num_workers):\n                os.remove(os.path.join(self.index_dir, f\"{i}.ivf\"))\n\n        else:\n            while not os.path.exists(self.index_path):\n                time.sleep(1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n        if node_rank != 0:\n            while not os.path.exists(self.index_file):\n                time.sleep(0.1)\n            return\n        self.wait_for_index_parts()\n        if os.path.exists(self.index_file):\n            return\n        index_parts = []\n        for i in range(num_workers):\n            index_part_file = os.path.join(self.index_cache_dir, f\"index_{i}.bin\")\n            if os.path.exists(index_part_file):\n                index_parts.append(index_part_file)\n        index_parts = sorted(index_parts)\n        with open(self.index_file, \"wb\") as f:\n            for index_part in index_parts:\n                with open(index_part, \"rb\") as g:\n                    shutil.copyfileobj(g, f)\n        for index_part in index_parts:\n            os.remove(index_part)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        cache_dir = os.path.join(self.dir, \"cache\")\n        os.makedirs(cache_dir, exist_ok=True)\n\n        if node_rank is None:\n            node_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n\n        if node_rank == 0:\n            while True:\n                index_files = [\n                    os.path.join(cache_dir, f\"index_{i}.bin\") for i in range(num_workers)\n                ]\n                if all(os.path.exists(f) for f in index_files):\n                    break\n                time.sleep(1)\n\n            self.merge_files(index_files)\n            for f in index_files:\n                os.remove(f)\n        else:\n            while not os.path.exists(self.index_file):\n                time.sleep(1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = int(os.environ.get(\"RANK\", -1))\n\n        if node_rank == -1:\n            node_rank = 0\n            num_workers = 1\n\n        if node_rank == 0:\n            # Wait until all parts are available\n            parts = []\n            while True:\n                part_files = glob.glob(os.path.join(self.index_path, f\"*.{self.extension}.*\"))\n                if len(part_files) == num_workers:\n                    break\n                else:\n                    time.sleep(1)\n\n            for part_file in part_files:\n                with open(part_file, \"rb\") as reader:\n                    part = pickle.load(reader)\n                    parts.append(part)\n\n            # Merge parts\n            self.merge_parts(parts)\n\n        else:\n            # Wait until the merged index file is available\n            while not os.path.exists(self.index_file):\n                time.sleep(1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n            num_workers = 1\n\n        index_id = self.config.indexing.index_id\n        index_path = self.config.indexing.index_path\n\n        if node_rank == 0:\n            logger.info(f\"Start merging for index {index_id}\")\n            cache_path = os.path.join(index_path, f\"{index_id}.cache\")\n            merged_index_file = os.path.join(index_path, f\"{index_id}.index\")\n\n            while True:\n                if len(os.listdir(cache_path)) < num_workers:\n                    time.sleep(1)\n                else:\n                    break\n\n            index_files = [os.path.join(cache_path, f) for f in os.listdir(cache_path)]\n            index_files = [f for f in index_files if os.path.isfile(f) and not f.endswith(\".lock\")]\n            index_files.sort()\n\n            with open(merged_index_file, \"wb\") as writer:\n                for index_file in index_files:\n                    with open(index_file, \"rb\") as reader:\n                        shutil.copyfileobj(reader, writer)\n\n            logger.info(f\"Finished merging for index {index_id}\")\n\n        else:\n            while True:\n                if not os.path.exists(os.path.join(index_path, f\"{index_id}.index\")):\n                    time.sleep(1)\n                else:\n                    break\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.node_rank\n\n        if node_rank == 0 and self.index is None:\n            # we are the master node, so we need to wait for all the parts\n            # to be available\n            parts = self.get_parts()\n            while len(parts) < num_workers:\n                time.sleep(0.1)\n                parts = self.get_parts()\n            # merge all parts into self.index\n            self.merge_parts(parts)\n            # write the merged index to disk\n            self.write()\n        else:\n            # we are a worker node, so we need to wait for the merged index\n            # to be available\n            while not os.path.exists(self.index_name):\n                time.sleep(0.1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.node_rank\n\n        if node_rank != 0:\n            while not os.path.exists(self.index_file):\n                time.sleep(0.1)\n            return\n\n        if num_workers > 1:\n            parts = f\"{self.index_file}.*.partial\"\n            while len(glob.glob(parts)) != num_workers:\n                time.sleep(0.1)\n\n        self.merge_parts(self.index_file, num_workers)\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if machine is None:\n        machine = Machine.get_default()\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.environ}\"\n\n    job = Job(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n    )\n\n    try:\n        job.execute()\n    except Exception as e:\n        raise e\n\n    print(f\"Job URL: {job.url}\")\n\n    while job.status() not in [\"finished\", \"failed\", \"stopped\"]:\n        time.sleep(5)\n\n    if job.status() == \"failed\":\n        raise Exception(f\"Job {job.name} failed\")\n\n    if job.status() == \"stopped\":\n        raise Exception(f\"Job {job.name} stopped\")\n\n    print(f\"Job {job.name} finished successfully\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if machine is None:\n        machine = get_default_machine()\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.environ}\"\n\n    job = Job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    try:\n        job.start()\n    except Exception as e:\n        raise e\n\n    job.wait_for_job_status()\n    job.print_job_url()\n\n    if job.status == \"failed\":\n        raise Exception(f\"Job {job.name} failed\")\n\n    return job\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is available\n    if not has_required_sdk():\n        raise Exception(\"The required SDK is not available.\")\n\n    # Create a data preparation machine job\n    job = Job(\n        name=name,\n        machine=machine if machine is not None else get_default_machine(),\n        command=command if command is not None else get_default_command(),\n        num_nodes=num_nodes,\n    )\n\n    # Start the job\n    job.start()\n\n    # Check the job status\n    while job.status() == \"pending\":\n        time.sleep(1)\n\n    # Print the job URL\n    print(job.url())\n\n    # Raise an exception if the job fails\n    if job.status() == \"failed\":\n        raise Exception(\"Job failed.\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not machine:\n        machine = Machine(\n            cpu=\"4\",\n            memory=\"16Gi\",\n            gpu=\"0\",\n            gpu_type=\"NO_GPU\",\n        )\n\n    if not command:\n        command = f\"cd {os.getcwd()} && {os.environ}\"\n\n    job = Job(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n    )\n\n    job.start()\n    job.wait_for_status(status=\"started\")\n    job.wait_for_status(status=\"completed\")\n\n    if job.status == \"failed\":\n        raise Exception(f\"Job {job.id} failed\")\n\n    print(f\"Job URL: {job.url}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is available\n    if not hasattr(studio.sdk, \"create_job\"):\n        raise Exception(\"This operator requires the Studio SDK to be available.\")\n\n    # Fetch the machine configuration if not provided\n    if machine is None:\n        machine = get_default_machine()\n\n    # Fetch the current working directory and environment variables if command is not provided\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.environ}\"\n\n    # Create the job using the Studio SDK\n    job = studio.sdk.create_job(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n    )\n\n    # Continuously check the job status and print the job URL when it starts\n    while True:\n        job.refresh()\n        if job.status == \"started\":\n            print(f\"Job URL: {job.url}\")\n            break\n        elif job.status == \"failed\":\n            raise Exception(\"Job failed.\")\n        time.sleep(1)\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if machine is None:\n        machine = get_default_machine()\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && export {os.environ} && python -m {__name__}\"\n\n    job_id = create_job(name, num_nodes, machine, command)\n\n    if job_id is None:\n        raise Exception(\"Failed to create job\")\n\n    job_url = f\"https://studio.iterative.ai/u/{os.environ['STUDIO_USERNAME']}/projects/{os.environ['STUDIO_PROJECT']}/jobs/{job_id}\"\n    print(f\"Job URL: {job_url}\")\n\n    job_status = get_job_status(job_id)\n\n    while job_status not in [\"success\", \"failed\", \"cancelled\"]:\n        time.sleep(5)\n        job_status = get_job_status(job_id)\n\n    if job_status == \"failed\":\n        raise Exception(\"Job failed\")\n\n    if job_status == \"cancelled\":\n        raise Exception(\"Job cancelled\")\n\n    print(\"Job completed successfully\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is available\n    if not hasattr(studio.api, \"create_job\"):\n        raise Exception(\"This operator requires the studio-sdk-python package to be installed.\")\n\n    # Fetch the current working directory and environment variables\n    cwd = os.getcwd()\n    env = os.environ.copy()\n\n    # Set the default machine configuration if not provided\n    if machine is None:\n        machine = Machine(\n            cpu=2,\n            memory=4,\n            gpu=0,\n            gpu_type=\"NO_GPU\",\n        )\n\n    # Set the default command if not provided\n    if command is None:\n        command = f\"cd {cwd} && env | grep -v '^LS_COLORS' | grep -v '^OLDPWD' | grep -v '^PATH' | grep -v '^HOME' | grep -v '^USER' | grep -v '^PWD' | grep -v '^LANG' | grep -v '^LC_' | grep -v '^_' | grep -v '^SHLVL' | grep -v '^SHELL' | grep -v '^LOGNAME' | grep -v '^MAIL' | grep -v '^TERM' | grep -v '^COLORTERM' | grep -v '^COLORFGBG' | grep -v '^DBUS_SESSION_BUS_ADDRESS' | grep -v '^XDG_RUNTIME_DIR' | grep -v '^WINDOWID' | grep -v '^DESKTOP_SESSION' | grep -v '^XDG_SESSION_ID' | grep -v '^XDG_DATA_DIRS' | grep -v '^XDG_SESSION_DESKTOP' | grep -v '^XDG_CURRENT_DESKTOP' | grep -v '^XDG_CONFIG_DIRS' | grep -v '^XDG_CONFIG_HOME' | grep -v '^XDG_CACHE_HOME' | grep -v '^XDG"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if machine is None:\n        machine = _get_default_machine()\n\n    if command is None:\n        command = f\"cd {os.getcwd()}; {os.environ}\"\n\n    job = _create_job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    try:\n        job.start()\n        job.wait_for_start()\n        print(f\"Job started at {job.url}\")\n        job.wait_for_completion()\n    except Exception as e:\n        print(f\"Job failed: {e}\")\n        raise e\n\n    if job.status == \"failed\":\n        raise Exception(f\"Job failed: {job.status_message}\")\n\n    print(f\"Job completed successfully: {job.url}\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not HAS_STUDIO_SDK:\n        raise ImportError(\n            \"The Studio SDK is required to run the operator. Please install it with `pip install studio-sdk`.\"\n        )\n\n    if machine is None:\n        machine = get_default_machine_config(num_nodes)\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.environ}\"\n\n    job = Job(\n        name=name,\n        machine=machine,\n        command=command,\n        start_session=True,\n    )\n\n    job.create()\n\n    while job.status() != \"Started\":\n        time.sleep(1)\n\n    print(f\"Job URL: {job.url}\")\n\n    if job.status() == \"Failed\":\n        raise Exception(f\"Job {name} failed\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    try:\n        import kfp_server_api\n        import kfp\n    except ImportError:\n        raise ImportError(\n            \"The kfp_server_api and kfp libraries are required to run the execute function.\"\n        )\n\n    # Get the default machine configuration if not provided\n    if machine is None:\n        machine = _get_default_machine()\n\n    # Get the default command if not provided\n    if command is None:\n        command = _get_default_command()\n\n    # Create a client to connect to the Kubeflow Pipelines API\n    client = kfp.Client()\n\n    # Create a data preparation machine job\n    job = kfp_server_api.V2beta1Job(\n        name=name,\n        pipeline_name=\"data-prep-pipeline\",\n        max_trial_count=1,\n        parallelism=num_nodes,\n        no_catchup=False,\n        pipeline_run_spec=kfp_server_api.V2beta1PipelineRunSpec(\n            pipeline_spec=kfp_server_api.V2beta1PipelineSpec(\n                tasks=[\n                    kfp_server_api.V2beta1PipelineTaskSpec(\n                        name=\"data-prep-task\",\n                        task_spec=kfp_server_api.V2beta1TaskSpec(\n                            executor_label=\"exec-data-prep\",\n                            component_ref=kfp_server_api.V2beta1ComponentReference(\n                                name=\"data-prep-component\"\n                            ),\n                            parameter_values={\n                                \"command\": command,\n                                \"machine\": machine,\n                            },\n                        ),\n                    )\n                ]\n            )\n        ),\n    )\n\n    # Submit the job to the Kubeflow Pipelines API\n    client.create_run_from_job_func(job, namespace=\"kubeflow\")\n\n    # Continuously check the job status until it starts\n    while True:\n        job_status = client.get_job_status(job.id, namespace=\"kubeflow\")\n        if job_status"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    try:\n        from studio.api.workflow.data_prep import (\n            DataPrepJob,\n            DataPrepJobStatus,\n        )\n        from studio.api.workflow.machine import Machine\n        from studio.experimental.client import get_studio_client\n    except ImportError as e:\n        raise ImportError(\n            \"The Studio SDK is required for this operation. \"\n            \"Please install the Studio SDK with `pip install studio-python-sdk`.\"\n        ) from e\n\n    if machine is None:\n        machine = Machine.default()\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.environ}\"\n\n    job = DataPrepJob(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n    )\n\n    client = get_studio_client()\n    job_id = client.data_prep.create_job(job)\n\n    job_status = client.data_prep.get_job_status(job_id)\n\n    while job_status != DataPrepJobStatus.COMPLETED:\n        if job_status == DataPrepJobStatus.FAILED:\n            raise Exception(\"Job failed\")\n\n        time.sleep(5)\n        job_status = client.data_prep.get_job_status(job_id)\n\n    print(f\"Job URL: {client.data_prep.get_job_url(job_id)}\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if machine is None:\n        machine = Machine(\n            cpu=1,\n            memory=1,\n            gpu=0,\n            gpu_type=\"NVIDIA T4\",\n            min_gpu_memory=0,\n        )\n\n    if command is None:\n        command = \"cd {0} && {1}\".format(\n            os.getcwd(), \" \".join(sys.argv)\n        )\n\n    job = Job(\n        name=name,\n        machine=machine,\n        num_nodes=num_nodes,\n        command=command,\n    )\n\n    job.create()\n\n    print(\n        \"Job started at {0}\".format(\n            job.get_job_url(),\n        )\n    )\n\n    while job.status() not in [\"FINISHED\", \"FAILED\"]:\n        time.sleep(30)\n\n    if job.status() == \"FAILED\":\n        raise Exception(\n            \"Job failed. Please check the logs at {0}\".format(\n                job.get_job_url(),\n            )\n        )\n\n    return\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    try:\n        from sdk import Studio\n    except ImportError as e:\n        raise ImportError(\n            \"The sdk is required to execute the operator. Please install the sdk: pip install studio-sdk\"\n        ) from e\n\n    if machine is None:\n        machine = Machine(\n            cpu=1,\n            memory=1024,\n            gpu=0,\n            gpu_type=None,\n        )\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.environ}\"\n\n    job = Studio().create_job(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n        project_id=os.environ.get(\"STUDIO_PROJECT_ID\"),\n    )\n\n    job_url = f\"https://studio.iterative.ai/u/{os.environ.get('STUDIO_USERNAME')}/projects/{os.environ.get('STUDIO_PROJECT_ID')}/jobs/{job.id}\"\n    print(f\"Job URL: {job_url}\")\n\n    while job.status == \"PENDING\":\n        job = Studio().get_job(job.id)\n        time.sleep(1)\n\n    if job.status == \"FAILED\":\n        raise Exception(f\"Job {job.id} failed\")\n\n    return job.id\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    try:\n        from kfp.v2.dsl import get_pipeline_conf, get_pipeline_root\n    except ImportError:\n        raise ImportError(\n            \"The required SDK is not available. Please install the Kubeflow Pipelines SDK.\"\n        )\n\n    pipeline_conf = get_pipeline_conf()\n    if pipeline_conf is None:\n        raise ValueError(\"Pipeline configuration is not available.\")\n\n    pipeline_root = get_pipeline_root()\n    if pipeline_root is None:\n        raise ValueError(\"Pipeline root is not available.\")\n\n    if machine is None:\n        machine = Machine(\n            cpu=\"1\",\n            memory=\"1Gi\",\n            accelerators=[],\n            image=\"python:3.8\",\n        )\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.environ}\"\n\n    job_name = f\"{name}-{uuid.uuid4().hex}\"\n    job_url = f\"{pipeline_conf.host}/pipeline/#/runs/details/{job_name}\"\n\n    logging.info(f\"Creating data preparation machine job: {job_name}\")\n    logging.info(f\"Job URL: {job_url}\")\n\n    job = create_job(\n        name=job_name,\n        pipeline_root=pipeline_root,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    logging.info(f\"Starting data preparation machine job: {job_name}\")\n    job.start()\n\n    logging.info(f\"Waiting for data preparation machine job to start: {job_name}\")\n    job.wait_for_start()\n\n    logging.info(f\"Data preparation machine job started: {job_name}\")\n\n    try:\n        while job.get_status() != \"Succeeded\":\n            time.sleep(10)\n    except Exception as e:\n        logging.error(f\"Data preparation machine job failed"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is available\n    if not is_available():\n        raise Exception(\n            \"The required SDK is not available. Please install the SDK and try again.\"\n        )\n\n    # Get the current working directory and environment variables\n    cwd = os.getcwd()\n    env = os.environ.copy()\n\n    # Create a default command if not provided\n    if command is None:\n        command = f\"cd {cwd} && {env}\"\n\n    # Create a default machine configuration if not provided\n    if machine is None:\n        machine = get_default_machine()\n\n    # Create a data preparation machine job using the Studio API\n    job = Job(\n        name=name,\n        machine=machine,\n        command=command,\n    )\n\n    # Submit the job\n    job.submit()\n\n    # Wait for the job to start\n    job.wait_for_start()\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")\n\n    # Continuously check the job status\n    while True:\n        status = job.status()\n        if status == \"running\":\n            print(\"Job is running.\")\n        elif status == \"succeeded\":\n            print(\"Job completed successfully.\")\n            break\n        elif status == \"failed\":\n            raise Exception(\"Job failed.\")\n        elif status == \"stopped\":\n            raise Exception(\"Job stopped.\")\n        elif status == \"stopping\":\n            raise Exception(\"Job is stopping.\")\n        else:\n            raise Exception(f\"Unknown job status: {status}\")\n\n        time.sleep(5)\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    try:\n        from studio.api.workflow.jobs import create_data_preparation_machine_job\n    except ImportError:\n        raise ImportError(\n            \"You need to install the studio package to use this operator. \"\n            \"You can install it with `pip install studio-python-sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            cpu=2,\n            memory=8,\n            gpu=0,\n            gpu_model=None,\n            min_gpu_memory=None,\n            min_gpu_cores=None,\n            min_gpu_free_memory=None,\n        )\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && export {os.environ}\"\n\n    job = create_data_preparation_machine_job(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n    )\n\n    print(f\"Job started at {job.url}\")\n\n    while job.status in [\"pending\", \"starting\", \"running\"]:\n        time.sleep(1)\n        job.reload()\n\n    if job.status == \"failed\":\n        raise Exception(f\"Job failed with error: {job.error}\")\n\n    if job.status == \"killed\":\n        raise Exception(f\"Job was killed with error: {job.error}\")\n\n    if job.status == \"succeeded\":\n        print(\"Job succeeded\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is available\n    if not hasattr(studio, \"job\"):\n        raise Exception(\n            \"This operator requires the Studio SDK to be available. Please upgrade your SDK version.\"\n        )\n\n    # Fetch the default machine configuration if not provided\n    if machine is None:\n        machine = Machine(\n            cpu=\"4\",\n            memory=\"8Gi\",\n            gpu=\"0\",\n            gpu_model=\"\",\n            min_gpu=\"0\",\n        )\n\n    # Set the default command if not provided\n    if command is None:\n        command = f\"cd {os.getcwd()} && env | grep -E '^(AWS|NVIDIA_|NCCL_|TF_|CUDA_|PYTHON|MXNET|PATH|LD_|LIBRARY_PATH|PYTHONPATH|PYTHONHOME|PYTHONUSERBASE|HOROVOD_|SPARK_|JAVA_|S3_|AZURE_|GOOGLE_|GCLOUD_|KAGGLE_|COMET_|NEURON_|ROSETTA_|HF_|TRANSFORMERS_|DIFFUSERS_|INVOKEAI_|DIFFUSERS_|ACCELE_|DEEPSPEED_|MEGATRON_|GOOGLE_|SLURM_|PBS_|SGE_|LSB_|JOB_|OMPI_|MV2_|PMI_|SLURM_|RAY_|SINGULARITY_|NCCL_|NCCL_ASYNC_ERROR_HANDLING|NCCL_IB_DISABLE|NCCL_IB_GID_INDEX|NCCL_IB_HCA|NCCL_IB_PCI_RELAXED_ORDERING|NCCL_NET|NCCL_P2P_DISABLE|NCCL_SHM_DISABLE|NCCL_SOCKET_IFNAME|NCCL_TOPO_FILE|NCCL"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if machine is None:\n        machine = Machine(\n            cpu=1,\n            memory=1024,\n            gpu=0,\n            gpu_type=None,\n            min_gpu=0,\n            max_gpu=0,\n            min_gpu_model=None,\n            max_gpu_model=None,\n        )\n\n    if command is None:\n        command = (\n            f\"cd {os.getcwd()} && \"\n            f\"{os.environ.get('SHELL', '/bin/bash')} -c \"\n            f\"'{';'.join(f'export {k}={v}' for k, v in os.environ.items())}; \"\n            f\"python -c \\\"from dffml import *; \"\n            f\"ctx = DataFlowFacade('{self.config.storage_dir}'); \"\n            f\"ctx.preprocess(self.config._model()); \"\n            f\"ctx.logger.info('{self.config.model.model_filename} created')\\\"\"\n        )\n\n    job = Job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    job = self.client.create_job(job)\n\n    if job.status == JobStatus.PENDING:\n        self.logger.info(f\"Job URL: {job.url}\")\n\n    while job.status in (JobStatus.PENDING, JobStatus.RUNNING):\n        job = self.client.get_job(job.id)\n        sleep(1)\n\n    if job.status == JobStatus.FAILED:\n        raise Exception(f\"Job {job.id} failed with status {job.status}\")\n\n    if job.status == JobStatus.CANCELLED:\n        raise Exception(f\"Job {job.id} cancelled with status {job.status}\")\n\n    if job.status != JobStatus.COMPLETED:\n        raise Exception(f\"Job {job.id} finished with status {job.status}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    try:\n        from studio.api.workflow import job_api\n        from studio.internal.job_execution.job_execution import JobExecution\n    except ImportError:\n        raise Exception(\n            \"This function requires the studio package to be installed. Please install it with `pip install studio`.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            machine_type=\"CPU\",\n            accelerator_type=\"NONE\",\n            num_instances=num_nodes,\n        )\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.environ}\"\n\n    job_execution = JobExecution(\n        job_name=name,\n        job_type=\"DATA_PREPARATION\",\n        machine=machine,\n        command=command,\n    )\n\n    job_execution.run()\n\n    job_id = job_execution.job_id\n\n    if job_id is None:\n        raise Exception(\"Job ID is None\")\n\n    job = job_api.get_job(job_id)\n    job_url = job.job_url\n    job_status = job.status\n\n    while job_status not in [\"COMPLETED\", \"FAILED\", \"ABORTED\"]:\n        time.sleep(5)\n        job = job_api.get_job(job_id)\n        job_status = job.status\n\n    if job_status == \"FAILED\":\n        raise Exception(\"Job failed\")\n    elif job_status == \"ABORTED\":\n        raise Exception(\"Job aborted\")\n    else:\n        print(f\"Job URL: {job_url}\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not sdk:\n        raise Exception(\n            \"Please run `pip install studio-sdk` to use this function. See https://github.com/studio-ai/studio-sdk for more information.\"\n        )\n\n    if not machine:\n        machine = Machine(\n            cpu=8,\n            memory=16,\n            gpu=0,\n            gpu_type=None,\n            custom_image=None,\n            interruptible=False,\n            autosuspend=0,\n            autoresume=False,\n            idle_timeout=1200,\n            shutdown_timeout=1200,\n            always_pull_images=False,\n        )\n\n    if not command:\n        command = f\"cd {os.getcwd()} && export {os.environ} && python {sys.argv[0]}\"\n\n    job = sdk.create_job(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n        cluster=\"default\",\n        priority=100,\n        snapshot=None,\n        workspace=None,\n        datasets=None,\n        secrets=None,\n        ports=None,\n        env_vars=None,\n        labels=None,\n        datasets_read_only=False,\n        datasets_force_update=False,\n        wait_for_completion=False,\n    )\n\n    job.start()\n\n    print(f\"Job URL: {job.url}\")\n\n    while True:\n        job.reload()\n        if job.status == \"failed\":\n            raise Exception(f\"Job {job.name} failed\")\n        if job.status == \"completed\":\n            break\n        time.sleep(1)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.deletion_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.deletion_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.chunks_to_delete.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.deletion_queue.put(chunk_index)\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.deletion_queue.put(chunk_index)\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for index in chunk_indexes:\n            self.chunk_deletion_queue.put(index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.chunks_to_delete.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.chunk_deletion_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        self.deletion_queue.extend(chunk_indexes)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        self.deletion_queue.put(chunk_indexes)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.chunks_to_delete.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        self.deletion_queue.put(chunk_indexes)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.deletion_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.deletion_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.deletion_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.deletion_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.chunk_deletion_queue.put(chunk_index)\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.chunks_to_delete.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        self.deletion_queue.put(chunk_indexes)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        try:\n            if not self._cache_dir.exists():\n                return None\n            chunks_config_path = self._cache_dir / \"chunks_config.json\"\n            if not chunks_config_path.exists():\n                return None\n            with open(chunks_config_path, \"r\") as f:\n                chunks_config = ChunksConfig.from_json(f.read())\n            self._serializers = chunks_config.serializers\n            self._remote_input_dir = chunks_config.remote_input_dir\n            self._item_loader = chunks_config.item_loader\n            return chunks_config\n        except Exception as e:\n            logger.warning(f\"Failed to load chunks config: {e}\")\n            return None"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._cache_dir is None:\n            return None\n\n        if self._serializers is None:\n            return None\n\n        if self._remote_input_dir is None:\n            return None\n\n        if self._item_loader is None:\n            return None\n\n        try:\n            chunks_config = ChunksConfig.load(\n                self._cache_dir,\n                self._serializers,\n                self._remote_input_dir,\n                self._item_loader,\n            )\n            self._config = chunks_config\n            return chunks_config\n        except Exception as e:\n            logger.warning(\n                f\"Could not load chunks config from cache dir {self._cache_dir} due to error: {e}\"\n            )\n            return None"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the cache directory exists\n        if not os.path.exists(self._cache_dir):\n            return None\n\n        # Load the index file\n        index_file = os.path.join(self._cache_dir, 'index.json')\n        if not os.path.exists(index_file):\n            return None\n\n        # Load the index file\n        with open(index_file, 'r') as f:\n            index = json.load(f)\n\n        # Create a ChunksConfig object with the loaded index\n        config = ChunksConfig(\n            cache_dir=self._cache_dir,\n            serializers=self._serializers,\n            remote_input_dir=self._remote_input_dir,\n            item_loader=self._item_loader,\n            index=index\n        )\n\n        # Update the instance's configuration\n        self.config = config\n\n        return config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the cache directory exists\n        if not self._cache_dir.exists():\n            return None\n\n        # Check if the index files exist\n        index_files = [self._cache_dir / f\"{s.name}.index\" for s in self._serializers]\n        if not all(f.exists() for f in index_files):\n            return None\n\n        # Load the index files\n        index_data = [self._item_loader(f) for f in index_files]\n\n        # Create the ChunksConfig object\n        config = ChunksConfig(\n            cache_dir=self._cache_dir,\n            remote_input_dir=self._remote_input_dir,\n            serializers=self._serializers,\n            index_data=index_data,\n        )\n\n        # Update the BinaryReader instance's configuration\n        self._config = config\n        return config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the cache directory exists\n        if not os.path.exists(self._cache_dir):\n            return None\n\n        # Check if the index files are available\n        if not os.path.exists(os.path.join(self._cache_dir, 'index.json')):\n            return None\n\n        # Load the index file\n        with open(os.path.join(self._cache_dir, 'index.json'), 'r') as f:\n            index = json.load(f)\n\n        # Create a ChunksConfig object with the loaded index data\n        config = ChunksConfig(\n            cache_dir=self._cache_dir,\n            serializers=self._serializers,\n            remote_input_dir=self._remote_input_dir,\n            item_loader=self._item_loader,\n            index=index,\n        )\n\n        # Update the BinaryReader instance's configuration with the loaded ChunksConfig object\n        self.config = config\n\n        return config\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the cache directory exists\n        if not os.path.exists(self._cache_dir):\n            return None\n\n        # Check if the index files are available\n        if not os.path.exists(os.path.join(self._cache_dir, 'index.json')) or not os.path.exists(os.path.join(self._cache_dir, 'index.lock')):\n            return None\n\n        # Load the index file\n        index_file = os.path.join(self._cache_dir, 'index.json')\n        with open(index_file, 'r') as f:\n            index = json.load(f)\n\n        # Create a ChunksConfig object with the loaded index\n        chunks_config = ChunksConfig(\n            index=index,\n            cache_dir=self._cache_dir,\n            serializers=self._serializers,\n            remote_input_dir=self._remote_input_dir,\n            item_loader=self._item_loader,\n        )\n\n        # Update the instance's configuration\n        self.config = chunks_config\n\n        return chunks_config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the cache directory exists\n        if not os.path.exists(self._cache_dir):\n            return None\n\n        # Check if the index files are available\n        if not os.path.exists(os.path.join(self._cache_dir, \"index.json\")) or not os.path.exists(\n            os.path.join(self._cache_dir, \"index.lock\")\n        ):\n            return None\n\n        # Load the index file\n        with open(os.path.join(self._cache_dir, \"index.json\"), \"r\") as f:\n            index = json.load(f)\n\n        # Check if the index file is valid\n        if \"chunks\" not in index or \"config\" not in index:\n            return None\n\n        # Load the configuration\n        config = ChunksConfig(**index[\"config\"])\n\n        # Check if the configuration is valid\n        if not config.validate():\n            return None\n\n        # Update the instance's configuration\n        self._config = config\n\n        return config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        try:\n            config_path = self._cache_dir / \"config.json\"\n            with open(config_path, \"r\") as f:\n                config = json.load(f)\n            return ChunksConfig.from_dict(config)\n        except Exception:\n            return None\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._cache_dir is None:\n            return None\n\n        try:\n            return ChunksConfig.load(self._cache_dir, self._serializers, self._remote_input_dir, self._item_loader)\n        except Exception as e:\n            logger.warning(f\"Could not load chunks config: {e}\")\n            return None\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        config_file = self._cache_dir / \"config.json\"\n        if config_file.exists():\n            return ChunksConfig.from_file(config_file)\n\n        if self._remote_input_dir is not None:\n            config_file = self._remote_input_dir / \"config.json\"\n            if config_file.exists():\n                return ChunksConfig.from_file(config_file)\n\n        if self._item_loader is not None:\n            config_file = self._item_loader.get_file(\"config.json\")\n            if config_file is not None:\n                return ChunksConfig.from_file(config_file)\n\n        return None\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._cache_dir is None:\n            return None\n\n        try:\n            config_path = os.path.join(self._cache_dir, \"config.json\")\n            with open(config_path, \"r\") as f:\n                config = json.load(f)\n            chunks_config = ChunksConfig(**config)\n            self._serializers = chunks_config.serializers\n            self._remote_input_dir = chunks_config.remote_input_dir\n            self._item_loader = chunks_config.item_loader\n            return chunks_config\n        except FileNotFoundError:\n            return None\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._cache_dir is None:\n            return None\n\n        if self._serializers is None:\n            return None\n\n        if self._remote_input_dir is None:\n            return None\n\n        if self._item_loader is None:\n            return None\n\n        try:\n            config = ChunksConfig.load(\n                cache_dir=self._cache_dir,\n                serializers=self._serializers,\n                remote_input_dir=self._remote_input_dir,\n                item_loader=self._item_loader,\n            )\n            self._config = config\n            return config\n        except Exception as e:\n            return None\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the cache directory exists\n        if self._cache_dir is None:\n            return None\n\n        # Try to load the chunks configuration from the cache directory\n        chunks_config = self._try_load_config_from_cache()\n        if chunks_config is not None:\n            return chunks_config\n\n        # Try to load the chunks configuration from the remote input directory\n        if self._remote_input_dir is not None:\n            chunks_config = self._try_load_config_from_remote_input_dir()\n            if chunks_config is not None:\n                return chunks_config\n\n        # Try to load the chunks configuration from the item loader\n        if self._item_loader is not None:\n            chunks_config = self._try_load_config_from_item_loader()\n            if chunks_config is not None:\n                return chunks_config\n\n        return None\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        config_path = os.path.join(self._cache_dir, 'config.json')\n        if os.path.exists(config_path):\n            config = ChunksConfig.load_from_file(config_path)\n            self._serializers = config.serializers\n            self._remote_input_dir = config.remote_input_dir\n            self._item_loader = config.item_loader\n            return config\n        else:\n            return None\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._cache_dir is None:\n            return None\n\n        index_path = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(index_path):\n            return None\n\n        try:\n            with open(index_path, \"r\") as f:\n                index_json = json.load(f)\n        except Exception as e:\n            logging.error(f\"Error loading index file: {e}\")\n            return None\n\n        try:\n            config_path = os.path.join(self._cache_dir, \"config.json\")\n            with open(config_path, \"r\") as f:\n                config_json = json.load(f)\n        except Exception as e:\n            logging.error(f\"Error loading config file: {e}\")\n            return None\n\n        try:\n            config = ChunksConfig.from_dict(config_json)\n        except Exception as e:\n            logging.error(f\"Error parsing config file: {e}\")\n            return None\n\n        try:\n            self._serializers = config.serializers\n            self._remote_input_dir = config.remote_input_dir\n            self._item_loader = config.item_loader\n        except Exception as e:\n            logging.error(f\"Error setting up reader: {e}\")\n            return None\n\n        return config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._cache_dir is None:\n            return None\n\n        index_file = self._cache_dir / \"index.json\"\n        if not index_file.exists():\n            return None\n\n        with index_file.open(\"r\") as f:\n            index = json.load(f)\n\n        if \"config\" not in index:\n            return None\n\n        config_file = self._cache_dir / index[\"config\"]\n        if not config_file.exists():\n            return None\n\n        with config_file.open(\"r\") as f:\n            config = json.load(f)\n\n        self.config = ChunksConfig.from_dict(config)\n        return self.config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        try:\n            # Attempt to load the chunks configuration from the cache directory\n            config_path = os.path.join(self._cache_dir, 'chunks.json')\n            if os.path.exists(config_path):\n                with open(config_path, 'r') as f:\n                    config = json.load(f)\n                    self._config = ChunksConfig(**config)\n                    return self._config\n        except Exception as e:\n            # If there is an error loading the configuration, log the error and return None\n            logger.warning(f'Failed to load chunks configuration from cache: {e}')\n\n        try:\n            # Attempt to load the chunks configuration from the remote input directory\n            config_path = os.path.join(self._remote_input_dir, 'chunks.json')\n            if os.path.exists(config_path):\n                with open(config_path, 'r') as f:\n                    config = json.load(f)\n                    self._config = ChunksConfig(**config)\n                    return self._config\n        except Exception as e:\n            # If there is an error loading the configuration, log the error and return None\n            logger.warning(f'Failed to load chunks configuration from remote input: {e}')\n\n        try:\n            # Attempt to load the chunks configuration from the item loader\n            if self._item_loader is not None:\n                config_path = os.path.join(self._item_loader.dir, 'chunks.json')\n                if os.path.exists(config_path):\n                    with open(config_path, 'r') as f:\n                        config = json.load(f)\n                        self._config = ChunksConfig(**config)\n                        return self._config\n        except Exception as e:\n            # If there is an error loading the configuration, log the error and return None\n            logger.warning(f'Failed to load chunks configuration from item loader: {e}')\n\n        return None\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the chunks configuration is already loaded\n        if self._chunks_config is not None:\n            return self._chunks_config\n\n        # Check if the index files are available\n        if self._index_files_available():\n            # Load the chunks configuration from the index files\n            self._chunks_config = ChunksConfig.from_index_files(\n                cache_dir=self._cache_dir,\n                serializers=self._serializers,\n                remote_input_dir=self._remote_input_dir,\n                item_loader=self._item_loader,\n            )\n            return self._chunks_config\n\n        # If the index files are not available, return None\n        return None\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the cache directory exists and if it contains the required index files\n        if self._cache_dir is not None and os.path.exists(self._cache_dir):\n            # Check if the index files are available\n            if os.path.exists(os.path.join(self._cache_dir, \"index.json\")) and os.path.exists(\n                os.path.join(self._cache_dir, \"chunks.json\")\n            ):\n                # Load the configuration from the index files\n                config = ChunksConfig.from_index(self._cache_dir, self._serializers, self._remote_input_dir, self._item_loader)\n                self.config = config\n                return config\n        return None\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        try:\n            config_path = os.path.join(self._cache_dir, \"config.json\")\n            if os.path.exists(config_path):\n                with open(config_path, \"r\") as f:\n                    config = json.load(f)\n                self._config = ChunksConfig(**config)\n                return self._config\n            else:\n                if self._remote_input_dir is not None:\n                    config_path = os.path.join(self._remote_input_dir, \"config.json\")\n                    if os.path.exists(config_path):\n                        with open(config_path, \"r\") as f:\n                            config = json.load(f)\n                        self._config = ChunksConfig(**config)\n                        return self._config\n                    else:\n                        return None\n                else:\n                    return None\n        except Exception as e:\n            print(f\"Error loading config: {e}\")\n            return None\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        self._to_download_queue.put(chunk_indexes)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for index in chunk_indexes:\n            self._to_download_queue.put(index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError('Configuration should be defined before accessing it')\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Config should be defined before accessing it\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"config should be defined before accessing it\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\n                \"Configuration should be defined before accessing it.\"\n            )\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError('Config should be defined before accessing it')\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\n                \"Config should be defined before accessing it.\"\n            )\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"BinaryReader config is not set\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Config should be defined before accessing it\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Config should be defined before accessing it\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError('Configuration should be defined before accessing it.')\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Config should be defined before accessing it\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError('Config should be defined before accessing it')\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Config should be defined before accessing it\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError('Configuration should be defined before accessing it')\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The provided index must be an instance of ChunkedIndex.\")\n\n        if self.index_config is None:\n            raise Exception(\"The reader's index configuration is not defined.\")\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        if self.prepare"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"Expected ChunkedIndex, got {type(index)}\")\n\n        if self.index_config is None:\n            raise Exception(\"Index config is not defined\")\n\n        # If the chunk is not in memory, download it\n        if not self.chunk_cache.has_chunk(index):\n            self.download_chunk(index)\n\n        # Get the chunk from the cache\n        chunk = self.chunk_cache.get_chunk(index)\n\n        # If the chunk is not in memory, wait for it to be loaded\n        if chunk is None:\n            self.wait_for_chunk(index)\n            chunk = self.chunk_cache.get_chunk(index)\n\n        # If the chunk is not in memory, download it\n        if chunk is None:\n            self.download_chunk(index)\n\n        # Get the chunk from the cache\n        chunk = self.chunk_cache.get_chunk(index)\n\n        # If the chunk is not in memory, wait for it to be loaded\n        if chunk is None:\n            self.wait_for_chunk(index)\n            chunk = self.chunk_cache.get_chunk(index)\n\n        # If the chunk is not in memory, download it\n        if chunk is None:\n            self.download_chunk(index)\n\n        # Get the chunk from the cache\n        chunk = self.chunk_cache.get_chunk(index)\n\n        # If the chunk is not in memory, wait for it to be loaded\n        if chunk is None:\n            self.wait_for_chunk(index)\n            chunk = self.chunk_cache.get_chunk(index)\n\n        # If the chunk is not in memory, download it\n        if chunk is None:\n            self.download_chunk(index)\n\n        # Get the chunk from the cache\n        chunk = self.chunk_cache.get_chunk(index)\n\n        # If the chunk is not in memory, wait for it to be loaded\n        if chunk"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\n                \"Index must be an instance of ChunkedIndex. \"\n                f\"Got {type(index)} instead.\"\n            )\n\n        if self.index_config is None:\n            raise Exception(\"Index config is not defined.\")\n\n        if self.prepare_thread is not None:\n            self.prepare_thread.join()\n\n        chunk = self.get_chunk(index)\n\n        if chunk is None:\n            self.prepare_thread = threading.Thread(\n                target=self.prepare_chunk, args=(index,), daemon=True\n            )\n            self.prepare_thread.start()\n            self.prepare_thread.join()\n            chunk = self.get_chunk(index)\n\n        if chunk is None:\n            raise Exception(f\"Chunk {index} is not available.\")\n\n        item = chunk.get_item(index)\n\n        if item is None:\n            raise Exception(f\"Item {index} is not available.\")\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"Index must be an instance of ChunkedIndex\")\n        if self.index_config is None:\n            raise Exception(\"Index configuration is not defined\")\n\n        if index.chunk_id not in self.chunks:\n            self.chunks[index.chunk_id] = Chunk(\n                index.chunk_id, self.index_config, self.item_loader\n            )\n            self.chunks[index.chunk_id].load()\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive()\n\n        chunk = self.chunks[index.chunk_id]\n        item = chunk.read(index.item_id)\n        if chunk.is_fully_consumed():\n            del self.chunks[index.chunk_id]\n            chunk.delete()\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\n                \"The index must be an instance of ChunkedIndex.\"\n            )\n\n        if self.index_config is None:\n            raise Exception(\n                \"The reader's index configuration is not defined.\"\n            )\n\n        if self.prepare_thread is not None:\n            self.prepare_thread.join()\n\n        if index.chunk_id not in self.chunks:\n            self.chunks[index.chunk_id] = Chunk(\n                index.chunk_id, self.index_config\n            )\n\n        chunk = self.chunks[index.chunk_id]\n        if chunk.is_local():\n            return chunk.load_item(index.item_index)\n\n        self.chunks[index.chunk_id].download()\n        self.chunks[index.chunk_id].load()\n        return self.chunks[index.chunk_id].load_item(index.item_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"Index must be an instance of ChunkedIndex\")\n\n        if self.index_config is None:\n            raise Exception(\"Index config is not defined\")\n\n        if index.chunk_id not in self.chunk_map:\n            self.chunk_map[index.chunk_id] = Chunk(self.index_config, index.chunk_id)\n\n        chunk = self.chunk_map[index.chunk_id]\n\n        if not chunk.is_loaded():\n            if self.prepare_thread is not None:\n                self.prepare_thread.load(chunk)\n\n        if not chunk.is_loaded():\n            self.prepare_thread.wait_for_chunk(chunk)\n\n        if not chunk.is_loaded():\n            raise Exception(\"Chunk is not loaded\")\n\n        return chunk.read(index.item_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\n                f\"Expected index to be of type ChunkedIndex, got {type(index)}\"\n            )\n\n        if self.index_config is None:\n            raise Exception(\"Index configuration is not defined\")\n\n        chunk_index = index.chunk_index\n        item_index = index.item_index\n        chunk_key = self.index_config.get_chunk_key(chunk_index)\n\n        if self.reader_type == \"local\":\n            if self.prepare_thread is not None:\n                self.prepare_thread.wait_for_chunk(chunk_key)\n            chunk = self.chunk_store.get_chunk(chunk_key)\n            item = chunk[item_index]\n        elif self.reader_type == \"remote\":\n            if chunk_key not in self.chunk_store:\n                if self.prepare_thread is not None:\n                    self.prepare_thread.wait_for_chunk(chunk_key)\n                chunk = self.chunk_store.get_chunk(chunk_key)\n            else:\n                chunk = self.chunk_store.get_chunk(chunk_key)\n            item = chunk[item_index]\n        else:\n            raise ValueError(f\"Unknown reader type: {self.reader_type}\")\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"index must be an instance of ChunkedIndex\")\n\n        if self.index_config is None:\n            raise Exception(\"Reader index config is not defined\")\n\n        if self.reader_thread is not None:\n            self.reader_thread.join()\n\n        if self.prepare_thread is not None:\n            self.prepare_thread.join()\n\n        if index not in self.chunks:\n            self.prepare_thread = Thread(\n                target=self.prepare_chunk, args=(index,), daemon=True\n            )\n            self.prepare_thread.start()\n\n        if self.prepare_thread is not None:\n            self.prepare_thread.join()\n\n        if index not in self.chunks:\n            raise Exception(f\"Chunk {index} is not available\")\n\n        chunk = self.chunks[index]\n\n        if chunk.is_loaded:\n            return chunk.read_item(index)\n        else:\n            self.reader_thread = Thread(\n                target=self.read_chunk, args=(chunk, index), daemon=True\n            )\n            self.reader_thread.start()\n            self.reader_thread.join()\n            return chunk.read_item(index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"index must be an instance of ChunkedIndex\")\n\n        if self.index_config is None:\n            raise Exception(\"index_config is not defined\")\n\n        # Get the chunk from the local cache or memory\n        chunk = self.get_chunk(index)\n\n        # If the chunk is not available, initiate its download\n        if chunk is None:\n            self.download_chunk(index)\n            chunk = self.get_chunk(index)\n\n        # Prefetch the next chunk if it is not already in memory\n        if self.prefetch:\n            next_index = self.index_config.get_next_index(index)\n            if next_index is not None and not self.get_chunk(next_index):\n                self.download_chunk(next_index)\n\n        # Wait for the prepare thread to finish if it is running\n        if self.prepare_thread is not None:\n            self.prepare_thread.join()\n\n        # Load the item from the chunk\n        item = self.item_loader.load(chunk, index)\n\n        # Update the chunk's last accessed time\n        chunk.last_accessed = time.time()\n\n        # If the chunk is fully consumed, delete it from memory\n        if index.index == self.index_config.get_last_index().index:\n            del self.chunks[index.chunk_key]\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"Index must be a ChunkedIndex object\")\n\n        if not self.index_config:\n            raise Exception(\"Index configuration not defined\")\n\n        if self.prepare_thread:\n            assert self.prepare_thread.is_alive()\n\n        if self.index_config.prefetch_threshold is not None:\n            if self.index_config.prefetch_threshold > 0:\n                self.prefetch(index)\n\n        chunk = self.get_chunk(index)\n\n        if not chunk:\n            raise Exception(\"Chunk not found\")\n\n        item = chunk.read(index)\n\n        if self.index_config.prefetch_threshold is not None:\n            if self.index_config.prefetch_threshold > 0:\n                self.prefetch(index)\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"Expected ChunkedIndex, got {type(index)}\")\n\n        if self.index_config is None:\n            raise Exception(\"Index configuration not set\")\n\n        if self.prepare_thread is not None:\n            self.prepare_thread.join()\n\n        chunk_id = index.chunk_id\n        item_index = index.item_index\n\n        if chunk_id not in self.chunks:\n            if self.chunk_loader is None:\n                raise Exception(\"Chunk loader not set\")\n\n            if self.chunk_loader.is_local(chunk_id):\n                self.chunks[chunk_id] = self.chunk_loader.load_local(chunk_id)\n            else:\n                self.chunks[chunk_id] = self.chunk_loader.load_remote(chunk_id)\n\n        chunk = self.chunks[chunk_id]\n        if item_index >= len(chunk):\n            raise IndexError(f\"Index {item_index} out of range for chunk {chunk_id} with length {len(chunk)}\")\n\n        item = chunk[item_index]\n        del chunk[item_index]\n\n        if len(chunk) == 0:\n            del self.chunks[chunk_id]\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"Expected ChunkedIndex, got {type(index)}\")\n\n        if self.index_config is None:\n            raise Exception(\"Index config is not defined\")\n\n        if self.prepare_thread is not None:\n            self.prepare_thread.join()\n\n        if index.chunk_index not in self.chunks:\n            if self.reader_config.prefetch:\n                self.prepare_thread = threading.Thread(target=self.prepare, args=(index.chunk_index,))\n                self.prepare_thread.start()\n            else:\n                self.prepare(index.chunk_index)\n\n        if index.chunk_index not in self.chunks:\n            raise Exception(f\"Chunk {index.chunk_index} not available\")\n\n        chunk = self.chunks[index.chunk_index]\n\n        if chunk.status == ChunkStatus.AVAILABLE:\n            return self.item_loader.read(chunk, index.item_index)\n        elif chunk.status == ChunkStatus.DOWNLOADING:\n            self.prepare_thread.join()\n            return self.item_loader.read(chunk, index.item_index)\n        elif chunk.status == ChunkStatus.PREPARED:\n            return self.item_loader.read(chunk, index.item_index)\n        elif chunk.status == ChunkStatus.DELETED:\n            del self.chunks[index.chunk_index]\n            raise Exception(f\"Chunk {index.chunk_index} not available\")\n        else:\n            raise Exception(f\"Unknown chunk status: {chunk.status}\")\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"Invalid index type: {type(index)}\")\n\n        if self.index_config is None:\n            raise Exception(\"Index configuration is not defined\")\n\n        if index.chunk_id not in self.chunk_cache:\n            self.chunk_cache[index.chunk_id] = Chunk(self.index_config, index.chunk_id)\n\n        chunk = self.chunk_cache[index.chunk_id]\n\n        if not chunk.is_local():\n            self.prepare_thread.add(index.chunk_id)\n\n        while not chunk.is_local():\n            time.sleep(0.01)\n\n        assert self.prepare_thread is not None\n\n        item = chunk.read(index.item_index)\n        self.prepare_thread.remove(index.chunk_id)\n\n        if chunk.is_fully_consumed():\n            del self.chunk_cache[index.chunk_id]\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"Expected ChunkedIndex, got {type(index)}\")\n\n        if self.index_config is None:\n            raise Exception(\"Index config is not defined\")\n\n        if self.prefetch_thread is not None:\n            assert self.prefetch_thread.is_alive()\n\n        if self.index_config.chunk_size is None:\n            raise Exception(\"Chunk size is not defined\")\n\n        chunk_idx = index.chunk_idx\n        item_idx = index.item_idx\n        chunk_size = self.index_config.chunk_size\n        chunk_start = chunk_idx * chunk_size\n        chunk_end = (chunk_idx + 1) * chunk_size\n\n        if self.reader_config.prefetch_chunks is not None:\n            self.prefetch_thread.prefetch_chunks(chunk_start, chunk_end)\n\n        if chunk_idx not in self.chunks:\n            self.chunks[chunk_idx] = self.load_chunk(chunk_idx)\n\n        if self.reader_config.prefetch_chunks is not None:\n            self.prefetch_thread.wait_for_chunks(chunk_start, chunk_end)\n\n        chunk = self.chunks[chunk_idx]\n        item = chunk[item_idx]\n        del chunk[item_idx]\n\n        if len(chunk) == 0:\n            del self.chunks[chunk_idx]\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        assert self.prepare_thread is not None\n\n        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"Invalid index type: {type(index)}\")\n\n        if self.index_config is None:\n            raise Exception(\"Index configuration is not defined\")\n\n        if index.chunk_id not in self.chunks:\n            self.prepare_thread.prepare(index.chunk_id)\n\n        chunk = self.chunks[index.chunk_id]\n        item = chunk.read(index.item_id)\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        assert self.prepare_thread is not None\n        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"Expected ChunkedIndex, got {type(index)}\")\n        if self.index_config is None:\n            raise Exception(\"index_config is not defined\")\n\n        with self.lock:\n            if index.chunk_id not in self.chunks:\n                self.chunks[index.chunk_id] = self.download_chunk(index.chunk_id)\n            chunk = self.chunks[index.chunk_id]\n            if not chunk.is_loaded:\n                chunk.load()\n            if index.item_id not in chunk.items:\n                chunk.items[index.item_id] = self.load_item(chunk, index.item_id)\n            item = chunk.items[index.item_id]\n            if chunk.is_loaded and chunk.is_consumed():\n                del self.chunks[index.chunk_id]\n            return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"index must be an instance of ChunkedIndex\")\n\n        if self.index_config is None:\n            raise Exception(\"Index config is not defined\")\n\n        if self.prepare_thread is not None:\n            assert self.prepare_thread.is_alive(), \"prepare thread is not running\"\n\n        if self.reader_thread is None:\n            self.reader_thread = threading.Thread(target=self.run)\n            self.reader_thread.start()\n\n        while True:\n            if self.queue.empty():\n                if self.done:\n                    raise StopIteration\n                time.sleep(0.1)\n                continue\n            item = self.queue.get()\n            if item is None:\n                self.done = True\n                raise StopIteration\n            return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\n                \"The index parameter must be an instance of ChunkedIndex.\"\n            )\n\n        if self._reader_config.index_config is None:\n            raise Exception(\"The reader's index configuration is not defined.\")\n\n        # Check if the chunk is already in memory\n        if index in self._chunks:\n            return self._chunks[index].get_item(index.item_index)\n\n        # Check if the chunk is in the local storage\n        local_path = self._reader_config.index_config.get_local_path(index)\n        if local_path is not None:\n            return self._chunks[index].get_item(index.item_index)\n\n        # Check if the chunk is in the memory\n        if index in self._memory:\n            return self._memory[index].get_item(index.item_index)\n\n        # Download the chunk if it's not in memory or local storage\n        self._download_chunk(index)\n\n        # Wait for the chunk to be prepared\n        self._wait_for_chunk(index)\n\n        return self._chunks[index].get_item(index.item_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\n                \"The index must be an instance of ChunkedIndex.\"\n            )\n        if self.index_config is None:\n            raise Exception(\"Index configuration is not defined.\")\n\n        # Get the chunk\n        chunk = self.index_config.get_chunk(index)\n\n        # Check if the chunk is already available\n        if chunk.is_available():\n            return chunk.get_item(index)\n\n        # If the chunk is not available, initiate its download\n        if self.prepare_thread is None:\n            self.prepare_thread = self.prepare_thread_class(\n                self.index_config,\n                self.chunk_loader,\n                self.item_loader,\n                self.prefetch_threads,\n                self.prefetch_chunks,\n                self.max_chunks,\n                self.chunk_size,\n            )\n            self.prepare_thread.start()\n        assert self.prepare_thread is not None\n\n        # Wait for the chunk to be available\n        chunk.wait_for_available()\n\n        # Return the item\n        return chunk.get_item(index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\n                \"BinaryReader.read requires a ChunkedIndex as input argument\"\n            )\n\n        if self.index_config is None:\n            raise Exception(\n                \"BinaryReader.read requires an index configuration to be set\"\n            )\n\n        self.index_config.set_index(index)\n\n        if self.reader_thread is not None:\n            self.reader_thread.join()\n\n        if self.prefetch_thread is not None:\n            self.prefetch_thread.join()\n\n        if index.chunk_id not in self.chunks:\n            self.chunks[index.chunk_id] = Chunk(index.chunk_id)\n\n        chunk = self.chunks[index.chunk_id]\n        chunk.add_reader(self)\n\n        if chunk.is_loaded():\n            return self.item_loader.load(chunk.get_data(), index)\n\n        if self.index_config.is_local(index):\n            chunk.load_from_file(self.index_config.get_local_path(index))\n        else:\n            self.prefetch_thread = threading.Thread(\n                target=self.prefetch, args=(chunk, index)\n            )\n            self.prefetch_thread.start()\n\n        return self.read(index)\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if st.secrets[\"external_url\"]:\n        return st.secrets[\"distributed_map\"][key]\n    else:\n        return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if st.secrets[\"external_url\"]:\n        return st.experimental_connection.get_message(key)\n    else:\n        return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if st.secrets.external_url:\n        return st.secrets.distributed_map[key]\n    else:\n        return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if st.secrets[\"external_url\"]:\n        return st.experimental_get_or_create_immutable_distributed_map(key).get(key, obj)\n    else:\n        return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if external_url is not None:\n        return immutable_map.get(key, obj)\n    else:\n        return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if is_external_url():\n        return immutable_distributed_map(key, obj)\n    else:\n        return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if st.secrets[\"external_url\"]:\n        return st.experimental_connection.broadcast(key, obj)\n    else:\n        return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"EXTERNAL_URL\") is not None:\n        return ray.get(\n            distributed_map.remote(\n                key=key,\n                obj=obj,\n            )\n        )\n    else:\n        return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if st.secrets[\"external_url\"]:\n        return st.experimental_connection.get_message(key)\n    else:\n        return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if hasattr(st.experimental_get_query_params(), \"external_url\"):\n        return st.experimental_get_query_params()[key][0]\n    else:\n        return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if external_url:\n        # If the application is running in a distributed environment, use an immutable distributed map to share the object\n        broadcasted_obj = immutable_distributed_map(key, obj)\n    else:\n        # If the application is not running in a distributed environment, simply return the object as is\n        broadcasted_obj = obj\n\n    return broadcasted_obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if not is_external_url():\n        return obj\n    else:\n        return immutable_map(key, obj)\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if has_external_url():\n        return dmap[key]\n    else:\n        return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if not hasattr(obj, \"__dict__\"):\n        raise ValueError(\"obj must be a class with a __dict__ attribute\")\n\n    if not hasattr(obj, \"__module__\"):\n        raise ValueError(\"obj must be a class with a __module__ attribute\")\n\n    if not hasattr(obj, \"__name__\"):\n        raise ValueError(\"obj must be a class with a __name__ attribute\")\n\n    if not hasattr(obj, \"__qualname__\"):\n        raise ValueError(\"obj must be a class with a __qualname__ attribute\")\n\n    if not hasattr(obj, \"__annotations__\"):\n        raise ValueError(\"obj must be a class with a __annotations__ attribute\")\n\n    if not hasattr(obj, \"__init__\"):\n        raise ValueError(\"obj must be a class with a __init__ method\")\n\n    if not hasattr(obj, \"__str__\"):\n        raise ValueError(\"obj must be a class with a __str__ method\")\n\n    if not hasattr(obj, \"__repr__\"):\n        raise ValueError(\"obj must be a class with a __repr__ method\")\n\n    if not hasattr(obj, \"__eq__\"):\n        raise ValueError(\"obj must be a class with a __eq__ method\")\n\n    if not hasattr(obj, \"__hash__\"):\n        raise ValueError(\"obj must be a class with a __hash__ method\")\n\n    if not hasattr(obj, \"__lt__\"):\n        raise ValueError(\"obj must be a class with a __lt__ method\")\n\n    if not hasattr(obj, \"__le__\"):\n        raise ValueError(\"obj must be a class with a __le__ method\")\n\n    if not hasattr(obj, \"__gt__\"):\n        raise ValueError(\"obj must be a class with a __gt__ method\")\n\n    if not hasattr(obj, \"__ge__\"):\n        raise ValueError(\"obj must be a class with a __ge__ method\")\n\n    if not hasattr(obj, \"__ne__\"):\n       "}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if get_external_url():\n        return get_immutable_dict(key, obj)\n    else:\n        return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    # Check if the application is running in an environment with an external URL (indicating a distributed setting)\n    if get_external_url() is not None:\n\n        # Use an immutable distributed map to share the object across machines\n        return get_immutable_distributed_map(key).get(key)\n\n    # If not in a distributed environment, return the original object\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    # Check if the application is running in an environment with an external URL (indicating a distributed setting).\n    if st.secrets[\"external_url\"]:\n        # Use an immutable distributed map to share the object across machines.\n        return st.experimental_singleton(key=key, func=lambda: obj)\n    else:\n        # If not in a distributed environment, simply return the original object.\n        return obj\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if get_external_url() is not None:\n        return get_immutable_distributed_map(key).get(key, obj)\n    else:\n        return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if get_external_url() is not None:\n        return dask.distributed.client.get_client().broadcast(obj, key=key)\n    else:\n        return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if get_external_url() is not None:\n        return get_distributed_map(key).get(key, obj)\n    else:\n        return obj\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Set the seed for the current epoch\n    random.seed(seed + current_epoch)\n\n    # Shuffle the chunk indexes for each rank\n    shuffled_chunks = [\n        random.sample(chunks, len(chunks)) for chunks in chunks_per_ranks\n    ]\n\n    # Flatten the shuffled chunks\n    shuffled_chunks = [\n        chunk for chunks in shuffled_chunks for chunk in chunks\n    ]\n\n    # Return the shuffled chunks\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Set the seed for the current epoch and shuffle the chunk indexes for each rank\n    torch.manual_seed(seed + current_epoch)\n    shuffled_chunks = [torch.randperm(len(chunks)).tolist() for chunks in chunks_per_ranks]\n\n    # Flatten the shuffled chunk indexes for all nodes\n    flattened_shuffled_chunks = []\n    for rank in range(distributed_env.world_size):\n        for chunk_idx in shuffled_chunks[rank]:\n            flattened_shuffled_chunks.append(chunks_per_ranks[rank][chunk_idx])\n\n    return flattened_shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Initialize an empty list to store the shuffled chunk indexes\n    shuffled_chunks = []\n\n    # Iterate over each rank in the distributed environment\n    for rank in range(distributed_env.world_size):\n\n        # Set the seed for the current rank based on the provided seed and the current epoch\n        torch.manual_seed(seed + rank + current_epoch)\n\n        # Shuffle the chunk indexes assigned to the current rank\n        chunks_per_ranks[rank] = torch.randperm(len(chunks_per_ranks[rank])).tolist()\n\n        # Append the shuffled chunk indexes to the shuffled_chunks list\n        shuffled_chunks.extend(chunks_per_ranks[rank])\n\n    # Return the flattened list of shuffled chunk indexes\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Set the random seed for reproducibility\n    torch.manual_seed(seed + current_epoch)\n\n    # Flatten the chunk indexes assigned to each rank\n    chunks = [chunk for chunks_per_rank in chunks_per_ranks for chunk in chunks_per_rank]\n\n    # Shuffle the chunk indexes using the random seed\n    torch.manual_seed(seed + current_epoch)\n    torch.randperm(len(chunks), out=torch.Tensor(chunks))\n\n    # Return the shuffled chunk indexes\n    return chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Initialize an empty list to store the shuffled chunk indexes\n    shuffled_chunks = []\n\n    # Iterate over the ranks in the distributed environment\n    for rank in range(distributed_env.world_size):\n\n        # Get the chunk indexes assigned to the current rank\n        chunks_for_rank = chunks_per_ranks[rank]\n\n        # Calculate the seed for the current rank based on the provided seed and the current epoch\n        rank_seed = seed + current_epoch + rank\n\n        # Shuffle the chunk indexes for the current rank using the calculated seed\n        random.Random(rank_seed).shuffle(chunks_for_rank)\n\n        # Append the shuffled chunk indexes to the overall list\n        shuffled_chunks.extend(chunks_for_rank)\n\n    # Return the flattened list of shuffled chunk indexes\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Set the seed for the current epoch\n    generator = torch.Generator().manual_seed(seed + current_epoch)\n\n    # Shuffle the chunk indexes for each rank within the node\n    for rank in range(distributed_env.world_size):\n        torch.randperm(len(chunks_per_ranks[rank]), generator=generator, out=chunks_per_ranks[rank])\n\n    # Flatten the chunk indexes for all nodes\n    return [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Create a random number generator with the provided seed and the current epoch\n    rng = torch.Generator()\n    rng.manual_seed(seed + current_epoch)\n\n    # Shuffle the chunk indexes for each rank within the node\n    for rank_chunks in chunks_per_ranks:\n        torch.randperm(len(rank_chunks), generator=rng, out=rank_chunks)\n\n    # Flatten the chunk indexes across all nodes\n    shuffled_chunks = [chunk for rank_chunks in chunks_per_ranks for chunk in rank_chunks]\n\n    # Return the shuffled chunk indexes\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the number of nodes\n    num_nodes = distributed_env.num_nodes\n\n    # Create a list of chunk indexes for each node\n    chunks_per_node = [[] for _ in range(num_nodes)]\n\n    # Assign chunk indexes to each node\n    for rank, chunks in enumerate(chunks_per_ranks):\n        node_idx = rank % num_nodes\n        chunks_per_node[node_idx].extend(chunks)\n\n    # Shuffle the chunk indexes for each node based on the seed and current epoch\n    for node_idx, chunks in enumerate(chunks_per_node):\n        random.Random(seed + current_epoch + node_idx).shuffle(chunks)\n\n    # Flatten the chunk indexes across all nodes\n    shuffled_chunks = [chunk for chunks in chunks_per_node for chunk in chunks]\n\n    return shuffled_chunks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Set the seed for the current epoch\n    torch.manual_seed(seed + current_epoch)\n\n    # Flatten the chunk indexes assigned to each rank\n    flat_chunks_per_ranks = [\n        chunk_idx for chunks_per_rank in chunks_per_ranks for chunk_idx in chunks_per_rank\n    ]\n\n    # Shuffle the chunk indexes\n    shuffled_chunks = torch.randperm(\n        len(flat_chunks_per_ranks), generator=torch.Generator().manual_seed(seed)\n    ).tolist()\n\n    # Return the shuffled chunk indexes\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Flatten the chunks_per_ranks list to get a list of all chunk indexes assigned to all nodes\n    chunks_per_ranks = [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    # Set the seed for the random number generator based on the provided seed and current epoch\n    torch.manual_seed(seed + current_epoch)\n\n    # Shuffle the chunk indexes using the random number generator\n    torch.randperm(len(chunks_per_ranks), generator=torch.Generator().manual_seed(seed + current_epoch))\n\n    # Return the shuffled chunk indexes\n    return chunks_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Initialize the random number generator with a seed that is unique to the current epoch and node\n    rng = torch.Generator()\n    rng.manual_seed(seed + current_epoch * distributed_env.world_size + distributed_env.rank)\n\n    # Shuffle the chunk indexes for each rank\n    shuffled_chunks_per_ranks = []\n    for chunks_per_rank in chunks_per_ranks:\n        shuffled_chunks_per_rank = torch.randperm(len(chunks_per_rank), generator=rng).tolist()\n        shuffled_chunks_per_ranks.append(shuffled_chunks_per_rank)\n\n    # Flatten the shuffled chunk indexes across all nodes\n    shuffled_chunks = [chunks_per_ranks[i][j] for i in range(len(chunks_per_ranks)) for j in shuffled_chunks_per_ranks[i]]\n\n    return shuffled_chunks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Generate a random number generator using the seed and the current epoch\n    rng = torch.Generator()\n    rng.manual_seed(seed + current_epoch)\n\n    # Shuffle the chunk indexes for each rank\n    shuffled_chunks = []\n    for chunks_per_rank in chunks_per_ranks:\n        shuffled_chunks.append(torch.randperm(len(chunks_per_rank), generator=rng).tolist())\n\n    # Flatten the shuffled chunk indexes\n    flattened_shuffled_chunks = []\n    for rank_idx in range(distributed_env.world_size):\n        for shuffled_chunk_idx in shuffled_chunks[rank_idx]:\n            flattened_shuffled_chunks.append(chunks_per_ranks[rank_idx][shuffled_chunk_idx])\n\n    return flattened_shuffled_chunks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the current rank and world size from the distributed environment\n    rank = distributed_env.rank\n    world_size = distributed_env.world_size\n\n    # Create a new seed based on the current epoch and the provided seed\n    new_seed = seed + current_epoch\n\n    # Flatten the chunk indexes assigned to the current rank\n    flattened_chunks = [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    # Shuffle the flattened chunk indexes using the new seed\n    flattened_chunks = shuffle(flattened_chunks, new_seed)\n\n    # Split the shuffled chunk indexes into chunks_per_ranks based on the world size\n    chunks_per_ranks = [\n        flattened_chunks[i::world_size] for i in range(world_size)\n    ]\n\n    # Return the shuffled chunk indexes for the current rank\n    return chunks_per_ranks[rank]\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the number of chunks per node.\n    chunks_per_node = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the total number of chunks.\n    total_chunks = sum(chunks_per_node)\n\n    # Generate a random permutation of chunk indexes.\n    shuffled_chunks = torch.randperm(total_chunks, generator=torch.Generator().manual_seed(seed + current_epoch))\n\n    # Split the permuted chunk indexes into chunks per node.\n    shuffled_chunks_per_node = torch.split(shuffled_chunks, chunks_per_node)\n\n    # Flatten the chunk indexes for each node into a single list.\n    shuffled_chunks_per_node = [chunks.tolist() for chunks in shuffled_chunks_per_node]\n\n    # Flatten the chunk indexes across all nodes into a single list.\n    shuffled_chunks = [chunk for chunks in shuffled_chunks_per_node for chunk in chunks]\n\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Create a list of chunk indexes for each rank\n    chunks_per_ranks = [chunks_per_ranks[rank] for rank in range(distributed_env.world_size)]\n\n    # Compute the total number of chunks\n    total_num_chunks = sum(len(chunks) for chunks in chunks_per_ranks)\n\n    # Create a list of all chunk indexes\n    all_chunks = list(range(total_num_chunks))\n\n    # Shuffle the chunk indexes based on the provided seed and current epoch\n    random.Random(seed + current_epoch).shuffle(all_chunks)\n\n    # Assign the shuffled chunk indexes to each rank\n    shuffled_chunks_per_ranks = [\n        all_chunks[i : i + len(chunks)]\n        for i, chunks in enumerate(chunks_per_ranks)\n        for _ in chunks\n    ]\n\n    # Flatten the list of shuffled chunk indexes\n    shuffled_chunks = [chunk for chunks in shuffled_chunks_per_ranks for chunk in chunks]\n\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Check if the current epoch is divisible by the world size, indicating a new epoch.\n    if current_epoch % distributed_env.world_size == 0:\n        # If the current epoch is divisible by the world size, increment the seed by 1 to ensure a different shuffle for each epoch.\n        seed += 1\n\n    # Create a new random number generator using the seed and the current epoch.\n    rng = torch.Generator()\n    rng.manual_seed(seed)\n\n    # Flatten the list of chunk indexes assigned to each rank.\n    chunks_per_ranks = [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    # Shuffle the flattened chunk indexes using the random number generator.\n    torch.randperm(len(chunks_per_ranks), generator=rng, out=chunks_per_ranks)\n\n    # Return the shuffled chunk indexes.\n    return chunks_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the total number of chunks across all nodes\n    total_chunks = sum(len(chunks) for chunks in chunks_per_ranks)\n\n    # Generate a random seed based on the provided seed and the current epoch\n    random_seed = seed + current_epoch\n\n    # Initialize a list to store the shuffled chunk indexes\n    shuffled_chunks = []\n\n    # Iterate over each node\n    for node_idx in range(distributed_env.num_nodes):\n\n        # Determine the chunk indexes assigned to the current node\n        node_chunks = chunks_per_ranks[node_idx]\n\n        # Shuffle the chunk indexes for the current node based on the random seed\n        node_chunks = random_shuffle(node_chunks, random_seed + node_idx)\n\n        # Append the shuffled chunk indexes to the list\n        shuffled_chunks.extend(node_chunks)\n\n    # Return the flattened list of shuffled chunk indexes\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Compute the total number of chunks across all nodes.\n    total_chunks = sum([len(chunks) for chunks in chunks_per_ranks])\n\n    # Compute the number of chunks per node.\n    chunks_per_node = total_chunks // distributed_env.world_size\n\n    # Compute the number of chunks that will be left over after distributing chunks evenly across nodes.\n    remaining_chunks = total_chunks % distributed_env.world_size\n\n    # Initialize a list to store the shuffled chunk indexes.\n    shuffled_chunks = []\n\n    # Iterate over each node in the distributed environment.\n    for node_rank in range(distributed_env.world_size):\n\n        # Compute the number of chunks to be assigned to the current node.\n        num_chunks_for_node = chunks_per_node + (1 if node_rank < remaining_chunks else 0)\n\n        # Get the chunk indexes assigned to the current node.\n        chunks_for_node = chunks_per_ranks[node_rank]\n\n        # Compute the seed for the shuffle for the current node.\n        node_seed = seed + node_rank + current_epoch\n\n        # Shuffle the chunk indexes for the current node using the computed seed.\n        random.Random(node_seed).shuffle(chunks_for_node)\n\n        # Append the shuffled chunk indexes to the shuffled_chunks list.\n        shuffled_chunks.extend(chunks_for_node[:num_chunks_for_node])\n\n    # Return the flattened shuffled chunk indexes.\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the total number of chunks across all nodes\n    total_num_chunks = sum(len(chunks) for chunks in chunks_per_ranks)\n\n    # Calculate the number of chunks per node\n    num_chunks_per_node = total_num_chunks // distributed_env.world_size\n\n    # Flatten the chunk indexes assigned to each node\n    flattened_chunks = [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    # Calculate the number of chunks in the last node\n    num_chunks_in_last_node = total_num_chunks - (\n        num_chunks_per_node * (distributed_env.world_size - 1)\n    )\n\n    # Calculate the start and end indices for the chunks in the last node\n    start_idx = total_num_chunks - num_chunks_in_last_node\n    end_idx = total_num_chunks\n\n    # Shuffle the chunk indexes for the last node\n    flattened_chunks[start_idx:end_idx] = shuffle_chunks(\n        flattened_chunks[start_idx:end_idx], seed, current_epoch\n    )\n\n    # Shuffle the chunk indexes for all other nodes\n    for i in range(distributed_env.world_size - 1):\n        start_idx = i * num_chunks_per_node\n        end_idx = start_idx + num_chunks_per_node\n        flattened_chunks[start_idx:end_idx] = shuffle_chunks(\n            flattened_chunks[start_idx:end_idx], seed, current_epoch\n        )\n\n    return flattened_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the number of nodes and the world size from the distributed environment\n    num_nodes = distributed_env.num_nodes\n    world_size = distributed_env.world_size\n\n    # Calculate the number of chunks per node\n    chunks_per_node = sum(map(len, chunks_per_ranks)) // num_nodes\n\n    # Initialize a list to store the shuffled chunk indexes for each node\n    shuffled_chunks_per_node = [[] for _ in range(num_nodes)]\n\n    # Initialize a list to store the shuffled chunk indexes for each rank\n    shuffled_chunks_per_rank = [[] for _ in range(world_size)]\n\n    # Initialize a random number generator with the seed and the current epoch\n    rng = torch.Generator()\n    rng.manual_seed(seed + current_epoch)\n\n    # Iterate over the chunks per rank\n    for rank, chunks in enumerate(chunks_per_ranks):\n        # Calculate the node index for the current rank\n        node_index = rank // world_size\n\n        # Shuffle the chunks using the random number generator\n        shuffled_chunks = torch.randperm(len(chunks), generator=rng).tolist()\n\n        # Assign the shuffled chunks to the corresponding node\n        shuffled_chunks_per_node[node_index].extend(shuffled_chunks)\n\n    # Flatten the shuffled chunk indexes for each node into a single list\n    shuffled_chunks_per_node = [\n        chunk for node_chunks in shuffled_chunks_per_node for chunk in node_chunks\n    ]\n\n    # Iterate over the shuffled chunk indexes for each node\n    for node_index, chunk_index in enumerate(shuffled_chunks_per_node):\n        # Calculate the rank index for the current node\n        rank_index = node_index * world_size\n\n        # Assign the chunk index to the corresponding rank\n        shuffled_"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    input_dir = None\n    for i, input in enumerate(inputs):\n        if i == 2:\n            break\n        if not os.path.isfile(input):\n            continue\n        if input_dir is None:\n            input_dir = os.path.dirname(input)\n        elif input_dir != os.path.dirname(input):\n            raise ValueError(f\"Inconsistent file paths: {input_dir} and {os.path.dirname(input)}\")\n\n    if input_dir is not None:\n        if os.path.basename(input_dir) == \"src\":\n            input_dir = os.path.dirname(input_dir)\n        else:\n            input_dir = os.path.join(input_dir, \"..\")\n        input_dir = os.path.abspath(input_dir)\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Check if the first two elements of the input sequence are valid file paths\n    if not (os.path.isfile(inputs[0]) and os.path.isfile(inputs[1])):\n        raise ValueError(\"The first two elements of the input sequence must be valid file paths.\")\n\n    # Resolve the indexed paths\n    indexed_path_1 = _resolve_indexed_path(inputs[0])\n    indexed_path_2 = _resolve_indexed_path(inputs[1])\n\n    # Check if the resolved indexed paths are consistent\n    if indexed_path_1 != indexed_path_2:\n        raise ValueError(\"The resolved indexed paths of the first two elements of the input sequence must be consistent.\")\n\n    # Format the path to include the project root or a specified depth in the file system\n    input_dir = _format_path(indexed_path_1, depth=2)\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Check if the first two elements of the input sequence are valid file paths\n    if isinstance(inputs[0], str) and isinstance(inputs[1], str):\n        # Resolve the indexed paths\n        path_0 = os.path.abspath(inputs[0])\n        path_1 = os.path.abspath(inputs[1])\n\n        # Check if the paths are consistent\n        if path_0 != path_1:\n            raise ValueError(\"Inconsistent file paths found.\")\n\n        # Format the path to include the project root or a specified depth in the file system\n        input_dir = os.path.dirname(path_0)\n        if input_dir.endswith(\"src\"):\n            input_dir = os.path.dirname(input_dir)\n        else:\n            input_dir = os.path.abspath(os.path.join(input_dir, \"..\"))\n\n        return input_dir\n\n    # Return None if no valid file paths are found\n    return None"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Initialize the input directory\n    input_dir = None\n\n    # Check if the first two elements of the inputs are valid file paths\n    if os.path.isfile(inputs[0]):\n        input_dir = os.path.dirname(inputs[0])\n    elif os.path.isfile(inputs[1]):\n        input_dir = os.path.dirname(inputs[1])\n\n    # Raise an error if inconsistent file paths are found\n    if input_dir is not None:\n        if os.path.isfile(inputs[0]) and os.path.dirname(inputs[0]) != input_dir:\n            raise ValueError(f\"Inconsistent file paths found: {inputs[0]} and {inputs[1]}\")\n        if os.path.isfile(inputs[1]) and os.path.dirname(inputs[1]) != input_dir:\n            raise ValueError(f\"Inconsistent file paths found: {inputs[0]} and {inputs[1]}\")\n\n    # Resolve indexed paths\n    if input_dir is not None:\n        input_dir = resolve_indexed_path(input_dir)\n\n    # Format the path to include the project root or a specified depth in the file system\n    if input_dir is not None:\n        input_dir = format_path(input_dir)\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Check if the first two elements of the input sequence are valid file paths\n    if not isinstance(inputs[0], str) or not isinstance(inputs[1], str):\n        return None\n\n    # Extract the file paths from the input sequence\n    file_path_0 = inputs[0]\n    file_path_1 = inputs[1]\n\n    # Check if the file paths are consistent\n    if file_path_0 != file_path_1:\n        raise ValueError(\"Inconsistent file paths\")\n\n    # Resolve the file path to an absolute path\n    file_path = os.path.abspath(file_path_0)\n\n    # Determine the project root directory\n    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n\n    # Format the path to include the project root or a specified depth in the file system\n    if file_path.startswith(project_root):\n        # If the file path is within the project root, return the path relative to the project root\n        return os.path.relpath(file_path, project_root)\n    else:\n        # If the file path is outside the project root, return the path with a specified depth in the file system\n        return os.path.join(\"..\", \"..\", \"..\", os.path.relpath(file_path, os.path.join(project_root, \"..\", \"..\", \"..\")))\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    input_dir = None\n    for i, input_file in enumerate(inputs[:2]):\n        if not os.path.isfile(input_file):\n            continue\n        input_dir = os.path.dirname(os.path.abspath(input_file))\n        if i == 0:\n            break\n        if input_dir != os.path.dirname(os.path.abspath(inputs[i])):\n            raise ValueError(\"Inconsistent file paths\")\n    if input_dir is None:\n        return None\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Check if the first two elements of the input sequence are valid file paths\n    if not (isinstance(inputs[0], str) and isinstance(inputs[1], str)):\n        return None\n\n    # Resolve the first two elements of the input sequence as file paths\n    path_0 = Path(inputs[0]).resolve()\n    path_1 = Path(inputs[1]).resolve()\n\n    # Check if the file paths are consistent\n    if path_0.name == path_1.name:\n        return None\n\n    # Determine the input directory based on the consistency of the file paths\n    if path_0.name == 'input':\n        input_dir = path_0\n    elif path_1.name == 'input':\n        input_dir = path_1\n    else:\n        return None\n\n    # Format the input directory path to include the project root or a specified depth\n    if input_dir.name == 'input':\n        input_dir = input_dir.parent\n\n    return str(input_dir)\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Initialize variables\n    input_dir = None\n    input_dir_set = False\n    input_dir_depth = None\n\n    # Iterate through the first two elements of the input sequence\n    for i, input in enumerate(inputs[:2]):\n        # Check if the input is a valid file path\n        if isinstance(input, str) and os.path.isfile(input):\n            # Extract the directory path from the file path\n            input_dir = os.path.dirname(input)\n            # Set the input directory depth if not already set\n            if not input_dir_depth:\n                input_dir_depth = len(input_dir.split(os.sep))\n            # Check if the input directory depth is consistent with the previous input\n            elif len(input_dir.split(os.sep)) != input_dir_depth:\n                raise ValueError(\n                    f\"Inconsistent input directory depth: {len(input_dir.split(os.sep))} != {input_dir_depth}\"\n                )\n            # Set the input directory flag if not already set\n            if not input_dir_set:\n                input_dir_set = True\n        # Check if the input is a valid directory path\n        elif isinstance(input, str) and os.path.isdir(input):\n            # Set the input directory if not already set\n            if not input_dir:\n                input_dir = input\n            # Set the input directory depth if not already set\n            if not input_dir_depth:\n                input_dir_depth = len(input_dir.split(os.sep))\n            # Check if the input directory depth is consistent with the previous input\n            elif len(input_dir.split(os.sep)) != input_dir_depth:\n                raise ValueError(\n                    f\"Inconsistent input directory depth: {len(input_dir.split(os.sep))} != {input_dir_depth}\"\n                )\n            # Set the input directory flag if not already set\n            if not input_dir_set:\n                input_dir_set = True\n\n    # If an input directory was found, resolve the index"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Check if the inputs are a Sequence\n    if not isinstance(inputs, Sequence):\n        raise TypeError(f\"Inputs must be a Sequence, not {type(inputs)}\")\n\n    # Check if the inputs are not empty\n    if not inputs:\n        raise ValueError(\"Inputs must not be empty\")\n\n    # Check if the first two elements of the inputs are file paths\n    if not isinstance(inputs[0], str) or not isinstance(inputs[1], str):\n        raise ValueError(\"The first two elements of inputs must be file paths\")\n\n    # Check if the first two elements of the inputs are consistent\n    if not inputs[0] == inputs[1]:\n        raise ValueError(\"The first two elements of inputs must be consistent\")\n\n    # Get the absolute path to the input directory\n    input_dir = os.path.abspath(inputs[0])\n\n    # Check if the input directory exists\n    if not os.path.exists(input_dir):\n        raise FileNotFoundError(f\"Input directory {input_dir} does not exist\")\n\n    # Check if the input directory is a directory\n    if not os.path.isdir(input_dir):\n        raise NotADirectoryError(f\"Input directory {input_dir} is not a directory\")\n\n    # Resolve indexed paths in the input directory\n    input_dir = os.path.realpath(input_dir)\n\n    # Check if the input directory is a subdirectory of the project root\n    if not input_dir.startswith(os.path.realpath(os.path.join(os.path.dirname(__file__), \"..\"))):\n        raise ValueError(f\"Input directory {input_dir} is not a subdirectory of the project root\")\n\n    # Format the input directory to include the project root or a specified depth in the file system\n    input_dir = os.path.join(os.path.dirname(__file__), \"..\", input_dir)\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Initialize variables\n    input_dir = None\n    input_dir_depth = None\n    input_dir_depth_set = False\n\n    # Iterate over the first two elements of the input sequence\n    for i, input_ in enumerate(inputs[:2]):\n        # Check if the input is a string or a pathlib.Path object\n        if isinstance(input_, str) or isinstance(input_, pathlib.Path):\n            # Resolve the input path\n            input_path = pathlib.Path(input_).resolve()\n            # Check if the input path is a directory\n            if input_path.is_dir():\n                # Set the input directory\n                input_dir = str(input_path)\n                # Set the input directory depth\n                input_dir_depth = i\n                # Set the input directory depth flag to True\n                input_dir_depth_set = True\n                # Break the loop\n                break\n            # Check if the input path is a file\n            elif input_path.is_file():\n                # Set the input directory to the parent directory of the input file\n                input_dir = str(input_path.parent)\n                # Set the input directory depth\n                input_dir_depth = i\n                # Set the input directory depth flag to True\n                input_dir_depth_set = True\n                # Break the loop\n                break\n\n    # Raise an error if the input directory depth is not set\n    if not input_dir_depth_set:\n        raise ValueError(\"No valid file paths found in the input sequence.\")\n\n    # Format the input directory path\n    input_dir = format_path(input_dir, input_dir_depth)\n\n    # Return the input directory\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Extract file paths from the first two elements of the input sequence\n    file_paths = [\n        os.path.abspath(input) for input in inputs[:2] if isinstance(input, str)\n    ]\n\n    # Check if the extracted file paths are consistent\n    if len(file_paths) == 2 and file_paths[0] != file_paths[1]:\n        raise ValueError(\n            \"Inconsistent file paths: {} and {}\".format(*file_paths)\n        )\n\n    # If only one valid file path is found, use it as the input directory\n    if file_paths:\n        input_dir = file_paths[0]\n    else:\n        return None\n\n    # Resolve indexed paths\n    input_dir = resolve_indexed_path(input_dir)\n\n    # Format the input directory to include the project root or a specified depth in the file system\n    if os.path.isdir(input_dir):\n        input_dir = os.path.abspath(input_dir)\n    else:\n        input_dir = os.path.abspath(os.path.dirname(input_dir))\n\n    # Return the formatted input directory\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Extract file paths from the first two elements of the input sequence\n    file_paths = [input_ for input_ in inputs[:2] if isinstance(input_, (str, Path))]\n\n    # Check if the file paths are consistent\n    if len(file_paths) == 2 and file_paths[0] != file_paths[1]:\n        raise ValueError(f\"Inconsistent file paths: {file_paths[0]}, {file_paths[1]}\")\n\n    # Resolve the file path and extract the parent directory\n    if file_paths:\n        file_path = Path(file_paths[0]).resolve()\n        parent_dir = file_path.parent\n    else:\n        return None\n\n    # Format the input directory path\n    input_dir = str(parent_dir)\n    if input_dir.startswith(str(Path(PROJECT_ROOT))):\n        input_dir = input_dir[len(str(PROJECT_ROOT)) + 1:]\n    else:\n        input_dir = \"/\".join(input_dir.split(os.sep)[-depth:])\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Initialize the input directory as None\n    input_dir = None\n\n    # Iterate over the first two elements of the input sequence\n    for i in range(min(2, len(inputs))):\n\n        # Get the current input\n        input_ = inputs[i]\n\n        # Check if the input is a valid file path\n        if isinstance(input_, str) and os.path.exists(input_):\n\n            # If the input directory is not yet set, set it to the parent directory of the input file\n            if input_dir is None:\n                input_dir = os.path.dirname(input_)\n\n            # If the input directory is already set and it does not match the parent directory of the current input file, raise an error\n            elif input_dir != os.path.dirname(input_):\n                raise ValueError(f\"Inconsistent file paths: {input_dir} and {input_}\")\n\n    # If an input directory was found, return it with the project root or a specified depth in the file system\n    if input_dir is not None:\n        return os.path.abspath(os.path.join(input_dir, \"..\" * (PROJECT_ROOT_DEPTH + 1)))\n\n    # If no input directory was found, return None\n    return None\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    input_dir = None\n\n    # Extract the first two elements of the input sequence\n    input_1 = inputs[0]\n    input_2 = inputs[1]\n\n    # Check if the first two elements are valid file paths\n    if os.path.isfile(input_1):\n        input_dir = os.path.dirname(input_1)\n    elif os.path.isfile(input_2):\n        input_dir = os.path.dirname(input_2)\n\n    # Check if both elements are valid file paths\n    if os.path.isfile(input_1) and os.path.isfile(input_2):\n        # If both elements are file paths, check if they are consistent\n        if os.path.dirname(input_1) != os.path.dirname(input_2):\n            raise ValueError(\"Inconsistent file paths\")\n\n    # Check if input_dir is None\n    if input_dir is None:\n        raise ValueError(\"No valid file paths found\")\n\n    # Resolve the indexed path\n    input_dir = resolve_indexed_path(input_dir)\n\n    # Format the input directory path\n    input_dir = format_path(input_dir)\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Extract the first two elements of the input sequence\n    input_a, input_b = inputs[:2]\n\n    # Check if the first element is a valid file path\n    if isinstance(input_a, str) and os.path.isfile(input_a):\n        # Resolve the file path to an absolute path\n        input_a = os.path.abspath(input_a)\n        # Get the directory containing the file\n        input_dir = os.path.dirname(input_a)\n        # Return the absolute path to the input directory\n        return input_dir\n\n    # Check if the second element is a valid file path\n    elif isinstance(input_b, str) and os.path.isfile(input_b):\n        # Resolve the file path to an absolute path\n        input_b = os.path.abspath(input_b)\n        # Get the directory containing the file\n        input_dir = os.path.dirname(input_b)\n        # Return the absolute path to the input directory\n        return input_dir\n\n    # If no valid file paths are found, return None\n    else:\n        return None\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if not inputs:\n        return None\n\n    if isinstance(inputs[0], str):\n        input_dir = inputs[0]\n    elif isinstance(inputs[0], Path):\n        input_dir = inputs[0].as_posix()\n    else:\n        return None\n\n    if isinstance(inputs[1], str):\n        input_dir2 = inputs[1]\n    elif isinstance(inputs[1], Path):\n        input_dir2 = inputs[1].as_posix()\n    else:\n        return None\n\n    if input_dir != input_dir2:\n        raise ValueError(\n            f\"Inconsistent file paths: {input_dir} and {input_dir2}\")\n\n    if not os.path.isabs(input_dir):\n        input_dir = os.path.join(PROJECT_ROOT, input_dir)\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Initialize variables to track the input directory and whether it has been set\n    input_dir = None\n    input_dir_set = False\n\n    # Iterate over the first two elements of the inputs\n    for input_ in inputs[:2]:\n\n        # Check if the input is a string\n        if isinstance(input_, str):\n\n            # If the input is a string, resolve it to an absolute path and get its parent directory\n            input_ = Path(input_).resolve()\n            input_dir_ = input_.parent\n\n            # If the input directory has not been set yet, set it and mark it as set\n            if not input_dir_set:\n                input_dir = input_dir_\n                input_dir_set = True\n\n            # If the input directory has already been set, check if it matches the current input directory\n            elif input_dir != input_dir_:\n\n                # If the input directory does not match the current input directory, raise an error\n                raise ValueError(\n                    \"Inconsistent input directories found: {} and {}\".format(\n                        input_dir, input_dir_\n                    )\n                )\n\n    # If no valid file paths were found, return None\n    if input_dir is None:\n        return None\n\n    # If a valid file path was found, format the path to include the project root or a specified depth in the file system\n    if depth is None:\n        input_dir = str(input_dir.relative_to(PROJECT_ROOT))\n    else:\n        input_dir = str(input_dir.relative_to(PROJECT_ROOT.parents[depth]))\n\n    # Return the formatted input directory\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Check if the first two elements of the input sequence are file paths\n    if isinstance(inputs[0], str) and isinstance(inputs[1], str):\n        # Check if the first two elements of the input sequence are consistent file paths\n        if inputs[0] != inputs[1]:\n            raise ValueError(\"Inconsistent file paths provided.\")\n        # Resolve the file path to an absolute path\n        file_path = os.path.abspath(inputs[0])\n        # Determine the input directory based on the file path\n        if file_path.endswith(\"project_root\"):\n            # Use the project root as the input directory\n            input_dir = os.path.dirname(os.path.dirname(os.path.dirname(file_path)))\n        else:\n            # Use a specified depth in the file system as the input directory\n            input_dir = os.path.dirname(os.path.dirname(file_path))\n    else:\n        # Return None if no valid file paths are found\n        return None\n\n    # Format the input directory to include the project root or a specified depth in the file system\n    if file_path.endswith(\"project_root\"):\n        input_dir = os.path.join(input_dir, \"project_root\")\n    else:\n        input_dir = os.path.join(input_dir, \"project_root\", \"project_root\")\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Initialize variables to store the input directory and a flag indicating if a valid input directory was found\n    input_dir = None\n    found_input_dir = False\n\n    # Iterate over the first two elements of the inputs\n    for i in range(min(len(inputs), 2)):\n        # Check if the current input is a valid file path\n        if isinstance(inputs[i], str) and os.path.isfile(inputs[i]):\n            # Extract the directory from the file path\n            dir_ = os.path.dirname(inputs[i])\n            # Check if the input directory has already been set\n            if input_dir is None:\n                # Set the input directory to the extracted directory\n                input_dir = dir_\n                # Set the flag indicating that a valid input directory was found\n                found_input_dir = True\n            else:\n                # Check if the extracted directory is consistent with the previously set input directory\n                if dir_ != input_dir:\n                    # Raise an error if the extracted directory is inconsistent with the previously set input directory\n                    raise ValueError(\n                        f\"Inconsistent input directory: {dir_} (expected {input_dir})\"\n                    )\n        # Check if the current input is a dictionary with a 'path' key\n        elif isinstance(inputs[i], dict) and \"path\" in inputs[i]:\n            # Extract the directory from the 'path' value\n            dir_ = os.path.dirname(inputs[i][\"path\"])\n            # Check if the input directory has already been set\n            if input_dir is None:\n                # Set the input directory to the extracted directory\n                input_dir = dir_\n                # Set the flag indicating that a valid input directory was found\n                found_input_dir = True\n            else:\n                # Check if the extracted directory is consistent with the previously set input directory\n                if dir_ != input_dir:\n                    # Raise an error if the extracted directory is inconsistent with the previously set input directory\n                    raise ValueError(\n                        f\"Inconsistent input directory: {dir_} (expected {input_dir})\"\n                    )"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Initialize variables\n    input_dir = None\n    input_dir_depth = 0\n\n    # Extract file paths from inputs\n    for input_ in inputs:\n        if isinstance(input_, str):\n            if not os.path.isfile(input_):\n                raise ValueError(f\"Invalid file path: {input_}\")\n            if input_dir is None:\n                input_dir = os.path.dirname(input_)\n            elif input_dir != os.path.dirname(input_):\n                raise ValueError(f\"Inconsistent file paths: {input_dir} and {os.path.dirname(input_)}\")\n            input_dir_depth = max(input_dir_depth, len(input_.split(os.sep)))\n\n    # Format input directory path\n    if input_dir is not None:\n        input_dir = os.path.abspath(input_dir)\n        if input_dir_depth > 1:\n            input_dir = os.path.join(os.path.dirname(input_dir), os.path.basename(input_dir))\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if enable:\n        try:\n            dns.resolver.set_default_resolver(dns.resolver.Resolver())\n            yield\n        finally:\n            dns.resolver.set_default_resolver(dns.resolver.get_default_resolver())\n    else:\n        try:\n            dns.resolver.set_default_resolver(dns.resolver.get_default_resolver())\n            yield\n        finally:\n            dns.resolver.set_default_resolver(dns.resolver.Resolver())"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            dns.resolver.set_default_resolver(dns.resolver.Resolver(configure=False))\n            dns.resolver.get_default_resolver().nameservers = [\"1.1.1.1\"]\n        yield\n    finally:\n        dns.resolver.set_default_resolver(dns.resolver.Resolver(configure=False))\n        dns.resolver.get_default_resolver().nameservers = [\"8.8.8.8\"]"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            # Set DNS optimization to enabled\n            ssl._create_default_https_context = ssl._create_unverified_context\n        yield\n    finally:\n        # Reset DNS optimization to disabled\n        ssl._create_default_https_context = ssl._create_default_https_context"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            dns.resolver.set_default_lifetime()\n            dns.resolver.set_default_resolver()\n        else:\n            dns.resolver.set_default_lifetime(0)\n            dns.resolver.set_default_resolver()\n        yield\n    finally:\n        dns.resolver.set_default_lifetime(0)\n        dns.resolver.set_default_resolver()"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Set the DNS optimization to the desired state\n    ssl._create_default_https_context = ssl._create_unverified_context if enable else ssl._create_default_https_context\n\n    # Execute the code within the context\n    try:\n        yield\n    # Ensure that DNS optimization is always disabled after the context\n    finally:\n        ssl._create_default_https_context = ssl._create_unverified_context"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if enable:\n        # Set DNS optimization to enabled\n        try:\n            # Attempt to enable DNS optimization\n            pass\n        except Exception as e:\n            # Handle any exceptions that occur during the attempt to enable DNS optimization\n            pass\n    else:\n        # Set DNS optimization to disabled\n        try:\n            # Attempt to disable DNS optimization\n            pass\n        except Exception as e:\n            # Handle any exceptions that occur during the attempt to disable DNS optimization\n            pass\n\n    # Execute the context\n    try:\n        yield\n    finally:\n        # Ensure that DNS optimization is disabled after the context is exited\n        try:\n            # Attempt to disable DNS optimization\n            pass\n        except Exception as e:\n            # Handle any exceptions that occur during the attempt to disable DNS optimization\n            pass"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Importing the necessary modules\n    import dns.resolver\n\n    # Retrieving the current DNS resolver options\n    options = dns.resolver.get_default_resolver().options\n\n    # Setting the DNS optimization option to the specified value\n    options.use_edns_subnet = enable\n\n    # Creating a context manager to execute the code within the context\n    with dns.resolver.get_default_resolver() as resolver:\n        # Executing the code within the context\n        yield\n\n    # Disabling DNS optimization after the context is exited\n    options.use_edns_subnet = False"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Save the current DNS optimization state\n    old_state = dns.get_resolve_conf_state()\n\n    # Enable or disable DNS optimization based on the input parameter\n    if enable:\n        dns.set_resolve_conf_state(True)\n    else:\n        dns.set_resolve_conf_state(False)\n\n    # Yield control to the context block\n    yield\n\n    # Ensure DNS optimization is always disabled after the context block\n    dns.set_resolve_conf_state(old_state)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if enable:\n        dns.resolver.set_default_resolver(dns.resolver.Resolver())\n    else:\n        dns.resolver.set_default_resolver(dns.resolver.Resolver(configure=False))\n    try:\n        yield\n    finally:\n        dns.resolver.set_default_resolver(dns.resolver.Resolver(configure=False))\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    with contextlib.suppress(Exception):\n        dns.resolver.set_default_resolver(dns.resolver.Resolver(configure=False))\n        dns.resolver.get_default_resolver().nameservers = [\"8.8.8.8\"]\n        dns.resolver.get_default_resolver().lifetime = 10.0\n        if enable:\n            dns.resolver.get_default_resolver().cache = dns.resolver.LRUCache()\n            dns.resolver.get_default_resolver().cache.clear()\n            dns.resolver.get_default_resolver().cache.capacity = 10000\n        yield\n    finally:\n        dns.resolver.get_default_resolver().cache = dns.resolver.LRUCache()\n        dns.resolver.get_default_resolver().cache.clear()\n        dns.resolver.get_default_resolver().cache.capacity = 0\n        dns.resolver.get_default_resolver().nameservers = []\n        dns.resolver.get_default_resolver().lifetime = 0.0"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            dns.resolver.set_default_resolver(dns.resolver.Resolver())\n        else:\n            dns.resolver.set_default_resolver(dns.resolver.Resolver(configure=False))\n        yield\n    finally:\n        dns.resolver.set_default_resolver(dns.resolver.Resolver(configure=False))\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        # Attempt to enable or disable DNS optimization based on the input parameter\n        if enable:\n            dns.resolver.set_default_resolver(dns.resolver.Resolver(configure=False))\n        else:\n            dns.resolver.set_default_resolver(dns.resolver.Resolver(configure=True))\n\n        # Execute the code block within the context\n        yield\n\n    finally:\n        # Ensure DNS optimization is always disabled after the context is exited\n        dns.resolver.set_default_resolver(dns.resolver.Resolver(configure=True))\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # If DNS optimization is currently enabled, disable it\n    if dns.resolver.get_default_resolver().use_edns:\n        dns.resolver.get_default_resolver().use_edns = False\n\n    # Set DNS optimization to the desired state\n    dns.resolver.get_default_resolver().use_edns = enable\n\n    # Yield control to the caller\n    yield\n\n    # Ensure DNS optimization is disabled after the context is exited\n    dns.resolver.get_default_resolver().use_edns = False\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            dns.resolver.set_default_resolver(dns.resolver.Resolver(configure=False))\n            dns.resolver.get_default_resolver().nameservers = [\n                \"1.1.1.1\",\n                \"1.0.0.1\",\n                \"8.8.8.8\",\n                \"8.8.4.4\",\n            ]\n        yield\n    finally:\n        dns.resolver.set_default_resolver(dns.resolver.Resolver(configure=False))\n        dns.resolver.get_default_resolver().nameservers = [\n            \"127.0.0.53\"\n        ]\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Import the necessary modules\n    import dns.resolver\n    import contextlib\n\n    # Get the current DNS resolver configuration\n    resolver_config = dns.resolver.get_default_resolver()\n\n    # Create a context manager that temporarily enables or disables DNS optimization\n    with contextlib.ExitStack() as stack:\n        # Push the necessary context managers onto the stack\n        stack.enter_context(dns.resolver.override_system_resolver(resolver_config))\n        stack.enter_context(dns.resolver.Resolver(configure=False))\n\n        # Enable or disable DNS optimization based on the 'enable' parameter\n        dns.resolver.set_default_resolver(resolver_config)\n        if not enable:\n            dns.resolver.reset_default_resolver()\n\n        # Yield control to the caller of the context manager\n        yield\n\n        # Ensure that DNS optimization is always disabled after the context manager exits\n        dns.resolver.reset_default_resolver()\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Initialize the original DNS optimization state\n    original_dns_state = dns.resolver.resolve.restore_state()\n\n    # Set the DNS optimization state based on the 'enable' parameter\n    dns.resolver.resolve.set_default_resolver(dns.resolver.resolve.get_default_resolver())\n\n    # Yield control to the caller of the context manager\n    yield\n\n    # Restore the original DNS optimization state after the context is exited\n    dns.resolver.resolve.restore_state(original_dns_state)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if enable:\n        try:\n            dns.resolver.reset_default_resolver()\n            dns.resolver.resolve(\"example.com\")\n        except dns.exception.DNSException:\n            pass\n\n    try:\n        yield\n    finally:\n        if not enable:\n            try:\n                dns.resolver.reset_default_resolver()\n                dns.resolver.resolve(\"example.com\")\n            except dns.exception.DNSException:\n                pass\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Set DNS optimization to the specified state\n    if enable:\n        dns.resolver.set_default_resolver(dns.resolver.Resolver())\n    else:\n        dns.resolver.set_default_resolver(dns.resolver.Resolver(configure=False))\n\n    # Yield control to the context body\n    yield\n\n    # Disable DNS optimization after the context body is executed\n    dns.resolver.set_default_resolver(dns.resolver.Resolver(configure=False))\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Import the necessary modules\n    import dns.resolver\n    import contextlib\n\n    # Create a resolver object\n    resolver = dns.resolver.Resolver()\n\n    # Check if DNS optimization is enabled or disabled\n    if enable:\n        # If enabled, set the resolver to optimize DNS\n        resolver.nameservers = ['8.8.8.8', '8.8.4.4']\n        resolver.lifetime = 10.0\n    else:\n        # If disabled, set the resolver to not optimize DNS\n        resolver.nameservers = ['127.0.0.1']\n        resolver.lifetime = 10.0\n\n    # Create a context manager to ensure that DNS optimization is always disabled after the context\n    with contextlib.ExitStack() as stack:\n        # Add the resolver to the context\n        stack.enter_context(resolver)\n        # Yield to the context\n        yield\n        # Reset the resolver to not optimize DNS\n        resolver.nameservers = ['127.0.0.1']\n        resolver.lifetime = 10.0\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Import the necessary modules\n    import socket\n    import functools\n\n    def decorator(func):\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n\n            # Get the current DNS configuration\n            dns_config = socket.getaddrinfo('localhost', 80, socket.AF_INET, 0, socket.SOCK_STREAM)\n\n            # Set the DNS configuration to the specified configuration\n            socket.set_default_socket(socket.socket, dns_config[0][4])\n\n            # Execute the function within the context\n            try:\n                return func(*args, **kwargs)\n\n            # Ensure DNS optimization is disabled after the function is executed\n            finally:\n                socket.set_default_socket(socket.socket, dns_config[0][4])\n\n        # Return the wrapper function\n        return wrapper\n\n    # Return the decorator function\n    return decorator\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    world_size = distributed_env.world_size\n    total_items = len(indexes)\n\n    # Calculate the number of items each rank should process\n    num_items_per_rank = total_items // world_size\n    if drop_last:\n        num_items_per_rank = total_items // world_size\n    else:\n        num_items_per_rank = (total_items + world_size - 1) // world_size\n\n    # Assign chunks and their intervals to each rank\n    chunks_per_rank = []\n    intervals_per_rank = []\n    for rank in range(world_size):\n        start = rank * num_items_per_rank\n        end = min(start + num_items_per_rank, total_items)\n        chunks_per_rank.append(indexes[start:end])\n        intervals_per_rank.append(chunk_intervals[start:end])\n\n    return chunks_per_rank, intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    world_size = distributed_env.world_size\n    total_items = len(indexes)\n\n    # Calculate the number of items each rank should process\n    num_items_per_rank = total_items // world_size\n    if drop_last:\n        num_items_per_rank = total_items // world_size\n    else:\n        num_items_per_rank = (total_items + world_size - 1) // world_size\n\n    # Assign chunks and their intervals to each rank\n    chunks_per_rank = []\n    intervals_per_rank = []\n    for rank in range(world_size):\n        start = rank * num_items_per_rank\n        end = min(start + num_items_per_rank, total_items)\n        chunks_per_rank.append(indexes[start:end])\n        intervals_per_rank.append(chunk_intervals[start:end])\n\n    return chunks_per_rank, intervals_per_rank\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    world_size = distributed_env.world_size\n    total_items = len(indexes)\n\n    if drop_last:\n        total_items -= total_items % world_size\n\n    items_per_rank = total_items // world_size\n    remainder = total_items % world_size\n\n    chunks_per_rank = []\n    chunks_intervals_per_rank = []\n\n    for rank in range(world_size):\n        start_index = rank * items_per_rank\n        end_index = (rank + 1) * items_per_rank\n\n        if rank < remainder:\n            start_index += rank\n            end_index += rank + 1\n        else:\n            start_index += remainder\n            end_index += remainder\n\n        chunks_per_rank.append(indexes[start_index:end_index])\n        chunks_intervals_per_rank.append(chunk_intervals[start_index:end_index])\n\n    return chunks_per_rank, chunks_intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    world_size = distributed_env.world_size\n    num_items = len(indexes)\n    num_items_per_rank = num_items // world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank * world_size\n    else:\n        num_items_per_rank = num_items_per_rank + num_items % world_size\n\n    assigned_chunks = []\n    assigned_chunk_intervals = []\n    for rank in range(world_size):\n        start_idx = rank * num_items_per_rank\n        end_idx = start_idx + num_items_per_rank\n        assigned_chunks.append(indexes[start_idx:end_idx])\n        assigned_chunk_intervals.append(chunk_intervals[start_idx:end_idx])\n    return assigned_chunks, assigned_chunk_intervals\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank * distributed_env.world_size\n    else:\n        num_items_per_rank = num_items_per_rank + (\n            len(indexes) - num_items_per_rank * distributed_env.world_size\n        )\n\n    # Assign chunks and intervals to each rank\n    chunks_per_rank = []\n    intervals_per_rank = []\n    for rank in range(distributed_env.world_size):\n        start_index = rank * num_items_per_rank\n        end_index = start_index + num_items_per_rank\n        chunks_per_rank.append(indexes[start_index:end_index])\n        intervals_per_rank.append(chunk_intervals[start_index:end_index])\n\n    return chunks_per_rank, intervals_per_rank\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    total_items = len(indexes)\n    world_size = distributed_env.world_size\n    num_items_per_rank = total_items // world_size\n    remainder = total_items % world_size\n\n    if drop_last:\n        num_items_per_rank = total_items // world_size\n        remainder = total_items % world_size\n    else:\n        num_items_per_rank = math.ceil(total_items / world_size)\n        remainder = total_items % world_size\n\n    # distribute chunks and their intervals to each rank\n    chunks_per_rank = []\n    intervals_per_rank = []\n    for rank in range(world_size):\n        start = rank * num_items_per_rank\n        end = start + num_items_per_rank\n        if rank < remainder:\n            start += rank\n            end += rank + 1\n        else:\n            start += remainder\n            end += remainder\n        chunks_per_rank.append(indexes[start:end])\n        intervals_per_rank.append(chunk_intervals[start:end])\n\n    return chunks_per_rank, intervals_per_rank\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    world_size = distributed_env.world_size\n    total_items = len(indexes)\n    num_items_per_rank = total_items // world_size\n    num_items_per_rank_remainder = total_items % world_size\n    num_items_last_rank = num_items_per_rank + num_items_per_rank_remainder\n\n    if drop_last:\n        num_items_per_rank = num_items_per_rank_remainder\n        num_items_last_rank = num_items_per_rank\n\n    # Assign chunks and their intervals to each rank\n    chunks_per_rank = []\n    intervals_per_rank = []\n    start = 0\n    for rank in range(world_size):\n        if rank == world_size - 1:\n            end = start + num_items_last_rank\n        else:\n            end = start + num_items_per_rank\n        chunks_per_rank.append(indexes[start:end])\n        intervals_per_rank.append(chunk_intervals[start:end])\n        start = end\n\n    return chunks_per_rank, intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    world_size = distributed_env.world_size\n    num_items = len(indexes)\n\n    if drop_last:\n        num_items -= num_items % world_size\n\n    num_items_per_rank = num_items // world_size\n\n    if distributed_env.rank == world_size - 1:\n        num_items_per_rank += num_items % world_size\n\n    # Calculate the start and end indices of the items assigned to this rank\n    start_index = distributed_env.rank * num_items_per_rank\n    end_index = start_index + num_items_per_rank\n\n    # Get the chunk indexes assigned to this rank\n    chunk_indexes = indexes[start_index:end_index]\n\n    # Get the chunk intervals assigned to this rank\n    chunk_intervals_per_rank = chunk_intervals[start_index:end_index]\n\n    return chunk_indexes, chunk_intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items_per_rank = _calculate_num_items_per_rank(\n        distributed_env.world_size, len(indexes), drop_last\n    )\n\n    # Assign chunks and their intervals to each rank\n    assigned_chunks = [\n        indexes[i : i + num_items_per_rank]\n        for i in range(0, len(indexes), num_items_per_rank)\n    ]\n    assigned_chunk_intervals = [\n        chunk_intervals[i : i + num_items_per_rank]\n        for i in range(0, len(chunk_intervals), num_items_per_rank)\n    ]\n\n    return assigned_chunks, assigned_chunk_intervals\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    num_items = len(indexes)\n    world_size = distributed_env.world_size\n    num_items_per_rank = num_items // world_size\n\n    if drop_last:\n        num_items_per_rank = num_items // world_size\n    else:\n        num_items_per_rank = (num_items + world_size - 1) // world_size\n\n    if num_items_per_rank == 0:\n        raise ValueError(\n            \"Cannot distribute {} items to {} ranks with \"\n            \"drop_last={}\".format(num_items, world_size, drop_last)\n        )\n\n    # Split indexes and chunk_intervals based on num_items_per_rank\n    indexes_split = [\n        indexes[i : i + num_items_per_rank]\n        for i in range(0, len(indexes), num_items_per_rank)\n    ]\n    chunk_intervals_split = [\n        chunk_intervals[i : i + num_items_per_rank]\n        for i in range(0, len(chunk_intervals), num_items_per_rank)\n    ]\n\n    # Assign chunks and intervals to each rank\n    chunks_per_rank = [[] for _ in range(world_size)]\n    intervals_per_rank = [[] for _ in range(world_size)]\n\n    for i, (chunk_indexes, chunk_intervals) in enumerate(\n        zip(indexes_split, chunk_intervals_split)\n    ):\n        rank = i % world_size\n        chunks_per_rank[rank].extend(chunk_indexes)\n        intervals_per_rank[rank].extend(chunk_intervals)\n\n    return chunks_per_rank, intervals_per_rank\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the total number of items\n    total_items = len(indexes)\n\n    # Calculate the number of items each rank should process\n    items_per_rank = total_items // distributed_env.world_size\n    if drop_last:\n        items_per_rank = total_items // distributed_env.world_size\n    else:\n        items_per_rank = (total_items + distributed_env.world_size - 1) // distributed_env.world_size\n\n    # Calculate the number of items to drop at the end if needed\n    if drop_last:\n        drop_last_items = total_items % distributed_env.world_size\n    else:\n        drop_last_items = 0\n\n    # Initialize lists to store chunk indexes and intervals for each rank\n    chunk_indexes_per_rank = []\n    chunk_intervals_per_rank = []\n\n    # Assign chunks and intervals to each rank\n    for rank in range(distributed_env.world_size):\n        # Calculate the start and end indexes for this rank\n        start_idx = rank * items_per_rank\n        end_idx = start_idx + items_per_rank\n\n        # Adjust the end index if dropping the last items\n        if drop_last and rank == distributed_env.world_size - 1:\n            end_idx -= drop_last_items\n\n        # Get the chunk indexes and intervals for this rank\n        rank_chunk_indexes = indexes[start_idx:end_idx]\n        rank_chunk_intervals = chunk_intervals[start_idx:end_idx]\n\n        # Append the chunk indexes and intervals to the corresponding lists\n        chunk_indexes_per_rank.append(rank_chunk_indexes)\n        chunk_intervals_per_rank.append(rank_chunk_intervals)\n\n    return chunk_indexes_per_rank, chunk_intervals_per_rank\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    total_items = len(indexes)\n    world_size = distributed_env.world_size\n\n    # Calculate the number of items each rank should process\n    num_items_per_rank = total_items // world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank * world_size\n    else:\n        num_items_per_rank += total_items % world_size\n\n    # Assign chunks and intervals to each rank\n    chunks_per_rank = []\n    intervals_per_rank = []\n    start = 0\n    for rank in range(world_size):\n        end = start + num_items_per_rank\n        chunks_per_rank.append(indexes[start:end])\n        intervals_per_rank.append(chunk_intervals[start:end])\n        start = end\n\n    return chunks_per_rank, intervals_per_rank\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Get the total number of items\n    total_items = len(indexes)\n\n    # Calculate the number of items each rank should process\n    num_items_per_rank = total_items // distributed_env.world_size\n\n    # If the total number of items is not evenly divisible by the world size, distribute the remaining items among the ranks\n    if total_items % distributed_env.world_size != 0:\n        num_items_per_rank += 1\n\n    # If drop_last is True, drop the last items to make the distribution even across all ranks\n    if drop_last:\n        num_items_per_rank = min(num_items_per_rank, total_items)\n\n    # Initialize lists to store the chunk indexes and intervals assigned to each rank\n    chunk_indexes_per_rank = []\n    chunk_intervals_per_rank = []\n\n    # Iterate over the ranks\n    for rank in range(distributed_env.world_size):\n        # Calculate the start and end indices of the chunks assigned to this rank\n        start_idx = rank * num_items_per_rank\n        end_idx = start_idx + num_items_per_rank\n\n        # Extract the chunk indexes and intervals assigned to this rank\n        chunk_indexes_per_rank.append(indexes[start_idx:end_idx])\n        chunk_intervals_per_rank.append(chunk_intervals[start_idx:end_idx])\n\n    return chunk_indexes_per_rank, chunk_intervals_per_rank\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Get the world size from the distributed environment\n    world_size = distributed_env.world_size\n\n    # Calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // world_size\n\n    # If drop_last is True, adjust the number of items per rank to account for the dropped items\n    if drop_last:\n        num_items_per_rank = (len(indexes) - len(indexes) % world_size) // world_size\n\n    # Calculate the start and end index for each rank\n    start_index = distributed_env.rank * num_items_per_rank\n    end_index = start_index + num_items_per_rank\n\n    # If drop_last is True, adjust the end index to account for the dropped items\n    if drop_last:\n        end_index = start_index + num_items_per_rank\n\n    # Get the chunk indexes and intervals for the current rank\n    chunk_indexes = indexes[start_index:end_index]\n    chunk_intervals = chunk_intervals[start_index:end_index]\n\n    # Return the chunk indexes and intervals for the current rank\n    return chunk_indexes, chunk_intervals\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Get the world size from the distributed environment\n    world_size = distributed_env.world_size\n\n    # Calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // world_size\n\n    # If drop_last is True, adjust the number of items per rank to account for the dropped items\n    if drop_last:\n        num_items_per_rank = num_items_per_rank * world_size\n\n    # Initialize lists to store the chunk indexes and intervals assigned to each rank\n    chunk_indexes = []\n    chunk_intervals_per_rank = []\n\n    # Iterate over the ranks\n    for rank in range(world_size):\n        # Calculate the start and end indices for the current rank\n        start_index = rank * num_items_per_rank\n        end_index = start_index + num_items_per_rank\n\n        # If drop_last is True, adjust the end index to account for the dropped items\n        if drop_last:\n            end_index = start_index + num_items_per_rank + len(indexes) % world_size\n\n        # Append the chunk indexes and intervals assigned to the current rank to the respective lists\n        chunk_indexes.append(indexes[start_index:end_index])\n        chunk_intervals_per_rank.append(chunk_intervals[start_index:end_index])\n\n    # Return the chunk indexes and intervals assigned to each rank\n    return chunk_indexes, chunk_intervals_per_rank\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    world_size = distributed_env.world_size\n    total_items = len(indexes)\n    num_items_per_rank = total_items // world_size\n    if drop_last:\n        num_items_per_rank = total_items // world_size\n    else:\n        num_items_per_rank = (total_items + world_size - 1) // world_size\n\n    chunk_indexes_for_rank = []\n    chunk_intervals_for_rank = []\n\n    for rank in range(world_size):\n        start_index = rank * num_items_per_rank\n        end_index = start_index + num_items_per_rank\n        if rank == world_size - 1 and not drop_last:\n            end_index = total_items\n        chunk_indexes_for_rank.append(indexes[start_index:end_index])\n        chunk_intervals_for_rank.append(chunk_intervals[start_index:end_index])\n\n    return chunk_indexes_for_rank, chunk_intervals_for_rank\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank // distributed_env.world_size * distributed_env.world_size\n\n    # Initialize lists to store the assigned chunks and intervals for each rank\n    assigned_chunks = [[] for _ in range(distributed_env.world_size)]\n    assigned_intervals = [[] for _ in range(distributed_env.world_size)]\n\n    # Assign chunks and intervals to each rank based on the distributed environment\n    for rank in range(distributed_env.world_size):\n        start = rank * num_items_per_rank\n        end = start + num_items_per_rank\n        assigned_chunks[rank] = indexes[start:end]\n        assigned_intervals[rank] = chunk_intervals[start:end]\n\n    return assigned_chunks, assigned_intervals\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n\n    # If drop_last is True, adjust the number of items per rank to ensure even distribution\n    if drop_last:\n        num_items_per_rank = num_items_per_rank * distributed_env.world_size\n\n    # Initialize lists to store the chunk indexes and intervals for each rank\n    indexes_for_ranks = []\n    intervals_for_ranks = []\n\n    # Iterate over the ranks\n    for rank in range(distributed_env.world_size):\n        # Calculate the start and end indices for the current rank\n        start_idx = rank * num_items_per_rank\n        end_idx = (rank + 1) * num_items_per_rank\n\n        # If drop_last is True and this is the last rank, adjust the end index to include all remaining items\n        if drop_last and rank == distributed_env.world_size - 1:\n            end_idx = len(indexes)\n\n        # Append the chunk indexes and intervals for the current rank to the corresponding lists\n        indexes_for_ranks.append(indexes[start_idx:end_idx])\n        intervals_for_ranks.append(chunk_intervals[start_idx:end_idx])\n\n    # Return the chunk indexes and intervals for each rank\n    return indexes_for_ranks, intervals_for_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n\n    # If drop_last is True, adjust the number of items per rank to account for the dropped items\n    if drop_last:\n        num_items_per_rank = num_items_per_rank * distributed_env.world_size\n\n    # Initialize a list to store the chunk indexes assigned to each rank\n    indexes_for_each_rank = [[] for _ in range(distributed_env.world_size)]\n\n    # Initialize a list to store the intervals of chunks assigned to each rank\n    intervals_for_each_rank = [[] for _ in range(distributed_env.world_size)]\n\n    # Assign chunks and their intervals to each rank\n    for rank in range(distributed_env.world_size):\n        # Calculate the start and end indices for the current rank\n        start_index = rank * num_items_per_rank\n        end_index = (rank + 1) * num_items_per_rank\n\n        # If drop_last is True and this is the last rank, adjust the end index to include all remaining items\n        if drop_last and rank == distributed_env.world_size - 1:\n            end_index = len(indexes)\n\n        # Assign the chunk indexes and intervals to the current rank\n        indexes_for_each_rank[rank] = indexes[start_index:end_index]\n        intervals_for_each_rank[rank] = chunk_intervals[start_index:end_index]\n\n    # Return the chunk indexes and intervals assigned to each rank\n    return indexes_for_each_rank, intervals_for_each_rank\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    total_items = len(indexes)\n    world_size = distributed_env.world_size\n    num_items_per_rank = total_items // world_size\n    num_items_last_rank = num_items_per_rank + total_items % world_size\n\n    if drop_last:\n        num_items_per_rank = num_items_last_rank\n\n    chunks_assigned_to_rank = []\n    intervals_assigned_to_rank = []\n\n    for rank in range(world_size):\n        if rank == world_size - 1:\n            num_items = num_items_last_rank\n        else:\n            num_items = num_items_per_rank\n\n        start_index = rank * num_items_per_rank\n        end_index = start_index + num_items\n        chunks_assigned_to_rank.append(indexes[start_index:end_index])\n        intervals_assigned_to_rank.append(chunk_intervals[start_index:end_index])\n\n    return chunks_assigned_to_rank, intervals_assigned_to_rank\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n        if self._contains_device:\n            kwargs['device'] = self._device\n        self._fn(item_metadata, output_dir, **kwargs)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n        self._fn(item_metadata, output_dir, **kwargs)\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {\n            'output_dir': output_dir,\n        }\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n\n        self._fn(item_metadata, **kwargs)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {\n            'device': self.device,\n            'is_last': is_last,\n        }\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n        self._fn(item_metadata, output_dir, **kwargs)\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {\n            '_contains_is_last': True,\n            'is_last': is_last,\n            'device': self.device,\n        }\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n        if self._contains_device:\n            kwargs['device'] = self.device\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n        if self._contains_device_info:\n            kwargs['device_info'] = self._device_info\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n        if self._contains_device_info:\n            kwargs['device_info'] = self._device_info\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        # Define a dictionary of keyword arguments to be passed to the transformation function\n        kwargs = {}\n\n        # If the transformation function expects the 'is_last' flag, add it to the keyword arguments\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n\n        # Call the transformation function with the item metadata, output directory, and keyword arguments\n        self._fn(item_metadata, output_dir, **kwargs)\n\n        # If the transformation function does not return a value, return None\n        return None\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {\n            'item_metadata': item_metadata,\n            'output_dir': output_dir\n        }\n\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n\n        if self._contains_device:\n            kwargs['device'] = self._device\n\n        self._fn(**kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            kwargs['device'] = self.device\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n        if self._contains_device:\n            kwargs['device'] = self.device\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] != '404':\n                raise e\n            time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] != '404':\n                raise e\n            time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == \"404\":\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] != '404':\n                raise\n            time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                print(f'Waiting for {obj.geturl()} to exist...')\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                time.sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                print('Waiting for file to exist')\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                print(f\"The file {obj.geturl()} does not exist. Waiting for {sleep_time} seconds before checking again.\")\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                print('The object does not exist.')\n                time.sleep(sleep_time)\n            else:\n                raise e\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    pass"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, Sequence):\n        raise TypeError(f\"inputs must be a Sequence, got {type(inputs)}\")\n\n    if not isinstance(output_dir, str):\n        raise TypeError(f\"output_dir must be a str, got {type(output_dir)}\")\n\n    if weights is not None and not isinstance(weights, list):\n        raise TypeError(f\"weights must be a list, got {type(weights)}\")\n\n    if chunk_size is not None and not isinstance(chunk_size, int):\n        raise TypeError(f\"chunk_size must be an int, got {type(chunk_size)}\")\n\n    if chunk_bytes is not None and not isinstance(chunk_bytes, (int, str)):\n        raise TypeError(f\"chunk_bytes must be an int or str, got {type(chunk_bytes)}\")\n\n    if compression is not None and not isinstance(compression, str):\n        raise TypeError(f\"compression must be a str, got {type(compression)}\")\n\n    if num_workers is not None and not isinstance(num_workers, int):\n        raise TypeError(f\"num_workers must be an int, got {type(num_workers)}\")\n\n    if not isinstance(fast_dev_run, bool):\n        raise TypeError(f\"fast_dev_run must be a bool, got {type(fast_dev_run)}\")\n\n    if num_nodes is not None and not isinstance(num_nodes, int):\n        raise TypeError(f\"num_nodes must be an int, got {type(num_nodes)}\")\n\n    if machine is not None and not isinstance(machine, str):\n        raise TypeError(f\"machine must be a str, got {type(machine)}\")\n\n    if num_downloaders is not None and not isinstance(num_downloaders, int):\n        raise TypeError(f\"num_downloaders must be an int, got {type(num_downloaders)}\")\n\n    if num_uploaders"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single input\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    #"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, Sequence):\n        raise ValueError(\"inputs must be a sequence\")\n\n    if not isinstance(output_dir, str):\n        raise ValueError(\"output_dir must be a string\")\n\n    if weights is not None and not isinstance(weights, List):\n        raise ValueError(\"weights must be a list\")\n\n    if chunk_size is not None and not isinstance(chunk_size, int):\n        raise ValueError(\"chunk_size must be an integer\")\n\n    if chunk_bytes is not None and not isinstance(chunk_bytes, (int, str)):\n        raise ValueError(\"chunk_bytes must be an integer or a string\")\n\n    if compression is not None and not isinstance(compression, str):\n        raise ValueError(\"compression must be a string\")\n\n    if num_workers is not None and not isinstance(num_workers, int):\n        raise ValueError(\"num_workers must be an integer\")\n\n    if not isinstance(fast_dev_run, bool):\n        raise ValueError(\"fast_dev_run must be a boolean\")\n\n    if num_nodes is not None and not isinstance(num_nodes, int):\n        raise ValueError(\"num_nodes must be an integer\")\n\n    if machine is not None and not isinstance(machine, str):\n        raise ValueError(\"machine must be a string\")\n\n    if num_downloaders is not None and not isinstance(num_downloaders, int):\n        raise ValueError(\"num_downloaders must be an integer\")\n\n    if num_uploaders is not None and not isinstance(num_uploaders, int):\n        raise ValueError(\"num_uploaders must be an integer\")\n\n    if not isinstance(reorder_files, bool):\n        raise ValueError(\"reorder_files must be a boolean\")\n\n    if reader is not None and not isinstance(reader, BaseReader):\n        raise ValueError(\"reader must be a BaseReader\")\n\n    if batch_size is not None and not isinstance(batch_size, int):\n        raise ValueError(\"batch_size must"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Create a new instance of the `Optimizer` class.\n    optimizer = Optimizer(\n        fn=fn,\n        inputs=inputs,\n        output_dir=output_dir,\n        weights=weights,\n        chunk_size=chunk_size,\n        chunk_bytes=chunk_bytes,\n        compression=compression,\n        num_workers=num_workers,\n        fast_dev_run=fast_dev_run,\n        num_nodes=num_nodes,\n        machine=machine,\n        num_downloaders=num_downloaders,\n        num_uploaders=num_uploaders,\n        reorder_files=reorder_files,\n        reader=reader,\n        batch_size=batch_size,\n    )\n\n    # Run the optimizer.\n    optimizer.run()\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the inputs are valid\n    if not inputs:\n        raise ValueError(\"Inputs must not be empty\")\n\n    # Check if the output directory is valid\n    if not output_dir:\n        raise ValueError(\"Output directory must be specified\")\n\n    # Check if the number of workers is valid\n    if num_workers is not None and num_workers < 1:\n        raise ValueError(\"Number of workers must be greater than 0\")\n\n    # Check if the number of nodes is valid\n    if num_nodes is not None and num_nodes < 1:\n        raise ValueError(\"Number of nodes must be greater than 0\")\n\n    # Check if the number of downloaders is valid\n    if num_downloaders is not None and num_downloaders < 1:\n        raise ValueError(\"Number of downloaders must be greater than 0\")\n\n    # Check if the number of uploaders is valid\n    if num_uploaders is not None and num_uploaders < 1:\n        raise ValueError(\"Number of uploaders must be greater than 0\")\n\n    # Check if the batch size is valid\n    if batch_size is not None and batch_size < 1:\n        raise ValueError(\"Batch size must be greater than 0\")\n\n    # Check if the reader is valid\n    if reader is not None and not isinstance(reader, BaseReader):\n        raise ValueError(\"Reader must be a subclass of BaseReader\")\n\n    # Check if the chunk size is valid\n    if chunk_size is not None and chunk_size < 1:\n        raise ValueError(\"Chunk size must be greater than 0\")\n\n    # Check if the chunk bytes is valid\n    if chunk_bytes is not None and chunk_bytes < 1:\n        raise ValueError(\"Chunk bytes must be greater than 0\")\n\n    # Check if the compression is valid\n    if compression is not None and compression not in [\"gzip\", \"bz2\", \"lzma\"]:\n        raise ValueError(\"Compression must be one of 'gzip', 'bz2', or 'lzma'\")\n\n    # Check if the weights are valid\n    if weights is not None and len"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the inputs are a list or a dictionary\n    if not isinstance(inputs, (list, dict)):\n        raise TypeError(\"Inputs must be a list or a dictionary\")\n\n    # Check if the output directory is a string\n    if not isinstance(output_dir, str):\n        raise TypeError(\"Output directory must be a string\")\n\n    # Check if the weights are a list\n    if weights is not None and not isinstance(weights, list):\n        raise TypeError(\"Weights must be a list\")\n\n    # Check if the chunk size is an integer\n    if chunk_size is not None and not isinstance(chunk_size, int):\n        raise TypeError(\"Chunk size must be an integer\")\n\n    # Check if the chunk bytes is an integer or a string\n    if chunk_bytes is not None and not isinstance(chunk_bytes, (int, str)):\n        raise TypeError(\"Chunk bytes must be an integer or a string\")\n\n    # Check if the compression is a string\n    if compression is not None and not isinstance(compression, str):\n        raise TypeError(\"Compression must be a string\")\n\n    # Check if the number of workers is an integer\n    if num_workers is not None and not isinstance(num_workers, int):\n        raise TypeError(\"Number of workers must be an integer\")\n\n    # Check if the number of nodes is an integer\n    if num_nodes is not None and not isinstance(num_nodes, int):\n        raise TypeError(\"Number of nodes must be an integer\")\n\n    # Check if the number of downloaders is an integer\n    if num_downloaders is not None and not isinstance(num_downloaders, int):\n        raise TypeError(\"Number of downloaders must be an integer\")\n\n    # Check if the number of uploaders is an integer\n    if num_uploaders is not None and not isinstance(num_uploaders, int):\n        raise TypeError(\"Number of uploaders must be an integer\")\n\n    # Check if the reorder files flag is a boolean\n    if not isinstance(reorder_files, bool):\n        raise Type"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, Sequence):\n        raise ValueError(\n            \"Inputs should be a Sequence of inputs, each of which should contain at least a valid filepath.\"\n        )\n\n    if not isinstance(output_dir, str):\n        raise ValueError(\"Output directory should be a string.\")\n\n    if weights is not None and not isinstance(weights, list):\n        raise ValueError(\"Weights should be a list of integers.\")\n\n    if chunk_size is not None and not isinstance(chunk_size, int):\n        raise ValueError(\"Chunk size should be an integer.\")\n\n    if chunk_bytes is not None and not isinstance(chunk_bytes, (int, str)):\n        raise ValueError(\"Chunk bytes should be an integer or a string.\")\n\n    if compression is not None and not isinstance(compression, str):\n        raise ValueError(\"Compression should be a string.\")\n\n    if num_workers is not None and not isinstance(num_workers, int):\n        raise ValueError(\"Number of workers should be an integer.\")\n\n    if not isinstance(fast_dev_run, bool):\n        raise ValueError(\"Fast dev run should be a boolean.\")\n\n    if num_nodes is not None and not isinstance(num_nodes, int):\n        raise ValueError(\"Number of nodes should be an integer.\")\n\n    if machine is not None and not isinstance(machine, str):\n        raise ValueError(\"Machine should be a string.\")\n\n    if num_downloaders is not None and not isinstance(num_downloaders, int):\n        raise ValueError(\"Number of downloaders should be an integer.\")\n\n    if num_uploaders is not None and not isinstance(num_uploaders, int):\n        raise ValueError(\"Number of uploaders should be an integer.\")\n\n    if not isinstance(reorder_files, bool):\n        raise ValueError(\"Reorder files should be a boolean.\")\n\n    if reader is not None and not isinstance(reader, BaseReader):\n        raise ValueError(\"Reader should be a subclass of BaseReader.\")\n\n    if batch_size is not None and not"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if inputs are a list or a tuple\n    if not isinstance(inputs, (list, tuple)):\n        raise ValueError(\"inputs must be a list or a tuple\")\n\n    # Check if inputs are not empty\n    if not inputs:\n        raise ValueError(\"inputs must not be empty\")\n\n    # Check if output_dir is a string\n    if not isinstance(output_dir, str):\n        raise ValueError(\"output_dir must be a string\")\n\n    # Check if weights is None or a list of integers\n    if weights is not None and not isinstance(weights, list) and not all(\n        isinstance(w, int) for w in weights\n    ):\n        raise ValueError(\"weights must be a list of integers or None\")\n\n    # Check if chunk_size is None or an integer\n    if chunk_size is not None and not isinstance(chunk_size, int):\n        raise ValueError(\"chunk_size must be an integer or None\")\n\n    # Check if chunk_bytes is None, an integer, or a string\n    if chunk_bytes is not None and not isinstance(chunk_bytes, (int, str)):\n        raise ValueError(\"chunk_bytes must be an integer, string, or None\")\n\n    # Check if compression is None or a string\n    if compression is not None and not isinstance(compression, str):\n        raise ValueError(\"compression must be a string or None\")\n\n    # Check if num_workers is None or an integer\n    if num_workers is not None and not isinstance(num_workers, int):\n        raise ValueError(\"num_workers must be an integer or None\")\n\n    # Check if fast_dev_run is a boolean\n    if not isinstance(fast_dev_run, bool):\n        raise ValueError(\"fast_dev_run must be a boolean\")\n\n    # Check if num_nodes is None or an integer\n    if num_nodes is not None and not isinstance(num_nodes, int):\n        raise ValueError(\"num_nodes must be an integer or None\")\n\n    # Check if machine is None or"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Initialize the reader if not provided\n    if reader is None:\n        reader = Reader()\n\n    # Initialize the writer\n    writer = Writer(\n        output_dir=output_dir,\n        compression=compression,\n        reorder_files=reorder_files,\n        batch_size=batch_size,\n    )\n\n    # If fast_dev_run is True, process only a subset of the inputs\n    if fast_dev_run:\n        inputs = inputs[:10]\n\n    # Initialize the chunker\n    chunker = Chunker(\n        chunk_size=chunk_size,\n        chunk_bytes=chunk_bytes,\n        weights=weights,\n        reorder_files=reorder_files,\n    )\n\n    # Initialize the executor\n    executor = Executor(\n        fn=fn,\n        inputs=inputs,\n        chunker=chunker,\n        writer=writer,\n        reader=reader,\n        num_workers=num_workers,\n        num_nodes=num_nodes,\n        machine=machine,\n        num_downloaders=num_downloaders,\n        num_uploaders=num_uploaders,\n    )\n\n    # Run the executor\n    executor.run()"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the inputs are a sequence of valid filepaths\n    if not all(isinstance(input, str) and os.path.exists(input) for input in inputs):\n        raise ValueError(\"Inputs must be a sequence of valid filepaths.\")\n\n    # Check if the output directory exists\n    if not os.path.exists(output_dir):\n        raise ValueError(\"Output directory does not exist.\")\n\n    # Check if the number of workers is valid\n    if num_workers is not None and num_workers < 1:\n        raise ValueError(\"Number of workers must be greater than 0.\")\n\n    # Check if the number of nodes is valid\n    if num_nodes is not None and num_nodes < 1:\n        raise ValueError(\"Number of nodes must be greater than 0.\")\n\n    # Check if the machine type is valid\n    if machine is not None and machine not in [\"cpu\", \"gpu\"]:\n        raise ValueError(\"Machine type must be either 'cpu' or 'gpu'.\")\n\n    # Check if the number of downloaders is valid\n    if num_downloaders is not None and num_downloaders < 1:\n        raise ValueError(\"Number of downloaders must be greater than 0.\")\n\n    # Check if the number of uploaders is valid\n    if num_uploaders is not None and num_uploaders < 1:\n        raise ValueError(\"Number of uploaders must be greater than 0.\")\n\n    # Check if the batch size is valid\n    if batch_size is not None and batch_size < 1:\n        raise ValueError(\"Batch size must be greater than 0.\")\n\n    # Check if the reader is valid\n    if reader is not None and not isinstance(reader, BaseReader):\n        raise ValueError(\"Reader must be an instance of BaseReader.\")\n\n    # Check if the chunk size is valid\n    if chunk_size is not None and chunk_size < 1:\n        raise ValueError(\"Chunk size must be greater than 0.\")\n\n    # Check if the chunk bytes is valid\n    if chunk_bytes is not None and chunk_bytes < 1:\n        raise ValueError(\"Chunk bytes must be greater than 0.\")"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the inputs are valid\n    if not inputs:\n        raise ValueError(\"No inputs provided\")\n\n    # Check if the output directory is valid\n    if not output_dir:\n        raise ValueError(\"Output directory is required\")\n\n    # Check if the chunk size is valid\n    if chunk_size is not None and chunk_size < 1:\n        raise ValueError(\"Chunk size must be greater than 0\")\n\n    # Check if the chunk bytes is valid\n    if chunk_bytes is not None and chunk_bytes < 1:\n        raise ValueError(\"Chunk bytes must be greater than 0\")\n\n    # Check if the compression is valid\n    if compression is not None and compression not in [\"gzip\", \"bzip2\", \"lz4\", \"zstd\"]:\n        raise ValueError(\"Invalid compression algorithm\")\n\n    # Check if the number of workers is valid\n    if num_workers is not None and num_workers < 1:\n        raise ValueError(\"Number of workers must be greater than 0\")\n\n    # Check if the number of nodes is valid\n    if num_nodes is not None and num_nodes < 1:\n        raise ValueError(\"Number of nodes must be greater than 0\")\n\n    # Check if the number of downloaders is valid\n    if num_downloaders is not None and num_downloaders < 1:\n        raise ValueError(\"Number of downloaders must be greater than 0\")\n\n    # Check if the number of uploaders is valid\n    if num_uploaders is not None and num_uploaders < 1:\n        raise ValueError(\"Number of uploaders must be greater than 0\")\n\n    # Check if the batch size is valid\n    if batch_size is not None and batch_size < 1:\n        raise ValueError(\"Batch size must be greater than 0\")\n\n    # Check if the reader is valid\n    if reader is not None and not isinstance(reader, BaseReader):\n        raise ValueError(\"Reader must be an instance of BaseReader\")\n\n    # Check if the fast_dev_run is valid\n    if not isinstance(fast_dev_run, bool):\n        raise ValueError(\"fast_dev"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the input is a single file\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    # Check if the input is a single element\n    if isinstance(inputs, dict):\n        inputs = [inputs]\n\n    # Check if the input is a list of files\n    if isinstance(inputs, list):\n        if len(inputs) > 0 and isinstance(inputs[0], str):\n            # If the input is a list of files, create a list of dictionaries\n            inputs = [{\"filepath\": filepath} for filepath in inputs]\n\n    # Check if the input is a list of dictionaries\n    if isinstance(inputs, list):\n        if len(inputs) > 0 and isinstance(inputs[0], dict):\n            # If the input is a list of dictionaries, create a list of dictionaries\n            inputs = [{\"filepath\": filepath} for filepath in inputs]\n\n    # Check if the input is a dictionary\n    if isinstance(inputs, dict):\n        # If the input is a dictionary, create a list of dictionaries\n        inputs = [inputs]\n\n    # Check if the input is a list of dictionaries\n    if isinstance(inputs, list):\n        if len(inputs) > 0 and isinstance(inputs[0], dict):\n            # If the input is a list of dictionaries, create a list of dictionaries\n            inputs = [{\"filepath\": filepath} for filepath in inputs]\n\n    # Check if the input is a list of dictionaries\n    if isinstance(inputs, list):\n        if len(inputs) > 0 and isinstance(inputs[0], dict):\n            # If the input is a list of dictionaries, create a list of dictionaries\n            inputs = [{\"filepath\": filepath} for filepath in inputs]\n\n    # Check if the input is a list of dictionaries\n    if isinstance(inputs, list):\n        if len(inputs) > 0 and isinstance(inputs[0], dict):\n            # If the input is a list of dictionaries, create a list"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the inputs are a sequence\n    if not isinstance(inputs, Sequence):\n        raise ValueError(f\"inputs must be a Sequence, found {type(inputs)}\")\n\n    # Check if the output directory exists\n    if not os.path.exists(output_dir):\n        raise ValueError(f\"output_dir {output_dir} does not exist\")\n\n    # Check if the reader is valid\n    if reader is not None and not isinstance(reader, BaseReader):\n        raise ValueError(f\"reader must be a BaseReader, found {type(reader)}\")\n\n    # Check if the batch size is valid\n    if batch_size is not None and batch_size <= 0:\n        raise ValueError(f\"batch_size must be positive, found {batch_size}\")\n\n    # Check if the chunk size is valid\n    if chunk_size is not None and chunk_size <= 0:\n        raise ValueError(f\"chunk_size must be positive, found {chunk_size}\")\n\n    # Check if the chunk bytes is valid\n    if chunk_bytes is not None and chunk_bytes <= 0:\n        raise ValueError(f\"chunk_bytes must be positive, found {chunk_bytes}\")\n\n    # Check if the compression is valid\n    if compression is not None and compression not in (\"gzip\", \"zstd\"):\n        raise ValueError(f\"compression must be either 'gzip' or 'zstd', found {compression}\")\n\n    # Check if the num_workers is valid\n    if num_workers is not None and num_workers <= 0:\n        raise ValueError(f\"num_workers must be positive, found {num_workers}\")\n\n    # Check if the num_nodes is valid\n    if num_nodes is not None and num_nodes <= 0:\n        raise ValueError(f\"num_nodes must be positive, found {num_nodes}\")\n\n    # Check if the num_downloaders is valid\n    if num_downloaders is not None and num_downloaders <= 0:\n        raise ValueError(f\"num_downloaders must be"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if num_workers is None:\n        num_workers = min(cpu_count(), 4)\n\n    if num_downloaders is None:\n        num_downloaders = num_workers\n\n    if num_uploaders is None:\n        num_uploaders = num_workers\n\n    if num_workers > 1:\n        if num_nodes is None:\n            num_nodes = num_workers\n\n        if machine is None:\n            machine = \"V100\"\n\n        if num_workers > num_nodes:\n            num_nodes = num_workers\n\n        if num_workers > num_nodes * 8:\n            num_nodes = ceil(num_workers / 8)\n\n        if num_nodes > 1:\n            if num_workers > num_nodes * 8:\n                num_nodes = ceil(num_workers / 8)\n\n            if num_nodes > 1:\n                if num_workers > num_nodes * 8:\n                    num_nodes = ceil(num_workers / 8)\n\n                if num_nodes > 1:\n                    if num_workers > num_nodes * 8:\n                        num_nodes = ceil(num_workers / 8)\n\n                    if num_nodes > 1:\n                        if num_workers > num_nodes * 8:\n                            num_nodes = ceil(num_workers / 8)\n\n                        if num_nodes > 1:\n                            if num_workers > num_nodes * 8:\n                                num_nodes = ceil(num_workers / 8)\n\n                            if num_nodes > 1:\n                                if num_workers > num_nodes * 8:\n                                    num_nodes = ceil(num_workers / 8)\n\n                                if num_nodes > 1:\n                                    if num_workers > num_nodes * 8:\n                                        num_nodes = ceil(num_workers / 8)\n\n                                    if num_nodes > 1:\n                                        if num_workers > num_nodes * 8:\n                                            num_nodes ="}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the inputs are a list or a tuple\n    if not isinstance(inputs, (list, tuple)):\n        raise ValueError(\"inputs must be a list or a tuple\")\n\n    # Check if the inputs are not empty\n    if not inputs:\n        raise ValueError(\"inputs must not be empty\")\n\n    # Check if the output directory exists\n    if not os.path.exists(output_dir):\n        raise ValueError(f\"output_dir {output_dir} does not exist\")\n\n    # Check if the weights are valid\n    if weights is not None:\n        if len(weights) != len(inputs):\n            raise ValueError(\n                f\"weights must have the same length as inputs ({len(weights)} != {len(inputs)})\"\n            )\n        if not all(isinstance(w, int) for w in weights):\n            raise ValueError(\"weights must be integers\")\n\n    # Check if the chunk size is valid\n    if chunk_size is not None:\n        if not isinstance(chunk_size, int):\n            raise ValueError(\"chunk_size must be an integer\")\n        if chunk_size <= 0:\n            raise ValueError(\"chunk_size must be positive\")\n\n    # Check if the chunk bytes is valid\n    if chunk_bytes is not None:\n        if isinstance(chunk_bytes, int):\n            if chunk_bytes <= 0:\n                raise ValueError(\"chunk_bytes must be positive\")\n        elif isinstance(chunk_bytes, str):\n            try:\n                chunk_bytes = parse_size(chunk_bytes)\n            except ValueError:\n                raise ValueError(f\"Invalid chunk_bytes value: {chunk_bytes}\")\n        else:\n            raise ValueError(\"chunk_bytes must be an integer or a string\")\n\n    # Check if the compression is valid\n    if compression is not None:\n        if compression not in [\"gzip\", \"lz4\", \"zstd\"]:\n            raise ValueError(\"compression must be one of ['gzip', 'lz4', 'zstd']\")\n\n    #"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if chunk_size is None and chunk_bytes is None:\n        raise ValueError(\"Either chunk_size or chunk_bytes must be specified\")\n\n    if num_workers is None:\n        num_workers = min(32, os.cpu_count() or 1)\n\n    if num_downloaders is None:\n        num_downloaders = num_workers\n\n    if num_uploaders is None:\n        num_uploaders = num_workers\n\n    if fast_dev_run:\n        inputs = inputs[:10]\n\n    if reader is None:\n        reader = DefaultReader()\n\n    if batch_size is None:\n        batch_size = len(inputs)\n\n    if compression is None:\n        compression = \"lz4\"\n\n    if chunk_bytes is not None:\n        chunk_bytes = parse_bytes(chunk_bytes)\n\n    if weights is None:\n        weights = [1] * len(inputs)\n\n    if num_nodes is not None:\n        if machine is None:\n            raise ValueError(\"machine must be specified if num_nodes is specified\")\n        if num_workers is None:\n            raise ValueError(\"num_workers must be specified if num_nodes is specified\")\n        if num_downloaders is None:\n            raise ValueError(\"num_downloaders must be specified if num_nodes is specified\")\n        if num_uploaders is None:\n            raise ValueError(\"num_uploaders must be specified if num_nodes is specified\")\n\n        if machine == \"v100\":\n            machine = \"V100\"\n\n        if machine == \"a100\":\n            machine = \"A100\"\n\n        if machine == \"a10g\":\n            machine = \"A10G\"\n\n        if machine == \"v100-32gb\":\n            machine = \"V100-32GB\"\n\n        if machine == \"a100-40gb\":\n            machine = \"A100-40GB\"\n\n        if machine == \"a100-80gb\":\n           "}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Initialize the reader if not provided\n    if reader is None:\n        reader = DefaultReader()\n\n    # Get the number of workers\n    num_workers = num_workers or get_num_workers()\n\n    # Get the number of downloaders per worker\n    num_downloaders = num_downloaders or get_num_downloaders()\n\n    # Get the number of uploaders per worker\n    num_uploaders = num_uploaders or get_num_uploaders()\n\n    # Get the number of nodes\n    num_nodes = num_nodes or get_num_nodes()\n\n    # Get the machine type\n    machine = machine or get_machine()\n\n    # Get the number of GPUs\n    num_gpus = get_num_gpus()\n\n    # Get the number of CPUs\n    num_cpus = get_num_cpus()\n\n    # Get the total number of workers\n    total_workers = num_workers * num_nodes\n\n    # Get the total number of downloaders\n    total_downloaders = num_downloaders * num_workers * num_nodes\n\n    # Get the total number of uploaders\n    total_uploaders = num_uploaders * num_workers * num_nodes\n\n    # Get the total number of workers and uploaders\n    total_workers_and_uploaders = total_workers + total_uploaders\n\n    # Get the total number of downloaders and workers\n    total_downloaders_and_workers = total_downloaders + total_workers\n\n    # Check if the number of workers is greater than 1\n    if num_workers > 1:\n        # Check if the number of workers is greater than the number of inputs\n        if num_workers > len(inputs):\n            # Set the number of workers to the number of inputs\n            num_workers = len(inputs)\n\n        # Check if the number of workers is greater than the number of nodes\n        if num_workers > num_nodes:\n            # Set the number of workers to the number of nodes\n            num_workers = num_nodes\n\n        # Check if the number"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Get the number of workers\n    num_workers = get_num_workers(num_workers)\n\n    # Check if the number of inputs is greater than the number of workers\n    if len(inputs) > num_workers:\n        # If so, use the default chunk size\n        chunk_size = chunk_size or DEFAULT_CHUNK_SIZE\n    else:\n        # Otherwise, set the chunk size to the number of inputs\n        chunk_size = len(inputs)\n\n    # Get the chunk bytes\n    chunk_bytes = get_chunk_bytes(chunk_bytes)\n\n    # Get the compression\n    compression = get_compression(compression)\n\n    # Get the reader\n    reader = reader or get_reader(inputs[0])\n\n    # Get the batch size\n    batch_size = batch_size or get_batch_size(inputs[0])\n\n    # Get the number of downloaders and uploaders\n    num_downloaders, num_uploaders = get_num_downloaders_and_uploaders(\n        num_downloaders, num_uploaders\n    )\n\n    # Get the chunk size\n    chunk_size = get_chunk_size(chunk_size, inputs[0])\n\n    # Get the number of chunks\n    num_chunks = get_num_chunks(inputs, chunk_size, batch_size)\n\n    # Get the number of chunks per worker\n    num_chunks_per_worker = get_num_chunks_per_worker(num_chunks, num_workers)\n\n    # Get the chunk indices\n    chunk_indices = get_chunk_indices(num_chunks, num_workers)\n\n    # Get the chunk sizes\n    chunk_sizes = get_chunk_sizes(num_chunks, num_chunks_per_worker)\n\n    # Get the chunk indices\n    chunk_indices = get_chunk_indices(num_chunks, num_workers)\n\n    # Get the chunk sizes\n    chunk_sizes = get_chunk_sizes(num_"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the inputs are a list of strings or a list of dictionaries\n    if isinstance(inputs[0], str):\n        # If the inputs are a list of strings, create a list of dictionaries with the filepath as the value\n        inputs = [{\"filepath\": filepath} for filepath in inputs]\n\n    # Create a list of chunks from the inputs\n    chunks = chunk_inputs(\n        inputs=inputs,\n        chunk_size=chunk_size,\n        chunk_bytes=chunk_bytes,\n        reorder_files=reorder_files,\n        batch_size=batch_size,\n    )\n\n    # Set the number of workers to use for processing\n    if num_workers is None:\n        num_workers = max(1, len(chunks))\n\n    # Set the number of downloaders and uploaders per worker\n    num_downloaders = num_downloaders or 0\n    num_uploaders = num_uploaders or 0\n\n    # Set the chunk size to use for processing\n    chunk_size = chunk_size or 1\n\n    # Create a list of functions to execute over each chunk\n    map_fn = partial(\n        map_chunk,\n        fn=fn,\n        output_dir=output_dir,\n        compression=compression,\n        reader=reader,\n        chunk_size=chunk_size,\n    )\n\n    # Create a list of weights to use for balancing work among workers\n    weights = weights or [1] * len(chunks)\n\n    # Create a list of arguments to pass to each worker\n    args = [\n        (\n            chunk,\n            weights[i],\n            num_downloaders,\n            num_uploaders,\n            map_fn,\n            i,\n            num_workers,\n        )\n        for i, chunk in enumerate(chunks)\n    ]\n\n    # If the fast_dev_run flag is set, process only a subset of the inputs\n    if fast_dev_run:\n        args = args[:1]\n\n    # Set the number of workers to use for processing\n    num_"}
{"namespace": "litdata.processing.functions.map", "completion": "    pass\n\n"}
{"namespace": "litdata.processing.functions.map", "completion": "    pass\n\n"}
{"namespace": "litdata.processing.functions.map", "completion": "    pass\n\n"}
{"namespace": "litdata.processing.functions.map", "completion": "    pass"}
{"namespace": "litdata.processing.functions.map", "completion": "    pass\n\n"}
{"namespace": "litdata.processing.functions.map", "completion": "    pass"}
{"namespace": "litdata.processing.functions.map", "completion": "    pass\n\n"}
{"namespace": "litdata.processing.functions.map", "completion": "    pass"}
{"namespace": "litdata.processing.functions.map", "completion": "    pass\n\n"}
{"namespace": "litdata.processing.functions.map", "completion": "    pass\n\n"}
{"namespace": "litdata.processing.functions.map", "completion": "    pass\n\n"}
{"namespace": "litdata.processing.functions.map", "completion": "    ...\n\n"}
{"namespace": "litdata.processing.functions.map", "completion": "    # Check if the inputs are a sequence\n    if not isinstance(inputs, Sequence):\n        raise ValueError(f\"inputs must be a Sequence, got {type(inputs)}\")\n\n    # Check if the output_dir is a string or a Dir object\n    if not isinstance(output_dir, (str, Dir)):\n        raise ValueError(f\"output_dir must be a string or a Dir object, got {type(output_dir)}\")\n\n    # Check if the weights are provided and are a list of integers\n    if weights is not None and not (isinstance(weights, list) and all(isinstance(w, int) for w in weights)):\n        raise ValueError(f\"weights must be a list of integers, got {weights}\")\n\n    # Check if the num_workers is a positive integer or None\n    if num_workers is not None and not (isinstance(num_workers, int) and num_workers > 0):\n        raise ValueError(f\"num_workers must be a positive integer or None, got {num_workers}\")\n\n    # Check if the fast_dev_run is a boolean or a positive integer\n    if not (isinstance(fast_dev_run, bool) or (isinstance(fast_dev_run, int) and fast_dev_run > 0)):\n        raise ValueError(f\"fast_dev_run must be a boolean or a positive integer, got {fast_dev_run}\")\n\n    # Check if the num_nodes is a positive integer or None\n    if num_nodes is not None and not (isinstance(num_nodes, int) and num_nodes > 0):\n        raise ValueError(f\"num_nodes must be a positive integer or None, got {num_nodes}\")\n\n    # Check if the machine is a string or None\n    if machine is not None and not isinstance(machine, str):\n        raise ValueError(f\"machine must be a string or None, got {machine}\")\n\n    # Check if the num_downloaders is a positive integer or None\n    if num_downloaders is not None and not (is"}
{"namespace": "litdata.processing.functions.map", "completion": "    # Check if inputs is a Sequence\n    if not isinstance(inputs, Sequence):\n        raise ValueError(\n            f\"Inputs must be a Sequence, but got {type(inputs)}.\"\n        )\n\n    # Check if output_dir is a string or Dir\n    if not isinstance(output_dir, (str, Dir)):\n        raise ValueError(\n            f\"Output directory must be a string or Dir object, but got {type(output_dir)}.\"\n        )\n\n    # Check if weights is a list of integers\n    if weights is not None and not all(isinstance(w, int) for w in weights):\n        raise ValueError(\n            f\"Weights must be a list of integers, but got {weights}.\"\n        )\n\n    # Check if num_workers is an integer\n    if num_workers is not None and not isinstance(num_workers, int):\n        raise ValueError(\n            f\"Number of workers must be an integer, but got {num_workers}.\"\n        )\n\n    # Check if fast_dev_run is a boolean or positive integer\n    if not isinstance(fast_dev_run, (bool, int)) or (\n        isinstance(fast_dev_run, int) and fast_dev_run <= 0\n    ):\n        raise ValueError(\n            f\"Fast dev run must be a boolean or a positive integer, but got {fast_dev_run}.\"\n        )\n\n    # Check if num_nodes is an integer\n    if num_nodes is not None and not isinstance(num_nodes, int):\n        raise ValueError(\n            f\"Number of nodes must be an integer, but got {num_nodes}.\"\n        )\n\n    # Check if machine is a string\n    if machine is not None and not isinstance(machine, str):\n        raise ValueError(\n            f\"Machine type must be a string, but got {machine}.\"\n        )\n\n    # Check if num_downloaders is an integer\n    if num_downloaders is not None and not isinstance(num_downloaders, int):\n        raise ValueError(\n           "}
{"namespace": "litdata.processing.functions.map", "completion": "    # Check if the output directory is empty\n    if error_when_not_empty and len(output_dir) > 0:\n        raise ValueError(f\"Output directory {output_dir} is not empty\")\n\n    # Check if the output directory is a Dir object\n    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    # Create a reader if not provided\n    if reader is None:\n        reader = get_reader(inputs)\n\n    # Check if the reader is a BaseReader object\n    if not isinstance(reader, BaseReader):\n        raise ValueError(f\"Reader {reader} is not a BaseReader object\")\n\n    # Check if the reader has a read_batch method\n    if not hasattr(reader, \"read_batch\"):\n        raise ValueError(f\"Reader {reader} does not have a read_batch method\")\n\n    # Check if the reader has a read_batch method that returns a list\n    if not isinstance(reader.read_batch(inputs[0]), list):\n        raise ValueError(f\"Reader {reader} does not return a list\")\n\n    # Check if the reader has a read_batch method that returns a list of strings\n    if not isinstance(reader.read_batch(inputs[0])[0], str):\n        raise ValueError(f\"Reader {reader} does not return a list of strings\")\n\n    # Check if the reader has a read_batch method that returns a list of strings of the same length as the input\n    if len(reader.read_batch(inputs[0])) != len(inputs):\n        raise ValueError(f\"Reader {reader} does not return a list of strings of the same length as the input\")\n\n    # Check if the reader has a read_batch method that returns a list of strings of the same length as the input\n    if len(reader.read_batch(inputs[0])) != len(inputs):\n        raise ValueError(f\"Reader {reader} does not return a list of strings of the same length as the input\")\n\n    # Check if the reader has a read_batch method that returns a"}
{"namespace": "litdata.processing.functions.map", "completion": "    if num_workers is None:\n        num_workers = get_num_workers()\n\n    if num_nodes is None:\n        num_nodes = get_num_nodes()\n\n    if machine is None:\n        machine = get_machine()\n\n    if num_downloaders is None:\n        num_downloaders = get_num_downloaders()\n\n    if num_uploaders is None:\n        num_uploaders = get_num_uploaders()\n\n    if reader is None:\n        reader = get_reader()\n\n    if batch_size is None:\n        batch_size = get_batch_size()\n\n    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    if output_dir.exists() and error_when_not_empty:\n        raise ValueError(f\"Output directory {output_dir} is not empty\")\n\n    if isinstance(fast_dev_run, bool):\n        fast_dev_run = int(fast_dev_run)\n\n    if fast_dev_run > 0:\n        inputs = inputs[:fast_dev_run]\n\n    if weights is None:\n        weights = [1] * len(inputs)\n\n    if reorder_files:\n        inputs = reorder_files_by_size(inputs)\n\n    if num_nodes > 1:\n        # TODO: Implement remote execution\n        raise NotImplementedError(\"Remote execution is not yet implemented\")\n    else:\n        local_map(\n            fn=fn,\n            inputs=inputs,\n            output_dir=output_dir,\n            weights=weights,\n            num_workers=num_workers,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reader=reader,\n            batch_size=batch_size,\n        )\n\n"}
{"namespace": "litdata.processing.functions.map", "completion": "    # Check if the output directory is empty\n    if error_when_not_empty and len(output_dir) > 0:\n        raise ValueError(f\"Output directory {output_dir} is not empty\")\n\n    # Get the number of workers\n    if num_workers is None:\n        num_workers = get_num_workers()\n\n    # Create a list of input paths\n    input_paths = [i.path if isinstance(i, Dir) else i for i in inputs]\n\n    # Check if the input paths are valid\n    if not all(isinstance(i, str) for i in input_paths):\n        raise ValueError(\"All inputs must be strings\")\n\n    # Check if the output directory is a valid Dir object\n    if not isinstance(output_dir, Dir):\n        raise ValueError(\"Output directory must be a Dir object\")\n\n    # Check if the output directory is empty\n    if len(output_dir) > 0:\n        raise ValueError(f\"Output directory {output_dir} is not empty\")\n\n    # Check if the weights are valid\n    if weights is not None:\n        if len(weights) != len(inputs):\n            raise ValueError(\"Weights must be the same length as inputs\")\n        if not all(isinstance(w, int) for w in weights):\n            raise ValueError(\"Weights must be integers\")\n        if any(w < 0 for w in weights):\n            raise ValueError(\"Weights must be non-negative\")\n\n    # Check if the number of workers is valid\n    if num_workers < 0:\n        raise ValueError(\"Number of workers must be non-negative\")\n\n    # Check if the fast_dev_run flag is valid\n    if isinstance(fast_dev_run, bool):\n        if fast_dev_run:\n            num_workers = 1\n    elif isinstance(fast_dev_run, int):\n        if fast_dev_run < 0:\n            raise ValueError(\"fast_dev_run must be non-negative\")\n        num_workers = min(num_workers, fast_dev_run)\n    else:"}
{"namespace": "litdata.processing.functions.map", "completion": "    # Setup\n    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"output_dir must be a string or a Dir object, not {type(output_dir)}\")\n    if error_when_not_empty and len(output_dir) > 0:\n        raise ValueError(f\"Output directory {output_dir} is not empty\")\n    if reader is None:\n        reader = BaseReader()\n    if num_workers is None:\n        num_workers = num_workers_or_default()\n    if isinstance(fast_dev_run, int):\n        fast_dev_run = fast_dev_run > 0\n    if fast_dev_run:\n        if isinstance(fast_dev_run, int):\n            fast_dev_run = min(fast_dev_run, len(inputs))\n        else:\n            fast_dev_run = len(inputs)\n    if isinstance(fast_dev_run, int):\n        inputs = inputs[:fast_dev_run]\n    if num_downloaders is None:\n        num_downloaders = num_downloaders_or_default()\n    if num_uploaders is None:\n        num_uploaders = num_uploaders_or_default()\n    if num_nodes is None:\n        num_nodes = num_nodes_or_default()\n    if machine is None:\n        machine = machine_or_default()\n    if num_workers > 1 and machine == \"local\":\n        raise ValueError(\"num_workers > 1 requires a remote machine\")\n    if num_workers > 1 and num_nodes > 1:\n        raise ValueError(\"num_workers > 1 requires num_nodes = 1\")\n    if num_workers > 1 and num_downloaders > 1:\n        raise ValueError(\"num_workers > 1 requires num_downloaders = 1\")\n    if num_workers > 1 and num_uploaders > 1:\n        raise ValueError(\"num_workers > 1 requires num_uploaders = 1\")"}
{"namespace": "litdata.processing.functions.map", "completion": "    # Check if the output directory is empty\n    if error_when_not_empty and len(output_dir) > 0:\n        raise ValueError(f\"Output directory {output_dir} is not empty\")\n\n    # Set the number of workers\n    num_workers = num_workers or num_nodes or 1\n\n    # Set the reader\n    reader = reader or default_reader()\n\n    # Set the batch size\n    batch_size = batch_size or 1\n\n    # Set the number of downloaders and uploaders\n    num_downloaders = num_downloaders or 1\n    num_uploaders = num_uploaders or 1\n\n    # Set the fast_dev_run flag\n    if fast_dev_run is True:\n        fast_dev_run = 1\n\n    # Get the list of input files\n    files = get_files(inputs, reader)\n\n    # Check if the output directory is empty\n    if error_when_not_empty and len(output_dir) > 0:\n        raise ValueError(f\"Output directory {output_dir} is not empty\")\n\n    # Reorder the files by file size\n    if reorder_files:\n        files = reorder_files_by_size(files)\n\n    # Split the files into batches\n    batches = split_into_batches(files, batch_size)\n\n    # Set the number of workers\n    num_workers = num_workers or num_nodes or 1\n\n    # Set the fast_dev_run flag\n    if fast_dev_run is True:\n        fast_dev_run = 1\n\n    # Set the number of downloaders and uploaders\n    num_downloaders = num_downloaders or 1\n    num_uploaders = num_uploaders or 1\n\n    # Set the batch size\n    batch_size = batch_size or 1\n\n    # Set the weights\n    if weights is None:\n        weights = [1] * len(batches)\n\n    # Create a list of tasks\n    tasks = []\n    for i, batch in enumerate(batches):\n        task"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n    output_dir.mkdir(exist_ok=True, parents=True)\n    if not error_when_not_empty:\n        output_dir.rm(recursive=True, force=True)\n    if not output_dir.exists():\n        output_dir.mkdir(parents=True)\n    if len(output_dir.ls()) > 0:\n        raise Exception(f\"{output_dir} is not empty.\")\n\n    if num_workers is None:\n        num_workers = os.cpu_count()\n\n    if isinstance(fast_dev_run, bool) and fast_dev_run:\n        fast_dev_run = 1\n\n    if fast_dev_run is not None and fast_dev_run > 0:\n        if isinstance(inputs, list):\n            inputs = inputs[:fast_dev_run]\n        elif isinstance(inputs, IterableDataset):\n            inputs = islice(inputs, fast_dev_run)\n        else:\n            raise Exception(f\"Invalid inputs type for fast_dev_run: {type(inputs)}\")\n\n    if num_nodes is not None:\n        assert machine is not None, \"machine must be specified if num_nodes is specified\"\n        assert num_workers is None, \"num_workers must be None if num_nodes is specified\"\n        num_workers = num_nodes * os.cpu_count()\n\n    if num_workers > 1:\n        if num_nodes is not None:\n            assert machine is not None, \"machine must be specified if num_nodes is specified\"\n            num_workers = num_nodes * os.cpu_count()\n        if num_downloaders is None:\n            num_downloaders = num_workers\n        if num_uploaders is None:\n            num_uploaders = num_workers\n        if num_downloaders > 0:\n            downloader = Downloader(num_downloaders, output_dir)\n        if num_uploaders > 0:\n            uploader ="}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        try:\n            task = queue_in.get()\n            if task is None:\n                break\n            index, files = task\n            for file in files:\n                if not os.path.exists(os.path.join(cache_dir, file)):\n                    input_dir.download(file, cache_dir)\n            queue_out.put(index)\n        except Exception as e:\n            print(f\"Error in download_data_target: {e}\")\n            queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        task = queue_in.get()\n        if task is None:\n            break\n        idx, file_paths = task\n        for file_path in file_paths:\n            file_name = os.path.basename(file_path)\n            cache_file_path = os.path.join(cache_dir, file_name)\n            if not os.path.exists(cache_file_path):\n                input_file_path = os.path.join(input_dir.path, file_name)\n                shutil.copyfile(input_file_path, cache_file_path)\n        queue_out.put(idx)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        task = queue_in.get()\n        if task is None:\n            break\n        idx, files = task\n        for file in files:\n            if input_dir.protocol == 'local':\n                src_path = input_dir.path + file\n            else:\n                src_path = input_dir.url + file\n            dst_path = cache_dir + file\n            if not os.path.exists(dst_path):\n                download_file(src_path, dst_path)\n        queue_out.put(idx)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        task = queue_in.get()\n        if task is None:\n            break\n        index, file_paths = task\n        for file_path in file_paths:\n            if not os.path.exists(os.path.join(cache_dir, file_path)):\n                input_dir.download_file(file_path, cache_dir)\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        # Fetch a download task from the input queue\n        task = queue_in.get()\n\n        # If the task is None, it means there are no more tasks to process, so break out of the loop\n        if task is None:\n            break\n\n        # Extract the index and file paths from the task\n        index, files = task\n\n        # Download each file that is not already downloaded\n        for file in files:\n            file_path = os.path.join(cache_dir, file)\n            if not os.path.exists(file_path):\n                # Get the source path or URL for the file\n                source_path = input_dir.get_path(file)\n\n                # Download the file using the input directory's download method\n                input_dir.download(source_path, file_path)\n\n        # Put the index of the completed task into the output queue\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Import libraries\n    import os\n    import shutil\n\n    # Initialize variables\n    file_paths = None\n\n    # Loop through tasks in input queue\n    while True:\n        # Get next task from input queue\n        task = queue_in.get()\n        # If task is None, exit loop\n        if task is None:\n            break\n        # Get index and file paths from task\n        index, file_paths = task\n        # Check if files are already downloaded\n        for file_path in file_paths:\n            # Get file name from file path\n            file_name = os.path.basename(file_path)\n            # Check if file already exists in cache directory\n            if not os.path.exists(os.path.join(cache_dir, file_name)):\n                # Download file to cache directory\n                input_dir.download_file(file_path, os.path.join(cache_dir, file_name))\n        # Put index into output queue to signal completion\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # The function continuously fetches download tasks from the input queue, checks if the files are already downloaded, downloads missing files, and then signals completion by putting the task index into the output queue.\n    while True:\n        # Fetch the next download task from the input queue\n        task = queue_in.get()\n        # If the task is None, it means the queue has been marked as done and the function should exit\n        if task is None:\n            break\n        # Get the index and file paths from the task\n        index, paths = task\n        # Check if the files are already downloaded\n        for path in paths:\n            # If the file is not already downloaded, download it\n            if not os.path.exists(os.path.join(cache_dir, path)):\n                # Get the source path or URL for the file\n                src = input_dir.get_path(path)\n                # Download the file to the cache directory\n                input_dir.download(src, os.path.join(cache_dir, path))\n        # Signal completion of the download task by putting the index into the output queue\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        try:\n            task = queue_in.get()\n            if task is None:\n                queue_in.task_done()\n                break\n            index, files = task\n            for file in files:\n                source = os.path.join(input_dir.path, file)\n                destination = os.path.join(cache_dir, file)\n                if not os.path.exists(destination):\n                    os.makedirs(os.path.dirname(destination), exist_ok=True)\n                    if input_dir.is_remote:\n                        input_dir.download_file(source, destination)\n                    else:\n                        shutil.copyfile(source, destination)\n            queue_out.put(index)\n            queue_in.task_done()\n        except Exception as e:\n            logger.error(f\"Error downloading data: {e}\")\n            queue_in.task_done()\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        task = queue_in.get()\n        if task is None:\n            break\n\n        index, files = task\n\n        for file in files:\n            src_path = input_dir.path(file)\n            dst_path = os.path.join(cache_dir, file)\n            if not os.path.exists(dst_path):\n                input_dir.download(src_path, dst_path)\n\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        task = queue_in.get()\n        if task is None:\n            break\n        index, file_paths = task\n        for file_path in file_paths:\n            local_path = os.path.join(cache_dir, file_path)\n            if not os.path.exists(local_path):\n                source_path = input_dir.get_path(file_path)\n                os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                download_file(source_path, local_path)\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Initialize the local cache directory if it does not exist\n    if not os.path.isdir(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n        # Get the next download task from the input queue\n        task = queue_in.get()\n\n        # If the task is None, it indicates that the queue has been exhausted and no more tasks will be received\n        if task is None:\n            break\n\n        # Unpack the task into its index and list of file paths\n        index, file_paths = task\n\n        # Check if the files for the current index are already downloaded\n        if not os.path.isdir(os.path.join(cache_dir, str(index))):\n            # If the files are not downloaded, create a directory for the current index in the local cache\n            os.makedirs(os.path.join(cache_dir, str(index)))\n\n            # Download each file in the list of file paths\n            for file_path in file_paths:\n                # Determine the source and destination paths for the current file\n                source_path = os.path.join(input_dir, file_path)\n                destination_path = os.path.join(cache_dir, str(index), file_path)\n\n                # Download the file from the source path to the destination path\n                shutil.copyfile(source_path, destination_path)\n\n        # Signal that the files for the current index are available by putting the index into the output queue\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        task = queue_in.get()\n        if task is None:\n            break\n        index, files = task\n        for file in files:\n            if not os.path.exists(os.path.join(cache_dir, file)):\n                if input_dir.is_remote:\n                    input_dir.download(file, cache_dir)\n                else:\n                    shutil.copy(os.path.join(input_dir.path, file), os.path.join(cache_dir, file))\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Initialize the number of completed tasks\n    completed = 0\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n\n        # Fetch the next download task from the input queue\n        task = queue_in.get()\n\n        # Check if the task is None, indicating that the queue is empty\n        if task is None:\n\n            # If the queue is empty, break out of the loop\n            break\n\n        # Get the index of the task and the list of file paths to download\n        idx, files = task\n\n        # Initialize a flag to track if all files are downloaded\n        all_downloaded = True\n\n        # Iterate over the list of file paths to download\n        for file in files:\n\n            # Check if the file is already downloaded by checking if it exists in the local cache directory\n            if not os.path.exists(os.path.join(cache_dir, file)):\n\n                # If the file is not downloaded, download it from the source directory to the local cache directory\n                input_dir.download_file(file, os.path.join(cache_dir, file))\n\n                # Set the flag to indicate that not all files are downloaded\n                all_downloaded = False\n\n        # If all files are downloaded, put the task index into the output queue to signal that the files are available\n        if all_downloaded:\n\n            # Increment the number of completed tasks\n            completed += 1\n\n            # Put the task index into the output queue to signal that the files are available\n            queue_out.put(idx)\n\n        # If the number of completed tasks equals the number of tasks in the input queue, break out of the loop\n        if completed == queue_in.qsize():\n\n            # Break out of the loop\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Loop until the input queue is empty\n    while True:\n        # Get the next task from the input queue\n        task = queue_in.get()\n        # If the task is None, it means the input queue is empty, so break the loop\n        if task is None:\n            break\n        # Unpack the task into its index and file paths\n        i, file_paths = task\n        # Loop through each file path in the task\n        for file_path in file_paths:\n            # Determine the local path for the file in the cache directory\n            local_path = os.path.join(cache_dir, file_path)\n            # If the file is not already downloaded, download it\n            if not os.path.exists(local_path):\n                # Determine the source path or URL for the file\n                source_path = input_dir.get_file_path(file_path)\n                # Download the file\n                input_dir.download_file(source_path, local_path)\n        # Put the task index into the output queue to signal that the files for that index are available\n        queue_out.put(i)\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Import necessary libraries\n    import os\n    import shutil\n    import urllib.request\n\n    # Loop indefinitely\n    while True:\n\n        # Get the next download task from the input queue\n        task = queue_in.get()\n\n        # Check if the task is None, indicating that the queue is empty\n        if task is None:\n\n            # Put None into the output queue to signal that the input queue is empty\n            queue_out.put(None)\n\n            # Break the loop to exit the function\n            break\n\n        # Get the index and file paths from the task\n        index, file_paths = task\n\n        # Loop through the file paths\n        for file_path in file_paths:\n\n            # Get the file name from the file path\n            file_name = os.path.basename(file_path)\n\n            # Construct the local file path by joining the cache directory and file name\n            local_file_path = os.path.join(cache_dir, file_name)\n\n            # Check if the file already exists in the cache directory\n            if not os.path.exists(local_file_path):\n\n                # Construct the source file path by joining the input directory and file path\n                source_file_path = os.path.join(input_dir, file_path)\n\n                # Download the file from the source file path and save it to the local file path\n                with urllib.request.urlopen(source_file_path) as response, open(local_file_path, 'wb') as out_file:\n                    shutil.copyfileobj(response, out_file)\n\n        # Put the index into the output queue to signal that the files for the index are available\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Initialize a progress bar for the download process\n    with tqdm(total=len(queue_in.queue), desc=\"Downloading data\", unit=\"file\", leave=False) as pbar:\n        # Loop until the input queue is empty\n        while not queue_in.empty():\n            # Get the next download task from the input queue\n            task = queue_in.get()\n            # Get the index of the task\n            index = task[0]\n            # Get the list of file paths to download\n            files = task[1]\n            # Loop over each file path\n            for file in files:\n                # Get the file name from the file path\n                file_name = file.split(\"/\")[-1]\n                # Construct the full path to the file in the cache directory\n                file_path = os.path.join(cache_dir, file_name)\n                # Check if the file already exists in the cache directory\n                if not os.path.exists(file_path):\n                    # If the file does not exist, download it from the source directory\n                    input_dir.get(file, file_path)\n            # Put the index of the completed task into the output queue to signal that the files for that index are available\n            queue_out.put(index)\n            # Update the progress bar to reflect the completion of the current task\n            pbar.update(1)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Import necessary modules\n    from os import path\n    from shutil import copyfile\n    from typing import List\n\n    # Define a function to download a file from a source path to a destination path\n    def _download_file(source_path: str, destination_path: str) -> None:\n        \"\"\"\n        This function downloads a file from a source path to a destination path.\n\n        Input-Output Arguments\n        :param source_path: str. The path to the file to be downloaded.\n        :param destination_path: str. The path to the destination where the file will be saved.\n        :return: None. There are no return values as the function's purpose is to perform side effects (downloading the file).\n        \"\"\"\n        # Check if the source path is a URL\n        if source_path.startswith(\"http\"):\n            # Download the file from the URL using requests\n            import requests\n            response = requests.get(source_path)\n            # Open the destination file in binary write mode\n            with open(destination_path, \"wb\") as file:\n                # Write the content of the response to the file\n                file.write(response.content)\n        else:\n            # Copy the file from the source path to the destination path\n            copyfile(source_path, destination_path)\n\n    # Define a function to download a list of files\n    def _download_files(file_paths: List[str], cache_dir: str) -> None:\n        \"\"\"\n        This function downloads a list of files from a source directory to a local cache directory.\n\n        Input-Output Arguments\n        :param file_paths: List[str]. A list of file paths to download. Each path is relative to the source directory.\n        :param cache_dir: str. The path to the local cache directory where the files will be downloaded.\n        :return: None. There are no return values as the function's purpose is to perform side effects (downloading files).\n        \"\"\"\n        # Loop over the file paths\n        for file_path in file_paths:\n            # Determine the destination path for the file\n            destination_path = path.join(cache_dir"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Importing necessary modules\n    import os\n    import shutil\n\n    # Creating a list of existing files in the cache directory\n    existing_files = os.listdir(cache_dir)\n\n    # Continuously fetching download tasks from the input queue\n    while True:\n        # Fetching the next download task from the input queue\n        task = queue_in.get()\n\n        # Checking if the task is None, indicating that there are no more tasks\n        if task is None:\n            # If there are no more tasks, break out of the loop\n            break\n\n        # Extracting the index and file paths from the task\n        index, files = task\n\n        # Checking if the files are already downloaded\n        if all(os.path.basename(file) in existing_files for file in files):\n            # If the files are already downloaded, put the index into the output queue to signal completion\n            queue_out.put(index)\n            continue\n\n        # Downloading the files using the input_dir's get() method\n        input_dir.get(files, cache_dir)\n\n        # Putting the index into the output queue to signal completion\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Create the cache directory if it does not exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Loop indefinitely\n    while True:\n        # Get the next download task from the input queue\n        task = queue_in.get()\n\n        # If the task is None, it means the queue has been exhausted and the loop should stop\n        if task is None:\n            break\n\n        # Get the index and file paths for the task\n        idx, files = task\n\n        # Download each file that is not already downloaded\n        for f in files:\n            # Determine the local path for the file\n            local_path = os.path.join(cache_dir, f)\n\n            # Check if the file already exists\n            if not os.path.exists(local_path):\n                # If the file does not exist, download it from the source directory\n                input_dir.download(f, local_path)\n\n        # Signal that the task is completed by putting the index into the output queue\n        queue_out.put(idx)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    import os\n    import urllib.request\n    from urllib.error import URLError\n\n    # Create the cache directory if it doesn't exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Loop until the input queue is empty\n    while True:\n        # Get the next download task from the input queue\n        task_index, files = queue_in.get()\n\n        # Check if the task index is None, indicating the end of the queue\n        if task_index is None:\n            # Put None into the output queue to signal the end of the queue\n            queue_out.put(None)\n            break\n\n        # Loop over each file path in the task\n        for file_path in files:\n            # Determine the local file path for the file\n            local_file_path = os.path.join(cache_dir, file_path)\n\n            # Check if the file already exists in the cache\n            if not os.path.exists(local_file_path):\n                # Download the file from the source directory\n                try:\n                    input_dir.download_file(file_path, local_file_path)\n                except URLError:\n                    # If the file cannot be downloaded, put None into the output queue to signal that the files for the task index are not available\n                    queue_out.put(None)\n                    break\n\n        # Put the task index into the output queue to signal that the files for the task index are available\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n\n        if isinstance(item, str):\n            file_path = item\n        else:\n            temp_dir, file_path = item\n\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n\n        if output_dir.scheme == 's3':\n            output_dir.upload(file_path, file_path)\n        else:\n            output_dir.move(file_path, file_path)\n\n        remove_queue.put(file_path)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n        if isinstance(item, str):\n            file_path = item\n        else:\n            tmp_dir, file_path = item\n            file_path = os.path.join(tmp_dir, file_path)\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n        if not os.path.exists(file_path):\n            continue\n        if output_dir.scheme == 's3':\n            output_dir.upload(file_path, file_path)\n        else:\n            output_dir.move(file_path, file_path)\n        remove_queue.put(file_path)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n        if isinstance(item, str):\n            file_path = item\n        else:\n            tmp_dir, file_path = item\n        if file_path.startswith(cache_dir):\n            file_path = file_path[len(cache_dir) :]\n        if file_path.startswith(\"/\"):\n            file_path = file_path[1:]\n        if output_dir.scheme == \"s3\":\n            output_dir.upload(file_path, tmp_dir)\n        else:\n            shutil.move(os.path.join(tmp_dir, file_path), os.path.join(output_dir.path, file_path))\n        remove_queue.put(tmp_dir)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item == 'terminate':\n            return\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n            file_path = os.path.join(temp_dir, file_path)\n        else:\n            file_path = item\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n        if output_dir.scheme == 's3':\n            output_dir.upload_file(file_path, file_path)\n        else:\n            output_dir.copy_file(file_path, file_path)\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        try:\n            item = upload_queue.get()\n            if item == \"terminate\":\n                break\n            if isinstance(item, tuple):\n                tmp_dir, file_path = item\n                file_path = os.path.join(tmp_dir, file_path)\n            else:\n                file_path = item\n            if not file_path.startswith(cache_dir):\n                file_path = os.path.join(cache_dir, file_path)\n            if output_dir.scheme == \"s3\":\n                output_dir.upload_file(file_path, file_path)\n            else:\n                os.makedirs(os.path.dirname(file_path), exist_ok=True)\n                shutil.move(file_path, file_path)\n            remove_queue.put(file_path)\n        except Exception as e:\n            logging.error(f\"Error in upload function: {e}\")\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n            file_path = os.path.join(temp_dir, file_path)\n        else:\n            file_path = item\n\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n\n        if output_dir.scheme == \"s3\":\n            bucket, key = output_dir.bucket, output_dir.key\n            s3_client.upload_file(file_path, bucket, key)\n        else:\n            output_file_path = os.path.join(output_dir.path, file_path)\n            os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n            shutil.move(file_path, output_file_path)\n\n        remove_queue.put(file_path)\n        upload_queue.task_done()\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item == \"terminate\":\n            break\n\n        if isinstance(item, str):\n            file_path = item\n        else:\n            tmp_dir, file_path = item\n\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n\n        if output_dir.scheme == \"s3\":\n            output_dir.upload(file_path, file_path)\n        else:\n            shutil.move(file_path, output_dir.path)\n\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item == \"DONE\":\n            break\n        if isinstance(item, tuple):\n            tmp_dir, file_path = item\n            file_path = os.path.join(tmp_dir, file_path)\n        else:\n            file_path = item\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n        if not os.path.exists(file_path):\n            continue\n        if output_dir.scheme == \"s3\":\n            output_dir.upload_file(file_path)\n        else:\n            output_dir.copy_file(file_path)\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n        if isinstance(item, str):\n            file_path = item\n        else:\n            tmp_dir, file_path = item\n            file_path = os.path.join(tmp_dir, file_path)\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n        if output_dir.scheme == \"s3\":\n            output_dir.upload(file_path, file_path)\n        else:\n            output_dir.copy(file_path, file_path)\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item == \"terminate\":\n            break\n\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n            file_path = os.path.join(temp_dir, file_path)\n\n        else:\n            file_path = item\n\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n\n        output_path = os.path.join(output_dir.path, os.path.relpath(file_path, cache_dir))\n        output_dir.mkdir(os.path.dirname(output_path), exist_ok=True)\n\n        if output_dir.scheme == \"s3\":\n            output_dir.upload(file_path, output_path)\n\n        else:\n            if os.path.exists(output_path):\n                os.remove(output_path)\n\n            shutil.move(file_path, output_path)\n\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n\n        if isinstance(item, tuple):\n            tmp_dir, file_path = item\n            file_path = os.path.join(tmp_dir, file_path)\n        else:\n            file_path = item\n\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n\n        if output_dir.scheme == 's3':\n            bucket_name, key_prefix = output_dir.netloc, output_dir.path.lstrip('/')\n            key = os.path.join(key_prefix, file_path.lstrip(cache_dir))\n            s3_client.upload_file(file_path, bucket_name, key)\n        else:\n            output_path = os.path.join(output_dir.path, file_path.lstrip(cache_dir))\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n            shutil.move(file_path, output_path)\n\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n            file_path = os.path.join(temp_dir, file_path)\n        else:\n            file_path = item\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n        output_path = os.path.join(output_dir.path, os.path.relpath(file_path, cache_dir))\n        if output_dir.scheme == 's3':\n            s3_client.upload_file(file_path, output_dir.bucket, output_path)\n        else:\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n            shutil.move(file_path, output_path)\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n        if isinstance(item, str):\n            file_path = item\n        else:\n            tmp_dir, file_path = item\n            if tmp_dir is not None:\n                shutil.rmtree(tmp_dir)\n        if file_path.startswith(cache_dir):\n            file_path = file_path[len(cache_dir):]\n        if output_dir.scheme == 's3':\n            s3_upload_file(output_dir, file_path)\n        else:\n            if not file_path.startswith(output_dir.path):\n                file_path = os.path.join(output_dir.path, file_path)\n            shutil.move(os.path.join(cache_dir, file_path), file_path)\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item == \"terminate\":\n            break\n        if isinstance(item, tuple):\n            # item is a tuple containing a temporary directory and a file path\n            tmp_dir, file_path = item\n            file_path = os.path.join(tmp_dir, file_path)\n        else:\n            # item is a file path\n            file_path = item\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n        if output_dir.scheme == \"s3\":\n            # upload to S3\n            output_dir.upload(file_path, file_path)\n        else:\n            # move to local directory\n            output_dir.move(file_path, file_path)\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        # Get an item from the upload queue\n        item = upload_queue.get()\n\n        # If the item is None, it indicates termination, so break out of the loop\n        if item is None:\n            break\n\n        # If the item is a tuple, it contains a temporary directory and a file path\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n        else:\n            # If the item is not a tuple, it is a file path\n            file_path = item\n            temp_dir = None\n\n        # Prepend the cache directory to the file path if it doesn't start with it\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n\n        # Create the target file path by removing the cache directory prefix\n        target_file_path = file_path[len(cache_dir) + 1 :]\n\n        # Upload the file to the target directory\n        output_dir.upload(file_path, target_file_path)\n\n        # If a temporary directory was provided, remove the file from it\n        if temp_dir is not None:\n            os.remove(os.path.join(temp_dir, file_path))\n\n        # Add the file path to the remove queue for removal after successful upload\n        remove_queue.put(file_path)\n\n        # Mark the upload as complete by sending None to the upload queue\n        upload_queue.task_done()\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n\n        if isinstance(item, tuple):\n            tmp_dir, file_path = item\n            file_path = os.path.join(tmp_dir, file_path)\n        else:\n            file_path = item\n\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n\n        if output_dir.scheme == \"s3\":\n            with open(file_path, \"rb\") as f:\n                output_dir.upload(file_path[len(cache_dir) + 1:], f)\n        else:\n            shutil.move(file_path, output_dir.path)\n\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n        if isinstance(item, tuple):\n            tmp_dir, file_path = item\n            file_path = os.path.join(tmp_dir, file_path)\n        else:\n            file_path = item\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n        if output_dir.scheme == \"s3\":\n            output_dir.upload(file_path, file_path)\n        else:\n            output_dir.move(file_path, file_path)\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # Set up a signal handler for SIGTERM\n    def sigterm_handler(_signo, _stack_frame):\n        sys.exit(0)\n    signal.signal(signal.SIGTERM, sigterm_handler)\n\n    # Initialize the uploader\n    uploader = Uploader(output_dir)\n\n    # Continuously process items from the upload queue\n    while True:\n        item = upload_queue.get()\n\n        # If the item is None, it indicates the end of the upload process\n        if item is None:\n            break\n\n        # If the item is a tuple, it contains a temporary directory and a file path\n        if isinstance(item, tuple):\n            tmp_dir, file_path = item\n            file_path = os.path.join(tmp_dir, file_path)\n        else:\n            # Otherwise, the item is a file path\n            file_path = item\n\n        # Check if the file path starts with the cache directory\n        if not file_path.startswith(cache_dir):\n            # If not, prepend the cache directory to the file path\n            file_path = os.path.join(cache_dir, file_path)\n\n        # Upload the file to the output directory\n        uploader.upload(file_path)\n\n        # Send the file path to the remove queue for removal after successful upload\n        remove_queue.put(file_path)\n\n        # Signal that the upload is complete\n        upload_queue.task_done()\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # Loop until a termination signal is received\n    while True:\n        # Get an item from the upload queue\n        item = upload_queue.get()\n        # If the item is None, it indicates a termination signal\n        if item is None:\n            # Put the termination signal back into the queue and exit the loop\n            upload_queue.put(None)\n            break\n        # If the item is a tuple, it contains a temporary directory and a file path\n        if isinstance(item, tuple):\n            # Extract the temporary directory and file path from the tuple\n            temp_dir, file_path = item\n            # Prepend the cache directory to the file path if it doesn't start with it\n            if not file_path.startswith(cache_dir):\n                file_path = os.path.join(cache_dir, file_path)\n        else:\n            # If the item is not a tuple, it is a file path\n            file_path = item\n            # Prepend the cache directory to the file path if it doesn't start with it\n            if not file_path.startswith(cache_dir):\n                file_path = os.path.join(cache_dir, file_path)\n            # Set the temporary directory to None\n            temp_dir = None\n\n        # Check if the output directory is an S3 bucket\n        if output_dir.scheme == \"s3\":\n            # If it is, upload the file to the bucket using the S3 client\n            output_dir.s3_client.upload_file(file_path, output_dir.bucket, output_dir.key + file_path.replace(cache_dir, \"\"))\n        else:\n            # If it's not an S3 bucket, move the file to the output directory\n            shutil.move(file_path, os.path.join(output_dir.path, file_path.replace(cache_dir, \"\")))\n\n        # Put the file path into the remove queue to indicate that it should be removed after successful upload\n        remove_queue.put(file_path)\n        # If a temporary directory was specified, remove the temporary directory\n        if temp_dir:"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # Set up a signal handler to catch termination signals\n    signal.signal(signal.SIGTERM, _sigterm_handler)\n\n    # Continuously process items from the upload queue until a termination signal is received\n    while True:\n        try:\n            item = upload_queue.get(timeout=1)\n            if isinstance(item, str):\n                file_path = item\n            else:\n                tmp_dir, file_path = item\n\n            # Check if the file path starts with the cache directory\n            if not file_path.startswith(cache_dir):\n                file_path = os.path.join(cache_dir, file_path)\n\n            # Upload the file to the target directory\n            if output_dir.scheme == \"s3\":\n                output_dir.upload_file(file_path, os.path.basename(file_path))\n            else:\n                output_dir.copy_file(file_path, os.path.basename(file_path))\n\n            # Add the file path to the remove queue for removal\n            remove_queue.put(file_path)\n\n            # Remove the temporary directory if it exists\n            if tmp_dir:\n                shutil.rmtree(tmp_dir)\n\n            # Signal that the task is done\n            upload_queue.task_done()\n        except queue.Empty:\n            # If the queue is empty, check if a termination signal was received\n            if _TERMINATE:\n                break\n            else:\n                continue\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": ""}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": ""}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": ""}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": ""}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": ""}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": ""}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": ""}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": ""}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": ""}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _pack_greedily(\n        num_workers: int,\n        items: List[Any],\n        weights: Optional[List[int]] = None,\n        file_size: bool = False,\n    ) -> Tuple[List[List[Any]], List[int]]:\n        \"\"\"\n        This function distributes a list of items among workers in a weighted manner. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution details for workers on the current node and returns a list of items for each worker, with the items shuffled.\n\n        Input-Output Arguments\n        :param num_workers: Int. The number of workers per node.\n        :param items: List[Any]. The items to be distributed among the workers.\n        :param weights: Optional[List[int]]. The weights for each item, used for distribution. If not provided, all items are considered to have equal weight.\n        :param file_size: Bool. A flag indicating whether to consider the items as files with sizes for printing purposes. If True, sizes are printed in megabytes; otherwise, the total weight of items is printed.\n        :return: List[List[Any]]. A list of lists, where each sublist contains the items assigned to a worker, shuffled randomly.\n        \"\"\"\n        if weights is None:\n            weights = [1] * len(items)\n\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        world_size = num_nodes * num_workers\n\n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n        \"\"\"\n        This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _pack_greedily(num_workers, user_items, weights=None, file_size=False):\n        \"\"\"\n        This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution details for workers on the current node and returns a list of items for each worker, with the items shuffled.\n\n        Input-Output Arguments\n        :param num_workers: Int. The number of workers per node.\n        :param user_items: List[Any]. The items to be distributed among the workers.\n        :param weights: Optional[List[int]]. The weights for each item, used for distribution. If not provided, all items are considered to have equal weight.\n        :param file_size: Bool. A flag indicating whether to consider the items as files with sizes for printing purposes. If True, sizes are printed in megabytes; otherwise, the total weight of items is printed.\n        :return: List[List[Any]]. A list of lists, where each sublist contains the items assigned to a worker, shuffled randomly.\n        \"\"\"\n        if weights is None:\n            weights = [1] * len(user_items)\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        world_size = num_nodes * num_workers\n\n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n        print(f\"Distributing {len(user_items)} items to {world_size} workers:\")\n        for i, worker_id in enumerate(worker_ids_this_node):\n            worker_item = worker_items[i]\n            worker_weight = worker_weights[i]\n            if file_size:\n                worker_weight = sum(worker_"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Print the distribution details for workers on the current node\n    for worker_id, worker_items in zip(worker_ids_this_node, worker_items):\n        if file_size:\n            worker_size = sum(item[1] for item in worker_items) / 1024 / 1024\n            print(f\"Worker {worker_id} has {len(worker_items)} items ({worker_size:.2f} MB)\")\n        else:\n            print(f\"Worker {worker_id} has {len(worker_items)} items ({sum(worker_weights[i] for i in worker_items)} weight)\")\n\n    # Return the list of items for each worker, with the items shuffled\n    return [list(map(lambda i: user_items[i], items)) for items in worker_items]"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _get_num_nodes() -> int:\n        \"\"\"\n        This function returns the number of nodes in the distributed system.\n\n        :return: Int. The number of nodes.\n        \"\"\"\n        return 1\n\n    def _get_node_rank() -> int:\n        \"\"\"\n        This function returns the rank of the current node in the distributed system.\n\n        :return: Int. The rank of the current node.\n        \"\"\"\n        return 0\n\n    def _pack_greedily(items: List[Any], weights: Optional[List[int]] = None, num_bins: int = 1) -> Tuple[List[List[Any]], List[int]]:\n        \"\"\"\n        This function distributes a list of items among bins in a greedy manner, optionally considering weights. It first sorts the items by weight, then iteratively assigns each item to the bin with the least weight so far. It returns the distribution of items among bins and the weights of each bin.\n\n        Input-Output Arguments\n        :param items: List[Any]. The items to be distributed among the bins.\n        :param weights: Optional[List[int]]. The weights for each item, used for distribution. If not provided, all items are considered to have equal weight.\n        :param num_bins: Int. The number of bins to distribute the items among.\n        :return: Tuple[List[List[Any]], List[int]]. A tuple containing the distribution of items among bins and the weights of each bin.\n        \"\"\"\n        if weights is None:\n            weights = [1] * len(items)\n        items_with_weights = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n        bins = [[] for _ in range(num_bins)]\n        bin_weights = [0] * num_bins\n        for item, weight in items_with_weights:\n            bin_idx = min(range(num_bins), key=lambda i: bin_weights[i])\n            bins[bin_idx].append(item)\n            bin_weights[bin_idx] += weight"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Initialize a dictionary to store the items assigned to each worker\n    worker_items = {i: [] for i in range(world_size)}\n\n    # If weights are not provided, assign equal weight to all items\n    if weights is None:\n        weights = [1] * len(user_items)\n\n    # Calculate the total weight of all items\n    total_weight = sum(weights)\n\n    # Assign items to workers based on weights\n    for item, weight in zip(user_items, weights):\n        # Calculate the number of items to assign to each worker\n        num_items = int(weight / total_weight * len(user_items))\n        # Assign the items to workers in a round-robin fashion\n        for i in range(num_items):\n            worker_items[i % world_size].append(item)\n\n    # Shuffle the items assigned to each worker\n    for i in range(world_size):\n        random.shuffle(worker_items[i])\n\n    # Print the distribution details for workers on the current node\n    for i in worker_ids_this_node:\n        worker_items_this_worker = worker_items[i]\n        if file_size:\n            worker_items_this_worker = [os.path.getsize(item) for item in worker_items_this_worker]\n        print(f\"Worker {i} gets {len(worker_items_this_worker)} items ({sum(worker_items_this_worker) / 1024 / 1024:.2f} MB)\")\n\n    # Return a list of items for each worker, with the items shuffled\n    return [worker_items[i] for i in worker_ids_this_node]"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Get the number of workers across all nodes\n    num_nodes = _get_num_nodes()\n    num_workers_total = num_nodes * num_workers\n\n    # Distribute items to workers based on weights\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=num_workers_total)\n\n    # Print distribution details for workers on the current node\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n    for worker_id, items, weight in zip(worker_ids_this_node, worker_items, worker_weights):\n        if file_size:\n            total_size_mb = sum(item[\"size\"] for item in items) / 1024 / 1024\n            print(f\"Worker {worker_id} gets {len(items)} files ({total_size_mb:.1f} MiB)\")\n        else:\n            print(f\"Worker {worker_id} gets {len(items)} items ({weight} tokens)\")\n\n    # Shuffle the items assigned to each worker\n    for items in worker_items:\n        random.shuffle(items)\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Get the number of nodes and the rank of the current node\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    # Calculate the total number of workers across all nodes\n    world_size = num_nodes * num_workers\n\n    # Distribute the items to the workers based on the provided weights\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n\n    # Get the worker IDs for the current node\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Print the distribution details for workers on the current node\n    _print_dist(worker_items, worker_weights, worker_ids_this_node, file_size)\n\n    # Return the list of items for each worker, shuffled randomly\n    return [shuffle(worker_items[i]) for i in worker_ids_this_node]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Print distribution details for workers on the current node\n    for worker_id in worker_ids_this_node:\n        items = worker_items[worker_id]\n        weights = worker_weights[worker_id]\n        total_weight = sum(weights)\n        if file_size:\n            total_weight = sum(weights) / 1024 / 1024\n        print(f\"Worker {worker_id} has {len(items)} items with total weight {total_weight}\")\n\n    # Shuffle items for each worker\n    for worker_id in worker_ids_this_node:\n        items = worker_items[worker_id]\n        random.shuffle(items)\n        worker_items[worker_id] = items\n\n    return worker_items\n\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    for i, worker_id in enumerate(worker_ids_this_node):\n        worker_item = worker_items[i]\n        worker_weight = worker_weights[i]\n        worker_size = sum(worker_weight)\n        if file_size:\n            worker_size = sum(worker_weight) / (1024 * 1024)\n        logger.info(f\"Rank {worker_id} | {len(worker_item)} | {worker_size:.1f} MB\")\n\n    return [random.sample(worker_item, len(worker_item)) for worker_item in worker_items]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if weights is None:\n        weights = [1] * len(user_items)\n    if len(weights) != len(user_items):\n        raise ValueError(\"Number of weights must match the number of items.\")\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    for worker_id in worker_ids_this_node:\n        items_for_worker = worker_items[worker_id]\n        weights_for_worker = worker_weights[worker_id]\n        total_weight = sum(weights_for_worker)\n        if file_size:\n            total_weight /= (1024 * 1024)\n        print(f\"Worker {worker_id} gets {len(items_for_worker)} items, with total weight {total_weight:.1f} MB.\")\n\n    return [list(np.random.permutation(items_for_worker)) for items_for_worker in worker_items]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _pack_greedily(items, weights, num_bins):\n        \"\"\"\n        This function distributes a list of items among a given number of bins (bins) in a greedy manner, optionally considering file sizes. It first calculates the total weight of items and the total size of files if file_size is True. It then iterates over the items, assigning each item to the bin with the smallest weight (or size if file_size is True). If the weight of an item exceeds the weight of the bin, it is assigned to the next bin. The function returns a list of items for each bin, with the items shuffled randomly.\n\n        Input-Output Arguments\n        :param items: List[Any]. The items to be distributed among the bins.\n        :param weights: Optional[List[int]]. The weights for each item, used for distribution. If not provided, all items are considered to have equal weight.\n        :param num_bins: Int. The number of bins to distribute the items among.\n        :param file_size: Bool. A flag indicating whether to consider the items as files with sizes for distribution. If True, sizes are used for distribution; otherwise, weights are used.\n        :return: Tuple[List[List[Any]], List[int]]. A tuple containing a list of lists, where each sublist contains the items assigned to a bin, shuffled randomly, and a list of the weights of each bin.\n        \"\"\"\n        # Calculate total weight or size\n        total_weight = sum(weights) if weights else len(items)\n\n        # Initialize bins and weights\n        bins = [[] for _ in range(num_bins)]\n        bin_weights = [0] * num_bins\n\n        # Distribute items greedily\n        for item, weight in zip(items, weights):\n            # Find the bin with the smallest weight\n            min_bin = min(range(num_bins), key=lambda i: bin_weights[i])\n\n            # Add item to the bin\n            bins[min_bin].append(item)\n            bin_weights[min_bin] += weight\n\n        # Shuffle items within each bin"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    num_items = len(user_items)\n    num_items_per_worker = (num_items + num_workers - 1) // num_workers\n    num_workers_to_use = num_nodes * num_workers if node_rank != num_nodes - 1 else num_items - (num_nodes - 1) * num_items_per_worker\n\n    out = []\n    for i in range(num_workers_to_use):\n        out.append(user_items[i * num_items_per_worker : (i + 1) * num_items_per_worker])\n    return out\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    num_workers_total = num_workers * num_nodes\n    num_items = len(user_items)\n    num_items_per_worker = num_items // num_workers_total\n    num_items_per_worker_remainder = num_items % num_workers_total\n\n    num_items_per_worker_list = [num_items_per_worker] * num_workers_total\n    num_items_per_worker_list[-num_items_per_worker_remainder:] = [\n        n + 1 for n in num_items_per_worker_list[-num_items_per_worker_remainder:]\n    ]\n\n    num_items_per_worker_cumsum = np.cumsum(num_items_per_worker_list)\n    num_items_per_worker_cumsum = np.insert(num_items_per_worker_cumsum, 0, 0)\n\n    user_items_split = [\n        user_items[\n            num_items_per_worker_cumsum[n] : num_items_per_worker_cumsum[n + 1]\n        ]\n        for n in range(num_workers_total)\n    ]\n\n    if len(user_items_split) != num_workers_total:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return user_items_split"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    num_workers_total = num_nodes * num_workers\n    num_items = len(user_items)\n\n    num_items_per_worker = num_items // num_workers_total\n    num_items_remainder = num_items % num_workers_total\n\n    worker_items = []\n    for i in range(num_workers_total):\n        start = i * num_items_per_worker + min(i, num_items_remainder)\n        end = (i + 1) * num_items_per_worker + min(i + 1, num_items_remainder)\n        worker_items.append(user_items[start:end])\n\n    assert len(worker_items) == num_workers_total, \"Improper assignment\"\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    num_workers_per_node = num_workers\n    num_workers = num_nodes * num_workers_per_node\n\n    num_items = len(user_items)\n    num_items_per_worker = num_items // num_workers\n    num_items_per_worker_list = [num_items_per_worker] * num_workers\n    num_items_remainder = num_items % num_workers\n\n    if num_items_remainder != 0:\n        num_items_per_worker_list[-num_items_remainder:] = [\n            num_items_per_worker + 1 for _ in range(num_items_remainder)\n        ]\n\n    assert sum(num_items_per_worker_list) == num_items\n\n    outputs = []\n    for i in range(num_workers):\n        outputs.append(\n            user_items[sum(num_items_per_worker_list[:i]) : sum(num_items_per_worker_list[: i + 1])]\n        )\n\n    assert len(outputs) == num_workers\n\n    return outputs\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    num_workers_total = num_nodes * num_workers\n    num_items_per_worker = len(user_items) // num_workers_total\n    num_items_remainder = len(user_items) % num_workers_total\n    num_items_per_worker_list = [num_items_per_worker] * num_workers_total\n    num_items_per_worker_list[-num_items_remainder:] = [\n        n + 1 for n in num_items_per_worker_list[-num_items_remainder:]\n    ]\n    assert sum(num_items_per_worker_list) == len(user_items)\n    num_items_per_worker_cumsum = np.cumsum(num_items_per_worker_list)\n    num_items_per_worker_cumsum = np.insert(num_items_per_worker_cumsum, 0, 0)\n    worker_id_for_rank = node_rank * num_workers + np.arange(num_workers)\n    worker_items = [\n        user_items[start:end]\n        for start, end in zip(\n            num_items_per_worker_cumsum[:-1], num_items_per_worker_cumsum[1:]\n        )\n    ]\n    worker_items = [worker_items[i] for i in worker_id_for_rank]\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\n            \"Improper worker<->items assignment, \"\n            f\"{len(worker_items)} workers vs {num_workers} workers\"\n        )\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    num_workers_per_node = num_workers\n    num_workers = num_nodes * num_workers_per_node\n\n    num_items = len(user_items)\n    num_items_per_worker = (num_items + num_workers - 1) // num_workers\n    num_items_last_worker = num_items - (num_workers - 1) * num_items_per_worker\n\n    worker_to_items = []\n    for i in range(num_workers):\n        if i == num_workers - 1:\n            start = i * num_items_per_worker\n            end = start + num_items_last_worker\n        else:\n            start = i * num_items_per_worker\n            end = start + num_items_per_worker\n        worker_to_items.append(user_items[start:end])\n\n    if len(worker_to_items) != num_workers:\n        raise RuntimeError(\n            f\"Improper worker to items assignment. Expected {num_workers} workers, but got {len(worker_to_items)} workers.\"\n        )\n    return worker_to_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    num_workers_per_node = num_workers // num_nodes\n    num_workers_total = num_workers_per_node * num_nodes\n\n    num_items = len(user_items)\n    num_items_per_worker = num_items // num_workers_total\n    num_items_remainder = num_items % num_workers_total\n\n    num_items_per_worker_list = [num_items_per_worker] * num_workers_total\n    num_items_per_worker_list[-num_items_remainder:] = [num_items_per_worker + 1] * num_items_remainder\n\n    num_items_per_worker_list_cumsum = np.cumsum(num_items_per_worker_list)\n    num_items_per_worker_list_cumsum = np.insert(num_items_per_worker_list_cumsum, 0, 0)\n\n    start_indices = num_items_per_worker_list_cumsum[:-1]\n    end_indices = num_items_per_worker_list_cumsum[1:]\n\n    worker_items = []\n    for start, end in zip(start_indices, end_indices):\n        worker_items.append(user_items[start:end])\n\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    total_num_workers = num_nodes * num_workers\n    num_items_per_worker = (len(user_items) + total_num_workers - 1) // total_num_workers\n    start_indices = list(range(0, len(user_items), num_items_per_worker))\n    end_indices = start_indices[1:] + [len(user_items)]\n    assigned_items = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n    if len(assigned_items) != total_num_workers:\n        raise RuntimeError(f\"Improper assignment of items to workers. Expected {total_num_workers} workers but found {len(assigned_items)}.\")\n    return assigned_items[node_rank * num_workers:(node_rank + 1) * num_workers]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of nodes and the current node's rank\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    # Calculate the total number of workers across all nodes\n    total_num_workers = num_workers * num_nodes\n\n    # Determine how many items each worker should process\n    num_items_per_worker = len(user_items) // total_num_workers\n    remainder = len(user_items) % total_num_workers\n\n    # Adjust for the remainder by adding extra items to the workers starting from the end of the list\n    for i in range(remainder):\n        user_items.append(user_items[-(i+1)])\n\n    # Calculate the start and end indices for each worker's items\n    worker_start_indices = [i * num_items_per_worker for i in range(total_num_workers)]\n    worker_end_indices = [(i+1) * num_items_per_worker for i in range(total_num_workers)]\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [user_items[start:end] for start, end in zip(worker_start_indices, worker_end_indices)]\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(worker_items) != total_num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    num_items = len(user_items)\n    num_items_per_worker = (num_items + num_workers - 1) // num_workers\n    num_workers_per_node = (num_workers + num_nodes - 1) // num_nodes\n\n    num_items_per_worker_last_node = num_items_per_worker - (\n        num_items_per_worker * num_workers_per_node - num_items\n    ) * (node_rank == num_nodes - 1)\n\n    num_workers_per_node = (num_workers + num_nodes - 1) // num_nodes\n    num_workers_last_node = num_workers - num_workers_per_node * (num_nodes - 1)\n\n    num_items_per_worker = (num_items + num_workers - 1) // num_workers\n    num_items_per_worker_last_node = num_items_per_worker - (\n        num_items_per_worker * num_workers_per_node - num_items\n    ) * (node_rank == num_nodes - 1)\n\n    if node_rank == num_nodes - 1:\n        num_workers_per_node = num_workers_last_node\n        num_items_per_worker = num_items_per_worker_last_node\n\n    num_items_list = [num_items_per_worker - 1] * (num_workers_per_node - 1)\n    num_items_list.append(\n        num_items - (num_items_per_worker * (num_workers_per_node - 1))\n    )\n    num_items_cumsum = np.cumsum(num_items_list)\n    num_items_cumsum = np.insert(num_items_cumsum, 0, 0)\n\n    workers_to_users ="}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of nodes and the current node's rank\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    # Calculate the total number of workers across all nodes\n    total_num_workers = num_nodes * num_workers\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_num_workers\n\n    # Calculate the remaining items that need to be distributed\n    remaining_items = len(user_items) % total_num_workers\n\n    # Initialize an empty list to store the assigned items for each worker\n    assigned_items = []\n\n    # Iterate over the workers and assign items to each one\n    for i in range(total_num_workers):\n        # Calculate the start and end indices for the current worker's items\n        start_idx = i * num_items_per_worker + min(i, remaining_items)\n        end_idx = (i + 1) * num_items_per_worker + min(i + 1, remaining_items)\n\n        # Get the items for the current worker\n        worker_items = user_items[start_idx:end_idx]\n\n        # Add the worker's items to the assigned_items list\n        assigned_items.append(worker_items)\n\n    # Check if the length of the assigned_items list is equal to the total number of workers\n    assert len(assigned_items) == total_num_workers, \"Improper assignment of items to workers\"\n\n    # Return the assigned_items list\n    return assigned_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of nodes and the current node's rank\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    # Calculate the total number of workers across all nodes\n    total_num_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_num_workers\n\n    # Calculate the remainder of items after distributing evenly among workers\n    remainder = len(user_items) % total_num_workers\n\n    # Initialize a list of lists to store the items for each worker\n    items_for_workers = [[] for _ in range(total_num_workers)]\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = list(range(0, len(user_items), num_items_per_worker))\n    end_indices = list(range(num_items_per_worker, len(user_items) + 1, num_items_per_worker))\n\n    # Distribute the items among the workers\n    for i, (start, end) in enumerate(zip(start_indices, end_indices)):\n        items_for_workers[i] = user_items[start:end]\n\n    # Distribute any remaining items to the last workers\n    if remainder > 0:\n        for i in range(remainder):\n            items_for_workers[-i-1].append(user_items[-i-1])\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(items_for_workers) != total_num_workers:\n        raise RuntimeError(f\"Improper assignment of items to workers. Expected {total_num_workers} workers, but got {len(items_for_workers)}.\")\n\n    # Return the list of lists containing the items for each worker\n    return items_for_workers"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    num_workers_per_node = num_workers\n    num_workers_total = num_nodes * num_workers_per_node\n\n    num_items = len(user_items)\n    num_items_per_worker = num_items // num_workers_total\n    num_items_per_worker_remainder = num_items % num_workers_total\n\n    # Distribute items to workers\n    worker_items = []\n    for i in range(num_workers_total):\n        start_index = i * num_items_per_worker + min(i, num_items_per_worker_remainder)\n        end_index = (i + 1) * num_items_per_worker + min(i + 1, num_items_per_worker_remainder)\n        worker_items.append(user_items[start_index:end_index])\n\n    # Check if the output list has the correct length\n    if len(worker_items) != num_workers_total:\n        raise RuntimeError(\n            f\"The output list has length {len(worker_items)} but it should have length {num_workers_total}.\"\n        )\n\n    # Return the worker items for the current node\n    start_index = node_rank * num_workers_per_node\n    end_index = start_index + num_workers_per_node\n    return worker_items[start_index:end_index]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of nodes and the current node's rank\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_workers\n\n    # Calculate the remainder of items\n    remainder = len(user_items) % total_workers\n\n    # Initialize the list of items for each worker\n    items_for_workers = [[] for _ in range(total_workers)]\n\n    # Assign items to workers sequentially\n    for i, item in enumerate(user_items):\n        worker_index = i % total_workers\n        items_for_workers[worker_index].append(item)\n\n    # Distribute any remaining items to the last workers\n    for i in range(remainder):\n        worker_index = (total_workers - remainder) + i\n        items_for_workers[worker_index].append(user_items[i])\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.cumsum([0] + [len(items) for items in items_for_workers])\n    end_indices = np.cumsum([len(items) for items in items_for_workers] + [0])\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(items_for_workers) != total_workers:\n        raise RuntimeError(f\"Improper assignment of items to workers. Expected {total_workers} workers, got {len(items_for_workers)}.\")\n\n    # Return the list of items for each worker\n    return items_for_workers"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    total_num_workers = num_workers * _get_num_nodes()\n    num_items_per_worker = (len(user_items) + total_num_workers - 1) // total_num_workers\n    num_items_per_worker_cumsum = np.cumsum(\n        [num_items_per_worker] * total_num_workers\n    )\n    num_items_per_worker_cumsum = np.concatenate(\n        [[0], num_items_per_worker_cumsum], axis=0\n    )\n    num_workers_with_extra_item = len(user_items) % total_num_workers\n    num_items_per_worker_cumsum[-num_workers_with_extra_item - 1 : -1] += 1\n\n    output = []\n    for i in range(total_num_workers):\n        start = num_items_per_worker_cumsum[i]\n        end = num_items_per_worker_cumsum[i + 1]\n        output.append(user_items[start:end])\n\n    if len(output) != total_num_workers:\n        raise RuntimeError(\n            f\"len(output) != total_num_workers, {len(output)} != {total_num_workers}\"\n        )\n    return output"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of nodes and the current node's rank\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    # Calculate the total number of workers across all nodes\n    total_num_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_num_workers\n    num_items_per_worker += len(user_items) % total_num_workers > node_rank\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = [i * num_items_per_worker for i in range(total_num_workers)]\n    end_indices = start_indices[1:] + [len(user_items)]\n\n    # Distribute any extra items to the last workers in the list\n    for i in range(len(user_items) % total_num_workers):\n        end_indices[-i - 1] += 1\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    items_for_workers = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(items_for_workers) != total_num_workers:\n        raise RuntimeError(\n            \"Something went wrong in mapping items to workers. \"\n            f\"Expected {total_num_workers} workers but got {len(items_for_workers)}.\"\n        )\n\n    return items_for_workers\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    num_workers_total = num_nodes * num_workers\n    num_items_per_worker = len(user_items) // num_workers_total\n    num_items_per_worker_adjusted = num_items_per_worker + 1\n    num_workers_with_extra_item = len(user_items) % num_workers_total\n\n    # Adjust for any remainder by adding extra items to the last workers\n    if num_workers_with_extra_item != 0:\n        num_items_per_worker_adjusted += 1\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.cumsum(\n        [0] + [num_items_per_worker_adjusted] * (num_workers_total - num_workers_with_extra_item)\n        + [num_items_per_worker_adjusted - 1] * num_workers_with_extra_item\n    )\n    end_indices = np.cumsum(\n        [num_items_per_worker_adjusted - 1] * (num_workers_total - num_workers_with_extra_item)\n        + [num_items_per_worker_adjusted] * num_workers_with_extra_item\n        + [0]\n    )\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    items_per_worker = [\n        user_items[start:end]\n        for start, end in zip(start_indices, end_indices)\n        if start <= end\n    ]\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(items_per_worker) != num_workers_total:\n        raise RuntimeError(\n            f\"Improper assignment of items to workers. Expected {num_workers_total} workers but got {len(items_per_worker)}.\"\n        )\n\n    return items_per_"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    total_num_workers = num_nodes * num_workers\n    num_items_per_worker = len(user_items) // total_num_workers\n    num_items_per_worker_list = [num_items_per_worker] * total_num_workers\n    num_items_per_worker_list_cumsum = np.cumsum(num_items_per_worker_list)\n    num_items_per_worker_list_cumsum = [\n        item - num_items_per_worker for item in num_items_per_worker_list_cumsum\n    ]\n    num_items_per_worker_list_cumsum = [0] + num_items_per_worker_list_cumsum\n    num_items_per_worker_list_cumsum[-1] += len(user_items) % total_num_workers\n\n    output = []\n    for i in range(total_num_workers):\n        start_idx = num_items_per_worker_list_cumsum[i]\n        end_idx = num_items_per_worker_list_cumsum[i + 1]\n        output.append(user_items[start_idx:end_idx])\n\n    if len(output) != total_num_workers:\n        raise RuntimeError(\n            f\"Error: data sizes mismatch. \"\n            f\"Output length is {len(output)} but it should be {total_num_workers}\"\n        )\n    return output\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    num_workers_total = num_workers * num_nodes\n    num_items = len(user_items)\n    num_items_per_worker = num_items // num_workers_total\n    num_items_remainder = num_items % num_workers_total\n\n    # Adjust for remainder\n    num_items_per_worker_adjusted = num_items_per_worker + (num_items_remainder > num_workers_total)\n    num_items_remainder_adjusted = num_items_remainder - (num_items_remainder > num_workers_total)\n\n    # Calculate start and end indices for each worker\n    worker_start_indices = [i * num_items_per_worker_adjusted for i in range(num_workers_total)]\n    worker_end_indices = [(i + 1) * num_items_per_worker_adjusted for i in range(num_workers_total)]\n\n    # Distribute remainder items to the last workers\n    for i in range(num_items_remainder_adjusted):\n        worker_end_indices[i] += 1\n\n    # Calculate cumulative sum of items for each worker\n    worker_cumsum = np.cumsum(worker_end_indices)\n\n    # Map items to workers\n    worker_items = [user_items[start:end] for start, end in zip(worker_cumsum[:-1], worker_cumsum[1:])]\n\n    # Check if the output list has the correct length\n    if len(worker_items) != num_workers_total:\n        raise RuntimeError(\n            f\"Error: Length of worker_items ({len(worker_items)}) does not match num_workers_total ({num_workers_total}).\"\n        )\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of nodes and the current node's rank\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    num_items = len(user_items)\n    num_items_per_worker = num_items // total_workers\n\n    # Calculate the number of workers in the current node\n    num_workers_in_node = num_workers if node_rank < num_nodes - 1 else num_workers - (total_workers % num_workers)\n\n    # Calculate the start and end indices for each worker\n    worker_indices = [i * num_items_per_worker for i in range(num_workers_in_node + 1)]\n    worker_indices[-1] = num_items\n\n    # Distribute the remainder of items among the last workers in the list\n    remainder = num_items % total_workers\n    for i in range(remainder):\n        worker_indices[-(i + 1)] += 1\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = []\n    for i in range(num_workers_in_node):\n        start = worker_indices[i]\n        end = worker_indices[i + 1]\n        worker_items.append(user_items[start:end])\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers_in_node:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def clean_cache(self):\n            \"\"\"\n            The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n            Input-Output Arguments\n            :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n            :return: No return values. This method performs operations on the filesystem but does not return any value.\n            \"\"\"\n            shutil.rmtree(self.cache_dir)\n            os.makedirs(self.cache_dir)\n\n\n        \"\"\"\n        The function cleans up the output directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        def clean_output(self):\n            \"\"\"\n            The function cleans up the output directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n            Input-Output Arguments\n            :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n            :return: No return values. This method performs operations on the filesystem but does not return any value.\n            \"\"\"\n            shutil.rmtree(self.output_dir)\n            os.makedirs(self.output_dir)\n\n\n        \"\"\"\n        The function cleans up the cache and output directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n       "}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def clean_cache(self):\n            import shutil\n            import os\n            try:\n                shutil.rmtree(self.cache_dir)\n                os.makedirs(self.cache_dir)\n            except OSError as e:\n                print(e)\n\n\n        \"\"\"\n        The function creates a cache file by concatenating the cache directory path and the filename, and then writing the data to the file.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param filename: str. The name of the file to be created.\n        :param data: str. The data to be written to the file.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        def create_cache_file(self, filename, data):\n            import os\n            with open(os.path.join(self.cache_dir, filename), 'w') as f:\n                f.write(data)\n\n\n        \"\"\"\n        The function checks if a cache file exists by concatenating the cache directory path and the filename, and then checking if the file exists.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param filename: str. The name of the file to be checked.\n        :return: bool. True if the file exists, False otherwise.\n        \"\"\"\n        def cache_file_exists(self, filename):\n            import os\n            return os.path.exists(os.path.join(self.cache_dir, filename))\n\n\n        \"\"\"\n        The function reads a cache file by concatenating the cache directory path and the filename, and then reading the data from the file.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param filename: str. The name of the file to be read.\n        :return: str. The data read from the file.\n        \"\"\"\n        def read_cache_file(self,"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def cleanup_cache_dirs(self):\n            if os.path.exists(self.cache_dir):\n                shutil.rmtree(self.cache_dir)\n            os.makedirs(self.cache_dir)\n            os.makedirs(self.cache_dir_train)\n            os.makedirs(self.cache_dir_test)\n\n\n        \"\"\"\n        The function takes in a list of file paths and a cache directory path, and copies the files from the list to the cache directory.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param file_list: list. A list of file paths to be copied to the cache directory.\n        :param cache_dir: str. The path to the cache directory where the files will be copied.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        def copy_files_to_cache(self, file_list, cache_dir):\n            for file_path in file_list:\n                shutil.copy(file_path, cache_dir)\n\n\n        \"\"\"\n        The function takes in a list of file paths and a cache directory path, and copies the files from the cache directory to the list.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param file_list: list. A list of file paths to be copied from the cache directory.\n        :param cache_dir: str. The path to the cache directory where the files are located.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        def copy_files_from_cache(self, file_list, cache_dir):\n            for file_path in file_list:\n                shutil.copy(os.path.join(cache_dir, os.path.basename(file_path)), file_path)\n\n\n        \"\"\"\n        The function takes in a list of file paths and a cache directory path,"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def cleanup_cache_dirs(self):\n            if os.path.exists(self.cache_dir):\n                shutil.rmtree(self.cache_dir)\n            os.makedirs(self.cache_dir)\n            os.makedirs(os.path.join(self.cache_dir, 'train'))\n            os.makedirs(os.path.join(self.cache_dir, 'test'))\n\n\n        \"\"\"\n        The function reads a text file and returns its contents as a string.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param file_path: str. The path to the text file to be read.\n        :return: str. The contents of the text file as a string.\n        \"\"\"\n\n        def read_text_file(self, file_path):\n            with open(file_path, 'r', encoding='utf-8') as f:\n                text = f.read()\n            return text\n\n\n        \"\"\"\n        The function reads a CSV file and returns its contents as a list of dictionaries.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param file_path: str. The path to the CSV file to be read.\n        :return: list. A list of dictionaries, where each dictionary represents a row in the CSV file.\n        \"\"\"\n\n        def read_csv_file(self, file_path):\n            with open(file_path, 'r', encoding='utf-8') as f:\n                reader = csv.DictReader(f)\n                rows = list(reader)\n            return rows\n\n\n        \"\"\"\n        The function writes a list of dictionaries to a CSV file.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param rows: list. A list of dictionaries to be written to the CSV file.\n        :param"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def clean_cache(self):\n            import os\n            import shutil\n            if os.path.exists(self.cache_dir):\n                shutil.rmtree(self.cache_dir)\n            os.makedirs(self.cache_dir)\n\n\n        \"\"\"\n        The function creates a new directory for the given split and returns the path to that directory.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param split: str. The name of the split for which the directory is to be created.\n        :return: str. The path to the newly created directory.\n        \"\"\"\n\n\n        def make_split_dir(self, split):\n            split_dir = os.path.join(self.cache_dir, split)\n            os.makedirs(split_dir, exist_ok=True)\n            return split_dir\n\n\n        \"\"\"\n        The function saves a DataFrame to a CSV file in the cache directory for the given split.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param split: str. The name of the split for which the DataFrame is to be saved.\n        :param df: pd.DataFrame. The DataFrame to be saved.\n        :param name: str. The name of the file to be saved.\n        :return: No return values. This method saves the DataFrame to a CSV file in the cache directory for the given split.\n        \"\"\"\n\n\n        def save_df_to_cache(self, split, df, name):\n            split_dir = self.make_split_dir(split)\n            df.to_csv(os.path.join(split_dir, name), index=False)\n\n\n        \"\"\"\n        The function loads a DataFrame from a CSV file in the cache directory for the given split.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param split"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def __init__(self):\n            self.clean_cache()\n            self.download_data()\n            self.preprocess_data()\n\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n\n        def clean_cache(self):\n            # First, we need to check if the cache exists\n            if os.path.exists(self.cache_dir):\n                # If it does, we need to remove it\n                shutil.rmtree(self.cache_dir)\n            # Then, we need to create the cache directory\n            os.makedirs(self.cache_dir)\n\n\n        \"\"\"\n        The function downloads the data from the URL specified in the class variable `data_url` and saves it to the file specified in the class variable `data_file`.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n\n        def download_data(self):\n            # First, we need to check if the data file exists\n            if not os.path.exists(self.data_file):\n                # If it doesn't, we need to download it\n                urlretrieve(self.data_url, self.data_file)\n\n\n        \"\"\"\n        The function preprocesses the data by converting it to a Pandas DataFrame and saving it to a CSV file.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\""}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def clean_cache(self):\n            dir_path = self.cache_dir\n            if os.path.exists(dir_path):\n                shutil.rmtree(dir_path)\n            os.makedirs(dir_path)\n\n\n        \"\"\"\n        The function takes a file name and returns the file name with the extension .json.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param file_name: str. The name of the file.\n        :return: str. The name of the file with the extension .json.\n        \"\"\"\n\n        def get_cache_file_name(self, file_name):\n            return file_name + '.json'\n\n\n        \"\"\"\n        The function takes a file name and returns the path to the file.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param file_name: str. The name of the file.\n        :return: str. The path to the file.\n        \"\"\"\n\n        def get_cache_file_path(self, file_name):\n            return os.path.join(self.cache_dir, file_name)\n\n\n        \"\"\"\n        The function takes a file name and returns the data stored in the file.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param file_name: str. The name of the file.\n        :return: dict. The data stored in the file.\n        \"\"\"\n\n        def get_cache_data(self, file_name):\n            file_path = self.get_cache_file_path(file_name)\n            with open(file_path, 'r') as f:\n                data = json.load(f)\n            return data\n\n\n        \"\"\"\n        The function takes a file name and data and stores the data in the file.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def clean_cache(self):\n            shutil.rmtree(CACHE_DIR, ignore_errors=True)\n            os.makedirs(CACHE_DIR)\n\n\n        \"\"\"\n        The function downloads a file from a given URL and saves it to a specified directory.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param url: str. The URL of the file to be downloaded.\n        :param cache_dir: str. The directory where the downloaded file will be saved.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n\n        def simple_download(self, url, cache_dir=CACHE_DIR):\n            filename = url.split(\"/\")[-1]\n            file_path = os.path.join(cache_dir, filename)\n            if not os.path.exists(file_path):\n                with open(file_path, \"wb\") as file:\n                    response = requests.get(url)\n                    file.write(response.content)\n\n\n        \"\"\"\n        The function retrieves the file path of a cached file based on the URL of the file.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param url: str. The URL of the file to be downloaded.\n        :param cache_dir: str. The directory where the downloaded file will be saved.\n        :return: str. The file path of the cached file.\n        \"\"\"\n\n\n        def get_cache_file(self, url, cache_dir=CACHE_DIR):\n            filename = url.split(\"/\")[-1]\n            file_path = os.path.join(cache_dir, filename)\n            return file_path\n\n\n        \"\"\"\n        The function downloads a file from a given URL and saves it to a specified directory.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def __init__(self, self):\n            self.clean_cache()\n            self.cache_dir = os.path.join(self.data_dir, 'cache')\n            self.cache_file = os.path.join(self.cache_dir, 'cache.pkl')\n\n\n        def clean_cache(self):\n            if os.path.exists(self.cache_dir):\n                shutil.rmtree(self.cache_dir)\n            os.makedirs(self.cache_dir)\n\n\n        def save_cache(self, cache):\n            with open(self.cache_file, 'wb') as f:\n                pickle.dump(cache, f)\n\n\n        def load_cache(self):\n            if os.path.exists(self.cache_file):\n                with open(self.cache_file, 'rb') as f:\n                    return pickle.load(f)\n            else:\n                return None\n\n\n        def process_data(self, data_dir):\n            cache = self.load_cache()\n            if cache is not None:\n                return cache\n            else:\n                # process data and save to cache\n                cache = {}\n                self.save_cache(cache)\n                return cache\n\n\n        def process_data(self, data_dir):\n            cache = self.load_cache()\n            if cache is not None:\n                return cache\n            else:\n                # process data and save to cache\n                cache = {}\n                self.save_cache(cache)\n                return cache\n\n\n        def process_data(self, data_dir):\n            cache = self.load_cache()\n            if cache is not None:\n                return cache\n            else:\n                # process data and save to cache\n                cache = {}\n                self.save_cache(cache)\n                return cache\n\n\n        def process_data(self, data_dir):\n            cache = self.load_cache()\n            if cache is not None:\n                return cache\n            else:\n                # process data and save to cache\n                cache = {}\n                self.save_cache(cache)\n               "}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def __init__(self, self):\n            self.clean_cache()\n            self.create_dir(self.data_dir)\n            self.create_dir(self.plot_dir)\n\n\n        \"\"\"\n        This function creates a directory if it does not already exist.\n\n        Input-Output Arguments\n        :param dir_path: str. The path of the directory to be created.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        def create_dir(self, dir_path):\n            if not os.path.exists(dir_path):\n                os.makedirs(dir_path)\n\n\n        \"\"\"\n        This function removes all files and subdirectories within a given directory.\n\n        Input-Output Arguments\n        :param dir_path: str. The path of the directory to be cleaned.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        def clean_dir(self, dir_path):\n            for filename in os.listdir(dir_path):\n                file_path = os.path.join(dir_path, filename)\n                try:\n                    if os.path.isfile(file_path) or os.path.islink(file_path):\n                        os.unlink(file_path)\n                    elif os.path.isdir(file_path):\n                        shutil.rmtree(file_path)\n                except Exception as e:\n                    print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n\n        \"\"\"\n        This function removes the cache directory and its contents, if it exists.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        def clean_cache(self):\n            if os.path.exists(self.cache_dir):\n                shutil.rmtree(self.cache_dir)\n\n\n        \"\"\"\n       "}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def __init__(self):\n\n            # Check if the cache directory exists and delete it if it does\n            if os.path.exists(self.cache_dir):\n                shutil.rmtree(self.cache_dir)\n\n            # Create the cache directory if it does not exist\n            if not os.path.exists(self.cache_dir):\n                os.makedirs(self.cache_dir)\n\n            # Check if the processed data directory exists and delete it if it does\n            if os.path.exists(self.processed_data_dir):\n                shutil.rmtree(self.processed_data_dir)\n\n            # Create the processed data directory if it does not exist\n            if not os.path.exists(self.processed_data_dir):\n                os.makedirs(self.processed_data_dir)\n\n\n        \"\"\"\n        The function downloads the dataset from the given URL and saves it to the specified directory.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n\n        def download_dataset(self):\n\n            # Download the dataset from the given URL and save it to the specified directory\n            urllib.request.urlretrieve(self.dataset_url, self.dataset_path)\n\n\n        \"\"\"\n        The function extracts the contents of a compressed file and saves them to a specified directory.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n\n        def extract_dataset(self):\n\n            # Open the compressed file in read mode\n            with open(self.dataset_path, 'rb') as f:\n\n                # Create a ZipFile object with the compressed file\n                with zipfile.ZipFile(f) as zip_ref:\n\n                    # Extract all the"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def clean_cache(self):\n            try:\n                shutil.rmtree(self.cache_dir)\n                os.makedirs(self.cache_dir)\n            except OSError as e:\n                if e.errno != errno.EEXIST:\n                    raise\n\n\n        \"\"\"\n        This function downloads the data from the specified URL, saves it to a local file, and returns the file path.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param data_url: str. The URL of the data to be downloaded.\n        :param cache_dir: str. The path to the cache directory where the downloaded data will be saved.\n        :return: str. The file path of the downloaded data.\n        \"\"\"\n        def fetch_and_cache(self, data_url, cache_dir):\n            os.makedirs(cache_dir, exist_ok=True)\n            filename = data_url.split(\"/\")[-1]\n            file_path = os.path.join(cache_dir, filename)\n            if not os.path.exists(file_path):\n                urllib.request.urlretrieve(data_url, file_path)\n            return file_path\n\n\n        \"\"\"\n        This function reads the data from a local file, cleans it, and returns it as a pandas DataFrame.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param data_dir: str. The path to the directory containing the data file.\n        :param cache_dir: str. The path to the cache directory where the downloaded data will be saved.\n        :return: pd.DataFrame. The cleaned data as a pandas DataFrame.\n        \"\"\"\n        def read_data(self, data_dir, cache_dir):\n            file_path = self.fetch_and_cache(data_url=data_dir, cache_dir=cache_dir)\n            df = pd.read_csv(file_path)\n            df"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "    def clean_cache(self):\n        shutil.rmtree(self.cache_dir, ignore_errors=True)\n        os.makedirs(self.cache_dir)\n\n\n        \"\"\"\n        The function checks if the cache directory is empty and if it is, it returns False. If the cache directory is not empty, it returns True.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: Boolean. Returns False if the cache directory is empty, and True if it is not empty.\n        \"\"\"\n\n\n    def is_cache_empty(self):\n        return len(os.listdir(self.cache_dir)) == 0\n\n\n        \"\"\"\n        The function checks if the cache is empty and if it is, it returns True. If the cache is not empty, it returns False.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: Boolean. Returns True if the cache is empty, and False if it is not empty.\n        \"\"\"\n\n\n    def has_cache(self):\n        return not self.is_cache_empty()\n\n\n        \"\"\"\n        The function checks if the cache directory is empty and if it is, it returns True. If the cache directory is not empty, it returns False.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: Boolean. Returns True if the cache directory is empty, and False if it is not empty.\n        \"\"\"\n\n\n    def is_cache_empty(self):\n        return len(os.listdir(self.cache_dir)) == 0\n\n\n        \"\"\"\n        The function checks if the cache is empty and if it is, it returns True. If the cache is not empty, it returns False.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: Boolean. Returns True if"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def clean_cache(self):\n            if os.path.exists(self.cache_dir):\n                shutil.rmtree(self.cache_dir)\n            os.makedirs(self.cache_dir)\n\n\n        \"\"\"\n        The function splits the dataset into training and testing sets.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param split_ratio: float. The ratio of the dataset to be used for training. The remaining portion will be used for testing.\n        :param random_state: int. The random seed to be used for reproducibility.\n        :return: tuple. A tuple containing the training and testing sets.\n        \"\"\"\n\n\n        def split_dataset(self, split_ratio, random_state):\n            dataset = self.dataset\n            train_dataset, test_dataset = train_test_split(dataset, test_size=split_ratio, random_state=random_state)\n            return train_dataset, test_dataset\n\n\n        \"\"\"\n        The function saves the training and testing sets to a cache directory.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param train_dataset: pandas DataFrame. The training set.\n        :param test_dataset: pandas DataFrame. The testing set.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n\n        def save_to_cache(self, train_dataset, test_dataset):\n            train_path = os.path.join(self.cache_dir, 'train.csv')\n            test_path = os.path.join(self.cache_dir, 'test.csv')\n            train_dataset.to_csv(train_path, index=False)\n            test_dataset.to_csv(test_path, index=False)\n\n\n        \"\"\"\n        The function loads the training and testing sets from a cache directory.\n\n        Input-Output Arguments\n        :param self: DataProcessor"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        @staticmethod\n        def cleanup_cache_directories(self):\n            \"\"\"\n            The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n            Input-Output Arguments\n            :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n            :return: No return values. This method performs operations on the filesystem but does not return any value.\n            \"\"\"\n            import os\n            import shutil\n\n            # Remove the cache directory if it exists\n            if os.path.exists(self.cache_directory):\n                shutil.rmtree(self.cache_directory)\n\n            # Create the cache directory if it does not exist\n            if not os.path.exists(self.cache_directory):\n                os.makedirs(self.cache_directory)\n\n            # Remove the cache directory if it exists\n            if os.path.exists(self.cache_directory_full):\n                shutil.rmtree(self.cache_directory_full)\n\n            # Create the cache directory if it does not exist\n            if not os.path.exists(self.cache_directory_full):\n                os.makedirs(self.cache_directory_full)\n\n        \"\"\"\n        The function creates a dictionary of data by reading data from a file and storing it in a dictionary.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: dict. A dictionary containing the data read from the file.\n        \"\"\"\n        @staticmethod\n        def create_data_dictionary(self):\n            \"\"\"\n            The function creates a dictionary of data by reading data from a file and storing it in a dictionary.\n\n            Input-Output Arguments\n            :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n            :return: dict. A dictionary containing the data read from the file.\n            \"\"\"\n            import json\n\n            # Open the file and load the data into a dictionary\n           "}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def clean_cache(self):\n            pass\n\n\n\n        \"\"\"\n        The function checks whether a cache file exists and if it does, it returns the contents of the file. If the file does not exist, it returns None.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param cache_file_name: str. The name of the cache file to be checked.\n        :return: str or None. If the cache file exists, it returns the contents of the file. If the file does not exist, it returns None.\n        \"\"\"\n\n\n        def check_cache(self, cache_file_name):\n            pass\n\n\n\n        \"\"\"\n        The function checks if a cache file exists, if it does, it returns the contents of the file, otherwise it calls the function to process the data and save the results to a cache file.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param cache_file_name: str. The name of the cache file to be checked.\n        :param function: function. The function to be called if the cache file does not exist.\n        :param kwargs: dict. The keyword arguments to be passed to the function.\n        :return: str or None. If the cache file exists, it returns the contents of the file. If the file does not exist, it returns None.\n        \"\"\"\n\n\n        def process_data(self, cache_file_name, function, **kwargs):\n            pass\n\n\n\n        \"\"\"\n        The function checks if a cache file exists, if it does, it returns the contents of the file, otherwise it calls the function to process the data and save the results to a cache file.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param cache_file_name: str. The name of the cache file to be checked.\n        :param function: function. The function to be called if the cache file does not exist.\n        :"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def cleanup(self):\n\n\n            # Check if the cache directory exists and delete it if it does\n            if os.path.exists(self.cache_dir):\n                shutil.rmtree(self.cache_dir)\n\n            # Create the cache directory\n            os.makedirs(self.cache_dir)\n\n            # Check if the output directory exists and delete it if it does\n            if os.path.exists(self.output_dir):\n                shutil.rmtree(self.output_dir)\n\n            # Create the output directory\n            os.makedirs(self.output_dir)\n\n\n        \"\"\"\n        The function loads a dataset from a file path and returns a pandas DataFrame containing the data.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param file_path: str. The path to the file containing the dataset.\n        :return: pandas.DataFrame. A DataFrame containing the loaded dataset.\n        \"\"\"\n\n\n        def load_dataset(self, file_path):\n\n\n            # Load the dataset from the file path\n            dataset = pd.read_csv(file_path, sep='\\t', header=None, names=['sequence', 'label'])\n\n            # Return the loaded dataset\n            return dataset\n\n\n        \"\"\"\n        The function saves a dataset to a file path.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :param dataset: pandas.DataFrame. The dataset to be saved.\n        :param file_path: str. The path to the file where the dataset will be saved.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n\n        def save_dataset(self, dataset, file_path):\n\n\n            # Save the dataset to the file path\n            dataset.to_csv(file_path, sep='\\t', index=False, header=False)\n\n\n        \"\"\"\n        The function creates a"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def clean_up_cache_directories(self):\n            # remove cache directories if they exist to prevent issues from corrupted files from previous runs\n            if os.path.exists(self.cache_dir):\n                shutil.rmtree(self.cache_dir)\n\n            # recreate cache directories to ensure they are available for use\n            os.makedirs(self.cache_dir, exist_ok=True)\n            os.makedirs(self.cache_dir_train, exist_ok=True)\n            os.makedirs(self.cache_dir_test, exist_ok=True)\n\n\n        \"\"\"\n        The function loads the data from a cache file if it exists, otherwise it creates the cache file by calling the create_cache method.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        def load_cache(self):\n            # load the cache file if it exists\n            if os.path.exists(self.cache_file):\n                with open(self.cache_file, 'rb') as f:\n                    self.cache = pickle.load(f)\n\n            # create the cache file if it does not exist\n            else:\n                self.create_cache()\n\n\n        \"\"\"\n        The function creates a cache file by calling the create_cache method, which creates a dictionary of data and stores it in the cache file.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        def create_cache(self):\n            # create the cache file\n            self.cache = self.create_cache()\n            with open(self.cache_file, 'wb') as f:\n                pickle.dump(self.cache, f)\n\n\n        \"\"\"\n        The function creates a cache by calling the create_cache method, which creates"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def clean_cache(self):\n            import os\n            import shutil\n\n            # Check if the cache directory exists and delete it if it does\n            if os.path.exists(self.cache_dir):\n                shutil.rmtree(self.cache_dir)\n\n            # Create the cache directory\n            os.makedirs(self.cache_dir)\n\n\n        \"\"\"\n        The function creates a dataset by downloading the data, extracting it, and loading it into memory.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        def make_dataset(self):\n            import os\n            import requests\n            import zipfile\n\n            # Check if the data directory exists and delete it if it does\n            if os.path.exists(self.data_dir):\n                shutil.rmtree(self.data_dir)\n\n            # Create the data directory\n            os.makedirs(self.data_dir)\n\n            # Download the data\n            url = self.config['data']['url']\n            filename = url.split('/')[-1]\n            filepath = os.path.join(self.data_dir, filename)\n            response = requests.get(url, stream=True)\n            with open(filepath, 'wb') as f:\n                f.write(response.content)\n\n            # Extract the data\n            with zipfile.ZipFile(filepath, 'r') as zip_ref:\n                zip_ref.extractall(self.data_dir)\n\n            # Load the data into memory\n            self.data = []\n            for root, dirs, files in os.walk(self.data_dir):\n                for file in files:\n                    filepath = os.path.join(root, file)\n                    with open(filepath, 'r') as f:\n                        self.data.append(f.read())\n\n\n        \"\"\"\n        The function returns the data that has been loaded into memory."}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def __init__(self):\n            self.clean_cache()\n            self.create_cache_dir()\n\n\n        def clean_cache(self):\n            \"\"\"\n            This method removes the cache directory if it exists.\n\n            Input-Output Arguments\n            :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n            :return: No return values. This method performs operations on the filesystem but does not return any value.\n            \"\"\"\n            if os.path.exists(self.cache_dir):\n                shutil.rmtree(self.cache_dir)\n\n\n        def create_cache_dir(self):\n            \"\"\"\n            This method creates the cache directory if it does not exist.\n\n            Input-Output Arguments\n            :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n            :return: No return values. This method performs operations on the filesystem but does not return any value.\n            \"\"\"\n            if not os.path.exists(self.cache_dir):\n                os.makedirs(self.cache_dir)\n\n\n        def process_data(self, data_dir):\n            \"\"\"\n            This method processes the data by performing the following steps:\n            - Cleaning the cache.\n            - Creating the cache directory.\n            - Loading the data from the specified data directory.\n            - Processing the data by removing the stop words, lemmatizing the words, and converting the words to lowercase.\n            - Splitting the data into training and testing sets.\n            - Saving the processed data to the cache directory.\n\n            Input-Output Arguments\n            :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n            :param data_dir: str. The directory containing the data files.\n            :return: tuple. A tuple containing the processed data.\n            \"\"\"\n            # Clean the cache\n            self.clean_cache()\n\n            # Create the cache directory\n            self.create_cache_dir()\n\n            # Load the data\n            data = self.load_data(data_dir)"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n        return [future.result() for future in futures]"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    num_cpus = multiprocessing.cpu_count()\n    with ThreadPoolExecutor(max_workers=num_cpus) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n        return [future.result() for future in futures]"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n        return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n        return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n        return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n        return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    # Get the number of CPUs available for parallel processing\n    num_cpus = os.cpu_count()\n\n    # Determine the number of workers to use based on the number of CPUs\n    num_workers = min(num_cpus, len(items))\n\n    # Create a ThreadPoolExecutor with the determined number of workers\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        # Submit tasks to the executor to compute the file size for each item in parallel\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    # Wait for all tasks to complete and retrieve the results\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n        return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    # Determine the number of workers based on the CPU count\n    num_workers = min(32, (os.cpu_count() or 1) + 4)\n\n    # Create a ThreadPoolExecutor with the determined number of workers\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        # Submit the file size retrieval tasks to the executor and collect the results\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    # Wait for all tasks to complete and retrieve the results\n    results = [future.result() for future in futures]\n\n    return results\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    num_workers = cpu_count()\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        element = str(Path(element).absolute())\n    if input_dir is None:\n        return os.path.exists(element)\n    else:\n        input_dir = str(Path(input_dir).absolute())\n        return element.startswith(input_dir) or os.path.exists(element)"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        if os.path.exists(element):\n            return True\n        elif input_dir:\n            if os.path.exists(os.path.join(input_dir, element)):\n                return True\n    return False"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        input_dir = str(Path(input_dir).absolute())\n        if isinstance(element, str):\n            element = str(Path(element).absolute())\n            return element.startswith(input_dir)\n        else:\n            return False\n    else:\n        if isinstance(element, str):\n            return Path(element).exists()\n        else:\n            return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        input_dir = str(Path(input_dir).absolute())\n\n    if isinstance(element, str):\n        element = str(Path(element).absolute())\n\n    if isinstance(element, str):\n        if input_dir is not None:\n            return element.startswith(input_dir)\n        else:\n            return os.path.exists(element)\n    else:\n        return False"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        if isinstance(element, str):\n            element = Path(element)\n            if element.is_absolute():\n                return element.is_relative_to(input_dir)\n            else:\n                return Path(input_dir, element).is_relative_to(input_dir)\n        else:\n            return False\n    else:\n        if isinstance(element, str):\n            element = Path(element)\n            if element.is_absolute():\n                return element.exists()\n            else:\n                return Path(element).exists()\n        else:\n            return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        input_dir = str(Path(input_dir).absolute())\n\n    if isinstance(element, str):\n        element = str(Path(element).absolute())\n\n    if input_dir is not None:\n        if element.startswith(input_dir):\n            return True\n\n    return Path(element).exists()\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        input_dir = str(Path(input_dir).absolute())\n        if isinstance(element, str):\n            element = str(Path(element).absolute())\n            if element.startswith(input_dir):\n                return True\n            else:\n                return False\n        else:\n            return False\n    else:\n        if isinstance(element, str):\n            element = str(Path(element).absolute())\n            if os.path.exists(element):\n                return True\n            else:\n                return False\n        else:\n            return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        element = str(element)\n\n    if input_dir is not None and not element.startswith(input_dir):\n        return False\n\n    if os.path.exists(element):\n        return True\n\n    if os.path.exists(Path(element).absolute()):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    # Check if the element is a string\n    if isinstance(element, str):\n        # Check if the element is an absolute path\n        if os.path.isabs(element):\n            # If it's an absolute path, check if it exists in the file system\n            return os.path.exists(element)\n        else:\n            # If it's a relative path, check if it exists within the input directory\n            if input_dir is not None:\n                return os.path.exists(os.path.join(input_dir, element))\n            else:\n                return False\n    else:\n        return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is None:\n        return os.path.exists(element)\n    else:\n        if os.path.isabs(element):\n            return element.startswith(input_dir)\n        else:\n            return os.path.exists(os.path.join(input_dir, element))\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is None:\n        return True\n    if isinstance(element, str):\n        element = str(Path(element).absolute())\n        if element.startswith(input_dir):\n            return True\n        if Path(element).exists():\n            return True\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        if input_dir is None:\n            return os.path.exists(element)\n        else:\n            return element.startswith(input_dir) or os.path.exists(element)\n    else:\n        return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, Path):\n        element = str(element.absolute())\n\n    if isinstance(element, str):\n        if input_dir is not None and element.startswith(input_dir):\n            return True\n        else:\n            return Path(element).exists()\n    else:\n        return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        input_dir = str(Path(input_dir).absolute())\n        if not element.startswith(input_dir):\n            element = os.path.join(input_dir, element)\n    if isinstance(element, str):\n        element = str(Path(element).absolute())\n        return os.path.exists(element)\n    else:\n        return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        if input_dir is not None and Path(element).is_absolute():\n            element = Path(input_dir) / element\n        return Path(element).exists()\n    else:\n        return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        if input_dir is not None:\n            if not element.startswith(input_dir):\n                element = os.path.join(input_dir, element)\n        return os.path.exists(element)\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        element = str(Path(element).absolute())\n    if input_dir is not None:\n        input_dir = str(Path(input_dir).absolute())\n        if input_dir in element:\n            return True\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        element = str(element)\n\n    if input_dir is not None and not isinstance(input_dir, str):\n        input_dir = str(input_dir)\n\n    if input_dir is not None:\n        input_dir = Path(input_dir).absolute()\n\n    if not is_path(element):\n        return False\n\n    if input_dir is not None:\n        return element.startswith(input_dir)\n\n    return Path(element).exists()\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        if os.path.isabs(element):\n            element = os.path.abspath(element)\n        else:\n            element = os.path.abspath(os.path.join(input_dir, element))\n        return os.path.exists(element)\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        if input_dir is not None:\n            if element.startswith(input_dir):\n                return True\n        return os.path.exists(element)\n    return False\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons < 64:\n                network_type = \"TinyFullyFusedMLP\"\n            else:\n                network_type = \"TinyMLP\"\n\n            network = getattr(tcnn, network_type)(\n                n_input_dims,\n                n_output_dims,\n                n_layers,\n                n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n        else:\n            network = nn.Sequential(\n                nn.Linear(n_input_dims, n_neurons),\n                getattr(nn, activation)(),\n                *[\n                    nn.Linear(n_neurons, n_neurons)\n                    for _ in range(n_layers - 2)\n                ],\n                nn.Linear(n_neurons, n_output_dims),\n                getattr(nn, output_activation)(),\n            )\n\n        return network\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons <= 64:\n                network_type = \"TinyFullyFusedMLP\"\n            else:\n                network_type = \"FullyFusedMLP\"\n\n            network = getattr(tcnn, network_type)(\n                n_input_dims,\n                n_output_dims,\n                n_layers,\n                n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n        else:\n            network = nn.Sequential(\n                nn.Linear(n_input_dims, n_neurons),\n                getattr(nn, activation)(),\n                *[\n                    nn.Linear(n_neurons, n_neurons)\n                    for _ in range(n_layers - 2)\n                ],\n                nn.Linear(n_neurons, n_output_dims),\n                getattr(nn, output_activation)(),\n            )\n\n        return network"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons < 1024:\n                network_type = \"FullyFusedMLP\"\n            else:\n                network_type = \"MLP\"\n            network = getattr(tcnn, network_type)(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_layers=n_layers,\n                n_neurons=n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n        else:\n            network = nn.Sequential(\n                nn.Linear(n_input_dims, n_neurons),\n                getattr(nn, activation)(),\n                *[\n                    nn.Linear(n_neurons, n_neurons)\n                    for _ in range(n_layers - 2)\n                ],\n                nn.Linear(n_neurons, n_output_dims),\n                getattr(nn, output_activation)(),\n            )\n\n        return network\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons <= 128:\n                network_tcnn = tcnn.Network(\n                    n_input_dims,\n                    n_output_dims,\n                    network_config={\n                        \"otype\": \"FullyFusedMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": output_activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 1,\n                    },\n                )\n            else:\n                network_tcnn = tcnn.Network(\n                    n_input_dims,\n                    n_output_dims,\n                    network_config={\n                        \"otype\": \"CutlassMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": output_activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 1,\n                    },\n                )\n            return network_tcnn\n\n        else:\n            layers = [nn.Linear(n_input_dims, n_neurons)]\n            if activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif activation == \"None\":\n                pass\n            else:\n                raise ValueError(\"Invalid activation function\")\n\n            for i in range(n_layers - 2):\n                layers.append(nn.Linear(n_neurons, n_neurons))\n                if activation == \"ReLU\":\n                    layers.append(nn.ReLU())\n                elif activation == \"None\":\n                    pass\n                else:\n                    raise ValueError(\"Invalid activation function\")\n\n            layers.append(nn.Linear(n_neurons, n_output_dims))\n            if output_activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                layers.append("}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"Number of layers must be greater than 0\"\n        assert n_neurons > 0, \"Number of neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons <= 128:\n                network_type = tcnn.Network\n            elif n_neurons <= 512:\n                network_type = tcnn.NetworkWithLoss\n            else:\n                network_type = tcnn.NetworkWithLoss\n\n            network = network_type(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_layers=n_layers,\n                n_neurons=n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n        else:\n            network = nn.Sequential(\n                nn.Linear(n_input_dims, n_neurons),\n                getattr(nn, activation)(),\n                *[\n                    nn.Linear(n_neurons, n_neurons)\n                    for _ in range(n_layers - 2)\n                ],\n                nn.Linear(n_neurons, n_output_dims),\n                getattr(nn, output_activation)(),\n            )\n\n        return network"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons < 16:\n                network_type = \"TinyFullyFusedMLP\"\n            else:\n                network_type = \"FullyFusedMLP\"\n\n            network = getattr(tcnn, network_type)(\n                n_input_dims,\n                n_output_dims,\n                n_layers,\n                n_neurons,\n                activation=getattr(nn, activation) if activation != \"None\" else nn.Identity(),\n                output_activation=getattr(nn, output_activation)\n                if output_activation != \"None\"\n                else nn.Identity(),\n            ).to(self.device)\n        else:\n            network = nn.Sequential(\n                nn.Linear(n_input_dims, n_neurons),\n                getattr(nn, activation)() if activation != \"None\" else nn.Identity(),\n                *[\n                    nn.Linear(n_neurons, n_neurons)\n                    for _ in range(n_layers - 2)\n                ],\n                nn.Linear(n_neurons, n_output_dims),\n                getattr(nn, output_activation)()\n                if output_activation != \"None\"\n                else nn.Identity(),\n            ).to(self.device)\n\n        return network"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons < 64:\n                network_tcnn = tcnn.Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    network_config={\n                        \"otype\": \"FullyFusedMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": output_activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 1,\n                    },\n                )\n            else:\n                network_tcnn = tcnn.Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    network_config={\n                        \"otype\": \"CutlassMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": output_activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 1,\n                    },\n                )\n            return network_tcnn\n        else:\n            network_torch = nn.Sequential(\n                nn.Linear(n_input_dims, n_neurons),\n                getattr(nn, activation)(),\n                *[\n                    nn.Linear(n_neurons, n_neurons)\n                    for _ in range(n_layers - 2)\n                ],\n                nn.Linear(n_neurons, n_output_dims),\n                getattr(nn, output_activation)(),\n            )\n            return network_torch\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons <= 128:\n                network_type = \"TinyFullyFusedMLP\"\n            elif n_neurons <= 256:\n                network_type = \"TinyMLP\"\n            else:\n                network_type = \"FullyFusedMLP\"\n\n            return getattr(tcnn, network_type)(\n                n_input_dims,\n                n_output_dims,\n                n_layers,\n                n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n        else:\n            layers = [\n                nn.Linear(n_input_dims, n_neurons),\n                getattr(nn, activation)() if activation != \"None\" else nn.Identity(),\n            ]\n            for _ in range(n_layers - 2):\n                layers.append(nn.Linear(n_neurons, n_neurons))\n                layers.append(\n                    getattr(nn, activation)() if activation != \"None\" else nn.Identity()\n                )\n            layers.append(nn.Linear(n_neurons, n_output_dims))\n            layers.append(\n                getattr(nn, output_activation)()\n                if output_activation != \"None\"\n                else nn.Identity()\n            )\n            return nn.Sequential(*layers)"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            # tinycudann\n            if n_neurons < 16:\n                tcnn_network_fn = tcnn.Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    network_config={\n                        \"otype\": \"FullyFusedMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": output_activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 1,\n                    },\n                )\n            else:\n                tcnn_network_fn = tcnn.Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    network_config={\n                        \"otype\": \"CutlassMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": output_activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 1,\n                    },\n                )\n            return tcnn_network_fn\n        else:\n            # pytorch\n            layers = [nn.Linear(n_input_dims, n_neurons)]\n            for i in range(n_layers - 1):\n                if activation == \"ReLU\":\n                    layers.append(nn.ReLU())\n                layers.append(nn.Linear(n_neurons, n_neurons))\n            if output_activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                layers.append(nn.Sigmoid())\n            layers.append(nn.Linear(n_neurons, n_output_dims))\n            return nn.Sequential(*layers)"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons < 16:\n                network_type = \"fully_connected\"\n            elif n_neurons < 64:\n                network_type = \"resnet\"\n            else:\n                network_type = \"mlp\"\n\n            network = getattr(tcnn, network_type)(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_layers=n_layers,\n                n_neurons=n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n        else:\n            network = nn.Sequential(\n                nn.Linear(n_input_dims, n_neurons),\n                getattr(nn, activation)(),\n                *[\n                    nn.Linear(n_neurons, n_neurons)\n                    for _ in range(n_layers - 2)\n                ],\n                nn.Linear(n_neurons, n_output_dims),\n                getattr(nn, output_activation)(),\n            )\n\n        return network\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0 and n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons <= 128:\n                model = tcnn.Network(n_input_dims, n_output_dims, model_id=0, n_layers=n_layers,\n                                     n_neurons=n_neurons, activation=activation, output_activation=output_activation)\n            elif n_neurons <= 256:\n                model = tcnn.Network(n_input_dims, n_output_dims, model_id=1, n_layers=n_layers,\n                                     n_neurons=n_neurons, activation=activation, output_activation=output_activation)\n            elif n_neurons <= 512:\n                model = tcnn.Network(n_input_dims, n_output_dims, model_id=2, n_layers=n_layers,\n                                     n_neurons=n_neurons, activation=activation, output_activation=output_activation)\n            elif n_neurons <= 1024:\n                model = tcnn.Network(n_input_dims, n_output_dims, model_id=3, n_layers=n_layers,\n                                     n_neurons=n_neurons, activation=activation, output_activation=output_activation)\n            else:\n                model = tcnn.Network(n_input_dims, n_output_dims, model_id=4, n_layers=n_layers,\n                                     n_neurons=n_neurons, activation=activation, output_activation=output_activation)\n        else:\n            model = torch.nn.Sequential()\n            model.add_module(\"input\", torch.nn.Linear(n_input_dims, n_neurons))\n            model.add_module(\"activation\", get_activation(activation))\n            for i in range(1, n_layers - 1"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            network = tcnn.Network(\n                n_input_dims,\n                n_output_dims,\n                network_config={\n                    \"otype\": \"FullyFusedMLP\",\n                    \"activation\": activation,\n                    \"output_activation\": output_activation,\n                    \"n_neurons\": n_neurons,\n                    \"n_hidden_layers\": n_layers - 1,\n                },\n            )\n        else:\n            network = torch.nn.Sequential(\n                torch.nn.Linear(n_input_dims, n_neurons),\n                get_activation(activation),\n                *[\n                    torch.nn.Linear(n_neurons, n_neurons)\n                    for _ in range(n_layers - 2)\n                ],\n                torch.nn.Linear(n_neurons, n_output_dims),\n                get_activation(output_activation),\n            )\n\n        return network\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0 and n_neurons > 0\n\n        if self.tcnn:\n            # tinycudann\n            if n_neurons < 64:\n                network_tcnn = tcnn.Network(\n                    n_input_dims, n_output_dims, network_config={\n                        \"otype\": \"FullyFusedMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": output_activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 1,\n                    }\n                )\n            else:\n                network_tcnn = tcnn.Network(\n                    n_input_dims, n_output_dims, network_config={\n                        \"otype\": \"CutlassMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": output_activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 1,\n                    }\n                )\n            return network_tcnn\n\n        # PyTorch\n        network_torch = []\n        for i in range(n_layers - 1):\n            network_torch.append(nn.Linear(n_input_dims if i == 0 else n_neurons, n_neurons))\n            if activation == \"ReLU\":\n                network_torch.append(nn.ReLU())\n        network_torch.append(nn.Linear(n_input_dims if n_layers == 1 else n_neurons, n_output_dims))\n        if output_activation == \"ReLU\":\n            network_torch.append(nn.ReLU())\n        elif output_activation == \"Sigmoid\":\n            network_torch.append(nn.Sigmoid())\n        return nn.Sequential(*network_torch)\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons <= 128:\n                network_class = getattr(tcnn, \"Network_tcnn_n_neurons_128\")\n            elif n_neurons <= 256:\n                network_class = getattr(tcnn, \"Network_tcnn_n_neurons_256\")\n            elif n_neurons <= 512:\n                network_class = getattr(tcnn, \"Network_tcnn_n_neurons_512\")\n            elif n_neurons <= 1024:\n                network_class = getattr(tcnn, \"Network_tcnn_n_neurons_1024\")\n            elif n_neurons <= 2048:\n                network_class = getattr(tcnn, \"Network_tcnn_n_neurons_2048\")\n            elif n_neurons <= 4096:\n                network_class = getattr(tcnn, \"Network_tcnn_n_neurons_4096\")\n            else:\n                raise ValueError(\n                    f\"Unsupported number of neurons for tinycudann: {n_neurons}\"\n                )\n\n            network = network_class(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_layers=n_layers,\n                activation=activation,\n                output_activation=output_activation,\n            )\n\n        else:\n            network = nn.Sequential()\n            network.add_module(\n                \"input_layer\",\n                nn.Linear(n_input_dims, n_neurons),\n            )\n\n            if activation == \"ReLU\":\n                network.add_module(\"input_activation\", nn.ReLU())\n\n            for i in range(n_layers - "}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons <= 128:\n                network_type = \"TinyFullyFusedMLP\"\n            elif n_neurons <= 256:\n                network_type = \"TinyMLP\"\n            elif n_neurons <= 512:\n                network_type = \"FullyFusedMLP\"\n            elif n_neurons <= 1024:\n                network_type = \"MLP\"\n            else:\n                raise NotImplementedError(f\"Unsupported number of neurons: {n_neurons}\")\n\n            network_fn = getattr(tcnn, network_type)\n            network = network_fn(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_layers=n_layers,\n                n_neurons=n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n        else:\n            network = nn.Sequential()\n            network.append(nn.Linear(n_input_dims, n_neurons))\n            network.append(getattr(nn, activation)())\n            for _ in range(n_layers - 2):\n                network.append(nn.Linear(n_neurons, n_neurons))\n                network.append(getattr(nn, activation)())\n            network.append(nn.Linear(n_neurons, n_output_dims))\n            network.append(getattr(nn, output_activation)())\n\n        return network\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons < 16:\n                network_type = \"TinyFullyFusedMLP\"\n            else:\n                network_type = \"FullyFusedMLP\"\n            return getattr(tcnn, network_type)(\n                n_input_dims,\n                n_output_dims,\n                n_layers,\n                n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n        else:\n            network = nn.Sequential(\n                nn.Linear(n_input_dims, n_neurons),\n                nn.ReLU(inplace=True),\n            )\n            for _ in range(n_layers - 2):\n                network.append(nn.Linear(n_neurons, n_neurons))\n                network.append(nn.ReLU(inplace=True))\n            network.append(nn.Linear(n_neurons, n_output_dims))\n            if output_activation == \"ReLU\":\n                network.append(nn.ReLU(inplace=True))\n            elif output_activation == \"Sigmoid\":\n                network.append(nn.Sigmoid())\n            return network\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            # tinycudann\n            if n_neurons <= 64:\n                network_class = tcnn.Network\n            elif n_neurons <= 128:\n                network_class = tcnn.NetworkWithLateActivation\n            else:\n                network_class = tcnn.NetworkWithOctree\n\n            return network_class(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                activation=activation,\n                output_activation=output_activation,\n                n_neurons=n_neurons,\n                n_hidden_layers=n_layers - 1,\n            )\n        else:\n            # PyTorch\n            network = torch.nn.Sequential()\n            network.append(torch.nn.Linear(n_input_dims, n_neurons))\n            network.append(get_activation(activation))\n\n            for _ in range(n_layers - 2):\n                network.append(torch.nn.Linear(n_neurons, n_neurons))\n                network.append(get_activation(activation))\n\n            network.append(torch.nn.Linear(n_neurons, n_output_dims))\n            network.append(get_activation(output_activation))\n\n            return network\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons < 2048:\n                network_type = \"FullyFusedMLP\"\n            else:\n                network_type = \"MLP\"\n\n            return getattr(tcnn, network_type)(\n                n_input_dims,\n                n_output_dims,\n                n_layers,\n                n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n        else:\n            network = nn.Sequential(\n                nn.Linear(n_input_dims, n_neurons),\n                getattr(nn, activation)(),\n            )\n\n            for _ in range(n_layers - 2):\n                network.append(nn.Linear(n_neurons, n_neurons))\n                network.append(getattr(nn, activation)())\n\n            network.append(nn.Linear(n_neurons, n_output_dims))\n            network.append(getattr(nn, output_activation)())\n\n            return network\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0 and n_neurons > 0\n        if self.tcnn:\n            if n_neurons <= 256:\n                network_type = \"TinyFullyFusedMLP\"\n            elif n_neurons <= 1024:\n                network_type = \"TinyMLP\"\n            else:\n                network_type = \"FullyFusedMLP\"\n\n            network = getattr(\n                tcnn,\n                network_type,\n            )(\n                n_input_dims,\n                n_output_dims,\n                n_layers,\n                n_neurons,\n                getattr(tcnn, activation),\n                getattr(tcnn, output_activation),\n            )\n        else:\n            network = nn.Sequential(\n                nn.Linear(n_input_dims, n_neurons),\n                getattr(nn, activation)(),\n                *[\n                    nn.Linear(n_neurons, n_neurons)\n                    for _ in range(n_layers - 2)\n                ],\n                nn.Linear(n_neurons, n_output_dims),\n                getattr(nn, output_activation)(),\n            )\n        return network\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons == 1:\n                network_type = \"FullyFusedMLP\"\n            else:\n                network_type = \"MLP\"\n            return getattr(\n                tcnn,\n                network_type,\n            )(\n                n_input_dims,\n                n_output_dims,\n                n_layers,\n                n_neurons,\n                getattr(torch.nn, activation)(),\n                getattr(torch.nn, output_activation)(),\n            )\n        else:\n            layers = [\n                torch.nn.Linear(n_input_dims, n_neurons),\n                getattr(torch.nn, activation)(),\n            ]\n            for _ in range(n_layers - 2):\n                layers.append(torch.nn.Linear(n_neurons, n_neurons))\n                layers.append(getattr(torch.nn, activation)())\n            layers.append(torch.nn.Linear(n_neurons, n_output_dims))\n            layers.append(getattr(torch.nn, output_activation)())\n            return torch.nn.Sequential(*layers)\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Create a list of offsets for the kernel\n        offsets = np.arange(-kernel_offset, kernel_offset + 1)\n\n        # Create a list of shifted signals\n        shifted_signals = [np.roll(signal, offset) for offset in offsets]\n\n        # Compute the median of the shifted signals\n        median_signal = np.median(shifted_signals, axis=0)\n\n        # Trim the median signal to remove edge effects\n        trimmed_median_signal = median_signal[kernel_offset:-kernel_offset]\n\n        return trimmed_median_signal\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Create a list of offsets to apply to the signal\n        offsets = list(range(-kernel_offset, kernel_offset + 1))\n\n        # Create a list of shifted versions of the signal\n        shifted_signals = [np.roll(signal, offset) for offset in offsets]\n\n        # Compute the median of the shifted signals\n        median_signal = np.median(shifted_signals, axis=0)\n\n        # Trim the median signal to remove edge effects\n        trimmed_median_signal = median_signal[kernel_offset:-kernel_offset]\n\n        return trimmed_median_signal\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Shift the signal by the kernel offset in both directions\n        shifted_signals = [\n            signal[i : i + len(signal) - 2 * kernel_offset]\n            for i in range(2 * kernel_offset + 1)\n        ]\n\n        # Compute the median of the shifted signals\n        median_signal = np.median(shifted_signals, axis=0)\n\n        # Trim the median signal to remove edge effects\n        trimmed_median_signal = median_signal[\n            kernel_offset : len(signal) - kernel_offset\n        ]\n\n        return trimmed_median_signal\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Generate shifted versions of the signal\n        shifted_signals = np.array([signal[i : i + len(signal) - 2 * kernel_offset] for i in range(2 * kernel_offset + 1)])\n\n        # Compute the median of the shifted signals\n        median = np.median(shifted_signals, axis=0)\n\n        # Trim the median to remove edge effects\n        trimmed_median = median[kernel_offset:-kernel_offset]\n\n        return trimmed_median\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Generate shifted versions of the signal\n        shifted_signals = [signal.copy() for _ in range(2 * kernel_offset + 1)]\n        for i in range(1, kernel_offset + 1):\n            shifted_signals[i][:-i] = signal[i:]\n            shifted_signals[-i][-i:] = signal[:-i]\n\n        # Compute the median of the shifted signals\n        median_signal = np.median(shifted_signals, axis=0)\n\n        # Trim the median array to remove edge effects\n        trimmed_median = median_signal[kernel_offset:-kernel_offset]\n\n        return trimmed_median\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the rolling median of the signal\n        median_signal = np.zeros_like(signal)\n        for i in range(len(signal)):\n            # Shift the signal by the kernel offset\n            shifted_signal = np.roll(signal, i - kernel_offset)\n            # Compute the median of the shifted signal\n            median_signal[i] = np.median(shifted_signal)\n\n        # Trim the rolling median array to remove edge effects\n        trimmed_median_signal = median_signal[kernel_offset:-kernel_offset]\n\n        return trimmed_median_signal\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the rolling median of the signal using the kernel offset\n        median_signal = np.zeros_like(signal)\n        for i in range(len(signal)):\n            start_index = max(0, i - kernel_offset)\n            end_index = min(len(signal), i + kernel_offset + 1)\n            median_signal[i] = np.median(signal[start_index:end_index])\n\n        # Trim the median signal to remove edge effects\n        trimmed_median_signal = median_signal[kernel_offset:-kernel_offset]\n\n        return trimmed_median_signal\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements to shift the signal\n        shift = kernel_offset\n\n        # Create a list of shifted signals\n        shifted_signals = [signal[i:i+len(signal)] for i in range(shift)]\n\n        # Stack the shifted signals along a new axis\n        shifted_signals = np.stack(shifted_signals, axis=1)\n\n        # Compute the median along the new axis\n        median_signal = np.median(shifted_signals, axis=1)\n\n        # Trim the median signal to remove edge effects\n        median_signal = median_signal[shift:-shift]\n\n        return median_signal\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Generate shifted versions of the signal\n        signal_shifted = np.array([signal[kernel_offset:], signal[:-kernel_offset]])\n\n        # Compute the rolling median of the shifted signals\n        median_array = np.median(signal_shifted, axis=0)\n\n        # Trim the median array to remove edge effects\n        trimmed_median_array = median_array[kernel_offset:-kernel_offset]\n\n        return trimmed_median_array\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Initialize an empty array to store the rolling median\n        rolling_median = np.zeros(signal.shape)\n\n        # Loop through each element of the signal\n        for i in range(len(signal)):\n            # Calculate the start and end indices for the shifted signal\n            start = max(0, i - kernel_offset)\n            end = min(len(signal), i + kernel_offset + 1)\n\n            # Compute the median of the shifted signal\n            median = np.median(signal[start:end])\n\n            # Store the median in the rolling median array\n            rolling_median[i] = median\n\n        # Trim the rolling median array to remove edge effects\n        trimmed_rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return trimmed_rolling_median\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the number of elements to shift the signal by\n        shift_amount = kernel_offset * 2 + 1\n\n        # Generate a 2D array of shifted signals by shifting the input signal by a range of values\n        shifted_signals = np.array([np.roll(signal, i) for i in range(-shift_amount, shift_amount + 1)])\n\n        # Compute the median of the shifted signals along the first axis\n        median_signal = np.median(shifted_signals, axis=0)\n\n        # Trim the median signal to remove edge effects introduced by the shifting process\n        trimmed_median_signal = median_signal[shift_amount:-shift_amount]\n\n        return trimmed_median_signal\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Initialize the rolling median array with zeros\n        rolling_median = np.zeros_like(signal)\n\n        # Shift the signal by the kernel offset in both directions\n        shifted_signal_left = np.roll(signal, -kernel_offset)\n        shifted_signal_right = np.roll(signal, kernel_offset)\n\n        # Compute the median of the shifted signals\n        median_left = np.median(shifted_signal_left, axis=1)\n        median_right = np.median(shifted_signal_right, axis=1)\n\n        # Trim the median arrays to remove edge effects\n        trimmed_median_left = median_left[kernel_offset:-kernel_offset]\n        trimmed_median_right = median_right[kernel_offset:-kernel_offset]\n\n        # Compute the rolling median by taking the mean of the trimmed medians\n        rolling_median[kernel_offset:-kernel_offset] = (trimmed_median_left + trimmed_median_right) / 2\n\n        return rolling_median\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Initialize an empty list to store the rolling median values\n        rolling_median = []\n\n        # Iterate over the signal indices\n        for i in range(len(signal)):\n\n            # Calculate the start and end indices for the current window\n            start = max(0, i - kernel_offset)\n            end = min(len(signal), i + kernel_offset + 1)\n\n            # Calculate the median of the current window and append it to the rolling median list\n            rolling_median.append(np.median(signal[start:end]))\n\n        # Trim the rolling median list to remove edge effects\n        trim_amount = kernel_offset\n        rolling_median = rolling_median[trim_amount:-trim_amount]\n\n        # Return the rolling median as a NumPy array\n        return np.array(rolling_median)\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the number of shifts needed to cover the entire range of the kernel\n        num_shifts = 2 * kernel_offset + 1\n\n        # Create a 2D array to store the shifted signals\n        shifted_signals = np.zeros((num_shifts, len(signal)))\n\n        # Generate shifted signals by shifting the input signal by the kernel offset\n        for i in range(num_shifts):\n            shifted_signals[i] = np.roll(signal, i - kernel_offset)\n\n        # Compute the rolling median by taking the median along the first axis (i.e., the shifted signals)\n        rolling_median = np.median(shifted_signals, axis=0)\n\n        # Trim the rolling median to remove edge effects introduced by the shifting process\n        trimmed_rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return trimmed_rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the number of shifts to apply\n        shifts = np.arange(-kernel_offset, kernel_offset + 1)\n\n        # Compute the number of elements to trim from the edges\n        trim_length = len(shifts) // 2\n\n        # Compute the rolling median by shifting the signal and computing the median\n        rolling_median = np.median(np.stack([np.roll(signal, shift) for shift in shifts]), axis=0)\n\n        # Trim the edges to remove edge effects\n        return rolling_median[trim_length:-trim_length]\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of shifts for the kernel\n        kernel_size = 2 * kernel_offset + 1\n\n        # Create a list of shifts for the kernel\n        shifts = np.arange(-kernel_offset, kernel_offset + 1)\n\n        # Calculate the rolling median by shifting the signal and computing the median\n        rolling_median = np.median(np.vstack([np.roll(signal, shift) for shift in shifts]), axis=0)\n\n        # Trim the rolling median to remove edge effects\n        trimmed_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return trimmed_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the number of elements in the signal\n        n_elements = len(signal)\n\n        # Create an array of indices for the signal\n        signal_indices = np.arange(n_elements)\n\n        # Create an array of indices for the kernel\n        kernel_indices = np.arange(kernel_offset, -kernel_offset - 1, -1)\n\n        # Compute the indices for the shifted signals\n        shifted_indices = signal_indices[:, np.newaxis] + kernel_indices\n\n        # Clip the indices to ensure they are within the valid range\n        clipped_indices = np.clip(shifted_indices, 0, n_elements - 1)\n\n        # Compute the shifted signals using the clipped indices\n        shifted_signals = signal[clipped_indices]\n\n        # Compute the median of the shifted signals along the first axis\n        median_signals = np.median(shifted_signals, axis=1)\n\n        # Trim the median signals to remove edge effects\n        trimmed_median_signals = median_signals[kernel_offset:-kernel_offset]\n\n        return trimmed_median_signals\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Create an array of indices for the signal\n        indices = np.arange(len(signal))\n\n        # Create an array of indices for the shifted signal\n        shifted_indices = np.arange(len(signal)) + kernel_offset\n\n        # Create a 2D array of indices for the shifted signal\n        shifted_indices_2d = np.tile(shifted_indices, (len(signal), 1))\n\n        # Calculate the difference between the shifted indices and the original indices\n        diff = shifted_indices_2d - indices.reshape(-1, 1)\n\n        # Create a boolean mask to filter the shifted indices based on the kernel offset\n        mask = np.abs(diff) <= kernel_offset\n\n        # Apply the mask to the shifted indices to get the valid shifted indices\n        valid_shifted_indices = shifted_indices_2d[mask]\n\n        # Create a 2D array of indices for the original signal\n        indices_2d = np.tile(indices, (len(signal), 1))\n\n        # Calculate the difference between the original indices and the shifted indices\n        diff = indices_2d - shifted_indices_2d\n\n        # Create a boolean mask to filter the original indices based on the kernel offset\n        mask = np.abs(diff) <= kernel_offset\n\n        # Apply the mask to the original indices to get the valid original indices\n        valid_indices = indices_2d[mask]\n\n        # Create a 2D array of the signal values\n        signal_2d = np.tile(signal, (len(signal), 1))\n\n        # Select the valid shifted signal values using the valid shifted indices\n        valid_shifted_signal = signal_2d[valid_shifted_indices, valid_indices]\n\n        # Compute the median of the valid shifted signal values along the second axis\n        rolling_median = np.median(valid_shifted_signal, axis=1)\n\n        # Trim the rolling median array to remove edge effects\n        rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n       "}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the number of elements in the rolling median array\n        rolling_median_length = len(signal) - 2 * kernel_offset\n\n        # Initialize an empty array to store the rolling median values\n        rolling_median = np.empty(rolling_median_length)\n\n        # Loop through the elements in the rolling median array\n        for i in range(rolling_median_length):\n            # Compute the start and end indices for the current element in the rolling median array\n            start = i\n            end = i + 2 * kernel_offset + 1\n\n            # Compute the median of the shifted signal within the current range\n            rolling_median[i] = np.median(signal[start:end])\n\n        # Trim the rolling median array to remove edge effects\n        rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        # Return the rolling median array\n        return rolling_median\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the number of shifts needed for the rolling median\n        shifts = np.arange(1, kernel_offset + 1)\n\n        # Shift the signal by the specified offsets\n        shifted_signals = [np.roll(signal, shift) for shift in shifts]\n        shifted_signals.append(signal)\n        shifted_signals.extend([np.roll(signal, -shift) for shift in shifts])\n\n        # Stack the shifted signals along a new axis\n        shifted_signals = np.stack(shifted_signals, axis=-1)\n\n        # Compute the median along the new axis\n        median = np.median(shifted_signals, axis=-1)\n\n        # Trim the median to remove edge effects\n        trimmed_median = median[kernel_offset:-kernel_offset]\n\n        return trimmed_median\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Initialize variables\n    min_hamming_dist = np.inf\n    min_rotation_shift = 0\n\n    # Calculate Hamming distance for each rotation shift\n    for rotation_shift_i in range(rotation_shift):\n        hamming_dist = hamming_distance_with_rotation(\n            template_probe, template_gallery, rotation_shift_i, nm_dist, weights\n        )\n\n        # Update minimum Hamming distance and rotation shift if necessary\n        if hamming_dist < min_hamming_dist:\n            min_hamming_dist = hamming_dist\n            min_rotation_shift = rotation_shift_i\n\n    return min_hamming_dist, min_rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Create a list of Hamming distances for each rotation shift\n    hamming_distances = []\n    for i in range(rotation_shift):\n        # Calculate the Hamming distance for the current rotation shift\n        hamming_distances.append(\n            hamming_distance_with_shift(\n                template_probe, template_gallery, i, nm_dist=nm_dist, weights=weights\n            )\n        )\n\n    # Find the minimum Hamming distance and its corresponding rotation shift\n    min_hamming_distance = min(hamming_distances)\n    min_rotation_shift = hamming_distances.index(min_hamming_distance)\n\n    return min_hamming_distance, min_rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Convert rotation shift to columns\n    rotation_shift = int(rotation_shift / 360 * template_probe.iris_code.shape[1])\n\n    # Calculate Hamming distance\n    if weights is None:\n        hamming_dist = np.sum(\n            np.abs(\n                np.roll(template_probe.iris_code, rotation_shift, axis=1)\n                - template_gallery.iris_code\n            )\n        )\n    else:\n        hamming_dist = np.sum(\n            np.abs(\n                np.roll(template_probe.iris_code, rotation_shift, axis=1)\n                - template_gallery.iris_code\n            )\n            * weights\n        )\n\n    # Calculate normalized Hamming distance\n    if nm_dist is not None:\n        hamming_dist = hamming_dist / nm_dist\n\n    return hamming_dist, rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance for each rotation shift\n    hamming_distances = [\n        hamming_distance_with_rotation(template_probe, template_gallery, i, nm_dist, weights)\n        for i in range(rotation_shift)\n    ]\n\n    # Find the minimum Hamming distance and corresponding rotation shift\n    min_distance, min_shift = min(hamming_distances, key=lambda x: x[0])\n\n    return min_distance, min_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance with no rotation shift\n    distance = hamming_distance_with_shift(template_probe, template_gallery, 0, nm_dist, weights)\n    min_distance = distance\n    min_shift = 0\n\n    # Calculate the Hamming distance for each rotation shift\n    for shift in range(1, rotation_shift + 1):\n        distance = hamming_distance_with_shift(\n            template_probe, template_gallery, shift, nm_dist, weights\n        )\n        if distance < min_distance:\n            min_distance = distance\n            min_shift = shift\n\n    # Return the minimum Hamming distance and the corresponding rotation shift\n    return min_distance, min_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if rotation_shift != 0:\n        template_probe.data = np.roll(template_probe.data, rotation_shift, axis=1)\n\n    if nm_dist is not None:\n        if weights is not None:\n            hd = np.sum(\n                weights[0]\n                * (\n                    (template_probe.data > nm_dist)\n                    != (template_gallery.data > nm_dist)\n                )\n                + weights[1]\n                * (\n                    (template_probe.data <= nm_dist)\n                    != (template_gallery.data <= nm_dist)\n                )\n            )\n        else:\n            hd = np.sum(\n                (template_probe.data > nm_dist)\n                != (template_gallery.data > nm_dist)\n            )\n    else:\n        if weights is not None:\n            hd = np.sum(\n                weights[0]\n                * (template_probe.data != template_gallery.data)\n                + weights[1]\n                * (template_probe.data == template_gallery.data)\n            )\n        else:\n            hd = np.sum(template_probe.data != template_gallery.data)\n\n    return hd, rotation_shift"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Convert rotation_shift to columns\n    rotation_shift_cols = rotation_shift * 16\n\n    # Calculate Hamming distance\n    hd = np.sum(\n        np.abs(\n            np.roll(template_probe.iris_code, rotation_shift_cols, axis=1)\n            - template_gallery.iris_code\n        )\n    )\n\n    # Calculate normalized Hamming distance if nm_dist is provided\n    if nm_dist is not None:\n        hd = hd / nm_dist\n\n    # Calculate weighted Hamming distance if weights is provided\n    if weights is not None:\n        hd = hd * weights\n\n    return hd, rotation_shift"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Create a list of Hamming distances\n    distances = []\n\n    # Loop through the rotation shifts\n    for i in range(rotation_shift):\n        # Calculate the Hamming distance for the current rotation shift\n        distances.append(\n            hamming_distance(\n                template_probe,\n                template_gallery,\n                i,\n                nm_dist=nm_dist,\n                weights=weights,\n            )\n        )\n\n    # Find the minimum Hamming distance and its corresponding rotation shift\n    min_distance = min(distances)\n    min_index = distances.index(min_distance)\n\n    # Return the minimum Hamming distance and the corresponding rotation shift\n    return min_distance, min_index\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if weights is None:\n        weights = [None, None]\n\n    # Initialize the minimum Hamming distance and the corresponding rotation shift\n    min_hamming_dist = np.inf\n    min_rotation_shift = rotation_shift\n\n    # Iterate over the rotation shifts\n    for rotation_shift in range(rotation_shift * 2 + 1):\n        # Calculate the Hamming distance for the current rotation shift\n        hamming_dist = iris_hamming_distance(\n            template_probe, template_gallery, rotation_shift, nm_dist, weights\n        )\n\n        # Update the minimum Hamming distance and the corresponding rotation shift if necessary\n        if hamming_dist < min_hamming_dist:\n            min_hamming_dist = hamming_dist\n            min_rotation_shift = rotation_shift\n\n    return min_hamming_dist, min_rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance for each rotation\n    hamm_dist = [\n        hamming_distance_calc(\n            template_probe.features,\n            template_gallery.features,\n            rotation_shift=rotation_shift,\n            nm_dist=nm_dist,\n            weights=weights,\n        )\n        for rotation_shift in range(rotation_shift)\n    ]\n\n    # Find the minimum Hamming distance and its corresponding rotation shift\n    min_hamm_dist = min(hamm_dist)\n    min_rotation_shift = hamm_dist.index(min_hamm_dist)\n\n    return min_hamm_dist, min_rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if nm_dist is None:\n        nm_dist = 0.0\n\n    if weights is None:\n        weights = [None]\n\n    min_hd = np.inf\n    min_rotation_shift = -1\n\n    for rotation_shift_columns in range(rotation_shift + 1):\n        for weight in weights:\n            hd = hamming_distance_with_rotation(\n                template_probe, template_gallery, rotation_shift_columns, nm_dist, weight\n            )\n            if hd < min_hd:\n                min_hd = hd\n                min_rotation_shift = rotation_shift_columns\n\n    return min_hd, min_rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if weights is None:\n        weights = [np.ones(template_probe.mask.shape)]\n\n    min_dist = np.inf\n    min_shift = 0\n\n    for shift in range(rotation_shift + 1):\n        dist = 0\n        for weight in weights:\n            dist += np.sum(\n                np.abs(\n                    np.roll(template_probe.mask * template_probe.iris, shift)\n                    - template_gallery.mask * template_gallery.iris\n                )\n                * weight\n            )\n        if nm_dist is not None:\n            dist /= nm_dist\n        if dist < min_dist:\n            min_dist = dist\n            min_shift = shift\n\n    return min_dist, min_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if nm_dist is None:\n        nm_dist = 0\n\n    if weights is None:\n        weights = [np.ones((template_probe.mask.shape[0], template_probe.mask.shape[1]))]\n\n    # Create a new template for the probe with the desired rotation shift\n    template_probe_rotated = template_probe.rotate(rotation_shift)\n\n    # Calculate the Hamming distance between the rotated probe and the gallery template\n    hd = _hamming_distance(template_probe_rotated, template_gallery, nm_dist, weights)\n\n    return hd, rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance for each rotation shift\n    hamming_dist = []\n    for r in range(rotation_shift):\n        # Shift the probe template\n        shifted_probe = np.roll(template_probe.data, r, axis=1)\n        # Calculate the Hamming distance between the shifted probe and the gallery template\n        if nm_dist is not None:\n            # Calculate the normalized Hamming distance\n            if weights is not None:\n                # Calculate the weighted Hamming distance\n                hamming_dist.append(\n                    normalized_weighted_hamming_distance(\n                        shifted_probe, template_gallery.data, nm_dist, weights\n                    )\n                )\n            else:\n                # Calculate the normalized Hamming distance\n                hamming_dist.append(\n                    normalized_hamming_distance(\n                        shifted_probe, template_gallery.data, nm_dist\n                    )\n                )\n        else:\n            # Calculate the Hamming distance\n            if weights is not None:\n                # Calculate the weighted Hamming distance\n                hamming_dist.append(\n                    weighted_hamming_distance(shifted_probe, template_gallery.data, weights)\n                )\n            else:\n                # Calculate the Hamming distance\n                hamming_dist.append(\n                    hamming_distance(shifted_probe, template_gallery.data)\n                )\n\n    # Find the minimum Hamming distance and the corresponding rotation shift\n    min_dist = min(hamming_dist)\n    min_shift = hamming_dist.index(min_dist)\n\n    return min_dist, min_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Initialize the minimum Hamming distance and the corresponding rotation shift\n    min_hamming_dist = np.inf\n    min_rotation_shift = -1\n\n    # Iterate over the possible rotation shifts\n    for i in range(rotation_shift):\n        # Calculate the Hamming distance between the probe and gallery templates\n        # after rotating the gallery template by the current rotation shift\n        hamming_dist = hamming_distance_calc(\n            template_probe,\n            template_gallery.rotate(i),\n            nm_dist=nm_dist,\n            weights=weights,\n        )\n\n        # Update the minimum Hamming distance and rotation shift if necessary\n        if hamming_dist < min_hamming_dist:\n            min_hamming_dist = hamming_dist\n            min_rotation_shift = i\n\n    # Return the minimum Hamming distance and the corresponding rotation shift\n    return min_hamming_dist, min_rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if nm_dist is None:\n        nm_dist = 0\n    if weights is None:\n        weights = [None, None]\n\n    hamm_dist = np.inf\n    min_rotation = 0\n\n    for i in range(rotation_shift):\n        dist = hamming_distance_with_shift(\n            template_probe.data, template_gallery.data, i, nm_dist, weights\n        )\n        if dist < hamm_dist:\n            hamm_dist = dist\n            min_rotation = i\n\n    return hamm_dist, min_rotation\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Convert rotation shift to columns\n    cols = rotation_shift\n\n    # Calculate Hamming distance\n    if nm_dist is None and weights is None:\n        # Unweighted Hamming distance\n        hamming_dist = np.sum(\n            np.abs(\n                np.roll(template_probe.iris_code, cols) - template_gallery.iris_code\n            )\n        )\n    elif nm_dist is not None and weights is None:\n        # Normalized Hamming distance\n        hamming_dist = np.sum(\n            np.abs(\n                np.roll(template_probe.iris_code, cols) - template_gallery.iris_code\n            )\n        ) / (2 * nm_dist)\n    elif nm_dist is None and weights is not None:\n        # Weighted Hamming distance\n        hamming_dist = np.sum(\n            np.abs(\n                np.roll(template_probe.iris_code, cols) - template_gallery.iris_code\n            )\n            * weights\n        )\n    else:\n        # Normalized and weighted Hamming distance\n        hamming_dist = np.sum(\n            np.abs(\n                np.roll(template_probe.iris_code, cols) - template_gallery.iris_code\n            )\n            * weights\n        ) / (2 * nm_dist)\n\n    return hamming_dist, cols\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if weights is None:\n        weights = [np.ones(template_probe.mask.shape, dtype=np.float64)]\n\n    # Calculate Hamming distance for each possible rotation\n    hamm_dist = np.zeros(rotation_shift * 2 + 1)\n    for i in range(rotation_shift * 2 + 1):\n        hamm_dist[i] = hamming_distance_with_mask(\n            template_probe.mask,\n            np.roll(template_gallery.mask, -i, axis=1),\n            weights=weights,\n        )\n\n    # Find the minimum Hamming distance and its corresponding rotation shift\n    min_hamm_dist = np.min(hamm_dist)\n    min_rotation_shift = np.argmin(hamm_dist) - rotation_shift\n\n    if nm_dist is not None:\n        min_hamm_dist /= nm_dist\n\n    return min_hamm_dist, min_rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if weights is None:\n        weights = [np.ones((1, 1))]\n    if nm_dist is None:\n        nm_dist = 0\n    hamm_dist = np.zeros((len(weights), rotation_shift * 2 + 1))\n    for w in range(len(weights)):\n        for i in range(rotation_shift * 2 + 1):\n            hamm_dist[w, i] = np.sum(\n                weights[w]\n                * np.abs(\n                    np.roll(template_probe.masked_iris, -i, axis=1)\n                    - template_gallery.masked_iris\n                )\n            )\n    hamm_dist_min = np.min(hamm_dist)\n    hamm_dist_min_shift = np.argmin(hamm_dist) - rotation_shift\n    if hamm_dist_min > nm_dist:\n        hamm_dist_min = -1\n        hamm_dist_min_shift = -1\n    return hamm_dist_min, hamm_dist_min_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Get the number of rows and columns of the probe template\n    nrows, ncols = template_probe.data.shape\n\n    # Calculate the number of columns for the Hamming distance calculation\n    # by adding the rotation shift to the number of columns of the probe template\n    # and taking the modulus of the result by the number of columns\n    ncols_hamming = (ncols + rotation_shift) % ncols\n\n    # Get the data of the probe template and the gallery template\n    data_probe = template_probe.data\n    data_gallery = template_gallery.data\n\n    # Initialize the Hamming distance to 0\n    hamming_dist = 0\n\n    # Loop through the rows of the probe template\n    for row in range(nrows):\n        # Get the row of the probe template\n        row_probe = data_probe[row, :]\n\n        # Initialize the minimum Hamming distance for the current row to infinity\n        min_hamming_dist = np.inf\n\n        # Loop through the possible rotation shifts\n        for shift in range(ncols):\n            # Calculate the Hamming distance for the current rotation shift\n            # by XORing the probe row with the gallery row after rotating it by the current rotation shift\n            hamming_dist = np.sum(np.bitwise_xor(row_probe, np.roll(data_gallery[row, :], shift)))\n\n            # If the Hamming distance is less than the minimum Hamming distance for the current row,\n            # update the minimum Hamming distance and the corresponding rotation shift\n            if hamming_dist < min_hamming_dist:\n                min_hamming_dist = hamming_dist\n                min_shift = shift\n\n        # Add the minimum Hamming distance for the current row to the overall Hamming distance\n        hamming_dist += min_hamming_dist\n\n    # If the normalized Hamming distance is specified, calculate it\n    if nm_dist is not None:\n        # Calculate the normalized Hamming distance by dividing the overall Hamming distance by the number of columns\n        # and multiplying it by the nonmatch distance\n        hamming"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize an empty list to store the starting points of the perpendicular bisectors\n        bisectors_start_points = []\n\n        # Initialize an empty list to store the ending points of the perpendicular bisectors\n        bisectors_end_points = []\n\n        # Initialize a counter for the number of iterations\n        i = 0\n\n        # Loop until the number of bisectors is reached or the maximum number of iterations is reached\n        while len(bisectors_start_points) < self.number_of_bisectors and i < self.max_iterations:\n            # Randomly select two points from the polygon's vertices\n            point1, point2 = np.random.choice(polygon, 2, replace=False)\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point1 - point2)\n\n            # Check if the distance between the two points is greater than the minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the midpoint between the two points\n                midpoint = (point1 + point2) / 2\n\n                # Calculate the vector representing the line between the two points\n                line_vector = point2 - point1\n\n                # Calculate the vector perpendicular to the line\n                perpendicular_vector = np.array([-line_vector[1], line_vector[0]])\n\n                # Normalize the perpendicular vector\n                unit_perpendicular_vector = perpendicular_vector / np.linalg.norm(perpendicular_vector)\n\n                # Calculate the starting point of the perpendicular bisector\n                bisector_start_point = midpoint - unit_perpendicular_vector * distance / 2\n\n                # Calculate the ending point of the perpendicular bisector\n                bisector_end_point = midpoint + unit_perpendicular_vector * distance / 2\n\n                # Append the starting and ending points of the perpendicular bisector to their respective lists\n                bisectors_start_points.append(bisector_start_point)\n                bisectors_end_points.append(bisector_end_point)"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize empty arrays to store the starting and ending points of the perpendicular bisectors\n        starting_points = np.empty((0, 2), dtype=np.float32)\n        ending_points = np.empty((0, 2), dtype=np.float32)\n\n        # Loop until the desired number of bisectors are calculated or the maximum number of iterations is reached\n        for _ in range(self.max_iterations):\n            # Randomly select two points from the polygon's vertices\n            point_1, point_2 = np.random.choice(polygon, 2, replace=False)\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point_1 - point_2)\n\n            # Check if the distance between the points is greater than the minimum distance specified\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the midpoint between the two points\n                midpoint = (point_1 + point_2) / 2\n\n                # Calculate the slope of the line passing through the two points\n                slope = (point_2[1] - point_1[1]) / (point_2[0] - point_1[0])\n\n                # Calculate the perpendicular slope\n                perpendicular_slope = -1 / slope\n\n                # Calculate the y-intercept of the line passing through the midpoint and perpendicular to the line between the two points\n                y_intercept = midpoint[1] - perpendicular_slope * midpoint[0]\n\n                # Calculate the starting and ending points of the perpendicular bisector\n                starting_point, ending_point = self._calculate_perpendicular_bisector_points(\n                    midpoint, perpendicular_slope, y_intercept\n                )\n\n                # Append the starting and ending points to the respective arrays\n                starting_points = np.append(starting_points, [starting_point], axis=0)\n                ending_points = np.append(ending_points, [ending_point], axis=0)\n\n            # Check if the desired number of bisectors have been"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize an empty list to store the bisectors\n        bisectors = []\n\n        # Initialize a counter to track the number of iterations\n        iterations = 0\n\n        # Continue the loop until the desired number of bisectors is obtained or the maximum number of iterations is reached\n        while len(bisectors) < self.number_of_bisectors and iterations < self.max_iterations:\n\n            # Randomly select two points from the polygon\n            point_1, point_2 = np.random.choice(polygon, size=2, replace=False)\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point_1 - point_2)\n\n            # Check if the distance between the points is greater than the minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n\n                # Calculate the midpoint between the two points\n                midpoint = (point_1 + point_2) / 2\n\n                # Calculate the vector representing the line between the two points\n                vector = point_2 - point_1\n\n                # Calculate the perpendicular vector to the line\n                perpendicular_vector = np.array([-vector[1], vector[0]])\n\n                # Normalize the perpendicular vector\n                unit_perpendicular_vector = perpendicular_vector / np.linalg.norm(\n                    perpendicular_vector\n                )\n\n                # Calculate the starting point of the perpendicular bisector\n                start_point = midpoint - unit_perpendicular_vector * distance / 2\n\n                # Calculate the ending point of the perpendicular bisector\n                end_point = midpoint + unit_perpendicular_vector * distance / 2\n\n                # Append the bisector to the list\n                bisectors.append((start_point, end_point))\n\n            # Increment the iteration counter\n            iterations += 1\n\n        # Check if the desired number of bisectors was obtained\n        if len(bisectors) < self.number_of_bisectors:\n\n            # Raise an error if the desired number of bisectors was not obtained within the maximum number of iterations"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Calculate the number of bisectors to calculate\n        num_bisectors = self.num_bisectors\n\n        # Initialize an array to store the starting points of the perpendicular bisectors\n        bisectors_start_points = np.zeros((num_bisectors, 2))\n\n        # Initialize an array to store the ending points of the perpendicular bisectors\n        bisectors_end_points = np.zeros((num_bisectors, 2))\n\n        # Initialize a counter to keep track of the number of bisectors that have been successfully calculated\n        num_bisectors_calculated = 0\n\n        # Initialize a counter to keep track of the number of iterations\n        iterations = 0\n\n        # Continue the loop until the required number of bisectors have been calculated or the maximum number of iterations has been reached\n        while num_bisectors_calculated < num_bisectors and iterations < self.max_iterations:\n\n            # Randomly select two points from the polygon's vertices\n            point1, point2 = np.random.choice(polygon, 2, replace=False)\n\n            # Calculate the distance between the two points\n            distance_between_points = np.linalg.norm(point2 - point1)\n\n            # Check if the distance between the points is greater than the minimum distance\n            if distance_between_points > min_distance_between_sector_points_in_px:\n\n                # Calculate the midpoint between the two points\n                midpoint = (point1 + point2) / 2\n\n                # Calculate the slope of the line passing through the two points\n                slope = (point2[1] - point1[1]) / (point2[0] - point1[0])\n\n                # Calculate the perpendicular slope\n                perpendicular_slope = -1 / slope\n\n                # Calculate the y-intercept of the line passing through the midpoint and perpendicular to the line between the two points\n                y_intercept = midpoint[1] - perpendicular_slope * midpoint[0]\n\n                # Calculate the starting point of the perpendicular bisector\n               "}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Calculate the number of bisectors to calculate\n        num_bisectors = self.num_bisectors\n\n        # Initialize an empty list to store the perpendicular bisectors\n        bisectors = []\n\n        # Initialize a counter to keep track of the number of iterations\n        iterations = 0\n\n        # Loop until the required number of bisectors are calculated or the maximum number of iterations is reached\n        while len(bisectors) < num_bisectors and iterations < self.max_iterations:\n            # Randomly select two points from the polygon's vertices\n            point1, point2 = np.random.choice(polygon, 2, replace=False)\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point1 - point2)\n\n            # Check if the distance between the points is greater than the minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the midpoint between the two points\n                midpoint = (point1 + point2) / 2\n\n                # Calculate the vector perpendicular to the line connecting the two points\n                vector = point1 - point2\n                perpendicular_vector = np.array([-vector[1], vector[0]])\n\n                # Normalize the perpendicular vector\n                unit_vector = perpendicular_vector / np.linalg.norm(perpendicular_vector)\n\n                # Calculate the starting point of the perpendicular bisector\n                start = midpoint - unit_vector * distance / 2\n\n                # Calculate the ending point of the perpendicular bisector\n                end = midpoint + unit_vector * distance / 2\n\n                # Add the perpendicular bisector to the list of bisectors\n                bisectors.append((start, end))\n\n            # Increment the iteration counter\n            iterations += 1\n\n        # Check if the required number of bisectors were found\n        if len(bisectors) < num_bisectors:\n            # If not, raise an error\n            raise EyeCentersEstimationError(\n                f\"Failed to find {num_bisectors} pairs of points with a"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize empty lists to store the starting and ending points of the perpendicular bisectors\n        bisectors_start = []\n        bisectors_end = []\n\n        # Initialize a counter to keep track of the number of iterations\n        i = 0\n\n        # Continue the loop until the desired number of bisectors are found or the maximum number of iterations is reached\n        while len(bisectors_start) < self.number_of_bisectors and i < self.max_iterations:\n\n            # Generate two random indices from the range of the polygon's vertices\n            random_index_1 = random.randint(0, len(polygon) - 1)\n            random_index_2 = random.randint(0, len(polygon) - 1)\n\n            # If the two indices are the same, skip this iteration and continue to the next one\n            if random_index_1 == random_index_2:\n                continue\n\n            # Extract the corresponding vertices based on the random indices\n            point_1 = polygon[random_index_1]\n            point_2 = polygon[random_index_2]\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point_1 - point_2)\n\n            # If the distance is less than the specified minimum distance, skip this iteration and continue to the next one\n            if distance < min_distance_between_sector_points_in_px:\n                continue\n\n            # Calculate the midpoint between the two points\n            midpoint = (point_1 + point_2) / 2\n\n            # Calculate the slope of the line segment connecting the two points\n            slope = (point_2[1] - point_1[1]) / (point_2[0] - point_1[0])\n\n            # Calculate the perpendicular slope of the line segment\n            perpendicular_slope = -1 / slope\n\n            # Calculate the y-intercept of the line passing through the midpoint with the perpendicular slope\n            y_intercept = midpoint[1] - perpendicular_slope * midpoint[0]\n\n            # Calculate the starting"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Calculate the number of points in the polygon\n        num_points = polygon.shape[0]\n\n        # Initialize an empty list to store the perpendicular bisectors\n        bisectors = []\n\n        # Initialize a counter to track the number of iterations\n        iterations = 0\n\n        # Loop until we have enough bisectors or reach the maximum number of iterations\n        while len(bisectors) < self.num_bisectors and iterations < self.max_iterations:\n            # Randomly choose two points from the polygon\n            i, j = np.random.choice(num_points, size=2, replace=False)\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(polygon[i] - polygon[j])\n\n            # Check if the distance between the points is greater than the minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the perpendicular bisector of the two points\n                bisector = (polygon[i] + polygon[j]) / 2\n\n                # Add the bisector to the list of bisectors\n                bisectors.append((bisector, (polygon[i], polygon[j])))\n\n            # Increment the iteration counter\n            iterations += 1\n\n        # Check if we have enough bisectors\n        if len(bisectors) < self.num_bisectors:\n            raise EyeCentersEstimationError(\n                f\"Failed to find enough pairs of points with a distance greater than {min_distance_between_sector_points_in_px} px after {iterations} iterations.\"\n            )\n\n        # Convert the list of bisectors to numpy arrays\n        bisectors_start = np.array([bisector[0] for bisector in bisectors])\n        bisectors_end = np.array([bisector[1] for bisector in bisectors])\n\n        return bisectors_start, bisectors_end\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize an empty list to store the perpendicular bisectors\n        perpendicular_bisectors = []\n\n        # Initialize the number of iterations\n        iterations = 0\n\n        # Continue the loop until the number of perpendicular bisectors is equal to the number of sectors\n        while len(perpendicular_bisectors) < self.number_of_sectors:\n\n            # Increment the number of iterations\n            iterations += 1\n\n            # If the number of iterations exceeds the maximum number of iterations, raise an error\n            if iterations > self.max_iterations:\n                raise EyeCentersEstimationError(\n                    \"Failed to find a sufficient number of point pairs that meet the distance criterion within the maximum number of iterations allowed.\"\n                )\n\n            # Select two random points from the polygon\n            point_a, point_b = np.random.choice(polygon, 2, replace=False)\n\n            # Calculate the distance between the two points\n            distance_between_points = np.linalg.norm(point_a - point_b)\n\n            # If the distance between the two points is less than the minimum distance, skip this iteration\n            if distance_between_points < min_distance_between_sector_points_in_px:\n                continue\n\n            # Calculate the midpoint between the two points\n            midpoint = (point_a + point_b) / 2\n\n            # Calculate the vector from the midpoint to the first point\n            vector = point_a - midpoint\n\n            # Calculate the perpendicular vector to the vector from the midpoint to the first point\n            perpendicular_vector = np.array([-vector[1], vector[0]])\n\n            # Normalize the perpendicular vector\n            perpendicular_vector /= np.linalg.norm(perpendicular_vector)\n\n            # Calculate the starting and ending points of the perpendicular bisector\n            start = midpoint - perpendicular_vector * distance_between_points / 2\n            end = midpoint + perpendicular_vector * distance_between_points / 2\n\n            # Add the perpendicular bisector to the list of perpendicular bisectors\n            perpendicular_bisectors.append((start, end))"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize an empty list to store the perpendicular bisectors\n        perpendicular_bisectors = []\n\n        # Initialize a counter to keep track of the number of iterations\n        iterations = 0\n\n        # Initialize a counter to keep track of the number of pairs of points that meet the distance criterion\n        num_pairs_meeting_criterion = 0\n\n        # Loop until the number of pairs of points that meet the distance criterion is equal to the number of bisectors to calculate or the maximum number of iterations is reached\n        while num_pairs_meeting_criterion < self.num_bisectors_to_calculate and iterations < self.max_iterations:\n\n            # Increment the iteration counter\n            iterations += 1\n\n            # Randomly select two points from the polygon's vertices\n            point1, point2 = np.random.choice(polygon, 2, replace=False)\n\n            # Calculate the distance between the two points\n            distance_between_points = np.linalg.norm(point1 - point2)\n\n            # Check if the distance between the two points is greater than the minimum distance specified\n            if distance_between_points > min_distance_between_sector_points_in_px:\n\n                # Calculate the perpendicular bisector of the line segment formed by the two points\n                perpendicular_bisector = self._calculate_perpendicular_bisector(point1, point2)\n\n                # Check if the perpendicular bisector is not None\n                if perpendicular_bisector is not None:\n\n                    # Append the perpendicular bisector to the list of perpendicular bisectors\n                    perpendicular_bisectors.append(perpendicular_bisector)\n\n                    # Increment the counter of pairs of points that meet the distance criterion\n                    num_pairs_meeting_criterion += 1\n\n        # Check if the number of pairs of points that meet the distance criterion is equal to the number of bisectors to calculate\n        if num_pairs_meeting_criterion < self.num_bisectors_to_calculate:\n\n            # Raise an error indicating that it was not possible to find enough pairs of points meeting the distance"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize an empty list to store the perpendicular bisectors\n        perpendicular_bisectors = []\n\n        # Loop until we have enough perpendicular bisectors\n        while len(perpendicular_bisectors) < self.n_bisectors:\n            # Generate a random pair of points from the polygon's vertices\n            point1, point2 = np.random.choice(polygon, size=2, replace=False)\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point1 - point2)\n\n            # Check if the distance is greater than the minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the perpendicular bisector of the two points\n                midpoint = (point1 + point2) / 2\n                direction = np.array([point1[1] - point2[1], point2[0] - point1[0]])\n                direction = direction / np.linalg.norm(direction)\n                perpendicular_bisectors.append((midpoint, direction))\n\n        # Convert the list of perpendicular bisectors to numpy arrays\n        perpendicular_bisectors = np.array(perpendicular_bisectors)\n\n        # Extract the starting points and directions of the perpendicular bisectors\n        start_points = perpendicular_bisectors[:, 0]\n        directions = perpendicular_bisectors[:, 1]\n\n        return start_points, directions\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Calculate the number of points in the polygon\n        num_points = polygon.shape[0]\n\n        # Initialize an empty list to store the pairs of points\n        pairs = []\n\n        # Initialize a counter for the number of iterations\n        iterations = 0\n\n        # Loop until we have enough pairs of points or reach the maximum number of iterations\n        while len(pairs) < self.num_bisectors and iterations < self.max_iterations:\n            # Randomly choose two indices from the range of indices in the polygon\n            i, j = np.random.choice(num_points, size=2, replace=False)\n\n            # Get the corresponding points from the polygon\n            p1, p2 = polygon[i], polygon[j]\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(p1 - p2)\n\n            # Check if the distance is greater than the minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n                # If it is, add the pair to the list of pairs\n                pairs.append((p1, p2))\n\n            # Increment the iteration counter\n            iterations += 1\n\n        # If we couldn't find enough pairs of points, raise an error\n        if len(pairs) < self.num_bisectors:\n            raise EyeCentersEstimationError(\n                \"Could not find enough pairs of points that meet the distance criterion within the maximum number of iterations.\"\n            )\n\n        # Initialize empty lists to store the starting and ending points of the perpendicular bisectors\n        starting_points = []\n        ending_points = []\n\n        # Loop through the pairs of points\n        for p1, p2 in pairs:\n            # Calculate the midpoint of the pair\n            midpoint = (p1 + p2) / 2\n\n            # Calculate the vector representing the line between the two points\n            vector = p2 - p1\n\n            # Calculate the perpendicular vector\n            perpendicular = np.array([-vector[1], vector[0]])\n\n            # Normalize the perpendicular vector\n            unit_perpendicular"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Calculate the number of points in the polygon\n        number_of_points = polygon.shape[0]\n\n        # Initialize an empty list to store the perpendicular bisectors\n        bisectors = []\n\n        # Initialize a counter to keep track of the number of iterations\n        counter = 0\n\n        # Continue the loop until we have enough bisectors or we reach the maximum number of iterations\n        while len(bisectors) < self.number_of_bisectors and counter < self.max_iterations:\n\n            # Select two random indices from the range of the number of points in the polygon\n            random_indices = np.random.choice(number_of_points, 2, replace=False)\n\n            # Sort the indices in ascending order\n            random_indices = np.sort(random_indices)\n\n            # Get the corresponding points from the polygon using the random indices\n            point_1 = polygon[random_indices[0]]\n            point_2 = polygon[random_indices[1]]\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point_1 - point_2)\n\n            # Check if the distance between the points is greater than the minimum distance specified\n            if distance > min_distance_between_sector_points_in_px:\n\n                # Calculate the midpoint between the two points\n                midpoint = (point_1 + point_2) / 2\n\n                # Calculate the slope of the line passing through the two points\n                slope = (point_2[1] - point_1[1]) / (point_2[0] - point_1[0])\n\n                # Calculate the perpendicular slope\n                perpendicular_slope = -1 / slope\n\n                # Calculate the y-intercept of the line passing through the midpoint and perpendicular to the line between the two points\n                y_intercept = midpoint[1] - perpendicular_slope * midpoint[0]\n\n                # Calculate the x-coordinate of the starting point of the perpendicular bisector\n                x1 = midpoint[0] - self.distance_between_sector_"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Calculate the number of possible point pairs\n        num_pairs = len(polygon) * (len(polygon) - 1) // 2\n\n        # Initialize variables to store the starting and ending points of the perpendicular bisectors\n        bisectors_start_points = np.zeros((self.num_bisectors, 2))\n        bisectors_end_points = np.zeros((self.num_bisectors, 2))\n\n        # Initialize a counter to track the number of bisectors found\n        num_bisectors_found = 0\n\n        # Initialize a variable to track the number of iterations\n        iterations = 0\n\n        # Continue the loop until the required number of bisectors are found or the maximum number of iterations is reached\n        while num_bisectors_found < self.num_bisectors and iterations < self.max_iterations:\n            # Increment the iteration counter\n            iterations += 1\n\n            # Choose two random points from the polygon\n            point_1, point_2 = np.random.choice(polygon, size=2, replace=False)\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point_1 - point_2)\n\n            # Check if the distance between the two points is greater than the minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the midpoint between the two points\n                midpoint = (point_1 + point_2) / 2\n\n                # Calculate the angle of the line passing through the midpoint and perpendicular to the line connecting the two points\n                angle = np.arctan2(point_2[1] - point_1[1], point_2[0] - point_1[0]) + np.pi / 2\n\n                # Calculate the starting and ending points of the perpendicular bisector\n                bisectors_start_points[num_bisectors_found] = midpoint\n                bisectors_end_points[num_bisectors_found] = midpoint + np.array(\n                    [np.cos("}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        bisectors_start_points = np.zeros((self.number_of_bisectors, 2))\n        bisectors_end_points = np.zeros((self.number_of_bisectors, 2))\n\n        for i in range(self.number_of_bisectors):\n            for j in range(self.max_iterations):\n                points = np.random.choice(polygon, 2, replace=False)\n                distance_between_points = np.linalg.norm(points[0] - points[1])\n                if distance_between_points > min_distance_between_sector_points_in_px:\n                    bisectors_start_points[i] = points[0]\n                    bisectors_end_points[i] = points[1]\n                    break\n            else:\n                raise EyeCentersEstimationError(\n                    \"Failed to find enough points pairs with a distance greater than the minimum distance.\"\n                )\n\n        return bisectors_start_points, bisectors_end_points\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Get the number of points in the polygon\n        n_points = len(polygon)\n\n        # Initialize an empty list to store the perpendicular bisectors\n        perpendicular_bisectors = []\n\n        # Iterate until the number of perpendicular bisectors is equal to the number of points in the polygon\n        while len(perpendicular_bisectors) < n_points:\n            # Generate two random indices between 0 and n_points - 1\n            idx1, idx2 = np.random.randint(0, n_points, size=2)\n\n            # Get the corresponding points from the polygon\n            point1 = polygon[idx1]\n            point2 = polygon[idx2]\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point1 - point2)\n\n            # Check if the distance between the two points is greater than the minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the perpendicular bisector of the line segment formed by the two points\n                bisector = self._calculate_perpendicular_bisector(point1, point2)\n\n                # Add the bisector to the list of perpendicular bisectors\n                perpendicular_bisectors.append(bisector)\n\n        # Convert the list of perpendicular bisectors to a numpy array\n        perpendicular_bisectors = np.array(perpendicular_bisectors)\n\n        # Separate the starting and ending points of the perpendicular bisectors\n        bisector_starts = perpendicular_bisectors[:, 0, :]\n        bisector_ends = perpendicular_bisectors[:, 1, :]\n\n        # Return the starting and ending points of the perpendicular bisectors\n        return bisector_starts, bisector_ends\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize variables\n        num_bisectors = self.num_bisectors\n        max_iterations = self.max_iterations\n        i = 0\n        points_1 = np.zeros((num_bisectors, 2))\n        points_2 = np.zeros((num_bisectors, 2))\n        bisectors_found = 0\n\n        # Loop until the desired number of bisectors are found or the maximum number of iterations is reached\n        while bisectors_found < num_bisectors and i < max_iterations:\n            # Choose two random points from the polygon\n            point_1, point_2 = np.random.choice(polygon, size=2, replace=False)\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point_1 - point_2)\n\n            # Check if the distance between the two points is greater than the minimum distance specified\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the midpoint of the two points\n                midpoint = (point_1 + point_2) / 2\n\n                # Calculate the vector from the midpoint to a point on the circumference of the circle\n                vector = point_1 - midpoint\n\n                # Calculate the vector perpendicular to the previous vector\n                perpendicular_vector = np.array([-vector[1], vector[0]])\n\n                # Normalize the perpendicular vector\n                unit_vector = perpendicular_vector / np.linalg.norm(perpendicular_vector)\n\n                # Calculate the starting point of the perpendicular bisector\n                start_point = midpoint - unit_vector * distance / 2\n\n                # Calculate the ending point of the perpendicular bisector\n                end_point = midpoint + unit_vector * distance / 2\n\n                # Store the starting and ending points of the perpendicular bisector\n                points_1[bisectors_found] = start_point\n                points_2[bisectors_found] = end_point\n\n                # Increment the number of bisectors found\n                bisectors_found += "}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize an empty list to store the bisectors\n        bisectors = []\n\n        # Iterate over the number of bisectors to calculate\n        for _ in range(self.number_of_bisectors):\n\n            # Initialize variables to keep track of the number of iterations and whether a suitable pair of points has been found\n            iterations = 0\n            suitable_pair_of_points_found = False\n\n            # Continue iterating until a suitable pair of points is found or the maximum number of iterations is reached\n            while not suitable_pair_of_points_found:\n\n                # Increment the iteration counter\n                iterations += 1\n\n                # If the maximum number of iterations has been reached, raise an error\n                if iterations > self.max_iterations:\n                    raise EyeCentersEstimationError(\n                        \"Failed to find a pair of points that meet the distance criterion within the maximum number of iterations.\"\n                    )\n\n                # Choose two random points from the polygon's vertices\n                point_1 = polygon[np.random.randint(len(polygon))]\n                point_2 = polygon[np.random.randint(len(polygon))]\n\n                # Calculate the distance between the two points\n                distance = np.linalg.norm(point_1 - point_2)\n\n                # If the distance between the two points is greater than the minimum distance specified,\n                # calculate the perpendicular bisector and add it to the list of bisectors\n                if distance > min_distance_between_sector_points_in_px:\n                    bisector = self._calculate_bisector(point_1, point_2)\n                    bisectors.append(bisector)\n\n                    # Set the suitable pair of points found flag to True\n                    suitable_pair_of_points_found = True\n\n        # Extract the starting and ending points of the perpendicular bisectors\n        starting_points = np.array([bisector[0] for bisector in bisectors])\n        ending_points = np.array([bisector[1] for bisector in bisectors])\n\n        # Return the starting and ending points of the perpendicular bisectors\n        return starting"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Calculate the number of points in the polygon\n        num_points = polygon.shape[0]\n\n        # Initialize variables to track the number of successful iterations\n        num_successful_iterations = 0\n        num_iterations = 0\n\n        # Initialize lists to store the starting and ending points of the perpendicular bisectors\n        bisectors_start = []\n        bisectors_end = []\n\n        # Continue iterating until the required number of successful iterations is reached or the maximum number of iterations is reached\n        while num_successful_iterations < self.num_bisectors and num_iterations < self.max_iterations:\n            # Randomly choose two points from the polygon\n            point_1_idx, point_2_idx = np.random.choice(num_points, size=2, replace=False)\n            point_1 = polygon[point_1_idx]\n            point_2 = polygon[point_2_idx]\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point_1 - point_2)\n\n            # Check if the distance between the points is greater than the specified minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the midpoint between the two points\n                midpoint = (point_1 + point_2) / 2\n\n                # Calculate the vector perpendicular to the line connecting the two points\n                vector = np.array([point_1[1] - point_2[1], point_2[0] - point_1[0]])\n                unit_vector = vector / np.linalg.norm(vector)\n\n                # Calculate the starting and ending points of the perpendicular bisector\n                start = midpoint - unit_vector * distance / 2\n                end = midpoint + unit_vector * distance / 2\n\n                # Append the starting and ending points to the respective lists\n                bisectors_start.append(start)\n                bisectors_end.append(end)\n\n                # Increment the number of successful iterations\n                num_successful_iterations += 1"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize the list to store the perpendicular bisectors\n        perpendicular_bisectors = []\n\n        # Calculate the number of points in the polygon\n        num_points = len(polygon)\n\n        # Check if there are enough points to calculate the specified number of bisectors\n        if num_points < self.num_bisectors:\n            # If there are not enough points, raise an error\n            raise EyeCentersEstimationError(\n                f\"Cannot calculate {self.num_bisectors} bisectors from {num_points} points.\"\n            )\n\n        # Initialize the iteration counter\n        iteration = 0\n\n        # Loop until the required number of bisectors are found or the maximum number of iterations is reached\n        while len(perpendicular_bisectors) < self.num_bisectors and iteration < self.max_iterations:\n            # Increment the iteration counter\n            iteration += 1\n\n            # Randomly choose two points from the polygon\n            point1, point2 = np.random.choice(polygon, size=2, replace=False)\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point2 - point1)\n\n            # Check if the distance between the two points is greater than the specified minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the midpoint of the two points\n                midpoint = (point1 + point2) / 2\n\n                # Calculate the vector from the midpoint to the first point\n                vector = point1 - midpoint\n\n                # Calculate the perpendicular vector\n                perpendicular = np.array([-vector[1], vector[0]])\n\n                # Normalize the perpendicular vector\n                unit_perpendicular = perpendicular / np.linalg.norm(perpendicular)\n\n                # Calculate the starting point of the perpendicular bisector\n                start = midpoint - unit_perpendicular * distance / 2\n\n                # Calculate the ending point of the perpendicular bisector\n                end = midpoint + unit_perpendicular * distance / 2\n\n                # Append the perpendicular bisector to"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize variables for storing the starting and ending points of the perpendicular bisectors\n        bisectors_start_points = np.zeros((self._number_of_bisectors, 2))\n        bisectors_end_points = np.zeros((self._number_of_bisectors, 2))\n\n        # Initialize a counter for the number of iterations\n        iteration_counter = 0\n\n        # Initialize a variable to store the number of valid pairs of points found\n        valid_pair_counter = 0\n\n        # Continue the loop until the required number of valid pairs of points is found or the maximum number of iterations is reached\n        while valid_pair_counter < self._number_of_bisectors and iteration_counter < self._max_iterations:\n            # Increment the iteration counter\n            iteration_counter += 1\n\n            # Generate two random indices from the polygon's vertices\n            random_index_1, random_index_2 = np.random.choice(len(polygon), 2, replace=False)\n\n            # Get the corresponding vertices using the random indices\n            point_1 = polygon[random_index_1]\n            point_2 = polygon[random_index_2]\n\n            # Calculate the distance between the two points\n            distance_between_points = np.linalg.norm(point_1 - point_2)\n\n            # Check if the distance between the points is greater than the specified minimum distance\n            if distance_between_points > min_distance_between_sector_points_in_px:\n                # If the distance is sufficient, calculate the perpendicular bisector of the two points\n                bisector = self._calculate_perpendicular_bisector(point_1, point_2)\n\n                # Store the starting and ending points of the perpendicular bisector\n                bisectors_start_points[valid_pair_counter] = bisector[0]\n                bisectors_end_points[valid_pair_counter] = bisector[1]\n\n                # Increment the valid pair counter\n                valid_pair_counter += 1\n\n        # If the required number of valid pairs of points is not found within the maximum number"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(*args, **kwargs)\n        result = self.run(*args, **kwargs)\n        for callback in self._callbacks:\n            callback.post_execute(*args, **kwargs)\n        return result\n\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execution(self, *args, **kwargs)\n        result = self.run(*args, **kwargs)\n        for callback in self._callbacks:\n            callback.after_execution(self, *args, **kwargs)\n        return result\n\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self.pre_execute(*args, **kwargs)\n        result = self.run(*args, **kwargs)\n        self.post_execute(*args, **kwargs)\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(*args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(*args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        # Execute pre-execution hooks\n        for callback in self._callbacks:\n            callback.pre_execute(*args, **kwargs)\n\n        # Execute main algorithm\n        result = self.run(*args, **kwargs)\n\n        # Execute post-execution hooks\n        for callback in self._callbacks:\n            callback.post_execute(*args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self._pre_execute()\n        result = self.run(*args, **kwargs)\n        self._post_execute()\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execution(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execution(self, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        # Execute pre-execution hooks\n        for callback in self._callbacks:\n            callback.pre_execute(*args, **kwargs)\n\n        # Run the main algorithm\n        result = self.run(*args, **kwargs)\n\n        # Execute post-execution hooks\n        for callback in self._callbacks:\n            callback.post_execute(*args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(*args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(*args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        # Execute pre-execution callbacks\n        for callback in self._callbacks:\n            callback.pre_execute(*args, **kwargs)\n\n        # Execute the main algorithm\n        result = self.run(*args, **kwargs)\n\n        # Execute post-execution callbacks\n        for callback in self._callbacks:\n            callback.post_execute(*args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(*args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(*args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        # Execute pre-execution hooks\n        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        # Execute the main algorithm\n        result = self.run(*args, **kwargs)\n\n        # Execute post-execution hooks\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_run(*args, **kwargs)\n        result = self.run(*args, **kwargs)\n        for callback in self._callbacks:\n            callback.after_run(*args, **kwargs)\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self.pre_execution_callbacks(*args, **kwargs)\n        result = self.run(*args, **kwargs)\n        self.post_execution_callbacks(*args, **kwargs)\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self.pre_execute(*args, **kwargs)\n        result = self.run(*args, **kwargs)\n        self.post_execute(*args, **kwargs)\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self.pre_execute()\n        result = self.run(*args, **kwargs)\n        self.post_execute()\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(*args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(*args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(*args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(*args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        # Execute pre-execution hooks\n        for callback in self._callbacks:\n            callback.pre_execute(*args, **kwargs)\n\n        # Execute the main algorithm\n        result = self.run(*args, **kwargs)\n\n        # Execute post-execution hooks\n        for callback in self._callbacks:\n            callback.post_execute(*args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_run(*args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_run(*args, **kwargs)\n\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output_obj = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return check_type(output_obj, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output_dict = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(output_dict, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return check_type(deserialized_output, type_definition)\n\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output_dict = json.loads(output)\n            return check_type(output_dict, type_definition)\n        except (json.JSONDecodeError, ValueError):\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.annotation is not param.empty:\n                if param.name in type_hints:\n                    input_type_hints[param.name] = type_hints[param.name]\n\n        if signature.return_annotation is not signature.empty:\n            if \"return\" in type_hints:\n                output_type_hints[\"return\"] = type_hints[\"return\"]\n\n        input_class_definitions = {\n            param_name: get_class_definition(param_type)\n            for param_name, param_type in input_type_hints.items()\n        }\n        output_class_definitions = {\n            param_name: get_class_definition(param_type)\n            for param_name, param_type in output_type_hints.items()\n        }\n\n        output_type = output_type_hints.get(\"return\", None)\n        function_type = FunctionType.SYMBOLIC\n\n        if isinstance(output_type, type) and issubclass(output_type, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif isinstance(output_type, Union):\n            for arg in output_type.__args__:\n                if isinstance(arg, type) and issubclass(arg, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                    break\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definitions=output_class_definitions,\n            function_type=function"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n\n        input_type_hints = {\n            parameter: type_hints[parameter]\n            for parameter in signature.parameters\n            if parameter in type_hints\n        }\n\n        output_type_hint = type_hints[\"return\"]\n\n        input_class_definitions = {\n            parameter: get_class_definition(type_hint)\n            for parameter, type_hint in input_type_hints.items()\n        }\n\n        if is_union(output_type_hint):\n            output_type = FunctionType.SYMBOLIC\n            output_class_definition = None\n        elif issubclass(output_type_hint, Embedding):\n            output_type = FunctionType.EMBEDDABLE\n            output_class_definition = get_class_definition(output_type_hint)\n        else:\n            output_type = FunctionType.SYMBOLIC\n            output_class_definition = get_class_definition(output_type_hint)\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hint=output_type_hint,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output_class_definition,\n            function_type=output_type,\n        )"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get the function's signature and type hints\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Extract the input and output type hints\n        input_type_hints = []\n        output_type_hint = None\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.POSITIONAL_OR_KEYWORD:\n                input_type_hints.append(type_hints.get(param_name, None))\n            elif param.kind == param.VAR_POSITIONAL:\n                input_type_hints.append(Tuple[type_hints.get(param_name, None)])\n            elif param.kind == param.VAR_KEYWORD:\n                input_type_hints.append(Dict[str, type_hints.get(param_name, None)])\n\n        output_type_hint = type_hints.get('return', None)\n\n        # Determine the function type based on the output type hint\n        if output_type_hint is None:\n            function_type = FunctionType.SYMBOLIC\n        elif issubclass(output_type_hint, Embedding) or (\n                isinstance(output_type_hint, type) and issubclass(output_type_hint, Union) and any(\n                issubclass(arg, Embedding) for arg in output_type_hint.__args__)):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        # Get the class definitions for the input and output types\n        input_class_definitions = [get_class_definition(input_type) for input_type in input_type_hints]\n        output_class_definition = get_class_definition(output_type_hint)\n\n        # Create the FunctionDescription object\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=func_object.__doc__,\n            input_type_hints="}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get the function's signature and type hints\n        sig = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Extract input and output type hints\n        input_type_hints = {}\n        output_type_hint = None\n        for param_name, param in sig.parameters.items():\n            if param.kind == param.POSITIONAL_OR_KEYWORD:\n                input_type_hints[param_name] = type_hints.get(param_name, Any)\n            elif param.kind == param.VAR_POSITIONAL:\n                input_type_hints[param_name] = Tuple[type_hints.get(param_name, Any), ...]\n            elif param.kind == param.VAR_KEYWORD:\n                input_type_hints[param_name] = Dict[str, type_hints.get(param_name, Any)]\n            else:\n                raise ValueError(f\"Unsupported parameter kind: {param.kind}\")\n\n        output_type_hint = type_hints.get('return', Any)\n\n        # Get class definitions for input and output types\n        input_class_definitions = {\n            param_name: get_class_definition(param_type)\n            for param_name, param_type in input_type_hints.items()\n        }\n\n        # Determine the function type based on the output type hint\n        if isclass(output_type_hint) or (\n                isinstance(output_type_hint, type) and issubclass(output_type_hint, Embedding)):\n            output_class_definition = get_class_definition(output_type_hint)\n            function_type = FunctionType.EMBEDDABLE\n        elif isinstance(output_type_hint, type) and issubclass(output_type_hint, Union):\n            output_class_definition = get_class_definition(output_type_hint.__args__[0])\n            function_type = FunctionType.EMBEDDABLE\n        else:"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get the function signature\n        func_signature = inspect.signature(func_object)\n\n        # Get the type hints for the function's parameters\n        func_type_hints = get_type_hints(func_object)\n\n        # Get the function's docstring\n        func_docstring = inspect.getdoc(func_object)\n\n        # Determine the input and output type hints\n        input_type_hints = [\n            func_type_hints[param]\n            for param in func_signature.parameters\n            if param != \"return\"\n        ]\n        output_type_hint = func_type_hints[\"return\"]\n\n        # Get the class definitions for the input and output types\n        input_class_definitions = [\n            get_class_definition(input_type_hint)\n            for input_type_hint in input_type_hints\n        ]\n        output_class_definition = get_class_definition(output_type_hint)\n\n        # Determine the function type based on the output type hint\n        if isinstance(output_type_hint, type) and issubclass(\n            output_type_hint, Embedding\n        ):\n            function_type = FunctionType.EMBEDDABLE\n        elif (\n            isinstance(output_type_hint, type)\n            and issubclass(output_type_hint, Union)\n            and any(\n                isinstance(arg, type) and issubclass(arg, Embedding)\n                for arg in output_type_hint.__args__\n            )\n        ):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        # Create the function description\n        function_description = FunctionDescription(\n            name=func_object.__name__,\n            docstring=func_docstring,\n            input_type_hints=input_type_hints,\n            output_type_hint=output_type_hint,\n            input_class_definitions=input_class_definitions,\n            output_class"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get the function's signature and type hints\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Extract the input and output type hints\n        input_type_hints = {\n            param.name: type_hint\n            for param, type_hint in type_hints.items()\n            if param != \"return\"\n        }\n        output_type_hint = type_hints[\"return\"]\n\n        # Get the docstring of the function\n        docstring = func_object.__doc__\n\n        # Get the class definitions for the input and output types\n        input_class_definitions = {\n            param: get_class_definition(type_hint)\n            for param, type_hint in input_type_hints.items()\n        }\n        output_class_definition = get_class_definition(output_type_hint)\n\n        # Determine the function type based on the output type hint\n        if is_embedding_class(output_type_hint):\n            function_type = FunctionType.EMBEDDABLE\n        elif is_union_type(output_type_hint) and any(\n            is_embedding_class(arg) for arg in get_args(output_type_hint)\n        ):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        # Create and return the FunctionDescription object\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hint=output_type_hint,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n        input_type_hint = type_hints.get('input')\n        output_type_hint = type_hints.get('return')\n        input_class_definition = get_class_definition(input_type_hint)\n        output_class_definition = get_class_definition(output_type_hint)\n        function_type = FunctionType.SYMBOLIC\n\n        if output_type_hint is not None:\n            if isclass(output_type_hint) and issubclass(output_type_hint, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n            elif isinstance(output_type_hint, Union):\n                for arg in output_type_hint.__args__:\n                    if isclass(arg) and issubclass(arg, Embedding):\n                        function_type = FunctionType.EMBEDDABLE\n                        break\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hint=input_type_hint,\n            output_type_hint=output_type_hint,\n            input_class_definition=input_class_definition,\n            output_class_definition=output_class_definition,\n            function_type=function_type\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        func_name = func_object.__name__\n        func_signature = inspect.signature(func_object)\n        func_type_hints = get_type_hints(func_object)\n        func_docstring = func_object.__doc__\n\n        func_input_type_hints = {\n            param_name: param_type\n            for param_name, param_type in func_type_hints.items()\n            if param_name in func_signature.parameters\n        }\n\n        func_output_type_hint = func_type_hints.get(\"return\", None)\n\n        func_input_class_defs = {\n            param_name: get_class_definition(param_type)\n            for param_name, param_type in func_input_type_hints.items()\n        }\n\n        func_output_class_def = None\n        if func_output_type_hint is not None:\n            if isinstance(func_output_type_hint, type) and issubclass(\n                func_output_type_hint, Embedding\n            ):\n                func_output_class_def = get_class_definition(func_output_type_hint)\n            elif (\n                isinstance(func_output_type_hint, GenericMeta)\n                and func_output_type_hint.__origin__ is Union\n            ):\n                func_output_class_def = get_class_definition(\n                    func_output_type_hint.__args__[0]\n                )\n\n        func_type = (\n            FunctionType.SYMBOLIC\n            if func_output_class_def is None\n            else FunctionType.EMBEDDABLE\n        )\n\n        return FunctionDescription(\n            name=func_name,\n            docstring=func_docstring,\n            input_type_hints=func_input_type_hints,\n            input_class_defs=func_input_class_defs,\n            output_type_hint=func_output_type_hint,\n            output_class_def=func_output_class_"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hint = type_hints.get('input', None)\n        output_type_hint = type_hints.get('return', None)\n\n        input_class_definition = get_class_definition(input_type_hint)\n        output_class_definition = get_class_definition(output_type_hint)\n\n        if output_type_hint and issubclass(output_type_hint, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif output_type_hint and issubclass(output_type_hint, Union):\n            function_type = FunctionType.EMBEDDABLE\n            output_class_definition = get_class_definition(output_type_hint.__args__[0])\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hint=input_type_hint,\n            output_type_hint=output_type_hint,\n            input_class_definition=input_class_definition,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.POSITIONAL_OR_KEYWORD:\n                if param.annotation is not param.empty:\n                    input_type_hints[param_name] = param.annotation\n            elif param.kind == param.VAR_POSITIONAL:\n                input_type_hints[param_name] = Tuple[param.annotation, ...]\n            elif param.kind == param.VAR_KEYWORD:\n                input_type_hints[param_name] = Dict[str, param.annotation]\n        if signature.return_annotation is not signature.empty:\n            output_type_hints = signature.return_annotation\n        input_class_definitions = {\n            param_name: get_class_definition(param_type)\n            for param_name, param_type in input_type_hints.items()\n        }\n        output_class_definitions = get_class_definition(output_type_hints)\n        if (\n            isinstance(output_class_definitions, ClassDefinition)\n            and issubclass(output_class_definitions.cls, Embedding)\n        ) or (\n            isinstance(output_class_definitions, Union)\n            and any(\n                issubclass(get_class_definition(t).cls, Embedding)\n                for t in output_class_definitions.__args__\n            )\n        ):\n            function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = FunctionType.EMBEDDABLE\n        return FunctionDescription(\n            func_object.__name__,\n            docstring,\n            input_type_hints,\n            input_class_definitions,\n            output_type_hints,\n            output_class_definitions,\n            function_type,\n        )"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n\n        input_type_hint = type_hints.pop('input', None)\n        input_class_definition = get_class_definition(input_type_hint)\n\n        output_type_hint = type_hints.pop('output', None)\n        output_class_definition = get_class_definition(output_type_hint)\n\n        if isinstance(output_type_hint, type) and issubclass(output_type_hint, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif isinstance(output_type_hint, UnionType) and any(\n                issubclass(arg, Embedding) for arg in output_type_hint.__args__\n        ):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hint=input_type_hint,\n            output_type_hint=output_type_hint,\n            input_class_definition=input_class_definition,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get the function signature\n        signature = inspect.signature(func_object)\n\n        # Get the type hints\n        type_hints = get_type_hints(func_object)\n\n        # Get the function name\n        func_name = func_object.__name__\n\n        # Get the docstring\n        docstring = func_object.__doc__\n\n        # Determine the input type hint\n        input_type_hint = type_hints.get(\"input\")\n\n        # Determine the output type hint\n        output_type_hint = type_hints.get(\"return\")\n\n        # Get the class definition for the input type hint\n        input_class_definition = get_class_definition(input_type_hint)\n\n        # Get the class definition for the output type hint\n        output_class_definition = get_class_definition(output_type_hint)\n\n        # Determine the function type based on the output type hint\n        function_type = FunctionType.SYMBOLIC\n        if output_type_hint is not None:\n            if issubclass(output_type_hint, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n            elif (\n                isinstance(output_type_hint, type)\n                and issubclass(output_type_hint, Union)\n                and any(\n                    issubclass(arg, Embedding)\n                    for arg in get_args(output_type_hint)\n                )\n            ):\n                function_type = FunctionType.EMBEDDABLE\n\n        # Create a FunctionDescription instance\n        function_description = FunctionDescription(\n            name=func_name,\n            docstring=docstring,\n            input_type_hint=input_type_hint,\n            output_type_hint=output_type_hint,\n            input_class_definition=input_class_definition,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )\n\n        return function_description\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get the function's signature\n        sig = inspect.signature(func_object)\n\n        # Get the function's type hints\n        type_hints = get_type_hints(func_object)\n\n        # Get the function's docstring\n        docstring = inspect.getdoc(func_object)\n\n        # Extract the input type hints\n        input_type_hints = {\n            param_name: param_type\n            for param_name, param_type in type_hints.items()\n            if param_name in sig.parameters\n        }\n\n        # Extract the output type hint\n        output_type_hint = type_hints.get(\"return\")\n\n        # Get the class definition for the input type hints\n        input_class_definitions = {\n            param_name: get_class_definition(param_type)\n            for param_name, param_type in input_type_hints.items()\n        }\n\n        # Get the class definition for the output type hint\n        if isinstance(output_type_hint, Class):\n            output_class_definition = output_type_hint\n            function_type = FunctionType.SYMBOLIC\n        elif isinstance(output_type_hint, Union):\n            output_class_definition = get_class_definition(output_type_hint)\n            function_type = FunctionType.SYMBOLIC\n        elif isinstance(output_type_hint, Embedding):\n            output_class_definition = get_class_definition(output_type_hint)\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            output_class_definition = None\n            function_type = FunctionType.SYMBOLIC\n\n        # Create the FunctionDescription object\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hint=output_type_hint,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output_class"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n        input_type_hint = type_hints.get(\"input\", None)\n        output_type_hint = type_hints.get(\"return\", None)\n        input_class_definition = get_class_definition(input_type_hint)\n        output_class_definition = get_class_definition(output_type_hint)\n        function_type = FunctionType.SYMBOLIC\n\n        if output_type_hint is not None:\n            if isclass(output_type_hint) or (\n                isinstance(output_type_hint, type) and issubclass(output_type_hint, Union)\n            ):\n                output_type_hint = get_args(output_type_hint)\n                if len(output_type_hint) == 1:\n                    output_type_hint = output_type_hint[0]\n                else:\n                    output_class_definition = []\n                    for output_type in output_type_hint:\n                        output_class_definition.append(get_class_definition(output_type))\n            if issubclass(output_type_hint, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hint=input_type_hint,\n            output_type_hint=output_type_hint,\n            input_class_definition=input_class_definition,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hint = type_hints.get(\"input\", None)\n        input_class_definition = get_class_definition(input_type_hint)\n\n        output_type_hint = type_hints.get(\"return\", None)\n        output_class_definition = get_class_definition(output_type_hint)\n\n        if output_type_hint is not None and issubclass(output_type_hint, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif output_type_hint is not None and (\n            issubclass(output_type_hint, Union)\n            or (\n                hasattr(output_type_hint, \"__origin__\")\n                and issubclass(output_type_hint.__origin__, Embedding)\n            )\n        ):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hint=input_type_hint,\n            input_class_definition=input_class_definition,\n            output_type_hint=output_type_hint,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        sig = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n\n        input_type_hints = {\n            param: type_hints[param]\n            for param in sig.parameters\n            if param in type_hints\n        }\n        output_type_hint = type_hints.get(\"return\", None)\n\n        input_class_definitions = {\n            param: get_class_definition(input_type_hints[param])\n            for param in input_type_hints\n        }\n        output_class_definition = get_class_definition(output_type_hint)\n\n        function_type = FunctionType.SYMBOLIC\n        if (\n            output_type_hint is not None\n            and is_subclass_or_union(output_type_hint, Embedding)\n        ):\n            function_type = FunctionType.EMBEDDABLE\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hint=output_type_hint,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        sig = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Get the input type hint\n        input_type_hint = type_hints.get(\"input\")\n        if input_type_hint is None:\n            input_type_hint = type_hints.get(\"inputs\")\n\n        # Get the output type hint\n        output_type_hint = type_hints.get(\"output\")\n        if output_type_hint is None:\n            output_type_hint = type_hints.get(\"outputs\")\n\n        # Get the class definitions for input and output types\n        input_class_definition = get_class_definition(input_type_hint)\n        output_class_definition = get_class_definition(output_type_hint)\n\n        # Determine the function type based on the output type hint\n        if output_type_hint is not None:\n            if isclass(output_type_hint) and issubclass(output_type_hint, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n            elif isinstance(output_type_hint, Union):\n                function_type = FunctionType.EMBEDDABLE\n                output_class_definition = get_class_definition(output_type_hint.__args__)\n            else:\n                function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        # Create and return the FunctionDescription instance\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=func_object.__doc__,\n            input_type_hint=input_type_hint,\n            output_type_hint=output_type_hint,\n            input_class_definition=input_class_definition,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n\n        input_type_hint = type_hints.get('input')\n        output_type_hint = type_hints.get('return')\n\n        input_class_definition = get_class_definition(input_type_hint)\n        output_class_definition = get_class_definition(output_type_hint)\n\n        if issubclass(output_type_hint, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif isinstance(output_type_hint, type) and issubclass(output_type_hint, Union):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hint=input_type_hint,\n            output_type_hint=output_type_hint,\n            input_class_definition=input_class_definition,\n            output_class_definition=output_class_definition,\n            function_type=function_type\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hint = type_hints.pop('input')\n        input_class_definition = get_class_definition(input_type_hint)\n\n        output_type_hint = type_hints.pop('return')\n        output_class_definition = get_class_definition(output_type_hint)\n\n        if is_subclass(output_type_hint, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif is_subclass(output_type_hint, Union):\n            function_type = FunctionType.SYMBOLIC\n            output_class_definition = get_class_definition(output_type_hint.__args__[0])\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hint=input_type_hint,\n            output_type_hint=output_type_hint,\n            input_class_definition=input_class_definition,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        sig = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hints = {}\n        for param_name, param in sig.parameters.items():\n            if param.annotation != inspect._empty:\n                input_type_hints[param_name] = param.annotation\n\n        output_type_hint = sig.return_annotation\n        output_class_definition = None\n\n        if output_type_hint is not inspect._empty:\n            output_class_definition = get_class_definition(output_type_hint)\n\n        function_type = FunctionType.SYMBOLIC\n        if is_embeddable(output_class_definition):\n            function_type = FunctionType.EMBEDDABLE\n        elif issubclass(output_class_definition, Union) and any(\n            is_embeddable(get_class_definition(arg))\n            for arg in get_args(output_class_definition)\n        ):\n            function_type = FunctionType.EMBEDDABLE\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hint=output_type_hint,\n            input_class_definitions={\n                param_name: get_class_definition(type_hint)\n                for param_name, type_hint in input_type_hints.items()\n            },\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )\n\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        pass\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        pass\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for seed in range(self.hash_count):\n            result = mmh3.hash(string, seed) % self.bit_size\n            self.bit_array[result] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for seed in range(self.hash_count):\n            result = mmh3.hash(string, seed) % self.bit_size\n            self.bit_array[result] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for seed in range(self.hash_count):\n            result = mmh3.hash(string, seed) % self.bit_size\n            self.bit_array[result] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        pass\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        pass\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for seed in range(self.hash_count):\n            result = mmh3.hash(string, seed) % self.size\n            self.bit_array[result] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for seed in range(self.hash_count):\n            result = mmh3.hash(string, seed) % self.bit_size\n            self.bit_array[result] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for seed in range(self.hash_count):\n            result = mmh3.hash(string, seed) % self.bit_size\n            self.bit_array[result] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for seed in range(self.hash_count):\n            result = mmh3.hash(string, seed) % self.bit_size\n            self.bit_array[result] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for seed in range(self.hash_count):\n            result = mmh3.hash(string, seed) % self.bit_size\n            self.bit_array[result] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        pass\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for seed in range(self.hash_count):\n            result = mmh3.hash(string, seed) % self.bit_size\n            self.bit_array[result] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for seed in range(self.hash_count):\n            result = mmh3.hash(string, seed) % self.size\n            self.bit_array[result] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for seed in range(self.hash_count):\n            result = mmh3.hash(string, seed) % self.bit_size\n            self.bit_array[result] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        pass\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for fn_name in hashlib.algorithms_guaranteed:\n            try:\n                loc = self.get_hash(fn_name, string)\n                self.bit_array[loc] = 1\n            except:\n                pass\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        pass\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        pass\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        # Load the bit array from persistence\n        bit_array = self.persistence.load()\n\n        # Calculate the expected length based on the BloomFilter size\n        expected_length = self.size // 8 + 1\n\n        # Check if the loaded bit array's length matches the expected length\n        if len(bit_array) != expected_length:\n            # If there is a mismatch, log a warning and reinitialize the bit array and indices\n            logging.warning(\n                \"Bit array length mismatch. Expected: %d, Loaded: %d\",\n                expected_length,\n                len(bit_array),\n            )\n            self.init_bit_array()\n            self.init_indices()\n            self.save()\n        else:\n            # If the lengths match, update the bit array and indices\n            self.bit_array = bit_array\n            self.init_indices()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        if self.persistence.load():\n            if len(self.bit_array) != self.size:\n                logging.warning('BloomFilter bit array size mismatch. Reinitializing and saving.')\n                self.init_bit_array()\n                self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            self.bit_array = self.persistence.load()\n            if len(self.bit_array) != self.size:\n                logging.warning(\n                    \"Loaded bit array length does not match expected length. Reinitializing and saving.\"\n                )\n                self.init_bit_array()\n                self.save()\n        except Exception as e:\n            logging.error(f\"Error loading bit array: {e}\")\n            self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        # Load the bit array from persistence\n        loaded_bit_array = self.persistence.load()\n\n        # Calculate the expected length based on the BloomFilter size\n        expected_length = self.size\n\n        # Check if the loaded bit array's length matches the expected length\n        if len(loaded_bit_array) != expected_length:\n            # Log a warning indicating potential corruption\n            logging.warning(\"Loaded bit array length does not match expected length. Possible corruption detected. Reinitializing and saving.\")\n\n            # Reinitialize the bit array and indices\n            self.init_bit_array()\n            self.save()\n\n        # Assign the loaded bit array to the BloomFilter instance\n        self.bit_array = loaded_bit_array\n\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            self.bit_array = self.persistence.load()\n        except Exception as e:\n            self.logger.warning(\n                f\"Error loading bit array from persistence: {e}. Reinitializing and saving new bit array.\"\n            )\n            self.init_bit_array()\n            self.save()\n            return\n\n        expected_length = self.size // 8 + (1 if self.size % 8 else 0)\n        if len(self.bit_array) != expected_length:\n            self.logger.warning(\n                f\"Loaded bit array length ({len(self.bit_array)}) does not match expected length ({expected_length}). Reinitializing and saving new bit array.\"\n            )\n            self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array, indices = self.persistence.load()\n        expected_length = self.size // 8\n        if len(bit_array) != expected_length:\n            logging.warning(\n                f\"Loaded bit array length {len(bit_array)} does not match expected length {expected_length}. Reinitializing and saving.\"\n            )\n            self.init_bit_array()\n            self.save()\n        else:\n            self.bit_array = bit_array\n            self.indices = indices\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        # Load the bit array from persistence\n        bit_array = self.persistence.load()\n\n        # Check if the loaded bit array's length matches the expected length\n        expected_length = self.size // 8\n        if len(bit_array) != expected_length:\n            # Log a warning if there is a mismatch\n            logging.warning(\"Loaded bit array length does not match the expected length. Reinitializing and saving.\")\n            # Reinitialize the bit array and indices\n            self.init_bit_array()\n            self.save()\n        else:\n            # Update the bit array and indices\n            self.bit_array = bit_array\n            self.indices = [i for i in range(self.size)]\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        if self.persistence.exists():\n            self.bit_array = self.persistence.load()\n            if len(self.bit_array) != self.size:\n                self.logger.warning(\n                    f\"Loaded bit array length {len(self.bit_array)} does not match expected length {self.size}. Reinitializing and saving.\"\n                )\n                self.init_bit_array()\n                self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        if self.persistence is not None:\n            try:\n                self.bit_array, self.indices = self.persistence.load()\n                if len(self.bit_array) != self.size:\n                    logging.warning(\n                        f\"Loaded bit array length {len(self.bit_array)} does not match expected length {self.size}. Reinitializing and saving...\"\n                    )\n                    self.init_bit_array()\n                    self.save()\n            except Exception as e:\n                logging.warning(f\"Error loading bit array: {e}. Reinitializing and saving...\")\n                self.init_bit_array()\n                self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        self.bit_array = self.persistence.load()\n        if len(self.bit_array) != self.size:\n            logging.warning(\n                f\"Loaded bit array length ({len(self.bit_array)}) does not match expected length ({self.size}). Reinitializing and saving.\"\n            )\n            self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            self.bit_array = self.persistence.load()\n            if len(self.bit_array) != self.size:\n                self.logger.warning(\n                    \"BloomFilter bit array length mismatch. Reinitializing and saving.\")\n                self.init_bit_array()\n                self.save()\n        except Exception as e:\n            self.logger.error(\n                \"Error loading BloomFilter bit array: {}\".format(str(e)))\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            self.bit_array = self.persistence.load()\n        except Exception as e:\n            self.logger.warning(f\"Error loading bit array from persistence: {e}\")\n            self.bit_array = self.init_bit_array()\n            self.save()\n            return\n\n        if len(self.bit_array) != self.size:\n            self.logger.warning(\n                f\"Loaded bit array length ({len(self.bit_array)}) does not match expected length ({self.size}). Reinitializing and saving.\"\n            )\n            self.bit_array = self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            self.bit_array = self.persistence.load()\n        except Exception as e:\n            self.logger.warning(f\"Error loading bit array from persistence: {e}\")\n            self.bit_array = self.init_bit_array()\n            self.indices = self.init_indices()\n            self.save()\n\n        if len(self.bit_array) != self.size:\n            self.logger.warning(\n                f\"Loaded bit array length ({len(self.bit_array)}) does not match expected length ({self.size}). Reinitializing and saving.\"\n            )\n            self.bit_array = self.init_bit_array()\n            self.indices = self.init_indices()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if len(bit_array) != self.size:\n            self.logger.warning('Loaded bit array length does not match expected length. Reinitializing and saving.')\n            self.init_bit_array()\n            self.save()\n        else:\n            self.bit_array = bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        if not self.persistence:\n            return\n\n        try:\n            with open(self.persistence, \"rb\") as f:\n                bit_array = pickle.load(f)\n\n            if len(bit_array) != self.size:\n                logger.warning(\"Loaded bit array length does not match expected length. Reinitializing and saving.\")\n                self.init_bit_array()\n                self.save()\n                return\n\n            self.bit_array = bit_array\n        except FileNotFoundError:\n            logger.warning(\"Persistence file not found. Reinitializing and saving.\")\n            self.init_bit_array()\n            self.save()\n        except Exception as e:\n            logger.error(f\"Error loading from persistence: {e}\")\n\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            self.bit_array = self.persistence.load()\n        except Exception as e:\n            logger.warning(f\"Failed to load bit array from persistence: {e}\")\n            self.init_bit_array()\n            self.save()\n            return\n\n        if len(self.bit_array) != self.size:\n            logger.warning(f\"Loaded bit array length ({len(self.bit_array)}) does not match expected size ({self.size}). Reinitializing and saving.\")\n            self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        expected_length = self.size // 8\n\n        if len(bit_array) != expected_length:\n            logger.warning(\"Loaded bit array length does not match the expected length. Possible data corruption. Reinitializing and saving.\")\n            self.init_bit_array()\n            self.save()\n        else:\n            self.bit_array = bit_array\n            self.indices = [i for i in range(self.size)]\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            self.bit_array = self.persistence.load()\n            if len(self.bit_array) != self.size:\n                logger.warning(\"Loaded bit array length does not match expected length. Reinitializing and saving.\")\n                self.init_bit_array()\n                self.save()\n        except Exception as e:\n            logger.error(f\"Error loading bit array: {e}\")\n            raise\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        self.bit_array = self.persistence.load()\n\n        if len(self.bit_array) != self.size:\n            self.logger.warning(\n                f\"Loaded bit array length ({len(self.bit_array)}) does not match expected length ({self.size}). Reinitializing and saving.\"\n            )\n            self.init_bit_array()\n            self.save()\n\n        self.indices = self.get_indices()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            bit_array = self.persistence.load()\n            if len(bit_array) != self.size:\n                logging.warning(\n                    \"Loaded bit array length does not match expected length. Reinitializing and saving.\"\n                )\n                self.init_bit_array()\n                self.save()\n            else:\n                self.bit_array = bit_array\n        except Exception as e:\n            logging.error(f\"Error loading bit array: {e}\")\n            self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Check if the string is possibly in the Bloom Filter\n        for i in range(self.hash_count):\n            # Generate a hash index using the i-th hash function\n            index = self.hash_functions[i](string) % self.size\n            # Check if the bit at the index is not set\n            if self.bit_array[index] == 0:\n                # If any bit is not set, the string is definitely not in the filter\n                return False\n        # If all bits are set, the string might be in the filter (with a possibility of false positives)\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Use the hash functions to generate indices\n        indices = [hash_function(string) % self.size for hash_function in self.hash_functions]\n\n        # Check if all bits at the generated indices are set\n        for index in indices:\n            if self.bit_array[index] == 0:\n                return False\n\n        # If all bits are set, the string might be in the filter\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Use the hash functions to generate indices in the bit array\n        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            # If any bit is not set, the string is definitely not in the filter\n            if self.bit_array[index] == 0:\n                return False\n        # If all bits are set, the string might be in the filter\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Check if the string is in the filter by checking if all bits at the generated indices are set\n        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Generate indices using the hash functions\n        indices = [hash_function(string) % self.size for hash_function in self.hash_functions]\n\n        # Check if all bits at the generated indices are set\n        for index in indices:\n            if self.bit_array[index] == 0:\n                return False\n\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Initialize a variable to keep track of whether all bits are set\n        all_bits_set = True\n\n        # Loop through each hash function\n        for hash_function in self.hash_functions:\n            # Generate an index using the hash function\n            index = hash_function(string) % self.size\n            # Check if the bit at the index is not set\n            if self.bit_array[index] == 0:\n                # If a bit is not set, set the all_bits_set variable to False\n                all_bits_set = False\n                # Break out of the loop\n                break\n\n        # Return whether all bits are set\n        return all_bits_set\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Iterate over the hash functions\n        for i in range(self.hash_count):\n            # Generate a hash index using the i-th hash function\n            index = self.hash_functions[i](string) % self.size\n            # Check if the bit at the index is not set\n            if self.bit_array[index] == 0:\n                # If a bit is not set, the string is definitely not in the filter\n                return False\n        # If all bits are set, the string might be in the filter\n        return True\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if 'distilled_model' in json_dict:\n            self.distilled_model = json_dict['distilled_model']\n        if 'current_model_stats' in json_dict:\n            self.current_model_stats = json_dict['current_model_stats']\n        if 'last_training_run' in json_dict:\n            self.last_training_run = json_dict['last_training_run']\n        if 'current_training_run' in json_dict:\n            self.current_training_run = json_dict['current_training_run']\n        if 'nr_of_training_runs' in json_dict:\n            self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        self.teacher_models = json_dict['teacher_models']\n\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict.get('distilled_model', None)\n        self.current_model_stats = json_dict.get('current_model_stats', None)\n        self.last_training_run = json_dict.get('last_training_run', None)\n        self.current_training_run = json_dict.get('current_training_run', None)\n        self.nr_of_training_runs = json_dict.get('nr_of_training_runs', 0)\n        self.teacher_models = json_dict.get('teacher_models', [])\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        self.teacher_models = json_dict['teacher_models']\n\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        else:\n            self.teacher_models = None\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if 'distilled_model' in json_dict:\n            self.distilled_model = json_dict['distilled_model']\n        if 'current_model_stats' in json_dict:\n            self.current_model_stats = json_dict['current_model_stats']\n        if 'last_training_run' in json_dict:\n            self.last_training_run = json_dict['last_training_run']\n        if 'current_training_run' in json_dict:\n            self.current_training_run = json_dict['current_training_run']\n        if 'nr_of_training_runs' in json_dict:\n            self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict.get('distilled_model')\n        self.current_model_stats = json_dict.get('current_model_stats')\n        self.last_training_run = json_dict.get('last_training_run')\n        self.current_training_run = json_dict.get('current_training_run')\n        self.nr_of_training_runs = json_dict.get('nr_of_training_runs')\n        self.teacher_models = json_dict.get('teacher_models')\n\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        else:\n            self.teacher_models = []\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        self.teacher_models = json_dict.get('teacher_models', [])\n\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n\n        return self\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if the model is a valid OpenAIConfig object\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model. Must be an instance of OpenAIConfig.\")\n\n        # Check if the system_message is a string\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system_message. Must be a string.\")\n\n        # Check if the prompt is a string\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt. Must be a string.\")\n\n        # Check if the temperature is a float between 0 and 1\n        if \"temperature\" in kwargs and (not isinstance(kwargs[\"temperature\"], float) or kwargs[\"temperature\"] < 0 or kwargs[\"temperature\"] > 1):\n            raise ValueError(\"Invalid temperature. Must be a float between 0 and 1.\")\n\n        # Check if the top_p is a float between 0 and 1\n        if \"top_p\" in kwargs and (not isinstance(kwargs[\"top_p\"], float) or kwargs[\"top_p\"] < 0 or kwargs[\"top_p\"] > 1):\n            raise ValueError(\"Invalid top_p. Must be a float between 0 and 1.\")\n\n        # Check if the frequency_penalty is a float between -2.0 and 2.0\n        if \"frequency_penalty\" in kwargs and (not isinstance(kwargs[\"frequency_penalty\"], float) or kwargs[\"frequency_penalty\"] < -2.0 or kwargs[\"frequency_penalty\"] > 2.0):\n            raise ValueError(\"Invalid frequency_penalty. Must be a float between -2.0 and 2.0.\")\n\n        # Check if the presence_penalty is a float between -2.0 and 2.0\n        if \"presence_penalty\" in kwargs and (not isinstance(kwargs[\"presence_penalty\"], float) or kwargs[\"presence_penalty\"] < -2.0 or kwargs[\"presence_penalty\"] > 2.0):"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if the model name is valid\n        if model.name not in self.models:\n            raise ValueError(f\"Invalid model name: {model.name}\")\n\n        # Check if the model has any parsing helper tokens\n        if not model.parsing_tokens:\n            raise ValueError(f\"Model {model.name} does not have any parsing helper tokens\")\n\n        # Check if the system message is not empty\n        if not system_message:\n            raise ValueError(\"System message cannot be empty\")\n\n        # Check if the prompt is not empty\n        if not prompt:\n            raise ValueError(\"Prompt cannot be empty\")\n\n        # Check if the API key is valid\n        if not self.api_key:\n            raise ValueError(\"API key cannot be empty\")\n\n        # Check if the API key is valid\n        if not self.api_key:\n            raise ValueError(\"API key cannot be empty\")\n\n        # Check if the API key is valid\n        if not self.api_key:\n            raise ValueError(\"API key cannot be empty\")\n\n        # Check if the API key is valid\n        if not self.api_key:\n            raise ValueError(\"API key cannot be empty\")\n\n        # Check if the API key is valid\n        if not self.api_key:\n            raise ValueError(\"API key cannot be empty\")\n\n        # Check if the API key is valid\n        if not self.api_key:\n            raise ValueError(\"API key cannot be empty\")\n\n        # Check if the API key is valid\n        if not self.api_key:\n            raise ValueError(\"API key cannot be empty\")\n\n        # Check if the API key is valid\n        if not self.api_key:\n            raise ValueError(\"API key cannot be empty\")\n\n        # Check if the API key is valid\n        if not self.api_key:\n            raise ValueError(\"API key cannot be empty\")\n\n        # Check if the API key is valid\n        if not self.api_key:\n            raise ValueError(\"API key cannot be empty\")\n\n        # Check if the API key is valid\n        if not self.api_"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        if not self.api_key:\n            raise ValueError(\"OpenAI API key not set.\")\n\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration.\")\n\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message.\")\n\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt.\")\n\n        # Validate additional parameters\n        temperature = kwargs.get(\"temperature\", 0.7)\n        top_p = kwargs.get(\"top_p\", 1)\n        frequency_penalty = kwargs.get(\"frequency_penalty\", 0)\n        presence_penalty = kwargs.get(\"presence_penalty\", 0)\n        max_new_tokens = kwargs.get(\"max_new_tokens\", 256)\n\n        if not isinstance(temperature, float) or temperature < 0 or temperature > 1:\n            raise ValueError(\"Invalid temperature value.\")\n\n        if not isinstance(top_p, float) or top_p < 0 or top_p > 1:\n            raise ValueError(\"Invalid top_p value.\")\n\n        if not isinstance(frequency_penalty, float) or frequency_penalty < 0 or frequency_penalty > 1:\n            raise ValueError(\"Invalid frequency_penalty value.\")\n\n        if not isinstance(presence_penalty, float) or presence_penalty < 0 or presence_penalty > 1:\n            raise ValueError(\"Invalid presence_penalty value.\")\n\n        if not isinstance(max_new_tokens, int) or max_new_tokens < 1:\n            raise ValueError(\"Invalid max_new_tokens value.\")\n\n        # Set up the API request\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            \"temperature\": temperature,\n            \"top_p\": top_p"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if the API key is valid\n        if not self.check_api_key():\n            raise ValueError(\"Invalid API key\")\n\n        # Set the model name and parsing helper tokens\n        model_name = model.model_name\n        parsing_helper_tokens = model.parsing_helper_tokens\n\n        # Validate the parameters\n        if not isinstance(model_name, str) or not isinstance(system_message, str) or not isinstance(prompt, str):\n            raise ValueError(\"Invalid parameter types\")\n\n        # Set the default values for optional parameters\n        temperature = kwargs.get(\"temperature\", 0.5)\n        top_p = kwargs.get(\"top_p\", 1)\n        frequency_penalty = kwargs.get(\"frequency_penalty\", 0)\n        presence_penalty = kwargs.get(\"presence_penalty\", 0)\n        max_new_tokens = kwargs.get(\"max_new_tokens\", 100)\n\n        # Validate the optional parameters\n        if not isinstance(temperature, float) or not isinstance(top_p, float) or not isinstance(frequency_penalty, float) or not isinstance(presence_penalty, float) or not isinstance(max_new_tokens, int):\n            raise ValueError(\"Invalid optional parameter types\")\n\n        # Set the request payload\n        payload = {\n            \"model\": model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"frequency_penalty\": frequency_penalty,\n            \"presence_penalty\": presence_penalty,\n            \"max_tokens\": max_new_tokens\n        }\n\n        # Send the request to the OpenAI API\n        for attempt in range(5):\n            try:\n                response = requests.post(\n                    \"https://api.openai."}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Handle API key verification\n        if not self.api_key:\n            raise ValueError(\"OpenAI API key not set\")\n\n        # Validate parameters\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration\")\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message\")\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt\")\n        if not isinstance(kwargs, dict):\n            raise ValueError(\"Invalid keyword arguments\")\n\n        # Set default values for parameters\n        temperature = kwargs.get(\"temperature\", 0.5)\n        top_p = kwargs.get(\"top_p\", 1.0)\n        frequency_penalty = kwargs.get(\"frequency_penalty\", 0.0)\n        presence_penalty = kwargs.get(\"presence_penalty\", 0.0)\n        max_new_tokens = kwargs.get(\"max_new_tokens\", 1024)\n\n        # Create the request payload\n        payload = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"frequency_penalty\": frequency_penalty,\n            \"presence_penalty\": presence_penalty,\n            \"max_tokens\": max_new_tokens,\n        }\n\n        # Retry the request up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                # Send the request to the OpenAI API\n                response = requests.post(\n                    \"https://api.openai.com/v1/chat/completions\",\n                    headers={\n                        \"Content-Type\": \"application/json\",\n                        \"Authorization\": f\"Bearer {self.api_key}\",\n                    },\n                    data=json.dumps(payload),"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if the API key is set\n        if not self.api_key:\n            raise ValueError(\"OpenAI API key not set\")\n\n        # Check if the model is set\n        if not model:\n            raise ValueError(\"OpenAI model not set\")\n\n        # Check if the system message is set\n        if not system_message:\n            raise ValueError(\"System message not set\")\n\n        # Check if the prompt is set\n        if not prompt:\n            raise ValueError(\"Prompt not set\")\n\n        # Set the default values for the optional parameters\n        temperature = kwargs.get('temperature', 0.7)\n        top_p = kwargs.get('top_p', 1)\n        frequency_penalty = kwargs.get('frequency_penalty', 0)\n        presence_penalty = kwargs.get('presence_penalty', 0)\n        max_new_tokens = kwargs.get('max_new_tokens', 2048)\n\n        # Set the default values for the optional parameters\n        temperature = kwargs.get('temperature', 0.7)\n        top_p = kwargs.get('top_p', 1)\n        frequency_penalty = kwargs.get('frequency_penalty', 0)\n        presence_penalty = kwargs.get('presence_penalty', 0)\n        max_new_tokens = kwargs.get('max_new_tokens', 2048)\n\n        # Create the prompt by combining the system message and user prompt\n        prompt = f\"{system_message}\\n{prompt}\"\n\n        # Set the retry counter to 0\n        retry_counter = 0\n\n        # Set the maximum number of retries to 5\n        max_retries = 5\n\n        # Set the initial retry delay to 1 second\n        retry_delay = 1\n\n        # Set the maximum retry delay to 32 seconds\n        max_retry_delay = 32\n\n        # Set the exponential backoff factor to 2"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        self.verify_api_key()\n\n        # Validate parameters\n        self.validate_parameters(model, system_message, prompt, **kwargs)\n\n        # Set default values for optional parameters\n        temperature = kwargs.get('temperature', 0.7)\n        top_p = kwargs.get('top_p', 1)\n        frequency_penalty = kwargs.get('frequency_penalty', 0)\n        presence_penalty = kwargs.get('presence_penalty', 0)\n        max_new_tokens = kwargs.get('max_new_tokens', 100)\n\n        # Set up retry parameters\n        max_retries = 5\n        retry_delay = 1\n\n        # Generate response\n        for i in range(max_retries):\n            try:\n                response = self.openai.Completion.create(\n                    model=model,\n                    prompt=f\"{system_message}\\n\\n{prompt}\",\n                    temperature=temperature,\n                    top_p=top_p,\n                    frequency_penalty=frequency_penalty,\n                    presence_penalty=presence_penalty,\n                    max_tokens=max_new_tokens,\n                    stop=model.parsing_helper_tokens,\n                )\n                break\n            except openai.error.RateLimitError as e:\n                if i == max_retries - 1:\n                    raise e\n                time.sleep(retry_delay)\n                retry_delay *= 2\n\n        # Process response\n        text = response.choices[0].text.strip()\n        text = self.process_response(text, model)\n\n        return text\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify that the API key is set\n        if not self.api_key:\n            raise ValueError(\"OpenAI API key not set\")\n\n        # Set the model name and parsing helper tokens\n        model_name = model[\"model_name\"]\n        parsing_helper_tokens = model[\"parsing_helper_tokens\"]\n\n        # Validate the parameters\n        if not isinstance(system_message, str):\n            raise ValueError(\"system_message must be a string\")\n        if not isinstance(prompt, str):\n            raise ValueError(\"prompt must be a string\")\n        if not isinstance(parsing_helper_tokens, list):\n            raise ValueError(\"parsing_helper_tokens must be a list\")\n\n        # Set the default parameters\n        default_parameters = {\n            \"temperature\": 0.7,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n            \"max_tokens\": 2048,\n        }\n\n        # Merge the default parameters with the provided parameters\n        parameters = {**default_parameters, **kwargs}\n\n        # Set the retry count and delay\n        retry_count = 0\n        retry_delay = 1\n\n        # Loop until a response is received or the maximum number of retries is reached\n        while retry_count < 5:\n            try:\n                # Send the request to the OpenAI API\n                response = openai.Completion.create(\n                    engine=model_name,\n                    prompt=system_message + prompt,\n                    **parameters\n                )\n\n                # Extract the generated text from the response\n                generated_text = response.choices[0].text\n\n                # Remove any parsing helper tokens from the generated text\n                for token in parsing_helper_tokens:\n                    generated_text = generated_text.replace(token, \"\")\n\n                # Return the generated text\n                return generated_text\n\n            except Exception as e:\n                # If an error occurs, log the error and retry with an exponential backoff\n                logging"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if not self.api_key:\n            raise ValueError(\"API key is not set\")\n\n        # Validate parameters\n        if not isinstance(model, OpenAIConfig):\n            raise TypeError(\"model must be an instance of OpenAIConfig\")\n        if not isinstance(system_message, str):\n            raise TypeError(\"system_message must be a string\")\n        if not isinstance(prompt, str):\n            raise TypeError(\"prompt must be a string\")\n\n        # Retry with exponential backoff up to 5 times\n        for i in range(5):\n            try:\n                # Generate response\n                response = openai.Completion.create(\n                    engine=model.model_name,\n                    prompt=system_message + \"\\n\" + prompt,\n                    **kwargs\n                )\n                break\n            except openai.error.APIError as e:\n                if i == 4:\n                    raise e\n                time.sleep((2 ** i) + random.random())\n\n        # Process response\n        text = response[\"choices\"][0][\"text\"]\n        text = text.replace(model.parsing_helper_token, \"\")\n\n        return text"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if the API key is valid\n        if not self.is_api_key_valid():\n            raise ValueError(\"Invalid API key\")\n\n        # Validate the parameters\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration\")\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message\")\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt\")\n\n        # Set the default values for optional parameters\n        temperature = kwargs.get('temperature', 0.7)\n        top_p = kwargs.get('top_p', 1)\n        frequency_penalty = kwargs.get('frequency_penalty', 0)\n        presence_penalty = kwargs.get('presence_penalty', 0)\n        max_new_tokens = kwargs.get('max_new_tokens', 256)\n\n        # Set the model and prompt for the API request\n        model_engine = model.model_name\n        prompt = f\"{system_message}\\n\\n{prompt}\"\n\n        # Retry the API request up to 5 times with exponential backoff\n        for attempt in range(5):\n            try:\n                # Make the API request\n                response = openai.Completion.create(\n                    engine=model_engine,\n                    prompt=prompt,\n                    temperature=temperature,\n                    top_p=top_p,\n                    frequency_penalty=frequency_penalty,\n                    presence_penalty=presence_penalty,\n                    max_tokens=max_new_tokens,\n                    n=1,\n                    stop=None,\n                    timeout=30,\n                )\n\n                # Process the response to remove any parsing helper tokens\n                text = response.choices[0].text.strip()\n                text = text.replace(model.parsing_helper_tokens, \"\")\n\n                return text\n\n            except Exception as e:\n                # Handle API request errors\n                if attempt == 4:\n                    raise e"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Validate the input parameters\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration. Please provide a valid OpenAIConfig object.\")\n\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message. Please provide a string.\")\n\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt. Please provide a string.\")\n\n        # Set up the API parameters\n        api_params = {\n            \"model\": model.model_name,\n            \"prompt\": prompt,\n            \"temperature\": kwargs.get(\"temperature\", 0.7),\n            \"top_p\": kwargs.get(\"top_p\", 1),\n            \"frequency_penalty\": kwargs.get(\"frequency_penalty\", 0),\n            \"presence_penalty\": kwargs.get(\"presence_penalty\", 0),\n            \"max_tokens\": kwargs.get(\"max_tokens\", 2048),\n            \"n\": kwargs.get(\"n\", 1),\n            \"stream\": kwargs.get(\"stream\", False),\n            \"stop\": kwargs.get(\"stop\", None),\n            \"echo\": kwargs.get(\"echo\", False),\n            \"logprobs\": kwargs.get(\"logprobs\", None),\n            \"best_of\": kwargs.get(\"best_of\", None),\n            \"logit_bias\": kwargs.get(\"logit_bias\", None),\n            \"user\": kwargs.get(\"user\", None),\n        }\n\n        # Set the API key\n        api_key = self.api_key\n\n        # Verify the API key\n        if not api_key:\n            raise ValueError(\"No API key provided. Please set the api_key attribute of the OpenAI_API class.\")\n\n        # Set up the API headers\n        api_headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key}\",\n        }\n\n        # Set up"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify the API key\n        if not self.verify_api_key():\n            return None\n\n        # Validate the parameters\n        if not self.validate_parameters(model, system_message, prompt, **kwargs):\n            return None\n\n        # Set the default values for optional parameters\n        temperature = kwargs.get('temperature', 0.7)\n        top_p = kwargs.get('top_p', 1)\n        frequency_penalty = kwargs.get('frequency_penalty', 0)\n        presence_penalty = kwargs.get('presence_penalty', 0)\n        max_new_tokens = kwargs.get('max_new_tokens', 1024)\n\n        # Set the model and prompt\n        model_engine = model.model_name\n        prompt = f\"{system_message}\\n\\n{prompt}\"\n\n        # Set the retry parameters\n        max_retries = 5\n        retry_delay = 1\n\n        # Generate the response\n        for attempt in range(max_retries):\n            try:\n                response = openai.Completion.create(\n                    engine=model_engine,\n                    prompt=prompt,\n                    temperature=temperature,\n                    top_p=top_p,\n                    frequency_penalty=frequency_penalty,\n                    presence_penalty=presence_penalty,\n                    max_tokens=max_new_tokens\n                )\n                break\n            except openai.error.RateLimitError:\n                if attempt == max_retries - 1:\n                    raise\n                time.sleep(retry_delay)\n                retry_delay *= 2\n\n        # Process the response\n        response_text = response.choices[0].text.strip()\n        response_text = response_text.replace(model.start_token, '').replace(model.end_token, '')\n        response_text = response_text.strip()\n\n        return response_text\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        if not self.api_key:\n            raise Exception(\"OpenAI API key not set\")\n\n        # Check if the model is valid\n        if model not in self.models:\n            raise Exception(f\"Invalid model: {model}\")\n\n        # Check if the system message is valid\n        if not system_message:\n            raise Exception(\"System message not set\")\n\n        # Check if the prompt is valid\n        if not prompt:\n            raise Exception(\"Prompt not set\")\n\n        # Set the default values for optional parameters\n        temperature = kwargs.get(\"temperature\", 0.7)\n        top_p = kwargs.get(\"top_p\", 1)\n        frequency_penalty = kwargs.get(\"frequency_penalty\", 0)\n        presence_penalty = kwargs.get(\"presence_penalty\", 0)\n        max_new_tokens = kwargs.get(\"max_new_tokens\", 1024)\n\n        # Set the default values for the number of retries and the initial backoff time\n        num_retries = 5\n        backoff_time = 1\n\n        # Set the base URL for the OpenAI API\n        base_url = \"https://api.openai.com/v1/completions\"\n\n        # Set the headers for the API request\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\",\n        }\n\n        # Set the data for the API request\n        data = {\n            \"model\": model,\n            \"prompt\": prompt,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"frequency_penalty\": frequency_penalty,\n            \"presence_penalty\": presence_penalty,\n            \"max_tokens\": max_new_tokens,\n            \"n\": 1,\n            \"stop\": None,\n        }\n\n        # Send the API request and retry on failure up to 5 times with exponential backoff\n        for i in range(num_ret"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Initialize the retry counter\n        retry_count = 0\n\n        # Retry the API call up to 5 times with exponential backoff\n        while retry_count < 5:\n            try:\n                # Call the OpenAI API to generate a response\n                response = openai.Completion.create(\n                    model=model,\n                    prompt=prompt,\n                    **kwargs\n                )\n\n                # Process the response to remove any parsing helper tokens\n                response_text = response['choices'][0]['text']\n                response_text = response_text.replace(system_message, '')\n\n                # Return the final text response\n                return response_text\n\n            except openai.error.APIError as e:\n                # Handle API errors\n                if e.http_status == 401:\n                    # Verify the API key\n                    openai.api_key = verify_api_key()\n                else:\n                    # Raise the API error\n                    raise e\n\n            except openai.error.InvalidRequestError as e:\n                # Handle invalid request errors\n                if e.error.code == 'invalid_api_key':\n                    # Verify the API key\n                    openai.api_key = verify_api_key()\n                else:\n                    # Raise the invalid request error\n                    raise e\n\n            except openai.error.RateLimitError as e:\n                # Handle rate limit errors\n                if e.error.code == 'rate_limit':\n                    # Wait for the specified amount of time before retrying\n                    time.sleep(int(e.headers['Retry-After']))\n                else:\n                    # Raise the rate limit error\n                    raise e\n\n            except openai.error.Timeout as e:\n                # Handle timeout errors\n                # Wait for the specified amount of time before retrying\n                time.sleep(int(e.headers['Retry-After']))\n\n            except openai.error.APIConnectionError as e:\n                # Handle connection errors\n                # Wait for the specified amount of time before retrying\n                time.sleep(int(e.headers['Retry-After']))\n\n            # Increment"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if the API key is valid\n        if not self.verify_api_key():\n            return None\n\n        # Define the default parameters for the API request\n        default_params = {\n            \"model\": model,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            \"temperature\": 0.7,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n            \"max_tokens\": 1024,\n        }\n\n        # Update the default parameters with any additional parameters passed in\n        default_params.update(kwargs)\n\n        # Define the retry parameters\n        retries = 5\n        retry_delay = 1\n\n        # Loop through the retries\n        for i in range(retries):\n            # Make the API request\n            response = self.make_request(default_params)\n\n            # Check if the request was successful\n            if response is not None:\n                # Process the response to remove any parsing helper tokens\n                text = self.process_response(response, model)\n\n                # Return the processed text\n                return text\n\n            # If the request was not successful, wait for the retry delay before trying again\n            time.sleep(retry_delay)\n\n            # Increase the retry delay for the next attempt\n            retry_delay *= 2\n\n        # If all retries have failed, return None\n        return None"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Set the API key and model name\n        api_key = self.api_key\n        model_name = model.model_name\n\n        # Check if the API key is valid\n        if not self.verify_api_key(api_key):\n            raise ValueError(\"Invalid API key\")\n\n        # Set the default parameters\n        default_params = {\n            \"temperature\": 0.7,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n            \"max_tokens\": 2048,\n        }\n\n        # Update the default parameters with the provided parameters\n        params = {**default_params, **kwargs}\n\n        # Set the request headers\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key}\",\n        }\n\n        # Set the request data\n        data = {\n            \"model\": model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            \"parameters\": params,\n        }\n\n        # Set the maximum number of retries\n        max_retries = 5\n\n        # Set the initial retry delay in seconds\n        retry_delay = 1\n\n        # Set the number of retries\n        retries = 0\n\n        # Set the response to None\n        response = None\n\n        # Loop until the response is received or the maximum number of retries is reached\n        while response is None and retries < max_retries:\n            try:\n                # Send the request to the OpenAI API\n                response = requests.post(\n                    \"https://api.openai.com/v1/chat/completions\",\n                    headers=headers,\n                    json=data,\n                )\n\n                # Check if the request was successful\n                response.raise_for_status()\n\n                # Get the response JSON\n                response_json = response.json()\n\n                # Check if the response has choices\n                if"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify that the API key is set\n        if not self.api_key:\n            raise ValueError(\"OpenAI API key is not set\")\n\n        # Check if the model is valid\n        if model not in self.models:\n            raise ValueError(f\"Invalid model: {model}\")\n\n        # Check if the system message is valid\n        if not system_message:\n            raise ValueError(\"System message is not set\")\n\n        # Check if the prompt is valid\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # Set the default parameters\n        params = {\n            \"model\": model,\n            \"prompt\": prompt,\n            \"temperature\": 0.7,\n            \"max_tokens\": 256,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n        }\n\n        # Update the parameters with any additional parameters provided\n        params.update(kwargs)\n\n        # Set the number of retries to 5\n        num_retries = 5\n\n        # Set the initial retry delay to 1 second\n        retry_delay = 1\n\n        # Loop until a response is received or the maximum number of retries is reached\n        for i in range(num_retries):\n            try:\n                # Send the request to the OpenAI API\n                response = requests.post(\n                    self.api_url,\n                    headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                    json=params,\n                )\n\n                # Raise an error if the request was not successful\n                response.raise_for_status()\n\n                # Parse the response JSON\n                response_json = response.json()\n\n                # Check if the response contains any parsing helper tokens\n                if \"parsing_helper_tokens\" in response_json:\n                    # Remove the parsing helper tokens from the response\n                    response_text = response_json[\"parsing_helper_tokens\"]\n                else:\n                    # Use the response text directly\n                    response_text = response_json[\""}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Initialize the OpenAI API client with the API key\n        openai.api_key = self.api_key\n\n        # Validate the model name and parsing helper tokens\n        model_name, parsing_helper_tokens = self.validate_model(model)\n\n        # Initialize the retry counter and initial backoff time\n        retry_counter = 0\n        backoff_time = 1\n\n        # Loop until a response is received or the maximum number of retries is reached\n        while True:\n            try:\n                # Send the generation request to the OpenAI API\n                response = openai.Completion.create(\n                    model=model_name,\n                    prompt=system_message + \"\\n\\n\" + prompt,\n                    **kwargs\n                )\n\n                # If the response is received, break out of the loop\n                break\n\n            except openai.error.RateLimitError as e:\n                # If the rate limit is exceeded, wait for the specified backoff time and retry\n                if retry_counter < self.max_retries:\n                    time.sleep(backoff_time)\n                    backoff_time *= 2\n                    retry_counter += 1\n                else:\n                    # If the maximum number of retries is reached, raise an error\n                    raise e\n\n        # Process the response to remove any parsing helper tokens\n        response_text = self.process_response(response, parsing_helper_tokens)\n\n        # Return the generated text response\n        return response_text\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if API key is valid\n        if not self.check_api_key():\n            raise Exception(\"Invalid API key\")\n\n        # Validate model parameters\n        if not self.validate_model_parameters(model, system_message, prompt, **kwargs):\n            raise Exception(\"Invalid model parameters\")\n\n        # Retry generation up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                # Generate response from OpenAI API\n                response = openai.Completion.create(\n                    model=model,\n                    prompt=prompt,\n                    temperature=kwargs.get(\"temperature\", 0.5),\n                    top_p=kwargs.get(\"top_p\", 1),\n                    frequency_penalty=kwargs.get(\"frequency_penalty\", 0),\n                    presence_penalty=kwargs.get(\"presence_penalty\", 0),\n                    max_tokens=kwargs.get(\"max_tokens\", 1024),\n                    n=kwargs.get(\"n\", 1),\n                    stop=kwargs.get(\"stop\", None),\n                    api_key=self.api_key,\n                )\n                break\n            except openai.error.OpenAIError as e:\n                # Handle API errors\n                if i == 4:\n                    raise Exception(f\"Failed to generate response after 5 retries: {e}\")\n                time.sleep((2 ** i) + random.random())\n\n        # Process response to remove parsing helper tokens\n        text = self.process_response(response, model)\n\n        # Return generated text\n        return text\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Initialize the OpenAI API with the provided API key\n        openai.api_key = self.api_key\n\n        # Verify that the API key is valid and set the organization ID\n        self.organization_id = self._verify_api_key()\n\n        # Check if the model is valid and set the model ID\n        self.model_id = self._check_model(model)\n\n        # Validate the parameters for the generation request\n        self._validate_params(**kwargs)\n\n        # Create the parameters for the generation request\n        params = self._create_params(model, system_message, prompt, **kwargs)\n\n        # Retry the generation request up to 5 times with exponential backoff\n        for attempt in range(1, 6):\n            try:\n                # Send the generation request to the OpenAI API\n                response = openai.Completion.create(**params)\n\n                # Process the response to remove any parsing helper tokens\n                text = self._process_response(response, model)\n\n                # Return the generated text\n                return text\n\n            except openai.error.APIError as e:\n                # Handle API errors\n                if e.http_status == 401:\n                    # Invalid API key\n                    raise ValueError(\"Invalid API key\")\n                elif e.http_status == 404:\n                    # Invalid model ID\n                    raise ValueError(\"Invalid model ID\")\n                elif e.http_status == 429:\n                    # Rate limit exceeded\n                    if attempt < 5:\n                        # Wait for the specified amount of time before retrying\n                        time.sleep(2 ** attempt)\n                    else:\n                        # Maximum number of retries reached\n                        raise e\n                else:\n                    # Other API error\n                    raise e\n\n            except Exception as e:\n                # Handle other errors\n                raise e\n\n        # Maximum number of retries reached\n        raise Exception(\"Maximum number of retries reached\")\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2:\n        raise ValueError(\"Matrix must be 2-dimensional.\")\n\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix must be square.\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric.\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2:\n        raise ValueError(\"The input matrix must be 2-dimensional.\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The input matrix must be square.\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The input matrix is not symmetric.\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2:\n        raise ValueError(\"Matrix is not 2-dimensional\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2:\n        raise ValueError(\"The matrix is not 2-dimensional.\")\n\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2:\n        raise ValueError(\"Matrix must be 2-dimensional\")\n\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix must be square\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2:\n        raise ValueError(\"x must be a 2-dimensional array\")\n\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"x must be a square matrix\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"x must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2:\n        raise ValueError(\"Matrix is not 2-dimensional.\")\n\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square.\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square.\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square.\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2:\n        raise ValueError(\"Matrix must be 2-dimensional\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix must be square\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2:\n        raise ValueError(\"The input matrix should be a 2-dimensional numpy array.\")\n\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The input matrix should be square.\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The input matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square.\")\n\n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert len(x.shape) == 2 and x.shape[0] == x.shape[1], \"The matrix is not square.\"\n    assert np.allclose(x, x.T), \"The matrix is not symmetric.\"\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2:\n        raise ValueError(\"x must be a 2-dimensional numpy array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"x must be a square matrix\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"x must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n    # Check if the diagonal elements are close to zero\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0])):\n        raise ValueError(\"The diagonal elements are not close to zero.\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n    # Check if the diagonal elements are close to zero\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0])):\n        raise ValueError(\"The diagonal elements are not close to zero.\")\n\n    # If all checks pass, the matrix is a distance matrix\n    return\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square\")\n\n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric\")\n\n    # Check if the diagonal elements are close to zero\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0])):\n        raise ValueError(\"The diagonal elements are not close to zero\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The given matrix is not a square matrix.\")\n\n    if not np.allclose(x, x.T, rtol=1e-05, atol=1e-08):\n        raise ValueError(\"The given matrix is not symmetric.\")\n\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0]), rtol=1e-05, atol=1e-08):\n        raise ValueError(\"The diagonal elements of the given matrix are not close to zero.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    # Check if the input matrix is a square matrix\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The input matrix is not a square matrix.\")\n\n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The input matrix is not symmetric.\")\n\n    # Check if the diagonal elements are close to zero\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements of the input matrix are not close to zero.\")\n\n    # If all checks pass, the matrix is a distance matrix\n    return\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square\")\n\n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric\")\n\n    # Check if the diagonal elements are close to zero\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0])):\n        raise ValueError(\"The diagonal elements are not close to zero\")\n\n    # If all checks pass, the matrix is a distance matrix\n    return\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input must be a numpy array.\")\n\n    if x.ndim != 2:\n        raise ValueError(\"Input must be a 2-dimensional array.\")\n\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Input must be a square matrix.\")\n\n    if not np.allclose(x, x.T, atol=1e-08):\n        raise ValueError(\"Input must be symmetric.\")\n\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0]), atol=1e-08):\n        raise ValueError(\"Diagonal elements of input must be close to zero.\")\n\n    return None"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n    if not np.allclose(x, x.T, rtol=1e-05, atol=1e-08):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0]), rtol=1e-05, atol=1e-08):\n        raise ValueError(\"The diagonal elements of the matrix are not close to zero.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n    # Check if the diagonal elements are close to zero\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0])):\n        raise ValueError(\"The diagonal elements are not close to zero.\")\n\n    print(\"The matrix is a distance matrix.\")\n\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n    # Check if the diagonal elements are close to zero\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0])):\n        raise ValueError(\"The diagonal elements of the matrix are not close to zero.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n    # Check if the diagonal elements are close to zero\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0])):\n        raise ValueError(\"The diagonal elements are not close to zero.\")\n\n    return None\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    # Check if the input is a numpy array\n    if not isinstance(x, np.ndarray):\n        raise ValueError(\"Input should be a numpy array\")\n\n    # Check if the input is a square matrix\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Input should be a square matrix\")\n\n    # Check if the diagonal elements are close to zero\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0])):\n        raise ValueError(\"Diagonal elements of distance matrix should be close to zero\")\n\n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Distance matrix should be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError('The given matrix is not a square matrix.')\n\n    if not np.allclose(x, x.T, atol=1e-8):\n        raise ValueError('The given matrix is not symmetric.')\n\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0]), atol=1e-8):\n        raise ValueError('The diagonal elements of the given matrix are not close to zero.')\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not a square matrix.\")\n\n    if not np.allclose(x, x.T, rtol=1e-05, atol=1e-08):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0]), rtol=1e-05, atol=1e-08):\n        raise ValueError(\"The diagonal elements of the matrix are not close to zero.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not a square matrix.\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements of the matrix are not close to zero.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n    # Check if the diagonal elements are close to zero\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements are not close to zero.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError('The input matrix must be a square matrix.')\n\n    if not np.allclose(x, x.T, atol=1e-8):\n        raise ValueError('The input matrix must be symmetric.')\n\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0]), atol=1e-8):\n        raise ValueError('The diagonal elements of the input matrix must be close to zero.')\n\n    return None\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix must be square\")\n\n    if not np.allclose(x, x.T, atol=1e-08):\n        raise ValueError(\"Matrix must be symmetric\")\n\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0]), atol=1e-08):\n        raise ValueError(\"Matrix diagonal must be zero\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.ndim != 2:\n        raise ValueError(\"The given matrix is not a distance matrix.\")\n\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The given matrix is not a distance matrix.\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The given matrix is not a distance matrix.\")\n\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The given matrix is not a distance matrix.\")\n\n    return\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n    # Check if the diagonal elements are close to zero\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0])):\n        raise ValueError(\"The diagonal elements are not close to zero.\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if not function_description.is_initialized:\n            function_description.is_initialized = True\n            return self.get_generation_case(args, kwargs, function_description, llm_parameters, func_hash)\n\n        # Check if the function is already initialized\n        if not function_description.is_initialized:\n            function_description.is_initialized = True\n            return self.get_generation_case(args, kwargs, function_description, llm_parameters, func_hash)\n\n        # Check if the function is already initialized\n        if not function_description.is_initialized:\n            function_description.is_initialized = True\n            return self.get_generation_case(args, kwargs, function_description, llm_parameters, func_hash)\n\n        # Check if the function is already initialized\n        if not function_description.is_initialized:\n            function_description.is_initialized = True\n            return self.get_generation_case(args, kwargs, function_description, llm_parameters, func_hash)\n\n        # Check if the function is already initialized\n        if not function_description.is_initialized:\n            function_description.is_initialized = True\n            return self.get_generation_case(args, kwargs, function_description, llm_parameters, func_hash)\n\n        # Check if the function is already initialized\n        if not function_description.is_initialized:\n            function_description.is_initialized = True\n            return self.get_generation_case(args, kwargs, function_description, llm_parameters, func_hash)\n\n        # Check if the function is already initialized\n        if not function_description.is_initialized:\n            function_description.is_initialized = True\n            return self.get_generation_case(args, kwargs, function_description, llm_parameters, func_hash)\n\n        # Check if the function is already initialized\n        if not function_description.is_initialized:\n            function_description.is"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        if not hasattr(self, 'function_data'):\n            self.function_data = {}\n\n        if not hasattr(self, 'function_data_initialized'):\n            self.function_data_initialized = {}\n\n        if not hasattr(self, 'function_data_examples'):\n            self.function_data_examples = {}\n\n        if not hasattr(self, 'function_data_examples_initialized'):\n            self.function_data_examples_initialized = {}\n\n        if not hasattr(self, 'function_data_examples_initialized_count'):\n            self.function_data_examples_initialized_count = {}\n\n        if not hasattr(self, 'function_data_examples_initialized_count_max'):\n            self.function_data_examples_initialized_count_max = {}\n\n        if not hasattr(self, 'function_data_examples_initialized_count_max_default'):\n            self.function_data_examples_initialized_count_max_default = 10\n\n        if not hasattr(self, 'function_data_examples_initialized_count_max_max'):\n            self.function_data_examples_initialized_count_max_max = 100\n\n        if not hasattr(self, 'function_data_examples_initialized_count_max_min'):\n            self.function_data_examples_initialized_count_max_min = 1\n\n        if not hasattr(self, 'function_data_examples_initialized_count_max_step'):\n            self.function_data_examples_initialized_count_max_step = 10\n\n        if not hasattr(self, 'function_data_examples_initialized_count_max_step_default'):\n            self.function_data_examples_initialized_count_max_step_default = 10\n\n        if not hasattr(self, 'function_data_examples_initialized_count_max_step_max'):\n            self.function_data_examples_initialized_count_max_step_max = 100"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        if not self.is_function_initialized(func_hash):\n            self.initialize_function(func_hash)\n\n        if self.is_function_initialized(func_hash):\n            return self.construct_prompt(args, kwargs, function_description, llm_parameters, func_hash), self.get_model(function_description), True, False\n\n        if self.is_function_distillable(function_description):\n            return self.construct_prompt(args, kwargs, function_description, llm_parameters, func_hash), self.get_model(function_description), True, False\n\n        if self.is_function_distillable(function_description):\n            return self.construct_prompt(args, kwargs, function_description, llm_parameters, func_hash), self.get_model(function_description), True, False\n\n        if self.is_function_distillable(function_description):\n            return self.construct_prompt(args, kwargs, function_description, llm_parameters, func_hash), self.get_model(function_description), True, False\n\n        if self.is_function_distillable(function_description):\n            return self.construct_prompt(args, kwargs, function_description, llm_parameters, func_hash), self.get_model(function_description), True, False\n\n        if self.is_function_distillable(function_description):\n            return self.construct_prompt(args, kwargs, function_description, llm_parameters, func_hash), self.get_model(function_description), True, False\n\n        if self.is_function_distillable(function_description):\n            return self.construct_prompt(args, kwargs, function_description, llm_parameters, func_hash), self.get_model(function_description), True, False\n\n        if self.is_function_distillable(function_description):\n            return self.construct_prompt(args, kwargs, function_description, llm_parameters, func_hash), self.get"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            # If the function is already initialized, return the model and prompt\n            return self.initialized_functions[func_hash]['prompt'], self.initialized_functions[func_hash]['model'], False, True\n\n        # Check if the function is already initialized for fine-tuning\n        if func_hash in self.initialized_functions_ft:\n            # If the function is already initialized for fine-tuning, return the model and prompt\n            return self.initialized_functions_ft[func_hash]['prompt'], self.initialized_functions_ft[func_hash]['model'], False, True\n\n        # Check if the function is already initialized for distillation\n        if func_hash in self.initialized_functions_distill:\n            # If the function is already initialized for distillation, return the model and prompt\n            return self.initialized_functions_distill[func_hash]['prompt'], self.initialized_functions_distill[func_hash]['model'], True, True\n\n        # Check if the function is already initialized for distillation and fine-tuning\n        if func_hash in self.initialized_functions_distill_ft:\n            # If the function is already initialized for distillation and fine-tuning, return the model and prompt\n            return self.initialized_functions_distill_ft[func_hash]['prompt'], self.initialized_functions_distill_ft[func_hash]['model'], True, True\n\n        # Check if the function is suitable for distillation\n        if function_description.suitable_for_distillation:\n            # If the function is suitable for distillation, select the distilled model\n            model = self.distilled_model\n        else:\n            # If the function is not suitable for distillation, select the teacher model\n            model = self.teacher_model\n\n        # Check if the function is already initialized for fine-tuning\n        if func_hash in self.initialized_functions_ft:\n            # If the function is already initialized for fine"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if not self.is_initialized(func_hash):\n            # Initialize the function data\n            self.initialize_function_data(function_description, func_hash)\n            # Return the constructed prompt, the selected model, and a boolean indicating if the model is suitable for distillation\n            return self.construct_prompt(args, kwargs, function_description, llm_parameters), self.model, self.is_model_suitable_for_distillation, False\n\n        # Check if the function is suitable for distillation\n        if self.is_model_suitable_for_distillation:\n            # Update the examples for fine-tuning\n            self.update_examples(args, kwargs, function_description, llm_parameters)\n            # Return the constructed prompt, the selected model, a boolean indicating if the model is suitable for distillation, and a boolean indicating if the function is already initialized and does not require saving examples for fine-tuning\n            return self.construct_prompt(args, kwargs, function_description, llm_parameters), self.model, self.is_model_suitable_for_distillation, True\n\n        # Return the constructed prompt, the selected model, a boolean indicating if the model is suitable for distillation, and a boolean indicating if the function is already initialized and does not require saving examples for fine-tuning\n        return self.construct_prompt(args, kwargs, function_description, llm_parameters), self.model, self.is_model_suitable_for_distillation, False"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if not self.is_initialized(func_hash):\n            # Initialize the function and return the appropriate prompt\n            return self.initialize_function(args, kwargs, function_description, llm_parameters, func_hash)\n\n        # Check if the function requires saving examples for fine-tuning\n        if self.save_examples:\n            # Save the example and return the appropriate prompt\n            return self.save_example(args, kwargs, function_description, llm_parameters, func_hash)\n\n        # Check if the model is suitable for distillation\n        if self.is_suitable_for_distillation(function_description):\n            # Use the distilled model and return the appropriate prompt\n            return self.get_distilled_prompt(args, kwargs, function_description, llm_parameters, func_hash)\n\n        # Use the teacher model and return the appropriate prompt\n        return self.get_teacher_prompt(args, kwargs, function_description, llm_parameters, func_hash)\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Determine if the model is suitable for distillation based on the function description and token count requirements\n        suitable_for_distillation = self.is_suitable_for_distillation(function_description, llm_parameters['max_new_tokens'])\n\n        # Initialize function-specific data if not already done\n        if func_hash not in self.function_specific_data:\n            self.initialize_function_specific_data(function_description, func_hash)\n\n        # Update examples for fine-tuning if necessary\n        self.update_examples_for_fine_tuning(function_description, args, kwargs, func_hash)\n\n        # Construct the prompt to be used for generation\n        prompt = self.construct_prompt(function_description, args, kwargs)\n\n        # Determine the appropriate model based on the suitability for distillation\n        model = self.distilled_model if suitable_for_distillation else self.teacher_model\n\n        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        is_initialized = self.function_specific_data[func_hash]['is_initialized']\n\n        # Return the constructed prompt, the selected model, a boolean indicating if the model is suitable for distillation, and a boolean indicating if the function is already initialized\n        return prompt, model, suitable_for_distillation, is_initialized"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if not self.is_initialized(func_hash):\n            # If not, initialize the function and return the generated prompt\n            prompt = self.initialize_function(args, kwargs, function_description, llm_parameters, func_hash)\n            return prompt, self.model, self.is_suitable_for_distillation, False\n\n        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if self.is_initialized(func_hash) and not self.is_suitable_for_distillation:\n            # If yes, return the generated prompt\n            prompt = self.generate_prompt(args, kwargs, function_description, llm_parameters, func_hash)\n            return prompt, self.model, self.is_suitable_for_distillation, True\n\n        # If the function is not already initialized and requires saving examples for fine-tuning\n        if not self.is_initialized(func_hash) and self.is_suitable_for_distillation:\n            # Update the examples for fine-tuning\n            self.update_examples(args, kwargs, function_description, func_hash)\n            # Construct the prompt to be used for generation\n            prompt = self.generate_prompt(args, kwargs, function_description, llm_parameters, func_hash)\n            # Return the generated prompt and the selected model\n            return prompt, self.model, self.is_suitable_for_distillation, False\n\n        # If the function is already initialized and requires saving examples for fine-tuning\n        if self.is_initialized(func_hash) and self.is_suitable_for_distillation:\n            # Update the examples for fine-tuning\n            self.update_examples(args, kwargs, function_description, func_hash)\n            # Construct the prompt to be used for generation\n            prompt = self.generate_prompt(args, kwargs, function_description, llm_parameters, func_hash)\n            # Return the generated prompt and the"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if function_description.is_initialized:\n            return function_description.prompt, function_description.model, function_description.suitable_for_distillation, function_description.is_initialized\n\n        # Get the model and prompt for the function description\n        model, prompt = self.get_model_and_prompt(function_description)\n\n        # Check if the model is suitable for distillation\n        suitable_for_distillation = self.is_model_suitable_for_distillation(model)\n\n        # Check if the model is a distilled model\n        is_distilled_model = self.is_distilled_model(model)\n\n        # Check if the model is a teacher model\n        is_teacher_model = self.is_teacher_model(model)\n\n        # Check if the model is a distilled model or a teacher model\n        if is_distilled_model or is_teacher_model:\n            # Check if the model is a distilled model\n            if is_distilled_model:\n                # Check if the model is suitable for distillation\n                if suitable_for_distillation:\n                    # Construct the prompt for distilled model\n                    prompt = self.construct_prompt_for_distilled_model(model, prompt, args, kwargs)\n                else:\n                    # Construct the prompt for teacher model\n                    prompt = self.construct_prompt_for_teacher_model(model, prompt, args, kwargs)\n            # Check if the model is a teacher model\n            else:\n                # Construct the prompt for teacher model\n                prompt = self.construct_prompt_for_teacher_model(model, prompt, args, kwargs)\n        # Check if the model is not a distilled model or a teacher model\n        else:\n            # Construct the prompt for the model\n            prompt = self.construct_prompt_for_model(model, prompt, args, kwargs)\n\n        # Check if the model is a distilled model\n        if is_distilled_"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        if not self.is_function_initialized(func_hash):\n            self.initialize_function(func_hash)\n\n        if self.is_function_initialized(func_hash):\n            return self.get_prompt(args, kwargs, function_description, llm_parameters), self.model, False, True\n\n        if not self.is_function_suitable_for_distillation(function_description):\n            return self.get_prompt(args, kwargs, function_description, llm_parameters), self.model, False, False\n\n        if self.is_function_suitable_for_distillation(function_description):\n            if self.is_function_initialized(func_hash):\n                return self.get_prompt(args, kwargs, function_description, llm_parameters), self.model, True, True\n\n            if not self.is_function_initialized(func_hash):\n                self.update_examples_for_fine_tuning(func_hash, args, kwargs, function_description)\n                return self.get_prompt(args, kwargs, function_description, llm_parameters), self.distilled_model, True, False\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Initialize function-specific data if not already done\n        if func_hash not in self.function_data:\n            self.function_data[func_hash] = {\n                \"initialized\": False,\n                \"examples\": [],\n                \"current_example\": 0,\n                \"distill_suitable\": False,\n                \"distill_model\": None,\n                \"distill_prompt\": None,\n                \"distill_token_count\": 0,\n                \"distill_max_tokens\": llm_parameters[\"max_tokens\"],\n                \"distill_temperature\": llm_parameters[\"temperature\"],\n                \"distill_top_p\": llm_parameters[\"top_p\"],\n                \"distill_n\": llm_parameters[\"n\"],\n                \"distill_stream\": llm_parameters[\"stream\"],\n                \"distill_logprobs\": llm_parameters[\"logprobs\"],\n                \"distill_stop\": llm_parameters[\"stop\"],\n                \"distill_max_new_tokens\": llm_parameters[\"max_new_tokens\"],\n                \"distill_presence_penalty\": llm_parameters[\"presence_penalty\"],\n                \"distill_frequency_penalty\": llm_parameters[\"frequency_penalty\"],\n                \"distill_best_of\": llm_parameters[\"best_of\"],\n                \"distill_logit_bias\": llm_parameters[\"logit_bias\"],\n                \"distill_user\": llm_parameters[\"user\"],\n                \"distill_model_kwargs\": llm_parameters[\"model_kwargs\"],\n                \"distill_request_timeout\": llm_parameters[\"request_timeout\"],\n                \"distill_retries\": llm_parameters[\"retries\"],\n                \"distill_verbose\": llm_parameters[\"verbose\"],\n                \"distill_callback\": llm_parameters[\"callback\"],\n                \"distill_callback_manager\": llm_parameters[\"callback_manager\"],\n                \"distill_conversation_id\": llm_parameters[\"conversation_id\"],\n                \"distill_parent_message_id\":"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Initialize function-specific data if not already done\n        if func_hash not in self.initialized_functions:\n            self.initialize_function(function_description, func_hash)\n\n        # Update examples for fine-tuning if necessary\n        if self.fine_tuning_enabled:\n            self.update_examples(function_description, args, kwargs, func_hash)\n\n        # Construct the prompt to be used for generation\n        prompt = self.construct_prompt(function_description, args, kwargs, func_hash)\n\n        # Determine the appropriate model based on the token count and suitability for distillation\n        model, suitable_for_distillation = self.select_model(prompt)\n\n        # Return the constructed prompt, selected model, suitability for distillation, and function initialization status\n        return prompt, model, suitable_for_distillation, func_hash in self.initialized_functions\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Get the model and prompt for the given function description\n        model, prompt = self.get_model_and_prompt(function_description)\n        # Check if the model is suitable for distillation\n        is_suitable_for_distillation = self.is_suitable_for_distillation(model, prompt, function_description)\n        # Check if the function is already initialized\n        is_initialized = self.is_initialized(function_description)\n        # If the function is not initialized, initialize it\n        if not is_initialized:\n            self.initialize(function_description)\n        # Update the examples for fine-tuning\n        self.update_examples_for_fine_tuning(function_description, args, kwargs)\n        # Construct the prompt for generation\n        prompt = self.construct_prompt(function_description, args, kwargs, llm_parameters)\n        # Return the prompt, model, and flags\n        return prompt, model, is_suitable_for_distillation, is_initialized\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if self.is_initialized(func_hash):\n            # Return the prompt, model, and flags without saving examples for fine-tuning\n            return self.get_prompt(args, kwargs, function_description, llm_parameters), self.get_model(function_description), False, True\n\n        # Get the model and token count for the function description\n        model, token_count = self.get_model_and_token_count(function_description)\n\n        # Check if the model is suitable for distillation and if the token count is within the range\n        if self.is_model_suitable_for_distillation(model) and self.is_token_count_within_range(token_count):\n            # Update the function description with the model and token count\n            function_description.model = model\n            function_description.token_count = token_count\n            # Return the prompt, model, and flags without saving examples for fine-tuning\n            return self.get_prompt(args, kwargs, function_description, llm_parameters), self.get_model(function_description), True, False\n\n        # Update the function description with the model and token count\n        function_description.model = model\n        function_description.token_count = token_count\n        # Save the function description and examples for fine-tuning\n        self.save_function_description_and_examples(function_description, args, kwargs)\n        # Return the prompt, model, and flags with saving examples for fine-tuning\n        return self.get_prompt(args, kwargs, function_description, llm_parameters), self.get_model(function_description), False, False"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if func_hash in self.initialized_functions:\n            return '', self.model_name, False, True\n\n        # Determine the appropriate model based on the function description and the number of tokens in the function description\n        model_name, suitable_for_distillation = self.determine_model(function_description)\n\n        # Update the examples for fine-tuning if necessary\n        if model_name != self.model_name:\n            self.update_examples(model_name, function_description, args, kwargs)\n\n        # Construct the prompt using the appropriate model and function description\n        prompt = self.construct_prompt(model_name, function_description, args, kwargs)\n\n        # Return the constructed prompt, the selected model, a boolean indicating if the model is suitable for distillation, and a boolean indicating if the function is already initialized and does not require saving examples for fine-tuning\n        return prompt, model_name, suitable_for_distillation, False\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            return None, None, None, True\n\n        # Initialize the function if it's not already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialize_function(function_description, args, kwargs, func_hash)\n\n        # Check if the function is suitable for distillation\n        if self.is_suitable_for_distillation(function_description, args, kwargs, func_hash):\n            model = self.distilled_model\n        else:\n            model = self.teacher_model\n\n        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if func_hash in self.initialized_functions:\n            return None, None, None, True\n\n        # Update the examples for fine-tuning if necessary\n        if self.update_examples:\n            self.update_examples(function_description, args, kwargs, func_hash)\n\n        # Construct the prompt for generation\n        prompt = self.construct_prompt(function_description, args, kwargs, func_hash)\n\n        return prompt, model, self.is_suitable_for_distillation(function_description, args, kwargs, func_hash), False\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Initialize function-specific data if not already done\n        if not self.initialized_functions.get(func_hash):\n            self.initialize_function(func_hash)\n\n        # Update examples for fine-tuning if necessary\n        self.update_examples(args, kwargs, function_description, func_hash)\n\n        # Construct the prompt based on the function description and arguments\n        prompt = self.construct_prompt(args, kwargs, function_description)\n\n        # Determine the appropriate model and prompt for the given function description and arguments\n        model, prompt, suitable_for_distillation = self.determine_model_and_prompt(\n            function_description, llm_parameters, prompt)\n\n        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        already_initialized = self.initialized_functions.get(func_hash)\n\n        # Return the constructed prompt, the selected model, a boolean indicating if the model is suitable for distillation, and a boolean indicating if the function is already initialized and does not require saving examples for fine-tuning\n        return prompt, model, suitable_for_distillation, already_initialized\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            # Function is already initialized, return the model and prompt\n            model = self.initialized_functions[func_hash]['model']\n            prompt = self.initialized_functions[func_hash]['prompt']\n            return prompt, model, False, True\n\n        # Check if the model is suitable for distillation\n        if self.is_model_suitable_for_distillation(function_description, llm_parameters):\n            # Model is suitable for distillation, use distilled model\n            model = self.distilled_model\n            prompt = self.get_distilled_prompt(function_description, args, kwargs, llm_parameters)\n        else:\n            # Model is not suitable for distillation, use teacher model\n            model = self.teacher_model\n            prompt = self.get_teacher_prompt(function_description, args, kwargs, llm_parameters)\n\n        # Check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            # Function is already initialized, return the model and prompt\n            return prompt, model, False, True\n\n        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if func_hash in self.initialized_functions_no_examples:\n            # Function is already initialized and does not require saving examples for fine-tuning\n            return prompt, model, False, False\n\n        # Initialize function-specific data\n        self.initialized_functions[func_hash] = {\n            'model': model,\n            'prompt': prompt,\n            'examples': []\n        }\n\n        # Return the model, prompt, and a flag indicating that the model is not suitable for distillation\n        return prompt, model, False, False\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if func_hash in self.initialized_functions:\n            return '', self.model, False, True\n\n        # Check if the function is in the list of functions to be fine-tuned\n        if func_hash in self.fine_tune_functions:\n            # Update examples for fine-tuning if necessary\n            self.update_examples(func_hash, args, kwargs, function_description)\n            # Return the prompt, model, and flags indicating that the model is not suitable for distillation and the function is not already initialized\n            return self.prompts[func_hash], self.model, False, False\n\n        # Check if the function is suitable for distillation\n        if self.is_suitable_for_distillation(args, kwargs, function_description):\n            # Return the prompt, model, and flags indicating that the model is suitable for distillation and the function is not already initialized\n            return self.prompts[func_hash], self.distilled_model, True, False\n\n        # Return the prompt, model, and flags indicating that the model is not suitable for distillation and the function is not already initialized\n        return self.prompts[func_hash], self.model, False, False\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the model is suitable for distillation\n        is_suitable_for_distillation = self.is_suitable_for_distillation(function_description, llm_parameters)\n\n        # Check if the model is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialize_function(func_hash)\n            return_prompt, selected_model = self.construct_prompt(args, kwargs, function_description, llm_parameters, is_suitable_for_distillation)\n            return return_prompt, selected_model, is_suitable_for_distillation, False\n\n        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if func_hash in self.initialized_functions and self.initialized_functions[func_hash]['is_initialized']:\n            return_prompt, selected_model = self.construct_prompt(args, kwargs, function_description, llm_parameters, is_suitable_for_distillation)\n            return return_prompt, selected_model, is_suitable_for_distillation, True\n\n        # Update the function's examples for fine-tuning\n        self.update_examples(function_description, args, kwargs, func_hash)\n        return_prompt, selected_model = self.construct_prompt(args, kwargs, function_description, llm_parameters, is_suitable_for_distillation)\n        return return_prompt, selected_model, is_suitable_for_distillation, False\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov_ = cov\n        for i in range(higham_max_iteration):\n            cov_ = (cov_ + cov_.T) / 2\n            eig_val, eig_vec = np.linalg.eigh(cov_)\n            eig_val[eig_val < 0] = 0\n            cov_ = eig_vec @ np.diag(eig_val) @ eig_vec.T\n            if np.allclose(cov_, cov):\n                break\n        cov = cov_\n    else:\n        eig_val, eig_vec = np.linalg.eigh(cov)\n        eig_val[eig_val < 0] = 0\n        cov = eig_vec @ np.diag(eig_val) @ eig_vec.T\n\n    return cov"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov_nearest = cov\n        for _ in range(higham_max_iteration):\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            eig_values, eig_vectors = np.linalg.eigh(cov_nearest)\n            eig_values = np.where(eig_values < 0, 0, eig_values)\n            cov_nearest = eig_vectors @ np.diag(eig_values) @ eig_vectors.T\n    else:\n        eig_values, eig_vectors = np.linalg.eigh(cov)\n        eig_values = np.where(eig_values < 0, 0, eig_values)\n        cov_nearest = eig_vectors @ np.diag(eig_values) @ eig_vectors.T\n\n    return cov_nearest"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        return cov_nearest_clip(cov)\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov_nearest = cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        cov_nearest = cov_nearest_clip(cov)\n\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov = _cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        cov = _cov_nearest_clip(cov)\n\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov = cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        cov = cov_nearest_eigval_clipped(cov)\n\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return _higham_nearest(cov, max_iteration=higham_max_iteration)\n    else:\n        return _clip_nearest(cov)\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov = cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        cov = cov_nearest_clip(cov)\n\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    cov = np.asarray(cov)\n    if cov.ndim != 2:\n        raise ValueError(\"The input covariance matrix must be a 2-dimensional array.\")\n    if cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"The input covariance matrix must be square.\")\n\n    if higham:\n        cov = _cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        cov = _cov_nearest_clip(cov)\n\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov_nearest = cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        cov_nearest = cov_nearest_eig(cov)\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov = cov_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        cov = cov_clip(cov)\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov = cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        cov = cov_nearest_clip(cov)\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov = cov_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        cov = cov_clip(cov)\n\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov = cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        cov = cov_nearest_clipped(cov)\n\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return _cov_nearest_higham(cov, higham_max_iteration)\n    else:\n        return _cov_nearest_clip(cov)\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if not higham:\n        # Clip eigenvalues to ensure positive definiteness\n        eig_val, eig_vec = np.linalg.eigh(cov)\n        eig_val = np.clip(eig_val, 0, None)\n        cov = eig_vec @ np.diag(eig_val) @ eig_vec.T\n    else:\n        # Use Higham & Nick (2002) algorithm to find nearest positive definite matrix\n        cov_diag = np.diag(cov)\n        cov_diag_pos = cov_diag > 0\n        cov_diag_pos_sum = cov_diag_pos.sum()\n        cov_diag_pos_sqrt = np.sqrt(cov_diag[cov_diag_pos])\n        cov_diag_pos_inv_sqrt = np.diag(1 / cov_diag_pos_sqrt)\n        cov_pos = cov[cov_diag_pos, :][:, cov_diag_pos]\n        cov_pos_sqrt = cov_diag_pos_inv_sqrt @ cov_pos @ cov_diag_pos_inv_sqrt\n        cov_pos_sqrt_inv = np.linalg.inv(cov_pos_sqrt)\n        cov_pos_sqrt_inv_sqrt = cov_diag_pos_sqrt[:, None] * cov_pos_sqrt_inv * cov_diag_pos_sqrt[None, :]\n        cov_pos_sqrt_inv_sqrt = np.clip(cov_pos_sqrt_inv_sqrt, -1, 1)\n        cov_pos_sqrt_inv_sqrt = cov_diag_pos_inv_sqrt @ cov_pos_sqrt_inv_sqrt @ cov_diag_pos_inv_sqrt\n        cov_pos_sqrt_inv_sqrt_diag = np.diag(cov_pos_sqrt_inv_sqrt)\n        cov_pos_sqrt_inv_sqrt_diag_pos = cov_pos_sqrt_inv_sqrt_diag > 0\n        cov_pos_sqrt_inv_sqrt_diag_pos"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    # Get the eigenvalues and eigenvectors of the covariance matrix\n    eig_val, eig_vec = np.linalg.eigh(cov)\n\n    # Compute the mean of the eigenvalues\n    eig_val_mean = eig_val.mean()\n\n    # Initialize the tolerance for the eigenvalues\n    eig_val_tol = 1e-8\n\n    # Initialize the counter for the iterations\n    iteration = 0\n\n    # If the Higham & Nick (2002) algorithm is used\n    if higham:\n        # Iterate until the tolerance is met or the maximum number of iterations is reached\n        while np.any(eig_val < eig_val_tol) and iteration < higham_max_iteration:\n            # Compute the number of negative eigenvalues\n            n_neg_eig_val = np.sum(eig_val < eig_val_tol)\n\n            # If there are negative eigenvalues\n            if n_neg_eig_val > 0:\n                # Replace negative eigenvalues with the mean of the eigenvalues\n                eig_val = np.array([eig_val_mean if eig_val_i < eig_val_tol else eig_val_i for eig_val_i in eig_val])\n\n            # Compute the updated covariance matrix\n            cov = eig_vec @ np.diag(eig_val) @ eig_vec.T\n\n            # Compute the updated eigenvalues and eigenvectors\n            eig_val, eig_vec = np.linalg.eigh(cov)\n\n            # Increment the iteration counter\n            iteration += 1\n\n    # If the Higham & Nick (2002) algorithm is not used\n    else:\n        # Replace negative eigenvalues with the mean of the eigenvalues\n        eig_val = np.array([eig_val_mean if eig_val_i < eig_val_tol else eig_val_i for eig_val_i in eig_val])\n\n        # Compute the updated covariance matrix\n        cov = eig_vec @"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    # Check if the input covariance matrix is already positive definite\n    if is_positive_definite(cov):\n        return cov\n\n    # If not, use the Higham & Nick (2002) algorithm to find the nearest positive definite matrix\n    if higham:\n        cov = nearest_positive_definite_matrix(cov, max_iteration=higham_max_iteration)\n    else:\n        # If the Higham & Nick (2002) algorithm is not used, clip eigenvalues to ensure positive definiteness\n        cov = cov_nearest_eigh(cov)\n\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    # Check if the input covariance matrix is symmetric\n    if not np.allclose(cov, cov.T):\n        raise ValueError(\"The input covariance matrix is not symmetric.\")\n\n    # Check if the input covariance matrix is positive semi-definite\n    if not np.all(np.linalg.eigvals(cov) >= 0):\n        raise ValueError(\"The input covariance matrix is not positive semi-definite.\")\n\n    # Check if the input covariance matrix is square\n    if cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"The input covariance matrix is not square.\")\n\n    # Check if the input covariance matrix is 2x2\n    if cov.shape[0] == 2:\n        # If the input covariance matrix is 2x2, we can use the analytical solution\n        # to compute the nearest positive definite matrix\n        w, v = np.linalg.eig(cov)\n        w = np.maximum(w, 0)\n        return v @ np.diag(w) @ v.T\n\n    # If the input covariance matrix is not 2x2, we use the Higham & Nick (2002) algorithm\n    # to compute the nearest positive definite matrix\n    if higham:\n        cov_ = cov.copy()\n        delta_s = np.zeros_like(cov_)\n        y = cov_.copy()\n\n        for _ in range(higham_max_iteration):\n            r = (y + y.T) * 0.5 - cov_\n            x = np.diag(np.diag(r))\n            norm_x = np.linalg.norm(x, ord=\"fro\")\n            if norm_x == 0:\n                break\n            tau = np.linalg.norm(r - x, ord=\"fro\") / norm_x\n            delta_s = tau * r / norm_x\n            y = cov_ + delta_s\n            cov_ = y.copy()\n\n        return y\n\n    # If the input covariance matrix is not 2x2 and the Higham & Nick (2002)"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    # Check if the input is a covariance matrix\n    if not np.allclose(cov, cov.T):\n        raise ValueError(\"Input matrix is not a covariance matrix\")\n\n    # Compute the nearest correlation matrix\n    corr = cov2corr(cov)\n\n    # Compute the nearest positive definite correlation matrix\n    if higham:\n        corr = nearest_correlation_matrix(corr, max_iteration=higham_max_iteration)\n    else:\n        corr = cov2corr(cov)\n        corr = np.clip(corr, -1, 1)\n\n    # Compute the nearest covariance matrix\n    cov = corr2cov(corr, np.diag(cov) ** 0.5)\n\n    return cov\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    if data_home is None:\n        data_home = os.path.join(\"~\", \"skfolio_data\")\n    data_home = os.path.expanduser(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    if data_home is None:\n        data_home = os.path.join('~', 'skfolio_data')\n    data_home = os.path.expanduser(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    if data_home is None:\n        data_home = os.path.join('~', 'skfolio_data')\n    data_home = os.path.expanduser(data_home)\n    shutil.rmtree(data_home)\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    if data_home is None:\n        data_home = os.path.join('~', 'skfolio_data')\n    data_home = os.path.expanduser(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    if data_home is None:\n        data_home = os.path.join(\"~\", \"skfolio_data\")\n    data_home = os.path.expanduser(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    if data_home is None:\n        data_home = Path(os.path.expanduser(\"~\")) / \"skfolio_data\"\n    elif isinstance(data_home, str):\n        data_home = Path(data_home)\n    else:\n        data_home = Path(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    if data_home is None:\n        data_home = os.path.join(\"~\", \"skfolio_data\")\n    data_home = os.path.expanduser(data_home)\n    shutil.rmtree(data_home)\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    if data_home is None:\n        data_home = Path.home() / \"skfolio_data\"\n\n    if not isinstance(data_home, Path):\n        data_home = Path(data_home)\n\n    data_home.mkdir(parents=True, exist_ok=True)\n\n    for path in data_home.iterdir():\n        if path.is_dir():\n            shutil.rmtree(path)\n        else:\n            path.unlink()\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    # Determine the correct data home directory path\n    data_home = get_data_home(data_home)\n\n    # Remove all the contents of the data home directory\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home=data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    pass"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj, str)\n    elif isinstance(obj, bytes):\n        return (obj, bytes)\n    elif isinstance(obj, list):\n        return (tuple(flatten_to_tuple(o) for o in obj), list)\n    elif isinstance(obj, tuple):\n        return (tuple(flatten_to_tuple(o) for o in obj), tuple)\n    elif isinstance(obj, dict):\n        return (tuple((k, flatten_to_tuple(v)) for k, v in obj.items()), dict)\n    elif isinstance(obj, Instances):\n        return (obj.to_tuple(), Instances)\n    elif isinstance(obj, Boxes):\n        return (obj.to_tuple(), Boxes)\n    elif isinstance(obj, ROIMasks):\n        return (obj.to_tuple(), ROIMasks)\n    else:\n        raise ValueError(f\"Unsupported type: {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return obj, lambda res: res\n    elif isinstance(obj, bytes):\n        return obj, lambda res: res\n    elif isinstance(obj, list):\n        return tuple(obj), lambda res: list(res)\n    elif isinstance(obj, tuple):\n        return tuple(obj), lambda res: tuple(res)\n    elif isinstance(obj, Mapping):\n        return tuple(obj.items()), lambda res: dict(res)\n    elif isinstance(obj, Instances):\n        return obj.to_tuple(), lambda res: Instances.from_tuple(res)\n    elif isinstance(obj, Boxes):\n        return obj.to_tuple(), lambda res: Boxes.from_tuple(res)\n    elif isinstance(obj, ROIMasks):\n        return obj.to_tuple(), lambda res: ROIMasks.from_tuple(res)\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj, str), lambda res: res\n    elif isinstance(obj, bytes):\n        return (obj, bytes), lambda res: res\n    elif isinstance(obj, list):\n        return [flatten_to_tuple(x) for x in obj], lambda res: [x[1](x[0]) for x in res]\n    elif isinstance(obj, tuple):\n        return tuple(flatten_to_tuple(x) for x in obj), lambda res: tuple(x[1](x[0]) for x in res)\n    elif isinstance(obj, dict):\n        return {k: flatten_to_tuple(v) for k, v in obj.items()}, lambda res: {k: v[1](v[0]) for k, v in res.items()}\n    elif isinstance(obj, Instances):\n        return obj.to_tuple(), lambda res: Instances.from_tuple(res)\n    elif isinstance(obj, Boxes):\n        return obj.to_tuple(), lambda res: Boxes.from_tuple(res)\n    elif isinstance(obj, ROIMasks):\n        return obj.to_tuple(), lambda res: ROIMasks.from_tuple(res)\n    else:\n        raise ValueError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return obj, str\n    elif isinstance(obj, bytes):\n        return obj, bytes\n    elif isinstance(obj, (list, tuple)):\n        res = [flatten_to_tuple(o) for o in obj]\n        return res, type(obj)\n    elif isinstance(obj, (dict, Mapping)):\n        res = {k: flatten_to_tuple(v) for k, v in obj.items()}\n        return res, type(obj)\n    elif isinstance(obj, Instances):\n        res = {k: flatten_to_tuple(v) for k, v in obj._fields.items()}\n        return res, Instances\n    elif isinstance(obj, Boxes):\n        res = {k: flatten_to_tuple(v) for k, v in obj._fields.items()}\n        return res, Boxes\n    elif isinstance(obj, ROIMasks):\n        res = {k: flatten_to_tuple(v) for k, v in obj._fields.items()}\n        return res, ROIMasks\n    else:\n        return obj, type(obj)"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return obj, Schema(lambda res: res)\n    elif isinstance(obj, list):\n        res = []\n        schemas = []\n        for o in obj:\n            r, s = flatten_to_tuple(o)\n            res.append(r)\n            schemas.append(s)\n        return res, Schema(lambda res: [s(r) for r, s in zip(res, schemas)])\n    elif isinstance(obj, tuple):\n        res = []\n        schemas = []\n        for o in obj:\n            r, s = flatten_to_tuple(o)\n            res.append(r)\n            schemas.append(s)\n        return res, Schema(lambda res: tuple(s(r) for r, s in zip(res, schemas)))\n    elif isinstance(obj, Mapping):\n        res = []\n        schemas = []\n        for k, v in obj.items():\n            r, s = flatten_to_tuple(v)\n            res.append(r)\n            schemas.append(s)\n        return res, Schema(lambda res: {k: s(r) for k, (r, s) in zip(obj.keys(), zip(res, schemas))})\n    elif isinstance(obj, Instances):\n        res = []\n        schemas = []\n        for o in obj.get_fields().values():\n            r, s = flatten_to_tuple(o)\n            res.append(r)\n            schemas.append(s)\n        return res, Schema(lambda res: Instances(res, obj.image_size))\n    elif isinstance(obj, Boxes):\n        return obj.tensor, Schema(lambda res: Boxes(res))\n    elif isinstance(obj, ROIMasks):\n        return obj.masks, Schema(lambda res: ROIMasks(res))\n    else:\n        raise TypeError(f\"Unsupported type {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), lambda res: res[0]\n    elif isinstance(obj, list):\n        res = []\n        schemas = []\n        for item in obj:\n            item_res, schema = flatten_to_tuple(item)\n            res.append(item_res)\n            schemas.append(schema)\n        return tuple(res), lambda res: [schema(r) for r, schema in zip(res, schemas)]\n    elif isinstance(obj, tuple):\n        res = []\n        schemas = []\n        for item in obj:\n            item_res, schema = flatten_to_tuple(item)\n            res.append(item_res)\n            schemas.append(schema)\n        return tuple(res), lambda res: tuple(schema(r) for r, schema in zip(res, schemas))\n    elif isinstance(obj, Mapping):\n        res = []\n        schemas = []\n        for k, v in obj.items():\n            v_res, schema = flatten_to_tuple(v)\n            res.append(v_res)\n            schemas.append(schema)\n        return tuple(res), lambda res: {k: schema(r) for k, r, schema in zip(obj.keys(), res, schemas)}\n    elif isinstance(obj, Instances):\n        res = []\n        schemas = []\n        for field in obj._fields:\n            field_res, schema = flatten_to_tuple(getattr(obj, field))\n            res.append(field_res)\n            schemas.append(schema)\n        return tuple(res), lambda res: Instances(**{field: schema(r) for field, r, schema in zip(obj._fields, res, schemas)})\n    elif isinstance(obj, Boxes):\n        return (obj.tensor,), lambda res: Boxes(res[0])\n    elif isinstance(obj, ROIMasks):\n        return (obj.masks,), lambda res: ROIMasks(res[0"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj, str),\n    elif isinstance(obj, bytes):\n        return (obj, bytes),\n    elif isinstance(obj, list):\n        return tuple(flatten_to_tuple(o) for o in obj), list\n    elif isinstance(obj, tuple):\n        return tuple(flatten_to_tuple(o) for o in obj), tuple\n    elif isinstance(obj, dict):\n        return tuple(flatten_to_tuple(o) for o in obj.values()), dict\n    elif isinstance(obj, Instances):\n        return tuple(flatten_to_tuple(o) for o in obj.to_tuple()), Instances\n    elif isinstance(obj, Boxes):\n        return tuple(flatten_to_tuple(o) for o in obj.tensor), Boxes\n    elif isinstance(obj, ROIMasks):\n        return tuple(flatten_to_tuple(o) for o in obj.to_tuple()), ROIMasks\n    else:\n        raise ValueError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return obj, str\n    elif isinstance(obj, bytes):\n        return obj, bytes\n    elif isinstance(obj, (list, tuple)):\n        res, schema = zip(*map(flatten_to_tuple, obj))\n        return res, list if isinstance(obj, list) else tuple\n    elif isinstance(obj, (dict, Mapping)):\n        res, schema = zip(*[(flatten_to_tuple(v), k) for k, v in obj.items()])\n        return res, lambda x: {k: v for v, k in zip(x, schema)}\n    elif isinstance(obj, Instances):\n        res, schema = zip(\n            *[(flatten_to_tuple(getattr(obj, k)), k) for k in obj._fields.keys()]\n        )\n        return res, lambda x: Instances(**{k: v for v, k in zip(x, schema)})\n    elif isinstance(obj, Boxes):\n        res, schema = flatten_to_tuple(obj.tensor)\n        return res, lambda x: Boxes(x)\n    elif isinstance(obj, ROIMasks):\n        res, schema = zip(\n            *[(flatten_to_tuple(m), h) for m, h in zip(obj.masks, obj.heights)]\n        )\n        return res, lambda x: ROIMasks(list(m for m, _ in x), list(h for _, h in x))\n    else:\n        return obj, lambda x: x"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return obj, str\n\n    if isinstance(obj, bytes):\n        return obj, bytes\n\n    if isinstance(obj, list):\n        res = []\n        for o in obj:\n            flattened, schema = flatten_to_tuple(o)\n            res.append(flattened)\n        return res, list\n\n    if isinstance(obj, tuple):\n        res = []\n        for o in obj:\n            flattened, schema = flatten_to_tuple(o)\n            res.append(flattened)\n        return res, tuple\n\n    if isinstance(obj, dict):\n        res = {}\n        for k, v in obj.items():\n            flattened, schema = flatten_to_tuple(v)\n            res[k] = flattened\n        return res, dict\n\n    if isinstance(obj, Instances):\n        res = {}\n        for k in obj._fields:\n            flattened, schema = flatten_to_tuple(getattr(obj, k))\n            res[k] = flattened\n        return res, Instances\n\n    if isinstance(obj, Boxes):\n        res = {}\n        for k in obj._fields:\n            flattened, schema = flatten_to_tuple(getattr(obj, k))\n            res[k] = flattened\n        return res, Boxes\n\n    if isinstance(obj, ROIMasks):\n        res = {}\n        for k in obj._fields:\n            flattened, schema = flatten_to_tuple(getattr(obj, k))\n            res[k] = flattened\n        return res, ROIMasks\n\n    raise TypeError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    # Check if the object is a string or bytes\n    if isinstance(obj, (str, bytes)):\n        # If it is a string or bytes, return the object as is\n        return obj\n    # Check if the object is a list\n    elif isinstance(obj, list):\n        # If it is a list, recursively flatten each element and return the result as a tuple\n        return tuple(flatten_to_tuple(x) for x in obj)\n    # Check if the object is a tuple\n    elif isinstance(obj, tuple):\n        # If it is a tuple, recursively flatten each element and return the result as a tuple\n        return tuple(flatten_to_tuple(x) for x in obj)\n    # Check if the object is a mapping collection\n    elif isinstance(obj, (dict, set)):\n        # If it is a mapping collection, recursively flatten each value and return the result as a tuple\n        return tuple(flatten_to_tuple(x) for x in obj)\n    # Check if the object is an Instances object\n    elif hasattr(obj, \"get_fields\"):\n        # If it is an Instances object, recursively flatten each field and return the result as a tuple\n        return tuple(flatten_to_tuple(x) for x in obj.get_fields().values())\n    # Check if the object is a Boxes object\n    elif hasattr(obj, \"tensor\"):\n        # If it is a Boxes object, return the tensor attribute of the object as a tuple\n        return tuple(flatten_to_tuple(obj.tensor))\n    # Check if the object is a ROIMasks object\n    elif hasattr(obj, \"masks\"):\n        # If it is a ROIMasks object, return the masks attribute of the object as a tuple\n        return tuple(flatten_to_tuple(obj.masks))\n    # If the object is none of the above types, return the object as is\n    else:\n        return obj"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj, _str_schema)\n    elif isinstance(obj, bytes):\n        return (obj, _bytes_schema)\n    elif isinstance(obj, list):\n        return (obj, _list_schema)\n    elif isinstance(obj, tuple):\n        return (obj, _tuple_schema)\n    elif isinstance(obj, dict):\n        return (obj, _dict_schema)\n    elif isinstance(obj, Instances):\n        return (obj, _instances_schema)\n    elif isinstance(obj, Boxes):\n        return (obj, _boxes_schema)\n    elif isinstance(obj, ROIMasks):\n        return (obj, _roi_masks_schema)\n    else:\n        raise ValueError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj, str)\n    elif isinstance(obj, bytes):\n        return (obj, bytes)\n    elif isinstance(obj, list):\n        res = []\n        for o in obj:\n            r, s = flatten_to_tuple(o)\n            res.append(r)\n        return (res, list)\n    elif isinstance(obj, tuple):\n        res = []\n        for o in obj:\n            r, s = flatten_to_tuple(o)\n            res.append(r)\n        return (res, tuple)\n    elif isinstance(obj, (dict, Mapping)):\n        res = {}\n        for k, v in obj.items():\n            r, s = flatten_to_tuple(v)\n            res[k] = r\n        return (res, dict)\n    elif isinstance(obj, Instances):\n        res = {}\n        res[\"image_size\"] = obj.image_size\n        res[\"fields\"] = {}\n        for k, v in obj.get_fields().items():\n            r, s = flatten_to_tuple(v)\n            res[\"fields\"][k] = r\n        return (res, Instances)\n    elif isinstance(obj, Boxes):\n        res = {}\n        res[\"tensor\"] = obj.tensor\n        res[\"mode\"] = obj.mode\n        return (res, Boxes)\n    elif isinstance(obj, ROIMasks):\n        res = {}\n        res[\"image_size\"] = obj.image_size\n        res[\"masks\"] = obj.masks\n        return (res, ROIMasks)\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return obj, str\n    elif isinstance(obj, bytes):\n        return obj, bytes\n    elif isinstance(obj, list):\n        res = []\n        for item in obj:\n            flattened_item, schema = flatten_to_tuple(item)\n            res.append(flattened_item)\n        return res, list\n    elif isinstance(obj, tuple):\n        res = []\n        for item in obj:\n            flattened_item, schema = flatten_to_tuple(item)\n            res.append(flattened_item)\n        return res, tuple\n    elif isinstance(obj, (Instances, Boxes, ROIMasks)):\n        return obj.to_tuple(), obj.__class__\n    elif isinstance(obj, Mapping):\n        res = {}\n        for key, value in obj.items():\n            flattened_value, schema = flatten_to_tuple(value)\n            res[key] = flattened_value\n        return res, Mapping\n    else:\n        return obj, None\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        res = (obj,)\n        schema = StrSchema\n    elif isinstance(obj, bytes):\n        res = (obj,)\n        schema = BytesSchema\n    elif isinstance(obj, list):\n        res = tuple(flatten_to_tuple(x) for x in obj)\n        schema = ListSchema\n    elif isinstance(obj, tuple):\n        res = tuple(flatten_to_tuple(x) for x in obj)\n        schema = TupleSchema\n    elif isinstance(obj, Mapping):\n        res = tuple(flatten_to_tuple(obj[k]) for k in sorted(obj.keys()))\n        schema = DictSchema\n    elif isinstance(obj, Instances):\n        res = tuple(flatten_to_tuple(obj.get_fields()))\n        schema = InstancesSchema\n    elif isinstance(obj, Boxes):\n        res = tuple(flatten_to_tuple(obj.tensor))\n        schema = BoxesSchema\n    elif isinstance(obj, ROIMasks):\n        res = tuple(flatten_to_tuple(obj.masks))\n        schema = ROIMasksSchema\n    else:\n        raise TypeError(f\"Unsupported type {type(obj)}\")\n\n    return res, schema\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), lambda res: res[0]\n    elif isinstance(obj, bytes):\n        return (obj,), lambda res: res[0]\n    elif isinstance(obj, list):\n        return tuple(flatten_to_tuple(o) for o in obj), lambda res: [schema(r) for r, schema in zip(res, obj)]\n    elif isinstance(obj, tuple):\n        return tuple(flatten_to_tuple(o) for o in obj), lambda res: tuple(schema(r) for r, schema in zip(res, obj))\n    elif isinstance(obj, dict):\n        return tuple(flatten_to_tuple(o) for o in obj.values()), lambda res: {k: schema(r) for r, (k, schema) in zip(res, obj.items())}\n    elif isinstance(obj, Instances):\n        return (obj.to(\"cpu\"),), lambda res: res[0].to(obj.device)\n    elif isinstance(obj, Boxes):\n        return (obj.to(\"cpu\"),), lambda res: res[0].to(obj.device)\n    elif isinstance(obj, ROIMasks):\n        return (obj.to(\"cpu\"),), lambda res: res[0].to(obj.device)\n    else:\n        raise ValueError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj, str)\n    elif isinstance(obj, bytes):\n        return (obj, bytes)\n    elif isinstance(obj, list):\n        return (\n            tuple(flatten_to_tuple(x) for x in obj),\n            list,\n        )\n    elif isinstance(obj, tuple):\n        return (\n            tuple(flatten_to_tuple(x) for x in obj),\n            tuple,\n        )\n    elif isinstance(obj, dict):\n        return (\n            tuple(\n                (flatten_to_tuple(k), flatten_to_tuple(v))\n                for k, v in sorted(obj.items())\n            ),\n            dict,\n        )\n    elif isinstance(obj, Instances):\n        return (\n            flatten_to_tuple(obj.to(\"cpu\")),\n            Instances,\n        )\n    elif isinstance(obj, Boxes):\n        return (\n            flatten_to_tuple(obj.tensor.cpu()),\n            Boxes,\n        )\n    elif isinstance(obj, ROIMasks):\n        return (\n            flatten_to_tuple(obj.masks.cpu()),\n            ROIMasks,\n        )\n    else:\n        raise ValueError(\n            \"Unsupported type {} for flattening\".format(type(obj))\n        )\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), StringSchema\n\n    if isinstance(obj, bytes):\n        return (obj,), BytesSchema\n\n    if isinstance(obj, list):\n        return (\n            tuple(flatten_to_tuple(o)[0] for o in obj),\n            ListSchema(len(obj)),\n        )\n\n    if isinstance(obj, tuple):\n        return (\n            tuple(flatten_to_tuple(o)[0] for o in obj),\n            TupleSchema(len(obj)),\n        )\n\n    if isinstance(obj, dict):\n        return (\n            tuple(\n                (key, flatten_to_tuple(value)[0])\n                for key, value in sorted(obj.items())\n            ),\n            DictSchema(len(obj)),\n        )\n\n    if isinstance(obj, Instances):\n        return (\n            flatten_to_tuple(obj.to(\"cpu\"))[0],\n            InstancesSchema(obj.image_size, len(obj)),\n        )\n\n    if isinstance(obj, Boxes):\n        return (\n            flatten_to_tuple(obj.to(\"cpu\"))[0],\n            BoxesSchema(len(obj)),\n        )\n\n    if isinstance(obj, ROIMasks):\n        return (\n            flatten_to_tuple(obj.to(\"cpu\"))[0],\n            ROIMasksSchema(len(obj)),\n        )\n\n    raise ValueError(f\"Unknown type: {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    # If the input object is a string, bytes, list, tuple, or mapping collection, it will be flattened recursively.\n    if isinstance(obj, (str, bytes, list, tuple, Mapping)):\n        res = []\n        for v in obj:\n            res.append(flatten_to_tuple(v))\n        return tuple(res)\n\n    # If the input object is an Instances, Boxes, or ROIMasks, it will be flattened using the `flatten_to_tuple` method of the object.\n    if isinstance(obj, (Instances, Boxes, ROIMasks)):\n        res = obj.flatten_to_tuple()\n        return res\n\n    # If the input object is an Image, it will be flattened using the `flatten_to_tuple` method of the object.\n    if isinstance(obj, Image):\n        res = obj.flatten_to_tuple()\n        return res\n\n    # If the input object is a tensor, it will be flattened using the `flatten_to_tuple` method of the object.\n    if isinstance(obj, torch.Tensor):\n        res = obj.flatten_to_tuple()\n        return res\n\n    # If the input object is a tuple, it will be flattened using the `flatten_to_tuple` method of the object.\n    if isinstance(obj, tuple):\n        res = obj.flatten_to_tuple()\n        return res\n\n    # If the input object is a BoxMode, it will be flattened using the `flatten_to_tuple` method of the object.\n    if isinstance(obj, BoxMode):\n        res = obj.flatten_to_tuple()\n        return res\n\n    # If the input object is a DeviceType, it will be flattened using the `flatten_to_tuple` method of the object.\n    if isinstance(obj, DeviceType):\n        res = obj.flatten_to_tuple()\n        return res\n\n    # If the input object is a Boxes, it"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    # If the input object is a string, bytes, list, tuple, or mapping collection, it is flattened recursively.\n    if isinstance(obj, (str, bytes, list, tuple, mapping)):\n        return (obj, type(obj)), {}\n\n    # If the input object is an instance of Instances, Boxes, or ROIMasks, it is flattened using their `to_tuple` method.\n    if isinstance(obj, (Instances, Boxes, ROIMasks)):\n        return obj.to_tuple(), type(obj)\n\n    # If the input object is a tensor, it is flattened using its `to_tuple` method.\n    if isinstance(obj, Tensor):\n        return obj.to_tuple(), type(obj)\n\n    # If the input object is a tuple, it is flattened recursively.\n    if isinstance(obj, tuple):\n        return tuple(flatten_to_tuple(o) for o in obj), type(obj)\n\n    # If the input object is a list, it is flattened recursively.\n    if isinstance(obj, list):\n        return [flatten_to_tuple(o) for o in obj], type(obj)\n\n    # If the input object is a mapping collection, it is flattened recursively.\n    if isinstance(obj, mapping):\n        return {k: flatten_to_tuple(v) for k, v in obj.items()}, type(obj)\n\n    # If the input object is none of the above, it is returned as is.\n    return obj, None\n"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # check that the input arrays are numpy arrays\n    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    # check that the input arrays have the correct shape\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"{names[0]} must be a 2D array of shape (n_groups, n_assets).\"\n        )\n    if equations.ndim != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array of shape (n_equations,).\")\n\n    # check that the input arrays have the correct data type\n    if groups.dtype != \"<U\" and groups.dtype != str:\n        raise TypeError(f\"{names[0]} must be a string array.\")\n    if equations.dtype != \"<U\" and equations.dtype != str:\n        raise TypeError(f\"{names[1]} must be a string array.\")\n\n    # check that the input arrays have the correct data type\n    if groups.dtype != \"<U\" and groups.dtype != str:\n        raise TypeError(f\"{names[0]} must be a string array.\")\n    if equations.dtype != \"<U\" and equations.dtype != str:\n        raise TypeError(f\"{names[1]} must be a string array.\")\n\n    # check that the input arrays have the correct data type\n    if groups.dtype != \"<U\" and groups.dtype != str:\n        raise TypeError(f\"{names[0]} must be a string array.\")\n    if equations.dtype != \"<U\" and equations.dtype != str:\n        raise TypeError(f\"{names[1]} must be a string array.\")\n\n    # check that the input arrays have the correct data type\n    if groups.dtype != \"<U\" and groups.dtype != str:\n        raise TypeError(f\"{names[0]} must be a string array.\")\n    if equations.dtype != \"<U\" and equations.dtype != str"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if any of the groups in the equations are part of the input groups\n    if not np.any([group in groups for group in equations]):\n        if raise_if_group_missing:\n            raise ValueError(\n                f\"None of the {names[1]} are part of the {names[0]}.\"\n            )\n        else:\n            warnings.warn(\n                f\"None of the {names[1]} are part of the {names[0]}.\",\n                stacklevel=2,\n            )\n            return None\n\n    # Initialize the left and right matrices\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n\n    # Iterate over the equations and populate the left and right matrices\n    for i, equation in enumerate(equations):\n        # Split the equation into the left and right sides\n        left_side, right_side = equation.split(\"=\")\n\n        # Split the left side into individual terms\n        terms = left_side.split(\"+\")\n\n        # Iterate over the terms and populate the left matrix\n        for term in terms:\n            # Strip any whitespace from the term\n            term = term.strip()\n\n            # Check if the term is a group\n            if term in groups:\n                # Get the index of the group in the groups array\n                group_index = np.where(groups == term)[0][0]\n\n                # Set the corresponding element in the left matrix to 1\n                left[i, group_index] = 1\n            else:\n                # If the term is not a group, it must be a coefficient\n                coefficient = float(term)\n\n                # Set the corresponding element in the left matrix to the coefficient\n                left[i, :] *= coefficient\n\n        # Set the corresponding element in the right matrix to the right side of the equation\n        right[i] = float(right_side)\n\n    # Normalize the left matrix if sum_to_one is True\n    if sum_to_one:\n        left /= left.sum(axis=1, keepdims=True)"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    n_groups, n_assets = groups.shape\n\n    left = np.zeros((len(equations), n_assets))\n    right = np.zeros(len(equations))\n\n    for i, equation in enumerate(equations):\n        parts = equation.split(\"=\")\n        if len(parts) != 2:\n            raise ValueError(\n                f\"Equation {i+1} in {names[1]} is not in the format 'group1 + group2 + ... = value'\"\n            )\n\n        left_side, right_side = parts\n        left_side = left_side.strip()\n        right_side = right_side.strip()\n\n        if not left_side:\n            raise ValueError(\n                f\"Left side of equation {i+1} in {names[1]} is empty\"\n            )\n\n        if not right_side:\n            raise ValueError(\n                f\"Right side of equation {i+1} in {names[1]} is empty\"\n            )\n\n        try:\n            right_value = float(right_side)\n        except ValueError:\n            raise ValueError(\n                f\"Right side of equation {i+1} in {names[1]} is not a number\"\n            )\n\n        left_groups = left_side.split(\"+\")\n        left_groups = [group.strip() for group in left_groups]\n\n        if not left_groups:\n            raise ValueError(\n                f\"Left side of equation {i+1} in {names[1]} is empty\"\n            )\n\n        for group in left_groups:\n            if group not in groups:\n                if raise_if_group_missing:\n                    raise ValueError(\n                        f\"Group '{group}' in equation {i+1} in {names[1]} is not found in {names[0]}\"\n                    )\n                else:\n                    warnings.warn(\n                        f\"Group '{group}' in equation {i+1} in {names[1]} is not found in {names[0]"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if groups is a 2D array\n    if not isinstance(groups, np.ndarray):\n        groups = np.array(groups)\n    if groups.ndim != 2:\n        raise ValueError(f\"{names[0]} must be a 2D array.\")\n\n    # Check if equations is a 1D array\n    if not isinstance(equations, np.ndarray):\n        equations = np.array(equations)\n    if equations.ndim != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array.\")\n\n    # Initialize empty lists to store the left and right matrices\n    left = []\n    right = []\n\n    # Iterate over the equations\n    for equation in equations:\n        # Split the equation into its left and right sides\n        left_side, right_side = equation.split(\"=\")\n\n        # Initialize the left and right matrices for this equation\n        equation_left = []\n        equation_right = []\n\n        # Iterate over the groups\n        for group in groups:\n            # Check if the group is mentioned in the equation\n            if group in left_side:\n                # If so, add a 1 to the left matrix for this group\n                equation_left.append(1)\n            else:\n                # Otherwise, add a 0 to the left matrix for this group\n                equation_left.append(0)\n\n            # Check if the group is mentioned in the right side of the equation\n            if group in right_side:\n                # If so, add a 1 to the right matrix for this group\n                equation_right.append(1)\n            else:\n                # Otherwise, add a 0 to the right matrix for this group\n                equation_right.append(0)\n\n        # Add the left and right matrices for this equation to the overall left and right matrices\n        left.append(equation_left)\n        right.append(equation_right)\n\n    # Convert the left and right matrices to numpy arrays\n    left = np.array(left)\n    right = np.array(right)\n\n    # Check if none of the groups were found in"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if groups and equations are numpy arrays\n    if not isinstance(groups, np.ndarray):\n        groups = np.array(groups)\n    if not isinstance(equations, np.ndarray):\n        equations = np.array(equations)\n\n    # Check if groups and equations have the correct number of dimensions\n    if groups.ndim != 2:\n        raise ValueError(f\"{names[0]} must be a 2D array.\")\n    if equations.ndim != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array.\")\n\n    # Check if groups and equations have the same number of columns\n    if groups.shape[1] != equations.shape[0]:\n        raise ValueError(f\"{names[0]} and {names[1]} must have the same number of columns.\")\n\n    # Check if the number of groups is equal to the number of equations\n    if groups.shape[0] != equations.shape[0]:\n        raise ValueError(f\"The number of groups ({groups.shape[0]}) must be equal to the number of equations ({equations.shape[0]}).\")\n\n    # Check if the number of assets is equal to the number of equations\n    if groups.shape[1] != equations.shape[0]:\n        raise ValueError(f\"The number of assets ({groups.shape[1]}) must be equal to the number of equations ({equations.shape[0]}).\")\n\n    # Check if all elements in groups are strings\n    if not np.issubdtype(groups.dtype, np.str_):\n        raise ValueError(f\"All elements in {names[0]} must be strings.\")\n\n    # Check if all elements in equations are strings\n    if not np.issubdtype(equations.dtype, np.str_):\n        raise ValueError(f\"All elements in {names[1]} must be strings.\")\n\n    # Check if all groups in equations are present in groups\n    for i, equation in enumerate(equations):\n        if not any(all(group == groups[j, :]) for j in range(groups."}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if the input is valid\n    if not isinstance(groups, np.ndarray):\n        raise ValueError(\n            f\"The input {names[0]} must be a numpy array. The input type is {type(groups)}.\"\n        )\n    if not isinstance(equations, np.ndarray):\n        raise ValueError(\n            f\"The input {names[1]} must be a numpy array. The input type is {type(equations)}.\"\n        )\n\n    # Check if the input is a 2D array\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"The input {names[0]} must be a 2D array. The input has {groups.ndim} dimensions.\"\n        )\n\n    # Check if the input is a 1D array\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"The input {names[1]} must be a 1D array. The input has {equations.ndim} dimensions.\"\n        )\n\n    # Check if the input is a string array\n    if not np.issubdtype(equations.dtype, np.str_):\n        raise ValueError(\n            f\"The input {names[1]} must be a string array. The input data type is {equations.dtype}.\"\n        )\n\n    # Check if the input contains only valid strings\n    for equation in equations:\n        if not isinstance(equation, str):\n            raise ValueError(\n                f\"The input {names[1]} must contain only strings. The input contains {type(equation)}.\"\n            )\n\n    # Check if the input is a string array\n    if not np.issubdtype(groups.dtype, np.str_):\n        raise ValueError(\n            f\"The input {names[0]} must be a string array. The input data type is {groups.dtype}.\"\n        )\n\n    # Check if the input contains only valid strings\n    for group in groups:\n        if not isinstance(group, str):\n            raise ValueError(\n                f\"The input {names[0]} must contain only strings. The input"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if any of the groups in the equations are part of the input groups\n    if not np.any([group in groups for group in equations]):\n        if raise_if_group_missing:\n            raise ValueError(\n                f\"None of the {names[0]} in the {names[1]} are part of the input {names[0]}.\"\n            )\n        else:\n            warnings.warn(\n                f\"None of the {names[0]} in the {names[1]} are part of the input {names[0]}.\"\n            )\n            return None\n\n    # Initialize the left and right matrices\n    left = []\n    right = []\n\n    # Loop through each equation\n    for equation in equations:\n        # Split the equation into its left and right sides\n        lhs, rhs = equation.split(\"=\")\n\n        # Initialize the left and right sides of the equation\n        left_side = []\n        right_side = 0\n\n        # Loop through each group in the left side of the equation\n        for group in groups:\n            # Check if the group is present in the left side of the equation\n            if group in lhs:\n                # If so, add a 1 to the left side matrix and add the group's weight to the right side\n                left_side.append(1)\n                right_side += groups[group]\n            else:\n                # If not, add a 0 to the left side matrix\n                left_side.append(0)\n\n        # Add the left and right sides of the equation to the matrices\n        left.append(left_side)\n        right.append(right_side)\n\n    # Convert the matrices to numpy arrays\n    left = np.array(left)\n    right = np.array(right)\n\n    # Check if the sum of each row in the left matrix is 1\n    if sum_to_one:\n        if not np.allclose(np.sum(left, axis=1), 1):\n            raise ValueError(\n                f\"The sum of each row in the left matrix should be 1, but it is not the case.\"\n            )"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check that groups and equations are numpy arrays\n    if not isinstance(groups, np.ndarray):\n        groups = np.array(groups)\n    if not isinstance(equations, np.ndarray):\n        equations = np.array(equations)\n\n    # Check that groups is a 2D array\n    if groups.ndim != 2:\n        raise ValueError(f\"{names[0]} must be a 2D array.\")\n\n    # Check that equations is a 1D array\n    if equations.ndim != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array.\")\n\n    # Check that all elements in equations are strings\n    if not all(isinstance(eq, str) for eq in equations):\n        raise ValueError(f\"All elements in {names[1]} must be strings.\")\n\n    # Check that all elements in groups are strings\n    if not all(isinstance(group, str) for group in groups.flatten()):\n        raise ValueError(f\"All elements in {names[0]} must be strings.\")\n\n    # Check that all groups in equations are present in groups\n    groups_set = set(groups.flatten())\n    for eq in equations:\n        if not all(group in groups_set for group in re.findall(r\"\\w+\", eq)):\n            if raise_if_group_missing:\n                raise ValueError(\n                    f\"All groups mentioned in {names[1]} must be present in {names[0]}.\"\n                )\n            else:\n                warnings.warn(\n                    f\"All groups mentioned in {names[1]} must be present in {names[0]}.\"\n                )\n                return None\n\n    # Initialize left and right matrices\n    n_groups = groups.shape[0]\n    n_assets = groups.shape[1]\n    n_equations = len(equations)\n    left = np.zeros((n_equations, n_groups))\n    right = np.zeros(n_equations)\n\n    # Parse and process each equation\n    for i, eq in enumerate(equations):\n        #"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Checking if groups is a 2D array\n    if not isinstance(groups, np.ndarray) or groups.ndim != 2:\n        raise ValueError(f\"{names[0]} must be a 2D array.\")\n\n    # Checking if equations is a 1D array\n    if not isinstance(equations, np.ndarray) or equations.ndim != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array.\")\n\n    # Checking if groups and equations have the same number of columns\n    if groups.shape[1] != equations.shape[0]:\n        raise ValueError(\n            f\"The number of columns in {names[0]} must be equal to the number of elements in {names[1]}.\"\n        )\n\n    # Checking if all elements in equations are strings\n    if not all(isinstance(eq, str) for eq in equations):\n        raise ValueError(f\"All elements in {names[1]} must be strings.\")\n\n    # Checking if all elements in groups are strings\n    if not np.issubdtype(groups.dtype, np.str_):\n        raise ValueError(f\"All elements in {names[0]} must be strings.\")\n\n    # Checking if all elements in groups are unique\n    if not np.all(np.unique(groups, axis=0).shape == groups.shape):\n        raise ValueError(f\"All elements in {names[0]} must be unique.\")\n\n    # Checking if all elements in equations are unique\n    if not np.all(np.unique(equations).shape == equations.shape):\n        raise ValueError(f\"All elements in {names[1]} must be unique.\")\n\n    # Checking if all elements in equations are valid equations\n    for eq in equations:\n        if not re.match(r\"^[+-]?\\d*\\.?\\d*[a-zA-Z]+[+-]?\\d*\\.?\\d*$\", eq):\n            raise ValueError(f\"Invalid equation: {eq}\")\n\n    # Checking if all elements in groups are"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    # Check if the groups array is empty or has only one row\n    if groups.size == 0 or groups.shape[0] == 1:\n        return None, None\n\n    # Check if the equations array is empty\n    if equations.size == 0:\n        return None, None\n\n    # Check if the groups array has only one column\n    if groups.shape[1] == 1:\n        raise ValueError(\n            f\"The {names[0]} array must have at least two columns, one for the group and the other for the assets.\"\n        )\n\n    # Check if the equations array has only one column\n    if equations.ndim == 1:\n        equations = equations.reshape(-1, 1)\n    elif equations.shape[1] != 1:\n        raise ValueError(\n            f\"The {names[1]} array must have exactly one column, representing the linear equations.\"\n        )\n\n    # Check if the groups array has a string type\n    if not np.issubdtype(groups.dtype, np.str_):\n        raise TypeError(\n            f\"The {names[0]} array must have a string type. Found {groups.dtype} instead.\"\n        )\n\n    # Check if the equations array has a string type\n    if not np.issubdtype(equations.dtype, np.str_):\n        raise TypeError(\n            f\"The {names[1]} array must have a string type. Found {equations.dtype} instead.\"\n        )\n\n    # Check if the groups array has a string type\n    if not np.issubdtype(groups.dtype, np.str_):\n        raise TypeError(\n            f\"The {names[0]} array must have a string type. Found {groups.dtype} instead.\"\n        )\n\n    # Check if the equations array has a string type\n    if not np.issubdtype(equations.dtype, np.str_):\n        raise TypeError(\n            f\"The {names[1]} array must have a string type. Found {equations"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if any of the groups in the equations are part of the input groups\n    if not np.any([group in groups for group in equations]):\n        if raise_if_group_missing:\n            raise ValueError(\n                f\"None of the {names[1]} are part of the {names[0]}\"\n            )\n        else:\n            warnings.warn(\n                f\"None of the {names[1]} are part of the {names[0]}\",\n                stacklevel=2,\n            )\n            return None\n\n    # Initialize empty lists for the left and right matrices\n    left = []\n    right = []\n\n    # Iterate over the equations\n    for equation in equations:\n        # Split the equation into its left and right parts\n        left_part, right_part = equation.split(\"=\")\n\n        # Split the left part into individual groups and signs\n        groups_and_signs = re.findall(r\"([a-zA-Z0-9_]+)([+-]?)\", left_part)\n\n        # Initialize the left and right values for this equation\n        left_value = 0\n        right_value = float(right_part)\n\n        # Iterate over the groups and signs\n        for group, sign in groups_and_signs:\n            # Check if the group is part of the input groups\n            if group in groups:\n                # Get the index of the group in the input groups\n                group_index = np.where(groups == group)[0][0]\n\n                # If the sign is positive, add the group to the left value\n                if sign == \"+\":\n                    left_value += group_index\n                # If the sign is negative, subtract the group from the left value\n                elif sign == \"-\":\n                    left_value -= group_index\n                # If the sign is empty, add the group to the left value with a positive sign\n                else:\n                    left_value += group_index\n            # If the group is not part of the input groups, raise an error or print a warning\n            else:\n                if raise_if_group_missing:\n                    raise ValueError"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert groups and equations to numpy arrays\n    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    # Check if groups and equations have the correct dimensions\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"{names[0]} must be a 2D array with shape (n_groups, n_assets).\"\n        )\n    if equations.ndim != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array.\")\n\n    # Check if each group has at least one asset\n    if np.any(np.sum(groups, axis=1) == 0):\n        raise ValueError(\"Each group must have at least one asset.\")\n\n    # Check if each equation contains at least one group\n    if np.any(equations == \"\"):\n        raise ValueError(\"Each equation must contain at least one group.\")\n\n    # Extract the unique groups mentioned in the equations\n    unique_groups = np.unique(np.concatenate([re.findall(r\"\\w+\", eq) for eq in equations]))\n\n    # Check if all groups mentioned in the equations are present in the input groups\n    missing_groups = set(unique_groups) - set(groups[:, 0])\n    if missing_groups:\n        if raise_if_group_missing:\n            raise ValueError(\n                f\"The following groups are mentioned in the equations but not found in {names[0]}: {missing_groups}\"\n            )\n        else:\n            warnings.warn(\n                f\"The following groups are mentioned in the equations but not found in {names[0]}: {missing_groups}. They will be ignored.\"\n            )\n\n    # Initialize the left and right matrices\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n\n    # Iterate over the equations and groups\n    for i, eq in enumerate(equations):\n        # Extract the groups and coefficients from the equation\n        eq_groups = re.findall(r\"(\\w+)\", eq)"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if groups and equations are numpy arrays\n    if not isinstance(groups, np.ndarray):\n        groups = np.array(groups)\n    if not isinstance(equations, np.ndarray):\n        equations = np.array(equations)\n\n    # Check if groups and equations have the correct number of dimensions\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"{names[0]} must be a 2D array with shape (n_groups, n_assets).\"\n        )\n    if equations.ndim != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array with shape (n_equations,).\")\n\n    # Check if groups and equations have the same number of assets\n    if groups.shape[1] != equations.shape[0]:\n        raise ValueError(\n            f\"{names[0]} and {names[1]} must have the same number of assets.\"\n        )\n\n    # Check if all elements in groups are non-negative integers\n    if not np.all(groups >= 0):\n        raise ValueError(f\"All elements in {names[0]} must be non-negative integers.\")\n\n    # Check if all elements in equations are strings\n    if not np.all([isinstance(eq, str) for eq in equations]):\n        raise ValueError(f\"All elements in {names[1]} must be strings.\")\n\n    # Check if all groups mentioned in the equations are in the groups array\n    groups_set = set(np.unique(groups))\n    for eq in equations:\n        eq_groups = re.findall(r\"g(\\d+)\", eq)\n        if not all(int(g) in groups_set for g in eq_groups):\n            if raise_if_group_missing:\n                raise ValueError(\n                    f\"Group(s) {[g for g in eq_groups if int(g) not in groups_set]} mentioned in an equation are not found in {names[0]}.\"\n                )\n            else:\n                warnings.warn(\n                    f\"Group(s) {[g for g in eq_groups"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if not isinstance(groups, np.ndarray):\n        groups = np.array(groups)\n\n    if not isinstance(equations, np.ndarray):\n        equations = np.array(equations)\n\n    # Check if groups is 2D\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"{names[0]} must be a 2D array, but got {groups.ndim}D array\"\n        )\n\n    # Check if equations is 1D\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"{names[1]} must be a 1D array, but got {equations.ndim}D array\"\n        )\n\n    # Check if all elements in groups are strings\n    if not np.issubdtype(groups.dtype, np.str_):\n        raise ValueError(\n            f\"All elements in {names[0]} must be strings, but got {groups.dtype}\"\n        )\n\n    # Check if all elements in equations are strings\n    if not np.issubdtype(equations.dtype, np.str_):\n        raise ValueError(\n            f\"All elements in {names[1]} must be strings, but got {equations.dtype}\"\n        )\n\n    # Check if all groups in equations are present in groups\n    if not np.all(np.isin(np.unique(equations), groups)):\n        if raise_if_group_missing:\n            raise ValueError(\n                f\"Not all groups in {names[1]} are present in {names[0]}\"\n            )\n        else:\n            warnings.warn(\n                f\"Not all groups in {names[1]} are present in {names[0]}. Ignoring them.\"\n            )\n\n    # Initialize left and right arrays\n    left = []\n    right = []\n\n    # Loop over equations\n    for eq in equations:\n        # Split equation into left and right parts\n        left_part, right_part = eq.split(\"=\")\n\n        # Initialize left and right arrays for this equation\n        left_eq = []"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    # Check if the input groups array is 2D\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"The {names[0]} array must be 2D, but it has {groups.ndim} dimensions.\"\n        )\n\n    # Check if the input equations array is 1D\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"The {names[1]} array must be 1D, but it has {equations.ndim} dimensions.\"\n        )\n\n    # Check if there are any missing groups in the equations\n    missing_groups = set()\n    for equation in equations:\n        for group in groups:\n            if group not in equation:\n                missing_groups.add(group)\n\n    if missing_groups:\n        if raise_if_group_missing:\n            raise ValueError(\n                f\"The following groups are mentioned in the {names[1]} but not found in the {names[0]}: {missing_groups}\"\n            )\n        else:\n            warnings.warn(\n                f\"The following groups are mentioned in the {names[1]} but not found in the {names[0]}: {missing_groups}\"\n            )\n\n    # Initialize the left and right matrices\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n\n    # Iterate over the equations and groups\n    for i, equation in enumerate(equations):\n        # Split the equation into terms\n        terms = equation.split()\n        # Iterate over the terms and update the left and right matrices\n        for term in terms:\n            if term.startswith(\"-\"):\n                # If the term starts with a minus sign, it represents a negative coefficient\n                sign = -1\n                term = term[1:]\n            else:\n                # Otherwise, the term is positive\n                sign = 1\n            # Check if the term is a group\n            if term in groups:\n                # If it is, update the"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if not isinstance(groups, np.ndarray):\n        groups = np.array(groups)\n\n    if not isinstance(equations, np.ndarray):\n        equations = np.array(equations)\n\n    # check if the groups array is 2D\n    if groups.ndim != 2:\n        raise ValueError(f\"{names[0]} must be a 2D array\")\n\n    # check if the equations array is 1D\n    if equations.ndim != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array\")\n\n    # check if the number of groups in the equations is equal to the number of columns in the groups array\n    if len(equations) != groups.shape[1]:\n        raise ValueError(\n            f\"The number of groups in {names[0]} must be equal to the number of equations in {names[1]}\"\n        )\n\n    # check if the groups in the equations are present in the groups array\n    for equation in equations:\n        if not any(group in equation for group in groups):\n            if raise_if_group_missing:\n                raise ValueError(\n                    f\"The following group is not present in {names[0]}: {equation}\"\n                )\n            else:\n                warnings.warn(\n                    f\"The following group is not present in {names[0]}: {equation}\"\n                )\n\n    # create the left and right matrices\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n\n    # loop through the equations\n    for i, equation in enumerate(equations):\n        # loop through the groups\n        for j, group in enumerate(groups):\n            # check if the group is present in the equation\n            if group in equation:\n                # if the group is present, set the corresponding element in the left matrix to 1\n                left[i, j] = 1\n\n        # check if the equation is a sum to one constraint\n        if sum_to_one:\n            # if it is, set the corresponding element in the right matrix to 1"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if not isinstance(groups, np.ndarray):\n        groups = np.array(groups)\n\n    if not isinstance(equations, np.ndarray):\n        equations = np.array(equations)\n\n    n_groups, n_assets = groups.shape\n    n_equations = len(equations)\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        equation_split = equation.split(\"=\")\n        if len(equation_split) != 2:\n            raise ValueError(\n                f\"Equation {i} is not in the correct format. It should be of the form 'group1 + group2 + ... = number'.\"\n            )\n\n        left_side = equation_split[0].strip()\n        right_side = equation_split[1].strip()\n\n        try:\n            right[i] = float(right_side)\n        except ValueError:\n            raise ValueError(\n                f\"Equation {i} is not in the correct format. The right side should be a number.\"\n            )\n\n        for group in left_side.split(\"+\"):\n            group = group.strip()\n            if group not in groups:\n                if raise_if_group_missing:\n                    raise ValueError(\n                        f\"Group {group} is not found in the {names[0]} array. Please check the spelling.\"\n                    )\n                else:\n                    warnings.warn(\n                        f\"Group {group} is not found in the {names[0]} array. Please check the spelling.\",\n                        stacklevel=2,\n                    )\n                    continue\n\n            group_index = np.where(groups == group)[0][0]\n            left[i, group_index] = 1\n\n    if sum_to_one:\n        left = np.append(left, np.ones((1, n_assets)), axis=0)\n        right = np.append(right, np.ones(1))\n\n    if np.all(left == 0):\n        return None, None\n\n    return"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if the input groups and equations are valid\n    if groups is None or equations is None:\n        raise ValueError(\"Both groups and equations must be provided.\")\n\n    # Check if the input groups and equations are of the correct type\n    if not isinstance(groups, (list, np.ndarray)) or not isinstance(equations, (list, np.ndarray)):\n        raise TypeError(\"Both groups and equations must be array-like objects.\")\n\n    # Check if the input groups and equations are not empty\n    if len(groups) == 0 or len(equations) == 0:\n        raise ValueError(\"Both groups and equations must not be empty.\")\n\n    # Check if the input groups and equations are of the correct shape\n    if len(groups.shape) != 2 or len(equations.shape) != 1:\n        raise ValueError(\"Both groups and equations must be 2D and 1D, respectively.\")\n\n    # Check if the input groups and equations have the correct number of elements\n    if groups.shape[1] != len(equations[0]):\n        raise ValueError(\"The number of groups must match the number of assets in each equation.\")\n\n    # Check if the input groups and equations are valid\n    if not all(isinstance(group, (list, np.ndarray)) for group in groups):\n        raise TypeError(\"Each group must be a list or numpy array.\")\n\n    # Check if the input groups and equations are valid\n    if not all(isinstance(equation, str) for equation in equations):\n        raise TypeError(\"Each equation must be a string.\")\n\n    # Check if the input groups and equations are valid\n    if not all(len(equation) == len(groups[0]) for equation in equations):\n        raise ValueError(\"Each equation must have the same number of assets as the number of groups.\")\n\n    # Check if the input groups and equations are valid\n    if not all(isinstance(group[0], (int, float)) for group in groups):\n        raise TypeError(\"Each group must contain only numeric values.\")\n\n    # Check if the input groups and equations are valid\n    if not all(isinstance(equation[0], str) for"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if groups is a numpy array\n    if not isinstance(groups, np.ndarray):\n        groups = np.array(groups)\n\n    # Check if equations is a numpy array\n    if not isinstance(equations, np.ndarray):\n        equations = np.array(equations)\n\n    # Check if groups and equations have the correct number of dimensions\n    if groups.ndim != 2:\n        raise ValueError(f\"{names[0]} must be a 2D array.\")\n    if equations.ndim != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array.\")\n\n    # Check if groups and equations have the correct data types\n    if groups.dtype != \"<U10\":\n        raise ValueError(f\"{names[0]} must be a string array.\")\n    if equations.dtype != \"<U100\":\n        raise ValueError(f\"{names[1]} must be a string array.\")\n\n    # Check if all groups in equations are present in groups\n    missing_groups = set(\n        [group for equation in equations for group in equation.split(\"+\") if group not in groups]\n    )\n    if missing_groups and raise_if_group_missing:\n        raise ValueError(f\"Groups {missing_groups} not found in {names[0]}.\")\n    elif missing_groups:\n        warnings.warn(f\"Groups {missing_groups} not found in {names[0]}.\")\n\n    # Check if all groups in equations are unique\n    unique_groups = set(groups.flatten())\n    if len(unique_groups) != groups.size:\n        raise ValueError(f\"All groups in {names[0]} must be unique.\")\n\n    # Check if all equations are valid\n    for equation in equations:\n        if not re.match(r\"^\\s*[+-]?\\d+(\\.\\d+)?\\s*(?:[+-]\\s*\\d+(\\.\\d+)?\\s*)*$\", equation):\n            raise ValueError(f\"Invalid equation: {equation}\")\n\n   "}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if any of the groups in the equations are part of the input groups\n    groups_in_equations = np.any(np.isin(groups, np.unique(equations)).any(axis=0))\n\n    # If none of the groups in the equations are part of the input groups, return None\n    if not groups_in_equations:\n        return None\n\n    # Initialize the left and right matrices\n    left = []\n    right = []\n\n    # Iterate over the equations\n    for eq in equations:\n        # Split the equation into two parts\n        eq_left, eq_right = eq.split(\"=\")\n\n        # Split the left part into individual terms\n        terms = eq_left.split(\"+\")\n\n        # Initialize the row for the left matrix\n        row = np.zeros(groups.shape[1])\n\n        # Iterate over the terms\n        for term in terms:\n            # Strip any leading/trailing whitespace from the term\n            term = term.strip()\n\n            # Check if the term is a group\n            if term in groups:\n                # Find the index of the group in the groups array\n                idx = np.where(groups == term)[0][0]\n\n                # Set the corresponding element in the row to 1\n                row[idx] = 1\n\n            # Check if the term is a constant\n            elif term.isdigit():\n                # Convert the constant to a float\n                constant = float(term)\n\n                # Add the constant to the right matrix\n                right.append(constant)\n\n        # Add the row to the left matrix\n        left.append(row)\n\n    # Convert the left and right matrices to numpy arrays\n    left = np.array(left)\n    right = np.array(right)\n\n    # If sum_to_one is True, add a constraint to ensure that all elements in a group sum to 1\n    if sum_to_one:\n        for group in groups:\n            # Find the indices of the assets in the group\n            idx = np.where(groups == group)[0]\n\n            # Add a constraint to ensure that all elements in the"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import os\n    import sys\n    import torch\n    import tempfile\n    import importlib\n    from detectron2.structures import Instances\n\n    # Create a temporary file to write the new class to\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n        # Write the new class to the temporary file\n        f.write(\"import torch\\n\")\n        f.write(\"from detectron2.structures import Instances\\n\")\n        f.write(\"from typing import List, Tuple, Union\\n\")\n        f.write(\"\\n\")\n        f.write(\"class Instances(Instances):\\n\")\n        f.write(\"    def __init__(self, image_size: Tuple[int, int], **kwargs):\\n\")\n        f.write(\"        super().__init__(image_size)\\n\")\n        f.write(\"        self.extra_fields = {}\\n\")\n        for field in fields:\n            f.write(f\"        self.{field[0]} = {field[1]}\\n\")\n        f.write(\"\\n\")\n        f.write(\"    def __getitem__(self, item):\\n\")\n        f.write(\"        instance = Instances(self.image_size)\\n\")\n        f.write(\"        for k, v in self.extra_fields.items():\\n\")\n        f.write(\"            instance.extra_fields[k] = v[item]\\n\")\n        f.write(\"        return instance\\n\")\n        f.write(\"\\n\")\n        f.write(\"    def to(self, device: Union[torch.device, str]):\\n\")\n        f.write(\"        cast_tensor = self.tensor.to(device)\\n\")\n        f.write(\"        cast_instance = Instances(self.image_size)\\n\")\n        f.write(\"        for k, v in self.extra_fields.items():\\n\")\n        f.write(\"            cast_instance.extra_fields[k] = v.to(device)\\n\")\n        f.write(\"        return cast_instance\\n\")"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import torch\n    import detectron2\n    import inspect\n    import tempfile\n    import os\n    import sys\n    import importlib\n    import types\n\n    # Get the original 'Instances' class from detectron2\n    Instances = detectron2.structures.instances.Instances\n\n    # Generate a new module for the new 'Instances' class\n    new_module_name = \"new_instances_module\"\n    new_module = types.ModuleType(new_module_name)\n\n    # Generate a new class for the new 'Instances' class\n    new_class_name = \"Instances\"\n    new_class_bases = (Instances,)\n    new_class_dict = {\n        \"__init__\": Instances.__init__,\n        \"__getitem__\": Instances.__getitem__,\n        \"__setitem__\": Instances.__setitem__,\n        \"__len__\": Instances.__len__,\n        \"__iter__\": Instances.__iter__,\n        \"__next__\": Instances.__next__,\n        \"__repr__\": Instances.__repr__,\n        \"__str__\": Instances.__str__,\n        \"__eq__\": Instances.__eq__,\n        \"__ne__\": Instances.__ne__,\n        \"__hash__\": Instances.__hash__,\n        \"__copy__\": Instances.__copy__,\n        \"__deepcopy__\": Instances.__deepcopy__,\n        \"__reduce__\": Instances.__reduce__,\n        \"__reduce_ex__\": Instances.__reduce_ex__,\n        \"__getstate__\": Instances.__getstate__,\n        \"__setstate__\": Instances.__setstate__,\n        \"__getnewargs__\": Instances.__getnewargs__,\n        \"__getnewargs_ex__\": Instances.__getnewargs_ex__,\n        \"__sizeof__\": Instances.__sizeof__,\n        \"__dir__\": Instances.__dir__,\n        \"__class__\": Instances.__class__,\n        \"__doc__\": Inst"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import torch\n    import detectron2\n    import inspect\n    import tempfile\n    import importlib\n    import os\n    import sys\n\n    # Get the source code of the 'Instances' class from detectron2\n    source_code = inspect.getsource(detectron2.structures.instances.Instances)\n\n    # Create a new module for the new class\n    new_module = tempfile.NamedTemporaryFile(suffix='.py', delete=False)\n    new_module.write(source_code.encode())\n    new_module.close()\n\n    # Import the new module\n    spec = importlib.util.spec_from_file_location('new_instances', new_module.name)\n    new_module = importlib.util.module_from_spec(spec)\n    sys.modules['new_instances'] = new_module\n    spec.loader.exec_module(new_module)\n\n    # Create the new class\n    class NewInstances(new_module.Instances):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.fields = fields\n\n    # Set up the environment for torchscript\n    torch.jit.register_script_class(NewInstances, 'NewInstances')\n\n    # Yield the new class\n    yield NewInstances\n\n    # Clean up\n    del sys.modules['new_instances']\n    os.remove(new_module.name)"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import os\n    import sys\n    import tempfile\n    import importlib\n    import torch\n    from detectron2.structures import Instances\n\n    # Define the class to be written to a temporary file\n    class Instances_new(Instances):\n        def __init__(self, **kwargs):\n            super().__init__(**kwargs)\n            self._fields = fields\n\n    # Write the class to a temporary file\n    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n        f.write('from detectron2.structures import Instances\\n')\n        f.write(f'class Instances_new(Instances):\\n')\n        f.write(f'    def __init__(self, **kwargs):\\n')\n        f.write(f'        super().__init__(**kwargs)\\n')\n        f.write(f'        self._fields = {fields}\\n')\n\n    # Import the class from the temporary file\n    spec = importlib.util.spec_from_file_location('instances_new', f.name)\n    module = importlib.util.module_from_spec(spec)\n    sys.modules['instances_new'] = module\n    spec.loader.exec_module(module)\n\n    # Set up the environment so that torchscript can recognize and work with the new class\n    torch.jit.script(Instances_new)\n\n    # Yield the new class\n    yield Instances_new\n\n    # Clean up by removing modifications\n    del sys.modules['instances_new']\n    os.remove(f.name)"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # Import statements\n    import os\n    import sys\n    import torch\n    from detectron2.structures import Instances\n\n    # Define the name of the module to be generated\n    module_name = \"instances_patch\"\n\n    # Define the code for the new module\n    module_code = f\"\"\""}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import torch\n    import detectron2\n    import tempfile\n    import importlib\n    import os\n\n    # Create a new module for the new class\n    new_module_name = \"new_instances_module\"\n    new_module = types.ModuleType(new_module_name)\n    new_module_path = tempfile.mkdtemp()\n    new_module_file_path = os.path.join(new_module_path, \"new_instances_module.py\")\n\n    # Write the new class definition to the module file\n    with open(new_module_file_path, \"w\") as f:\n        f.write(\"import torch\\n\")\n        f.write(\"from detectron2.structures import Instances\\n\")\n        f.write(\"class NewInstances(Instances):\\n\")\n        for field in fields:\n            f.write(f\"    {field[0]} = torch.jit.Attribute({field[1]}, {field[2]})\\n\")\n\n    # Import the new module\n    spec = importlib.util.spec_from_file_location(new_module_name, new_module_file_path)\n    new_module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(new_module)\n\n    # Replace the original 'Instances' class with the new one\n    detectron2.structures.Instances = new_module.NewInstances\n\n    # Set up the environment for torchscript\n    torch.jit.script(new_module.NewInstances)\n\n    # Yield the new class\n    yield new_module.NewInstances\n\n    # Clean up by removing modifications\n    detectron2.structures.Instances = detectron2.structures.instances.Instances\n    importlib.reload(detectron2.structures)\n    importlib.reload(detectron2.structures.instances)\n    importlib.reload(new_module)\n    os.remove(new_module_file_path)\n    os.rmdir(new"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import os\n    import sys\n    import tempfile\n    import importlib\n    import torch\n    import detectron2\n    from detectron2.structures import Instances\n\n    # Define the new 'Instances' class\n    class Instances(Instances):\n        pass\n\n    # Generate the new class definition\n    class_definition = \"class Instances(Instances):\\n\"\n    for field_name, field_type in fields:\n        class_definition += f\"    {field_name}: {field_type}\\n\"\n\n    # Write the new class definition to a temporary file\n    temp_dir = tempfile.mkdtemp()\n    temp_file = os.path.join(temp_dir, \"instances.py\")\n    with open(temp_file, \"w\") as f:\n        f.write(class_definition)\n\n    # Import the new class definition\n    sys.path.insert(0, temp_dir)\n    module = importlib.import_module(\"instances\")\n    new_class = getattr(module, \"Instances\")\n\n    # Replace the original 'Instances' class with the new one\n    detectron2.structures.Instances = new_class\n\n    # Set up torchscript to recognize the new class\n    torch.jit.register_script_class(new_class)\n\n    # Yield the new class for the duration of the context\n    yield new_class\n\n    # Clean up modifications\n    del sys.path[0]\n    os.remove(temp_file)\n    os.rmdir(temp_dir)\n    detectron2.structures.Instances = Instances\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # Import necessary modules\n    import os\n    import sys\n    import torch\n    import tempfile\n    import importlib\n    from detectron2.structures import Instances\n\n    # Create a temporary directory\n    temp_dir = tempfile.TemporaryDirectory()\n\n    # Define the name of the new module\n    new_module_name = 'new_instances'\n\n    # Define the path to the new module\n    new_module_path = os.path.join(temp_dir.name, new_module_name + '.py')\n\n    # Write the new module to the temporary file\n    with open(new_module_path, 'w') as f:\n        f.write('import torch\\n')\n        f.write('from detectron2.structures import Instances\\n')\n        f.write('class NewInstances(Instances):\\n')\n        for field in fields:\n            f.write(f'    {field[0]} = torch.jit.ScriptObject()\\n')\n        f.write('    def __init__(self, **kwargs):\\n')\n        for field in fields:\n            f.write(f'        self.{field[0]} = kwargs.pop(\"{field[0]}\", None)\\n')\n        f.write('        super().__init__(**kwargs)\\n')\n\n    # Import the new module\n    spec = importlib.util.spec_from_file_location(new_module_name, new_module_path)\n    new_module = importlib.util.module_from_spec(spec)\n    sys.modules[new_module_name] = new_module\n    spec.loader.exec_module(new_module)\n\n    # Set up the environment for torchscript\n    torch.jit.set_annotated_patterns_enabled(True)\n\n    # Yield the new class\n    yield new_module.NewInstances\n\n    # Clean up\n    del sys.modules[new_module_name]\n    temp_dir.cleanup()\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # Import packages\n    import os\n    import torch\n    import tempfile\n    import importlib\n    import detectron2.structures\n\n    # Define the name of the new class\n    class_name = \"Instances\"\n\n    # Define the module name for the new class\n    module_name = \"instances\"\n\n    # Define the path to the temporary file\n    temp_file_path = os.path.join(tempfile.gettempdir(), f\"{module_name}.py\")\n\n    # Define the code for the new class\n    class_code = f\"import torch\\n\\nclass {class_name}:\\n\"\n    for field in fields:\n        class_code += f\"    {field[0]} = torch.jit.Attribute({field[1]}, {field[2]})\\n\"\n\n    # Write the code to a temporary file\n    with open(temp_file_path, \"w\") as f:\n        f.write(class_code)\n\n    # Import the new module\n    spec = importlib.util.spec_from_file_location(module_name, temp_file_path)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n\n    # Replace the 'Instances' class in detectron2 with the new class\n    detectron2.structures.Instances = getattr(module, class_name)\n\n    # Yield the new class\n    yield getattr(module, class_name)\n\n    # Remove the temporary file\n    os.remove(temp_file_path)\n\n    # Remove the new class from the module\n    delattr(module, class_name)\n\n    # Remove the module from sys.modules\n    del sys.modules[module_name]\n\n    # Restore the original 'Instances' class in detectron2\n    detectron2.structures.Instances = original_instances\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import torch\n    import detectron2\n    from detectron2.structures import Instances\n\n    # Define the new class\n    class Instances(torch.nn.Module):\n        def __init__(self, fields):\n            super().__init__()\n            for field_name, field_type in fields:\n                setattr(self, field_name, field_type)\n\n        def __getitem__(self, index):\n            return {field_name: getattr(self, field_name)[index] for field_name, _ in fields}\n\n        def __len__(self):\n            return len(getattr(self, fields[0][0]))\n\n    # Generate the new module\n    module_code = f\"\"\""}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import os\n    import sys\n    import tempfile\n    import importlib\n    from detectron2.structures import Instances\n\n    # Define the new class using the input fields\n    new_class_def = f\"class Instances(Instances):\\n\"\n    for field in fields:\n        new_class_def += f\"    {field[0]}: {field[1]}\\n\"\n\n    # Create a temporary file for the new class\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n        f.write(new_class_def)\n        temp_file_name = f.name\n\n    # Import the new class from the temporary file\n    module_name = os.path.splitext(os.path.basename(temp_file_name))[0]\n    spec = importlib.util.spec_from_file_location(module_name, temp_file_name)\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[module_name] = module\n    spec.loader.exec_module(module)\n\n    # Replace the 'Instances' class in detectron2 with the new class\n    Instances.__class__ = getattr(module, \"Instances\")\n\n    # Set up the environment for torchscript to recognize the new class\n    import torch\n    torch.jit._recursive.register_attribute(Instances, \"__module__\", \"detectron2.structures\")\n    torch.jit._recursive.register_attribute(Instances, \"__name__\", \"Instances\")\n    torch.jit._recursive.register_attribute(Instances, \"__qualname__\", \"detectron2.structures.Instances\")\n\n    # Yield the new class\n    yield Instances\n\n    # Clean up after the context manager exits\n    del sys.modules[module_name]\n    os.remove(temp_file_name)\n    torch.jit._recursive.unregister_attribute(Instances, \"__module__\")\n    torch.j"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    from detectron2.structures import Instances\n    from detectron2.utils.env import TORCH_VERSION\n    import torch\n    import sys\n    import os\n    import inspect\n\n    # Define the new class\n    class ScriptableInstances(Instances):\n        __annotations__ = {k: v for k, v in fields}\n\n    # Get the source code of the new class\n    source = inspect.getsource(ScriptableInstances)\n\n    # Define the name of the new module and the path to the temporary file\n    module_name = \"scriptable_instances\"\n    temp_file = f\"{module_name}.py\"\n\n    # Write the source code to the temporary file\n    with open(temp_file, \"w\") as f:\n        f.write(source)\n\n    # Add the temporary file's directory to the system path\n    sys.path.append(os.path.dirname(temp_file))\n\n    # Import the new module\n    module = __import__(module_name)\n\n    # Get the new class from the module\n    ScriptableInstances = getattr(module, \"ScriptableInstances\")\n\n    # Replace the 'Instances' class in detectron2 with the new class\n    Instances = ScriptableInstances\n\n    # Set up the environment for torchscript\n    if TORCH_VERSION >= (1, 8):\n        torch._C._jit_set_class_attribute(\n            \"ScriptableInstances\", \"__torch_function__\", torch._C.ScriptClass\n        )\n\n    # Yield the new class\n    yield\n\n    # Clean up after the context is exited\n    # Remove the temporary file and its directory from the system path\n    os.remove(temp_file)\n    sys.path.remove(os.path.dirname(temp_file))\n\n    # Restore the original 'Instances' class in detectron2\n    Instances = Instances\n\n    # Remove the torchscript class attribute\n    if TORCH_VERSION >= (1, 8):\n        torch._C._jit_set_class"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import torch\n    import torch.nn as nn\n    import torch.jit as jit\n    import detectron2\n    from detectron2.structures import Instances\n    import importlib\n    import tempfile\n    import os\n    import sys\n\n    class Instances(nn.Module):\n        __constants__ = ['_fields']\n\n        def __init__(self, num_instances=0, **kwargs):\n            super().__init__()\n            self._fields = {}\n            values = list(kwargs.values())\n            for k, v in zip(fields, values):\n                self._fields[k] = v\n\n        def __getattr__(self, key):\n            if key == \"_fields\" or key in self._fields:\n                return getattr(self, key)\n            if key in self._fields:\n                return self._fields[key]\n            raise AttributeError(\"Cannot find field '{}' in the given Instances!\".format(key))\n\n        def __setattr__(self, key, value):\n            if key == \"_fields\":\n                return super().__setattr__(key, value)\n            self._fields[key] = value\n\n        def has(self, key):\n            return key in self._fields\n\n        def get(self, key):\n            return self._fields.get(key)\n\n        def set(self, key, value):\n            self._fields[key] = value\n\n        def keys(self):\n            return self._fields.keys()\n\n        def values(self):\n            return self._fields.values()\n\n        def items(self):\n            return self._fields.items()\n\n        def __len__(self):\n            field_lengths = [v.shape[0] for v in self._fields.values()]\n            if len(field_lengths) > 0 and all(x == field_lengths[0] for x in field_lengths):\n                return field_lengths[0]\n            else:\n                raise ValueError(\"Instances do not have consistent field lengths\")\n\n        def __iter__(self):\n            raise NotImplementedError(\""}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # Import dependencies\n    import torch\n    import detectron2\n    from detectron2.structures import Instances\n    import tempfile\n    import importlib\n    import sys\n    import os\n\n    # Define the module name and path for the new class\n    module_name = \"new_instances\"\n    module_path = os.path.join(tempfile.gettempdir(), module_name + \".py\")\n\n    # Define the class definition\n    class_definition = f\"\"\""}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import os\n    import sys\n    import tempfile\n    import torch\n    import detectron2\n\n    # Generate the code for the new class\n    code = f\"class Instances:\\n\"\n    for field_name, field_type in fields:\n        code += f\"    {field_name}: {field_type}\\n\"\n    code += f\"    def __init__(self, **kwargs):\\n\"\n    for field_name, field_type in fields:\n        code += f\"        self.{field_name} = kwargs.get('{field_name}', None)\\n\"\n    code += f\"    def __getitem__(self, item):\\n\"\n    code += f\"        return getattr(self, item)\\n\"\n    code += f\"    def to(self, device):\\n\"\n    code += f\"        for field_name in self.fields():\\n\"\n    code += f\"            setattr(self, field_name, getattr(self, field_name).to(device))\\n\"\n    code += f\"        return self\\n\"\n    code += f\"    def fields(self):\\n\"\n    code += f\"        return {[f'\\\"{field_name}\\\"' for field_name, _ in fields]}\\n\"\n\n    # Write the code to a temporary file\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as temp_file:\n        temp_file.write(code)\n        temp_file_name = temp_file.name\n\n    # Add the temporary file to the system path\n    sys.path.append(os.path.dirname(temp_file_name))\n\n    # Import the new class\n    from instances import Instances\n\n    # Replace the 'Instances' class in detectron2 with the new class\n    detectron2.structures.instances.Instances = Instances\n\n    # Set up the environment for torchscript\n    torch.jit.frontend.torchscript_passes.register_pass(torch.jit.frontend.torchscript"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    from detectron2.structures import Instances\n    from detectron2.utils.env import TORCH_VERSION\n\n    import torch\n    import os\n    import sys\n    import tempfile\n    import importlib\n    import types\n\n    # Generate the new class\n    class NewInstances(Instances):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            for field in fields:\n                setattr(self, field[0], field[1])\n\n    # Write the new class to a temporary file\n    with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as f:\n        f.write(b'from detectron2.structures import Instances\\n\\n')\n        f.write(b'class NewInstances(Instances):\\n')\n        for field in fields:\n            f.write(f'\\t{field[0]} = {field[1]}\\n'.encode())\n        f.write(b'\\tdef __init__(self, *args, **kwargs):\\n')\n        f.write(b'\\t\\tsuper().__init__(*args, **kwargs)\\n')\n        for field in fields:\n            f.write(f'\\t\\tself.{field[0]} = {field[1]}\\n'.encode())\n        f.write(b'\\n')\n        f.close()\n\n    # Import the new class from the temporary file\n    spec = importlib.util.spec_from_file_location('new_instances', f.name)\n    new_module = importlib.util.module_from_spec(spec)\n    sys.modules['new_instances'] = new_module\n    spec.loader.exec_module(new_module)\n    NewInstances = new_module.NewInstances\n\n    # Set up the environment to work with the new class\n    torch._C._jit_set_class_attribute(torch._C.ScriptClass, 'NewInstances', NewInstances)\n    torch._C._jit_set_class_attribute(torch._"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import os\n    import sys\n    import tempfile\n    import importlib\n    import torch\n    import detectron2\n\n    # Get the path to the detectron2 installation\n    detectron2_path = os.path.dirname(detectron2.__file__)\n\n    # Define the name of the module and class\n    module_name = 'instances'\n    class_name = 'Instances'\n\n    # Define the code for the new class\n    class_code = f'''"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import torch\n    from detectron2.structures import Instances\n    from torch.jit import ScriptModule\n\n    # Create a new class with the specified fields\n    class NewInstances(Instances):\n        __annotations__ = {name: type for name, type in fields}\n\n    # Create a new module with the new class\n    new_module_name = \"new_instances_module\"\n    new_module_code = f\"import torch\\nclass {new_module_name}(Instances):\\n\"\n    for name, type in fields:\n        new_module_code += f\"    {name}: torch.Tensor\\n\"\n\n    # Write the new module to a temporary file\n    with open(f\"{new_module_name}.py\", \"w\") as f:\n        f.write(new_module_code)\n\n    # Import the new module\n    import importlib\n    import sys\n\n    sys.path.append(\".\")\n    importlib.import_module(new_module_name)\n\n    # Set up the environment for torchscript\n    torch.jit.frontend.torch_version = (1, 13)\n    torch.jit.frontend.torch_version_for_onnx = \"1.13\"\n    torch.jit.frontend.torch_version_for_ort = \"1.13\"\n    torch.jit.frontend.torch_version_for_fx = \"1.13\"\n    torch.jit.frontend.torch_version_for_mobile = \"1.13\"\n\n    # Replace the original 'Instances' class with the new one\n    Instances = NewInstances\n\n    # Yield the new class\n    yield NewInstances\n\n    # Clean up\n    import os\n\n    os.remove(f\"{new_module_name}.py\")\n    del sys.modules[new_module_name]\n    torch.jit.frontend.torch_version = None\n    torch.jit.frontend.torch_version_"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import torch\n    import torch.nn as nn\n    import torch.jit as jit\n    from detectron2.structures import Instances\n\n    # Define the new class\n    class Instances(nn.Module):\n        def __init__(self, *args, **kwargs):\n            super().__init__()\n            self.fields = nn.ModuleList()\n            for field in fields:\n                self.fields.append(nn.Parameter(torch.tensor(field)))\n\n        def __getitem__(self, idx):\n            return self.fields[idx]\n\n        def __len__(self):\n            return len(self.fields)\n\n        def __repr__(self):\n            return f\"Instances({len(self.fields)})\"\n\n    # Create a temporary file for the new class\n    import tempfile\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n        f.write('class Instances:\\n')\n        for field in fields:\n            f.write(f'    {field[0]} = {field[1]}\\n')\n        f.close()\n\n    # Import the new class\n    import importlib.util\n    spec = importlib.util.spec_from_file_location('instances_module', f.name)\n    instances_module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(instances_module)\n\n    # Replace the original 'Instances' class with the new one\n    Instances.__class__ = instances_module.Instances\n\n    # Set up the environment for torchscript\n    jit.unused(Instances)\n\n    # Yield the new class\n    yield Instances\n\n    # Clean up\n    import os\n    os.remove(f.name)\n    Instances.__class__ = Instances\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import torch\n    import torch.nn as nn\n    import torch.jit as jit\n    from detectron2.structures import Instances\n\n    # Create a new module for the new 'Instances' class\n    new_module = torch.jit.script(type(\"NewInstances\", (Instances,), {}))\n    new_module.__module__ = \"new_instances\"\n\n    # Create a new class that inherits from the new module and adds the fields\n    new_class = type(\"NewInstances\", (new_module,), {})\n\n    # Create a new module for the new class\n    new_class_module = torch.jit.script(new_class)\n    new_class_module.__module__ = \"new_instances_class\"\n\n    # Create a new class that inherits from the new class module and adds the fields\n    new_class_with_fields = type(\"NewInstancesWithFields\", (new_class_module,), {})\n\n    # Add the fields to the new class\n    for field in fields:\n        new_class_with_fields.__setattr__(field[0], field[1])\n\n    # Write the new class to a temporary file\n    with open(\"new_instances_class.py\", \"w\") as f:\n        f.write(jit.script(new_class_with_fields).__repr__())\n\n    # Import the new class\n    import new_instances_class\n\n    # Replace the 'Instances' class with the new class\n    Instances = new_class_with_fields\n\n    # Set up the environment for torchscript\n    torch.jit.script(Instances)\n\n    # Yield the new class\n    yield Instances\n\n    # Clean up by removing the temporary file\n    import os\n\n    os.remove(\"new_instances_class.py\")\n\n    # Reset the 'Instances' class\n    Instances = Instances\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Importing required libraries\n    import numpy as np\n    from PIL import Image\n\n    # Opening the image file\n    image = Image.open(file_name)\n\n    # Checking if the image has EXIF data\n    if hasattr(image, '_getexif'):\n        # Getting the EXIF data\n        exif = image._getexif()\n        # Checking if the EXIF data contains the orientation tag\n        if exif and 274 in exif:\n            # Getting the orientation value from the EXIF data\n            orientation = exif[274]\n            # Rotating the image based on the orientation value\n            if orientation == 3:\n                image = image.rotate(180, expand=True)\n            elif orientation == 6:\n                image = image.rotate(270, expand=True)\n            elif orientation == 8:\n                image = image.rotate(90, expand=True)\n\n    # Converting the image to the specified format\n    if format == 'RGB':\n        image = image.convert('RGB')\n    elif format == 'BGR':\n        image = image.convert('RGB')\n        image = np.asarray(image)\n        image = image[:, :, ::-1]\n    elif format == 'YUV-BT.601':\n        image = image.convert('YCbCr')\n        image = np.asarray(image)\n        image = image[:, :, (0, 2, 1)]\n        image = image / 255.0\n    else:\n        image = image.convert(format)\n\n    # Converting the image to a NumPy array\n    image = np.asarray(image)\n\n    # Returning the image\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    from PIL import Image\n    import numpy as np\n\n    # Open the image using PIL\n    img = Image.open(file_name)\n\n    # Get the EXIF orientation tag, if present\n    orientation = None\n    if hasattr(img, '_getexif'):\n        exif = img._getexif()\n        if exif is not None:\n            orientation = exif.get(0x0112)\n\n    # Orient the image based on the EXIF orientation tag\n    if orientation is not None:\n        if orientation == 3:\n            img = img.rotate(180, expand=True)\n        elif orientation == 6:\n            img = img.rotate(270, expand=True)\n        elif orientation == 8:\n            img = img.rotate(90, expand=True)\n\n    # Convert the image to the specified format\n    if format is not None:\n        if format == 'BGR':\n            img = img.convert('RGB')\n            img = np.array(img)\n            img = img[:, :, ::-1]\n        elif format == 'YUV-BT.601':\n            img = img.convert('YCbCr')\n            img = np.array(img)\n            img = img[:, :, (0, 1, 2)]\n            img = img.astype(np.float32)\n            img[:, :, 0] = (img[:, :, 0] - 16) / 219\n            img[:, :, 1:] = (img[:, :, 1:] - 128) / 224\n        else:\n            img = img.convert(format)\n            img = np.array(img)\n\n    return img\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Import necessary libraries\n    from PIL import Image, ExifTags\n    import numpy as np\n\n    # Open the image using PIL\n    image = Image.open(file_name)\n\n    # Check for EXIF orientation tag\n    exif = image._getexif()\n    orientation_key = 274  # corresponds to orientation tag\n    if exif and orientation_key in exif:\n        orientation = exif[orientation_key]\n\n        # Perform the necessary orientation correction\n        rotate_values = {\n            1: [],\n            2: [Image.FLIP_LEFT_RIGHT],\n            3: [Image.ROTATE_180],\n            4: [Image.FLIP_TOP_BOTTOM],\n            5: [Image.FLIP_LEFT_RIGHT, Image.ROTATE_90],\n            6: [Image.ROTATE_270],\n            7: [Image.FLIP_TOP_BOTTOM, Image.ROTATE_90],\n            8: [Image.ROTATE_90],\n        }\n        for method in rotate_values[orientation]:\n            image = image.transpose(method)\n\n    # Convert to the specified format\n    if format == \"BGR\":\n        image = image.convert(\"RGB\")\n        image = np.array(image)\n        image = image[:, :, ::-1]  # Convert RGB to BGR\n    elif format == \"YUV-BT.601\":\n        image = image.convert(\"YCbCr\")\n        image = np.array(image)\n        image = image[:, :, (0, 2, 1)]  # Swap the Cr and Cb channels\n        image = image / 255.0  # Normalize to the 0-1 range\n    else:\n        image = image.convert(format)\n        image = np.array(image)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Importing the necessary libraries\n    from PIL import Image, ImageOps\n    import numpy as np\n\n    # Opening the image file using PIL\n    image = Image.open(file_name)\n\n    # Checking if the image has EXIF data and applying any necessary orientation corrections\n    if hasattr(image, '_getexif'):\n        exif = image._getexif()\n        if exif is not None:\n            orientation_key = 274  # cf ExifTags\n            if orientation_key in exif:\n                orientation = exif[orientation_key]\n\n                rotate_values = {\n                    3: Image.ROTATE_180,\n                    6: Image.ROTATE_270,\n                    8: Image.ROTATE_90\n                }\n\n                if orientation in rotate_values:\n                    image = image.transpose(rotate_values[orientation])\n\n    # Converting the image to the specified format\n    if format is not None:\n        if format in ['RGB', 'RGBA', 'CMYK', 'YCbCr', 'LAB', 'HSV', 'I', 'F']:\n            image = image.convert(format)\n        elif format == 'BGR':\n            image = image.convert('RGB')\n            image = np.asarray(image)\n            image = image[:, :, ::-1]\n        elif format == 'YUV-BT.601':\n            image = image.convert('YCbCr')\n            image = np.asarray(image)\n            image = np.dot(image, [[0.299, 0.587, 0.114], [-0.168736, -0.331264, 0.5], [0.5, -0.418688, -0.081312]])\n            image /= 255.\n\n    # Converting the image to a NumPy array and returning it\n    image = np.asarray(image)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    from PIL import Image\n    import numpy as np\n\n    # Open the image using PIL\n    img = Image.open(file_name)\n\n    # Check the image's EXIF data for orientation information\n    exif_orientation_tag = 0x0112\n    if hasattr(img, \"_getexif\"):\n        exif_data = img._getexif()\n        if exif_data is not None and exif_orientation_tag in exif_data:\n            orientation = exif_data[exif_orientation_tag]\n\n            # Perform the necessary orientation correction\n            if orientation == 3:\n                img = img.transpose(Image.ROTATE_180)\n            elif orientation == 6:\n                img = img.transpose(Image.ROTATE_270)\n            elif orientation == 8:\n                img = img.transpose(Image.ROTATE_90)\n\n    # Convert the image to the desired format\n    if format == \"BGR\":\n        img = img.convert(\"RGB\")\n        img_np = np.array(img)\n        img_np = img_np[:, :, ::-1]\n    elif format == \"YUV-BT.601\":\n        img = img.convert(\"YCbCr\")\n        img_np = np.array(img)\n        img_np = img_np[:, :, (0, 2, 1)]\n        img_np = img_np.astype(np.float)\n        img_np[:, :, 0] /= 255.0\n        img_np[:, :, 1:] /= 128.0\n        img_np[:, :, 1:] -= 0.5\n    else:\n        img = img.convert(format)\n        img_np = np.array(img)\n\n    return img_np\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    import numpy as np\n    from PIL import Image\n    from PIL.ExifTags import TAGS\n\n    # Open the image file\n    image = Image.open(file_name)\n\n    # Get the image's EXIF data\n    exif_data = image.getexif()\n\n    # Check if the image has EXIF data\n    if exif_data:\n        # Get the orientation tag\n        orientation = exif_data.get(0x0112)\n\n        # Check if the orientation tag exists\n        if orientation:\n            # Check the orientation value\n            if orientation == 3:\n                # Rotate 180 degrees\n                image = image.rotate(180, expand=True)\n            elif orientation == 6:\n                # Rotate 270 degrees\n                image = image.rotate(270, expand=True)\n            elif orientation == 8:\n                # Rotate 90 degrees\n                image = image.rotate(90, expand=True)\n\n    # Convert the image to the desired format\n    if format == \"BGR\":\n        # Convert the image to RGB and then to BGR\n        image = image.convert(\"RGB\")\n        image = np.array(image)[:, :, ::-1]\n    elif format == \"YUV-BT.601\":\n        # Convert the image to YUV-BT.601\n        image = image.convert(\"YCbCr\")\n        image = np.array(image)\n        image = image[:, :, (0, 2, 1)]\n        image = image.astype(np.float32)\n        image[:, :, 0] = (image[:, :, 0] - 16) / 219\n        image[:, :, 1:] = (image[:, :, 1:] - 128) / 224\n    else:\n        # Convert the image to the specified format\n        image = image.convert(format)\n        image = np.array(image)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Importing the necessary libraries\n    from PIL import Image, ImageOps\n    import numpy as np\n\n    # Opening the image using PIL\n    image = Image.open(file_name)\n\n    # Checking if the image has an EXIF orientation tag\n    if hasattr(image, '_getexif'):\n        exif = image._getexif()\n        if exif is not None:\n            orientation = exif.get(274, None)\n\n            # Applying the necessary orientation correction\n            if orientation == 1:\n                # No orientation correction needed\n                pass\n            elif orientation == 2:\n                # Flip left-to-right\n                image = ImageOps.mirror(image)\n            elif orientation == 3:\n                # Rotate 180 degrees\n                image = image.rotate(180)\n            elif orientation == 4:\n                # Flip top-to-bottom\n                image = ImageOps.flip(image)\n            elif orientation == 5:\n                # Transpose\n                image = image.rotate(-90, expand=True)\n                image = ImageOps.mirror(image)\n            elif orientation == 6:\n                # Rotate 90 degrees\n                image = image.rotate(-90, expand=True)\n            elif orientation == 7:\n                # Transverse\n                image = image.rotate(90, expand=True)\n                image = ImageOps.mirror(image)\n            elif orientation == 8:\n                # Rotate 270 degrees\n                image = image.rotate(90, expand=True)\n\n    # Converting the image to the desired format\n    if format == \"BGR\":\n        # Convert the image to RGB and then to BGR\n        image = image.convert(\"RGB\")\n        image = np.array(image)\n        image = image[:, :, ::-1]\n    elif format == \"YUV-BT.601\":\n        # Convert the image to YUV-BT.601\n        image = image.convert(\"YCbCr\")\n        image = np.array(image)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Importing necessary libraries\n    from PIL import Image, ExifTags\n    import numpy as np\n\n    # Opening the image file\n    image = Image.open(file_name)\n\n    # Getting the EXIF data from the image\n    exif_data = image.getexif()\n\n    # Checking if the EXIF data contains the orientation tag\n    if exif_data and 274 in exif_data:\n        # Getting the orientation value from the EXIF data\n        orientation = exif_data[274]\n\n        # Rotating the image based on the orientation value\n        if orientation == 3:\n            image = image.rotate(180, expand=True)\n        elif orientation == 6:\n            image = image.rotate(270, expand=True)\n        elif orientation == 8:\n            image = image.rotate(90, expand=True)\n\n    # Converting the image to the specified format\n    if format == \"BGR\":\n        # Converting the image to RGB format\n        image = image.convert(\"RGB\")\n\n        # Converting the image to a numpy array\n        image = np.array(image)\n\n        # Reordering the color channels to BGR\n        image = image[:, :, ::-1]\n\n    elif format == \"YUV-BT.601\":\n        # Converting the image to YUV format\n        image = image.convert(\"YCbCr\")\n\n        # Converting the image to a numpy array\n        image = np.array(image)\n\n        # Normalizing the pixel values to the range of 0-1\n        image = image / 255.0\n\n        # Reordering the color channels to YUV\n        image = image[:, :, (0, 2, 1)]\n\n    elif format is not None:\n        # Converting the image to the specified format\n        image = image.convert(format)\n\n        # Converting the image to a numpy array\n        image = np.array(image)\n\n    else:\n        # Converting the image to a numpy array\n       "}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    import numpy as np\n    from PIL import Image\n\n    # Read the image using PIL\n    image = Image.open(file_name)\n\n    # Get the image's EXIF data\n    exif_data = image._getexif()\n\n    # Check if the image has EXIF data\n    if exif_data is not None:\n        # Get the orientation value from the EXIF data\n        orientation = exif_data.get(274)\n\n        # Apply the necessary orientation correction\n        if orientation == 3:\n            image = image.rotate(180, expand=True)\n        elif orientation == 6:\n            image = image.rotate(270, expand=True)\n        elif orientation == 8:\n            image = image.rotate(90, expand=True)\n\n    # Convert the image to the specified format\n    if format == \"BGR\":\n        image = image.convert(\"RGB\")\n        image = np.array(image)\n        image = image[:, :, ::-1]\n    elif format == \"YUV-BT.601\":\n        image = image.convert(\"YCbCr\")\n        image = np.array(image)\n        image = image[:, :, ::-1]\n        image = image / 255.0\n    else:\n        image = image.convert(format)\n        image = np.array(image)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Import the necessary libraries\n    from PIL import Image, ExifTags\n    import numpy as np\n\n    # Open the image file\n    image = Image.open(file_name)\n\n    # Get the image's EXIF data\n    exif_data = image.getexif()\n\n    # Check if the EXIF data contains the orientation tag\n    if exif_data and exif_data.get(274):\n        # Get the orientation value\n        orientation = exif_data.get(274)\n\n        # Perform the necessary orientation correction\n        if orientation == 3:\n            image = image.rotate(180, expand=True)\n        elif orientation == 6:\n            image = image.rotate(270, expand=True)\n        elif orientation == 8:\n            image = image.rotate(90, expand=True)\n\n    # Convert the image to the specified format\n    if format == \"BGR\":\n        image = image.convert(\"RGB\")\n        image = np.array(image)\n        image = image[:, :, ::-1]\n    elif format == \"YUV-BT.601\":\n        image = image.convert(\"YCbCr\")\n        image = np.array(image)\n        image = image.astype(np.float)\n        image[:, :, 0] = (image[:, :, 0] - 16) / 219\n        image[:, :, 1] = (image[:, :, 1] - 128) / 224\n        image[:, :, 2] = (image[:, :, 2] - 128) / 224\n    elif format in ExifTags.TAGS.keys():\n        image = image.convert(format)\n        image = np.array(image)\n    else:\n        image = image.convert(image.mode)\n        image = np.array(image)\n\n    # Return the image\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Import necessary libraries\n    import numpy as np\n    from PIL import Image\n    import piexif\n\n    # Open the image file and extract its EXIF data\n    im = Image.open(file_name)\n    exif_dict = piexif.load(im.info[\"exif\"])\n\n    # Check if EXIF data exists and if it contains orientation information\n    if \"0th\" in exif_dict and piexif.ImageIFD.Orientation in exif_dict[\"0th\"]:\n        orientation = exif_dict[\"0th\"].pop(piexif.ImageIFD.Orientation)\n        if orientation == 2:\n            im = im.transpose(Image.FLIP_LEFT_RIGHT)\n        elif orientation == 3:\n            im = im.rotate(180)\n        elif orientation == 4:\n            im = im.rotate(180).transpose(Image.FLIP_LEFT_RIGHT)\n        elif orientation == 5:\n            im = im.rotate(-90, expand=True).transpose(Image.FLIP_LEFT_RIGHT)\n        elif orientation == 6:\n            im = im.rotate(-90, expand=True)\n        elif orientation == 7:\n            im = im.rotate(90, expand=True).transpose(Image.FLIP_LEFT_RIGHT)\n        elif orientation == 8:\n            im = im.rotate(90, expand=True)\n\n    # Convert the image to the specified format\n    if format == \"BGR\":\n        im = im.convert(\"RGB\")\n        im = np.array(im)\n    elif format == \"YUV-BT.601\":\n        im = im.convert(\"YCbCr\")\n        im = np.array(im)\n        im = im[:, :, (0, 2, 1)].astype(np.float)\n        im[:, :, 0] /= 255.0\n        im[:, :, 1:] /= 128.0\n        im[:, :, 1:] -= "}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    import PIL.Image\n    import numpy as np\n    import PIL.ExifTags\n\n    # Open the image file\n    image = PIL.Image.open(file_name)\n\n    # Check if the image has EXIF data\n    if hasattr(image, '_getexif'):\n        exif = image._getexif()\n        if exif is not None:\n            # Get the orientation value from the EXIF data\n            orientation = exif.get(PIL.ExifTags.TAGS.get('Orientation'))\n            # Perform orientation correction if necessary\n            if orientation == 3:\n                image = image.rotate(180, expand=True)\n            elif orientation == 6:\n                image = image.rotate(270, expand=True)\n            elif orientation == 8:\n                image = image.rotate(90, expand=True)\n\n    # Convert the image to the specified format\n    if format is None:\n        # Use the original format if no format is specified\n        pass\n    elif format == \"BGR\":\n        # Convert the image to BGR format\n        image = image.convert('RGB')\n        image = np.array(image)\n        image = image[:, :, ::-1].copy()\n    elif format == \"YUV-BT.601\":\n        # Convert the image to YUV-BT.601 format\n        image = image.convert('YCbCr')\n        image = np.array(image)\n        image = image[:, :, (0, 2, 1)].copy()\n        image = image.astype(np.float)\n        image[:, :, 0] /= 255.0\n        image[:, :, 1:] -= 128.0\n        image[:, :, 1:] /= 224.0\n    else:\n        # Convert the image to the specified format\n        image = image.convert(format)\n        image = np.array(image)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Importing necessary modules\n    import numpy as np\n    from PIL import Image\n    from PIL.ExifTags import TAGS\n\n    # Opening the image file\n    image = Image.open(file_name)\n\n    # Extracting EXIF data from the image\n    exif = image.getexif()\n\n    # Creating a dictionary to store the orientation tag value\n    orientation = {\n        TAGS[k]: v\n        for k, v in exif.items()\n        if k in TAGS and TAGS[k] == \"Orientation\"\n    }\n\n    # Checking if the orientation tag is present in the EXIF data\n    if orientation:\n        # Getting the value of the orientation tag\n        value = orientation[\"Orientation\"]\n\n        # Checking the value of the orientation tag and rotating the image accordingly\n        if value == 3:\n            image = image.rotate(180, expand=True)\n        elif value == 6:\n            image = image.rotate(270, expand=True)\n        elif value == 8:\n            image = image.rotate(90, expand=True)\n\n    # Converting the image to the specified format\n    if format == \"BGR\":\n        image = image.convert(\"RGB\")\n        image = np.asarray(image).astype(np.float32)\n        image = image[:, :, ::-1]\n    elif format == \"YUV-BT.601\":\n        image = image.convert(\"YCbCr\")\n        image = np.asarray(image).astype(np.float32)\n        image = image[:, :, (0, 2, 1)]\n        image[:, :, 0] *= 1.0 / 255.0\n        image[:, :, 1:] *= 1.0 / 128.0\n    else:\n        image = image.convert(format)\n        image = np.asarray(image).astype(np.float32)\n\n    # Returning the image\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Importing the required libraries\n    import cv2\n    import numpy as np\n    from PIL import Image\n\n    # Reading the image\n    image = cv2.imread(file_name)\n\n    # Getting the EXIF data from the image\n    exif_data = image.getexif()\n\n    # Checking if the EXIF data contains the orientation tag\n    if exif_data and exif_data.get(274):\n\n        # Getting the orientation value from the EXIF data\n        orientation = exif_data.get(274)\n\n        # Applying the orientation correction if necessary\n        if orientation == 3:\n            image = cv2.rotate(image, cv2.ROTATE_180)\n        elif orientation == 6:\n            image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n        elif orientation == 8:\n            image = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n\n    # Checking if the format is specified and converting the image to the specified format\n    if format:\n\n        # Checking if the format is supported by PIL\n        if format in Image.MODES:\n\n            # Converting the image to the specified format\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = Image.fromarray(image)\n            image = image.convert(format)\n            image = np.array(image)\n\n        # Checking if the format is \"BGR\"\n        elif format == \"BGR\":\n\n            # Converting the image to \"BGR\" format\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Checking if the format is \"YUV-BT.601\"\n        elif format == \"YUV-BT.601\":\n\n            # Converting the image to \"YUV-BT.601\" format\n            image = cv2.cv"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    from PIL import Image\n    import numpy as np\n\n    # Read the image\n    img = Image.open(file_name)\n\n    # Check if the image has EXIF data\n    if hasattr(img, '_getexif'):\n        exif = img._getexif()\n        if exif is not None:\n            # Get the orientation value\n            orientation = exif.get(274)\n            if orientation is not None:\n                # Orient the image based on the EXIF data\n                if orientation == 3:\n                    img = img.rotate(180, expand=True)\n                elif orientation == 6:\n                    img = img.rotate(270, expand=True)\n                elif orientation == 8:\n                    img = img.rotate(90, expand=True)\n\n    # Convert the image to the specified format\n    if format is None:\n        # Use the original format if no format is specified\n        img_array = np.array(img)\n    elif format == 'BGR':\n        # Convert to BGR format\n        img_array = np.array(img.convert('RGB'))[:, :, ::-1]\n    elif format == 'YUV-BT.601':\n        # Convert to YUV-BT.601 format\n        img_array = np.array(img.convert('YCbCr'))\n        img_array = img_array[:, :, (0, 2, 1)]\n        img_array = img_array / 255.0\n    else:\n        # Convert to the specified format\n        img_array = np.array(img.convert(format))\n\n    return img_array\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Importing necessary modules\n    import numpy as np\n    from PIL import Image\n    import exifread\n\n    # Opening the image file using PIL\n    image = Image.open(file_name)\n\n    # Extracting EXIF data from the image\n    exif_data = exifread.process_file(image)\n\n    # Checking if EXIF data exists and if it does, extracting the orientation value\n    if 'Image Orientation' in exif_data:\n        orientation = exif_data['Image Orientation'].values[0]\n    else:\n        orientation = 1\n\n    # Rotating the image based on the orientation value\n    if orientation == 3:\n        image = image.rotate(180, expand=True)\n    elif orientation == 6:\n        image = image.rotate(270, expand=True)\n    elif orientation == 8:\n        image = image.rotate(90, expand=True)\n\n    # Converting the image to the desired format\n    if format is not None:\n        if format == 'BGR':\n            image = image.convert('RGB')\n            image = np.array(image)\n            image = image[:, :, ::-1].copy()\n        elif format == 'YUV-BT.601':\n            image = image.convert('YCbCr')\n            image = np.array(image)\n            image = image[:, :, (0, 2, 1)].copy()\n            image = image.astype(np.float32)\n            image[:, :, 0] = (image[:, :, 0] - 16.0) / 255.0\n            image[:, :, 1:] = (image[:, :, 1:] - 128.0) / 255.0\n        else:\n            image = image.convert(format)\n            image = np.array(image)\n\n    # Returning the image as a NumPy array\n    return image\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Importing the required libraries\n    from PIL import Image, ExifTags\n    import numpy as np\n\n    # Opening the image file using PIL\n    img = Image.open(file_name)\n\n    # Extracting the EXIF data from the image\n    exif = img.getexif()\n\n    # Checking if the EXIF data contains the orientation tag\n    if exif and 274 in exif:\n\n        # Getting the orientation value from the EXIF data\n        orientation = exif[274]\n\n        # Rotating the image based on the orientation value\n        if orientation == 3:\n            img = img.rotate(180, expand=True)\n        elif orientation == 6:\n            img = img.rotate(270, expand=True)\n        elif orientation == 8:\n            img = img.rotate(90, expand=True)\n\n    # Converting the image to the desired format\n    if format == 'BGR':\n        img = img.convert('RGB')\n        img = np.array(img)\n    elif format == 'YUV-BT.601':\n        img = img.convert('YCbCr')\n        img = np.array(img)\n        img = img[:, :, (0, 2, 1)]  # Swapping the Cb and Cr channels\n        img = img / 255.0  # Normalizing the pixel values to the range 0-1\n    else:\n        img = img.convert(format)\n        img = np.array(img)\n\n    # Returning the image as a NumPy array\n    return img\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    import PIL.Image\n    import numpy as np\n\n    # Open the image using PIL\n    image = PIL.Image.open(file_name)\n\n    # Orient the image based on the EXIF data\n    image = PIL.ImageOps.exif_transpose(image)\n\n    # Convert the image to the desired format\n    if format is None:\n        # Use the original format if not specified\n        pass\n    elif format == \"BGR\":\n        # Convert the image to RGB and then to BGR\n        image = image.convert(\"RGB\")\n        image = np.array(image)\n        image = image[:, :, ::-1]\n    elif format == \"YUV-BT.601\":\n        # Convert the image to YUV-BT.601\n        image = image.convert(\"YCbCr\")\n        image = np.array(image)\n        image = image[:, :, [0, 2, 1]]\n        image = image.astype(np.float32)\n        image[:, :, 0] /= 255.0\n        image[:, :, 1:] /= 255.0 - 128.0\n    else:\n        # Convert the image to the specified format\n        image = image.convert(format)\n        image = np.array(image)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Importing necessary libraries\n    import numpy as np\n    from PIL import Image, ImageOps\n\n    # Opening the image using PIL\n    image = Image.open(file_name)\n\n    # Converting to the desired format\n    if format is None:\n        pass\n    elif format == \"BGR\":\n        if image.mode == \"RGBA\":\n            image = image.convert(\"RGB\")\n        elif image.mode == \"RGB\":\n            pass\n        elif image.mode == \"L\":\n            image = ImageOps.colorize(image, \"black\", \"white\")\n        else:\n            raise ValueError(\"Unsupported PIL color mode for BGR conversion.\")\n    elif format == \"YUV-BT.601\":\n        if image.mode == \"RGBA\":\n            image = image.convert(\"RGB\")\n        elif image.mode == \"RGB\":\n            pass\n        elif image.mode == \"L\":\n            image = ImageOps.colorize(image, \"black\", \"white\")\n        else:\n            raise ValueError(\"Unsupported PIL color mode for YUV conversion.\")\n        image = np.array(image).astype(np.float)\n        image[:, :, 0] = (image[:, :, 0] - 16) * (235 / 219)\n        image[:, :, 1] = image[:, :, 1] - 128\n        image[:, :, 2] = (image[:, :, 2] - 16) * (240 / 224)\n        image = image.astype(np.uint8)\n    else:\n        image = image.convert(format)\n\n    # Returning the image data as a NumPy array\n    return np.array(image)\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Import necessary libraries\n    import numpy as np\n    import cv2\n    from PIL import Image\n\n    # Read the image file using OpenCV\n    image = cv2.imread(file_name)\n\n    # Check if the image was successfully read\n    if image is None:\n        raise ValueError(\"Failed to read the image file.\")\n\n    # Get the orientation value from the EXIF data\n    orientation = cv2.IMREAD_UNCHANGED\n\n    # Apply orientation correction if necessary\n    if orientation is not None:\n        if orientation == 3:\n            image = cv2.rotate(image, cv2.ROTATE_180)\n        elif orientation == 6:\n            image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n        elif orientation == 8:\n            image = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n\n    # Convert the image to the specified format if necessary\n    if format is not None:\n        if format == \"YUV-BT.601\":\n            # Convert the image to YUV format\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n            # Scale the Y channel to the range of 0-1\n            image[:, :, 0] = image[:, :, 0] / 255.0\n        elif format == \"BGR\":\n            # Convert the image to BGR format\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        else:\n            # Convert the image to the specified format\n            image = Image.fromarray(image)\n            image = image.convert(format)\n            image = np.array(image)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation = polygon\n    segmentation = annotation[\"segmentation\"]\n    if isinstance(segmentation, list):\n        # polygons\n        polygons = [np.asarray(p).reshape(-1, 2) for p in segmentation]\n        # transform polygons\n        polygons = transforms.apply_polygons(polygons)\n        # clip polygons to image size\n        polygons = [p.clip(min=0) for p in polygons]\n        # round to integer (pixel) coordinates\n        polygons = [np.round(p).flatten().astype(np.int32) for p in polygons]\n        annotation[\"segmentation\"] = polygons\n    else:\n        # RLE\n        mask = mask_util.decode(segmentation)\n        # transform mask RLE\n        mask = transforms.apply_segmentation(mask)\n        # transform mask back to RLE\n        mask = mask_util.encode(np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\"))[0]\n        annotation[\"segmentation\"] = mask\n\n    # keypoints\n    keypoints = np.asarray(annotation[\"keypoints\"]).reshape(-1, 3)\n    keypoints[:, :2] = transforms.apply_coords(keypoints[:, :2])\n    keypoints[:, :2] = keypoints[:, :2].clip(min=0)\n    annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform polygon\n    segmentation = annotation[\"segmentation\"]\n    if isinstance(segmentation, list):\n        # Polygon\n        polygons = [np.asarray(p).reshape(-1, 2) for p in segmentation]\n        annotation[\"segmentation\"] = [\n            p.reshape(-1) for p in transforms.apply_polygons(polygons)\n        ]\n    else:\n        # RLE\n        mask = mask_util.decode(segmentation)\n        count, transformed_mask = transforms.apply_segmentation(mask)\n        assert (\n            tuple(transformed_mask.shape[:2]) == image_size\n        ), transformed_mask.shape\n        annotation[\"segmentation\"] = mask_util.encode(\n            np.asarray(transformed_mask, order=\"F\", dtype=\"uint8\")\n        )\n\n    # Transform keypoints\n    keypoints = utils.transform_keypoint_annotations(\n        annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n    )\n    annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (list, tuple)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation = polygon\n    segmentation = annotation[\"segmentation\"]\n    if isinstance(segmentation, list):\n        # polygons\n        polygons = [np.asarray(p).reshape(-1, 2) for p in segmentation]\n        # transform polygons\n        polygons = transforms.apply_polygons(polygons)\n        # clip to image size\n        polygons = [p.clip(min=0) for p in polygons]\n        # round and make np.int64\n        polygons = [polygons_.round().astype(np.int64) for polygons_ in polygons]\n        annotation[\"segmentation\"] = polygons\n    else:\n        # RLE\n        mask = mask_util.decode(segmentation)\n        count, transform = transforms.apply_segmentation(mask)\n        assert tuple(mask.shape[:2]) == tuple(transform.shape[:2])\n        assert mask.dtype == np.uint8\n        segmentation = mask_util.encode(np.asarray(transform, order=\"F\", dtype=np.uint8))\n        annotation[\"segmentation\"] = segmentation\n\n    # keypoints\n    keypoints = transform_keypoint_annotations(\n        annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n    )\n    annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation = polygon\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon -> list[list[float]]\n            polygons = [np.asarray(p).reshape(-1, 2) for p in segm]\n            annotation[\"segmentation\"] = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n        else:\n            # uncompressed RLE\n            annotation[\"segmentation\"] = transforms.apply_segmentation(segm)\n    # keypoints\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation = polygon\n    segmentation = annotation[\"segmentation\"]\n    if isinstance(segmentation, list):\n        # polygons\n        polygons = [np.asarray(p).reshape(-1, 2) for p in segmentation]\n        annotation[\"segmentation\"] = [\n            p.reshape(-1) for p in transforms.apply_polygons(polygons)\n        ]\n    else:\n        # RLE\n        mask = mask_util.decode(segmentation)\n        mask = transforms.apply_segmentation(mask)\n        annotation[\"segmentation\"] = mask_util.encode(\n            np.asarray(mask, order=\"F\", dtype=\"uint8\")\n        )\n\n    # keypoints\n    keypoints = transform_keypoint_annotations(\n        annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n    )\n    annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation = polygon\n    segmentation = annotation[\"segmentation\"]\n    if isinstance(segmentation, list):\n        # polygons\n        polygons = [np.asarray(p).reshape(-1, 2) for p in segmentation]\n        # transform polygons\n        polygons = transforms.apply_polygons(polygons)\n        # clip polygons to image size\n        polygons = [p.clip(min=0) for p in polygons]\n        # round to integer (pixel) values\n        polygons = [np.round(p).flatten().astype(\"int32\") for p in polygons]\n        annotation[\"segmentation\"] = polygons\n    else:\n        # RLE\n        mask = mask_util.decode(segmentation)\n        # transform mask\n        mask = transforms.apply_segmentation(mask)\n        # clip mask to image size\n        mask = mask[: image_size[0], : image_size[1]]\n        rle = mask_util.encode(np.array(mask, order=\"F\", dtype=\"uint8\"))\n        annotation[\"segmentation\"] = rle\n\n    # keypoints\n    keypoints = transforms.apply_coords(annotation[\"keypoints\"])\n    annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (list, tuple)):\n        transforms = TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform polygon and RLE masks\n    segmentation = annotation[\"segmentation\"]\n    if isinstance(segmentation, list):\n        # Polygon\n        segmentation = [\n            transforms.apply_segmentation(obj) for obj in segmentation\n        ]\n    elif isinstance(segmentation, dict):\n        # RLE\n        segmentation = {\n            k: transforms.apply_segmentation(v) for k, v in segmentation.items()\n        }\n    else:\n        raise ValueError(\n            \"Cannot transform segmentation of type '{}'\".format(\n                type(segmentation)\n            )\n        )\n    annotation[\"segmentation\"] = segmentation\n\n    # Transform keypoints\n    keypoints = annotation[\"keypoints\"]\n    keypoints = transforms.apply_coords(keypoints)\n    annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (list, tuple)):\n        transforms = T.TransformList(transforms)\n\n    # Transform bounding box\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox\"] = bbox\n\n    # Transform segmentation\n    segmentation = annotation[\"segmentation\"]\n    if isinstance(segmentation, list):\n        # Polygon\n        segmentation = [\n            transforms.apply_segmentation(obj) for obj in segmentation\n        ]\n    else:\n        # RLE\n        segmentation = transforms.apply_segmentation(segmentation)\n    annotation[\"segmentation\"] = segmentation\n\n    # Transform keypoints\n    keypoints = annotation[\"keypoints\"]\n    keypoints = transforms.apply_coords(keypoints)\n    annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation = polygon\n    segmentation = annotation[\"segmentation\"]\n    if isinstance(segmentation, list):\n        # polygons\n        polygons = [np.asarray(p).reshape(-1, 2) for p in segmentation]\n        # transform polygons\n        polygons = transforms.apply_polygons(polygons)\n        # clip to image size\n        polygons = [p.clip(min=0) for p in polygons]\n        # round coordinates and make counterclockwise if needed\n        polygons = [\n            np.round(p).astype(np.float32) for p in polygons\n        ]\n        annotation[\"segmentation\"] = polygons\n    else:\n        # RLE\n        mask = mask_util.decode(segmentation)\n        # transform mask RLE\n        mask = transforms.apply_segmentation(mask)\n        # clip to image size\n        mask = mask[: image_size[0], : image_size[1]]\n        rle = mask_util.encode(np.array(mask, order=\"F\", dtype=\"uint8\"))\n        annotation[\"segmentation\"] = rle\n\n    # keypoints\n    keypoints = annotation[\"keypoints\"]\n    keypoints = transforms.apply_coords(keypoints)\n    annotation[\"keypoints\"] = keypoints\n\n    return annotation\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform polygon or RLE masks\n    segmentation = annotation[\"segmentation\"]\n    if isinstance(segmentation, list):\n        # Polygon\n        segmentation = [\n            transforms.apply_segmentation(obj) for obj in segmentation\n        ]\n    else:\n        # RLE\n        segmentation = transforms.apply_segmentation(segmentation)\n    annotation[\"segmentation\"] = segmentation\n\n    # Transform keypoints\n    keypoints = annotation[\"keypoints\"]\n    keypoints = transforms.apply_coords(keypoints)\n    annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (list, tuple)):\n        transforms = T.TransformList(transforms)\n\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox\"] = bbox\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            segmentation = annotation[\"segmentation\"]\n            annotation[\"segmentation\"] = [\n                transforms.apply_segmentation(obj) for obj in segmentation\n            ]\n        else:\n            annotation[\"segmentation\"] = transforms.apply_segmentation(\n                annotation[\"segmentation\"]\n            )\n\n    if \"keypoints\" in annotation:\n        keypoints = transforms.apply_coords(annotation[\"keypoints\"])\n        annotation[\"keypoints\"] = keypoints\n\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (list, tuple)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation = polygons\n    segmentation = annotation[\"segmentation\"]\n    if isinstance(segmentation, list):\n        # polygons\n        polygons = [np.asarray(p).reshape(-1, 2) for p in segmentation]\n        # transform polygons\n        polygons = transforms.apply_polygons(polygons)\n        # clip polygons to image size\n        polygons = [p.clip(min=0) for p in polygons]\n        # round and make counterclockwise if all points outside\n        polygons = [\n            p if len(p) < 3 else make_polygon(p) if Polygon(p).is_valid else []\n            for p in polygons\n        ]\n        annotation[\"segmentation\"] = polygons\n    else:\n        # RLE\n        mask = mask_util.decode(segmentation)\n        mask = transforms.apply_segmentation(mask)\n        assert tuple(mask.shape[:2]) == image_size\n        annotation[\"segmentation\"] = mask\n\n    # keypoints\n    keypoints = annotation[\"keypoints\"]\n    if isinstance(keypoints, list):\n        keypoints = transforms.apply_coords(\n            [np.asarray(keypoints).reshape(-1, 3)]\n        )[0].reshape(-1)\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (list, tuple)):\n        transforms = T.TransformList(transforms)\n\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox\"] = bbox\n\n    if \"segmentation\" in annotation:\n        segmentation = annotation[\"segmentation\"]\n        if isinstance(segmentation, list):\n            segmentation = PolygonMasks([np.asarray(p).reshape(-1, 2) for p in segmentation], image_size)\n        elif isinstance(segmentation, dict):\n            segmentation = RLEMasks(segmentation, image_size)\n        else:\n            raise ValueError(f\"Cannot transform segmentation of type '{type(segmentation)}'\")\n\n        segmentation = transforms.apply_segmentation(segmentation)\n        annotation[\"segmentation\"] = segmentation.polygons if isinstance(segmentation, PolygonMasks) else segmentation.rle\n\n    if \"keypoints\" in annotation:\n        keypoints = transforms.apply_coords(annotation[\"keypoints\"].reshape(-1, 3))\n        annotation[\"keypoints\"] = keypoints[..., :2].reshape(-1)\n\n        if keypoint_hflip_indices is not None:\n            keypoints = keypoints[:, keypoint_hflip_indices, :]\n            keypoints = keypoints[:, :, ::-1]\n            annotation[\"keypoints\"] = keypoints.reshape(-1)\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (list, tuple)):\n        transforms = TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation = polygon\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        # either list[list[float]] or list[dict]\n        if isinstance(segm, list):\n            # polygons\n            polygons = [np.asarray(p).reshape(-1, 2) for p in segm]\n            annotation[\"segmentation\"] = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n        else:\n            # RLE\n            mask = mask_util.decode(segm)\n            mask = transforms.apply_segmentation(mask)\n            annotation[\"segmentation\"] = mask_util.encode(\n                np.asarray(mask, order=\"F\", dtype=\"uint8\")\n            )\n    # keypoints\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation = polygons\n    if \"segmentation\" in annotation:\n        segmentation = annotation[\"segmentation\"]\n        # each segment is a list of float\n        if isinstance(segmentation, list):\n            # one segment per instance, unflatten the list\n            segmentation = [x.reshape(-1, 2) for x in segmentation]\n        else:\n            # masks or RLE\n            segmentation = [segmentation]\n        # filter empty segments\n        segmentation = [x for x in segmentation if len(x) > 0]\n        valid_segmentation = []\n        for segment in segmentation:\n            # filter out invalid polygons (< 3 points)\n            if len(segment) < 3:\n                continue\n            # transform polygon\n            segment = transforms.apply_segmentation(segment)\n            # filter out degenerated polygons\n            if len(segment) < 3:\n                continue\n            valid_segmentation.append(segment)\n        if valid_segmentation:\n            annotation[\"segmentation\"] = valid_segmentation\n        else:\n            # empty list of polygons\n            annotation.pop(\"segmentation\", None)\n\n    # keypoints\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation = polygon\n    if \"segmentation\" in annotation:\n        segmentation = annotation[\"segmentation\"]\n        # each segment is a list[float]\n        if isinstance(segmentation, list):\n            # one path to rule them all\n            # NOTE the segment is a 0-indexed array\n            segmentation = [x.reshape(-1, 2) for x in segmentation]\n            annotation[\"segmentation\"] = [\n                x.reshape(-1) for x in transforms.apply_polygons(segmentation)\n            ]\n        # segment is a dict with coordinates\n        elif isinstance(segmentation, dict):\n            segmentation = [segmentation]\n            annotation[\"segmentation\"] = transforms.apply_coords(segmentation)\n        # segment is run-length encoding (RLE)\n        elif isinstance(segmentation, RLE):\n            segmentation = [segmentation]\n            annotation[\"segmentation\"] = transforms.apply_segmentation(segmentation)\n        else:\n            raise ValueError(\n                f\"Cannot transform segmentation of type '{type(segmentation)}'!\"\n            )\n\n    # keypoints\n    if \"keypoints\" in annotation:\n        keypoints = transforms.apply_coords(\n            [annotation[\"keypoints\"].reshape(-1, 3)]\n        )[0].reshape(-1, 3)\n        annotation[\"keypoints\"] = keypoints\n        # Horizont"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (list, tuple)):\n        transforms = TransformList(transforms)\n\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform polygon or uncompressed RLE masks\n    segmentation = annotation[\"segmentation\"]\n    if isinstance(segmentation, list):\n        # polygon\n        if len(segmentation) == 0:\n            annotation[\"segmentation\"] = segmentation\n        else:\n            annotation[\"segmentation\"] = transforms.apply_polygons(\n                [BoxMode.convert(segmentation, BoxMode.XYWHA_ABS, BoxMode.XYXY_ABS)]\n            )[0]\n\n    # Transform keypoints\n    keypoints = annotation[\"keypoints\"]\n    keypoints = transforms.apply_coords(keypoints)\n    annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform polygon or mask\n    segmentation = annotation[\"segmentation\"]\n    if isinstance(segmentation, list):\n        # Polygon format\n        if len(segmentation) == 0:\n            transformed_segmentation = []\n        else:\n            # transform polygon\n            transformed_segmentation = transforms.apply_polygons(segmentation)\n    else:\n        # RLE format\n        mask = mask_util.decode(segmentation)\n        # transform RLE\n        mask = transforms.apply_segmentation(mask)\n        transformed_segmentation = mask_util.encode(\n            np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\")\n        )[0]\n    annotation[\"segmentation\"] = transformed_segmentation\n\n    # Transform keypoints\n    keypoints = np.asarray(annotation[\"keypoints\"]).reshape(-1, 3)\n    keypoints[..., :2] = transforms.apply_coords(keypoints[..., :2])\n    keypoints[..., :2] = keypoints[..., :2].clip(min=0)\n    annotation[\"keypoints\"] = keypoints.flatten().tolist()\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation = polygon\n    if \"segmentation\" in annotation:\n        segmentation = annotation[\"segmentation\"]\n        # each segment is a list[float]\n        if isinstance(segmentation, list):\n            # PolygonMasks are list[list[float]]\n            if not isinstance(segmentation[0], list):\n                polygons = [segmentation]\n            else:\n                polygons = segmentation\n        else:\n            polygons = [segmentation]\n        # transform polygons w.r.t. the transformations\n        new_segmentation = []\n        for polygon in polygons:\n            # Uses transforms.apply_polygons to transform the polygon\n            new_poly = transforms.apply_polygons(polygon)\n            # This is a list[list[float]]\n            new_segmentation.append(new_poly)\n\n        annotation[\"segmentation\"] = new_segmentation\n\n    # keypoints = list[int]\n    if \"keypoints\" in annotation:\n        keypoints = transforms.apply_coords(annotation[\"keypoints\"])\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (list, tuple)):\n        transforms = TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation = polygon\n    if \"segmentation\" in annotation:\n        segmentation = annotation[\"segmentation\"]\n        # each segment is a list[float]\n        if isinstance(segmentation, list):\n            # one path to rule them all\n            # NOTE the nested list[list[float]]\n            segmentation = [\n                # each segment is a list[float]\n                segment.clip(min=0)\n                for segment in transforms.apply_segmentation(segmentation)\n            ]\n            # filter out irrelevant polygons\n            annotation[\"segmentation\"] = [\n                segment\n                for segment in segmentation\n                if len(segment) >= 3\n                # `np.isfinite` is needed to filter out cases\n                # where a narrow polygon would be split into two infinite parts\n                and np.all(np.isfinite(segment))\n                and not np.allclose(segment[0], segment[1])\n            ]\n        # segment is RLE\n        else:\n            annotation[\"segmentation\"] = transforms.apply_segmentation(\n                segmentation,\n                transforms.apply_box([bbox])[0],\n            )\n\n    # keypoints\n    if \"keypoints\" in annotation:\n        keypoints = transforms.apply_coords(annotation[\"keypoints\"])\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if self.angle % 360 == 0:\n            return coords\n\n        if len(coords) == 0:\n            return coords\n\n        return self.rm_coords(coords)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if self.angle % 360 == 0:\n            return coords\n        else:\n            return np.dot(coords, self.rm_coords)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0:\n            return coords\n        if self.angle % 360 == 0:\n            return coords\n        return coords @ self.rm_coords\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if coords.shape[0] == 0:\n            return coords\n        elif self.angle % 360 == 0:\n            return coords\n        else:\n            return np.dot(coords, self.rm_coords.T)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if coords.shape[0] == 0 or self.angle % 360 == 0:\n            return coords\n        else:\n            return np.dot(coords, self.rm_coords)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if coords is None or coords.shape[0] == 0:\n            return coords\n\n        if self.angle % 360 == 0:\n            return coords\n\n        return np.dot(coords, self.rm_coords)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if not coords.any() or self.angle % 360 == 0:\n            return coords\n\n        return np.dot(coords, self.rm_coords)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0:\n            return coords\n\n        if self.angle % 360 == 0:\n            return coords\n\n        return np.dot(coords, self.rm_coords.T)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        # Check if the input coordinates are empty or if the rotation angle is a multiple of 360 degrees\n        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n\n        # Apply the rotation transformation to the input coordinates using the rotation matrix\n        coords = np.dot(coords, self.rm_coords)\n\n        # Return the transformed coordinates\n        return coords\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if self.angle % 360 == 0:\n            return coords\n        if len(coords) == 0:\n            return coords\n        return np.dot(coords, self.rm_coords)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0:\n            return coords\n        if self.angle % 360 == 0:\n            return coords\n\n        return np.dot(coords, self.rm_coords)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if not coords:\n            return coords\n        if self.angle % 360 == 0:\n            return coords\n        return np.dot(coords, self.rm_coords)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        # Check if the input array is empty or if the rotation angle is a multiple of 360 degrees\n        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n\n        # Apply the rotation transformation to each coordinate in the input array\n        coords = np.dot(coords, self.rm_coords)\n\n        # Return the transformed coordinates\n        return coords\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        else:\n            return coords @ self.rm_coords.T\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if self.angle % 360 == 0:\n            return coords\n\n        return np.dot(coords, self.rm_coords)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if coords.shape[0] == 0:\n            return coords\n\n        if self.angle % 360 == 0:\n            return coords\n\n        return np.dot(coords, self.rm_coords.T)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n\n        # Apply the rotation matrix to the input coordinates\n        rotated_coords = np.dot(coords, self.rm_coords)\n\n        return rotated_coords\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0:\n            return coords\n        if self.angle % 360 == 0:\n            return coords\n\n        # Convert the input coordinates to a NumPy array\n        coords = np.array(coords)\n\n        # Apply the rotation transformation using the instance's rotation matrix\n        coords = np.dot(coords, self.rm_coords)\n\n        # Return the transformed coordinates as a NumPy array\n        return coords\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0:\n            return coords\n        elif self.angle % 360 == 0:\n            return coords\n        else:\n            return np.dot(coords, self.rm_coords)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return np.dot(coords, self.rm_coords)\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Define a helper function to compute the flops\n    def flop_count_operators_jit(model, inputs):\n        # Define a dictionary to store the operator names and their corresponding flop counts\n        flop_dict = defaultdict(float)\n\n        # Define a dictionary to store the operator names and their corresponding flop counts\n        flop_dict = defaultdict(float)\n\n        # Define a dictionary to store the operator names and their corresponding flop counts\n        flop_dict = defaultdict(float)\n\n        # Define a dictionary to store the operator names and their corresponding flop counts\n        flop_dict = defaultdict(float)\n\n        # Define a dictionary to store the operator names and their corresponding flop counts\n        flop_dict = defaultdict(float)\n\n        # Define a dictionary to store the operator names and their corresponding flop counts\n        flop_dict = defaultdict(float)\n\n        # Define a dictionary to store the operator names and their corresponding flop counts\n        flop_dict = defaultdict(float)\n\n        # Define a dictionary to store the operator names and their corresponding flop counts\n        flop_dict = defaultdict(float)\n\n        # Define a dictionary to store the operator names and their corresponding flop counts\n        flop_dict = defaultdict(float)\n\n        # Define a dictionary to store the operator names and their corresponding flop counts\n        flop_dict = defaultdict(float)\n\n        # Define a dictionary to store the operator names and their corresponding flop counts\n        flop_dict = defaultdict(float)\n\n        # Define a dictionary to store the operator names and their corresponding flop counts\n        flop_dict = defaultdict(float)\n\n        # Define a dictionary to store the operator names and their corresponding flop counts\n        flop_dict = defaultdict(float)\n\n        # Define a dictionary to store the operator names and their corresponding flop counts\n        flop_dict = defaultdict(float)\n\n        # Define a dictionary to store the operator names and their corresponding flop counts\n        flop_dict = defaultdict(float)\n\n        # Define a dictionary to store the operator names and their corresponding flop counts\n       "}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Define a dictionary to store the flops count for each operator\n    flops_dict = defaultdict(float)\n\n    # Define a hook function to accumulate the flops count for each operator\n    def count_flops(module, input, output):\n        flops_dict[module.__class__.__name__] += flop_count_table[module.__class__.__name__]\n\n    # Register the hook function for each module in the model\n    for module in model.modules():\n        module.register_forward_hook(count_flops)\n\n    # Run the model with the given inputs to trigger the hook function\n    with torch.no_grad():\n        model(inputs)\n\n    # Convert the flops count to Gflop\n    for key in flops_dict:\n        flops_dict[key] /= 1e9\n\n    return flops_dict"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Create a dictionary to store the operator names and their corresponding flops\n    flops_dict = defaultdict(float)\n\n    # Define a function to handle the flops computation\n    def handle_flops(module, input, output):\n        if isinstance(module, nn.Conv2d):\n            # Compute the flops for Conv2d layers\n            flops = module.in_channels * module.out_channels * module.kernel_size[0] * module.kernel_size[1] * output.shape[-2] * output.shape[-1] / module.groups\n            flops_dict[module.__class__.__name__] += flops\n        elif isinstance(module, nn.Linear):\n            # Compute the flops for Linear layers\n            flops = module.in_features * module.out_features\n            flops_dict[module.__class__.__name__] += flops\n        elif isinstance(module, nn.BatchNorm2d):\n            # Compute the flops for BatchNorm2d layers\n            flops = module.num_features * output.shape[-2] * output.shape[-1]\n            flops_dict[module.__class__.__name__] += flops\n        elif isinstance(module, nn.ReLU):\n            # Compute the flops for ReLU layers\n            flops = output.numel()\n            flops_dict[module.__class__.__name__] += flops\n        elif isinstance(module, nn.MaxPool2d):\n            # Compute the flops for MaxPool2d layers\n            flops = module.kernel_size * module.kernel_size * output.shape[-2] * output.shape[-1]\n            flops_dict[module.__class__.__name__] += flops\n        elif isinstance(module, nn.AvgPool2d):\n            # Compute the flops for AvgPool2d layers\n            flops = module.kernel_size * module.kernel_size * output.shape[-2] * output.shape[-1]\n            flops_dict[module.__class__.__name__"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Set the model to evaluation mode\n    model.eval()\n\n    # Create a dictionary to store the operator names and their corresponding flops\n    operator_flops = defaultdict(float)\n\n    # Define a hook function to extract the operator names and their corresponding flops\n    def extract_flops_hook(module, input, output):\n        if isinstance(module, torch.nn.Conv2d):\n            # Compute the flops for a convolutional layer\n            kernel_size = module.kernel_size[0] * module.kernel_size[1]\n            in_channels = module.in_channels\n            out_channels = module.out_channels\n            flops = kernel_size * in_channels * output.shape[-2] * output.shape[-1] * out_channels\n            operator_flops[module.__class__.__name__] += flops\n        elif isinstance(module, torch.nn.Linear):\n            # Compute the flops for a fully connected layer\n            in_features = module.in_features\n            out_features = module.out_features\n            flops = in_features * out_features\n            operator_flops[module.__class__.__name__] += flops\n        elif isinstance(module, torch.nn.BatchNorm2d):\n            # Compute the flops for a batch normalization layer\n            flops = module.num_features * output.shape[-2] * output.shape[-1]\n            operator_flops[module.__class__.__name__] += flops\n        elif isinstance(module, torch.nn.ReLU):\n            # Compute the flops for a ReLU activation layer\n            flops = output.numel()\n            operator_flops[module.__class__.__name__] += flops\n        elif isinstance(module, torch.nn.MaxPool2d):\n            # Compute the flops for a max pooling layer\n            kernel_size = module.kernel_size\n            flops = kernel_size * output.shape[-2] * output.shape[-1]\n            operator_flops[module.__class__.__name"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Initialize the flops count dictionary\n    flops_count = defaultdict(float)\n\n    # Define a hook to collect the flops count for each operator\n    def count_flops(module, input, output):\n        flops_count[module.__class__.__name__] += flop_count_table[module.__class__.__name__]\n\n    # Register the hook for each module in the model\n    for module in model.modules():\n        module.register_buffer(\"__flops__\", torch.zeros(1, dtype=torch.float64))\n        module.register_forward_hook(count_flops)\n\n    # Run the model with the given inputs\n    with torch.no_grad():\n        for input in inputs:\n            model(input)\n\n    # Unregister the hooks\n    for module in model.modules():\n        module.remove_forward_hook(count_flops)\n\n    # Convert the flops count to Gflop\n    for op, count in flops_count.items():\n        flops_count[op] = count / 1e9\n\n    return flops_count\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Create a dictionary to store the operator names and their corresponding flops\n    operator_flops = defaultdict(float)\n\n    # Define a hook function to extract operator names and their flops\n    def extract_flops(module, input, output):\n        if hasattr(module, 'total_ops') and hasattr(module, 'total_params'):\n            # Extract the operator name\n            op_name = module.__class__.__name__\n            # Compute the flops\n            flops = module.total_ops * 2.0 / 1e9\n            # Store the flops in the dictionary\n            operator_flops[op_name] += flops\n\n    # Register the hook function for all modules in the model\n    for module in model.modules():\n        module.register_forward_hook(extract_flops)\n\n    # Prepare the inputs for the model\n    inputs = [{\"image\": input} for input in inputs]\n\n    # Run the model with the inputs and compute the flops\n    with torch.no_grad():\n        model(inputs)\n\n    # Return the operator flops dictionary\n    return operator_flops"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Initialize the flops count dictionary\n    flops_count = defaultdict(float)\n\n    # Define a hook function to collect the flops count for each operator\n    def count_hook(module, input, output):\n        flops_count[module.__class__.__name__] += count_flops(module, input, output)\n\n    # Register the hook function for all the modules in the model\n    for module in model.modules():\n        module.register_forward_hook(count_hook)\n\n    # Run the model with the given inputs to collect the flops count\n    with torch.no_grad():\n        for inputs_ in inputs:\n            model(inputs_)\n\n    # Unregister the hook function from all the modules\n    for module in model.modules():\n        module.register_forward_hook(None)\n\n    # Convert the flops count to Gflops\n    flops_count = {k: v / 1e9 for k, v in flops_count.items()}\n\n    return flops_count"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Define a dictionary to store the operator names and their corresponding Gflop counts\n    flops_dict = defaultdict(float)\n\n    # Define a hook function to collect the flops for each operator\n    def count_flops(module, input, output):\n        # Get the operator name\n        op_name = type(module).__name__\n        # Calculate the flops for the operator\n        flops = count_ops(module, input, output)\n        # Convert the flops to Gflops\n        flops_giga = flops / 1e9\n        # Update the flops count for the operator\n        flops_dict[op_name] += flops_giga\n\n    # Register the hook function for all modules in the model\n    for module in model.modules():\n        module.register_forward_hook(count_flops)\n\n    # Run the model with the given inputs to trigger the hook function\n    with torch.no_grad():\n        for inputs_ in inputs:\n            model(inputs_)\n\n    # Return the flops dictionary\n    return flops_dict\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Create a dictionary to store the operator names and their corresponding flops counts\n    flops_dict = defaultdict(float)\n\n    # Create a dictionary to store the operator names and their corresponding flops counts\n    flops_dict = defaultdict(float)\n\n    # Define a function to compute the flops for a given operator\n    def compute_flops(op):\n        # Get the operator name\n        op_name = str(op)\n\n        # Get the input and output shapes\n        input_shape = op.in_shapes[0]\n        output_shape = op.out_shapes[0]\n\n        # Get the number of input and output elements\n        input_elements = input_shape.numel()\n        output_elements = output_shape.numel()\n\n        # Compute the number of flops\n        flops = input_elements + output_elements\n\n        # Update the flops count for the operator\n        flops_dict[op_name] += flops\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Create a jit model from the input model\n    jit_model = torch.jit.trace(model, inputs)\n\n    # Compute the flops for each operator in the model\n    jit_model.graph.apply(compute_flops)\n\n    # Convert the flops counts to Gflops\n    for op_name in flops_dict:\n        flops_dict[op_name] /= 1e9\n\n    # Return the flops dictionary\n    return flops_dict"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Define a function to compute the flops for a given model and input\n    def _compute_flops(model, inputs):\n        # Create a dictionary to store the flops count for each operator\n        flops_dict = defaultdict(float)\n\n        # Define a hook function to accumulate the flops for each operator\n        def _accumulate_flops(module, inputs, outputs):\n            if isinstance(module, torch.nn.Conv2d):\n                # Compute the flops for Conv2d layers\n                batch_size = inputs[0].shape[0]\n                in_channels = module.in_channels\n                out_channels = module.out_channels\n                kernel_size = module.kernel_size[0] * module.kernel_size[1]\n                output_size = outputs.shape[2] * outputs.shape[3]\n                flops = batch_size * in_channels * out_channels * kernel_size * output_size / 1e9\n                flops_dict[module.__class__.__name__] += flops\n            elif isinstance(module, torch.nn.Linear):\n                # Compute the flops for Linear layers\n                batch_size = inputs[0].shape[0]\n                in_features = module.in_features\n                out_features = module.out_features\n                flops = batch_size * in_features * out_features / 1e9\n                flops_dict[module.__class__.__name__] += flops\n\n        # Register the hook function for all Conv2d and Linear layers\n        for module in model.modules():\n            if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n                module.register_forward_hook(_accumulate_flops)\n\n        # Run the model with the given inputs\n        with torch.no_grad():\n            model(inputs)\n\n        # Unregister the hook function to prevent accumulating flops for subsequent runs\n        for module in model.modules():\n            if isinstance(module, (torch.nn.Conv2"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Create a temporary directory to store the jit-compiled model\n    temp_dir = tempfile.TemporaryDirectory()\n    temp_dir_path = temp_dir.name\n\n    # Define a function to compute the flops for a given model and input\n    def compute_flops(model, inputs):\n        # Define a dictionary to store the flops for each operator\n        flops_dict = defaultdict(float)\n\n        # Define a hook to accumulate the flops for each operator\n        def hook(module, inputs, outputs):\n            if isinstance(module, nn.Conv2d):\n                flops_dict[module.__class__.__name__] += (\n                    module.in_channels\n                    * module.out_channels\n                    * module.kernel_size[0]\n                    * module.kernel_size[1]\n                    * outputs.shape[2]\n                    * outputs.shape[3]\n                    / 1e9\n                )\n            elif isinstance(module, nn.BatchNorm2d):\n                flops_dict[module.__class__.__name__] += (\n                    module.num_features * outputs.shape[2] * outputs.shape[3] / 1e9\n                )\n            elif isinstance(module, nn.Linear):\n                flops_dict[module.__class__.__name__] += (\n                    module.in_features * module.out_features / 1e9\n                )\n            elif isinstance(module, nn.ReLU):\n                flops_dict[module.__class__.__name__] += (\n                    outputs.numel() / 1e9\n                )\n            elif isinstance(module, nn.MaxPool2d):\n                flops_dict[module.__class__.__name__] += (\n                    module.kernel_size * module.kernel_size * outputs.shape[2] * outputs.shape[3] / 1e9\n                )\n\n        # Register the hook for all modules in the model\n        for module in model.modules():\n            module.register_forward_hook(hook)\n\n        # Run the model with the given input"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Set the model to evaluation mode\n    model.eval()\n\n    # Define a dictionary to store the flops count for each operator\n    flops_dict = defaultdict(float)\n\n    # Iterate over the inputs\n    for inputs in inputs:\n\n        # Wrap the model with jit for flops computation\n        wrapped_model = torch.jit.trace(model, [inputs])\n\n        # Run the model with the given input\n        _ = wrapped_model(inputs)\n\n        # Get the flops count for each operator\n        flops_dict_current = flop_count(wrapped_model, (inputs,))\n\n        # Update the flops count for each operator\n        for op, flops in flops_dict_current.items():\n            flops_dict[op] += flops\n\n    # Compute the average flops count for each operator\n    for op in flops_dict:\n        flops_dict[op] /= len(inputs)\n\n    # Return the flops count for each operator\n    return flops_dict"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Define a dictionary to store the flops count for each operator\n    flops_dict = defaultdict(float)\n\n    # Define a hook function to extract the flops count for each operator\n    def count_hook(module, input, output):\n        # Get the operator name\n        op_name = module.__class__.__name__\n        # Get the input shape\n        input_shape = input[0].shape\n        # Get the output shape\n        output_shape = output.shape\n        # Compute the flops count\n        flops = compute_flops(module, input_shape, output_shape)\n        # Update the flops count for the operator\n        flops_dict[op_name] += flops\n\n    # Register the hook function for each module in the model\n    for module in model.modules():\n        module.register_forward_hook(count_hook)\n\n    # Run the model with the provided inputs\n    with torch.no_grad():\n        for inputs in inputs:\n            model(inputs)\n\n    # Convert the flops count to Gflop\n    for op_name, flops in flops_dict.items():\n        flops_dict[op_name] = flops / 1e9\n\n    return flops_dict\n\n\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Define a dictionary to store the flops count for each operator\n    flops_dict = defaultdict(float)\n\n    # Define a hook function to extract the flops count for each operator\n    def count_hook(module, input, output):\n        # Get the operator name\n        op_name = module.__class__.__name__\n        # Get the input shape\n        input_shape = input[0].shape\n        # Get the output shape\n        output_shape = output.shape\n        # Calculate the flops count for this operator\n        flops = count_flops(module, input_shape, output_shape)\n        # Update the flops count for this operator\n        flops_dict[op_name] += flops\n\n    # Register the hook function for all modules in the model\n    for module in model.modules():\n        module.register_forward_hook(count_hook)\n\n    # Define a list to store the flops count for each input\n    flops_list = []\n\n    # Run the model with each input and compute the flops\n    for input in inputs:\n        # Run the model with the input\n        output = model(input)\n        # Compute the flops for this input\n        flops = sum(flops_dict.values())\n        # Add the flops count to the list\n        flops_list.append(flops)\n\n    # Compute the average flops count across all inputs\n    avg_flops = sum(flops_list) / len(flops_list)\n\n    # Update the flops count for each operator with the average flops count\n    for op_name in flops_dict:\n        flops_dict[op_name] = avg_flops\n\n    # Return the flops count for each operator\n    return flops_dict\n\n\n\n\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Define the operator count dictionary\n    operator_counts = defaultdict(float)\n\n    # Define the input size\n    input_size = (3, 1280, 1280)\n\n    # Define the input type\n    input_type = torch.float32\n\n    # Define the input tensor\n    input_tensor = torch.randn(input_size).to(input_type)\n\n    # Define a hook function to count the number of operations for each operator\n    def count_operations(module, input, output):\n        if isinstance(module, nn.Conv2d):\n            # Get the number of output channels, kernel size, and input size\n            output_channels, input_channels, kernel_size, _, _ = module.weight.size()\n            _, _, H, W = input[0].size()\n\n            # Compute the number of operations for a 2D convolution\n            operations = output_channels * input_channels * kernel_size ** 2 * H * W\n\n            # Update the operator count\n            operator_counts[module.__class__.__name__] += operations\n\n        elif isinstance(module, nn.Linear):\n            # Get the number of input and output features\n            input_features = module.in_features\n            output_features = module.out_features\n\n            # Compute the number of operations for a linear layer\n            operations = input_features * output_features\n\n            # Update the operator count\n            operator_counts[module.__class__.__name__] += operations\n\n        elif isinstance(module, nn.BatchNorm2d):\n            # Get the number of input channels\n            input_channels = module.num_features\n\n            # Compute the number of operations for a batch normalization layer\n            operations = 2 * input_channels\n\n            # Update the operator count\n            operator_counts[module.__class__.__name__] += operations\n\n        elif isinstance(module, nn.ReLU):\n            # Get the number of input channels\n            _, _, H, W = input[0].size()\n\n            # Compute the number of"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Create a dictionary to store the flops for each operator\n    flops_dict = defaultdict(float)\n\n    # Iterate over each input\n    for inputs_per_image in inputs:\n        # Get the input image\n        image = inputs_per_image[\"image\"]\n\n        # Create a function to compute the flops\n        def compute_flops(model, inputs):\n            # Wrap the model in a nn.Sequential\n            model = nn.Sequential(model)\n\n            # Create a dictionary to store the flops for each operator\n            flops_dict = defaultdict(float)\n\n            # Define a hook function to compute the flops for each operator\n            def hook_fn(module, input, output):\n                # Get the number of inputs and outputs\n                num_inputs = len(input[0]) if isinstance(input, tuple) else 1\n                num_outputs = len(output) if isinstance(output, tuple) else 1\n\n                # Compute the flops for the operator\n                flops = num_inputs * num_outputs\n\n                # Update the flops for the operator\n                flops_dict[module.__class__.__name__] += flops\n\n            # Register the hook function for each module in the model\n            for module in model.modules():\n                module.register_forward_hook(hook_fn)\n\n            # Run the model with the input\n            model(inputs)\n\n            # Return the flops dictionary\n            return flops_dict\n\n        # Compute the flops for the model\n        flops = compute_flops(model, image)\n\n        # Update the flops dictionary\n        for key, value in flops.items():\n            flops_dict[key] += value\n\n    # Convert the flops dictionary to a defaultdict\n    flops_dict = defaultdict(float, flops_dict)\n\n    # Return the flops dictionary\n    return flops_dict\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Register the operators with the flop_count_ops library\n    torch.ops.load_library(\"build/libflop_count_ops.so\")\n\n    # Create a dictionary to store the operator names and their corresponding flop counts\n    flop_dict = defaultdict(float)\n\n    # Define a function to handle the flop count for each operator\n    def count_hook(op, input, output):\n        flop_dict[op.operator_name()] += op.weighted_flops()\n\n    # Define a function to handle the flop count for each module\n    def flop_counter_hook(module, input, output):\n        if len(list(module.children())) > 0:\n            return\n        if hasattr(module, \"weight\") and module.weight is not None:\n            # Perform flop count for nn.Conv2d\n            flop_dict[module.__class__.__name__] += (\n                module.in_channels * module.out_channels * module.kernel_size[0] * module.kernel_size[1] * output[0].numel()\n            )\n        elif isinstance(module, nn.ReLU):\n            # Perform flop count for nn.ReLU\n            flop_dict[module.__class__.__name__] += output[0].numel()\n        elif isinstance(module, nn.Linear):\n            # Perform flop count for nn.Linear\n            flop_dict[module.__class__.__name__] += module.in_features * module.out_features * output[0].numel()\n\n    # Register the hooks for counting flops\n    model.apply(flop_counter_hook)\n    model.apply(count_hook)\n\n    # Run the model with the given inputs to trigger the flop count\n    model(inputs)\n\n    # Convert the flop counts to Gflop and return the result\n    return {k: v / 1e9 for k, v in flop_dict.items()}\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Register a hook to collect the flops count\n    flops_count = defaultdict(float)\n    hook_handle = model.backbone.register_forward_hook(\n        lambda module, inputs, outputs: flop_count_ops(\n            module, inputs, outputs, flops_count\n        )\n    )\n\n    # Run the model with the provided inputs\n    with torch.no_grad():\n        model(inputs)\n\n    # Remove the hook\n    hook_handle.remove()\n\n    # Return the flops count\n    return flops_count\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Define the input and output names for the model\n    input_names = [\"image\"]\n    output_names = [\"output\"]\n\n    # Define the custom operator support dictionary\n    custom_ops = {torch.ops.quantized.embedding_bag_byte.embedding_bag: 0}\n\n    # Define the jit trace options\n    trace_options = torch.jit.TracerState(\n        torch._C._get_tracing_state(),\n        torch._C._get_dispatch_key_tracing_state(),\n        torch._C._get_operator_call_tracing_state(),\n        torch._C._get_module_call_tracing_state(),\n        torch._C._get_custom_operator_tracing_state(),\n        custom_ops,\n    )\n\n    # Define the jit trace mode\n    trace_mode = \"real\"\n\n    # Define the jit trace\n    trace = torch.jit.trace(model, inputs, input_names=input_names, output_names=output_names, tracer_state=trace_options, _force_outplace=True, _check_trace=False, _check_inputs=False, _check_tolerance=False, _check_device=False, _check_dtype=False, _check_properties=False, _check_tensors=False, _check_is_script_module=False, _check_is_tracing=False, _check_input_requires_grad=False, _check_output_used=False, _check_output_has_grad=False, _check_output_requires_grad=False, _check_output_has_dtype=False, _check_output_has_device=False, _check_output_has_layout=False, _check_output_has_memory_format=False, _check_output_has_names=False, _check_output_has_args=False, _check_output_has_kwargs=False, _check_output_has_other_outputs=False, _check_output_has_inplace_outputs=False, _check"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Create a dictionary to store the flops count for each operator\n    flops_count = defaultdict(float)\n\n    # Create a dictionary to store the flops count for each operator\n    flops_count = defaultdict(float)\n\n    # Define a function to compute the flops count for a given operator\n    def compute_flops_count(module, input, output):\n        flops_count[module.__class__.__name__] += flop_count_table[module.__class__.__name__](module, input, output)\n\n    # Register the flops count computation function as a hook for each module in the model\n    for module in model.modules():\n        module.register_forward_hook(compute_flops_count)\n\n    # Run the model with the provided inputs to trigger the computation of flops\n    model(inputs)\n\n    # Return the dictionary containing the flops count for each operator\n    return flops_count\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        if self.rm_image is None:\n            return img\n\n        rotated = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n\n        return rotated"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or self.rm_image is None:\n            return img\n\n        interp = interp if interp is not None else self.interp\n\n        if interp == cv2.INTER_NEAREST:\n            img = img.astype(np.uint8)\n\n        rotated = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n\n        return rotated"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        if self.rm_image is None:\n            return img\n\n        if self.angle % 360 == 0:\n            return img\n\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.size == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n\n        return img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        if self.rm_image is None:\n            return img\n\n        if self.angle % 360 == 0:\n            return img\n\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.size == 0:\n            return img\n        if self.rm_image is None:\n            self.rm_image = cv2.getRotationMatrix2D((self.w / 2, self.h / 2), self.angle, 1)\n        if interp is None:\n            interp = self.interp\n        img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n        return img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        if self.rm_image is not None:\n            img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n\n        return img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        if self.rm_image is not None:\n            img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n\n        return img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.size == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        img = self.apply_coords(img, interp=interp)\n        return img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        if self.rm_image is not None:\n            return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n        else:\n            return img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or self.rm_image is None:\n            return img\n\n        interp = interp if interp is not None else self.interp\n\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        if self.rm_image is None:\n            return img\n\n        if self.rm_image.shape[0] == 2:\n            rm_image = self.rm_image\n        else:\n            rm_image = self.rm_image[:2, :]\n\n        img = cv2.warpAffine(img, rm_image, (self.bound_w, self.bound_h), flags=interp)\n\n        return img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or self.rm_image is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n\n        return img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        if self.rm_image is not None:\n            img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n\n        return img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        if self.rm_image is None:\n            return img\n\n        rotated_img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n\n        return rotated_img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or self.rm_image is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        if self.rm_image.shape[0] == 2:\n            rot_img = cv2.warpAffine(img, self.rm_image, (self.w, self.h), flags=interp)\n        else:\n            rot_img = cv2.warpAffine(img, self.rm_image[:2], (self.w, self.h), flags=interp)\n\n        return rot_img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.size == 0:\n            return img\n\n        if self.rm_image.shape[0] == 2:\n            img = cv2.warpAffine(img, self.rm_image, (int(self.w), int(self.h)), flags=cv2.INTER_NEAREST if interp is None else interp)\n        else:\n            img = cv2.warpAffine(img, self.rm_image, (int(self.bound_w), int(self.bound_h)), flags=cv2.INTER_NEAREST if interp is None else interp)\n        return img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or self.rm_image is None:\n            return img\n\n        interp = interp if interp is not None else self.interp\n\n        if interp is None:\n            interp = cv2.INTER_LINEAR\n\n        rotated = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n\n        return rotated\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or self.rm_image is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        img = np.rot90(img)\n        img = np.flip(img, axis=1)\n        img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n        img = img[:self.h, :self.w]\n\n        return img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        # If the angle is a multiple of 360 degrees, return the original image\n        if self.angle % 360 == 0:\n            return img\n\n        # Otherwise, rotate the image using the rotation matrix\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if self.instance_mode == ColorMode.IMAGE_BW:\n            # change the color of text to white (in 3-channel images)\n            self.output.img = self.output.img[:, :, ::-1]\n\n        self.overlay_instances(\n            masks=masks,\n            boxes=boxes,\n            labels=labels,\n            keypoints=keypoints,\n            assigned_colors=self.metadata.get(\"thing_colors\", None),\n        )\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self.instance_mode == ColorMode.IMAGE_BW:\n            self.output.img = self.output.img[:, :, ::-1]\n        image = self.output\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if self.overlay_instances:\n            self.overlay_mask(masks=masks, boxes=boxes, labels=labels)\n\n        if self.overlay_boxes:\n            self.overlay_boxes(boxes=boxes, labels=labels)\n\n        if self.overlay_class_names:\n            self.overlay_class_names(boxes=boxes, labels=labels)\n\n        if keypoints is not None:\n            self.overlay_keypoints(keypoints=keypoints)\n\n        return image"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = image[:, :, ::-1]\n        visualizer = Visualizer(image, self.metadata, instance_mode=self.instance_mode)\n        if \"pred_boxes\" in predictions:\n            boxes = predictions[\"pred_boxes\"]\n            boxes = boxes if isinstance(boxes, list) else boxes.tensor\n            boxes = np.asarray(boxes)\n            # plot predictions\n            labels = None\n            if \"pred_classes\" in predictions:\n                labels = _create_text_labels(predictions[\"pred_classes\"], predictions[\"scores\"], self.metadata.get(\"thing_classes\", None))\n            if \"pred_masks\" in predictions or \"pred_masks_rle\" in predictions:\n                masks = predictions.get(\"pred_masks\", None)\n                if masks is None:\n                    masks = _decode_rle_masks(predictions[\"pred_masks_rle\"], predictions[\"pred_boxes\"], image.shape[0], image.shape[1])\n                masks = np.asarray(masks)\n                masks = [GenericMask(x, image.shape[0], image.shape[1]) for x in masks]\n            if \"pred_keypoints\" in predictions:\n                keypoints = predictions[\"pred_keypoints\"]\n                keypoints = keypoints if isinstance(keypoints, list) else keypoints.tensor\n                keypoints = np.asarray(keypoints)\n            visualizer.overlay_instances(\n                labels=labels,\n                boxes=boxes,\n                masks=masks,\n                keypoints=keypoints,\n            )\n        visualizer = visualizer.get_output()\n        processed_results = {\"instances\": visualizer}\n        return processed_results"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Convert image from Matplotlib RGB format to PIL.Image RGB format\n        image = Image.fromarray(image)\n\n        # Create a visualizer with the specified instance mode and image.\n        visualizer = Visualizer(image, self.metadata, instance_mode=self.instance_mode)\n\n        # If there are instance predictions, draw them on the image.\n        if \"instances\" in predictions:\n            instances = predictions[\"instances\"].to(self.cpu_device)\n            visualizer.draw_instance_predictions(predictions=instances)\n\n        # Convert the image back to OpenCV BGR format and return it.\n        return cv2.cvtColor(image, cv2.COLOR_RGB2BGR)"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = image[:, :, ::-1]\n\n        # Convert the image from BGR format to RGB format\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Visualize ground truth boxes\n        if len(dataset_dict[\"annotations\"]) > 0:\n            self.draw_dataset_dict(dataset_dict)\n\n        # Convert predictions to COCO format\n        coco_image, coco_predictions = convert_to_coco_api(image, predictions)\n\n        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = image[:, :, ::-1]\n\n        # Convert the image from BGR format to RGB format\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Visualize ground truth boxes\n        if len(dataset_dict[\"annotations\"]) > 0:\n            self.draw_dataset_dict(dataset_dict)\n\n        # Convert predictions to COCO format\n        coco_image, coco_predictions = convert_to_coco_api(image, predictions)\n\n        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = image[:, :, ::-1]\n\n        # Convert the image from BGR format to RGB format\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Visualize ground truth boxes\n        if len(dataset_dict[\"annotations\"]) > 0:\n            self.draw_dataset_dict(dataset_dict)\n\n        # Convert predictions to COCO format\n        coco_image, coco_predictions = convert_to_coco_api(image, predictions)\n\n        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = image[:, :, ::-1]\n\n        # Convert the image from BGR format to RGB format\n        image = cv2.cvtColor(image"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if self.instance_mode == ColorMode.IMAGE_BW:  # change color space\n            self.output.input_format = \"BGR\"\n            predictions = predictions.to(self.cpu_device)\n            masks = predictions.pred_masks.to(self.cpu_device)\n\n        colors = compute_colors_for_labels(classes, self.metadata.get(\"thing_colors\", None), self.metadata.get(\"thing_classes\", None))\n        if self.overlay_instances:\n            self.overlay_instances(\n                masks=masks,\n                boxes=boxes,\n                labels=labels,\n                keypoints=keypoints,\n                assigned_colors=colors,\n            )\n        if self.overlay_boxes:\n            self.overlay_boxes(boxes=boxes, labels=labels, assigned_colors=colors)\n        if self.overlay_class_names:\n            self.overlay_class_names(boxes=boxes, labels=labels, assigned_colors=colors)\n        if self.overlay_keypoints:\n            self.overlay_keypoints(keypoints=keypoints, assigned_colors=colors)\n        if self"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if self.instance_mode == ColorMode.IMAGE_BW:  # change color space for a grayscale image\n            self.output.input_format = \"BGR\"\n            predictions.pred_masks = predictions.pred_masks.to(self.cpu_device).numpy()\n            predictions.pred_masks = Image.fromarray(predictions.pred_masks, mode=\"L\")\n            self.output.input_format = \"RGB\"\n\n        if self.overlay_instances:\n            self.overlay_mask(masks=masks, boxes=boxes, labels=labels, keypoints=keypoints)\n        if self.overlay_boxes:\n            self.overlay_boxes(boxes=boxes, labels=labels)\n        if self.overlay_class_names:\n            self.overlay_class_names(boxes=boxes, labels=labels)\n\n        if keypoints is not None:\n            self.overlay_keypoints(keypoints=keypoints)\n\n        if self.instance_mode == ColorMode.SEGMENTATION and self.process_prediction_for_panoptic_seg:\n            self._create_panoptic_segmentation_colormap()\n            panop"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = image[:, :, ::-1]\n        visualizer = Visualizer(image, self.metadata, instance_mode=self.instance_mode)\n        if \"pred_boxes\" in predictions:\n            boxes = predictions[\"pred_boxes\"]\n            boxes = boxes if isinstance(boxes, list) else boxes.tensor\n            boxes = np.asarray(boxes)\n            boxes = Boxes(boxes)\n            # Scale boxes from absolute coordinates to relative coordinates\n            # and convert to matplotlib format:\n            # x0, y0, width, height\n            boxes = boxes.scale(1.0 / self.scale).tensor\n            boxes = boxes.cpu().numpy()\n            boxes = boxes.astype(np.int32)\n            boxes = boxes.tolist()\n            labels = predictions[\"pred_classes\"] if \"pred_classes\" in predictions else None\n            labels = [self.metadata.thing_classes[i] for i in labels] if labels is not None else None\n            vis = visualizer.overlay_instances(boxes=boxes, labels=labels)\n        if \"pred_masks\" in predictions:\n            masks = predictions[\"pred_masks\"].cpu().numpy()\n            if self.instance_mode == ColorMode.IMAGE_BW:\n                # Convert grayscale mask to 3-channel mask for visualization\n                masks = np.repeat(masks[:, :, :, np.newaxis], 3, axis=3)\n            vis = visualizer.overlay_instances(masks=masks)\n        if \"pred_keypoints\" in predictions:\n            keypoints = predictions[\"pred_keypoints\"].cpu().numpy()\n            vis = visualizer.overlay_instances(keypoints=keypoints)\n\n        vis_output = vis.get_image()\n        return vis_output.get_image()"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = cv2.cvtColor(np.asarray(self.output.get_image()), cv2.COLOR_RGB2BGR)\n\n        # Convert image from Matplotlib RGB format to OpenCV BGR format.\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\n        # Check if the predictions are empty.\n        if len(predictions) == 0:\n            return image\n\n        # Check if the predictions are in Detectron2 format.\n        if isinstance(predictions, detectron2.structures.Instances):\n            # Convert predictions to COCO format.\n            predictions = predictions.to(\"cpu\")\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions = predictions.to(torch.device(\"cpu\"))\n            predictions"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = image[:, :, ::-1]\n        visualizer = Visualizer(image, self.metadata, instance_mode=ColorMode.IMAGE_BW)\n        instances = predictions[\"instances\"].to(self.cpu_device)\n        vis_output = visualizer.draw_instance_predictions(predictions=instances)\n        return vis_output.get_image()\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = image[:, :, ::-1]\n        visualizer = Visualizer(image, self.metadata, instance_mode=self.instance_mode)\n        if \"pred_boxes\" in predictions:\n            boxes = predictions[\"pred_boxes\"]\n            boxes = boxes.tensor.numpy() if isinstance(boxes, Boxes) else boxes\n            boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWHA_ABS)\n            predictions[\"pred_boxes\"] = boxes\n\n        # Draw instance-level predictions with alpha=0.5.\n        visualizer.draw_instance_predictions(predictions)\n        return visualizer.get_output().get_image()\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if self.instance_mode == ColorMode.IMAGE_BW:  # change color space\n            self.output.img = cv2.cvtColor(self.output.img, cv2.COLOR_RGB2GRAY)\n            cmap = None\n        else:\n            cmap = random_color(self.metadata.get(\"thing_colors\", None), self.metadata.get(\"thing_dataset_id_to_contiguous_id\", None))\n\n        if keypoints is not None:\n            keypoints = keypoints.numpy()\n\n        if predictions.has(\"pred_boxes\"):\n            self.overlay_instances(\n                masks=masks,\n                boxes=boxes,\n                labels=labels,\n                keypoints=keypoints,\n                assigned_colors=cmap,\n            )\n        else:\n            self.overlay_class_names(\n                masks=masks,\n                boxes=boxes,\n                labels=labels,\n                keypoints=keypoints,\n                assigned_colors=cmap,\n            )\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n\n        if self._instance_mode == ColorMode.IMAGE_BW:  # Grayscale image, colorized instance masks\n            frame = self.output.get_image()\n            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n            frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n            predictions.pred_masks = predictions.pred_masks[:, 0, :, :]\n        else:\n            frame = self.output.get_image()\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if self.overlay_instances:\n            self.overlay_mask(masks=masks, alpha=0.5)\n\n        if self.overlay_boxes:\n            self.overlay_boxes(boxes=boxes, alpha=0.5)\n\n        if self.overlay_class_names:\n            self.overlay_class_names(\n                boxes=boxes,\n                classes=classes,\n            )\n\n        if self.overlay_scores:\n            self.overlay_scores(\n                boxes=boxes,\n                scores=scores,\n            )\n\n        if self.overlay_keypoints:\n            keypoints = predictions.pred_keypoints[predictions.has(\"pred_keypoints\")] if predictions.has(\n                \"pred_keypoints\"\n            ) else None\n            self.overlay_keypoints(keypoints=keypoint"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = image[:, :, ::-1]\n        visualizer = Visualizer(image, self.metadata, scale=1, instance_mode=ColorMode.IMAGE_BW)\n        # Do not draw segmentation masks\n        predictions = predictions.to(\"cpu\")\n\n        vis_output = visualizer.draw_instance_predictions(predictions=predictions)\n        vis_output.get_image()[:, :, ::-1]\n        return vis_output.get_image()[:, :, ::-1]\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if self.overlay_instances:\n            if masks is not None:\n                alpha = 0.5\n                for mask in masks:\n                    mask_image = mask.draw_mask(self.output, alpha=alpha)\n                    image = cv2.addWeighted(image, alpha, mask_image, 1 - alpha, 0)\n            else:\n                # draw boxes\n                image = self.overlay_boxes(boxes=boxes, labels=labels)\n\n            # draw masks\n            if keypoints is not None:\n                image = self.overlay_keypoints(keypoints=keypoints)\n\n        # draw boxes\n        image = self.overlay_boxes(boxes=boxes, labels=labels)\n\n        # draw masks\n        if keypoints is not None:\n            image = self.overlay_keypoints(keypoints=keypoints)\n\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Convert image from Matplotlib RGB format to PIL RGB format\n        image = Image.fromarray(image)\n        self.output = VisImage(image, scale=scale)\n        self._setup_figure(image)\n\n        overlay_alpha = 0.5\n\n        # Convert predictions to numpy arrays\n        boxes = predictions.pred_boxes.tensor.numpy() if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores.numpy() if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.numpy() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n\n        # Convert masks to numpy arrays\n        masks = predictions.pred_masks.numpy() if predictions.has(\"pred_masks\") else None\n        keypoints = predictions.pred_keypoints.numpy() if predictions.has(\"pred_keypoints\") else None\n\n        # If the image mode is grayscale, convert the image to RGB\n        if predictions.has(\"pred_masks\"):\n            if len(predictions.pred_masks) == 0:\n                return self.output\n\n            # Select the masks for the predicted classes\n            masks = masks[range(len(predictions)), classes]\n\n            # Check if masks are empty\n            if len(masks) == 0:\n                return self.output\n\n            # Convert grayscale masks to RGB masks\n            masks = np.repeat(masks[:, :, :, np.newaxis], 3, axis=3)\n            masks = masks * self.metadata.get(\"pixel_mean\", 255)\n            masks = masks.astype(np.uint8)\n\n            # If the image mode is grayscale"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = image[:, :, ::-1]\n        visualizer = Visualizer(image, self.metadata, instance_mode=self.instance_mode)\n        if \"pred_boxes\" in predictions:\n            boxes = predictions[\"pred_boxes\"]\n            boxes = boxes.tensor.cpu().numpy()\n            scores = predictions[\"scores\"].cpu().numpy()[:, None]\n            classes = predictions[\"pred_classes\"].cpu().numpy()[:, None]\n            labels = [self.metadata.thing_classes[i] for i in classes]\n            if self.instance_mode == ColorMode.IMAGE_BW:\n                # Black white colors for each instance have been computed already.\n                # Set the unique color per instance\n                colors = predictions[\"pred_masks\"].cpu().numpy()\n                for i in range(len(colors)):\n                    colors[i] = colors[i, :, :, 0]\n                labels = None\n            else:\n                colors = compute_colors_for_labels(labels).tolist()\n            if \"pred_masks\" in predictions:\n                masks = predictions[\"pred_masks\"].cpu().numpy()\n                # masks has shape (N, Hmask, Wmask) with the Hmask and Wmask varying depending on the side of the mask\n                masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n            else:\n                masks = None\n            if \"pred_keypoints\" in predictions:\n                keypoints = predictions[\"pred_keypoints\"].cpu().numpy()\n            else:\n                keypoints = None\n\n            if self.instance_mode == ColorMode.SEGMENTATION and self.process_and_apply_mask:\n                # The given mask is of shape (N, 1, Hmask, Wmask).\n                # The first dimension N is the number of instances.\n                # The second dimension 1 is the number of classes.\n                # We use the class"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = image[:, :, ::-1]\n        visualizer = Visualizer(image, self.metadata, instance_mode=self.instance_mode)\n        instances = predictions[\"instances\"].to(self.cpu_device)\n        vis_output = visualizer.draw_instance_predictions(predictions=instances)\n        return vis_output.get_image()\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self.instance_mode == ColorMode.IMAGE_BW:\n            # change to color image for printing\n            predictions = predictions.to(\"cpu\")\n            self._output.input_image = cv2.cvtColor(self._output.input_image, cv2.COLOR_GRAY2BGR)\n            self.instance_mode = ColorMode.IMAGE\n        elif self.instance_mode == ColorMode.SEGMENTATION and self.process_and_draw_instance_predictions.segmentation_bitmap_format == \"polygon\":\n            raise ValueError(\"The instance visualization does not support polygon format.\")\n        elif self.instance_mode == ColorMode.IMAGE and self.process_and_draw_instance_predictions.segmentation_bitmap_format == \"mask\":\n            # Use RGB color to visualize the masks\n            predictions = predictions.to(\"cpu\")\n            self._output.input_image = cv2.cvtColor(self._output.input_image, cv2.COLOR_GRAY2BGR)\n\n        if \"pred_masks\" in predictions:\n            masks = predictions.pred_masks[:, 0].cpu().numpy()\n            # masks = masks[:, 1:2, :, :]\n            masks = [GenericMask(x, self._output.height, self._output.width) for x in masks]\n        else:\n            masks = None\n\n        if \"pred_masks_rle\" in predictions:\n            rles = predictions.pred_masks_rle\n        else:\n            rles = None\n\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n\n        if self.instance_mode == ColorMode.SEGMENTATION and self.process_and_draw_instance_predictions.segmentation_bitmap_format == \"mask\":\n            # Draw segmentation masks on image"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = image[:, :, ::-1]\n\n        # Setup visualization style\n        if self.instance_mode == ColorMode.SEGMENTATION and self.process_and_apply_colormap:\n            instance_mode = \"bbox\"\n            show_masks = True\n            show_keypoints = False\n            show_boxes = False\n        else:\n            if self.instance_mode == ColorMode.IMAGE_BW:\n                image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n                # Convert grayscale image to 3 channels for visualization\n                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n            elif self.instance_mode == ColorMode.IMAGE:\n                pass\n            else:\n                # Default to 'bbox' mode\n                instance_mode = \"bbox\"\n                show_masks = True\n                show_keypoints = False\n                show_boxes = True\n\n        # Process predictions\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        masks = predictions.pred_masks if predictions.has(\"pred_masks\") else None\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        # Process masks\n        if show_masks:\n            if masks is None:\n                # Use the predicted boxes to create masks\n                # This happens when masks == None and predictions.has(\"pred_masks\") is False\n                # e.g. RPN and Mask R-CNN (vanilla)\n                masks = R.polygons_to_bitmask(predictions.pred_boxes, predictions.image_size, predictions.mask_size)\n            else:\n                #"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        return np.array(self.canvas.get_image()[:, :, :3], dtype='uint8')\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the image from the canvas\n        img = self.canvas.get_image()\n\n        # Convert the image from RGBA to RGB\n        img = img[:, :, :3]\n\n        # Convert the image to uint8 type\n        img = np.ascontiguousarray(img, dtype=np.uint8)\n\n        # Return the image\n        return img\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        return self.canvas.get_image()[:, :, :3].astype(np.uint8)\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s = canvas.tostring_rgb()\n        image = np.frombuffer(s, dtype='uint8')\n        image = image.reshape(canvas.get_width_height()[::-1] + (3,))\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the image data from the canvas\n        image_data = self.canvas.get_image_data()\n\n        # Convert the image data to a numpy array\n        image = np.frombuffer(image_data, np.uint8)\n\n        # Reshape the array to match the image dimensions\n        image = image.reshape(self.canvas.height, self.canvas.width, 4)\n\n        # Convert the image from RGBA to RGB format\n        image = image[:, :, :3]\n\n        # Return the image\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the image from the canvas\n        canvas = self.canvas\n        s = canvas.tostring_rgb()\n\n        # Get the image dimensions\n        H, W = canvas.get_width_height()[::-1]\n\n        # Convert the image to RGB format and return it\n        return np.fromstring(s, np.uint8).reshape((H, W, 3))\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the canvas associated with the VisImage instance\n        canvas = self.canvas\n\n        # Get the image dimensions from the canvas\n        width, height = canvas.get_width_height()[:2]\n\n        # Get the image data from the canvas as a numpy ndarray of uint8 type\n        image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n\n        # Reshape the image data to (H, W, 3) format\n        image = image.reshape(height, width, 3)\n\n        # Return the image\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s = canvas.tostring_rgb()\n        image = np.fromstring(s, dtype='uint8')\n        image = image.reshape(canvas.get_width_height()[::-1] + (3,))\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the image from the canvas\n        self.canvas.draw()\n        s, (width, height) = self.canvas.print_to_buffer()\n        # Convert the image from RGBA to RGB format\n        buf = np.frombuffer(s, np.uint8)\n        buf.shape = (height, width, 4)\n        image = buf[:, :, :3].copy()\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the current state of the canvas\n        canvas = self.canvas\n\n        # Get the image dimensions from the canvas\n        width, height = canvas.get_width_height()[:2]\n\n        # Get the image data from the canvas\n        buf = canvas.get_image_data()\n\n        # Convert the image data to a numpy ndarray\n        image = np.frombuffer(buf, np.uint8).reshape((height, width, 4))\n\n        # Convert the image from RGBA to RGB format\n        image = image[:, :, :3]\n\n        # Return the image\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s = canvas.tostring_rgb()\n        (w, h) = canvas.get_width_height()\n        return np.fromstring(s, np.uint8).reshape((h, w, 3))\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the current state of the canvas as a numpy ndarray of RGBA format\n        img = self.canvas.get_image()\n\n        # Convert the image from RGBA to RGB format\n        img = img[:, :, :3]\n\n        # Return the image as a numpy ndarray of uint8 type\n        return img.astype(np.uint8)\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s = canvas.tostring_rgb()\n        image = np.fromstring(s, dtype='uint8')\n        image = image.reshape(canvas.get_width_height()[::-1] + (3,))\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the canvas associated with the VisImage instance\n        canvas = self.canvas\n\n        # Get the dimensions of the canvas\n        w, h = canvas.get_width_height()\n\n        # Get the visualized image from the canvas\n        image = canvas.get_image()\n\n        # Convert the image from RGBA to RGB format\n        image = np.asarray(image)[:, :, :3]\n\n        # Return the image as a numpy ndarray of uint8 type\n        return image.astype(np.uint8)\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the current state of the canvas\n        canvas_state = self.canvas.get_state()\n\n        # Extract the image data from the canvas state\n        image_data = canvas_state['image']\n\n        # Convert the image data from RGBA to RGB format\n        image_data = image_data[:, :, :3]\n\n        # Return the image data as a numpy ndarray of uint8 type\n        return image_data.astype(np.uint8)\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the image data from the canvas\n        image_data = self.canvas.get_image_data()\n\n        # Convert the image data from RGBA to RGB format\n        image = np.frombuffer(image_data, np.uint8)\n        image = image.reshape((self.canvas.height, self.canvas.width, 4))\n        image = image[:, :, :3]\n\n        return image\n\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the canvas from the VisImage instance\n        canvas = self.canvas\n\n        # Get the image from the canvas\n        s, (width, height) = canvas.print_to_buffer()\n\n        # Convert the image from RGBA to RGB format\n        buf = np.fromstring(s, dtype=np.uint8)\n        buf.shape = (height, width, 4)\n        buf = buf[:, :, :3]\n\n        # Return the image\n        return buf\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        image = self.canvas.get_image()[:, :, :3]\n        image = np.ascontiguousarray(image)\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        return self.canvas.get_image()[:, :, :3]\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the current state of the canvas associated with the VisImage instance\n        canvas = self.canvas\n\n        # Get the image dimensions from the current state of the canvas\n        W, H = canvas.get_width_height()[:2]\n\n        # Get the image data from the canvas as a numpy ndarray of RGBA format\n        img = canvas.get_image()\n\n        # Convert the image data from RGBA to RGB format\n        img = img[::-1, :, :3].transpose(2, 0, 1)\n\n        # Resize the image to the dimensions determined by the canvas\n        img = np.ascontiguousarray(np.rollaxis(img, 0, 3).reshape((H, W, 3)))\n\n        # Return the image as a numpy ndarray of uint8 type\n        return img.astype(np.uint8)\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        dataset_dict = dic\n        annos = [\n            utils.transform_instance_annotations(obj, dataset_dict[\"transforms\"], dataset_dict[\"image\"])\n            for obj in dataset_dict.pop(\"annotations\")\n            if obj.get(\"iscrowd\", 0) == 0\n        ]\n        instances = utils.annotations_to_instances(annos, dataset_dict[\"image\"].shape[:2])\n        vis_output = self.draw_instance_predictions(dataset_dict[\"image\"], instances)\n        return vis_output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                masks = self.convert_poly_to_mask(masks, self.output.height, self.output.width)\n                self.overlay_instances(\n                    labels=category_ids, masks=masks, assigned_colors=self.metadata.get(\"thing_colors\")\n                )\n            if \"keypoints\" in annos[0]:\n                keypoints = [x[\"keypoints\"] for x in annos]\n                edges = [x[\"edges\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                self.overlay_instances(\n                    labels=category_ids, keypoints=keypoints, edges=edges, assigned_colors=self.metadata.get(\"thing_colors\")\n                )\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            sem_seg = cv2.imread(dic[\"sem_seg_file_name\"], cv2.IMREAD_GRAYSCALE)\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n        pan_seg = dic.get(\"pan_seg\", None)\n        if pan_seg is not None:\n            self.draw_panoptic_seg_predictions(pan_seg.pan_seg, segments_info=pan_seg.segments_info)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                edges = [x[\"edge_color\"] if \"edge_color\" in x else (0, 255, 0) for x in annos]\n                alpha = [x[\"alpha\"] if \"alpha\" in x else 0.5 for x in annos]\n                draw_sem_seg(\n                    self.output, masks, category_ids, self.metadata.thing_colors, edges, alpha\n                )\n            if \"keypoints\" in annos[0]:\n                keypoints = [x[\"keypoints\"] for x in annos]\n                edges = [x[\"edge_color\"] if \"edge_color\" in x else (0, 255, 0) for x in annos]\n                alpha = [x[\"alpha\"] if \"alpha\" in x else 0.5 for x in annos]\n                draw_instance_predictions(\n                    self.output, keypoints, alpha=alpha, edges=edges\n                )\n            if \"bbox\" in annos[0]:\n                boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n                scores = [x[\"score\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                labels = _create_text_labels(\n                    category_ids,\n                    scores,\n                    self.metadata.get(\"thing_classes\", None),\n                )\n                draw_boxes(self.output, boxes, labels)\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            sem_seg = cv2.imread(dic[\"sem_seg_file_name\"], c"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                masks = self.convert_poly_to_mask(masks, self.output.height, self.output.width)\n                self.draw_binary_masks(masks, colors=self._jitter([self._get_mask_color(x) for x in category_ids]))\n            if \"keypoints\" in annos[0]:\n                keypoints = [x[\"keypoints\"] for x in annos]\n                edges = [x[\"edges\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                self.draw_instance_keypoints(keypoints, edges, colors=self._jitter([self._get_mask_color(x) for x in category_ids]))\n            if \"bbox\" in annos[0]:\n                boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                self.overlay_instances(\n                    labels=category_ids,\n                    boxes=boxes,\n                    masks=None,\n                    keypoints=None,\n                    assigned_colors=self._jitter([self._get_mask_color(x) for x in category_ids]),\n                )\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            sem_seg = cv2.imread(dic[\"sem_seg_file_name\"], cv2.IMREAD_GRAYSCALE)\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg, area_threshold="}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                edge_colors = [\n                    self._jitter([x / 255 for x in self._class_colors[c]])\n                    for c in category_ids\n                ]\n                drawn_masks = self.draw_and_connect_masks(\n                    masks, edge_colors, category_ids, alpha=0.5\n                )\n                self.output.ax.imshow(drawn_masks.get_image())\n            if \"keypoints\" in annos[0]:\n                keypoints = [x[\"keypoints\"] for x in annos]\n                edges = [x[\"edges\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                edge_colors = [\n                    self._jitter([x / 255 for x in self._class_colors[c]])\n                    for c in category_ids\n                ]\n                self.overlay_instances(\n                    masks=None,\n                    boxes=None,\n                    labels=None,\n                    keypoints=keypoints,\n                    assigned_colors=edge_colors,\n                    edges=edges,\n                )\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"semantic_segmentation\" in dic:\n            sem_seg = dic[\"semantic_segmentation\"]\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n\n        pan_seg = dic.get(\"pan_seg\", None)\n        seg_info = dic.get(\"seg_info\", None)\n        if pan_seg is not None and seg_info is not None:\n            self.draw"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                edges = [x[\"edge\"] for x in annos]\n                areas = [x[\"area\"] for x in annos]\n                self.overlay_instances(\n                    labels=category_ids,\n                    masks=masks,\n                    edges=edges,\n                    areas=areas,\n                )\n            if \"keypoints\" in annos[0]:\n                keypoints = [x[\"keypoints\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                self.overlay_instances(\n                    labels=category_ids,\n                    keypoints=keypoints,\n                )\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"semantic\" in dic:\n            sem_seg = dic[\"semantic\"]\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n        pan_seg = dic.get(\"pan_seg\", None)\n        if pan_seg is not None:\n            self.draw_panoptic_seg(\n                pan_seg.transpose(2, 0, 1),\n                segment_ids=pan_seg[3],\n                area_threshold=0,\n                alpha=0.5,\n            )\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                edges = [x[\"edge_id\"] for x in annos]\n                areas = [x[\"area\"] for x in annos]\n                self.overlay_instances(\n                    labels=category_ids, masks=masks, edge_ids=edges, areas=areas\n                )\n            else:\n                keypoints = [x[\"keypoints\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                self.overlay_instances(labels=category_ids, keypoints=keypoints)\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            sem_seg = cv2.imread(dic[\"sem_seg_file_name\"], cv2.IMREAD_GRAYSCALE)\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n\n        pan_seg = dic.get(\"pan_seg\", None)\n        seg_info = dic.get(\"seg_info\", None)\n        if pan_seg is None and \"pan_seg_file_name\" in dic:\n            with PathManager.open(dic[\"pan_seg_file_name\"], \"rb\") as f:\n                pan_seg = np.asarray(Image.open(f), dtype=\"uint32\")\n                pan_seg = cv2.resize(\n                    pan_seg, (dic[\"width\"], dic[\"height\"]), interpolation=cv2.INTER_NEAREST\n                )\n        if seg_info is not None and \"label_divisor\" in seg_info:\n            # assumes that `seg_info` is"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annotations = dic.get(\"annotations\", None)\n        if annotations:\n            self._draw_annotations(annotations, self.output.height)\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg:\n            self._draw_sem_seg(sem_seg, self.output.height, alpha=0.5)\n        instances = dic.get(\"instances\", None)\n        if instances:\n            self._draw_instance_predictions(instances)\n        keypoints = dic.get(\"keypoints\", None)\n        if keypoints:\n            self._draw_keypoints(keypoints)\n        panoptic_seg = dic.get(\"panoptic_seg\", None)\n        segments_info = dic.get(\"segments_info\", None)\n        if panoptic_seg is not None and segments_info is not None:\n            self._draw_panoptic_seg_predictions(\n                panoptic_seg, segments_info, area_threshold=0, alpha=0.3\n            )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        dataset_dict = copy.deepcopy(dic)  # Ensure that the input dictionary remains unchanged.\n        annos = [\n            utils.transform_instance_annotations(obj, dataset_dict[\"transforms\"], dataset_dict[\"image\"])\n            for obj in dataset_dict.pop(\"annotations\")\n            if obj.get(\"iscrowd\", 0) == 0\n        ]\n        instances = utils.annotations_to_instances(annos, dataset_dict[\"image\"].shape[:2])\n        dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n        self.draw_dataset_dict(dataset_dict)\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                edges = [x[\"edge_color\"] if \"edge_color\" in x else (0, 255, 0) for x in annos]\n                alpha = [x[\"alpha\"] if \"alpha\" in x else 0.5 for x in annos]\n                img = self.draw_and_connect_masks(img, masks, category_ids, edges, alpha)\n            if \"keypoints\" in annos[0]:\n                keypoints = [x[\"keypoints\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                img = self.draw_and_connect_keypoints(img, keypoints, category_ids)\n        if \"sem_seg\" in dic:\n            img = self.draw_sem_seg(img, dic[\"sem_seg\"], alpha=0.5)\n        if \"pan_seg\" in dic:\n            pan_seg_pred = dic[\"pan_seg\"].argmax(axis=0)\n            img = self.draw_panoptic_seg_predictions(img.copy(), pan_seg_pred, alpha=0.5)\n        return img\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        dic = copy.deepcopy(dic)  # don't modify original data, may contain some other consumers\n        img = utils.read_image(dic.pop(\"file_name\"), format=dic.pop(\"image_format\", None))\n        if \"annotations\" in dic:\n            annotations = dic.pop(\"annotations\")\n            if \"segmentation\" in annotations[0]:\n                masks = [x[\"segmentation\"] for x in annotations]\n                category_ids = [x[\"category_id\"] for x in annotations]\n                edge_colors = kwargs.pop(\n                    \"edge_colors\",\n                    [\n                        k for k, v in COCO_CATEGORIES.items()\n                        if v in category_ids\n                    ],\n                )\n                alpha = kwargs.pop(\"alpha\", 0.5)\n                instance_colors = kwargs.pop(\n                    \"instance_colors\",\n                    [\n                        [x / 255 for x in ColorHash(c).rgb]\n                        for c in COCO_CATEGORIES.values()\n                    ],\n                )\n                if len(edge_colors) > len(instance_colors):\n                    edge_colors = edge_colors[: len(instance_colors)]\n                draw_msk = kwargs.pop(\"draw_instance_contours\", True)\n                self.overlay_instances(\n                    masks=masks,\n                    edge_colors=edge_colors,\n                    alpha=alpha,\n                    instance_colors=instance_colors,\n                    draw_instance_contours=draw_msk,\n                )\n            if \"keypoints\" in annotations[0]:\n                keypoints = [x[\"keypoints\"] for x in annotations]\n                category_ids = [x[\"category_id\"] for x in annotations]\n                instance_colors = kwargs.pop(\n                    \"instance_colors\",\n                    [\n                        [x / 255 for x in ColorHash(c).rgb]\n                        for c in COCO"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                masks = self.convert_poly_to_mask(masks, self.output.height, self.output.width)\n                self.overlay_instances(\n                    masks=masks,\n                    labels=category_ids,\n                    boxes=None,\n                    masks_alpha=0.5,\n                )\n                del dic[\"annotations\"]\n\n            keypoints = [x[\"keypoints\"] for x in annos]  # In Detectron2, there is no distinction between person keypoints and object keypoints\n            # for resizing a bit better\n            key_width = 2 * self.output.width / 1333\n            key_height = 2 * self.output.height / 768\n            if len(keypoints[0]) > 0:\n                keypoints = np.asarray(keypoints, dtype=np.float32).reshape(\n                    (-1, 3, len(keypoints[0][0]))\n                )\n                keypoints[:, 0, :] *= key_width\n                keypoints[:, 1, :] *= key_height\n                self.overlay_instances(\n                    boxes=None,\n                    labels=None,\n                    keypoints=keypoints,\n                    assigned_colors=None,\n                    keypoint_alpha=0.7,\n                )\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            sem_seg = cv2.imread(dic[\"sem_seg_file_name\"], cv2.IMREAD_GRAYSCALE)\n        if sem_seg is not None:\n            self.draw_sem_"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annotations = dic.get(\"annotations\", None)\n        if annotations:\n            self._draw_annotations(annotations, self.output.height)\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg:\n            self._draw_sem_seg(sem_seg, self.output.height, alpha=0.5)\n        instances = dic.get(\"instances\", None)\n        if instances:\n            self._draw_instances(instances, self.output.height)\n        pan_seg = dic.get(\"pan_seg\", None)\n        if pan_seg:\n            self._draw_panoptic_seg_predictions(pan_seg, self.output.height, alpha=0.5)\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                edges = [x[\"edge\"] for x in annos]\n                colors = [x[\"color\"] for x in annos]\n                alpha = [x[\"alpha\"] for x in annos]\n                if masks:\n                    self.draw_binary_masks(masks, colors, alpha)\n                if category_ids:\n                    self.draw_and_connect_keypoints(\n                        annos, category_ids, edges, alpha=alpha\n                    )\n            else:\n                self.draw_and_connect_boxes(annos)\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n                sem_seg = Image.open(f)\n                sem_seg = np.asarray(sem_seg, dtype=\"uint8\")\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg)\n\n        pan_seg = dic.get(\"pan_seg\", None)\n        if pan_seg is None and \"pan_seg_file_name\" in dic:\n            with PathManager.open(dic[\"pan_seg_file_name\"], \"rb\") as f:\n                pan_seg = Image.open(f)\n                pan_seg = np.asarray(pan_seg, dtype=\"uint8\")\n        if pan_seg is not None:\n            self.draw_panoptic_seg_predictions(pan_seg, segments_info=dic.get(\"segments_info\", None))\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        dic = copy.deepcopy(dic)  # don't modify original data, may contain some other consumers\n        img = utils.read_image(dic.pop(\"file_name\"), format=dic.pop(\"image_format\", \"BGR\"))\n        if \"annotations\" in dic and len(dic[\"annotations\"]) == 0:\n            del dic[\"annotations\"]\n        vis_gen = visualizer._create_visualizer_generator(\n            img, metadata=self.metadata, instance_mode=self.instance_mode\n        )\n\n        if \"panoptic_seg\" in dic:\n            panoptic_seg, segments_info = dic.pop(\"panoptic_seg\")\n            vis_gen = visualizer.draw_panoptic_seg_predictions(\n                vis_gen, panoptic_seg.astype(np.int32), segments_info, area_threshold=0, alpha=0.6\n            )\n\n        if \"sem_seg\" in dic:\n            vis_gen = visualizer.draw_sem_seg(\n                vis_gen, dic.pop(\"sem_seg\"), area_threshold=0, alpha=0.6\n            )\n\n        if \"instances\" in dic:\n            dic[\"instances\"] = visualizer._instances_to_coco_json(\n                dic.pop(\"instances\"), img.shape[0], img.shape[1]\n            )\n        vis_output = vis_gen.get_output()\n        processed_results = detector_postprocess(instances_to_dict(dic[\"instances\"]), img.shape[0], img.shape[1])\n        vis_output = visualizer._create_visualizer_predictions(vis_output, processed_results)\n        return vis_output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Get the image data from the dictionary\n        img = dic.pop(\"image\")\n        if \"annotations\" in dic and len(dic[\"annotations\"]) > 0:\n            self.draw_annotations(img.shape[:2], dic.pop(\"annotations\"))\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(img.shape[:2], dic.pop(\"sem_seg\"), alpha=0.5)\n        if \"keypoints\" in dic:\n            # assume the boxes are in xywh format,\n            # so we need to convert it to xyxy format\n            dic[\"keypoints\"] = dic[\"keypoints\"].reshape(dic[\"keypoints\"].shape[0], -1, 3)\n            boxes = BoxMode.convert(dic.pop(\"keypoints\")[:, :, :2], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n            self.draw_instance_predictions(dic.pop(\"pred_boxes\", boxes))\n            self.draw_keypoints(dic.pop(\"pred_keypoints\", dic[\"keypoints\"]))\n        if \"proposal_boxes\" in dic:\n            self.draw_boxes(dic.pop(\"proposal_boxes\"))\n        if \"proposal_objectness\" in dic:\n            self.draw_proposals(dic.pop(\"proposal_boxes\"), dic.pop(\"proposal_objectness\"), dic.pop(\"proposal_ious\"))\n        if \"pred_boxes\" in dic:\n            self.draw_boxes(dic.pop(\"pred_boxes\"))\n        if \"pred_masks\" in dic:\n            self.draw_masks(dic.pop(\"pred_masks\"))\n        if \"pred_keypoints\" in dic:\n            self.draw_keypoints(dic.pop(\"pred_keypoints\"))\n        if \"pred_keypoint_heatmaps\" in dic:\n            self.draw_keypoints(dic.pop(\"pred_keypoints\"),"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        dataset_dict = copy.deepcopy(dic)  # TODO avoid modifying the input data\n        annos = [\n            utils.transform_instance_annotations(obj, dataset_dict[\"transforms\"], dataset_dict[\"image\"])\n            for obj in dataset_dict.pop(\"annotations\")\n            if obj.get(\"iscrowd\", 0) == 0\n        ]\n        instances = utils.annotations_to_instances(annos, dataset_dict[\"image\"].shape[:2])\n        dataset_dict.pop(\"annotations\")\n        dataset_dict.pop(\"sem_seg_file_name\", None)\n        dataset_dict = Instances.from_json(dataset_dict, instances)\n\n        if \"sem_seg\" in dataset_dict:\n            sem_seg = dataset_dict.sem_seg.argmax(dim=0).to(torch.uint8)\n            instances.gt_masks = BitMasks(\n                torch.zeros_like(sem_seg, dtype=torch.bool, device=sem_seg.device)\n            )  # a workaround for PanopticFPN\n            instances.gt_classes = torch.zeros((0,), dtype=torch.int32)  # a workaround for PanopticFPN\n            instances.gt_boxes = Boxes(\n                torch.zeros((0, 4), dtype=torch.float32)\n            )  # a workaround for PanopticFPN\n            instances.gt_masks.masks = instances.gt_masks.masks.to(device=sem_seg.device)\n            instances.gt_masks.masks = instances.gt_masks.masks.to(dtype=torch.bool)\n            instances.gt_masks.masks[sem_seg > 0] = True\n            instances.gt_classes = torch.ones(\n                len(instances.gt_masks.masks), dtype=torch.int32, device=sem_seg.device\n            )\n            instances.gt_boxes = Boxes(instances.gt_mas"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        dataset_dict = copy.deepcopy(dic)  # Ensure that the input dict remains unchanged\n        annos = [\n            utils.transform_instance_annotations(obj, dataset_dict[\"transforms\"], dataset_dict[\"image\"])\n            for obj in dataset_dict.pop(\"annotations\")\n            if obj.get(\"iscrowd\", 0) == 0\n        ]\n        instances = utils.annotations_to_instances(annos, dataset_dict[\"image\"].shape[:2])\n        dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n        self._processed_dataset_dicts.append(dataset_dict)\n        return dataset_dict\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Extract the image from the dictionary\n        img = dic.pop(\"image\")\n        # If the image is not a numpy array, convert it to a numpy array\n        if not isinstance(img, np.ndarray):\n            img = np.asarray(img)\n        # If the image is a 2D array, convert it to a 3D array with a single channel\n        if len(img.shape) == 2:\n            img = img[:, :, None]\n        # If the image has a single channel, repeat it three times to create a 3D array with three channels\n        if img.shape[2] == 1:\n            img = np.repeat(img, 3, axis=2)\n        # If the image has four channels (RGBA), remove the alpha channel\n        if img.shape[2] == 4:\n            img = img[:, :, :3]\n        # If the image has a single channel, convert it to a 3D array with three identical channels\n        if img.shape[2] == 1:\n            img = np.repeat(img, 3, axis=2)\n\n        # If the image is in float format, convert it to a uint8 format\n        if img.dtype == np.float32:\n            img = (img * 255).astype(np.uint8)\n        # If the image is in uint16 format, convert it to a uint8 format\n        elif img.dtype == np.uint16:\n            img = (img / 256).astype(np.uint8)\n        # If the image is in uint8 format, do nothing\n        elif img.dtype == np.uint8:\n            pass\n        # If the image is in any other format, raise an error\n        else:\n            raise ValueError(f\"Unsupported image dtype {img.dtype}\")\n\n        # Create a VisImage object from the image data\n        self.img = img\n        # Draw annotations on the image\n        self.draw_and_connect_all(dic)\n        # Return the image object with the annotations drawn on it\n        return self."}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw image\n        img = dic.pop(\"image\")\n        if \"annotations\" in dic and len(dic[\"annotations\"]) > 0:\n            self.draw_annotations(img.copy(), dic.pop(\"annotations\"))\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(img.copy(), dic.pop(\"sem_seg\"), alpha=0.5)\n        if \"keypoints\" in dic:\n            # Unfortunately, the keypoints blending does not support alpha\n            # so we have to store the image in a temp file\n            with self._jpeg_bytes_to_file(img) as f:\n                keypoints_img = cv2.imread(f, cv2.IMREAD_COLOR)\n                keypoints_img = self.overlay_instances(\n                    mask_img=keypoints_img, instances=dic.pop(\"keypoints\")\n                )\n            img = img.copy()\n            self.overlay_img(img, keypoints_img, alpha=0.5)\n        if \"pan_seg\" in dic:\n            pan_seg_img = self.convert_from_tensor(dic.pop(\"pan_seg\"))\n            self.overlay_seg_mask(img, pan_seg_img)\n        return img\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # Convert the binary mask to a polygon representation\n        polygons = binary_mask_to_polygon(binary_mask)\n\n        # Draw the polygons on the image\n        self.draw_polygons(polygons, color, edge_color=edge_color, text=text, alpha=alpha, area_threshold=area_threshold)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # Convert the binary mask to a polygon\n        polygons = binary_mask_to_polygon(binary_mask)\n\n        # Draw the polygon on the image\n        self.draw_polygon(polygons, color, edge_color=edge_color, alpha=alpha, area_threshold=area_threshold)\n\n        # If text is specified, draw it on the image\n        if text is not None:\n            self.draw_text(text, (polygons[0][0][0], polygons[0][0][1] - 2))\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # Convert the binary mask to a polygon representation\n        polygons = binary_mask_to_polygon(binary_mask)\n\n        # Draw the polygons on the image\n        self.overlay_polygons(polygons, color, edge_color, text, alpha, area_threshold)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if edge_color is None:\n            edge_color = color\n\n        if self.output.has_image:\n            # Draw polygons on the image\n            rle = mask_util.encode(np.asfortranarray(binary_mask))\n            self.output.image = draw_polygons(\n                self.output.image, rle, color, edge_color, alpha=alpha, area_threshold=area_threshold\n            )\n\n        if text is not None:\n            # Draw text on the image\n            self.output.image = draw_text(\n                self.output.image, text, (10, 20), color=edge_color, horizontal_alignment=\"left\"\n            )\n\n        return self.output\n\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = self.compute_colors(binary_mask.shape[0], binary_mask.shape[1], None, None, True)[0]\n        color = np.array(color)\n        text_color = color = color * 0.7\n        h, w = binary_mask.shape[:2]\n        img = self.output.get_image()\n        contours = measure.find_contours(binary_mask, 0.5)\n        for contour in contours:\n            contour = np.flip(contour, axis=1)\n            # Draw the mask\n            img = cv2.polylines(img, [contour.astype(np.int32)], True, color, 2)\n            # Draw the polygon edges\n            if edge_color is not None:\n                img = cv2.polylines(img, [contour.astype(np.int32)], True, edge_color, 1)\n            # Draw the text\n            if text is not None:\n                center = np.mean(contour, axis=0).astype(np.int32)\n                center = (int(center[0]), int(center[1]))\n                cv2.putText(img, text, center, cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1)\n        return img\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if binary_mask.ndim == 2:\n            binary_mask = np.stack([binary_mask] * 3, axis=-1)\n\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if edge_color is None:\n            edge_color = color\n\n        if text is not None:\n            text_color = random_color(rgb=True, maximum=1)\n            text_color = (text_color * 255).astype(np.uint8).tolist()\n\n        image = self.output.image\n        output = image.copy()\n        polygons = []\n        for c in range(3):\n            padded_mask = np.zeros(\n                (binary_mask.shape[0] + 2, binary_mask.shape[1] + 2), dtype=np.uint8\n            )\n            padded_mask[1:-1, 1:-1] = binary_mask[:, :, c]\n            contours = find_contours(padded_mask, 0.5)\n            for verts in contours:\n                verts = np.fliplr(verts) - 1\n                p = Polygon(verts, facecolor=color[c], edgecolor=edge_color[c], alpha=alpha)\n                polygons.append(p)\n        if len(polygons) > 0:\n            polygons = PatchCollection(\n                polygons, match_original=True, edgecolors=None, linewidths=0\n            )\n            output = self.overlay_mask(polygons, output)\n        if text is not None:\n            output = self.overlay_text(text, output, color=text_color)\n        return output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if binary_mask.ndim != 2:\n            raise ValueError(\n                f\"Masks with ndim != 2 are not supported, i.e. {binary_mask.shape}.\"\n            )\n        binary_mask = np.ascontiguousarray(binary_mask)\n\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        color = np.array(color)\n        if color.ndim == 1:\n            color = color[:, None] * 255\n        color = color.astype(np.uint8).reshape(1, 1, -1)\n\n        if edge_color is None:\n            edge_color = color\n\n        edge_color = np.array(edge_color)\n        if edge_color.ndim == 1:\n            edge_color = edge_color[:, None] * 255\n        edge_color = edge_color.astype(np.uint8).reshape(1, 1, -1)\n\n        if text is None:\n            text = \"\"\n\n        # draw polygons on the image\n        contours = measure.find_contours(binary_mask, 0.5)\n        for contour in contours:\n            contour = close_contour(contour)\n            contour = np.round(contour).astype(int)\n            cv2.fillPoly(self.output.img, [contour], edge_color)\n\n        # draw the mask on the image\n        for c in contours:\n            area = cv2.contourArea(c.astype(np.float32))\n            if area > area_threshold:\n                cv2.fillPoly(self.output.img, [c.astype(np.int32)], color)\n\n        # draw text on the image\n        if text:\n            cv2.putText(\n                self.output.img,\n                text,\n                (self.output.img.shape[1] // 2, self.output.img.shape[0] // 2),"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # Convert the binary mask to a numpy array of shape (H, W, 4)\n        binary_mask_array = np.asarray(binary_mask)\n        if binary_mask_array.shape[0] == 0 or binary_mask_array.shape[1] == 0:\n            return self.output\n\n        # If the binary mask is a single channel, convert it to a 3-channel mask\n        if binary_mask_array.ndim == 2:\n            binary_mask_array = np.dstack(\n                [binary_mask_array, binary_mask_array, binary_mask_array]\n            )\n\n        # If the binary mask has 3 channels, convert it to a single channel mask\n        if binary_mask_array.shape[2] == 3:\n            binary_mask_array = np.sum(binary_mask_array, axis=2)\n            binary_mask_array = np.clip(binary_mask_array, 0, 1)\n\n        # Create a mask image with the same size as the input image\n        mask_image = np.zeros(\n            (binary_mask_array.shape[0], binary_mask_array.shape[1], 4), dtype=np.uint8\n        )\n\n        # If the binary mask has 4 channels, copy the alpha channel to the mask image\n        if binary_mask_array.shape[2] == 4:\n            mask_image[:, :, 3] = binary_mask_array[:, :, 3]\n\n        # If the color is not specified, choose a random color\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        # If the color is a single number, convert it to a 3-channel color\n        if not isinstance(color, (tuple, list)):\n            color = color_val(color)\n\n        # Set the color of the mask\n        mask_image[:, :, :3] = color\n\n        # If the alpha value is not specified, set it to 0.5\n        if alpha is None:\n            alpha = 0.5"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        if edge_color is None:\n            edge_color = color\n\n        if isinstance(binary_mask, PolygonMasks):\n            binary_mask = binary_mask.polygons\n\n        if isinstance(binary_mask, list):\n            polygons = binary_mask\n        else:\n            # binary mask can be uint8 or boolean\n            binary_mask = np.asarray(binary_mask)\n            # preprocess polygon by detection and filling holes\n            fortran_input = np.ascontiguousarray(binary_mask)\n            padded_binary_mask = np.pad(fortran_input, ((1, 1), (1, 1)), mode=\"constant\", constant_values=0)\n            contours, _ = cv2.findContours(\n                padded_binary_mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE\n            )\n            contours = contours[-2:]\n            # prepare data for polygon\n            polygons = []\n            for contour in contours:\n                contour = contour.flatten().tolist()\n                # after padding and subtracting 1 we may get -1 values in our contours, so we clip it in here\n                contour = [i if i >= 0 else 0 for i in contour]\n                polygons.append(contour)\n        # draw polygons\n        self.draw_polygons(polygons, color, edge_color=edge_color, alpha=alpha, area_threshold=area_threshold)\n\n        if text is not None:\n            pos = text.split(\",\")\n            posl = len(pos)\n            if posl % 2 != 0:\n                raise ValueError(\n                    \"There should be even number of comma-separated integers in the string.\"\n                )\n\n            pos = np.array(pos, dtype=np.int32).reshape((-1, 2))\n            pos = pos + np.array([self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = self.next_color\n        if edge_color is None:\n            edge_color = color\n\n        # Draw polygons on the image\n        for polygon in self.get_mask_polygons(binary_mask):\n            # Check if the polygon area is above the threshold\n            if polygon.area >= area_threshold:\n                # Draw the polygon on the image\n                self.draw_polygon(polygon, color, edge_color=edge_color, alpha=alpha)\n\n        # Draw text on the image\n        if text is not None:\n            self.draw_text(text, (10, 20), color=color)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = np.array(color)\n        color = color * 255\n        color = color.astype(np.uint8)\n\n        if edge_color is None:\n            edge_color = color\n        edge_color = np.array(edge_color)\n        edge_color = edge_color * 255\n        edge_color = edge_color.astype(np.uint8)\n\n        # Draw polygons on the image\n        contours = []\n        for c in measure.find_contours(binary_mask, 0.5):\n            contours.append(np.array(c).reshape(-1, 2))\n        img = np.ascontiguousarray(img)\n        cv2.polylines(img, contours, True, edge_color, 1)\n\n        # Draw text on the image\n        if text is not None:\n            cv2.putText(img, text, (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, edge_color)\n\n        return img\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # Convert the binary mask to a polygons representation\n        polygons = []\n        for contour in measure.find_contours(binary_mask, 0.5):\n            # Flip from (row, col) representation to (x, y)\n            # and subtract the padding pixel\n            contour = np.flip(contour, axis=1)\n            contour = contour - 1\n            contour = contour.reshape(-1)\n            # After padding and subtracting 1 we may get -2 points from `measure.find_contours`, which will cause an error in\n            # `plt.plot`.\n            contour = np.clip(contour, 0, binary_mask.shape[1])\n            polygons.append(contour)\n\n        # Draw the polygons on the image\n        r = js['class_name']._create_annotation_font(color, edge_color)\n        for polygon in polygons:\n            self.output.ax.add_patch(\n                Polygon(polygon, fill=color, edgecolor=edge_color, alpha=alpha, linewidth=1, linestyle=\"-\")\n            )\n            if text is not None:\n                # Place a text at the top left corner of the polygon\n                x0, y0 = min(polygon[0::2]), min(polygon[1::2])\n                self.output.ax.text(\n                    x0,\n                    y0,\n                    text,\n                    family=\"sans-serif\",\n                    bbox=dict(facecolor=\"black\", alpha=0.8, edgecolor=\"none\", pad=1, linewidth=1),\n                    fontsize=12,\n                    color=\"white\",\n                    horizontalalignment=\"left\",\n                    verticalalignment=\"top\",\n                )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        import pycocotools.mask as mask_util\n        import matplotlib.colors as mplc\n        import matplotlib.patches as patches\n        import matplotlib.patheffects as patheffects\n\n        if color is None:\n            color = self.generate_color()\n        if edge_color is None:\n            edge_color = color\n\n        if isinstance(edge_color, str):\n            edge_color = mplc.to_rgb(edge_color) + (1,)\n        elif isinstance(edge_color, tuple):\n            edge_color = edge_color + (1,)\n\n        if isinstance(color, str):\n            color = mplc.to_rgb(color) + (1,)\n        elif isinstance(color, tuple):\n            color = color + (1,)\n\n        # convert to COCO's 1-based pixel format\n        binary_mask = np.asfortranarray(binary_mask.astype(np.uint8))\n\n        # area_threshold = 0 leads to large masks, but better looking boundaries\n        rle = mask_util.encode(binary_mask)[0]\n        rle[\"counts\"] = rle[\"counts\"].decode(\"ascii\")\n        m = mask_util.decode(rle)\n        contours, hierarchy = cv2.findContours(m.astype(np.uint8), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n\n        if contours:\n            contour_polygons = []\n            contour_edgecolors = []\n            for contour in contours:\n                if contour.size >= 6:\n                    contour = np.flip(contour[:, 0, :])\n                    contour = np.hstack([contour, contour[0]])\n                    contour = np.reshape(contour, (-1, 2))\n                    contour_polygons.append(Polygon(contour))\n                    contour_edgecolors.append(edge_color)\n\n            p = P"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        import matplotlib.colors as mplc\n        import cv2\n\n        if color is None:\n            color = self.compute_colors(binary_mask, alpha=1)[0]\n        color = mplc.to_rgb(color)\n        color = (color[0] * 255, color[1] * 255, color[2] * 255)\n        color = color[::-1]  # pycocotools expects BGR\n\n        polygons = []\n        pad = np.ones(binary_mask.shape[0:2], dtype=np.uint8)\n        padded_binary_mask = np.concatenate([binary_mask, pad], axis=1)\n        components = cv2.findContours(\n            padded_binary_mask.astype(np.uint8), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE\n        )\n        if len(components) == 3:\n            # For compatibility with different OpenCV versions\n            components = components[1]\n        else:\n            components = components[0]\n        for comp in components:\n            if comp.shape[0] < 3:\n                continue\n            comp = comp.flatten().tolist()\n            # After the contour is rotated by 90 degrees, the segmentation has 2 types:\n            # 1. The text can be horizontal: the y values of all points are the same\n            # 2. The text can be oblique: the y values of all points are not the same\n            if max(comp[1::2]) - min(comp[1::2]) > 10:\n                # slice the array every two elements\n                # comp = [x0, y0, x1, y1, x2, y2, ...]\n                comp = comp[:8]\n                comp = np.array(comp).reshape((-1, 2)).tolist()\n                comp.append([comp[0][0], comp[0][1]])\n            comp = np.array(comp, dtype=np.int"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        import cv2\n        import numpy as np\n        import matplotlib.colors as mplc\n        from detectron2.utils.visualizer import Visualizer\n        from detectron2.structures import PolygonMasks, BitMasks\n\n        if self._output is None:\n            return self.output\n\n        if color is None:\n            color = self._jitter([0, 255], type=\"uint8\")\n\n        if isinstance(self._output, VisImage):\n            self._output = self._output.img\n\n        if isinstance(binary_mask, PolygonMasks):\n            binary_mask = binary_mask.polygons[0].convert(\"mask\")\n\n        if isinstance(binary_mask, BitMasks):\n            binary_mask = binary_mask.get_mask_tensor(0)\n\n        binary_mask = binary_mask.cpu().numpy()\n        contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        for contour in contours:\n            contour = contour.astype(\"float\")\n            # Approximate the contour and reduce the number of points according to\n            # the number of vertices of the polygon\n            approx = cv2.approxPolyDP(contour, 1.0, True)\n            # Skip small polygons\n            if approx.shape[0] < 3:\n                continue\n            if not np.array_equal(approx[0], approx[-1]):\n                approx = np.vstack((approx, approx[0]))\n            cv2.polylines(self._output, [approx], True, color, thickness=2)\n\n        if edge_color is not None:\n            binary_mask = binary_mask.astype(np.uint8)\n            contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n            for contour in contours:\n                cont"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        import random\n        import matplotlib.colors as mplc\n        import matplotlib.patches as patches\n        import matplotlib.pyplot as plt\n        from detectron2.utils.visualizer import Visualizer\n\n        assert binary_mask.ndim == 2, \"Masks must be of shape (H, W)\"\n        binary_mask_area = binary_mask.sum().item()\n        if binary_mask_area == 0:\n            return self.output\n\n        if color is None:\n            color = random.choice(self._jitter([self.COLORS[None]]))\n        else:\n            # Generate color for this instance\n            if type(color) == str:\n                color = mplc.to_rgb(color)\n            elif isinstance(color, tuple):\n                assert len(color) == 3\n                for channel in color:\n                    assert 0 <= channel <= 255\n            else:\n                raise ValueError(\"Invalid color format. Please provide a string or a tuple of RGB values.\")\n            color = tuple([x / 255 for x in color])\n\n        # Construct a polygon for the mask\n        polygons = []\n        contours = None\n        if not self.show_masks_on_separate_images:\n            # Draw the mask on the image\n            if contours is None:\n                contours = measure.find_contours(binary_mask, 0.5)\n            for verts in contours:\n                # Subtract the padding and flip (y, x) to (x, y)\n                verts = np.fliplr(verts) - 1\n                p = patches.Polygon(verts, facecolor=\"none\", edgecolor=color)\n                polygons.append(p)\n        else:\n            # Draw the mask on a new image\n            polygons.append(patches.Polygon(np.array([[0, 0], [0, 1], [1, 1], [1, 0]]), facecolor=color))\n\n        # Draw the polygons on the image\n        for polygon in polygons:\n           "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # If no color is specified, choose a random color\n        if color is None:\n            color = self.random_color()\n\n        # If no edge color is specified, use the same color as the mask\n        if edge_color is None:\n            edge_color = color\n\n        # Convert the binary mask to a numpy array of uint8 type\n        binary_mask = np.asarray(binary_mask)\n\n        # Find the contours of the binary mask\n        contours, _ = cv2.findContours(binary_mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n\n        # If the binary mask is empty, return the original image\n        if len(contours) == 0:\n            return self.output\n\n        # If the binary mask is not empty, draw the contours on the image\n        for contour in contours:\n            # Calculate the area of the contour\n            contour_area = cv2.contourArea(contour)\n\n            # If the contour's area is smaller than the threshold, skip it\n            if contour_area < area_threshold:\n                continue\n\n            # If the contour is a polygon, draw it\n            if len(contour) >= 3:\n                cv2.fillPoly(self.output.img, [contour], color)\n\n            # If the contour is a line, draw it\n            else:\n                cv2.polylines(self.output.img, [contour], True, color, 2)\n\n        # If the edge color is specified, draw the contours on the image with the edge color\n        if edge_color is not None:\n            for contour in contours:\n                # Calculate the area of the contour\n                contour_area = cv2.contourArea(contour)\n\n                # If the contour's area is smaller than the threshold, skip it\n                if contour_area < area_threshold:\n                    continue\n\n                # If the contour is a polygon, draw its edges\n                if len(contour) >= 3:"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # Convert the binary mask to a numpy array of shape (H, W, 3)\n        binary_mask_np = binary_mask.cpu().detach().numpy()\n        binary_mask_np = np.repeat(binary_mask_np[:, :, np.newaxis], 3, axis=2)\n\n        # If no color is provided, generate a random color\n        if color is None:\n            color = random.choice(COLORS)\n\n        # If no edge color is provided, use the same color as the mask\n        if edge_color is None:\n            edge_color = color\n\n        # Convert the color and edge color to RGBA format\n        color = mplc.to_rgba(color)\n        edge_color = mplc.to_rgba(edge_color)\n\n        # Create a new image with the same size as the binary mask\n        result = np.zeros((binary_mask_np.shape[0], binary_mask_np.shape[1], 4), dtype=np.uint8)\n\n        # Draw the binary mask on the image\n        result[binary_mask_np == 1] = np.asarray(color) * 255\n\n        # Draw the polygon edges on the image\n        contours, _ = cv2.findContours(binary_mask_np.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        for contour in contours:\n            contour = contour.astype(\"int\")\n            cv2.drawContours(result, [contour], -1, np.asarray(edge_color) * 255, 1)\n\n        # Draw the text on the image\n        if text is not None:\n            # Get the font properties\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            font_scale = 0.5\n            thickness = 1\n\n            # Get the text size\n            text_size, _ = cv2.getTextSize(text, font, font_scale, thickness)"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = self.compute_colors(binary_mask, random_color=True)[0]\n        if edge_color is None:\n            edge_color = color\n\n        binary_mask = np.asarray(binary_mask)\n\n        # Check if the binary mask is multi-channel or not\n        if binary_mask.ndim == 3:\n            binary_mask = binary_mask[..., 0]\n\n        # Add alpha channel\n        if binary_mask.dtype == bool:\n            binary_mask = binary_mask.astype(\"uint8\") * 255\n\n        polygons = []\n        for contour in measure.find_contours(binary_mask, 0.5):\n            # Flip from (row, col) representation to (x, y)\n            # and subtract the padding pixel\n            contour = np.flip(contour, axis=1) - (0.5, 0.5)\n            contour = np.round(contour).astype(np.int32)\n            polygons.append(contour)\n\n        # Draw the polygons on the image\n        rgb_img = self.output.get_image()\n        output = draw_polygons(rgb_img, polygons, color, edge_color, alpha=alpha)\n        self.output.reset_image(output)\n\n        if text is not None:\n            self.draw_text(text, org=(15, 15), color=edge_color, horizontal_alignment=\"left\")\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = self.random_color()\n        if edge_color is None:\n            edge_color = color\n\n        if self.output.has_image:\n            canvas = self.output.cpu().numpy()\n        else:\n            canvas = np.zeros(binary_mask.shape + (3,))\n\n        # Fill in the mask with the provided color\n        canvas[binary_mask] = color\n\n        # Draw the mask on the canvas\n        contours = None\n        if self.has_mask:\n            # Draw the mask contours on the canvas\n            contours = cv2.findContours(\n                binary_mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n            )\n            contours = contours[0] if len(contours) == 2 else contours[1]\n            cv2.drawContours(canvas, contours, -1, edge_color, 1, cv2.LINE_AA)\n\n        # Draw the mask text on the canvas\n        if text is not None:\n            cv2.putText(\n                canvas,\n                text,\n                (20, 20),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.5,\n                edge_color,\n                1,\n                cv2.LINE_AA,\n            )\n\n        # Draw the mask contours on the canvas\n        if contours is not None:\n            for contour in contours:\n                contour = contour.squeeze(1)\n                if len(contour) < 3:\n                    continue\n                if cv2.contourArea(contour) < area_threshold:\n                    continue\n                if len(contour) == 3:\n                    # Draw a triangle\n                    cv2.drawContours(canvas, [contour], 0, edge_color, -1)\n                else:\n                    # Draw a polygon\n                    cv2.drawContours(canvas, [contour], 0, color"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if size_as_tensor:\n        assert (\n            input.image_size == other.image_size\n        ), f\"{msg} image_size {input.image_size} != {other.image_size}\"\n    else:\n        assert (\n            input.image_size == other.image_size\n        ), f\"{msg} image_size {tuple(input.image_size)} != {tuple(other.image_size)}\"\n\n    for k, v in input.get_fields().items():\n        if isinstance(v, Boxes):\n            assert torch.allclose(v.tensor, other.get(k).tensor, rtol=rtol), (\n                f\"{msg} Boxes mismatch for {k}: \"\n                f\"{v.tensor.cpu().numpy()} != {other.get(k).tensor.cpu().numpy()}\"\n            )\n        elif isinstance(v, ROIMasks):\n            assert v == other.get(k), f\"{msg} ROIMasks mismatch for {k}: {v} != {other.get(k)}\"\n        elif isinstance(v, torch.Tensor):\n            assert torch.allclose(v, other.get(k), rtol=rtol), (\n                f\"{msg} Tensor mismatch for {k}: \"\n                f\"{v.cpu().numpy()} != {other.get(k).cpu().numpy()}\"\n            )\n        else:\n            raise ValueError(f\"Unknown type: {type(v)}\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size), rtol=rtol\n        ), msg + f\"image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, msg + f\"image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    for k in input.get_fields().keys():\n        if type(input.get(k)) == Boxes:\n            assert torch.allclose(input.get(k).tensor, other.get(k).tensor, rtol=rtol), (\n                msg + f\"{k} mismatch: {input.get(k)} vs {other.get(k)}\"\n            )\n        elif type(input.get(k)) == torch.Tensor:\n            assert torch.allclose(input.get(k), other.get(k), rtol=rtol), msg + f\"{k} mismatch: {input.get(k)} vs {other.get(k)}\"\n        elif type(input.get(k)) == ROIMasks:\n            assert input.get(k).tensor.shape == other.get(k).tensor.shape, (\n                msg + f\"{k} shape mismatch: {input.get(k).tensor.shape} vs {other.get(k).tensor.shape}\"\n            )\n            assert torch.allclose(input.get(k).tensor, other.get(k).tensor, rtol=rtol), (\n                msg + f\"{k} mismatch: {input.get(k)} vs {other.get(k)}\"\n            )\n        elif type(input.get(k)) == list:\n            assert len(input.get(k)) == len(other.get(k)), msg + f\"{k} length mismatch: {len(input.get(k))} vs {len(other.get(k))}\""}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size, rtol=rtol), msg + \"image_size mismatch\"\n    else:\n        assert input.image_size == other.image_size, msg + \"image_size mismatch\"\n\n    for field in input.get_fields().keys():\n        if type(input.get(field)) == Boxes:\n            assert torch.allclose(input.get(field).tensor, other.get(field).tensor, rtol=rtol), msg + f\"{field} mismatch\"\n        elif type(input.get(field)) == ROIMasks:\n            assert input.get(field).masks.allclose(other.get(field).masks), msg + f\"{field} mismatch\"\n        elif type(input.get(field)) == torch.Tensor:\n            assert torch.allclose(input.get(field), other.get(field), rtol=rtol), msg + f\"{field} mismatch\"\n        else:\n            raise ValueError(f\"Unexpected field type: {type(input.get(field))}\")\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not isinstance(input, Instances) or not isinstance(other, Instances):\n        raise ValueError(\"Inputs to assert_instances_allclose must be of type Instances\")\n\n    assert input.image_size == other.image_size, f\"{msg} image_size of instances does not match\"\n\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size, dtype=torch.float64),\n            torch.as_tensor(other.image_size, dtype=torch.float64),\n        ), f\"{msg} image_size of instances does not match as tensors\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size of instances does not match\"\n\n    for k, v in input.get_fields().items():\n        if isinstance(v, Boxes):\n            assert torch.allclose(v.tensor, getattr(other, k).tensor, atol=1e-4), (\n                f\"{msg} Boxes field '{k}' does not match. \"\n                f\"Expected {v}, got {getattr(other, k)}\"\n            )\n        elif isinstance(v, ROIMasks):\n            assert v == getattr(other, k), (\n                f\"{msg} ROIMasks field '{k}' does not match. \"\n                f\"Expected {v}, got {getattr(other, k)}\"\n            )\n        elif isinstance(v, torch.Tensor):\n            assert torch.allclose(v, getattr(other, k), atol=1e-4), (\n                f\"{msg} Tensor field '{k}' does not match. \"\n                f\"Expected {v}, got {getattr(other, k)}\"\n            )\n        else:\n            raise ValueError(f\"Unknown field type: {type(v)}\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not isinstance(input, Instances) or not isinstance(other, Instances):\n        raise ValueError(\"Inputs to assert_instances_allclose must be of type Instances\")\n\n    assert input.image_size == other.image_size, f\"{msg} image_size of instances does not match: {input} != {other}\"\n\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size, dtype=torch.int64),\n            torch.as_tensor(other.image_size, dtype=torch.int64),\n        ), f\"{msg} image_size of instances does not match: {input} != {other}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size of instances does not match: {input} != {other}\"\n\n    for attr in input.get_fields().keys():\n        if type(input.get(attr)) != type(other.get(attr)):\n            raise ValueError(\n                f\"{msg} Attribute '{attr}' of type {type(input.get(attr))} in input Instances \"\n                f\"does not match type {type(other.get(attr))} in output Instances.\"\n            )\n\n        if isinstance(input.get(attr), Boxes):\n            assert torch.allclose(input.get(attr).tensor, other.get(attr).tensor, atol=1e-4), (\n                f\"{msg} Boxes in field '{attr}' of input Instances ({input.get(attr)}) \"\n                f\"and output Instances ({other.get(attr)}) do not match.\"\n            )\n        elif isinstance(input.get(attr), ROIMasks):\n            assert input.get(attr).get_full_size_masks(input.image_size) == other.get(attr).get_full_size_masks(\n                other.image_size\n            ), f\"{msg} Masks in field '{attr}' of input Inst"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if the image sizes are equal\n    if size_as_tensor:\n        assert torch.equal(input.image_size, other.image_size), msg + \"image_size not equal\"\n    else:\n        assert input.image_size == other.image_size, msg + \"image_size not equal\"\n\n    # Check if the fields are equal or close\n    for field in input.get_fields().keys():\n        if field == \"scores\":\n            assert torch.allclose(input.scores, other.scores, rtol=rtol), msg + \"scores not equal\"\n        elif field == \"pred_boxes\":\n            assert torch.allclose(input.pred_boxes.tensor, other.pred_boxes.tensor, rtol=rtol), msg + \"pred_boxes not equal\"\n        elif field == \"pred_masks\":\n            assert torch.allclose(input.pred_masks.tensor, other.pred_masks.tensor, rtol=rtol), msg + \"pred_masks not equal\"\n        elif isinstance(input.get_fields()[field], torch.Tensor):\n            assert torch.allclose(input.get_fields()[field], other.get_fields()[field], rtol=rtol), msg + field + \" not equal\"\n        else:\n            raise ValueError(f\"Unknown field type: {type(input.get_fields()[field])}\")\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size), rtol=0\n        ), f\"{msg} image_size {input.image_size} != {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size {input.image_size} != {other.image_size}\"\n\n    for k, v in input.__dict__.items():\n        if isinstance(v, Boxes):\n            assert torch.allclose(v.tensor, getattr(other, k).tensor, rtol=rtol), (\n                f\"{msg} {k} of the two instances are not equal!\"\n                f\"\\n{getattr(other, k).tensor}\"\n                f\"\\n{v.tensor}\"\n            )\n        elif isinstance(v, ROIMasks):\n            assert torch.allclose(v.tensor, getattr(other, k).tensor, rtol=rtol), (\n                f\"{msg} {k} of the two instances are not equal!\"\n                f\"\\n{getattr(other, k).tensor}\"\n                f\"\\n{v.tensor}\"\n            )\n        elif isinstance(v, torch.Tensor):\n            assert torch.allclose(v, getattr(other, k), rtol=rtol), (\n                f\"{msg} {k} of the two instances are not equal!\"\n                f\"\\n{getattr(other, k)}\"\n                f\"\\n{v}\"\n            )\n        elif isinstance(v, (int, str)):\n            assert v == getattr(other, k), f\"{msg} {k} of the two instances are not equal!\"\n        else:\n            raise ValueError(f\"Unknown type {type(v)}\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(other, Instances)\n    assert len(input) == len(other), \"{}Their # of instances are different: {} vs {}\".format(\n        msg, len(input), len(other)\n    )\n\n    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size), (\n            \"{}Their image_size are different: {} vs {}\".format(\n                msg, input.image_size, other.image_size\n            )\n        )\n    else:\n        assert input.image_size == other.image_size, \"{}Their image_size are different: {} vs {}\".format(\n            msg, input.image_size, other.image_size\n        )\n\n    for attr in input.get_fields().keys():\n        if type(input.get(attr)) is list:\n            assert len(input.get(attr)) == len(other.get(attr))\n            for i, j in zip(input.get(attr), other.get(attr)):\n                assert_instances_allclose(i, j, rtol=rtol, msg=msg)\n        elif isinstance(input.get(attr), Boxes):\n            assert torch.allclose(input.get(attr).tensor, other.get(attr).tensor, rtol=rtol), (\n                \"{}Value of attr '{}' in input is different from other.\".format(msg, attr)\n            )\n        elif isinstance(input.get(attr), ROIMasks):\n            assert torch.allclose(input.get(attr).tensor, other.get(attr).tensor, rtol=rtol), (\n                \"{}Value of attr '{}' in input is different from other.\".format(msg, attr)\n            )\n        elif isinstance(input.get(attr), torch.Tensor):\n            assert torch.allclose(input.get(attr), other.get(attr), rtol=rtol), (\n                \"{}Value of attr '{}' in input is different from other.\".format(msg,"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if the image sizes are equal\n    if size_as_tensor:\n        assert input.image_size == other.image_size, msg + \"image_size\"\n    else:\n        assert input.image_size == tuple(other.image_size), msg + \"image_size\"\n\n    # Check if the fields are equal or close\n    for key in input.get_fields().keys():\n        if isinstance(input.get_fields()[key], Boxes):\n            assert input.get_fields()[key] == other.get_fields()[key], msg + key\n        elif isinstance(input.get_fields()[key], ROIMasks):\n            assert input.get_fields()[key] == other.get_fields()[key], msg + key\n        elif isinstance(input.get_fields()[key], torch.Tensor):\n            assert torch.allclose(\n                input.get_fields()[key], other.get_fields()[key], rtol=rtol\n            ), msg + key\n        else:\n            raise ValueError(\n                f\"{msg} {key} is of unsupported type {type(input.get_fields()[key])}\"\n            )\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(other, Instances), \"Inputs to assert_instances_allclose must be of type Instances\"\n\n    assert len(input) == len(other), \"{}Instances do not have the same length\".format(msg)\n\n    if size_as_tensor:\n        assert torch.equal(input.image_size, other.image_size), \"{}image_size of Instances do not match\".format(msg)\n    else:\n        assert input.image_size == other.image_size, \"{}image_size of Instances do not match\".format(msg)\n\n    for k in input.get_fields().keys():\n        if type(input.get(k)) == torch.Tensor:\n            assert torch.allclose(input.get(k), other.get(k), rtol=rtol), \"{}Tensor field {} of Instances do not match\".format(msg, k)\n        elif type(input.get(k)) == Boxes:\n            assert torch.allclose(input.get(k).tensor, other.get(k).tensor, rtol=rtol), \"{}Boxes field {} of Instances do not match\".format(msg, k)\n        elif type(input.get(k)) == ROIMasks:\n            assert input.get(k).tensor.allclose(other.get(k).tensor, rtol=rtol), \"{}ROIMasks field {} of Instances do not match\".format(msg, k)\n        else:\n            raise ValueError(\"{}Field {} of Instances does not have a supported type\".format(msg, k))\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size, rtol=rtol)\n    else:\n        assert input.image_size == other.image_size\n\n    for field in input.get_fields().keys():\n        if isinstance(input.get_fields()[field], Boxes):\n            assert torch.allclose(input.get_fields()[field].tensor, other.get_fields()[field].tensor, rtol=rtol), msg\n        elif isinstance(input.get_fields()[field], ROIMasks):\n            assert torch.allclose(input.get_fields()[field].tensor, other.get_fields()[field].tensor, rtol=rtol), msg\n        elif isinstance(input.get_fields()[field], torch.Tensor):\n            assert torch.allclose(input.get_fields()[field], other.get_fields()[field], rtol=rtol), msg\n        else:\n            raise ValueError(f\"Field {field} is of unsupported type {type(input.get_fields()[field])}\")\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(other, Instances), (\n        \"Inputs to assert_instances_allclose must be of type Instances. Got {}, {} instead\".format(\n            type(input), type(other)\n        )\n    )\n\n    if size_as_tensor:\n        assert (input.image_size == other.image_size).all(), (\n            \"{}: image_size of instances do not match: {} vs {}\".format(\n                msg, input.image_size, other.image_size\n            )\n        )\n    else:\n        assert input.image_size == other.image_size, (\n            \"{}: image_size of instances do not match: {} vs {}\".format(\n                msg, input.image_size, other.image_size\n            )\n        )\n\n    for attr in input.get_fields().keys():\n        if attr == \"scores\":\n            assert torch.allclose(\n                input.scores if input.has(attr) else torch.empty(0),\n                other.scores if other.has(attr) else torch.empty(0),\n                rtol=rtol,\n            ), (\n                \"{}: scores of instances do not match: {} vs {}\".format(\n                    msg, input.scores, other.scores\n                )\n            )\n        elif attr == \"pred_masks\":\n            assert torch.allclose(\n                input.pred_masks if input.has(attr) else torch.empty(0, 0, 0),\n                other.pred_masks if other.has(attr) else torch.empty(0, 0, 0),\n                rtol=rtol,\n            ), (\n                \"{}: pred_masks of instances do not match: {} vs {}\".format(\n                    msg, input.pred_masks, other.pred_masks\n                )\n            )\n        elif attr == \"pred_boxes\":\n            assert torch.allclose(\n                input.pred_boxes.tensor if input.has(attr) else torch.empty(0,"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if isinstance(input, tuple) and isinstance(other, tuple):\n        assert input == other, msg + f\"Image size mismatch: {input} vs {other}\"\n    elif size_as_tensor:\n        assert torch.allclose(input.tensor, other.tensor), msg + f\"Image size mismatch: {input} vs {other}\"\n    else:\n        assert input == other, msg + f\"Image size mismatch: {input} vs {other}\"\n\n    for k, v in input.get_fields().items():\n        if isinstance(v, Boxes):\n            assert torch.allclose(v.tensor, other.get_fields()[k].tensor), msg + f\"Boxes mismatch for field {k}\"\n        elif isinstance(v, ROIMasks):\n            assert v.tensor.allclose(other.get_fields()[k].tensor), msg + f\"ROIMasks mismatch for field {k}\"\n        elif isinstance(v, torch.Tensor):\n            assert torch.allclose(v, other.get_fields()[k], atol=1e-5), msg + f\"Tensor mismatch for field {k}\"\n        else:\n            raise ValueError(f\"Unsupported field type {type(v)} for field {k}\")\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not isinstance(input, Instances) or not isinstance(other, Instances):\n        raise ValueError(\"Inputs to assert_instances_allclose must be of type Instances\")\n\n    assert input.image_size == other.image_size, \"{}: image_size of input ({}) does not match image_size of other ({})\".format(msg, input.image_size, other.image_size)\n\n    if not size_as_tensor:\n        assert input.image_size[0] == other.image_size[0], \"{}: height of image_size of input ({}) does not match height of image_size of other ({})\".format(msg, input.image_size[0], other.image_size[0])\n        assert input.image_size[1] == other.image_size[1], \"{}: width of image_size of input ({}) does not match width of image_size of other ({})\".format(msg, input.image_size[1], other.image_size[1])\n\n    for k, v in input.get_fields().items():\n        if isinstance(v, Boxes):\n            assert torch.allclose(v.tensor, getattr(other, k).tensor, rtol=rtol), \"{}: Boxes field '{}' of input does not match output\".format(msg, k)\n        elif isinstance(v, torch.Tensor):\n            assert torch.allclose(v, getattr(other, k), rtol=rtol), \"{}: Tensor field '{}' of input does not match output\".format(msg, k)\n        elif isinstance(v, ROIMasks):\n            assert v == getattr(other, k), \"{}: ROIMasks field '{}' of input does not match output\".format(msg, k)\n        elif isinstance(v, PolygonMasks):\n            assert v == getattr(other, k), \"{}: PolygonMasks field '{}' of input does not match output\".format(msg, k)\n        else:\n            raise ValueError(\"{}: Unknown field type for field '{}"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not input.has(\"image_size\") or not other.has(\"image_size\"):\n        raise ValueError(\"Instances must have their image_size property set.\")\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size), rtol=rtol\n        ), msg + \"image_size of tensors are different\"\n    else:\n        assert input.image_size == other.image_size, msg + \"image_size of tensors are different\"\n\n    for k, v in input.get_fields().items():\n        if isinstance(v, Boxes):\n            assert torch.allclose(v.tensor, other.get(k).tensor, rtol=rtol), msg + \"boxes are different in \" + k\n        elif isinstance(v, RotatedBoxes):\n            assert torch.allclose(v.tensor, other.get(k).tensor, rtol=rtol), msg + \"boxes are different in \" + k\n        elif isinstance(v, PolygonMasks) or isinstance(v, BitMasks):\n            assert v == other.get(k), msg + \"Masks are different in \" + k\n        elif isinstance(v, RotatedBoxes):\n            assert v == other.get(k), msg + \"RotatedBoxes are different in \" + k\n        elif isinstance(v, BoxMode):\n            assert v == other.get(k), msg + \"BoxModes are different in \" + k\n        elif isinstance(v, torch.Tensor):\n            assert torch.allclose(v, other.get(k), rtol=rtol), msg + \"tensors are different in \" + k\n        elif isinstance(v, np.ndarray):\n            assert np.allclose(v, other.get(k), rtol=rtol), msg + \"ndarrays are different in \" + k\n        elif isinstance(v, (int, float, str)):\n            assert v == other.get(k), msg +"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not isinstance(input, Instances) or not isinstance(other, Instances):\n        raise ValueError(\"Inputs to assert_instances_allclose must be of type Instances\")\n\n    assert input.image_size == other.image_size, (\n        f\"{msg}Expected image_size to be equal, but got {input.image_size} and {other.image_size}\"\n    )\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size)\n        ), f\"{msg}Expected image_size to be equal, but got {input.image_size} and {other.image_size}\"\n\n    fields1 = input.get_fields()\n    fields2 = other.get_fields()\n    assert set(fields1.keys()) == set(\n        fields2.keys()\n    ), f\"{msg}Expected fields to be equal, but got {fields1.keys()} and {fields2.keys()}\"\n\n    for k, v1 in fields1.items():\n        v2 = fields2[k]\n        if isinstance(v1, Boxes):\n            assert torch.allclose(v1.tensor, v2.tensor), (\n                f\"{msg}Expected Boxes to be equal, but got {v1} and {v2}\"\n            )\n        elif isinstance(v1, ROIMasks):\n            assert torch.allclose(v1.tensor, v2.tensor), (\n                f\"{msg}Expected ROIMasks to be equal, but got {v1} and {v2}\"\n            )\n        elif isinstance(v1, torch.Tensor):\n            assert torch.allclose(v1, v2, rtol=rtol), (\n                f\"{msg}Expected Tensor to be equal, but got {v1} and {v2}\"\n            )\n        else:\n            raise ValueError(\n                f\"{msg}Unsupported type {type(v1)} encountered in Instances. Found {v1} and"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(other, Instances), (\n        \"Inputs to assert_instances_allclose must be of type Instances.\"\n    )\n\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size), rtol=rtol\n        ), f\"{msg} image_size of instances do not match: {input.image_size} vs {other.image_size}\"\n    else:\n        assert (\n            input.image_size == other.image_size\n        ), f\"{msg} image_size of instances do not match: {input.image_size} vs {other.image_size}\"\n\n    for attr, input_value in input.__dict__.items():\n        if isinstance(input_value, Boxes):\n            assert torch.allclose(\n                input_value.tensor, other.__dict__[attr].tensor, rtol=rtol\n            ), f\"{msg} Boxes field {attr} does not match: {input_value} vs {other.__dict__[attr]}\"\n        elif isinstance(input_value, ROIMasks):\n            assert input_value == other.__dict__[attr], f\"{msg} ROIMasks field {attr} does not match.\"\n        elif isinstance(input_value, torch.Tensor):\n            assert torch.allclose(\n                input_value, other.__dict__[attr], rtol=rtol\n            ), f\"{msg} Tensor field {attr} does not match: {input_value} vs {other.__dict__[attr]}\"\n        else:\n            raise ValueError(f\"Unknown field type for instances: {type(input_value)}\")\n\n    return None\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(other, Instances), \"Inputs to assert_instances_allclose must be of type Instances\"\n\n    if input.image_size != other.image_size:\n        raise AssertionError(f\"{msg} image_size not equal: {input.image_size} vs {other.image_size}\")\n\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size, dtype=torch.float64),\n            torch.as_tensor(other.image_size, dtype=torch.float64),\n            rtol=rtol,\n        ), f\"{msg} image_size not allclose: {input.image_size} vs {other.image_size}\"\n\n    for k, v in input.get_fields().items():\n        if isinstance(v, Boxes):\n            assert torch.allclose(v.tensor, other.get(k).tensor, rtol=rtol), f\"{msg} Boxes not allclose: {v} vs {other.get(k)}\"\n        elif isinstance(v, ROIMasks):\n            assert v.tensor.allclose(other.get(k).tensor), f\"{msg} ROIMasks not allclose: {v} vs {other.get(k)}\"\n        elif isinstance(v, torch.Tensor):\n            assert torch.allclose(v, other.get(k), rtol=rtol), f\"{msg} Tensor not allclose: {v} vs {other.get(k)}\"\n        else:\n            raise ValueError(f\"Unknown field type {type(v)}\")\n\n    for k in other.get_fields().keys():\n        assert k in input.get_fields(), f\"{msg} {k} not found in input\"\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size, rtol=rtol), f\"{msg} image_size not close\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size not equal\"\n    for k, v in input.get_fields().items():\n        if isinstance(v, Boxes):\n            assert torch.allclose(v.tensor, other.get(k).tensor, rtol=rtol), f\"{msg} {k} not close\"\n        elif isinstance(v, ROIMasks):\n            assert torch.allclose(v.tensor, other.get(k).tensor, rtol=rtol), f\"{msg} {k} not close\"\n        elif isinstance(v, torch.Tensor):\n            assert torch.allclose(v, other.get(k), rtol=rtol), f\"{msg} {k} not close\"\n        else:\n            raise ValueError(f\"{msg} {k} is of unsupported type {type(v)}\")\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not isinstance(input, Instances):\n        raise ValueError(\"Argument 'input' to assert_instances_allclose must be of type 'Instances'\")\n    if not isinstance(other, Instances):\n        raise ValueError(\"Argument 'other' to assert_instances_allclose must be of type 'Instances'\")\n\n    if size_as_tensor:\n        assert torch.equal(input.image_size, other.image_size), (\n            \"image_size of input: \" + str(input.image_size) + \" is different from \"\n            \"image_size of other: \" + str(other.image_size)\n        )\n    else:\n        assert input.image_size == other.image_size, (\n            \"image_size of input: \" + str(input.image_size) + \" is different from \"\n            \"image_size of other: \" + str(other.image_size)\n        )\n\n    for attr in [\"gt_boxes\", \"pred_boxes\", \"pred_masks\", \"scores\", \"pred_classes\"]:\n        if input.has(attr) and other.has(attr):\n            if isinstance(input.get(attr), Boxes) and isinstance(other.get(attr), Boxes):\n                assert torch.allclose(input.get(attr).tensor, other.get(attr).tensor, rtol=rtol), (\n                    \"boxes of input do not match boxes of other \"\n                    \"(rtol={}, attr={})\".format(rtol, attr)\n                )\n            elif isinstance(input.get(attr), ROIMasks) and isinstance(other.get(attr), ROIMasks):\n                assert torch.allclose(input.get(attr).tensor, other.get(attr).tensor, rtol=rtol), (\n                    \"ROIMasks of input do not match ROIMasks of other \"\n                    \"(rtol={}, attr={})\".format(rtol, attr)\n                )\n            elif isinstance(input.get(attr), torch.Tensor) and isinstance(other."}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return js['class_name'](self).area()\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.widths * self.heights\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.widths * self.heights\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.widths * self.heights\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        if self.mode == \"xyxy\":\n            boxes = self.data\n            area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n        else:\n            area = self.data[:, 2] * self.data[:, 3]\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.widths * self.heights\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        boxes = self.tensor\n        return boxes[:, 2] * boxes[:, 3]\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        boxes = self.tensor\n        return (boxes[:, 2] * boxes[:, 3]).view(-1)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # Compute the area of each box by multiplying the width and height\n        area = self.size[:, 0] * self.size[:, 1]\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return js['class_name'](self.widths * self.heights)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return js['class_name'].area()\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # Check if the boxes' data is a tensor\n        if isinstance(self.tensor, torch.Tensor):\n            # If the boxes' data is a tensor, check if it has at least 2 dimensions\n            if self.tensor.dim() < 2:\n                # If it has less than 2 dimensions, raise an error\n                raise ValueError(\n                    \"RotatedBoxes.area() expects a tensor of at least 2 dimensions.\")\n        else:\n            # If the boxes' data is not a tensor, raise an error\n            raise ValueError(\n                \"RotatedBoxes.area() expects a tensor as input.\")\n\n        # Get the width and height of each box\n        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n\n        # Compute the area of each box by multiplying the width and height\n        area = widths * heights\n\n        # Return the computed area of each box\n        return area\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    return registry.PROPOSAL_GENERATORS[\n        cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    ](cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    return registry.PROPOSAL_GENERATORS[\n        cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    ](cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    return registry.PROPOSAL_GENERATORS[\n        cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    ](cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    return registry.PROPOSAL_GENERATORS[\n        cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    ](cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    return registry.PROPOSAL_GENERATORS[cfg.MODEL.PROPOSAL_GENERATOR.NAME](cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    return registry.PROPOSAL_GENERATORS[cfg.MODEL.PROPOSAL_GENERATOR.NAME](\n        cfg, input_shape\n    )"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    # Retrieve the proposal generator from the registry\n    proposal_generator = registry.PROPOSAL_GENERATORS[cfg.MODEL.PROPOSAL_GENERATOR.NAME](cfg, input_shape)\n    return proposal_generator"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    # Get the proposal generator's class from the registry using the specified name\n    proposal_generator = registry.PROPOSAL_GENERATORS[cfg.MODEL.PROPOSAL_GENERATOR.NAME]\n\n    # Initialize the proposal generator with the given configuration and input shape\n    return proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    return registry.PROPOSAL_GENERATORS[\n        cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    ](cfg, input_shape)\n\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        return registry.PROPOSAL_GENERATORS[\n            cfg.MODEL.PROPOSAL_GENERATOR.NAME\n        ](cfg, input_shape)\n\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        return registry.PROPOSAL_GENERATOR[cfg.MODEL.PROPOSAL_GENERATOR.NAME](\n            cfg, input_shape\n        )\n\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    # Get the proposal generator from the registry using the specified name and configuration\n    proposal_generator = registry.PROPOSAL_GENERATORS[\n        cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    ](cfg, input_shape)\n    return proposal_generator\n\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    from detectron2.modeling import PROPOSAL_GENERATOR_REGISTRY\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(cfg.MODEL.PROPOSAL_GENERATOR.NAME)(cfg, input_shape)\n\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        return registry.PROPOSAL_GENERATOR[cfg.MODEL.PROPOSAL_GENERATOR.NAME](cfg, input_shape)\n\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    proposal_generator = registry.PROPOSAL_GENERATORS[\n        cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    ](cfg, input_shape)\n    return proposal_generator\n\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    proposal_generator = registry.PROPOSAL_GENERATORS[\n        cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    ](cfg, input_shape)\n    return proposal_generator\n\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    proposal_generator = registry.PROPOSAL_GENERATORS[\n        cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    ](cfg, input_shape)\n    return proposal_generator\n\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(cfg.MODEL.PROPOSAL_GENERATOR.NAME)(cfg, input_shape)\n\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        return registry.PROPOSAL_GENERATOR[cfg.MODEL.PROPOSAL_GENERATOR.NAME](\n            cfg, input_shape\n        )\n\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    return registry.PROPOSAL_GENERATORS[\n        cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    ](cfg, input_shape)\n\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = []\n        for proposals_per_image in proposals:\n            gt_classes_per_image = proposals_per_image.gt_classes\n            # wrap the classes in tensor\n            if gt_classes_per_image.dtype == torch.int64:\n                gt_classes_per_image = gt_classes_per_image.to(dtype=torch.int32)\n\n            gt_classes.append(gt_classes_per_image)\n        gt_classes = torch.cat(gt_classes, dim=0)\n        loss_cls = F.cross_entropy(scores, gt_classes)\n\n        # parse box regression outputs\n        if self.box_reg_loss_type == \"smooth_l1\":\n            gt_proposal_deltas = []\n            for proposals_per_image in proposals:\n                gt_proposal_deltas_per_image = proposals_per_image.gt_boxes.tensor - proposals_per_image.proposal_boxes.tensor\n                gt_proposal_deltas.append(gt_proposal_deltas_per_image)\n            gt_proposal_deltas = torch.cat(gt_proposal_deltas, dim=0)\n            loss_box_reg = smooth_l1_loss(\n                proposal_deltas,\n                gt_proposal_deltas,\n                self.box_reg_loss_weight,\n                reduction=\"mean\",\n                beta=self.smooth_l1_beta,\n            )\n        elif self.box_reg_loss_type == \"giou\":\n            loss_box_reg = giou_loss(\n                Boxes(proposal_deltas),\n                [gt.gt_boxes for gt in proposals],\n                reduction=\"mean\",\n            )\n        else:\n            raise ValueError(f\"Invalid box reg loss type {self.box_reg_"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Extract the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Extract the ground truth boxes, labels, and proposal boxes from the proposals\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_labels = [x.gt_classes for x in proposals]\n        proposal_boxes = [x.proposal_boxes for x in proposals]\n\n        # Calculate the classification loss using the cross entropy loss function\n        loss_cls = self.loss_cls(scores, gt_labels, proposal_boxes)\n\n        # Calculate the box regression loss using the smooth L1 loss function\n        loss_box_reg = self.loss_box_reg(proposal_deltas, gt_boxes, proposal_boxes)\n\n        # Return the losses as a dictionary\n        return {\n            \"loss_cls\": self.loss_weight.cls * loss_cls,\n            \"loss_box_reg\": self.loss_weight.bbox_reg * loss_box_reg\n        }"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Get the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Get the ground truth boxes and classes from the proposals\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_classes = [x.gt_classes for x in proposals]\n\n        # Calculate the classification loss\n        loss_cls = self.loss_evaluator(scores, gt_classes)\n\n        # Calculate the box regression loss\n        loss_box_reg = self.box_reg_loss(\n            proposal_deltas, gt_boxes, proposals, self.box2box_transform, self.smooth_l1_beta\n        )\n\n        # Scale the losses by their respective weights\n        loss_cls = loss_cls * self.loss_weight.cls\n        loss_box_reg = loss_box_reg * self.loss_weight.box_reg\n\n        # Return the losses as a dictionary\n        return {\n            \"loss_cls\": loss_cls,\n            \"loss_box_reg\": loss_box_reg\n        }\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        losses = {\n            \"loss_cls\": F.cross_entropy(scores, labels),\n            \"loss_box_reg\": smooth_l1_loss(\n                proposal_deltas[:, :4],\n                regression_targets,\n                self.box_reg_loss_type,\n                self.smooth_l1_beta,\n            ),\n        }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Get the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Get the ground truth boxes and classes from the proposals\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_classes = [x.gt_classes for x in proposals]\n\n        # Calculate the classification loss\n        loss_cls = self.loss_evaluator(scores, gt_classes)\n\n        # Calculate the box regression loss\n        loss_box_reg = self.box_reg_loss_evaluator(proposal_deltas, gt_boxes, gt_classes)\n\n        # Scale the classification loss by its weight\n        loss_cls *= self.loss_weight.get(\"loss_cls\", 1.0)\n\n        # Scale the box regression loss by its weight\n        loss_box_reg *= self.loss_weight.get(\"loss_box_reg\", 1.0)\n\n        # Return the losses as a dictionary\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = []\n        for proposals_per_image in proposals:\n            gt_classes_per_image = proposals_per_image.gt_classes\n            # bg_label = num_classes\n            # gt_classes_per_image = torch.cat([gt_classes_per_image, torch.full_like(gt_classes_per_image[:1], bg_label)])\n            gt_classes.append(gt_classes_per_image)\n\n        logits_pred = torch.cat([x.pred_logits for x in scores], dim=0)\n        cls_loss = F.cross_entropy(logits_pred, torch.cat(gt_classes, dim=0))\n\n        # parse box regression outputs\n        if len(proposal_deltas) > 0:\n            proposal_deltas = torch.cat(proposal_deltas, dim=0)\n            gt_proposal_deltas = []\n            for proposals_per_image in proposals:\n                gt_proposal_deltas_per_image = self.box2box_transform.get_deltas(\n                    proposals_per_image.proposal_boxes.tensor, proposals_per_image.gt_boxes.tensor\n                )\n                gt_proposal_deltas.append(gt_proposal_deltas_per_image)\n            gt_proposal_deltas = torch.cat(gt_proposal_deltas, dim=0)\n            proposal_deltas = proposal_deltas.reshape(-1, 4)\n            box_loss = F.smooth_l1_loss(\n                proposal_deltas, gt_proposal_deltas, reduction=\"none\"\n            )\n        else:\n            box_loss = proposal_deltas.sum()  # zero loss\n\n        return {\n            \"loss_cls\": self.loss_weight.sigmoid * cls"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Unpack the predictions and proposals\n        scores, proposal_deltas = predictions\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_classes = [x.gt_classes for x in proposals]\n        proposal_boxes = [x.proposal_boxes for x in proposals]\n\n        # Calculate the classification loss\n        classification_loss = F.cross_entropy(scores, gt_classes, reduction=\"mean\")\n\n        # Calculate the box regression loss\n        proposal_deltas = proposal_deltas.view(-1, 4)\n        gt_boxes = torch.cat(gt_boxes, dim=0)\n        proposal_boxes = torch.cat(proposal_boxes, dim=0)\n        box_regression_loss = (\n            smooth_l1_loss(\n                proposal_deltas,\n                self.box2box_transform.get_deltas(proposal_boxes, gt_boxes),\n                beta=1 / 9,\n                reduction=\"mean\",\n            )\n            * self.loss_weight.box_reg\n        )\n\n        # Return the losses as a dictionary\n        return {\n            \"loss_cls\": classification_loss,\n            \"loss_box_reg\": box_regression_loss,\n        }\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # Parse classification and box regression predictions\n        gt_classes = []\n        gt_boxes = []\n        for proposals_per_image in proposals:\n            gt_classes_per_image = proposals_per_image.gt_classes.to(dtype=torch.int64)\n            gt_boxes_per_image = proposals_per_image.gt_boxes\n            gt_classes.append(gt_classes_per_image)\n            gt_boxes.append(gt_boxes_per_image)\n\n        # Reshape the scores and proposal deltas tensors\n        scores_flatten = []\n        for per_image_scores in scores:\n            scores_flatten.append(per_image_scores.view(-1))\n        scores_flatten = torch.cat(scores_flatten, dim=0)\n        proposal_deltas_flatten = []\n        for per_image_proposal_deltas in proposal_deltas:\n            proposal_deltas_flatten.append(per_image_proposal_deltas.view(-1))\n        proposal_deltas_flatten = torch.cat(proposal_deltas_flatten, dim=0)\n\n        # Calculate classification loss\n        valid_idxs = torch.nonzero(gt_classes_flatten >= 0).squeeze(1)\n        gt_classes_flatten = gt_classes_flatten[valid_idxs]\n        class_loss = F.cross_entropy(scores_flatten[valid_idxs], gt_classes_flatten)\n\n        # Calculate box regression loss\n        box_loss = smooth_l1_loss(\n            proposal_deltas_flatten[valid_idxs],\n            gt_boxes_flatten[valid_idxs],\n            beta=0.1,\n        )\n\n        # Return the losses as a dictionary\n        return {\n            \"loss_cls\": class_loss,\n            \"loss_box"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        losses = {}\n        if self.training:\n            labels, regression_targets = self.label_and_sample_anchors(proposals)\n            loss_classification = F.cross_entropy(scores, labels.long(), reduction=\"mean\")\n            loss_box_regression = smooth_l1_loss(\n                proposal_deltas[loss_classification.nonzero()],\n                regression_targets[loss_classification.nonzero()],\n                beta=1 / 9,\n                reduction=\"mean\",\n            )\n            losses = {\n                \"loss_classifier\": loss_classification,\n                \"loss_box_reg\": loss_box_regression,\n            }\n        else:\n            boxes = self.predict_boxes_for_gt_classes(\n                proposals, scores\n            )\n            losses = {\n                \"loss_classifier\": F.cross_entropy(scores, labels.long()),\n                \"loss_box_reg\": smooth_l1_loss(\n                    proposal_deltas, regression_targets, beta=1 / 9\n                ),\n            }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        losses = {\n            \"loss_cls\": self.softmax_cross_entropy_loss(scores, proposals),\n            \"loss_box_reg\": self.smooth_l1_loss(\n                proposal_deltas, proposals, self.box2box_transform, self.smooth_l1_beta\n            ),\n        }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Get the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Get the ground truth classes and boxes from the proposals\n        gt_classes = []\n        gt_boxes = []\n        for proposals_per_image in proposals:\n            gt_classes_per_image = proposals_per_image.gt_classes\n            gt_boxes_per_image = proposals_per_image.gt_boxes\n            # guard against no boxes\n            if gt_boxes_per_image.numel() == 0:\n                gt_boxes_per_image = torch.zeros((1, 4), device=gt_boxes_per_image.device)\n            gt_classes.append(gt_classes_per_image)\n            gt_boxes.append(gt_boxes_per_image)\n\n        # Concatenate the ground truth classes and boxes into a single tensor\n        gt_classes = cat(gt_classes, dim=0)\n        gt_boxes = cat(gt_boxes, dim=0)\n\n        # Calculate the classification loss\n        loss_cls = self.loss_cls(scores, gt_classes) * self.loss_weight.get(\"loss_cls\", 1.0)\n\n        # Calculate the box regression loss\n        loss_box_reg = self.loss_box_reg(proposal_deltas, gt_boxes) * self.loss_weight.get(\n            \"loss_box_reg\", 1.0\n        )\n\n        # Return the losses as a dictionary\n        return {\n            \"loss_cls\": loss_cls,\n            \"loss_box_reg\": loss_box_reg,\n        }\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        losses = {\n            \"loss_cls\": self.softmax_cross_entropy_loss(scores, proposals),\n            \"loss_box_reg\": self.smooth_l1_loss(\n                proposal_deltas, proposals, self.box2box_transform, self.smooth_l1_beta\n            ),\n        }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Get the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Get the ground truth boxes and classes from the proposals\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_classes = [x.gt_classes for x in proposals]\n\n        # Calculate the classification loss using the F.cross_entropy function\n        loss_cls = F.cross_entropy(scores, gt_classes, reduction=\"mean\")\n\n        # Calculate the box regression loss using the smooth_l1_loss function\n        loss_box_reg = smooth_l1_loss(\n            proposal_deltas,\n            gt_boxes,\n            beta=0,\n            reduction=\"mean\",\n        )\n\n        # Return the losses as a dictionary\n        return {\n            \"loss_cls\": self.loss_weight[0] * loss_cls,\n            \"loss_box_reg\": self.loss_weight[1] * loss_box_reg,\n        }\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        losses = {}\n        if self.training:\n            labels, regression_targets = self.prepare_targets(proposals)\n            loss_classification = F.cross_entropy(scores, labels.long(), reduction=\"mean\")\n            loss_box_reg = smooth_l1_loss(\n                proposal_deltas,\n                regression_targets,\n                beta=1 / 9,\n                reduction=\"mean\",\n            )\n            losses = {\n                \"loss_classifier\": loss_classification,\n                \"loss_box_reg\": loss_box_reg,\n            }\n        else:\n            boxes, scores, labels = self.inference_targets(scores, proposal_deltas)\n            losses = {\n                \"loss_classifier\": F.cross_entropy(scores, labels.long(), reduction=\"mean\"),\n                \"loss_box_reg\": smooth_l1_loss(\n                    proposal_deltas,\n                    regression_targets,\n                    beta=1 / 9,\n                    reduction=\"mean\",\n                ),\n            }\n        return losses\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        losses = {}\n        dtype = scores.dtype\n        device = scores.device\n\n        gt_classes = []\n        gt_boxes = []\n        for proposals_per_image in proposals:\n            gt_classes_per_image = proposals_per_image.gt_classes.to(dtype=dtype)\n            gt_boxes_per_image = proposals_per_image.gt_boxes.to(device=device)\n            gt_classes.append(gt_classes_per_image)\n            gt_boxes.append(gt_boxes_per_image)\n\n        with torch.no_grad():\n            proposals = self.loss_evaluator.resample(proposals, gt_classes)\n\n        log_classification_loss = F.log_softmax(scores, dim=1)\n        classification_loss = F.nll_loss(\n            log_classification_loss,\n            proposals.gt_classes,\n            reduction=\"sum\",\n        )\n        losses[\"loss_cls\"] = classification_loss / scores.numel()\n\n        proposal_deltas = cat(proposal_deltas, dim=1)\n        box_loss = smooth_l1_loss(\n            proposal_deltas,\n            cat(proposals.proposal_deltas, dim=1),\n            beta=0.1,\n            reduction=\"sum\",\n        )\n        losses[\"loss_box_reg\"] = box_loss / proposal_deltas.numel()\n\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        losses = {\n            \"loss_cls\": F.cross_entropy(scores, labels),\n            \"loss_box_reg\": smooth_l1_loss(\n                proposal_deltas[positive_inds],\n                regression_targets[positive_inds],\n                beta=1 / 9,\n                size_average=self.box_reg_loss_type == \"smooth_l1\",\n            ),\n        }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        losses = {\n            \"loss_cls\": self.softmax_cross_entropy_loss(scores, proposals),\n            \"loss_box_reg\": self.smooth_l1_loss(\n                proposal_deltas[:, 4:], proposals.gt_boxes.tensor, beta=1 / 9, size_average=False\n            ),\n        }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        losses = {\n            \"loss_cls\": F.cross_entropy(scores, labels),\n            \"loss_box_reg\": smooth_l1_loss(\n                proposal_deltas[:, :4],\n                gt_boxes[:, :4],\n                beta=1 / 9,\n                reduction=\"mean\",\n            ),\n        }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        losses = {}\n\n        if self.training:\n            gt_classes = []\n            gt_boxes = []\n            for proposals_per_image in proposals:\n                gt_classes_per_image = proposals_per_image.gt_classes\n                gt_boxes_per_image = proposals_per_image.gt_boxes\n                gt_classes.append(gt_classes_per_image)\n                gt_boxes.append(gt_boxes_per_image)\n\n            # In FPN, concat_boxes() takes the different level of anchors and\n            # concatenate them in the first dimension.\n            gt_boxes = cat(gt_boxes, dim=0)\n            gt_classes = cat(gt_classes, dim=0)\n\n            # Log the number of positive/negative examples\n            positive_samples_per_image = gt_classes.gt(0).sum(dim=1)\n            negative_samples_per_image = (gt_classes == 0).sum(dim=1)\n            positive_samples = positive_samples_per_image.sum()\n            negative_samples = negative_samples_per_image.sum()\n            positive_samples_per_image = positive_samples_per_image.to(torch.float).mean()\n            negative_samples_per_image = negative_samples_per_image.to(torch.float).mean()\n            positive_samples_per_image = positive_samples_per_image.item()\n            negative_samples_per_image = negative_samples_per_image.item()\n            logger.info(\n                f\"{positive_samples} positive examples in {len(gt_boxes)} images. \"\n                f\"Positive samples per image: {positive_samples_per_image}. \"\n                f\"Negative samples: {negative_samples} ({negative_samples_per_image} per image)\"\n            )\n\n            # Log the classification accuracy\n            pred_classes = scores.argmax"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = []\n        for proposals_per_image in proposals:\n            gt_classes_per_image = proposals_per_image.gt_classes\n            # wrap the last value\n            if (\n                gt_classes_per_image.numel() == 0\n            ):  # pylint: disable=no-else-continue\n                gt_classes.append(gt_classes_per_image.new_zeros((1,)) - 1)\n            else:\n                gt_classes.append(gt_classes_per_image)\n        gt_classes = torch.cat(gt_classes, dim=0)\n        loss_cls = self.loss_cls(scores, gt_classes)\n\n        # parse box regression outputs\n        if any([len(p.proposal_boxes) == 0 for p in proposals]):\n            return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_cls.new_tensor(0)}\n\n        proposal_boxes = [x.proposal_boxes for x in proposals]\n        box_features = self.box_predictor(features, proposal_boxes)\n\n        loss_box_reg = self.loss_box_reg(box_features, gt_classes, proposal_deltas)\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_cls = TRACKER_REGISTRY.get(tracker_name)\n    tracker = tracker_cls(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_class = TRACKER_REGISTRY.get(tracker_name)\n    tracker = tracker_class(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker = TRACKER_REGISTRY.get(tracker_name)(cfg)\n    return tracker\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.name\n    tracker_cls = TRACKER_REGISTRY.get(tracker_name)\n    tracker = tracker_cls(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_class = TRACKER_REGISTRY.get(tracker_name)\n    tracker = tracker_class(cfg)\n\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker = TRACKER_REGISTRY.get(tracker_name)\n    tracker = tracker(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.name\n    tracker = TRACKER_REGISTRY.get(tracker_name)(cfg)\n\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker = TRACKER_REGISTRY.get(tracker_name)(cfg)\n\n    return tracker\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_cls = TRACKER_REGISTRY.get(tracker_name)\n    tracker = tracker_cls(cfg)\n    return tracker\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.name\n    tracker_cls = TRACKER_REGISTRY.get(tracker_name)\n    tracker = tracker_cls(cfg)\n    return tracker\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker = TRACKER_REGISTRY.get(tracker_name)(cfg)\n    return tracker\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.name\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)\n    tracker_head = tracker_head(cfg)\n\n    return tracker_head"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker = TRACKER_REGISTRY.get(tracker_name)(cfg)\n\n    return tracker\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.name\n    tracker = TRACKER_REGISTRY.get(tracker_name)(cfg)\n    return tracker\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker = TRACKER_REGISTRY.get(tracker_name)\n    return tracker(cfg)\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker = TRACKER_REGISTRY.get(tracker_name)(cfg)\n    return tracker\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_head = TRACKER_REGISTRY.get(tracker_name)\n\n    return tracker_head(cfg)\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_head = TRACKER_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.name\n    tracker = TRACKER_HEADS_REGISTRY.get(tracker_name)\n    tracker = tracker(cfg)\n    return tracker\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_head = TRACKER_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head\n\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.bbox_xform_clip)\n        dh = torch.clamp(dh, max=self.bbox_xform_clip)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.bbox_xform_clip)\n        dh = torch.clamp(dh, max=self.bbox_xform_clip)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.bbox_xform_clip)\n        dh = torch.clamp(dh, max=self.bbox_xform_clip)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        # x1\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        # x2\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        # y2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        # x1\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        # x2\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        # y2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        # x1\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        # x2\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        # y2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Check if the input deltas and boxes have the correct shape\n        assert deltas.shape[1] % 4 == 0, \"deltas' shape[1] % 4 should be 0, but it is {}\".format(\n            deltas.shape[1])\n\n        # Get the number of boxes and the number of classes\n        num_boxes = deltas.shape[0]\n        num_classes = deltas.shape[1] // 4\n\n        # Get the box coordinates and the box dimensions\n        boxes_x1 = boxes[:, 0]\n        boxes_y1 = boxes[:, 1]\n        boxes_x2 = boxes[:, 2]\n        boxes_y2 = boxes[:, 3]\n        boxes_w = boxes_x2 - boxes_x1 + 1\n        boxes_h = boxes_y2 - boxes_y1 + 1\n\n        # Get the box center coordinates and the box dimensions\n        ctr_x = boxes_x1 + 0.5 * boxes_w\n        ctr_y = boxes_y1 + 0.5 * boxes_h\n\n        # Get the transformation deltas for each class\n        deltas = deltas.view(num_boxes, num_classes, 4)\n        dx = deltas[:, :, 0::4]\n        dy = deltas[:, :, 1::4]\n        dw = deltas[:, :, 2::4]\n        dh = deltas[:, :, 3::4]\n\n        # Apply the transformation deltas to the box coordinates and dimensions\n        pred_ctr_x = dx * boxes_w.unsqueeze(1) + ctr_x.unsqueeze(1)\n        pred_ctr_y = dy * boxes_h.unsqueeze(1) + ctr_y.unsqueeze(1)\n        pred_w = torch.exp(dw) * boxes_w.unsqueeze(1)\n        pred_h = torch.exp(dh) * boxes_h.unsqueeze(1)"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        output = self.process(image)\n        if anno_type is None:\n            return output\n        elif isinstance(anno_type, str):\n            if anno_type in output:\n                return output[anno_type]\n            else:\n                return None\n        elif isinstance(anno_type, list):\n            return {k: output[k] for k in anno_type if k in output}\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image with the general annotation instruction\n        output = self.process(image)\n\n        # If no specific annotation type is requested, return the entire processed output\n        if anno_type is None:\n            return output\n\n        # If a specific annotation type is requested, filter the output and return only that annotation\n        if isinstance(anno_type, str):\n            return output[anno_type]\n\n        # If multiple annotation types are requested, filter the output and return a dictionary of those types found in the output\n        if isinstance(anno_type, list):\n            return {k: v for k, v in output.items() if k in anno_type}\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image with the general annotation instruction\n        output = self.processor.run(image)\n\n        # If a specific annotation type is requested, filter the output and return only that annotation\n        if anno_type is not None:\n            if isinstance(anno_type, str):\n                return output[anno_type]\n            elif isinstance(anno_type, list):\n                return {k: output[k] for k in anno_type}\n\n        # If no specific annotation type is requested, return the entire processed output\n        return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image using the general annotation instruction\n        output = self.process(image)\n\n        # If an annotation type is specified, filter the output\n        if anno_type is not None:\n            # If the annotation type is a list, return a dictionary of annotations\n            if isinstance(anno_type, list):\n                annotations = {}\n                for t in anno_type:\n                    if t in output:\n                        annotations[t] = output[t]\n                return annotations\n            # If the annotation type is a string, return a single annotation\n            elif isinstance(anno_type, str):\n                if anno_type in output:\n                    return output[anno_type]\n                else:\n                    return None\n        # If no annotation type is specified, return the entire processed output\n        else:\n            return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        output = self.process(image)\n        if anno_type is None:\n            return output\n        elif isinstance(anno_type, str):\n            if anno_type in output:\n                return output[anno_type]\n            else:\n                return None\n        elif isinstance(anno_type, list):\n            return {key: output[key] for key in anno_type if key in output}\n        else:\n            raise ValueError(\"anno_type must be a string or a list of strings.\")\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Call the annotator's run method to process the image and get the output\n        output = self.annotator.run(image)\n\n        # If an annotation type is specified, filter the output to only include that type\n        if anno_type is not None:\n            if isinstance(anno_type, str):\n                if anno_type in output:\n                    return output[anno_type]\n                else:\n                    return None\n            elif isinstance(anno_type, list):\n                filtered_output = {}\n                for t in anno_type:\n                    if t in output:\n                        filtered_output[t] = output[t]\n                return filtered_output\n\n        # If no annotation type is specified, return the entire processed output\n        return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if not anno_type:\n            return self.process(image)\n        elif isinstance(anno_type, str):\n            return self.process(image)[anno_type]\n        elif isinstance(anno_type, list):\n            return {k: self.process(image)[k] for k in anno_type}\n        else:\n            raise ValueError(\"Invalid anno_type parameter. Must be None, str, or list of str.\")"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image with the general annotation instruction\n        processed_output = self.process_image(image)\n\n        # If no specific annotation type is requested, return the entire processed output\n        if anno_type is None:\n            return processed_output\n\n        # If a single annotation type is requested, return that annotation if it exists in the output\n        if isinstance(anno_type, str):\n            if anno_type in processed_output:\n                return processed_output[anno_type]\n            else:\n                return None\n\n        # If multiple annotation types are requested, return a dictionary of those types found in the output\n        if isinstance(anno_type, list):\n            annotations = {}\n            for t in anno_type:\n                if t in processed_output:\n                    annotations[t] = processed_output[t]\n            return annotations\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        output = self.process(image)\n        if anno_type is None:\n            return output\n        elif isinstance(anno_type, str):\n            return output[anno_type]\n        elif isinstance(anno_type, list):\n            return {k: v for k, v in output.items() if k in anno_type}\n        else:\n            raise ValueError(f\"Invalid anno_type: {anno_type}\")\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image using the general annotation instruction\n        output = self.process(image)\n\n        # If no specific annotation type is requested, return the entire processed output\n        if anno_type is None:\n            return output\n\n        # If a specific annotation type is requested, filter the output and return the requested annotations\n        if isinstance(anno_type, str):\n            return output[anno_type]\n        else:\n            return {anno: output[anno] for anno in anno_type}\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.process(image)\n        elif isinstance(anno_type, str):\n            return self.process(image)[anno_type]\n        elif isinstance(anno_type, list):\n            return {k: v for k, v in self.process(image).items() if k in anno_type}\n        else:\n            raise ValueError(\n                \"Invalid value for anno_type. Must be None, str, or list of str.\"\n            )\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Run the processor with the given image\n        output = self.processor.run(image)\n\n        # If no annotation type is specified, return the entire output\n        if anno_type is None:\n            return output\n\n        # If the annotation type is a string, return the annotation of that type\n        if isinstance(anno_type, str):\n            if anno_type in output:\n                return output[anno_type]\n            else:\n                return None\n\n        # If the annotation type is a list, return a dictionary of annotations of those types\n        if isinstance(anno_type, list):\n            annotations = {}\n            for a_type in anno_type:\n                if a_type in output:\n                    annotations[a_type] = output[a_type]\n            return annotations\n\n        # If the annotation type is not a string or a list, raise an error\n        raise ValueError(\"Invalid annotation type: {}\".format(anno_type))"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image with the general annotation instruction\n        output = self.annotator.run(image)\n\n        # If no specific annotation type is requested, return the entire processed output\n        if anno_type is None:\n            return output\n\n        # If a single annotation type is requested, return only that annotation\n        if isinstance(anno_type, str):\n            return output[anno_type]\n\n        # If multiple annotation types are requested, return a dictionary of those types found in the output\n        else:\n            return {k: v for k, v in output.items() if k in anno_type}\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        return self.annotator.run(image, anno_type=anno_type)\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        return self.annotator.run(image, anno_type)\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Check if the anno_type parameter is a string or a list of strings\n        if isinstance(anno_type, str):\n            # If it's a string, convert it to a list of strings\n            anno_type = [anno_type]\n\n        # Process the image with the general annotation instruction\n        output = self.process(image)\n\n        # If an annotation type is specified, filter the output\n        if anno_type:\n            # Create an empty dictionary to store the filtered annotations\n            filtered_output = {}\n            # Iterate through the annotation types\n            for at in anno_type:\n                # If the annotation type is found in the output, add it to the filtered output\n                if at in output:\n                    filtered_output[at] = output[at]\n            # If there are no filtered annotations, return None\n            if not filtered_output:\n                return None\n            # If there's only one filtered annotation, return it as a single value\n            if len(filtered_output) == 1:\n                return list(filtered_output.values())[0]\n            # If there are multiple filtered annotations, return them as a dictionary\n            return filtered_output\n\n        # If no annotation type is specified, return the entire processed output\n        return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image with the general annotation instruction\n        output = self.general_anno(image)\n\n        # If no annotation type is specified, return the entire output\n        if anno_type is None:\n            return output\n\n        # If a single annotation type is specified, return that annotation\n        if isinstance(anno_type, str):\n            return output[anno_type]\n\n        # If a list of annotation types is specified, return a dictionary of those annotations\n        if isinstance(anno_type, list):\n            return {k: v for k, v in output.items() if k in anno_type}\n\n        # If an unsupported type is specified, raise a TypeError\n        raise TypeError(f\"Unsupported annotation type: {anno_type}\")\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image with the general annotation instruction\n        output = self.process(image)\n\n        # If no specific annotation type is requested, return the entire processed output\n        if anno_type is None:\n            return output\n\n        # If a specific annotation type is requested, filter the output and return the requested annotations\n        if isinstance(anno_type, str):\n            return output.get(anno_type, None)\n        elif isinstance(anno_type, list):\n            return {k: v for k, v in output.items() if k in anno_type}\n        else:\n            raise ValueError(\"Invalid anno_type parameter. It should be either a string or a list of strings.\")\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        output = self.processor.process(image)\n\n        if anno_type is None:\n            return output\n        elif isinstance(anno_type, str):\n            if anno_type in output:\n                return output[anno_type]\n            else:\n                return None\n        elif isinstance(anno_type, list):\n            filtered_output = {}\n            for a_type in anno_type:\n                if a_type in output:\n                    filtered_output[a_type] = output[a_type]\n            return filtered_output\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        output = self.processor.run(image)\n\n        if anno_type is None:\n            return output\n        elif isinstance(anno_type, str):\n            return output[anno_type]\n        else:\n            return {key: output[key] for key in anno_type if key in output}\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize_string(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url, score in self.inverted_index[keyword].items():\n                    if url not in scores:\n                        scores[url] = score\n                    else:\n                        scores[url] += score\n\n        # Aggregate the scores for each URL\n        aggregated_scores = {}\n        for url, score in scores.items():\n            aggregated_scores[url] = score\n\n        return aggregated_scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize_text(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.index:\n                for url, score in self.index[keyword].items():\n                    if url not in scores:\n                        scores[url] = score\n                    else:\n                        scores[url] += score\n\n        # Aggregate the scores for URLs\n        aggregated_scores = {}\n        for url, score in scores.items():\n            aggregated_scores[url] = score\n\n        return aggregated_scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url, score in self.inverted_index[keyword].items():\n                    if url in scores:\n                        scores[url] += score\n                    else:\n                        scores[url] = score\n\n        # Aggregate the scores for URLs\n        aggregated_scores = {}\n        for url, score in scores.items():\n            aggregated_scores[url] = score\n\n        return aggregated_scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        normalized_query = normalize_string(query)\n\n        # Split the normalized query string into keywords\n        keywords = split_into_keywords(normalized_query)\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            for url, score in self.bm25.get_scores(keyword).items():\n                scores[url] = scores.get(url, 0) + score\n\n        # Aggregate the scores for URLs\n        aggregated_scores = {url: score for url, score in scores.items() if score > 0}\n\n        return aggregated_scores\n\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize_text(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url, score in self.inverted_index[keyword].items():\n                    if url in scores:\n                        scores[url] += score\n                    else:\n                        scores[url] = score\n\n        # Aggregate the scores for URLs\n        aggregated_scores = {}\n        for url, score in scores.items():\n            aggregated_scores[url] = score\n\n        return aggregated_scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = normalize(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url, score in self.inverted_index[keyword].items():\n                    if url not in scores:\n                        scores[url] = 0\n                    scores[url] += score\n\n        # Aggregate the scores for URLs\n        aggregated_scores = {}\n        for url, score in scores.items():\n            aggregated_scores[url] = score\n\n        return aggregated_scores\n\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = self.split_query(query)\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url, score in self.inverted_index[keyword].items():\n                    if url in scores:\n                        scores[url] += score\n                    else:\n                        scores[url] = score\n\n        # Aggregate the scores for each URL\n        aggregated_scores = {}\n        for url, score in scores.items():\n            aggregated_scores[url] = score\n\n        return aggregated_scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        normalized_query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = self.split_query(normalized_query)\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.index:\n                for url, score in self.index[keyword].items():\n                    if url not in scores:\n                        scores[url] = 0\n                    scores[url] += score\n\n        # Aggregate the scores for each URL\n        aggregated_scores = {}\n        for url, score in scores.items():\n            aggregated_scores[url] = score\n\n        return aggregated_scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize_string(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.index:\n                for url, score in self.index[keyword].items():\n                    if url not in scores:\n                        scores[url] = 0\n                    scores[url] += score\n\n        # Aggregate the scores for URLs\n        aggregated_scores = {}\n        for url, score in scores.items():\n            aggregated_scores[url] = score\n\n        return aggregated_scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = self.split_query(query)\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = self.bm25(keywords)\n\n        # Aggregate the scores for each URL\n        aggregated_scores = {}\n        for url, url_scores in scores.items():\n            aggregated_scores[url] = sum(url_scores.values())\n\n        return aggregated_scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize(query)\n\n        # Split the query string into keywords\n        keywords = query.split()\n\n        # Initialize an empty dictionary to store the scores for each URL\n        scores = {}\n\n        # Iterate over each keyword in the query\n        for keyword in keywords:\n\n            # Get the inverted index for the keyword\n            inverted_index = self.inverted_index.get(keyword, {})\n\n            # Iterate over each URL in the inverted index\n            for url, score in inverted_index.items():\n\n                # If the URL is not in the scores dictionary, add it with the BM25 score\n                if url not in scores:\n                    scores[url] = score\n\n                # If the URL is already in the scores dictionary, add the BM25 score to the existing value\n                else:\n                    scores[url] += score\n\n        # Return the scores dictionary\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string and split it into keywords\n        query = self.normalize_text(query)\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url, score in self.inverted_index[keyword].items():\n                    if url not in scores:\n                        scores[url] = score\n                    else:\n                        scores[url] += score\n\n        # Aggregate the scores for URLs\n        aggregated_scores = {}\n        for url, score in scores.items():\n            aggregated_scores[url] = score\n\n        return aggregated_scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = self.normalize_query(query)\n        keywords = query.split()\n        scores = {}\n\n        for keyword in keywords:\n            for url in self.index:\n                if keyword in self.index[url]:\n                    if url not in scores:\n                        scores[url] = 0\n                    scores[url] += self.bm25_score(keyword, url)\n\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.index:\n                for url, score in self.index[keyword].items():\n                    if url in scores:\n                    # Aggregate the scores for each URL\n                    scores[url] += score\n            else:\n                # If the keyword is not found in the index, assign a score of 0\n                scores[url] = 0\n\n        # Return the aggregated scores for each URL\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = self.normalize_query(query)\n        keywords = self.split_query(query)\n\n        scores = {}\n\n        for keyword in keywords:\n            if keyword in self.index:\n                for url, score in self.index[keyword].items():\n                    if url not in scores:\n                        scores[url] = score\n                    else:\n                        scores[url] += score\n\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string and split it into keywords\n        keywords = self.normalize(query).split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            for url, tf in self.index[keyword].items():\n                idf = self.idf[keyword]\n                bm25 = self.bm25(tf, idf)\n                if url in scores:\n                    scores[url] += bm25\n                else:\n                    scores[url] = bm25\n\n        # Return the aggregated scores for each URL\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = self.normalize(query)\n        keywords = query.split()\n        scores = {}\n        for keyword in keywords:\n            for url in self.index:\n                if keyword in self.index[url]:\n                    if url not in scores:\n                        scores[url] = 0\n                    scores[url] += self.calculate_bm25_score(keyword, url, query)\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = normalize_text(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Initialize a dictionary to store the BM25 scores for each URL\n        scores = defaultdict(float)\n\n        # Calculate the BM25 score for each keyword across URLs\n        for keyword in keywords:\n            if keyword in self.index:\n                for url, score in self.index[keyword].items():\n                    scores[url] += score\n\n        # Aggregate the scores for each URL\n        aggregated_scores = {url: sum(score) for url, score in scores.items()}\n\n        # Return the aggregated scores as a dictionary\n        return aggregated_scores\n\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = self.normalize_string(query)\n        keywords = query.split()\n\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url in self.inverted_index[keyword]:\n                    if url not in scores:\n                        scores[url] = self.bm25(\n                            keyword, url, self.inverted_index, self.document_lengths\n                        )\n                    else:\n                        scores[url] += self.bm25(\n                            keyword, url, self.inverted_index, self.document_lengths\n                        )\n\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = self.normalize(query)\n        keywords = query.split()\n\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url, score in self.inverted_index[keyword].items():\n                    if url in scores:\n                        scores[url] += score\n                    else:\n                        scores[url] = score\n\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for doc in documents:\n            self.index(doc[0], doc[1])\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for document in documents:\n            self.index(document[0], document[1])\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        boxes = self.tensor\n        height, width = box_size\n        boxes = boxes.clone()\n        boxes[:, 4::5].clamp_(min=-90, max=0)\n        inds = (boxes[:, 4] >= -clip_angle_threshold).nonzero().squeeze(1)\n        boxes[inds, 0::5] = boxes[inds, 0::5].clamp(min=0, max=width)\n        boxes[inds, 1::5] = boxes[inds, 1::5].clamp(min=0, max=height)\n        boxes[inds, 2::5] = boxes[inds, 2::5].clamp(min=0, max=width)\n        boxes[inds, 3::5] = boxes[inds, 3::5].clamp(min=0, max=height)\n        boxes[:, 2::5] -= boxes[:, 0::5]\n        boxes[:, 3::5] -= boxes[:, 1::5]\n        boxes[:, 0::5] += boxes[:, 2::5] / 2\n        boxes[:, 1::5] += boxes[:, 3::5] / 2\n        return boxes\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        boxes = self.tensor\n        height, width = box_size\n\n        boxes = boxes.clone()\n\n        boxes[:, 4] = (boxes[:, 4] + 180) % 360 - 180\n\n        clipped_inds = (torch.abs(boxes[:, 4]) < clip_angle_threshold) & (\n            (boxes[:, 2] > boxes[:, 3]) & (boxes[:, 2] > 0.5 * width) & (boxes[:, 3] > 0.5 * height)\n        )\n\n        x1 = boxes[clipped_inds, 0] - boxes[clipped_inds, 2] / 2.0\n        y1 = boxes[clipped_inds, 1] - boxes[clipped_inds, 3] / 2.0\n        x2 = boxes[clipped_inds, 0] + boxes[clipped_inds, 2] / 2.0\n        y2 = boxes[clipped_inds, 1] + boxes[clipped_inds, 3] / 2.0\n\n        x1 = torch.clamp(x1, min=0, max=width)\n        x2 = torch.clamp(x2, min=0, max=width)\n        y1 = torch.clamp(y1, min=0, max=height)\n        y2 = torch.clamp(y2, min=0, max=height)\n\n        boxes[clipped_inds, 0] = (x1 + x2) / 2.0\n        boxes[clipped_inds, 2] = x2 - x1\n        boxes[clipped_inds, 1] = (y1 + y2) / 2.0\n        boxes[clipped_inds, 3] = y2 - y1\n\n        self.tensor = boxes\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles to be within the range (-180, 180] degrees\n        angles_deg = self.tensor[:, 4] % 360\n\n        # Identify the indices of the boxes that are nearly horizontal\n        nearly_horizontal_indices = torch.where(\n            torch.abs(angles_deg) <= clip_angle_threshold\n        )[0]\n\n        # Convert the boxes to (x1, y1, x2, y2) format for clipping\n        boxes_x1y1x2y2 = self.tensor.clone()\n        boxes_x1y1x2y2[:, 0] = self.tensor[:, 0] - self.tensor[:, 2] / 2.0\n        boxes_x1y1x2y2[:, 1] = self.tensor[:, 1] - self.tensor[:, 3] / 2.0\n        boxes_x1y1x2y2[:, 2] = self.tensor[:, 0] + self.tensor[:, 2] / 2.0\n        boxes_x1y1x2y2[:, 3] = self.tensor[:, 1] + self.tensor[:, 3] / 2.0\n\n        # Clip the x and y coordinates of the nearly horizontal boxes\n        boxes_x1y1x2y2[nearly_horizontal_indices, 0] = torch.clamp(\n            boxes_x1y1x2y2[nearly_horizontal_indices, 0], 0, box_size[1]\n        )\n        boxes_x1y1x2y2[nearly_horizontal_indices, 1] = torch.clamp(\n            boxes_x1y1x2y2[nearly_horizontal_indices, 1], 0, box_size[0]\n        )\n        boxes_x1y1x2y2[nearly_horizontal_indices, 2] = torch.clamp(\n            boxes_x1y1x2y2[nearly_horizontal_ind"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to (-180, 180]\n        angles = self.tensor[:, 4] % 180\n\n        # Identify boxes that are nearly horizontal\n        horizontal_indices = torch.where(torch.abs(angles) < clip_angle_threshold)[0]\n\n        # Convert boxes to (x1, y1, x2, y2) representation\n        boxes_xyxy = self.tensor.clone()\n        boxes_xyxy[:, 0:2] -= boxes_xyxy[:, 2:4] / 2\n        boxes_xyxy[:, 2:4] += boxes_xyxy[:, 0:2]\n\n        # Clip x and y coordinates\n        boxes_xyxy[horizontal_indices, 0::2].clamp_(min=0, max=box_size[1])\n        boxes_xyxy[horizontal_indices, 1::2].clamp_(min=0, max=box_size[0])\n\n        # Convert boxes back to (center x, center y, width, height, angle) representation\n        self.tensor[:, 0:2] = (boxes_xyxy[:, 0:2] + boxes_xyxy[:, 2:4]) / 2\n        self.tensor[:, 2:4] = boxes_xyxy[:, 2:4] - boxes_xyxy[:, 0:2]\n\n        # Ensure numerical errors do not increase box sizes\n        self.tensor[:, 2:4].clamp_(min=0)\n\n        return self"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles to be within the range (-180, 180] degrees\n        self.tensor[:, 4] = self.tensor[:, 4] % 180\n\n        # Identify the indices of the boxes that are nearly horizontal\n        horizontal_indices = torch.where(\n            torch.abs(self.tensor[:, 4]) < clip_angle_threshold\n        )[0]\n\n        # Convert the boxes to (x1, y1, x2, y2) representation\n        boxes_xyxy = self.tensor.clone()\n        boxes_xyxy[:, 0:2] = self.tensor[:, 0:2] - self.tensor[:, 2:4] / 2\n        boxes_xyxy[:, 2:4] = self.tensor[:, 0:2] + self.tensor[:, 2:4] / 2\n\n        # Clamp the x and y coordinates to ensure they do not exceed the box size limits\n        boxes_xyxy[horizontal_indices, 0::2] = torch.clamp(\n            boxes_xyxy[horizontal_indices, 0::2], min=0, max=box_size[1]\n        )\n        boxes_xyxy[horizontal_indices, 1::2] = torch.clamp(\n            boxes_xyxy[horizontal_indices, 1::2], min=0, max=box_size[0]\n        )\n\n        # Convert the boxes back to (center x, center y, width, height, angle) representation\n        self.tensor[:, 0:2] = (boxes_xyxy[:, 0:2] + boxes_xyxy[:, 2:4]) / 2\n        self.tensor[:, 2:4] = boxes_xyxy[:, 2:4] - boxes_xyxy[:, 0:2]\n\n        # Ensure numerical errors do not increase the box sizes\n        self.tensor[:, 2:4] = torch.max(self.tensor[:, 2:4], torch.zeros_like(self.tensor"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to (-180, 180] degrees\n        self.tensor[..., 4] = (self.tensor[..., 4] + 180) % 360 - 180\n\n        # Identify indices of boxes that are nearly horizontal\n        horizontal_indices = torch.where(\n            torch.abs(self.tensor[..., 4]) <= clip_angle_threshold\n        )[0]\n\n        # Convert the boxes to (x1, y1, x2, y2) format\n        boxes_xyxy = self.tensor.clone()\n        boxes_xyxy[..., 2] = self.tensor[..., 0] + self.tensor[..., 2] / 2.0\n        boxes_xyxy[..., 3] = self.tensor[..., 1] + self.tensor[..., 3] / 2.0\n        boxes_xyxy[..., 0] = self.tensor[..., 0] - self.tensor[..., 2] / 2.0\n        boxes_xyxy[..., 1] = self.tensor[..., 1] - self.tensor[..., 3] / 2.0\n\n        # Clamp the x and y coordinates\n        boxes_xyxy[horizontal_indices, 0::2].clamp_(min=0, max=box_size[1])\n        boxes_xyxy[horizontal_indices, 1::2].clamp_(min=0, max=box_size[0])\n\n        # Convert the boxes back to (center x, center y, width, height, angle) format\n        self.tensor[..., 0] = (boxes_xyxy[..., 0] + boxes_xyxy[..., 2]) / 2.0\n        self.tensor[..., 1] = (boxes_xyxy[..., 1] + boxes_xyxy[..., 3]) / 2.0\n        self.tensor[..., 2] = boxes_xyxy[..., 2] - boxes_xyxy[..."}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles to be within the range (-180, 180] degrees\n        angles_deg = (self.tensor[:, 4] * 180.0 / math.pi) % 360.0\n        angles_deg[angles_deg > 180.0] -= 360.0\n\n        # Identify the indices of the boxes that are nearly horizontal\n        horizontal_indices = torch.where(torch.abs(angles_deg) < clip_angle_threshold)[0]\n\n        # Convert the boxes to (x1, y1, x2, y2) representation for horizontal boxes\n        horizontal_boxes = self.tensor[horizontal_indices].clone()\n        horizontal_boxes[:, 2:] += horizontal_boxes[:, :2]\n\n        # Clamp the x and y coordinates to ensure they do not exceed the box_size limits\n        horizontal_boxes[:, [0, 2]] = torch.clamp(horizontal_boxes[:, [0, 2]], min=0, max=box_size[1])\n        horizontal_boxes[:, [1, 3]] = torch.clamp(horizontal_boxes[:, [1, 3]], min=0, max=box_size[0])\n\n        # Convert the boxes back to (center x, center y, width, height, angle) representation\n        horizontal_boxes[:, 2:] -= horizontal_boxes[:, :2]\n        self.tensor[horizontal_indices] = horizontal_boxes\n\n        # Convert the boxes to (x1, y1, x2, y2) representation for non-horizontal boxes\n        non_horizontal_indices = torch.where(torch.abs(angles_deg) >= clip_angle_threshold)[0]\n        non_horizontal_boxes = self.tensor[non_horizontal_indices].clone()\n        non_horizontal_boxes[:, 2:] += non_horizontal_boxes[:, :2]\n\n        # Clamp the x and y coordinates to ensure they do not exceed the box_size limits\n        non_horizontal"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        boxes = self.tensor\n        height, width = box_size\n        pi = torch.tensor(math.pi)\n        boxes = boxes.clone()\n        boxes[:, 4] = (boxes[:, 4] + 180) % 360 - 180\n        abs_angle = torch.abs(boxes[:, 4])\n        small_angle_inds = torch.nonzero(abs_angle <= clip_angle_threshold).squeeze(1)\n        if len(small_angle_inds) > 0:\n            x1 = boxes[small_angle_inds, 0] - boxes[small_angle_inds, 2] / 2\n            x2 = boxes[small_angle_inds, 0] + boxes[small_angle_inds, 2] / 2\n            y1 = boxes[small_angle_inds, 1] - boxes[small_angle_inds, 3] / 2\n            y2 = boxes[small_angle_inds, 1] + boxes[small_angle_inds, 3] / 2\n            x1 = torch.clamp(x1, min=0, max=width)\n            x2 = torch.clamp(x2, min=0, max=width)\n            y1 = torch.clamp(y1, min=0, max=height)\n            y2 = torch.clamp(y2, min=0, max=height)\n            boxes[small_angle_inds, 0] = (x1 + x2) / 2\n            boxes[small_angle_inds, 1] = (y1 + y2) / 2\n            boxes[small_angle_inds, 2] = x2 - x1\n            boxes[small_angle_inds, 3] = y2 - y1\n        self.tensor = boxes\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        boxes = self.tensor\n        height, width = box_size\n        boxes = boxes.clone()  # avoid in-place modification\n\n        boxes[:, 4] = (boxes[:, 4] + 180) % 360 - 180\n\n        # Clip boxes with near horizontal angle to the image boundaries.\n        # Avoid clipping boxes with nearly horizontal angle that also have large\n        # aspect ratios.\n        horizontal_indices = torch.where(\n            torch.logical_and(\n                torch.abs(boxes[:, 4]) < clip_angle_threshold,\n                boxes[:, 2] / boxes[:, 3] < (width / height) * 1.5,\n            )\n        )[0]\n        boxes[horizontal_indices, 0] = boxes[horizontal_indices, 0].clamp(0, width)\n        boxes[horizontal_indices, 1] = boxes[horizontal_indices, 1].clamp(0, height)\n\n        # Clip boxes with large aspect ratios to the image boundaries.\n        large_aspect_ratio_indices = torch.where(\n            boxes[:, 2] / boxes[:, 3] > (width / height) * 1.5\n        )[0]\n        boxes[large_aspect_ratio_indices, 0] = boxes[large_aspect_ratio_indices, 0].clamp(\n            0, width\n        )\n        boxes[large_aspect_ratio_indices, 1] = boxes[large_aspect_ratio_indices, 1].clamp(\n            0, height\n        )\n\n        # Clip rotated boxes to the image boundaries.\n        xc, yc, w, h, a = boxes.unbind(dim=1)\n        x1, y1, x2, y2 = _boxes_to_x1y1x2y2(xc, yc, w, h, a)\n        boxes = torch.stack((x1, y1, x2, y2), dim=1)\n       "}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to (-180, 180]\n        angles = self.tensor[..., 4] % 180\n\n        # Identify indices of nearly horizontal boxes\n        nearly_horizontal = torch.abs(angles) <= clip_angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation for clipping\n        boxes_xyxy = self.tensor[nearly_horizontal, :4].clone()\n        boxes_xyxy[:, :2] -= boxes_xyxy[:, 2:] / 2  # top-left\n        boxes_xyxy[:, 2:] += boxes_xyxy[:, :2]  # bottom-right\n\n        # Clip x and y coordinates\n        boxes_xyxy[:, [0, 2]] = boxes_xyxy[:, [0, 2]].clamp(min=0, max=box_size[1])\n        boxes_xyxy[:, [1, 3]] = boxes_xyxy[:, [1, 3]].clamp(min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        clipped_boxes = boxes_xyxy.clone()\n        clipped_boxes[:, :2] = (boxes_xyxy[:, :2] + boxes_xyxy[:, 2:]) / 2  # center x, center y\n        clipped_boxes[:, 2:] -= clipped_boxes[:, :2]  # width, height\n\n        # Update tensor attribute\n        self.tensor[nearly_horizontal] = clipped_boxes\n\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        if self.tensor.numel() == 0:\n            return\n\n        assert len(box_size) == 2\n        height, width = box_size\n        self.tensor[:, 0::5].clamp_(min=0, max=width)\n        self.tensor[:, 1::5].clamp_(min=0, max=height)\n        self.tensor[:, 2::5].clamp_(min=0, max=width)\n        self.tensor[:, 3::5].clamp_(min=0, max=height)\n\n        # normalize angle to (-180, 180]\n        angle = self.tensor[:, 4::5]\n        angle = (angle + 180) % 360 - 180\n\n        # convert to horizontal boxes and clip\n        horizontal_mask = torch.abs(angle) < clip_angle_threshold\n        horizontal_boxes = self.tensor[horizontal_mask]\n        horizontal_boxes[:, 0::5] = torch.clamp(horizontal_boxes[:, 0::5], min=0, max=width)\n        horizontal_boxes[:, 1::5] = torch.clamp(horizontal_boxes[:, 1::5], min=0, max=height)\n        horizontal_boxes[:, 2::5] = torch.clamp(horizontal_boxes[:, 2::5], min=0, max=width)\n        horizontal_boxes[:, 3::5] = torch.clamp(horizontal_boxes[:, 3::5], min=0, max=height)\n        self.tensor[horizontal_mask] = horizontal_boxes\n\n        # convert to vertical boxes and clip\n        vertical_mask = torch.abs(angle) >= (90 - clip_angle_threshold)\n        vertical_boxes = self.tensor[vertical_mask]\n        vertical_boxes[:, 0::5] = torch.clamp(vertical_boxes[:, 0::5], min=0, max=width)\n        vertical_boxes[:, 1::5"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to be within [-180, 180] degrees\n        angles_normalized = normalize_angles(self.angles)\n\n        # Identify boxes that are nearly horizontal\n        nearly_horizontal_mask = torch.abs(angles_normalized) < clip_angle_threshold\n\n        # Convert boxes to (x1, y1, x2, y2) representation for clipping\n        boxes_xyxy = self.tensor.clone()\n        boxes_xyxy[nearly_horizontal_mask, 2:] = boxes_xyxy[nearly_horizontal_mask, 2:] + boxes_xyxy[nearly_horizontal_mask, :2]\n\n        # Clamp x and y coordinates to ensure they fit within the box size limits\n        boxes_xyxy[:, [0, 2]] = boxes_xyxy[:, [0, 2]].clamp(min=0, max=box_size[1])\n        boxes_xyxy[:, [1, 3]] = boxes_xyxy[:, [1, 3]].clamp(min=0, max=box_size[0])\n\n        # Convert boxes back to (center x, center y, width, height, angle) representation\n        self.tensor[nearly_horizontal_mask, :2] = (boxes_xyxy[nearly_horizontal_mask, 0] + boxes_xyxy[nearly_horizontal_mask, 2]) / 2\n        self.tensor[nearly_horizontal_mask, 2] = boxes_xyxy[nearly_horizontal_mask, 2] - boxes_xyxy[nearly_horizontal_mask, 0]\n        self.tensor[nearly_horizontal_mask, 3] = boxes_xyxy[nearly_horizontal_mask, 3] - boxes_xyxy[nearly_horizontal_mask, 1]\n\n        # Ensure numerical errors do not increase box sizes\n        self.tensor[:, 2:] = self.tensor[:, 2:].clamp(min=0)\n\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        if self.tensor.numel() == 0:\n            return\n        boxes = self.tensor\n        assert boxes.shape[-1] == 5, 'RotatedBoxes should have shapes of (*, 5), get {} instead.'.format(\n            boxes.shape)\n        original_type = boxes.dtype\n        boxes = boxes.float()  # avoid all_finite check\n        boxes[..., 2:4].clamp_(min=1.0)\n\n        # convert angle to (0, 180]\n        boxes[..., 4] = (boxes[..., 4] + 180) % 360 - 180\n\n        # convert angle to (-90, 90]\n        boxes[..., 2:4:2].clamp_(min=1e-3)  # avoid theta=0\n        boxes[..., 4] = torch.where(boxes[..., 4] > 90, boxes[..., 4] - 180, boxes[..., 4])\n        boxes[..., 4] = torch.where(boxes[..., 4] < -90, boxes[..., 4] + 180, boxes[..., 4])\n\n        # convert boxes to horizontal boxes\n        is_near_horizontal = (abs(boxes[..., 4]) < clip_angle_threshold)\n        boxes_near_horizontal = boxes[is_near_horizontal]\n        boxes_not_near_horizontal = boxes[~is_near_horizontal]\n        converted_boxes_near_horizontal = self.convert_to_horizontal_boxes(\n            boxes_near_horizontal)\n        converted_boxes = torch.cat(\n            (converted_boxes_near_horizontal, boxes_not_near_horizontal), dim=0)\n\n        # clip boxes\n        converted_boxes[..., 0::2].clamp_(min=0, max=box_size[1])\n        converted_boxes[..., 1::2].clamp_(min=0, max=box_size[0])"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles of the boxes to be within the range (-180, 180] degrees\n        self.tensor[..., 4] = (self.tensor[..., 4] + 180) % 360 - 180\n\n        # Identify the indices of the boxes that are nearly horizontal\n        nearly_horizontal_indices = torch.where(\n            torch.abs(self.tensor[..., 4]) < clip_angle_threshold\n        )[0]\n\n        # Clip the boxes that are nearly horizontal\n        if len(nearly_horizontal_indices) > 0:\n            # Convert the boxes to (x1, y1, x2, y2) representation\n            boxes_xyxy = self.tensor[nearly_horizontal_indices].clone()\n            boxes_xyxy[..., 2:4] += boxes_xyxy[..., :2]\n\n            # Clamp the x and y coordinates to ensure they do not exceed the box_size limits\n            boxes_xyxy[..., [0, 2]] = boxes_xyxy[..., [0, 2]].clamp(0, box_size[1])\n            boxes_xyxy[..., [1, 3]] = boxes_xyxy[..., [1, 3]].clamp(0, box_size[0])\n\n            # Convert the boxes back to (center x, center y, width, height, angle) representation\n            boxes_xywha = boxes_xyxy.clone()\n            boxes_xywha[..., 2:4] -= boxes_xywha[..., :2]\n\n            # Ensure numerical errors do not increase the box sizes\n            boxes_xywha[..., 2:4] = boxes_xywha[..., 2:4].clamp(min=0)\n\n            # Update the boxes in the RotatedBoxes instance\n            self.tensor[nearly_horizontal_indices] = boxes_xywha\n\n        # Normalize the angles of the boxes to be within the range (-90, 90] degrees\n        self."}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        assert len(box_size) == 2, \"box_size should be of length 2\"\n        assert clip_angle_threshold >= 0.0, \"clip_angle_threshold should be non-negative\"\n\n        height, width = box_size\n\n        boxes = self.tensor\n        boxes = boxes.clone()\n\n        boxes[:, 4::5] = (boxes[:, 4::5] + 180) % 360 - 180\n\n        # Clip boxes with angles close to 0 or 90 degrees\n        x1 = boxes[:, 0::5]\n        y1 = boxes[:, 1::5]\n        x2 = boxes[:, 2::5]\n        y2 = boxes[:, 3::5]\n\n        delta_x = x2 - x1\n        delta_y = y2 - y1\n        angles = torch.atan2(delta_y, delta_x) * 180 / math.pi\n        abs_angles = torch.abs(angles)\n\n        # Find boxes with angles close to 0 or 90 degrees\n        small_angle_indices = torch.nonzero(\n            torch.logical_or(\n                torch.abs(90 - abs_angles) < clip_angle_threshold,\n                torch.abs(abs_angles) < clip_angle_threshold,\n            )\n        ).squeeze(1)\n\n        # Clip boxes with angles close to 0 or 90 degrees\n        boxes[small_angle_indices, 0::5] = torch.clamp(\n            boxes[small_angle_indices, 0::5], min=0, max=width\n        )\n        boxes[small_angle_indices, 1::5] = torch.clamp(\n            boxes[small_angle_indices, 1::5], min=0, max=height\n        )\n        boxes[small_angle_indices, 2::5] = torch.clamp(\n            boxes[small_angle_indices, 2::5], min="}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.tensor = self.tensor.clone()\n        boxes = self.tensor\n        boxes[:, 2::4].clamp_(min=1e-3)\n        boxes[:, 4::6].clamp_(min=1e-3)\n        boxes[:, 2::6].clamp_(max=box_size[1])\n        boxes[:, 4::6].clamp_(max=box_size[0])\n        boxes[:, 0::6].clamp_(min=0, max=box_size[1])\n        boxes[:, 1::6].clamp_(min=0, max=box_size[0])\n        boxes[:, 3::6].clamp_(min=0, max=box_size[1])\n        boxes[:, 5::6].clamp_(min=0, max=box_size[0])\n        return self\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        boxes = self.tensor\n        if boxes.numel() == 0:\n            return\n        assert boxes.shape[-1] == 5, 'RotatedBoxes should have shapes of (*, 5), get {} instead.'.format(\n            boxes.shape)\n        original_type = boxes.dtype\n        boxes = boxes.float()  # avoid casts, for example, from torch.uint8\n\n        boxes = boxes.clone()  # avoid in-place modification\n\n        # normalize angle into [0, 180]\n        boxes[..., 4] = (boxes[..., 4] + 180) % 360 - 180\n\n        # convert boxes with angle close to 0 or 180 to boxes with angle close to 90 or -90\n        # make clockwise box with angle close to 0 or 180 to counter-clockwise, and make counter-clockwise box with angle close to 0 or 180 to clockwise\n        # need to convert the angle to radians to use torch.sin and torch.cos\n        radian = boxes[..., 4] * (math.pi / 180.0)\n        # find the offset of x and y according to the angle\n        # clockwise, x offset is width, y offset is 0; counter-clockwise, x offset is 0, y offset is height\n        offset_x = torch.sin(radian) * boxes[..., 2]\n        offset_y = torch.cos(radian) * boxes[..., 3]\n        # if angle is 0 or 180, convert it to 90 or -90\n        offset_x[torch.abs(offset_x) < 1e-5] = boxes[..., 3][torch.abs(offset_x) < 1e-5]\n        offset_y[torch.abs(offset_y) < 1e-5] = boxes[..., 2][torch.abs(offset_y) < 1e-5]\n        # x1 = ctr_x + offset_x / 2"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to be within the range (-180, 180] degrees\n        angles_normalized = self.tensor[..., 4] % 180\n\n        # Identify indices of boxes that are nearly horizontal\n        idx = torch.where(torch.abs(angles_normalized) < clip_angle_threshold)\n\n        # Convert the boxes with nearly horizontal angles to (x1, y1, x2, y2) representation\n        boxes_near_horizontal = self.tensor[idx][..., :4]\n        x_ctr, y_ctr, width, height = boxes_near_horizontal.unbind(dim=-1)\n        x1 = x_ctr - 0.5 * width\n        y1 = y_ctr - 0.5 * height\n        x2 = x_ctr + 0.5 * width\n        y2 = y_ctr + 0.5 * height\n        boxes_near_horizontal = torch.stack((x1, y1, x2, y2), dim=-1)\n\n        # Clamp the x and y coordinates\n        boxes_near_horizontal[..., ::2].clamp_(min=0, max=box_size[1])\n        boxes_near_horizontal[..., 1::2].clamp_(min=0, max=box_size[0])\n\n        # Convert the boxes back to (center x, center y, width, height, angle) representation\n        boxes_near_horizontal_normalized = convert_boxes_to_rotated_boxes(boxes_near_horizontal)\n        self.tensor[idx] = boxes_near_horizontal_normalized\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        boxes = self.tensor\n        height, width = box_size\n        boxes = boxes.clone()  # avoid in-place modification\n\n        # clip boxes\n        boxes[:, 0].clamp_(min=0, max=width)\n        boxes[:, 1].clamp_(min=0, max=height)\n        boxes[:, 2].clamp_(min=0, max=width)\n        boxes[:, 3].clamp_(min=0, max=height)\n\n        # clip boxes by angle\n        angles_in_deg = boxes[:, 4]\n        # convert angles to [0, 180]\n        angles_in_abs = torch.abs(angles_in_deg)\n        # find boxes with horizontal angle\n        horizontal_mask = (\n            torch.abs(angles_in_abs - 90) < clip_angle_threshold\n        ) | (torch.abs(angles_in_abs - 90) < clip_angle_threshold)\n        horizontal_boxes = boxes[horizontal_mask]\n        if horizontal_boxes.numel() > 0:\n            x_ctr, y_ctr, width, height, angle = horizontal_boxes.unbind(1)\n            # convert angle from [-90, 0] to [0, 180]\n            angle = torch.where(angle < 0, angle + 90, angle)\n            # convert angle from [0, 180] to [0, 360]\n            angle = angle * 2\n            # convert angle to radian\n            angle = angle * math.pi / 180.0\n            dx = width * 0.5 * torch.cos(angle)\n            dy = height * 0.5 * torch.sin(angle)\n            # calculate horizontal offset\n            x_ctr_horizontal_offset = dx * 0.5\n            # calculate vertical offset\n            y_ctr_horizontal_offset = dy * 0.5\n            # apply offset to center point\n            x_left = x_ctr - x_ctr_horizontal_offset\n            x_right = x_ctr"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        if not self.is_empty():\n            assert len(box_size) == 2, \"box_size should be of length 2\"\n            boxes = self.tensor\n            assert boxes.shape[-1] == 5, \"boxes should have 5 dimensions\"\n            assert torch.isfinite(boxes).all(), \"Box tensor contains infinite or NaN values\"\n            assert (boxes[:, 2:4] > 0).all().item(), \"Boxes should have positive width and height\"\n\n            boxes = boxes.clone()\n            boxes[:, 2:4] = boxes[:, 2:4].clamp(min=1.0)\n\n            w, h = box_size\n            boxes[:, 0::5] = boxes[:, 0::5].clamp(min=0, max=w)\n            boxes[:, 1::5] = boxes[:, 1::5].clamp(min=0, max=h)\n            boxes[:, 2::5] = boxes[:, 2::5].clamp(min=0, max=w)\n            boxes[:, 3::5] = boxes[:, 3::5].clamp(min=0, max=h)\n\n            self.tensor = boxes\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n\n        return statistics\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for item in self.data:\n            if item['type'] == 'doc':\n                statistics['doc'] += 1\n            elif item['type'] == 'gen':\n                statistics['gen'] += 1\n            elif item['type'] == 'kno':\n                statistics['kno'] += 1\n            elif item['type'] == 'num':\n                statistics['num'] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for item in self.data:\n            if item['type'] == 'doc':\n                statistics['doc'] += 1\n            elif item['type'] == 'gen':\n                statistics['gen'] += 1\n            elif item['type'] == 'kno':\n                statistics['kno'] += 1\n            elif item['type'] == 'num':\n                statistics['num'] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            \"doc\": 0,\n            \"gen\": 0,\n            \"kno\": 0,\n            \"num\": 0\n        }\n\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        for d in self.data:\n            if d['type'] in statistics:\n                statistics[d['type']] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n\n        for obj in self.data:\n            if obj['type'] == 'doc':\n                statistics['doc'] += 1\n            elif obj['type'] == 'gen':\n                statistics['gen'] += 1\n            elif obj['type'] == 'kno':\n                statistics['kno'] += 1\n            elif obj['type'] == 'num':\n                statistics['num'] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for obj in self.data:\n            obj_type = obj['type']\n            if obj_type in statistics:\n                statistics[obj_type] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        for item in self.data:\n            if item['type'] == 'doc':\n                statistics['doc'] += 1\n            elif item['type'] == 'gen':\n                statistics['gen'] += 1\n            elif item['type'] == 'kno':\n                statistics['kno'] += 1\n            elif item['type'] == 'num':\n                statistics['num'] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        doc_count = 0\n        gen_count = 0\n        kno_count = 0\n        num_count = 0\n\n        for item in self.data:\n            if item['type'] == 'doc':\n                doc_count += 1\n            elif item['type'] == 'gen':\n                gen_count += 1\n            elif item['type'] == 'kno':\n                kno_count += 1\n            elif item['type'] == 'num':\n                num_count += 1\n\n        self.statistics = {\n            'doc': doc_count,\n            'gen': gen_count,\n            'kno': kno_count,\n            'num': num_count\n        }\n\n        return self.statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary with keys corresponding to the types and values set to 0\n        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        # Iterate over the data attribute of the instance\n        for item in self.data:\n            # Get the type of the current item\n            item_type = item['type']\n            # Increment the count of the corresponding type in the statistics dictionary\n            statistics[item_type] += 1\n\n        # Return the statistics dictionary\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary\n        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        # Iterate over the data and update the statistics\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary with zero counts for each type\n        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        # Iterate over the data and count the occurrences of each type\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n\n        return statistics\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for obj in self.data:\n            if obj['type'] == 'doc':\n                statistics['doc'] += 1\n            elif obj['type'] == 'gen':\n                statistics['gen'] += 1\n            elif obj['type'] == 'kno':\n                statistics['kno'] += 1\n            elif obj['type'] == 'num':\n                statistics['num'] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            item_type = item['type']\n            if item_type in statistics:\n                statistics[item_type] += 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary with zero counts for each type\n        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        # Iterate over the data attribute of the instance\n        for item in self.data:\n            # Get the type of the current item\n            item_type = item['type']\n\n            # Check if the type is one of the four expected types\n            if item_type in statistics:\n                # Increment the count of the corresponding type in the statistics dictionary\n                statistics[item_type] += 1\n\n        # Return the statistics dictionary\n        return statistics\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from . import NECKS\n    from mmdet.models.necks import NECKS as MMDET_NECKS\n\n    if hasattr(NECKS, cfg.type):\n        neck = getattr(NECKS, cfg.type)(**cfg)\n    else:\n        neck = getattr(MMDET_NECKS, cfg.type)(**cfg)\n\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from .necks import NECKS\n    from mmdet.models.necks import NECKS as MMDET_NECKS\n\n    if cfg['type'] in NECKS:\n        return NECKS[cfg['type']](**cfg)\n    elif cfg['type'] in MMDET_NECKS:\n        return MMDET_NECKS[cfg['type']](**cfg)\n    else:\n        raise ValueError(f\"Neck type {cfg['type']} not supported.\")"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    # Import the necessary modules\n    from .necks import NECKS\n    from mmdet.models.necks import NECKS as MMDET_NECKS\n\n    # Check if the specified neck type is available in the NECKS module dictionary\n    if cfg['type'] in NECKS:\n        # Build the neck using NECKS\n        neck = NECKS.build(cfg)\n    else:\n        # Build the neck using MMDET_NECKS\n        neck = MMDET_NECKS.build(cfg)\n\n    # Return the built neck model instance\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if hasattr(cfg, 'type'):\n        cfg = cfg.copy()\n\n        neck_type = cfg.pop('type')\n        if neck_type not in NECKS:\n            raise KeyError('Unrecognized neck type {}'.format(neck_type))\n        else:\n            neck = NECKS.get(neck_type)\n\n        if hasattr(neck, 'build_neck'):\n            neck = neck.build_neck(cfg)\n            return neck\n\n        raise NotImplementedError\n\n    else:\n        raise KeyError('`neck` must be contained in the model config dictionary.')"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from .fpn import FPN\n    from .bifpn import BiFPN\n    from .necks import NECKS\n    from mmdet.models.necks import NECKS as MMDET_NECKS\n\n    if cfg['type'] in NECKS:\n        return NECKS[cfg['type']](**cfg)\n    elif cfg['type'] in MMDET_NECKS:\n        return MMDET_NECKS[cfg['type']](**cfg)\n    else:\n        raise ValueError(f\"Neck type {cfg['type']} not supported.\")"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from . import NECKS\n    from mmdet.models.necks import NECKS as MMDET_NECKS\n\n    assert cfg.type in NECKS.keys() or cfg.type in MMDET_NECKS.keys(), \\\n        f'neck type {cfg.type} not found'\n\n    if cfg.type in NECKS.keys():\n        neck = NECKS.get(cfg.type)(**cfg)\n    else:\n        neck = MMDET_NECKS.get(cfg.type)(**cfg)\n\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from .necks import NECKS\n    from mmdet.models import NECKS as MMDET_NECKS\n\n    assert cfg.type in NECKS.keys() or cfg.type in MMDET_NECKS.keys()\n\n    if cfg.type in NECKS.keys():\n        neck = NECKS.get(cfg.type)(cfg)\n    else:\n        neck = MMDET_NECKS.get(cfg.type)(**cfg)\n\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from mmdet.models.necks import NECKS\n    from mmdet.models.necks import MMDET_NECKS\n\n    assert cfg.type in NECKS.keys() or cfg.type in MMDET_NECKS.keys()\n\n    if cfg.type in NECKS.keys():\n        neck = NECKS.get(cfg.type)\n    else:\n        neck = MMDET_NECKS.get(cfg.type)\n\n    return neck(**cfg.kwargs)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    # Check if the specified neck type is available in the NECKS module dictionary\n    if cfg['type'] in NECKS:\n        # If so, build the neck using NECKS\n        return NECKS.build(cfg)\n    # Otherwise, build the neck using MMDET_NECKS\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if hasattr(cfg, 'type'):\n        cfg = cfg.copy()\n\n        neck_type = cfg.pop('type')\n\n        if neck_type not in NECKS:\n            raise KeyError('{} is not in the neck registry'.format(\n                neck_type))\n        else:\n            neck = NECKS.get(neck_type)\n\n        return neck(**cfg)\n\n    else:\n        raise KeyError('`neck` must be in the cfg')"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from .necks import NECKS\n    from mmdet.models import NECKS as MMDET_NECKS\n\n    assert cfg.type in NECKS.keys() or cfg.type in MMDET_NECKS.keys(), \\\n        f'neck type {cfg.type} not found'\n    return NECKS.get(cfg.type, MMDET_NECKS.get(cfg.type))(**cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from mmdet.models.builder import NECKS\n    from mmdet3d.models.necks import MMDET_NECKS\n    from mmdet3d.models.necks import NECKS\n\n    if cfg['type'] in NECKS:\n        return NECKS.build(cfg)\n    elif cfg['type'] in MMDET_NECKS:\n        return MMDET_NECKS.build(cfg)\n    else:\n        raise ValueError(f\"Neck type {cfg['type']} not supported\")"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if hasattr(cfg, 'type'):\n        cfg = cfg.copy()\n\n        neck_type = cfg.pop('type')\n        if neck_type not in NECKS:\n            raise KeyError('{} is not in the neck registry'.format(neck_type))\n        else:\n            neck = NECKS.get(neck_type)\n\n    elif hasattr(cfg, 'frozen_stages'):\n        neck_type = 'FPN'\n        if neck_type not in NECKS:\n            raise KeyError('{} is not in the neck registry'.format(neck_type))\n        else:\n            neck = NECKS.get(neck_type)\n\n    else:\n        raise KeyError('neck must have \"type\" key')\n\n    return neck(**cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    # Import the necessary modules\n    from mmcv.utils import Registry\n    from mmseg.models import NECKS\n    from mmdet.models import NECKS as MMDET_NECKS\n\n    # Create a registry for custom neck models\n    CUSTOM_NECKS = Registry('custom_neck')\n\n    # Check if the specified neck type is available in the NECKS module dictionary\n    if cfg['type'] in NECKS:\n        # Build the neck using NECKS\n        neck = NECKS.build(cfg)\n    else:\n        # Build the neck using MMDET_NECKS\n        neck = MMDET_NECKS.build(cfg)\n\n    # Return the built neck model instance\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from mmdet.models.necks import FPN, BFP, ChannelMapper, HRFPN\n    from .necks import *\n    from . import NECKS\n\n    assert cfg.type in NECKS, f'cfg.type: {cfg.type} is not found in NECKS'\n\n    if cfg.type in NECKS:\n        neck = NECKS.build(cfg)\n    elif cfg.type in ['FPN', 'BFP', 'ChannelMapper', 'HRFPN']:\n        neck = eval(cfg.type)(**cfg)\n    else:\n        raise ValueError(f'{cfg.type} is not found')\n\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from . import NECKS\n    from mmdet.models.necks import NECKS as MMDET_NECKS\n\n    if cfg['type'] in NECKS:\n        return NECKS[cfg['type']](**cfg)\n    elif cfg['type'] in MMDET_NECKS:\n        return MMDET_NECKS[cfg['type']](**cfg)\n    else:\n        raise ValueError(f\"Neck type {cfg['type']} not supported\")\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if hasattr(cfg, 'type'):\n        type = cfg.type\n    else:\n        raise ValueError('neck must specify a type')\n\n    if hasattr(cfg, 'pop'):\n        cfg.pop('type')\n        cfg.pop('pop')\n\n    if hasattr(cfg, 'treelstm'):\n        cfg.pop('treelstm')\n\n    if hasattr(cfg, 'treegru'):\n        cfg.pop('treegru')\n\n    if hasattr(cfg, 'treelstm_gru'):\n        cfg.pop('treelstm_gru')\n\n    if hasattr(cfg, 'treelstm_gru_v2'):\n        cfg.pop('treelstm_gru_v2')\n\n    if hasattr(cfg, 'treelstm_gru_v3'):\n        cfg.pop('treelstm_gru_v3')\n\n    if hasattr(cfg, 'treelstm_gru_v4'):\n        cfg.pop('treelstm_gru_v4')\n\n    if hasattr(cfg, 'treelstm_gru_v5'):\n        cfg.pop('treelstm_gru_v5')\n\n    if hasattr(cfg, 'treelstm_gru_v6'):\n        cfg.pop('treelstm_gru_v6')\n\n    if hasattr(cfg, 'treelstm_gru_v7'):\n        cfg.pop('treelstm_gru_v7')\n\n    if hasattr(cfg, 'treelstm_gru_v8'):\n        cfg.pop('treelstm_gru_v8')\n\n    if hasattr(cfg, 'treelstm_gru_v9'):\n        cfg.pop('treelstm_gru_v9')\n\n    if hasattr(cfg, 'treelstm_gru_v10'):"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from .necks import NECKS\n    from mmdet.models.necks import NECKS as MMDET_NECKS\n\n    assert cfg.type in NECKS.keys(), f'type {cfg.type} is not support, support {list(NECKS.keys())}'\n    neck = NECKS.get(cfg.type)(cfg)\n    return neck\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from .fpn import FPN\n    from .bifpn import BiFPN\n    from .yolo_neck import YOLOV3Neck\n    from .pafpn import PAFPN\n    from .csp_pan import CSPPAN\n    from .nas_fpn import NASFPN\n    from .rfp import RFP\n    from .yolox_pafpn import YOLOXPAFPN\n\n    NECKS = {\n        'FPN': FPN,\n        'BiFPN': BiFPN,\n        'YOLOV3Neck': YOLOV3Neck,\n        'PAFPN': PAFPN,\n        'CSPPAN': CSPPAN,\n        'NASFPN': NASFPN,\n        'RFP': RFP,\n        'YOLOXPAFPN': YOLOXPAFPN\n    }\n\n    MMDET_NECKS = {\n        'FPN': 'mmdet.models.necks.FPN',\n        'BFP': 'mmdet.models.necks.BFP',\n        'ChannelMapper': 'mmdet.models.necks.ChannelMapper',\n        'CTResNetNeck': 'mmdet.models.necks.CTResNetNeck',\n        'DilatedEncoder': 'mmdet.models.necks.DilatedEncoder',\n        'FPG': 'mmdet.models.necks.FPG',\n        'FPN_CARAFE': 'mmdet.models.necks.FPN_CARAFE',\n        'HRFPN': 'mmdet.models.necks.HRFPN',\n        'NASFPN': 'mmdet.models.necks.NASFPN',\n        'NASFCOS_FPN': 'mmdet.models.necks.NASFCOS_FPN',\n        'PAFPN': 'mmdet.models.necks.PAFPN',\n        'RFP': 'mmdet.models.necks.RFP',\n        'SSDNeck': 'mmdet.models.necks."}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from .fpn import FPN\n    from .bifpn import BiFPN\n    from .nasfpn import NASFPN\n    from .pafpn import PAFPN\n    from .rfp import RFP\n    from .yolo_neck import YOLOV3Neck\n    from .csp_pan import CSPPAN\n    from .csp_darknet import CSPDarknet\n    from .csp_pan_neck import CSPPANNeck\n    from .yolox_pafpn import YOLOXPAFPN\n    from .yolov6_pafpn import YOLOv6RepBiFPNNeck\n    from .yolov7_neck import YOLOv7Neck\n    from .yolov8_neck import YOLOv8Neck\n    from .yolov5_neck import YOLOv5Neck\n    from .yolov5_pafpn import YOLOv5PAFPN\n    from .yolov7_pafpn import YOLOv7PAFPN\n    from .yolov8_pafpn import YOLOv8PAFPN\n    from .yolov5_neck import YOLOv5Neck\n    from .yolov5_pafpn import YOLOv5PAFPN\n    from .yolov7_pafpn import YOLOv7PAFPN\n    from .yolov8_pafpn import YOLOv8PAFPN\n    from .yolov5_neck import YOLOv5Neck\n    from .yolov5_pafpn import YOLOv5PAFPN\n    from .yolov7_pafpn import YOLOv7PAFPN\n    from .yolov8_pafpn import YOLOv8PAFPN\n    from .yolov5_neck import YOLOv5Neck\n    from .yolov5_pafpn import YOLOv5PAFPN\n    from .yolov7_pafpn import YOLOv7"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg['type']\n\n    if loss_type == 'cross_entropy':\n        from torch.nn import CrossEntropyLoss\n        return CrossEntropyLoss()\n\n    elif loss_type == 'binary_cross_entropy':\n        from torch.nn import BCELoss\n        return BCELoss()\n\n    elif loss_type == 'mse':\n        from torch.nn import MSELoss\n        return MSELoss()\n\n    elif loss_type == 'smooth_l1':\n        from torch.nn import SmoothL1Loss\n        return SmoothL1Loss()\n\n    elif loss_type == 'l1':\n        from torch.nn import L1Loss\n        return L1Loss()\n\n    elif loss_type == 'nll':\n        from torch.nn import NLLLoss\n        return NLLLoss()\n\n    elif loss_type == 'poisson_nll':\n        from torch.nn import PoissonNLLLoss\n        return PoissonNLLLoss()\n\n    elif loss_type == 'kld':\n        from torch.nn import KLDivLoss\n        return KLDivLoss()\n\n    elif loss_type == 'hinge_embedding':\n        from torch.nn import HingeEmbeddingLoss\n        return HingeEmbeddingLoss()\n\n    elif loss_type == 'multi_label_margin':\n        from torch.nn import MultiLabelMarginLoss\n        return MultiLabelMarginLoss()\n\n    elif loss_type == 'soft_margin':\n        from torch.nn import SoftMarginLoss\n        return SoftMarginLoss()\n\n    elif loss_type == 'multi_label_soft_margin':\n        from torch.nn import MultiLabelSoftMarginLoss\n        return MultiLabelSoftMarginLoss()\n\n    elif loss_type == 'cosine_embedding':\n        from torch.nn import CosineEmbeddingLoss\n        return CosineEmbeddingLoss()\n\n    elif loss_type == 'triplet"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg.type == \"MSELoss\":\n        return nn.MSELoss()\n    elif cfg.type == \"BCELoss\":\n        return nn.BCELoss()\n    elif cfg.type == \"BCEWithLogitsLoss\":\n        return nn.BCEWithLogitsLoss()\n    elif cfg.type == \"CrossEntropyLoss\":\n        return nn.CrossEntropyLoss()\n    elif cfg.type == \"NLLLoss\":\n        return nn.NLLLoss()\n    elif cfg.type == \"L1Loss\":\n        return nn.L1Loss()\n    elif cfg.type == \"SmoothL1Loss\":\n        return nn.SmoothL1Loss()\n    elif cfg.type == \"KLDivLoss\":\n        return nn.KLDivLoss()\n    elif cfg.type == \"MarginRankingLoss\":\n        return nn.MarginRankingLoss()\n    elif cfg.type == \"MultiLabelMarginLoss\":\n        return nn.MultiLabelMarginLoss()\n    elif cfg.type == \"SoftMarginLoss\":\n        return nn.SoftMarginLoss()\n    elif cfg.type == \"MultiLabelSoftMarginLoss\":\n        return nn.MultiLabelSoftMarginLoss()\n    elif cfg.type == \"CosineEmbeddingLoss\":\n        return nn.CosineEmbeddingLoss()\n    elif cfg.type == \"MultiMarginLoss\":\n        return nn.MultiMarginLoss()\n    elif cfg.type == \"TripletMarginLoss\":\n        return nn.TripletMarginLoss()\n    elif cfg.type == \"HingeEmbeddingLoss\":\n        return nn.HingeEmbeddingLoss()\n    elif cfg.type == \"PoissonNLLLoss\":\n        return nn.PoissonNLLLoss()\n    elif cfg.type == \"CTCLoss\":\n        return"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg.type == 'mse':\n        from torch.nn import MSELoss\n        return MSELoss()\n\n    elif cfg.type == 'l1':\n        from torch.nn import L1Loss\n        return L1Loss()\n\n    elif cfg.type == 'bce':\n        from torch.nn import BCELoss\n        return BCELoss()\n\n    elif cfg.type == 'bce_logit':\n        from torch.nn import BCEWithLogitsLoss\n        return BCEWithLogitsLoss()\n\n    elif cfg.type == 'cross_entropy':\n        from torch.nn import CrossEntropyLoss\n        return CrossEntropyLoss()\n\n    elif cfg.type == 'nll':\n        from torch.nn import NLLLoss\n        return NLLLoss()\n\n    elif cfg.type == 'poisson_nll':\n        from torch.nn import PoissonNLLLoss\n        return PoissonNLLLoss()\n\n    elif cfg.type == 'kld':\n        from torch.nn import KLDivLoss\n        return KLDivLoss()\n\n    elif cfg.type == 'cosine_embedding':\n        from torch.nn import CosineEmbeddingLoss\n        return CosineEmbeddingLoss()\n\n    elif cfg.type == 'hinge_embedding':\n        from torch.nn import HingeEmbeddingLoss\n        return HingeEmbeddingLoss()\n\n    elif cfg.type == 'margin_ranking':\n        from torch.nn import MarginRankingLoss\n        return MarginRankingLoss()\n\n    elif cfg.type == 'multilabel_margin':\n        from torch.nn import MultiLabelMarginLoss\n        return MultiLabelMarginLoss()\n\n    elif cfg.type == 'smooth_l1':\n        from torch.nn import SmoothL1Loss\n        return SmoothL1Loss()\n\n    elif cfg.type == 'soft_"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg[\"type\"] == \"cross_entropy\":\n        from torch.nn import CrossEntropyLoss\n        return CrossEntropyLoss()\n    elif cfg[\"type\"] == \"binary_cross_entropy\":\n        from torch.nn import BCELoss\n        return BCELoss()\n    elif cfg[\"type\"] == \"bce_with_logits\":\n        from torch.nn import BCEWithLogitsLoss\n        return BCEWithLogitsLoss()\n    elif cfg[\"type\"] == \"mse\":\n        from torch.nn import MSELoss\n        return MSELoss()\n    elif cfg[\"type\"] == \"l1\":\n        from torch.nn import L1Loss\n        return L1Loss()\n    elif cfg[\"type\"] == \"smooth_l1\":\n        from torch.nn import SmoothL1Loss\n        return SmoothL1Loss()\n    elif cfg[\"type\"] == \"kldiv\":\n        from torch.nn import KLDivLoss\n        return KLDivLoss()\n    elif cfg[\"type\"] == \"nll\":\n        from torch.nn import NLLLoss\n        return NLLLoss()\n    elif cfg[\"type\"] == \"poisson_nll\":\n        from torch.nn import PoissonNLLLoss\n        return PoissonNLLLoss()\n    elif cfg[\"type\"] == \"hinge_embedding\":\n        from torch.nn import HingeEmbeddingLoss\n        return HingeEmbeddingLoss()\n    elif cfg[\"type\"] == \"multilabel_margin\":\n        from torch.nn import MultiLabelMarginLoss\n        return MultiLabelMarginLoss()\n    elif cfg[\"type\"] == \"soft_margin\":\n        from torch.nn import SoftMarginLoss\n        return SoftMarginLoss()\n    elif cfg[\"type\"] == \"multi_label_soft_margin\":\n        from torch.nn import MultiLabelSoftMarginLoss\n        return MultiLabelSoftMarginLoss()\n    elif cfg[\"type"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg[\"type\"]\n    if loss_type == \"MSE\":\n        from torch.nn import MSELoss\n        return MSELoss(**cfg[\"kwargs\"])\n    elif loss_type == \"L1\":\n        from torch.nn import L1Loss\n        return L1Loss(**cfg[\"kwargs\"])\n    elif loss_type == \"BCE\":\n        from torch.nn import BCELoss\n        return BCELoss(**cfg[\"kwargs\"])\n    elif loss_type == \"BCEWithLogits\":\n        from torch.nn import BCEWithLogitsLoss\n        return BCEWithLogitsLoss(**cfg[\"kwargs\"])\n    elif loss_type == \"CrossEntropy\":\n        from torch.nn import CrossEntropyLoss\n        return CrossEntropyLoss(**cfg[\"kwargs\"])\n    elif loss_type == \"NLL\":\n        from torch.nn import NLLLoss\n        return NLLLoss(**cfg[\"kwargs\"])\n    elif loss_type == \"PoissonNLL\":\n        from torch.nn import PoissonNLLLoss\n        return PoissonNLLLoss(**cfg[\"kwargs\"])\n    elif loss_type == \"KLDiv\":\n        from torch.nn import KLDivLoss\n        return KLDivLoss(**cfg[\"kwargs\"])\n    elif loss_type == \"HingeEmbedding\":\n        from torch.nn import HingeEmbeddingLoss\n        return HingeEmbeddingLoss(**cfg[\"kwargs\"])\n    elif loss_type == \"MultiLabelMargin\":\n        from torch.nn import MultiLabelMarginLoss\n        return MultiLabelMarginLoss(**cfg[\"kwargs\"])\n    elif loss_type == \"SmoothL1\":\n        from torch.nn import SmoothL1Loss\n        return SmoothL1Loss(**cfg[\"kwargs\"])\n    elif loss_type == \"SoftMargin\":\n        from torch.nn import SoftMarginLoss\n        return SoftMarginLoss(**cfg[\"kwargs\"])"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg['type']\n\n    if loss_type == 'mse':\n        from torch.nn import MSELoss\n        return MSELoss()\n\n    elif loss_type == 'l1':\n        from torch.nn import L1Loss\n        return L1Loss()\n\n    elif loss_type == 'bce':\n        from torch.nn import BCELoss\n        return BCELoss()\n\n    elif loss_type == 'ce':\n        from torch.nn import CrossEntropyLoss\n        return CrossEntropyLoss()\n\n    elif loss_type == 'nll':\n        from torch.nn import NLLLoss\n        return NLLLoss()\n\n    elif loss_type == 'poisson':\n        from torch.nn import PoissonNLLLoss\n        return PoissonNLLLoss()\n\n    elif loss_type == 'kld':\n        from torch.nn import KLDivLoss\n        return KLDivLoss()\n\n    elif loss_type == 'hinge':\n        from torch.nn import HingeEmbeddingLoss\n        return HingeEmbeddingLoss()\n\n    elif loss_type == 'cosine':\n        from torch.nn import CosineEmbeddingLoss\n        return CosineEmbeddingLoss()\n\n    elif loss_type == 'dice':\n        from monai.losses import DiceLoss\n        return DiceLoss()\n\n    elif loss_type == 'tversky':\n        from monai.losses import TverskyLoss\n        return TverskyLoss()\n\n    elif loss_type == 'focal':\n        from monai.losses import FocalLoss\n        return FocalLoss()\n\n    elif loss_type == 'dice_focal':\n        from monai.losses import DiceFocalLoss\n        return DiceFocalLoss()\n\n    elif loss_type == 'dice_ce':\n        from monai.losses import DiceCELoss\n        return DiceCELoss()\n\n    elif"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] == 'ce':\n        from torch.nn import CrossEntropyLoss\n        return CrossEntropyLoss()\n    elif cfg['type'] == 'bce':\n        from torch.nn import BCELoss\n        return BCELoss()\n    elif cfg['type'] == 'mse':\n        from torch.nn import MSELoss\n        return MSELoss()\n    elif cfg['type'] == 'nll':\n        from torch.nn import NLLLoss\n        return NLLLoss()\n    elif cfg['type'] == 'l1':\n        from torch.nn import L1Loss\n        return L1Loss()\n    elif cfg['type'] == 'smooth_l1':\n        from torch.nn import SmoothL1Loss\n        return SmoothL1Loss()\n    elif cfg['type'] == 'kld':\n        from torch.nn import KLDivLoss\n        return KLDivLoss()\n    elif cfg['type'] == 'bce_with_logits':\n        from torch.nn import BCEWithLogitsLoss\n        return BCEWithLogitsLoss()\n    elif cfg['type'] == 'ce_with_logits':\n        from torch.nn import CrossEntropyLoss\n        return CrossEntropyLoss()\n    elif cfg['type'] == 'nll_with_logits':\n        from torch.nn import NLLLoss\n        return NLLLoss()\n    elif cfg['type'] == 'poisson_nll':\n        from torch.nn import PoissonNLLLoss\n        return PoissonNLLLoss()\n    elif cfg['type'] == 'hinge_embedding':\n        from torch.nn import HingeEmbeddingLoss\n        return HingeEmbeddingLoss()\n    elif cfg['type'] == 'multi_label_margin':\n        from torch.nn import MultiLabelMarginLoss\n        return MultiLabelMarginLoss()\n    elif cfg['type'] == 'soft_margin':\n        from tor"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] == 'mse':\n        from torch.nn import MSELoss\n        return MSELoss()\n\n    elif cfg['type'] == 'l1':\n        from torch.nn import L1Loss\n        return L1Loss()\n\n    elif cfg['type'] == 'bce':\n        from torch.nn import BCELoss\n        return BCELoss()\n\n    elif cfg['type'] == 'ce':\n        from torch.nn import CrossEntropyLoss\n        return CrossEntropyLoss()\n\n    elif cfg['type'] == 'nll':\n        from torch.nn import NLLLoss\n        return NLLLoss()\n\n    elif cfg['type'] == 'dice':\n        from monai.losses import DiceLoss\n        return DiceLoss()\n\n    elif cfg['type'] == 'dice_ce':\n        from monai.losses import DiceCELoss\n        return DiceCELoss()\n\n    elif cfg['type'] == 'dice_focal':\n        from monai.losses import DiceFocalLoss\n        return DiceFocalLoss()\n\n    elif cfg['type'] == 'dice_ce_focal':\n        from monai.losses import DiceCEFocalLoss\n        return DiceCEFocalLoss()\n\n    elif cfg['type'] == 'dice_ce_focal_bce':\n        from monai.losses import DiceCEFocalLoss\n        return DiceCEFocalLoss(include_bce=True)\n\n    elif cfg['type'] == 'dice_ce_focal_bce_ce':\n        from monai.losses import DiceCEFocalLoss\n        return DiceCEFocalLoss(include_bce=True, include_ce=True)\n\n    elif cfg['type'] == 'dice_ce_focal_bce_ce_dice':\n        from monai.losses import DiceCEFocalLoss\n        return D"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg.type\n\n    if loss_type == \"MSE\":\n        from torch.nn import MSELoss\n        loss_fn = MSELoss()\n    elif loss_type == \"BCE\":\n        from torch.nn import BCELoss\n        loss_fn = BCELoss()\n    elif loss_type == \"BCEWithLogits\":\n        from torch.nn import BCEWithLogitsLoss\n        loss_fn = BCEWithLogitsLoss()\n    elif loss_type == \"CrossEntropy\":\n        from torch.nn import CrossEntropyLoss\n        loss_fn = CrossEntropyLoss()\n    elif loss_type == \"NLL\":\n        from torch.nn import NLLLoss\n        loss_fn = NLLLoss()\n    elif loss_type == \"L1\":\n        from torch.nn import L1Loss\n        loss_fn = L1Loss()\n    elif loss_type == \"SmoothL1\":\n        from torch.nn import SmoothL1Loss\n        loss_fn = SmoothL1Loss()\n    elif loss_type == \"KLDiv\":\n        from torch.nn import KLDivLoss\n        loss_fn = KLDivLoss()\n    elif loss_type == \"MarginRanking\":\n        from torch.nn import MarginRankingLoss\n        loss_fn = MarginRankingLoss()\n    elif loss_type == \"HingeEmbedding\":\n        from torch.nn import HingeEmbeddingLoss\n        loss_fn = HingeEmbeddingLoss()\n    elif loss_type == \"MultiLabelMargin\":\n        from torch.nn import MultiLabelMarginLoss\n        loss_fn = MultiLabelMarginLoss()\n    elif loss_type == \"CosineEmbedding\":\n        from torch.nn import CosineEmbeddingLoss\n        loss_fn = CosineEmbeddingLoss()\n    elif loss_type == \"MultiMargin\":\n        from torch.nn import MultiMarginLoss"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg.get('type', 'mse')\n    if loss_type == 'mse':\n        from torch.nn import MSELoss\n        return MSELoss()\n    elif loss_type == 'l1':\n        from torch.nn import L1Loss\n        return L1Loss()\n    elif loss_type == 'bce':\n        from torch.nn import BCELoss\n        return BCELoss()\n    elif loss_type == 'ce':\n        from torch.nn import CrossEntropyLoss\n        return CrossEntropyLoss()\n    elif loss_type == 'nll':\n        from torch.nn import NLLLoss\n        return NLLLoss()\n    elif loss_type == 'poisson_nll':\n        from torch.nn import PoissonNLLLoss\n        return PoissonNLLLoss()\n    elif loss_type == 'kld':\n        from torch.nn import KLDivLoss\n        return KLDivLoss()\n    elif loss_type == 'huber':\n        from torch.nn import SmoothL1Loss\n        return SmoothL1Loss()\n    elif loss_type == 'cosine':\n        from torch.nn import CosineEmbeddingLoss\n        return CosineEmbeddingLoss()\n    elif loss_type == 'hinge_embedding':\n        from torch.nn import HingeEmbeddingLoss\n        return HingeEmbeddingLoss()\n    elif loss_type == 'margin_ranking':\n        from torch.nn import MarginRankingLoss\n        return MarginRankingLoss()\n    elif loss_type == 'multilabel_margin':\n        from torch.nn import MultiLabelMarginLoss\n        return MultiLabelMarginLoss()\n    elif loss_type == 'soft_margin':\n        from torch.nn import SoftMarginLoss\n        return SoftMarginLoss()\n    elif loss_type == 'triplet':\n        from torch.nn import TripletMarginLoss\n        return TripletM"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg.type == \"CrossEntropyLoss\":\n        from torch.nn import CrossEntropyLoss\n\n        return CrossEntropyLoss()\n\n    if cfg.type == \"BinaryCrossEntropyLoss\":\n        from torch.nn import BCELoss\n\n        return BCELoss()\n\n    if cfg.type == \"BCEWithLogitsLoss\":\n        from torch.nn import BCEWithLogitsLoss\n\n        return BCEWithLogitsLoss()\n\n    if cfg.type == \"MSELoss\":\n        from torch.nn import MSELoss\n\n        return MSELoss()\n\n    if cfg.type == \"L1Loss\":\n        from torch.nn import L1Loss\n\n        return L1Loss()\n\n    if cfg.type == \"SmoothL1Loss\":\n        from torch.nn import SmoothL1Loss\n\n        return SmoothL1Loss()\n\n    if cfg.type == \"HuberLoss\":\n        from torch.nn import HuberLoss\n\n        return HuberLoss()\n\n    if cfg.type == \"CosineEmbeddingLoss\":\n        from torch.nn import CosineEmbeddingLoss\n\n        return CosineEmbeddingLoss()\n\n    if cfg.type == \"HingeEmbeddingLoss\":\n        from torch.nn import HingeEmbeddingLoss\n\n        return HingeEmbeddingLoss()\n\n    if cfg.type == \"MultiLabelMarginLoss\":\n        from torch.nn import MultiLabelMarginLoss\n\n        return MultiLabelMarginLoss()\n\n    if cfg.type == \"MultiLabelSoftMarginLoss\":\n        from torch.nn import MultiLabelSoftMarginLoss\n\n        return MultiLabelSoftMarginLoss()\n\n    if cfg.type == \"MultiMarginLoss\":\n        from torch.nn import MultiMarginLoss\n\n        return MultiMarginLoss()\n\n    if cfg.type == \"Po"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] == 'cross_entropy':\n        from torch.nn import CrossEntropyLoss\n        return CrossEntropyLoss()\n    elif cfg['type'] == 'binary_cross_entropy':\n        from torch.nn import BCELoss\n        return BCELoss()\n    elif cfg['type'] == 'mse':\n        from torch.nn import MSELoss\n        return MSELoss()\n    elif cfg['type'] == 'bce_with_logits':\n        from torch.nn import BCEWithLogitsLoss\n        return BCEWithLogitsLoss()\n    elif cfg['type'] == 'dice':\n        from monai.losses import DiceLoss\n        return DiceLoss(sigmoid=True)\n    elif cfg['type'] == 'dice_ce':\n        from monai.losses import DiceCELoss\n        return DiceCELoss(sigmoid=True)\n    elif cfg['type'] == 'dice_ce_edge':\n        from monai.losses import DiceCEEdgeLoss\n        return DiceCEEdgeLoss(sigmoid=True)\n    elif cfg['type'] == 'dice_ce_edge_ce':\n        from monai.losses import DiceCEEdgeLoss\n        return DiceCEEdgeLoss(sigmoid=True, include_ce=True)\n    elif cfg['type'] == 'dice_ce_edge_ce_ce':\n        from monai.losses import DiceCEEdgeLoss\n        return DiceCEEdgeLoss(sigmoid=True, include_ce=True, include_ce_ce=True)\n    elif cfg['type'] == 'dice_ce_edge_ce_ce_ce':\n        from monai.losses import DiceCEEdgeLoss\n        return DiceCEEdgeLoss(sigmoid=True, include_ce=True, include_ce_ce=True, include_ce_ce_ce=True)\n    elif cfg['type'] == 'dice_ce_edge"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg.type == \"CrossEntropyLoss\":\n        return nn.CrossEntropyLoss()\n    elif cfg.type == \"BCELoss\":\n        return nn.BCELoss()\n    elif cfg.type == \"BCEWithLogitsLoss\":\n        return nn.BCEWithLogitsLoss()\n    elif cfg.type == \"MSELoss\":\n        return nn.MSELoss()\n    elif cfg.type == \"L1Loss\":\n        return nn.L1Loss()\n    elif cfg.type == \"SmoothL1Loss\":\n        return nn.SmoothL1Loss()\n    elif cfg.type == \"NLLLoss\":\n        return nn.NLLLoss()\n    elif cfg.type == \"PoissonNLLLoss\":\n        return nn.PoissonNLLLoss()\n    elif cfg.type == \"KLDivLoss\":\n        return nn.KLDivLoss()\n    elif cfg.type == \"BCEWithLogitsLoss\":\n        return nn.BCEWithLogitsLoss()\n    elif cfg.type == \"HingeEmbeddingLoss\":\n        return nn.HingeEmbeddingLoss()\n    elif cfg.type == \"MultiLabelSoftMarginLoss\":\n        return nn.MultiLabelSoftMarginLoss()\n    elif cfg.type == \"CosineEmbeddingLoss\":\n        return nn.CosineEmbeddingLoss()\n    elif cfg.type == \"CTCLoss\":\n        return nn.CTCLoss()\n    elif cfg.type == \"MultiMarginLoss\":\n        return nn.MultiMarginLoss()\n    elif cfg.type == \"TripletMarginLoss\":\n        return nn.TripletMarginLoss()\n    elif cfg.type == \"NLLLoss2d\":\n        return nn.NLLLoss2d()\n    elif cfg.type == \"NLLLoss\":\n        return nn."}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    # Check the type of loss specified in the configuration\n    if cfg.type == \"mse\":\n        # Import the Mean Squared Error (MSE) loss function from the torch.nn module\n        from torch.nn import MSELoss\n        # Create an instance of the MSE loss function\n        loss = MSELoss()\n    elif cfg.type == \"bce\":\n        # Import the Binary Cross Entropy (BCE) loss function from the torch.nn module\n        from torch.nn import BCELoss\n        # Create an instance of the BCE loss function\n        loss = BCELoss()\n    elif cfg.type == \"ce\":\n        # Import the Cross Entropy (CE) loss function from the torch.nn module\n        from torch.nn import CrossEntropyLoss\n        # Create an instance of the CE loss function\n        loss = CrossEntropyLoss()\n    else:\n        # Raise a ValueError if the type of loss specified in the configuration is not supported\n        raise ValueError(f\"Loss type {cfg.type} is not supported.\")\n\n    # Return the loss function object\n    return loss"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg.loss.type == \"cross_entropy\":\n        from torch.nn import CrossEntropyLoss\n\n        return CrossEntropyLoss()\n\n    elif cfg.loss.type == \"mse\":\n        from torch.nn import MSELoss\n\n        return MSELoss()\n\n    elif cfg.loss.type == \"l1\":\n        from torch.nn import L1Loss\n\n        return L1Loss()\n\n    elif cfg.loss.type == \"bce\":\n        from torch.nn import BCELoss\n\n        return BCELoss()\n\n    elif cfg.loss.type == \"bce_with_logits\":\n        from torch.nn import BCEWithLogitsLoss\n\n        return BCEWithLogitsLoss()\n\n    elif cfg.loss.type == \"dice_loss\":\n        from monai.losses import DiceLoss\n\n        return DiceLoss(include_background=True, sigmoid=True)\n\n    elif cfg.loss.type == \"focal_loss\":\n        from monai.losses import FocalLoss\n\n        return FocalLoss(include_background=True, gamma=2.0, weight=None, reduction=\"mean\")\n\n    elif cfg.loss.type == \"tversky_loss\":\n        from monai.losses import TverskyLoss\n\n        return TverskyLoss(include_background=True, sigmoid=True)\n\n    elif cfg.loss.type == \"dice_ce_loss\":\n        from monai.losses import DiceCELoss\n\n        return DiceCELoss(include_background=True, sigmoid=True)\n\n    elif cfg.loss.type == \"dice_focal_loss\":\n        from monai.losses import DiceFocalLoss\n\n        return DiceFocalLoss(include_background=True, sigmoid=True)\n\n    elif cfg.loss.type == \"dice_ce_focal_loss\":\n        from monai"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg['type']\n\n    if loss_type == 'MSE':\n        from torch.nn import MSELoss\n        loss_fn = MSELoss()\n\n    elif loss_type == 'BCE':\n        from torch.nn import BCELoss\n        loss_fn = BCELoss()\n\n    elif loss_type == 'BCEWithLogits':\n        from torch.nn import BCEWithLogitsLoss\n        loss_fn = BCEWithLogitsLoss()\n\n    elif loss_type == 'CCE':\n        from torch.nn import CrossEntropyLoss\n        loss_fn = CrossEntropyLoss()\n\n    elif loss_type == 'NLL':\n        from torch.nn import NLLLoss\n        loss_fn = NLLLoss()\n\n    elif loss_type == 'L1':\n        from torch.nn import L1Loss\n        loss_fn = L1Loss()\n\n    elif loss_type == 'Huber':\n        from torch.nn import SmoothL1Loss\n        loss_fn = SmoothL1Loss()\n\n    elif loss_type == 'CosineEmbedding':\n        from torch.nn import CosineEmbeddingLoss\n        loss_fn = CosineEmbeddingLoss()\n\n    elif loss_type == 'CTCLoss':\n        from torch.nn import CTCLoss\n        loss_fn = CTCLoss()\n\n    elif loss_type == 'NLLLoss2d':\n        from torch.nn import NLLLoss2d\n        loss_fn = NLLLoss2d()\n\n    elif loss_type == 'KLDivLoss':\n        from torch.nn import KLDivLoss\n        loss_fn = KLDivLoss()\n\n    elif loss_type == 'BCELoss':\n        from torch.nn import BCELoss\n        loss_fn = BCELoss()\n\n    elif loss_type == 'MarginRankingLoss':\n        from torch.nn import MarginRank"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] == 'cross_entropy':\n        from torch.nn import CrossEntropyLoss\n        return CrossEntropyLoss()\n    elif cfg['type'] == 'bce':\n        from torch.nn import BCELoss\n        return BCELoss()\n    elif cfg['type'] == 'mse':\n        from torch.nn import MSELoss\n        return MSELoss()\n    elif cfg['type'] == 'l1':\n        from torch.nn import L1Loss\n        return L1Loss()\n    elif cfg['type'] == 'smooth_l1':\n        from torch.nn import SmoothL1Loss\n        return SmoothL1Loss()\n    elif cfg['type'] == 'focal_loss':\n        from lib.losses.focal_loss import FocalLoss\n        return FocalLoss(gamma=cfg['gamma'])\n    elif cfg['type'] == 'dice_loss':\n        from lib.losses.dice_loss import DiceLoss\n        return DiceLoss()\n    elif cfg['type'] == 'tversky_loss':\n        from lib.losses.tversky_loss import TverskyLoss\n        return TverskyLoss(alpha=cfg['alpha'], beta=cfg['beta'])\n    elif cfg['type'] == 'focal_tversky_loss':\n        from lib.losses.focal_tversky_loss import FocalTverskyLoss\n        return FocalTverskyLoss(alpha=cfg['alpha'], beta=cfg['beta'], gamma=cfg['gamma'])\n    elif cfg['type'] == 'lovasz_loss':\n        from lib.losses.lovasz_loss import LovaszLoss\n        return LovaszLoss()\n    elif cfg['type'] == 'jaccard_loss':\n        from lib.losses.jaccard_loss import JaccardLoss\n        return JaccardLoss()\n    elif cfg['type'] == 'soft_jaccard_loss':\n       "}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    # Get the loss type from the configuration\n    loss_type = cfg.loss.type\n\n    # Initialize the loss function\n    loss = None\n\n    # Build the loss function based on the type\n    if loss_type == \"mse\":\n        from torch.nn import MSELoss\n        loss = MSELoss()\n    elif loss_type == \"bce\":\n        from torch.nn import BCELoss\n        loss = BCELoss()\n    elif loss_type == \"ce\":\n        from torch.nn import CrossEntropyLoss\n        loss = CrossEntropyLoss()\n    elif loss_type == \"nll\":\n        from torch.nn import NLLLoss\n        loss = NLLLoss()\n    elif loss_type == \"l1\":\n        from torch.nn import L1Loss\n        loss = L1Loss()\n    elif loss_type == \"smooth_l1\":\n        from torch.nn import SmoothL1Loss\n        loss = SmoothL1Loss()\n    elif loss_type == \"kld\":\n        from torch.nn import KLDivLoss\n        loss = KLDivLoss()\n    elif loss_type == \"cosine_embedding\":\n        from torch.nn import CosineEmbeddingLoss\n        loss = CosineEmbeddingLoss()\n    elif loss_type == \"hinge_embedding\":\n        from torch.nn import HingeEmbeddingLoss\n        loss = HingeEmbeddingLoss()\n    elif loss_type == \"multi_margin\":\n        from torch.nn import MultiMarginLoss\n        loss = MultiMarginLoss()\n    elif loss_type == \"triplet_margin\":\n        from torch.nn import TripletMarginLoss\n        loss = TripletMarginLoss()\n    elif loss_type == \"ctc\":\n        from torch.nn import CTCLoss\n        loss = CTCLoss()\n    elif loss_type == \"nll2d\":\n        from torch.nn import NLLLoss2d\n        loss"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg['type']\n    if loss_type == 'cross_entropy':\n        from torch.nn import CrossEntropyLoss\n        return CrossEntropyLoss()\n    elif loss_type == 'binary_cross_entropy':\n        from torch.nn import BCELoss\n        return BCELoss()\n    elif loss_type == 'mean_squared_error':\n        from torch.nn import MSELoss\n        return MSELoss()\n    elif loss_type == 'dice_loss':\n        from monai.losses import DiceLoss\n        return DiceLoss()\n    else:\n        raise ValueError(f\"Unsupported loss type: {loss_type}\")\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] == 'mse':\n        from torch import nn\n        loss = nn.MSELoss()\n    elif cfg['type'] == 'cross_entropy':\n        from torch import nn\n        loss = nn.CrossEntropyLoss()\n    elif cfg['type'] == 'bce':\n        from torch import nn\n        loss = nn.BCELoss()\n    elif cfg['type'] == 'bce_logits':\n        from torch import nn\n        loss = nn.BCEWithLogitsLoss()\n    elif cfg['type'] == 'dice':\n        from .dice import DiceLoss\n        loss = DiceLoss(**cfg['kwargs'])\n    elif cfg['type'] == 'focal':\n        from .focal import FocalLoss\n        loss = FocalLoss(**cfg['kwargs'])\n    elif cfg['type'] == 'tversky':\n        from .tversky import TverskyLoss\n        loss = TverskyLoss(**cfg['kwargs'])\n    elif cfg['type'] == 'focal_tversky':\n        from .focal_tversky import FocalTverskyLoss\n        loss = FocalTverskyLoss(**cfg['kwargs'])\n    elif cfg['type'] == 'lovasz':\n        from .lovasz import LovaszLoss\n        loss = LovaszLoss(**cfg['kwargs'])\n    elif cfg['type'] == 'soft_jaccard':\n        from .soft_jaccard import SoftJaccardLoss\n        loss = SoftJaccardLoss(**cfg['kwargs'])\n    elif cfg['type'] == 'bce_dice':\n        from .bce_dice import BceDiceLoss\n        loss = BceDiceLoss(**cfg['kwargs'])\n    elif cfg['type'] == 'bce_logits_dice':\n        from .bce_logits_dice import BceLogitsDiceLoss\n        loss = BceLogitsD"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS\n    from . import MMDET_HEADS\n\n    if cfg['type'] in HEADS:\n        return HEADS[cfg['type']](**cfg)\n    else:\n        return MMDET_HEADS[cfg['type']](**cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS\n    from . import MMDET_HEADS\n\n    if cfg.type in HEADS:\n        return HEADS[cfg.type](cfg)\n    else:\n        return MMDET_HEADS[cfg.type](cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg[\"type\"] in HEADS:\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS\n    from mmdet.models import HEADS as MMDET_HEADS\n\n    if cfg[\"type\"] in HEADS:\n        return HEADS[cfg[\"type\"]](**cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS\n    from mmdet.models import build_head as build_mmdet_head\n\n    if cfg['type'] in HEADS:\n        return HEADS[cfg['type']](**cfg)\n    else:\n        return build_mmdet_head(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS\n    from mmdet.models import HEADS as MMDET_HEADS\n\n    if cfg[\"type\"] in HEADS:\n        return HEADS[cfg[\"type\"]](**cfg)\n    else:\n        return MMDET_HEADS.build_head(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    # Check if the specified head type is available in the HEADS module dictionary\n    if cfg['type'] in HEADS:\n        # Build the head using the HEADS module\n        return HEADS.build(cfg)\n    else:\n        # Use the MMDET_HEADS module to build the head\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS\n    from . import MMDET_HEADS\n\n    if cfg['type'] in HEADS:\n        return HEADS[cfg['type']](**cfg)\n    elif cfg['type'] in MMDET_HEADS:\n        return MMDET_HEADS[cfg['type']](**cfg)\n    else:\n        raise NotImplementedError(f\"Head type {cfg['type']} not implemented.\")"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS\n    from mmdet.models import HEADS as MMDET_HEADS\n\n    if cfg[\"type\"] in HEADS:\n        return HEADS[cfg[\"type\"]](**cfg)\n    elif cfg[\"type\"] in MMDET_HEADS:\n        return MMDET_HEADS[cfg[\"type\"]](**cfg)\n    else:\n        raise ValueError(f\"Head type {cfg['type']} not found in HEADS or MMDET_HEADS.\")"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS\n    from mmdet.models import HEADS as MMDET_HEADS\n\n    if cfg['type'] in HEADS:\n        return HEADS[cfg['type']](**cfg)\n    elif cfg['type'] in MMDET_HEADS:\n        return MMDET_HEADS[cfg['type']](**cfg)\n    else:\n        raise ValueError(f'Head type {cfg[\"type\"]} not supported')"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS\n    from . import MMDET_HEADS\n\n    if cfg.get('type', None) in HEADS:\n        return HEADS[cfg.get('type')](cfg)\n    else:\n        return MMDET_HEADS[cfg.get('type')](cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from mmdet.models import HEADS, build_head\n    from mmdet3d.models import HEADS as MMDET3D_HEADS\n\n    if cfg['type'] in HEADS:\n        return build_head(cfg)\n    elif cfg['type'] in MMDET3D_HEADS:\n        return build_head(cfg)\n    else:\n        raise ValueError(f'Head type {cfg[\"type\"]} not supported')"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS\n    from mmdet.models import build_head as build_mmdet_head\n\n    if cfg.type in HEADS:\n        return HEADS.build(cfg)\n    else:\n        return build_mmdet_head(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS\n    from . import MMDET_HEADS\n\n    if cfg[\"type\"] in HEADS:\n        return HEADS[cfg[\"type\"]](**cfg)\n    else:\n        return MMDET_HEADS[cfg[\"type\"]](**cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] not in HEADS:\n        raise KeyError(f'Unknown head type {cfg[\"type\"]}, supported types are: {HEADS.keys()}')\n    else:\n        return HEADS[cfg['type']](**cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS\n    from mmdet.models import HEADS as MMDET_HEADS\n\n    if cfg.type in HEADS:\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from mmdet.models import HEADS, build_head\n    from mmcv import ConfigDict\n\n    if cfg.type in HEADS:\n        head = HEADS.build(cfg)\n    else:\n        from mmdet3d.models import HEADS as MMDET_HEADS\n        if cfg.type in MMDET_HEADS:\n            head = MMDET_HEADS.build(cfg)\n        else:\n            raise ValueError(f\"{cfg.type} is not a valid head type.\")\n\n    return head\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from mmdet.models import HEADS, build_head\n    from mmcv.utils import Registry\n\n    if cfg.get(\"type\", None) in HEADS._module_dict.keys():\n        head = build_head(cfg)\n    else:\n        try:\n            from mmdet3d.models import MMDET3D_HEADS\n        except:\n            mmdet3d_heads = Registry(\"3d_heads\")\n            MMDET3D_HEADS = mmdet3d_heads\n        assert cfg.get(\"type\", None) in MMDET3D_HEADS._module_dict.keys(), f\"Head type {cfg.get('type', None)} not found in either MMDET_HEADS or MMDET3D_HEADS\"\n        head = build_head(cfg)\n\n    return head"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    # Import the modules for building the head\n    from mmdet.models.builder import HEADS, build_head\n    from mmdet3d.models.builder import HEADS as MMDET3D_HEADS\n\n    # Check if the specified head type is available in the HEADS module dictionary\n    if cfg['type'] in HEADS:\n        # Build the head using the HEADS module\n        return build_head(cfg)\n    # If the specified head type is not available in the HEADS module dictionary, use the MMDET3D_HEADS module to build the head\n    else:\n        return MMDET3D_HEADS.build(cfg)\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS:\n        head = HEADS.build(cfg)\n    else:\n        head = MMDET_HEADS.build(cfg)\n    return head\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is deprecated, '\n                      'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is deprecated, '\n                      'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is deprecated, '\n                      'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is deprecated, '\n                      'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is deprecated, '\n                      'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg is deprecated, \" \"please specify them in model\",\n            UserWarning,\n        )\n    assert (\n        cfg.get(\"train_cfg\") is None or train_cfg is None\n    ), \"train_cfg specified in both outer field and model field \"\n    assert (\n        cfg.get(\"test_cfg\") is None or test_cfg is None\n    ), \"test_cfg specified in both outer field and model field \"\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg)\n    )\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg are not allowed to be specified '\n                      'as input arguments.', UserWarning)\n    if 'train_cfg' in cfg:\n        warnings.warn('train_cfg is not allowed in model config dict.',\n                      UserWarning)\n    if 'test_cfg' in cfg:\n        warnings.warn('test_cfg is not allowed in model config dict.',\n                      UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from mmseg.models import build_segmentor\n\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg is deprecated, \" \"please specify them in model\",\n            UserWarning,\n        )\n    assert (\n        cfg.get(\"train_cfg\") is None or train_cfg is None\n    ), \"train_cfg specified in both outer field and model field \"\n    assert (\n        cfg.get(\"test_cfg\") is None or test_cfg is None\n    ), \"test_cfg specified in both outer field and model field \"\n    return build_segmentor(cfg, dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg are not allowed to be specified '\n                      'as keyword arguments for build_detector().', UserWarning)\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    # Warn the user if the training or testing configurations are specified both in the function arguments and the model configuration.\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is deprecated, '\n                      'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    # Ensure that the training and testing configurations are not duplicated.\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    # Check if training and testing configurations are specified both in the function arguments and the model configuration\n    if train_cfg is not None and 'train_cfg' in cfg:\n        raise ValueError('train_cfg specified in both outer field and model field ')\n    if test_cfg is not None and 'test_cfg' in cfg:\n        raise ValueError('test_cfg specified in both outer field and model field ')\n\n    # Merge the training and testing configurations with the model configuration\n    if train_cfg is not None:\n        cfg.train_cfg = train_cfg\n    if test_cfg is not None:\n        cfg.test_cfg = test_cfg\n\n    # Build the segmentor model based on the provided configurations\n    model = MODELS.build(cfg)\n\n    # Return the built segmentor model\n    return model\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg is deprecated, \" \"please specify them in model\",\n            UserWarning,\n        )\n    assert (\n        cfg.get(\"train_cfg\") is None or train_cfg is None\n    ), \"train_cfg specified in both outer field and model field \"\n    assert (\n        cfg.get(\"test_cfg\") is None or test_cfg is None\n    ), \"test_cfg specified in both outer field and model field \"\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg)\n    )\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    # Check if the training or testing configurations are specified both in the function arguments and the model configuration\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg are only allowed to be specified '\n                      'at the model level, and will be ignored if specified '\n                      'at both the model and the function level. ')\n    if train_cfg is None:\n        train_cfg = cfg.get('train_cfg', None)\n    if test_cfg is None:\n        test_cfg = cfg.get('test_cfg', None)\n\n    # Build the segmentor model\n    model = build_from_cfg(cfg.model, MODELS, dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    return model"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    # Check if the training or testing configurations are specified both in the function arguments and the model configuration\n    if train_cfg is not None and train_cfg != cfg.get('train_cfg', None):\n        raise ValueError('train_cfg specified in both outer field and model field ')\n    if test_cfg is not None and test_cfg != cfg.get('test_cfg', None):\n        raise ValueError('test_cfg specified in both outer field and model field ')\n\n    # If the training or testing configurations are not specified in the function arguments, use the configurations specified in the model configuration\n    train_cfg = cfg.get('train_cfg', None)\n    test_cfg = cfg.get('test_cfg', None)\n\n    # Create a segmentor model instance\n    model = build_segmentor(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\n\n    # Return the segmentor model instance\n    return model\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    # Check if the training or testing configurations are specified both in the function arguments and the model configuration.\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg are only allowed to be specified '\n                      'at a model level, and will be ignored if specified '\n                      'at both model and dataset levels. ', UserWarning)\n\n    # Ensure that the training or testing configurations are not duplicated in the cfg dictionary.\n    if ('train_cfg' in cfg) or ('test_cfg' in cfg):\n        warnings.warn('train_cfg and test_cfg are set in both outside '\n                      'and model config. Please, delete the unnecessary '\n                      'parts.', UserWarning)\n\n    # Build the segmentor model based on the provided configurations.\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is deprecated, '\n                      'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is deprecated, '\n                      'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is deprecated, '\n                      'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is deprecated, '\n                      'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg is deprecated, \" \"please specify them in model\",\n            UserWarning,\n        )\n    assert (\n        cfg.get(\"train_cfg\") is None or train_cfg is None\n    ), \"train_cfg specified in both outer field and model field \"\n    assert (\n        cfg.get(\"test_cfg\") is None or test_cfg is None\n    ), \"test_cfg specified in both outer field and model field \"\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg)\n    )\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is set in '\n                      'build_detector, will be ignored', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is deprecated, '\n                      'please specify them in model')\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    from mmdet.models import build_detector as build_detector_mmdet\n    return build_detector_mmdet(cfg, train_cfg=train_cfg, test_cfg=test_cfg)"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is set in both outer field and model field, '\n                      f'use outer field or model field only. ')\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    from ..builder import DETECTORS, build_backbone, build_head, build_neck\n    from .base import BaseDetector\n    from .single_stage import SingleStageDetector\n    from .two_stage import TwoStageDetector\n\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    # If train_cfg or test_cfg is specified in the outer field, it should not be specified in the model field.\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is set in outer field, '\n            'it will be ignored and loaded from cfg.data.')\n\n    # If train_cfg or test_cfg is specified in the model field, it should not be specified in the outer field.\n    if 'train_cfg' in cfg or 'test_cfg' in cfg:\n        warnings.warn(\n            'train_cfg and test_cfg are set in `model` field, '\n            'it will be ignored and loaded from `cfg.data.train_cfg` '\n            'and `cfg.data.test_cfg` respectively')\n\n    # Build the detector model based on the configuration.\n    if cfg.type in DETECTORS:\n        return DETECTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    elif cfg.type in MMDET_DETECTORS:\n        return MMDET_DETECTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        raise ValueError(f'{cfg.type} is not registered in either DETECTORS or MMDET_DETECTORS')\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    # Warn the user if train_cfg or test_cfg is passed directly to the function\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is deprecated, '\n                      'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n\n    # Get the type of detector from the configuration dictionary\n    if cfg.get('type', None) is None:\n        raise KeyError('the cfg dict must contain the key \"type\"')\n    cfg_ = cfg.copy()\n\n    # Check if the detector type is in the DETECTORS registry\n    if cfg_['type'] in DETECTORS:\n        # Build the detector using the DETECTORS registry\n        return DETECTORS.build(\n            cfg_, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n    # Check if the detector type is in the MMDET_DETECTORS registry\n    if cfg_['type'] in MMDET_DETECTORS:\n        # Build the detector using the MMDET_DETECTORS registry\n        return MMDET_DETECTORS.build(\n            cfg_, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n    # If the detector type is not recognized, raise an error\n    raise NotImplementedError(f'Unrecognized detector type {cfg_[\"type\"]}')\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    # If train_cfg or test_cfg is specified in both the outer field and model field, raise a warning\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is set in both outer field and model field, '\n            'use outer field by ignoring local field.', UserWarning)\n\n    # If train_cfg or test_cfg is specified in the outer field, raise an assertion error\n    assert train_cfg is None or train_cfg.__class__.__name__ != 'ConfigDict', \\\n        'train_cfg specified in both outer field and model field '\n    assert test_cfg is None or test_cfg.__class__.__name__ != 'ConfigDict', \\\n        'test_cfg specified in both outer field and model field '\n\n    # If train_cfg or test_cfg is specified in the model field, raise an assertion error\n    if 'train_cfg' in cfg:\n        warnings.warn('train_cfg is set in the model field, use '\n                      'train_cfg in the outer field by ignoring the model field.',\n                      UserWarning)\n        assert train_cfg is None, 'train_cfg specified in both outer field and ' \\\n            'model field '\n    if 'test_cfg' in cfg:\n        warnings.warn('test_cfg is set in the model field, use '\n                      'test_cfg in the outer field by ignoring the '\n                      'model field.', UserWarning)\n        assert test_cfg is None, 'test_cfg specified in both outer field and ' \\\n            'model field '\n\n    # If train_cfg or test_cfg is specified in the outer field, use them\n    if train_cfg is not None:\n        cfg.train_cfg = train_cfg\n    if test_cfg is not None:\n        cfg.test_cfg = test_cfg\n\n    # If the type of the detector is not specified, raise an assertion error\n    if 'type' not in cfg:\n        raise KeyError('`cfg` must contain the key \"type\".')\n\n    # Get the type of the detector\n    c"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    from mmdet.models import build_detector\n    return build_detector(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    total_dt_num = len(dt_annos)\n    total_gt_num = len(gt_annos)\n    num_ious = len(metric)\n    ar_array = np.zeros(num_ious, dtype=np.float32)\n    num_gt = total_gt_num\n    num_dt = total_dt_num\n    num_valid_gt = 0\n    for i in range(len(gt_annos)):\n        rets = kitti_eval(gt_annos[i], dt_annos[i], metric, label2cat)\n        num_valid_gt += rets[\"num_gts\"]\n        for j in range(num_ious):\n            ar_array[j] += rets[\"recall\"][j]\n    recalls = ar_array / max(num_valid_gt, 1)\n    recalls = np.nan_to_num(recalls)\n    return recalls\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    from . import kitti_common as kitti\n    from .kitti_object_eval_python.eval import get_official_eval_result\n\n    assert len(gt_annos) == len(dt_annos)\n    if box_type_3d == 'Depth' or box_type_3d == 'Camera':\n        gt_annos = kitti.transform_annotations_to_kitti_format(\n            gt_annos, box_mode_3d=box_mode_3d)\n        dt_annos = kitti.transform_annotations_to_kitti_format(\n            dt_annos, box_mode_3d=box_mode_3d)\n    else:\n        raise NotImplementedError(\n            f'unsupported box_type {box_type_3d} for kitti dataset')\n\n    result_str, ret_dict = get_official_eval_result(gt_annos, dt_annos, metric)\n\n    if logger is None:\n        print(result_str)\n    else:\n        if isinstance(logger, str):\n            print(f'| {logger}: ', end='')\n        print(result_str)\n\n    mAP_bbox = np.mean([float(x['bbox'][0::4]) for x in ret_dict])\n    for label in range(1, len(label2cat) + 1):\n        result_str, ret_dict = get_official_eval_result(\n            gt_annos, dt_annos, metric, label)\n        if logger is None:\n            print(result_str)\n        else:\n            if isinstance(logger, str):\n                print(f'| {logger}: ', end='')\n            print(result_str)\n    ret_dict['mAP_bbox'] = mAP_bbox\n    return ret_dict"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    from . import kitti_common as kitti\n    from .kitti_object_eval_python.evaluate import kitti_object_eval\n\n    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d is None:\n        box_type_3d = getattr(kitti.Object3d, 'Box3D')\n    if box_mode_3d is None:\n        box_mode_3d = getattr(kitti.Object3d, 'Box3DMode')\n\n    device_id = dt_annos[0]['metadata']['image_idx']\n\n    # parse detection and ground truths\n    gt_boxes = []\n    gt_names = []\n    difficultys = []\n    dt_boxes = []\n    dt_names = []\n    dt_scores = []\n    gt_boxes_ignore = []\n\n    for i in range(len(gt_annos)):\n        # parse detected annotations\n        det_anno = dt_annos[i]\n        bboxes = det_anno['bbox']\n        if isinstance(bboxes, np.ndarray):\n            det_boxes = bboxes.reshape(-1, bboxes.shape[-1])\n        else:\n            det_boxes = np.array(bboxes, dtype=np.float32).reshape(-1, bboxes.shape[-1])\n        det_boxes = box_type_3d(\n            det_boxes,\n            box_mode_3d=box_mode_3d,\n            origin=(0.5, 0.5, 0.5))\n        det_names = det_anno['name']\n        dt_scores.append(det_anno['score'])\n        dt_boxes.append(det_boxes)\n        dt_names.append(det_names)\n\n        # parse gt annotations\n        gt_anno = gt_annos[i]\n        bboxes"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    from .indoor_common_utils import check_last_line_break\n    from .indoor_eval import get_official_eval_result, get_coco_eval_result\n\n    if not isinstance(metric, list):\n        metric = [metric]\n    allowed_metrics = ['bbox', 'bev', '3d']\n    if not set(metric).issubset(set(allowed_metrics)):\n        raise KeyError('metric {} is not supported'.format(metric))\n\n    cat_ids = [\n        d['category_id'] for d in gt_annos\n    ] if 'category_id' in gt_annos[0].keys() else []\n    if box_type_3d is None:\n        box_type_3d, box_mode_3d = get_box_type(gt_annos[0])\n    result_texts = []\n    result_dict = {}\n    for i in range(1, len(sorted(cat_ids)) + 1):\n        result_dict['{}'.format(label2cat[i])] = {}\n        for m in metric:\n            result_dict['{}'.format(label2cat[i])]['{}AP_50'.format(m)] = 0.0\n            result_dict['{}'.format(label2cat[i])]['{}AP_25'.format(m)] = 0.0\n            result_dict['{}'.format(label2cat[i])]['{}AP_75'.format(m)] = 0.0\n            result_dict['{}'.format(label2cat[i])]['{}AR_100'.format(m)] = 0.0\n            result_dict['{}'.format(label2cat[i])]['{}AR_50'.format(m)] = 0.0\n            result_dict['{}'.format(label2cat[i])]['{}AR_25'.format(m)] = 0.0\n            result_dict['{}'.format(label2cat[i])]['{}AR_75'.format(m)] = 0.0"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    from . import kitti_common as kitti\n    from .kitti_object_eval_python.eval import get_official_eval_result\n\n    assert len(gt_annos) == len(dt_annos)\n    if box_type_3d is None:\n        box_type_3d, box_mode_3d = 'KittiBox3D', 'lidar'\n    if box_mode_3d != 'lidar':\n        print('Unsupported box_mode_3d %s!' % box_mode_3d)\n        raise ValueError('Only lidar type boxes are supported.')\n\n    result_str, result_dict = '', dict()\n    for idx, metric_item in enumerate(metric):\n        if metric_item < 1:\n            continue\n        iou_thresh = metric_item\n        mean_recall, mean_precision, ap_class = [], [], []\n        for label in range(len(label2cat)):\n            gt_annos_label = [anno for anno in gt_annos if anno['name'] == label2cat[label]]\n            dt_annos_label = [anno for anno in dt_annos if anno['name'] == label2cat[label]]\n            anno_eval = kitti.evaluate_object(gt_annos_label, dt_annos_label,\n                                             box_type_3d=box_type_3d,\n                                             box_mode_3d=box_mode_3d,\n                                             iou_thresh=iou_thresh)\n\n            if anno_eval is None:\n                recall = -1\n                precision = -1\n                ap_class = -1\n            else:\n                recall = anno_eval['recall'][-1]\n                precision = anno_eval['precision'][-1]\n                ap_class = anno_eval['ap']\n            mean_recall.append(recall)\n            mean_precision.append(precision)\n            ap_class.append(ap_"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    try:\n        from nuscenes.eval.detection.config import config_factory\n        from nuscenes.eval.detection.evaluate import NuScenesEval\n    except:\n        raise ImportError('Please run \"pip install nuscenes-devkit\" to install nuscenes-devkit first.')\n\n    assert len(dt_annos) == len(gt_annos)\n    for gt, dt in zip(gt_annos, dt_annos):\n        assert gt['metadata']['token'] == dt['metadata']['token']\n    if box_type_3d is None:\n        box_type_3d, box_mode_3d = 'LiDAR', 'lidar'\n    if box_mode_3d != 'lidar':\n        raise ValueError('Indoor only supports lidar !!!')\n\n    nusc_eval = NuScenesEval(\n        gt_annos,\n        config=config_factory('indoor'),\n        result_path='',\n        eval_set='val',\n        output_dir='',\n        verbose=False)\n\n    result_dict = nusc_eval.main(\n        submission_path=dt_annos,\n        submission_version='detection_results_nusc',\n        verbose=False)\n\n    if logger is None:\n        log_dir = result_dict['meta']\n        logger = logging.getLogger(__name__)\n        result_path = osp.join(log_dir, 'indoor_det_results.json')\n        with open(result_path, 'w') as f:\n            json.dump(result_dict, f, indent=2)\n        logger.info('Results saved to %s' % result_path)\n        logger.info('Eval time: %f' % result_dict['meta']['eval_time'])\n        logger.info('eval_time: {:.0f}s'.format(\n            result_dict['meta']['eval_time']))\n        logger.info('mAP:')\n        for i, ap_class in en"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    from .indoor_common import get_official_eval_result\n\n    if box_type_3d is None:\n        from .box_np_ops import LiDARInstance3DBoxes\n        box_type_3d = LiDARInstance3DBoxes\n    if box_mode_3d is None:\n        box_mode_3d = Box3DMode.LIDAR\n\n    metric = np.array(metric, dtype=np.float32)\n    label2cat = {int(k): v for k, v in label2cat.items()}\n    gt_annos = [\n        box_type_3d(anno[\"gt_boxes_lidar\"], anno[\"gt_labels_3d\"],\n                    anno[\"gt_names_3d\"]) for anno in gt_annos\n    ]\n    dt_annos = [\n        box_type_3d(anno[\"boxes_lidar\"], anno[\"scores\"], anno[\"labels\"])\n        for anno in dt_annos\n    ]\n    result_str, result_dict = get_official_eval_result(gt_annos, dt_annos,\n                                                       metric, label2cat)\n\n    if logger is None:\n        print(result_str)\n    else:\n        if isinstance(logger, str):\n            from .logger import get_root_logger\n            logger = get_root_logger(\n                logger=logger,\n                log_file=None,\n                log_level=logging.INFO)\n        logger.info('\\n' + result_str)\n\n    return result_dict\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    from . import bev_box_overlaps\n    from . import d3_box_overlaps\n    from . import kitti_common as kitti\n    from .kitti_common import get_label_annos\n\n    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d == 'Depth':\n        gt_boxes = [anno['gt_boxes_upright_depth'] if anno['gt_boxes_upright_depth'] is not None else anno['gt_boxes'] for anno in gt_annos]\n        dt_boxes = [anno['boxes_upright_depth'] if anno['boxes_upright_depth'] is not None else anno['boxes'] for anno in dt_annos]\n    else:\n        gt_boxes = [anno['gt_boxes'] for anno in gt_annos]\n        dt_boxes = [anno['boxes'] for anno in dt_annos]\n\n    if box_type_3d == 'Depth':\n        ious = d3_box_overlaps(gt_boxes, dt_boxes, box_mode_3d=box_mode_3d)\n    else:\n        ious = bev_box_overlaps(gt_boxes, dt_boxes)\n\n    if isinstance(metric, list):\n        metrics = metric\n    else:\n        metrics = [metric]\n    header = ['class']\n    for i in range(len(metrics)):\n        header.append(f'AP_{metrics[i]}')\n        header.append(f'AR_{metrics[i]}')\n    header.append('num_samples')\n    results = {}\n    for i, cls in enumerate(label2cat):\n        # get results of each class\n        if box_type_3d == 'Depth':\n            rec, prec, ap = get_official_eval_result(gt_annos, dt_annos, i, metrics,"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    from . import kitti_common as kitti\n    from .kitti_object_eval_python.eval import get_official_eval_result\n\n    assert len(gt_annos) == len(dt_annos)\n    if box_type_3d is None:\n        box_type_3d, box_mode_3d = kitti.Box3DMode, kitti.Box3DMode.Depth\n    num_example = len(gt_annos)\n    eval_types = [\"detection\"]\n    ap_result_str, ap_dict = \"\", dict()\n    for i, current_class in enumerate(label2cat):\n        result_str, ret_dict = do_eval_class(gt_annos, dt_annos, current_class,\n                                             metric, box_type_3d, box_mode_3d)\n        if result_str != \"\":\n            ap_result_str += result_str\n            for k in ret_dict.keys():\n                ap_dict[k] = float(ret_dict[k]) / num_example\n    ret_dict = ap_dict\n\n    print(ap_result_str)\n    return ret_dict, ap_dict\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    from . import average_precision_2d_detection\n\n    ## convert gt_annos to region format\n    gt_boxes = []\n    gt_labels = []\n    difficultys = []\n    for anno in gt_annos:\n        gt_boxes.append(anno[\"gt_boxes_lidar\"])\n        gt_labels.append(anno[\"gt_names\"])\n        difficultys += anno[\"difficulty\"]\n\n    gt_boxes = np.concatenate(gt_boxes, 0)\n    gt_labels = np.concatenate(gt_labels, 0)\n    if len(gt_boxes) == 0:\n        selected = np.zeros([0], dtype=bool)\n    else:\n        selected = np.ones([gt_boxes.shape[0]], dtype=bool)\n\n    # convert det_annos to region format\n    dt_boxes = []\n    dt_scores = []\n    dt_labels = []\n    for anno in dt_annos:\n        dt_boxes.append(anno[\"boxes_lidar\"])\n        dt_scores.append(anno[\"scores\"])\n        dt_labels.append(anno[\"label\"])\n\n    dt_boxes = np.concatenate(dt_boxes, 0).astype(np.float32)\n    dt_scores = np.concatenate(dt_scores, 0).astype(np.float32)\n    dt_labels = np.concatenate(dt_labels, 0).astype(np.int64)\n\n    ## eval\n    recall, precision, ap = average_precision_2d_detection(\n        gt_boxes,\n        gt_labels,\n        dt_boxes,\n        dt_labels,\n        dt_scores,\n        metric,\n        label2cat,\n        difficultys,\n        logger=logger,\n        box_type_3d=box_type"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    total_dt_num = len(dt_annos)\n    total_gt_num = len(gt_annos)\n    num_thresh = len(metric)\n    num_label = len(label2cat)\n    num_pred = num_thresh * num_label\n    num_recall = num_label\n    ar_array = np.zeros(num_pred, dtype=np.float32)\n    thresh_array = np.zeros(num_pred, dtype=np.float32)\n    recall_array = np.zeros(num_recall, dtype=np.float32)\n    precision_array = np.zeros(num_recall, dtype=np.float32)\n    re = collect_stats_ind(gt_annos, dt_annos, metric, label2cat,\n                           box_type_3d=box_type_3d, box_mode_3d=box_mode_3d)\n    for i in range(len(metric)):\n        for j in range(len(label2cat)):\n            idx = i * num_label + j\n            dt_num = re[j][0]\n            gt_num = re[j][1]\n            if gt_num > 0:\n                dt_ind = re[j][2][i]\n                dt_ind = dt_ind[np.argsort(-dt_ind[:, -1])]\n                dt_num = dt_ind.shape[0]\n                for k in range(dt_num):\n                    thresh = dt_ind[k, -1]\n                    recall_array[j] = np.sum(dt_ind[k, :k + 1] > 0) / gt_num\n                    precision_array[j] = np.sum(dt_ind[k, :k + 1] > 0) / (k + 1)\n                    ar_array[idx] = np.max(precision_array)\n                    thresh_array[idx] = thresh\n    ret_dict"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    total_dt_num = len(dt_annos)\n    total_gt_num = len(gt_annos)\n    num_thresh = len(metric)\n    num_gt_classes = len(label2cat)\n\n    if box_type_3d is None:\n        box_type_3d, box_mode_3d = get_box_type(gt_annos[0][\"gt_boxes_upright_depth\"])\n\n    iou_thresh_list = [i for i in metric]\n    ap_class_str_list = [str(j) for j in range(num_gt_classes)]\n    indoor_eval_dict = {}\n    for i, thresh in enumerate(iou_thresh_list):\n        # prepare dict for different iou thresh\n        eval_dict = {}\n        for j in range(num_gt_classes):\n            eval_dict[j] = {}\n        cur_thresh = {iou_thresh_list[i]: eval_dict}\n        indoor_eval_dict.update(cur_thresh)\n\n    for i in range(total_dt_num):\n        # different object types will be seperated\n        cur_dc = {}\n        for j in range(num_gt_classes):\n            cur_dc[j] = []\n        # get valid detections\n        for j in range(num_gt_classes):\n            label_str = label2cat[j]\n            for k in range(len(dt_annos[i][\"name\"])):\n                if dt_annos[i][\"name\"][k] == label_str:\n                    cur_dc[j].append(k)\n        # get suppressed results\n        for j in range(num_gt_classes):\n            label_str = label2cat[j]\n            for k in range(len(dt_annos[i][\"name\"])):\n                if dt_annos[i][\"name\"][k] == label_str:\n                    # get i"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    from . import average_precision_2d_detection\n    from . import average_precision_3d_detection\n    from . import average_precision_BEV_detection\n    from . import average_precision_3d_detection_with_yaw\n    from . import average_precision_BEV_detection_with_yaw\n\n    # Evaluate the detection results\n    if box_type_3d == 'Depth':\n        # Evaluate 3D detection results\n        if box_mode_3d == 'lidar':\n            # Evaluate 3D detection results in lidar coordinate system\n            return average_precision_3d_detection.indoor_eval(gt_annos, dt_annos, metric, label2cat, logger)\n        elif box_mode_3d == 'camera':\n            # Evaluate 3D detection results in camera coordinate system\n            return average_precision_BEV_detection.indoor_eval(gt_annos, dt_annos, metric, label2cat, logger)\n        elif box_mode_3d == 'depth':\n            # Evaluate 3D detection results in depth coordinate system\n            return average_precision_3d_detection_with_yaw.indoor_eval(gt_annos, dt_annos, metric, label2cat, logger)\n        elif box_mode_3d == 'lidar_camera_depth':\n            # Evaluate 3D detection results in lidar, camera, and depth coordinate systems\n            return average_precision_BEV_detection_with_yaw.indoor_eval(gt_annos, dt_annos, metric, label2cat, logger)\n        else:\n            # Raise an error for unsupported box mode\n            raise NotImplementedError\n    else:\n        # Evaluate 2D detection results\n        return average_precision_2d_detection.indoor_eval(gt_annos, dt_annos, metric, label2cat, logger)\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    try:\n        from nuscenes.eval.detection.config import config_factory\n        from nuscenes.eval.detection.evaluate import NuScenesEval\n    except:\n        raise ImportError('Please run \"pip install nuscenes-devkit\" to install nuscenes-devkit first.')\n\n    from .class_names import get_nus_class\n    from .data_classes import Box3DList\n    from .data_classes import get_box_type\n\n    assert len(dt_annos) == len(gt_annos)\n\n    nus_eval = NuScenesEval(\n        gt_annos,\n        config=config_factory('detection_cvpr_2019'),\n        result_path='./.nuscenes-eval-logs',\n        eval_set='val',\n        verbose=False)\n\n    used_classes = [\n        'car', 'truck', 'construction_vehicle', 'bus', 'trailer',\n        'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'\n    ]\n    nus_eval.main(render_curves=False, plot_examples=0, verbose=False)\n\n    eval_results = dict()\n    for i in range(len(used_classes)):\n        eval_results[used_classes[i]] = dict()\n        for j in range(len(metric)):\n            eval_results[used_classes[i]][f'AP_{metric[j]}'] = 0\n            eval_results[used_classes[i]][f'AR_{metric[j]}'] = 0\n\n    for i in range(len(dt_annos)):\n        name = dt_annos[i]['name']\n        dims = dt_annos[i]['dimensions']\n        score = dt_annos[i]['score']\n        rotation = dt_annos[i]['rotation']\n        box_type_3d, box_mode_3d ="}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    from . import indoor_common_utils\n\n    assert len(dt_annos) == len(gt_annos)\n    num_examples = len(gt_annos)\n    split_parts = np.array(metric)\n    num_parts = len(split_parts)\n    assert num_parts >= 1\n    ret_dict = {}\n    if num_parts > 1:\n        for i in range(num_parts):\n            ret_dict.update(indoor_common_utils.create_small_table(\n                gt_annos, dt_annos, metric=split_parts[i], label2cat=label2cat))\n    else:\n        ret_dict.update(indoor_common_utils.create_small_table(\n            gt_annos, dt_annos, label2cat=label2cat))\n\n    headers = ['classes']\n    for i in range(num_parts):\n        ret_dict_i = {k: v[i] for k, v in ret_dict.items()}\n        headers.append(f'AP_{split_parts[i]}')\n        headers.append(f'AR_{split_parts[i]}')\n        if i == 0:\n            ret_dict_ = ret_dict_i\n        else:\n            for k in ret_dict:\n                ret_dict_[k] += ret_dict_i[k]\n    ret_dict = ret_dict_\n    for k in ret_dict:\n        ret_dict[k] = [v / num_parts for v in ret_dict[k]]\n\n    ret_dict.update(indoor_common_utils.get_official_eval_result(gt_annos, dt_annos, label2cat))\n\n    if logger is None:\n        ret_dict['mAP'] = ret_dict['mAP']\n    else:\n        if isinstance(logger, str):\n            logger = logging.getLogger(logger)\n        logger.info('Validation results')\n        logger.info('mAP: {:.4f}'.format(ret_dict['mAP']"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    try:\n        from pathlib import Path\n        from .rotate_iou_gpu import rotate_iou_gpu_eval\n    except:\n        from rotate_iou_gpu import rotate_iou_gpu_eval\n    from .rotate_iou import rotate_iou_cpu\n\n    if box_type_3d is None:\n        from .box_encoding_utils import box_encoder\n        box_encoder = box_encoder(box_type_3d, box_mode_3d)\n\n    assert len(dt_annos) == len(gt_annos)\n    total_gts = 0\n    total_dts = 0\n    num_valid_gts = 0\n    for i in range(len(gt_annos)):\n        rets = box_encoder.encode_box3d(gt_annos[i])\n        num_valid_gt = len([0 for x in rets if x['name'] != 'DontCare'])\n        total_gts += len(gt_annos[i]['name'])\n        total_dts += len(dt_annos[i]['name'])\n        num_valid_gts += num_valid_gt\n\n    print(f'num_valid_gts: {num_valid_gts}, total_gts: {total_gts}, total_dts: {total_dts}')\n    if box_type_3d == 'Depth':\n        ret_boxes = True\n    else:\n        ret_boxes = False\n\n    metric_dict = {}\n    for thresh in metric:\n        iou_th_list = np.array(thresh)\n        recall_list, precision_list, gt_num_list = [], [], []\n        for i in range(len(gt_annos)):\n            rets = box_encoder.encode_box3d(gt_annos[i])\n            gt_boxes = []\n            for ret in rets:\n                if ret['name'] == 'DontCare':\n                    continue\n                gt_box"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    from . import average_precision_2d_detection\n\n    # convert gt and dt to KITTI format\n    gt_annos = [\n        label2kitti(anno, label2cat) for anno in gt_annos\n    ]\n    dt_annos = [\n        label2kitti(anno, label2cat) for anno in dt_annos\n    ]\n\n    # convert detection format\n    if box_type_3d is not None and box_mode_3d is not None:\n        dt_annos = convert_detection_to_kitti_3d(dt_annos, box_type_3d,\n                                                 box_mode_3d)\n\n    # get label list for calculating mAP\n    label_list = list(label2cat.keys())\n\n    # calculate mAP and mAR\n    ap_result_str, ap_dict = average_precision_2d_detection(\n        gt_annos, dt_annos, metric, label_list)\n\n    # prepare for logger\n    for i, ap in enumerate(ap_result_str):\n        logger.info(ap)\n\n    # prepare for tensorboard\n    results = {}\n    for i, class_name in enumerate(label2cat.values()):\n        if class_name in ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier']:\n            results.update({\n                f'AP_{class_name}': ap_dict[i]\n            })\n    results.update({'mAP': np.mean(list(ap_dict.values()))})\n\n    return results\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    from . import rotational_nms_gpu\n    from . import center_to_corner_box2d\n    from . import limit_period\n    from . import get_label_annos\n    from . import eval_map_recall\n    from . import print_str\n    from . import get_official_eval_result\n\n    assert len(gt_annos) == len(dt_annos)\n    num_example = len(gt_annos)\n    eval_results = []\n    for i in range(num_example):\n        rets = eval_map_recall(\n            gt_annos[i],\n            dt_annos[i],\n            metric,\n            label2cat,\n            box_type_3d=box_type_3d,\n            box_mode_3d=box_mode_3d,\n        )\n        for j, num_example in enumerate(rets):\n            eval_results[j] += num_example\n    eval_map_recall = [x / num_example for x in eval_results]\n    ap_list = eval_map_recall[0::4]\n    ar_list = eval_map_recall[1::4]\n    ap_list_str = \" \".join([\"%.1f\" % x for x in ap_list])\n    ar_list_str = \" \".join([\"%.1f\" % x for x in ar_list])\n    num_dict = dict()\n    num_dict[\"gt_num\"] = eval_map_recall[2]\n    eval_results = dict()\n    for i, thresh in enumerate(metric):\n        eval_results[\"recall/iou=%.2f\" % thresh] = eval_map_recall[4 * i + 0]\n        eval_results[\"precision/iou=%.2f\" % thresh] = eval_map_recall[4 * i + 1]\n        eval_results[\"recall/iou=%.2f/difficult\" % thresh]"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    total_dt_num = len(dt_annos)\n    total_gt_num = len(gt_annos)\n    num_thresh = len(metric)\n    num_gt_for_cls = [0 for _ in range(num_thresh)]\n    num_dt_for_cls = [0 for _ in range(num_thresh)]\n    num_matched_for_cls = [0 for _ in range(num_thresh)]\n    for i in range(total_gt_num):\n        gt = gt_annos[i]\n        dt = dt_annos[i]\n        if dt['name'] != gt['name']:\n            raise Exception('gt name not equal dt name')\n        gt_cat = label2cat[gt['label']]\n        for j in range(num_thresh):\n            gt_bbox = gt['bbox']\n            dt_bbox = dt['bbox']\n            iou_thresh = metric[j]\n            if box_type_3d == 'LiDAR':\n                iou_thresh = -1\n            if dt_bbox.shape[0] != 0 and gt_bbox.shape[0] != 0:\n                # TODO: consider to rename the 3d iou\n                iou3d_thresh = iou_thresh if box_type_3d == 'LiDAR' else -1.0\n                gt_boxes_ignore = np.array(gt['ignore_boxes'], dtype=np.float32)\n                ious = box_np_ops.bev_box_overlaps_nearest_bev(\n                    gt_bbox, dt_bbox, box_mode_3d, iou_thresh, iou3d_thresh)\n                gt_assigned = gt_boxes_ignore.shape[0]\n                argmax_ious = ious.argmax(axis=1)\n                direct_assignment = np.arange(ious"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    from nuscenes.eval.detection.config import config_factory\n    from nuscenes.eval.detection.data_classes import EvalBoxes\n    from nuscenes.eval.detection.data_classes import EvalBox, EvalBoxes\n    from nuscenes.eval.detection.utils import category_to_boxcat_mapping, category_to_size_mapping\n\n    assert len(dt_annos) == len(gt_annos)\n\n    eval_config = config_factory('indoor_3d')\n    result_path = {}\n\n    for i, metric_it in enumerate(metric):\n        dt_annos_it = []\n        gt_annos_it = []\n\n        for j in range(len(dt_annos)):\n            # convert detection box to eval box\n            dt_boxes = [\n                EvalBox(**dt_anno) for dt_anno in dt_annos[j]\n            ]\n            dt_eval_boxes = EvalBoxes(dt_boxes,\n                                      box_type_3d=box_type_3d,\n                                      box_mode_3d=box_mode_3d,\n                                      class_field='label')\n            dt_annos_it.append(dt_eval_boxes)\n\n            # convert ground truth box to eval box\n            gt_boxes = [\n                EvalBox(**gt_anno) for gt_anno in gt_annos[j]\n            ]\n            gt_eval_boxes = EvalBoxes(gt_boxes,\n                                      box_type_3d=box_type_3d,\n                                      box_mode_3d=box_mode_3d,\n                                      class_field='label')\n            gt_annos_it.append(gt_eval_boxes)\n\n        result_path[f'metric_{i}'] = eval_config.eval_main(\n            gt_annos=gt_annos_it,\n            dt_annos=dt_annos"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        from .LiDAR_box import LiDARBox\n        return LiDARBox, \"LiDAR\"\n    elif box_type == \"Camera\":\n        from .camera_box import CameraBox\n        return CameraBox, \"Camera\"\n    elif box_type == \"Depth\":\n        from .depth_box import DepthBox\n        return DepthBox, \"Depth\"\n    else:\n        raise ValueError(\"box_type must be one of LiDAR, Camera, or Depth\")"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        from .lidar_box import LiDARBox\n\n        return LiDARBox, \"LiDAR\"\n    elif box_type == \"Camera\":\n        from .camera_box import CameraBox\n\n        return CameraBox, \"Camera\"\n    elif box_type == \"Depth\":\n        from .depth_box import DepthBox\n\n        return DepthBox, \"Depth\"\n    else:\n        raise ValueError(\"box_type must be one of LiDAR, Camera, or Depth\")"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        from .LiDAR import LiDARBox3D\n        return LiDARBox3D, \"LiDAR\"\n    elif box_type == \"Camera\":\n        from .Camera import CameraBox3D\n        return CameraBox3D, \"Camera\"\n    elif box_type == \"Depth\":\n        from .Depth import DepthBox3D\n        return DepthBox3D, \"Depth\"\n    else:\n        raise ValueError(f\"Unknown box type: {box_type}\")"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        from .LiDAR_box import LiDARBox\n        return LiDARBox, \"LiDAR\"\n    elif box_type == \"Camera\":\n        from .Camera_box import CameraBox\n        return CameraBox, \"Camera\"\n    elif box_type == \"Depth\":\n        from .Depth_box import DepthBox\n        return DepthBox, \"Depth\"\n    else:\n        raise ValueError(\"box_type should be one of LiDAR, Camera, or Depth\")"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        from .lidar_box import LiDARBox\n        return LiDARBox, \"LiDAR\"\n    elif box_type == \"Camera\":\n        from .camera_box import CameraBox\n        return CameraBox, \"Camera\"\n    elif box_type == \"Depth\":\n        from .depth_box import DepthBox\n        return DepthBox, \"Depth\"\n    else:\n        raise ValueError(f\"Unknown box type: {box_type}\")"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_type_3d = \"LiDARBox3D\"\n        mode = \"LiDAR\"\n    elif box_type == \"Camera\":\n        box_type_3d = \"CameraBox3D\"\n        mode = \"Camera\"\n    elif box_type == \"Depth\":\n        box_type_3d = \"DepthBox3D\"\n        mode = \"Depth\"\n    else:\n        raise ValueError(\"Error: %s is not a valid box type\" % box_type)\n\n    return box_type_3d, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        from .LidarBox import LidarBox\n        return LidarBox, \"LiDAR\"\n    elif box_type == \"Camera\":\n        from .CameraBox import CameraBox\n        return CameraBox, \"Camera\"\n    elif box_type == \"Depth\":\n        from .DepthBox import DepthBox\n        return DepthBox, \"Depth\"\n    else:\n        raise ValueError(f\"Unknown box type {box_type}\")"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_class = LiDARInstance3DBoxes\n        mode = \"LiDAR\"\n    elif box_type == \"Camera\":\n        box_class = CameraInstance3DBoxes\n        mode = \"Camera\"\n    elif box_type == \"Depth\":\n        box_class = DepthInstance3DBoxes\n        mode = \"Depth\"\n    else:\n        raise ValueError(f\"Unrecognized box type {box_type}\")\n\n    return box_class, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_type_3d = \"Box3D\"\n        mode = \"lidar\"\n    elif box_type == \"Camera\":\n        box_type_3d = \"Box3D\"\n        mode = \"camera\"\n    elif box_type == \"Depth\":\n        box_type_3d = \"DepthBox3D\"\n        mode = \"depth\"\n    else:\n        raise ValueError(\"Error: %s is not among the 3D box structure types!\" % box_type)\n\n    return box_type_3d, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_type_3d = \"LiDARBox3D\"\n        mode = \"LiDAR\"\n    elif box_type == \"Camera\":\n        box_type_3d = \"CameraBox3D\"\n        mode = \"Camera\"\n    elif box_type == \"Depth\":\n        box_type_3d = \"DepthBox3D\"\n        mode = \"Depth\"\n    else:\n        raise ValueError(\"Error: %s is not among known box_types in the config file\" % box_type)\n\n    return box_type_3d, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        from .LiDAR_box import LiDARBox\n\n        box_class = LiDARBox\n        box_mode = \"LiDAR\"\n    elif box_type == \"Camera\":\n        from .camera_box import CameraBox\n\n        box_class = CameraBox\n        box_mode = \"Camera\"\n    elif box_type == \"Depth\":\n        from .depth_box import DepthBox\n\n        box_class = DepthBox\n        box_mode = \"Depth\"\n    else:\n        raise ValueError(f\"Unknown box type {box_type}\")\n\n    return box_class, box_mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        from .lidar_box import LiDARBox\n        return LiDARBox, \"LiDAR\"\n    elif box_type == \"Camera\":\n        from .camera_box import CameraBox\n        return CameraBox, \"Camera\"\n    elif box_type == \"Depth\":\n        from .depth_box import DepthBox\n        return DepthBox, \"Depth\"\n    else:\n        raise ValueError(\"box_type must be one of LiDAR, Camera, or Depth\")\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        from .box_3d_lidar import Box3DLidar\n        return Box3DLidar, \"LiDAR\"\n    elif box_type == \"Camera\":\n        from .box_3d_camera import Box3DCamera\n        return Box3DCamera, \"Camera\"\n    elif box_type == \"Depth\":\n        from .box_3d_depth import Box3DDepth\n        return Box3DDepth, \"Depth\"\n    else:\n        raise ValueError(\"box_type must be one of LiDAR, Camera, or Depth\")\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_type_3d = LiDARInstance3DBoxes\n        rcnn_mode = \"xyzwhl\"\n    elif box_type == \"Camera\":\n        box_type_3d = CameraInstance3DBoxes\n        rcnn_mode = \"xyxy\"\n    elif box_type == \"Depth\":\n        box_type_3d = DepthInstance3DBoxes\n        rcnn_mode = \"xyzwhl\"\n    else:\n        raise ValueError(f\"Unrecognized box type {box_type}\")\n    return box_type_3d, rcnn_mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        from .LidarBox import LidarBox\n\n        box_class = LidarBox\n        mode = \"LiDAR\"\n    elif box_type == \"Camera\":\n        from .CameraBox import CameraBox\n\n        box_class = CameraBox\n        mode = \"Camera\"\n    elif box_type == \"Depth\":\n        from .DepthBox import DepthBox\n\n        box_class = DepthBox\n        mode = \"Depth\"\n    else:\n        raise ValueError(f\"Unknown box type: {box_type}\")\n\n    return box_class, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        from .LidarBox3d import LidarBox3d\n\n        box_mode = LidarBox3d.box_mode\n        return LidarBox3d, box_mode\n    elif box_type == \"Camera\":\n        from .CameraBox3d import CameraBox3d\n\n        box_mode = CameraBox3d.box_mode\n        return CameraBox3d, box_mode\n    elif box_type == \"Depth\":\n        from .DepthBox3d import DepthBox3d\n\n        box_mode = DepthBox3d.box_mode\n        return DepthBox3d, box_mode\n    else:\n        raise ValueError(\"unknown box type\")"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_type_3d = \"LiDARBox3D\"\n        box_mode = Box3DMode.LiDAR\n    elif box_type == \"Camera\":\n        box_type_3d = \"CameraInstance3DBoxes\"\n        box_mode = Box3DMode.DEPTH\n    elif box_type == \"Depth\":\n        box_type_3d = \"DepthInstance3DBoxes\"\n        box_mode = Box3DMode.DEPTH\n    else:\n        raise ValueError(\"Error: Invalid box_type {}\".format(box_type))\n\n    return box_type_3d, box_mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_type_3d = \"LiDARBox3D\"\n        box_mode_3d = \"LiDAR\"\n    elif box_type == \"Camera\":\n        box_type_3d = \"CameraBox3D\"\n        box_mode_3d = \"Camera\"\n    elif box_type == \"Depth\":\n        box_type_3d = \"DepthBox3D\"\n        box_mode_3d = \"Depth\"\n    else:\n        raise ValueError(\"Error: Invalid box type: {}\".format(box_type))\n\n    return box_type_3d, box_mode_3d\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        from .LiDARBox import LiDARBox\n        box_class = LiDARBox\n        mode = \"LiDAR\"\n    elif box_type == \"Camera\":\n        from .CameraBox import CameraBox\n        box_class = CameraBox\n        mode = \"Camera\"\n    elif box_type == \"Depth\":\n        from .DepthBox import DepthBox\n        box_class = DepthBox\n        mode = \"Depth\"\n    else:\n        raise ValueError(\"Unknown box type: {}\".format(box_type))\n\n    return box_class, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        from .LiDAR_box import LiDARBox, LiDARBoxMode\n        return LiDARBox, LiDARBoxMode\n    elif box_type == \"Camera\":\n        from .camera_box import CameraBox, CameraBoxMode\n        return CameraBox, CameraBoxMode\n    elif box_type == \"Depth\":\n        from .depth_box import DepthBox, DepthBoxMode\n        return DepthBox, DepthBoxMode\n    else:\n        raise ValueError(\"Unknown box type: {}\".format(box_type))\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('model is required')\n\n    if messages:\n      if not isinstance(messages, list):\n        raise TypeError('messages must be a list')\n      for message in messages:\n        if not isinstance(message, Message):\n          if not isinstance(message, dict):\n            raise TypeError('messages must be a list of Message or dict-like objects')\n          message = Message(**message)\n        if not message.role:\n          raise RequestError('message.role is required')\n        if not message.content:\n          raise RequestError('message.content is required')\n        if message.images:\n          message.images = [\n            Image(**image) if isinstance(image, dict) else image for image in message.images\n          ]\n\n    if format == 'json':\n      options = options or Options()\n      options.format = 'json'\n\n    if stream:\n      return self._chat_stream(\n        model=model,\n        messages=messages,\n        options=options,\n        keep_alive=keep_alive,\n      )\n\n    return self._chat(\n      model=model,\n      messages=messages,\n      options=options,\n      keep_alive=keep_alive,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model == '':\n      raise RequestError('model is required')\n\n    if messages is None:\n      messages = []\n    elif not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n    elif not all(isinstance(m, (Message, dict)) for m in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if format == 'json':\n      options = options or Options()\n      options.format = format\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is not None:\n      options.keep_alive = keep_alive\n\n    if format == 'json':\n      options.format = 'json'\n\n    if format == '':\n      options.format = ''\n\n    if format == 'json':\n      options.format = 'json'\n\n    if format == '':\n      options.format = ''\n\n    if format == 'json':\n      options.format = 'json'\n\n    if format == '':\n      options.format = ''\n\n    if format == 'json':\n      options.format = 'json'\n\n    if format == '':\n      options.format = ''\n\n    if format == 'json':\n      options.format = 'json'\n\n    if format == '':\n      options.format = ''\n\n    if format == 'json':\n      options.format = 'json'\n\n    if format == '':\n      options.format = ''\n\n    if format == 'json':\n      options.format = 'json'\n\n    if format == '':\n      options.format = ''\n\n    if format == 'json':\n      options.format = 'json'\n\n    if format == '':\n      options.format = ''\n\n    if format == 'json':\n      options.format = 'json'\n\n    if format == '':\n      options.format = ''\n\n    if format == 'json':\n      options.format = 'json'\n\n    if format == '':\n      options.format = ''\n\n    if format =="}
{"namespace": "ollama._client.Client.chat", "completion": "    # Check if model is provided\n    if not model:\n      raise RequestError('You must provide a model parameter.')\n\n    # Validate messages\n    if messages is None:\n      messages = []\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n    for message in messages:\n      if not isinstance(message, (Message, dict)):\n        raise TypeError(\n          'message must be a dict or Message, not ' + type(message).__name__\n        )\n\n    # Create request\n    request = {\n      'model': model,\n      'messages': messages,\n      'stream': stream,\n      'format': format,\n    }\n    if options is not None:\n      request.update(options)\n\n    # Send request\n    if not stream:\n      return self.request(\n        'chat',\n        request,\n        keep_alive=keep_alive,\n      )\n    else:\n      return self.request_stream(\n        'chat',\n        request,\n        keep_alive=keep_alive,\n      )\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('model is required')\n\n    if not messages:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n\n    if not all(isinstance(m, (Message, dict)) for m in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not isinstance(stream, bool):\n      raise TypeError('stream must be a bool')\n\n    if format not in ('', 'json'):\n      raise ValueError('format must be one of (\"\", \"json\")')\n\n    if options is not None and not isinstance(options, Options):\n      raise TypeError('options must be an instance of Options')\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, str):\n        try:\n          keep_alive = float(keep_alive)\n        except ValueError:\n          raise ValueError('keep_alive must be a float or a string representing a float')\n      elif not isinstance(keep_alive, float):\n        raise TypeError('keep_alive must be a float or a string representing a float')\n\n    if keep_alive is not None:\n      keep_alive = int(keep_alive)\n\n    if format == 'json':\n      response = self.request(\n        'POST',\n        '/chat/completions',\n        json={\n          'model': model,\n          'messages': messages,\n          'stream': stream,\n          'options': options,\n          'keep_alive': keep_alive,\n        },\n        stream=stream,\n      )\n    else:\n      response = self.request(\n        'POST',\n        '/chat/completions',\n        json={\n          'model': model,\n          'messages': messages,\n          'stream': stream,\n          'options': options,\n          'keep_alive': keep_alive,\n        },\n        stream=stream,\n        accept='text/event-stream',\n      )\n\n    if response."}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('model is required')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n\n    if not all(isinstance(m, (Message, dict)) for m in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not isinstance(stream, bool):\n      raise TypeError('stream must be a boolean')\n\n    if format not in ('', 'json'):\n      raise ValueError('format must be either \"\" or \"json\"')\n\n    if options is not None and not isinstance(options, dict):\n      raise TypeError('options must be a dict')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError('keep_alive must be a float or string')\n\n    if keep_alive is not None:\n      try:\n        keep_alive = float(keep_alive)\n      except ValueError:\n        raise ValueError('keep_alive must be a float or a string representing a float')\n\n    if keep_alive is not None:\n      if keep_alive <= 0:\n        raise ValueError('keep_alive must be greater than 0')\n\n    if keep_alive is not None:\n      keep_alive = str(keep_alive)\n\n    if keep_alive is not None:\n      if keep_alive.endswith('.0'):\n        keep_alive = keep_alive[:-2]\n\n    if keep_alive is not None:\n      if '.' in keep_alive:\n        keep_alive = float(keep_alive)\n\n    if keep_alive is not None:\n      if keep_alive < 0:\n        raise ValueError('keep_alive must be greater than 0')\n\n    if keep_alive is not None:\n      if keep_alive > 10:\n        raise ValueError('keep_alive must be less than 1"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('model is required')\n\n    if not messages:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    for message in messages:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not isinstance(options, dict):\n      options = {}\n\n    if keep_alive:\n      options['keep_alive'] = keep_alive\n\n    if format == 'json':\n      options['response_format'] = 'json'\n\n    options['stream'] = stream\n\n    return self.request(\n      'chat',\n      model=model,\n      messages=[\n        {\n          'role': message['role'],\n          'content': message['content'],\n          'images': [\n            {\n              'data': encode_image(image)\n            }\n            for image in message.get('images', [])\n          ]\n        }\n        for message in messages\n      ],\n      options=options,\n    )\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('You must provide a model identifier.')\n\n    if not messages:\n      messages = []\n    elif not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n    else:\n      messages = [\n        m if isinstance(m, Message) else Message(**m)\n        for m in messages\n      ]\n\n    if format == 'json':\n      options = options or Options()\n      options.response_format = format\n\n    if stream:\n      return self._chat_stream(\n        model,\n        messages,\n        options=options,\n        keep_alive=keep_alive,\n      )\n\n    response = self._request(\n      'chat',\n      model,\n      messages,\n      options=options,\n      keep_alive=keep_alive,\n    )\n\n    if format == 'json':\n      return response\n\n    return ChatResponse.from_dict(response)\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('You must provide a model')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError(\n        'messages must be a list of Message or dict-like objects'\n      )\n\n    if not all(isinstance(m, (Message, dict)) for m in messages):\n      raise TypeError(\n        'messages must be a list of Message or dict-like objects'\n      )\n\n    if not messages:\n      messages = [{'role': 'system', 'content': model}]\n    elif not isinstance(messages[0], dict):\n      messages = [m.dict() for m in messages]\n\n    if not messages[0].get('role'):\n      messages[0]['role'] = 'system'\n\n    if not messages[0].get('content'):\n      messages[0]['content'] = model\n\n    if not messages[0].get('name'):\n      messages[0]['name'] = 'system'\n\n    if not messages[0].get('function_call'):\n      messages[0]['function_call'] = None\n\n    if not messages[0].get('function_name'):\n      messages[0]['function_name'] = None\n\n    if not messages[0].get('function_to_call'):\n      messages[0]['function_to_call'] = None\n\n    if not messages[0].get('arguments'):\n      messages[0]['arguments'] = None\n\n    if not messages[0].get('is_chat_function'):\n      messages[0]['is_chat_function'] = None\n\n    if not messages[0].get('is_chat_function_call'):\n      messages[0]['is_chat_function_call'] = None\n\n    if not messages[0].get('function_call'):\n      messages[0]['function_call'] = None\n\n    if not messages[0].get('function_name'):\n      messages[0]['function_name'] = None\n\n    if not messages[0].get('"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('You must provide a model')\n\n    if not messages:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n\n    if not all(isinstance(message, (Message, dict)) for message in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if options is None:\n      options = {}\n\n    if keep_alive is not None:\n      options['timeout'] = keep_alive\n\n    if format == 'json':\n      options['response_format'] = 'json'\n\n    try:\n      response = self.request(\n        'POST',\n        f'{self.api_base}/chat/completions',\n        json={\n          'model': model,\n          'messages': messages,\n          'stream': stream,\n          **options,\n        },\n        stream=stream,\n      )\n    except ResponseError as e:\n      if e.status == 400:\n        raise RequestError(e.message) from e\n      raise\n\n    if not stream:\n      return response\n\n    def iterator():\n      for chunk in response.iter_bytes():\n        if format == 'json':\n          yield json.loads(chunk)\n        else:\n          yield chunk\n\n    return iterator()\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model == '':\n      raise RequestError('model is required')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError(f'messages must be a list of Message or dict-like objects, not {type(messages)}')\n\n    for message in messages:\n      if not isinstance(message, Message):\n        if not isinstance(message, dict):\n          raise TypeError(f'message must be a Message or dict-like object, not {type(message)}')\n        message = Message(**message)\n\n    if format == 'json':\n      if options is None:\n        options = {}\n      options = {**options, 'format': 'json'}\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, float):\n        keep_alive = int(keep_alive)\n      if not isinstance(keep_alive, int):\n        raise TypeError(f'keep_alive must be a float or int, not {type(keep_alive)}')\n\n    if options is None:\n      options = {}\n\n    if stream:\n      return self.chat_stream(model, messages, options, keep_alive)\n\n    else:\n      return self.chat_complete(model, messages, options, keep_alive)\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('You must provide a model')\n\n    if not messages:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n\n    if not all(isinstance(m, (Message, dict)) for m in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not isinstance(options, Options):\n      options = Options(**options or {})\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, str):\n        keep_alive = keep_alive.lower()\n        if keep_alive == 'true':\n          keep_alive = True\n        elif keep_alive == 'false':\n          keep_alive = False\n        else:\n          raise TypeError('keep_alive must be a boolean or a string')\n      if not isinstance(keep_alive, bool):\n        raise TypeError('keep_alive must be a boolean or a string')\n\n    if stream:\n      return self._chat_stream(\n        model,\n        messages,\n        format,\n        options,\n        keep_alive,\n      )\n    else:\n      return self._chat(\n        model,\n        messages,\n        format,\n        options,\n        keep_alive,\n      )\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model is None:\n      raise RequestError('model is required')\n    if messages is None:\n      messages = []\n    if isinstance(messages, dict):\n      messages = [messages]\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n    for message in messages:\n      if not isinstance(message, (Message, dict)):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if 'role' not in message:\n        raise RequestError('messages must have a role')\n      if 'content' not in message:\n        raise RequestError('messages must have content')\n      if 'images' in message:\n        message['images'] = [\n          image.encode() if isinstance(image, Image) else image\n          for image in message['images']\n        ]\n    if not isinstance(stream, bool):\n      raise TypeError('stream must be a bool')\n    if format not in ['', 'json']:\n      raise TypeError('format must be one of [\"\", \"json\"]')\n    if options is not None and not isinstance(options, dict):\n      raise TypeError('options must be a dict')\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError('keep_alive must be a float or str')\n\n    params = {\n      'model': model,\n      'messages': messages,\n      'stream': stream,\n      'format': format,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    try:\n      response = self.request('chat', params)\n    except ResponseError as e:\n      raise e\n\n    if stream:\n      return (\n        ChatResponse(response)\n        for response in response.iter_lines()\n        if response\n      )\n    else:\n      return ChatResponse(response)"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('You must provide a model parameter.')\n\n    if not messages:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n\n    if not all(isinstance(m, (Message, dict)) for m in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if format not in ('', 'json'):\n      raise RequestError(f'Invalid format {format}')\n\n    if options is None:\n      options = {}\n\n    if isinstance(keep_alive, str):\n      keep_alive = keep_alive.lower()\n      if keep_alive == 'true':\n        keep_alive = True\n      elif keep_alive == 'false':\n        keep_alive = False\n      else:\n        raise RequestError(f'Invalid value for keep_alive: {keep_alive}')\n\n    if keep_alive is True:\n      keep_alive = 'true'\n\n    if keep_alive is False:\n      keep_alive = 'false'\n\n    if keep_alive is not None:\n      options['keep_alive'] = keep_alive\n\n    if stream:\n      return self.chat_stream(model, messages, options)\n\n    data = {\n      'model': model,\n      'messages': [m.dict() if isinstance(m, Message) else m for m in messages],\n      'options': options,\n    }\n\n    if format:\n      data['format'] = format\n\n    headers = {'Content-Type': 'application/json'}\n    response = self.request(\n      'POST',\n      '/chat/completions',\n      json=data,\n      headers=headers,\n    )\n\n    if response.status_code != 200:\n      raise ResponseError(response)\n\n    return response.json()\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError(f'model is required')\n\n    if messages is None:\n      messages = []\n    elif not isinstance(messages, list):\n      raise TypeError(f'messages must be a list of Message or dict-like objects')\n    else:\n      messages = [\n        message if isinstance(message, Message) else Message(**message)\n        for message in messages\n      ]\n\n    if options is None:\n      options = {}\n    elif isinstance(options, dict):\n      options = Options(**options)\n\n    if isinstance(keep_alive, (float, str)):\n      keep_alive = float(keep_alive)\n    elif keep_alive is not None:\n      raise TypeError(f'keep_alive must be a float or str')\n\n    request = {\n      'model': model,\n      'messages': messages,\n      'stream': stream,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    if format == 'json':\n      request = json.dumps(request, cls=JSONEncoder)\n\n    if stream:\n      return self.post(\n        'chat/completions',\n        data=request,\n        stream=True,\n        headers={'Accept': 'text/event-stream'},\n      )\n    else:\n      return self.post('chat/completions', data=request)\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError(f'model is required')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError(f'messages must be a list of Message or dict-like objects')\n\n    if any(not isinstance(m, (Message, dict)) for m in messages):\n      raise TypeError(f'messages must be a list of Message or dict-like objects')\n\n    messages = [Message(**m) if isinstance(m, dict) else m for m in messages]\n\n    if not all(m.role in ['system', 'user', 'assistant'] for m in messages):\n      raise RequestError(f'messages must have roles of system, user, or assistant')\n\n    if not all(m.content for m in messages):\n      raise RequestError(f'messages must have content')\n\n    if any(m.images for m in messages):\n      for m in messages:\n        for i, image in enumerate(m.images):\n          m.images[i] = self.encode(image)\n\n    if options is None:\n      options = {}\n\n    if keep_alive is not None:\n      options['keep_alive'] = keep_alive\n\n    if format == 'json':\n      options['response_format'] = 'json'\n\n    response = self.request(\n      'POST',\n      f'chat/{model}/chat',\n      json={\n        'messages': messages,\n        'options': options,\n      },\n      stream=stream,\n    )\n\n    if stream:\n      return (ChatResponse(**r) for r in response)\n\n    return ChatResponse(**response)\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model is None:\n      raise RequestError('model is required')\n\n    if messages is None:\n      messages = []\n    elif isinstance(messages, dict):\n      messages = [messages]\n    elif not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if format == 'json':\n      format = 'json_lines'\n\n    if options is None:\n      options = {}\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, str):\n        keep_alive = float(keep_alive)\n      options['keep_alive'] = keep_alive\n\n    if 'model' not in options:\n      options['model'] = model\n\n    if 'messages' not in options:\n      options['messages'] = messages\n\n    if 'format' not in options:\n      options['format'] = format\n\n    if 'stream' not in options:\n      options['stream'] = stream\n\n    if 'keep_alive' not in options:\n      options['keep_alive'] = keep_alive\n\n    if 'images' in options:\n      for image in options['images']:\n        if not isinstance(image, Image):\n          image = Image(**image)\n        image.encode()\n\n    if 'function_call' in options:\n      if not isinstance(options['function_call'], FunctionCall):\n        options['function_call'] = FunctionCall(**options['function_call'])\n\n    if 'function_call' in options:\n      if options['function_call'].arguments is not None:\n        for argument in options['function_call'].arguments:\n          if not isinstance(argument, FunctionCallArgument):\n            argument = FunctionCallArgument(**argument)\n\n    if 'function_call' in options:\n      if options['function_call'].function_name is not None:\n        if not isinstance(options['function_call'].function_name, str):\n          options['function_call'].function_name = str(options['function_call'].function_name)\n\n    if 'function_"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('model is required')\n    if not messages:\n      messages = []\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n    if not all(isinstance(m, (Message, dict)) for m in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    data = {\n      'model': model,\n      'messages': [m if isinstance(m, dict) else m.dict() for m in messages],\n      'stream': stream,\n      'format': format,\n      'options': options.dict() if options else {},\n      'keep_alive': keep_alive,\n    }\n\n    r = self.post('/chat', json=data)\n    if r.status_code != 200:\n      raise ResponseError(r.status_code, r.text)\n\n    if not stream:\n      return ChatResponse.parse_obj(r.json())\n\n    def gen() -> Iterator[ChatResponse]:\n      for line in r.iter_lines():\n        if not line:\n          continue\n        data = json.loads(line)\n        yield ChatResponse.parse_obj(data)\n\n    return gen()\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('You must provide a model')\n    if not messages:\n      messages = []\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n    if not all(isinstance(m, (Message, dict)) for m in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n    if format and format not in ('', 'json'):\n      raise ValueError('format must be either \"\" or \"json\"')\n\n    if not options:\n      options = {}\n\n    if keep_alive:\n      options['keep_alive'] = keep_alive\n\n    if not isinstance(keep_alive, (float, str)):\n      raise TypeError('keep_alive must be a float or a string')\n\n    data = {\n      'model': model,\n      'messages': [m if isinstance(m, dict) else m.dict() for m in messages],\n      'stream': stream,\n      'format': format,\n      **options,\n    }\n\n    if stream:\n      return self.request_stream(\n        'chat',\n        data=data,\n        data_type='json',\n        response_type='json_stream',\n      )\n\n    return self.request(\n      'chat',\n      data=data,\n      data_type='json',\n      response_type='json',\n    )\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError(f'No model provided')\n\n    if messages is None:\n      messages = []\n\n    if isinstance(messages, (str, bytes)):\n      raise TypeError(f'messages must be a list of Message or dict-like objects')\n\n    if isinstance(messages, Mapping):\n      messages = [messages]\n\n    if not isinstance(messages, list):\n      raise TypeError(f'messages must be a list of Message or dict-like objects')\n\n    for message in messages:\n      if not isinstance(message, (Message, Mapping)):\n        raise TypeError(f'messages must be a list of Message or dict-like objects')\n\n    if format == 'json':\n      request_body = {\n        'model': model,\n        'messages': [\n          message if isinstance(message, Mapping) else message.dict()\n          for message in messages\n        ],\n        'stream': stream,\n      }\n      if options:\n        request_body.update(options.dict())\n      if keep_alive:\n        request_body['keep_alive'] = keep_alive\n\n      try:\n        response = self._request(\n          method='POST',\n          path='/chat/completions',\n          json=request_body,\n          stream=stream,\n        )\n      except ResponseError as e:\n        raise e\n\n      if stream:\n        return self._process_chat_stream(response)\n      else:\n        return ChatResponse.from_dict(response.json())\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('model is required')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n\n    if not all(isinstance(m, (Message, dict)) for m in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not messages:\n      raise RequestError('messages is required')\n\n    if not isinstance(stream, bool):\n      raise TypeError('stream must be a bool')\n\n    if format not in ('', 'json'):\n      raise RequestError('format must be one of (\"\", \"json\")')\n\n    if options and not isinstance(options, Options):\n      raise TypeError('options must be an instance of Options')\n\n    if keep_alive:\n      if isinstance(keep_alive, str):\n        keep_alive = float(keep_alive)\n      elif not isinstance(keep_alive, float):\n        raise TypeError('keep_alive must be a float or a string')\n\n    if keep_alive and not 0 < keep_alive <= 120:\n      raise RequestError('keep_alive must be between 0 and 120')\n\n    if keep_alive and not stream:\n      raise RequestError('keep_alive requires stream=True')\n\n    data = {\n      'model': model,\n      'messages': [\n        m if isinstance(m, dict) else m.dict()\n        for m in messages\n      ],\n      'stream': stream,\n    }\n\n    if options:\n      data.update(options.dict())\n\n    if keep_alive:\n      data['keep_alive'] = keep_alive\n\n    if format:\n      data['format'] = format\n\n    if stream:\n      return self.stream_chat(data)\n    else:\n      return self.request_chat(data)\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    pass\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    response = self.post(\n      \"models/{model}/pull\".format(model=model),\n      insecure=insecure,\n      stream=stream,\n    )\n\n    if not stream:\n      return response.json()\n\n    return response.iter_json()"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = f\"{self._url}/models/{model}/pull\"\n    params = {\"insecure\": insecure}\n\n    try:\n      response = self._session.post(url, params=params, stream=stream)\n      response.raise_for_status()\n    except requests.exceptions.HTTPError as e:\n      raise ResponseError(e)\n\n    if stream:\n      return (ProgressResponse(**r.json()) for r in response.iter_lines())\n    else:\n      return ProgressResponse(**response.json())\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    if stream:\n      return self._stream(\n        self._post,\n        f'/{model}/pull',\n        expect_progress=True,\n        insecure=insecure,\n      )\n    return self._post(\n      f'/{model}/pull',\n      expect_progress=True,\n      insecure=insecure,\n    )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    # Send a POST request to the server to pull the model\n    response = self._post(\n      f\"/v1/models/{model}/pull\",\n      stream=stream,\n      insecure=insecure,\n    )\n\n    # If the request fails, raise a ResponseError\n    if not response.ok:\n      raise ResponseError(response)\n\n    # If the response is streamed, return an iterator of ProgressResponse objects\n    if stream:\n      return map(ProgressResponse.parse, response.iter_lines())\n\n    # Otherwise, return a single ProgressResponse object\n    return ProgressResponse.parse(response.json())\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = f\"{self.base_url}/models/{model}/pull\"\n    headers = {\"Content-Type\": \"application/json\"}\n    body = {\"insecure\": insecure}\n\n    if stream:\n      return self._stream(url, headers, body)\n    else:\n      return self._request(url, headers, body)\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    if stream:\n      return self.pull_stream(model, insecure)\n    else:\n      return self.pull_block(model, insecure)\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = f\"{self.base_url}/models/{model}/pull\"\n    params = {\"stream\": stream}\n    try:\n      r = self.session.post(url, params=params, verify=not insecure)\n      r.raise_for_status()\n    except requests.exceptions.HTTPError as e:\n      raise ResponseError(e)\n    if stream:\n      return r.iter_lines()\n    else:\n      return r.json()"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = f\"{self._url}/models/{model}/pull\"\n    params = {\n      \"insecure\": insecure,\n    }\n    if stream:\n      return self._stream_response(url, params=params)\n    return self._get_response(url, params=params)\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = self._url.join(f\"/v1/models/{model}/pull\")\n    params = {\"stream\": stream}\n    with self._client.stream(\n      \"POST\",\n      url,\n      params=params,\n      insecure=insecure,\n      stream=stream,\n    ) as response:\n      return response.iter_json()\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = f\"{self.server_url}/api/v1/models/{model}/pull\"\n    headers = {\"Authorization\": f\"Bearer {self.token}\"}\n    with httpx.stream(\"POST\", url, headers=headers, verify=not insecure) as response:\n      if response.is_error:\n        raise ResponseError(response)\n\n      if stream:\n        yield from response.aiter_json()\n      else:\n        return response.json()\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    if stream:\n      return self._pull_stream(model=model, insecure=insecure)\n    else:\n      return self._pull(model=model, insecure=insecure)\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    headers = {\n      'Content-Type': 'application/json',\n    }\n\n    params = {\n      'model': model,\n      'insecure': insecure,\n    }\n\n    if stream:\n      response = self._stream_request(\n        method='POST',\n        url='/v1/models/pull',\n        params=params,\n        headers=headers,\n      )\n\n      return (\n        ProgressResponse(**response)\n        for response in response\n      )\n\n    else:\n      response = self._request(\n        method='POST',\n        url='/v1/models/pull',\n        params=params,\n        headers=headers,\n      )\n\n      return ProgressResponse(**response)\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    if stream:\n      return self.stream(\n        self._pull,\n        model=model,\n        insecure=insecure,\n      )\n    else:\n      return self._pull(\n        model=model,\n        insecure=insecure,\n      )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    try:\n      response = self._post(\n        f\"{self._api_prefix}/models/{model}/pull\",\n        insecure=insecure,\n        stream=stream,\n      )\n    except ResponseError as error:\n      raise ResponseError(\n        f\"Failed to pull model {model}\",\n        error.response,\n        error.response.status_code,\n      ) from error\n\n    if stream:\n      return (\n        ProgressResponse(**item)\n        for item in response.iter_json()\n      )\n    return ProgressResponse(**response.json())"}
{"namespace": "ollama._client.Client.pull", "completion": "    def _iter():\n      response = self.post(\n        f\"{self.url}/models/{model}/pull\",\n        stream=True,\n        insecure=insecure,\n      )\n      for line in response.iter_lines():\n        if line:\n          yield ProgressResponse.parse_raw(line)\n\n    if stream:\n      return _iter()\n\n    response = self.post(\n      f\"{self.url}/models/{model}/pull\",\n      stream=False,\n      insecure=insecure,\n    )\n    if response.status_code != 200:\n      raise ResponseError(response)\n    return ProgressResponse.parse_raw(response.content)\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    try:\n      response = self._client.post(\n        f\"/models/{model}/pull\",\n        stream=stream,\n        insecure=insecure,\n      )\n    except ResponseError as error:\n      raise ResponseError(\n        f\"Failed to pull model {model}: {error.message}\"\n      ) from error\n\n    if stream:\n      return map(ProgressResponse.from_response, response)\n\n    return ProgressResponse.from_response(response)"}
{"namespace": "ollama._client.Client.pull", "completion": "    if stream:\n      return self._pull_stream(model, insecure)\n    else:\n      return self._pull_block(model, insecure)\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    # Build the request URL\n    url = f'{self.base_url}/models/{model}/pull'\n\n    # Create the request headers\n    headers = {\n      'Content-Type': 'application/json',\n    }\n\n    # Create the request data\n    data = {\n      'insecure': insecure,\n    }\n\n    # Send the request and handle the response\n    response = requests.post(url, headers=headers, data=json.dumps(data))\n    if response.status_code != 200:\n      raise ResponseError(response.status_code, response.text)\n\n    # If the response is not streamed, return a single ProgressResponse object\n    if not stream:\n      return ProgressResponse.from_json(response.text)\n\n    # If the response is streamed, return an iterator of ProgressResponse objects\n    return self._stream_response(response, ProgressResponse.from_json)\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    # Send a POST request to pull a model from a server.\n    #\n    # Args:\n    #   model: The name of the model to be pulled from the server.\n    #   insecure: If True, the request will be made over an insecure connection.\n    #   stream: If True, the response will be streamed.\n    #\n    # Returns:\n    #   Union[Mapping[str, Any], Iterator[Mapping[str, Any]]]:\n    #     - If stream is False, a `ProgressResponse` object is returned.\n    #     - If stream is True, an iterator of `ProgressResponse` objects is returned.\n    #\n    # Raises:\n    #   ResponseError: If the request fails.\n\n    # Check if the model name is valid\n    if not isinstance(model, str):\n      raise ValueError(\"Model name must be a string\")\n\n    # Check if the model name is not empty\n    if not model:\n      raise ValueError(\"Model name cannot be empty\")\n\n    # Check if the model name is not too long\n    if len(model) > 255:\n      raise ValueError(\"Model name cannot be longer than 255 characters\")\n\n    # Check if the model name contains only valid characters\n    if not re.match(r\"^[a-zA-Z0-9_\\-]+$\", model):\n      raise ValueError(\"Model name can only contain alphanumeric characters, underscores, and hyphens\")\n\n    # Check if the model name does not start with a number\n    if re.match(r\"^[0-9]\", model):\n      raise ValueError(\"Model name cannot start with a number\")\n\n    # Check if the model name does not start with a hyphen\n    if re.match(r\"^\\-\", model):\n      raise ValueError(\"Model name cannot start with a hyphen\")\n\n    # Check if the model name does not end with a hyphen\n    if re.match(r\"\\-$\", model):\n      raise ValueError(\"Model name cannot end with a hyphen\")\n\n    # Check if the model name does not contain consecutive hyphens\n   "}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not stream:\n      return self._request(\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        stream=stream,\n        raw=raw,\n        format=format,\n        images=images,\n        options=options,\n        keep_alive=keep_alive,\n      )\n\n    return self._request_stream(\n      model=model,\n      prompt=prompt,\n      system=system,\n      template=template,\n      context=context,\n      stream=stream,\n      raw=raw,\n      format=format,\n      images=images,\n      options=options,\n      keep_alive=keep_alive,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not prompt and not images:\n      raise ValueError('prompt or images is required')\n\n    if not prompt and not system:\n      raise ValueError('prompt or system is required')\n\n    if not prompt and not template:\n      raise ValueError('prompt or template is required')\n\n    if not prompt and not context:\n      raise ValueError('prompt or context is required')\n\n    if not prompt and not options:\n      raise ValueError('prompt or options is required')\n\n    if not prompt and not keep_alive:\n      raise ValueError('prompt or keep_alive is required')\n\n    if not prompt and not format:\n      raise ValueError('prompt or format is required')\n\n    if not prompt and not raw:\n      raise ValueError('prompt or raw is required')\n\n    if not prompt and not stream:\n      raise ValueError('prompt or stream is required')\n\n    if not prompt and not context:\n      raise ValueError('prompt or context is required')\n\n    if not prompt and not options:\n      raise ValueError('prompt or options is required')\n\n    if not prompt and not keep_alive:\n      raise ValueError('prompt or keep_alive is required')\n\n    if not prompt and not format:\n      raise ValueError('prompt or format is required')\n\n    if not prompt and not raw:\n      raise ValueError('prompt or raw is required')\n\n    if not prompt and not stream:\n      raise ValueError('prompt or stream is required')\n\n    if not prompt and not context:\n      raise ValueError('prompt or context is required')\n\n    if not prompt and not options:\n      raise ValueError('prompt or options is required')\n\n    if not prompt and not keep_alive:\n      raise ValueError('prompt or keep_alive is required')\n\n    if not prompt and not format:\n      raise ValueError('prompt or format is required')\n\n    if not prompt and not raw:\n      raise ValueError('prompt or raw is required')\n\n    if not"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise Exception('model is required')\n\n    if not self.api_key:\n      raise Exception('api_key is required')\n\n    if not self.api_base:\n      raise Exception('api_base is required')\n\n    if not self.api_type:\n      raise Exception('api_type is required')\n\n    if not self.api_version:\n      raise Exception('api_version is required')\n\n    if not self.organization:\n      raise Exception('organization is required')\n\n    if not self.request_timeout:\n      raise Exception('request_timeout is required')\n\n    if not self.connect_timeout:\n      raise Exception('connect_timeout is required')\n\n    if not self.read_timeout:\n      raise Exception('read_timeout is required')\n\n    if not self.retries:\n      raise Exception('retries is required')\n\n    if not self.user_agent:\n      raise Exception('user_agent is required')\n\n    if not self.verbose:\n      raise Exception('verbose is required')\n\n    if not self.verify:\n      raise Exception('verify is required')\n\n    if not self.proxy:\n      raise Exception('proxy is required')\n\n    if not self.max_tokens:\n      raise Exception('max_tokens is required')\n\n    if not self.temperature:\n      raise Exception('temperature is required')\n\n    if not self.top_p:\n      raise Exception('top_p is required')\n\n    if not self.presence_penalty:\n      raise Exception('presence_penalty is required')\n\n    if not self.frequency_penalty:\n      raise Exception('frequency_penalty is required')\n\n    if not self.best_of:\n      raise Exception('best_of is required')\n\n    if not self.n:\n      raise Exception('n is required')\n\n    if not self.logit_bias:\n      raise Exception('logit_bias is required')\n\n    if not self.logprobs:\n      raise Exception('logprobs is required')\n\n    if"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    params = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    if stream:\n      return self.post_stream(\n        '/generate',\n        params=params,\n        headers=self.headers,\n      )\n\n    return self.post(\n      '/generate',\n      params=params,\n      headers=self.headers,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not prompt and not images:\n      raise ValueError('prompt or images is required')\n\n    if not stream:\n      return self._request(\n        model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        raw=raw,\n        format=format,\n        images=images,\n        options=options,\n        keep_alive=keep_alive,\n      )\n\n    return self._stream_request(\n      model,\n      prompt=prompt,\n      system=system,\n      template=template,\n      context=context,\n      raw=raw,\n      format=format,\n      images=images,\n      options=options,\n      keep_alive=keep_alive,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise Exception('model is required')\n\n    if not stream:\n      return self._request(\n        model,\n        prompt,\n        system,\n        template,\n        context,\n        stream,\n        raw,\n        format,\n        images,\n        options,\n        keep_alive,\n      )\n\n    return self._stream(\n      model,\n      prompt,\n      system,\n      template,\n      context,\n      stream,\n      raw,\n      format,\n      images,\n      options,\n      keep_alive,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if stream:\n      return self.stream_generate(\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        format=format,\n        images=images,\n        options=options,\n        keep_alive=keep_alive,\n      )\n\n    if format == 'json':\n      return self.post(\n        '/generate',\n        {\n          'model': model,\n          'prompt': prompt,\n          'system': system,\n          'template': template,\n          'context': context,\n          'format': format,\n          'images': images,\n          'options': options,\n          'keep_alive': keep_alive,\n        },\n        raw=raw,\n      )\n\n    return self.post(\n      '/generate',\n      {\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context,\n        'images': images,\n        'options': options,\n        'keep_alive': keep_alive,\n      },\n      raw=raw,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if model is None:\n      raise ValueError('model is required')\n\n    if stream:\n      return self._stream(\n        model,\n        prompt,\n        system,\n        template,\n        context,\n        format,\n        images,\n        options,\n        keep_alive,\n      )\n\n    response = self._request(\n      model,\n      prompt,\n      system,\n      template,\n      context,\n      format,\n      images,\n      options,\n      keep_alive,\n    )\n\n    if raw:\n      return response\n\n    return self._parse_response(response)\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not prompt and not system:\n      raise ValueError('prompt or system is required')\n\n    if not stream:\n      return self._request(\n        model,\n        prompt,\n        system,\n        template,\n        context,\n        stream,\n        raw,\n        format,\n        images,\n        options,\n        keep_alive,\n      )\n\n    return self._request_stream(\n      model,\n      prompt,\n      system,\n      template,\n      context,\n      stream,\n      raw,\n      format,\n      images,\n      options,\n      keep_alive,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise Exception('model required')\n\n    if not isinstance(prompt, str):\n      raise Exception('prompt must be a string')\n\n    if not isinstance(system, str):\n      raise Exception('system must be a string')\n\n    if not isinstance(template, str):\n      raise Exception('template must be a string')\n\n    if not isinstance(context, list):\n      raise Exception('context must be a list')\n\n    if not isinstance(stream, bool):\n      raise Exception('stream must be a boolean')\n\n    if not isinstance(raw, bool):\n      raise Exception('raw must be a boolean')\n\n    if not isinstance(format, str):\n      raise Exception('format must be a string')\n\n    if not isinstance(images, list):\n      raise Exception('images must be a list')\n\n    if not isinstance(options, Options):\n      raise Exception('options must be a Options instance')\n\n    if not isinstance(keep_alive, (float, str, type(None))):\n      raise Exception('keep_alive must be a float, string, or None')\n\n    if not isinstance(images, list):\n      raise Exception('images must be a list')\n\n    if not isinstance(options, Options):\n      raise Exception('options must be a Options instance')\n\n    if not isinstance(keep_alive, (float, str, type(None))):\n      raise Exception('keep_alive must be a float, string, or None')\n\n    if not isinstance(images, list):\n      raise Exception('images must be a list')\n\n    if not isinstance(options, Options):\n      raise Exception('options must be a Options instance')\n\n    if not isinstance(keep_alive, (float, str, type(None))):\n      raise Exception('keep_alive must be a float, string, or None')\n\n    if not isinstance(images, list):\n      raise Exception('images must be a list')\n\n    if not isinstance(options, Options):\n      raise Exception('options must be a Options instance')\n\n    if not isinstance("}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not isinstance(model, str):\n      raise TypeError(f'model must be a string, not {type(model)}')\n\n    if context is None:\n      context = []\n\n    if not isinstance(context, (list, tuple)):\n      raise TypeError(f'context must be a list or tuple, not {type(context)}')\n\n    if not isinstance(stream, bool):\n      raise TypeError(f'stream must be a bool, not {type(stream)}')\n\n    if not isinstance(raw, bool):\n      raise TypeError(f'raw must be a bool, not {type(raw)}')\n\n    if format not in ('', 'json'):\n      raise ValueError(f'format must be either \"\" or \"json\", not {format}')\n\n    if images is not None and not isinstance(images, (list, tuple)):\n      raise TypeError(f'images must be a list or tuple, not {type(images)}')\n\n    if options is not None and not isinstance(options, Options):\n      raise TypeError(f'options must be an instance of Options, not {type(options)}')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError(f'keep_alive must be a float or a string, not {type(keep_alive)}')\n\n    if not isinstance(prompt, str):\n      raise TypeError(f'prompt must be a string, not {type(prompt)}')\n\n    if not isinstance(system, str):\n      raise TypeError(f'system must be a string, not {type(system)}')\n\n    if not isinstance(template, str):\n      raise TypeError(f'template must be a string, not {type(template)}')\n\n    if stream:\n      return self._stream(\n        model,\n        prompt,\n        system,\n        template,\n        context,\n        raw,\n        format,\n        images,\n        options,\n        keep_al"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not isinstance(prompt, str):\n      raise TypeError(f'prompt must be a string, not {type(prompt)}')\n\n    if not isinstance(system, str):\n      raise TypeError(f'system must be a string, not {type(system)}')\n\n    if not isinstance(template, str):\n      raise TypeError(f'template must be a string, not {type(template)}')\n\n    if context is None:\n      context = []\n\n    if not isinstance(context, (list, tuple)):\n      raise TypeError(f'context must be a list or tuple, not {type(context)}')\n\n    if not isinstance(stream, bool):\n      raise TypeError(f'stream must be a bool, not {type(stream)}')\n\n    if not isinstance(raw, bool):\n      raise TypeError(f'raw must be a bool, not {type(raw)}')\n\n    if not isinstance(format, str):\n      raise TypeError(f'format must be a string, not {type(format)}')\n\n    if images is not None and not isinstance(images, (list, tuple)):\n      raise TypeError(f'images must be a list or tuple, not {type(images)}')\n\n    if options is not None and not isinstance(options, Options):\n      raise TypeError(f'options must be an instance of Options, not {type(options)}')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError(f'keep_alive must be a float or a string, not {type(keep_alive)}')\n\n    if stream:\n      return self._generate_stream(\n        model,\n        prompt,\n        system,\n        template,\n        context,\n        raw,\n        format,\n        images,\n        options,\n        keep_alive,\n      )\n    else:\n      return self._generate_single(\n        model,\n        prompt,\n        system,\n        template"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not isinstance(context, (list, tuple)):\n      context = []\n\n    if images is not None:\n      images = [image.decode('utf-8') if isinstance(image, bytes) else image for image in images]\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, str):\n        keep_alive = float(keep_alive)\n      options.keep_alive = keep_alive\n\n    if stream:\n      return self._stream(\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        format=format,\n        images=images,\n        options=options,\n      )\n\n    response = self._request(\n      model=model,\n      prompt=prompt,\n      system=system,\n      template=template,\n      context=context,\n      format=format,\n      images=images,\n      options=options,\n    )\n\n    if raw:\n      return response\n\n    return self._process_response(response)"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not context:\n      context = []\n\n    if not images:\n      images = []\n\n    if not options:\n      options = Options()\n\n    if not keep_alive:\n      keep_alive = '0'\n\n    if isinstance(keep_alive, float):\n      keep_alive = str(keep_alive)\n\n    if keep_alive == '0':\n      keep_alive = None\n\n    if not format:\n      format = ''\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be either an empty string or \"json\"')\n\n    if stream:\n      return self._generate_stream(\n        model,\n        prompt,\n        system,\n        template,\n        context,\n        raw,\n        format,\n        images,\n        options,\n        keep_alive,\n      )\n\n    return self._generate(\n      model,\n      prompt,\n      system,\n      template,\n      context,\n      raw,\n      format,\n      images,\n      options,\n      keep_alive,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not isinstance(context, (list, tuple)):\n      context = []\n\n    if not isinstance(images, (list, tuple)):\n      images = []\n\n    if not isinstance(keep_alive, (int, float)):\n      keep_alive = None\n\n    if not isinstance(options, Options):\n      options = Options()\n\n    if format and format not in ('', 'json'):\n      raise ValueError('format must be either an empty string or \"json\"')\n\n    if not isinstance(prompt, str):\n      raise TypeError('prompt must be a string')\n\n    if not isinstance(system, str):\n      raise TypeError('system must be a string')\n\n    if not isinstance(template, str):\n      raise TypeError('template must be a string')\n\n    if not isinstance(stream, bool):\n      raise TypeError('stream must be a boolean')\n\n    if not isinstance(raw, bool):\n      raise TypeError('raw must be a boolean')\n\n    if not isinstance(format, str):\n      raise TypeError('format must be a string')\n\n    if not isinstance(images, list):\n      raise TypeError('images must be a list')\n\n    if not isinstance(options, Options):\n      raise TypeError('options must be an instance of Options')\n\n    if not isinstance(keep_alive, (float, str, None)):\n      raise TypeError('keep_alive must be a float, a string, or None')\n\n    if not isinstance(context, list):\n      raise TypeError('context must be a list')\n\n    if not isinstance(stream, bool):\n      raise TypeError('stream must be a boolean')\n\n    if not isinstance(raw, bool):\n      raise TypeError('raw must be a boolean')\n\n    if not isinstance(format, str):\n      raise TypeError('format must be a string')\n\n    if not isinstance(images, list):\n      raise TypeError('images must be a list')\n\n    if not"}
{"namespace": "ollama._client.Client.generate", "completion": "    if model is None:\n      raise ValueError('model is required')\n\n    if model not in self.models:\n      raise ValueError(f'model {model} not found')\n\n    if context is None:\n      context = []\n\n    if images is None:\n      images = []\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is None:\n      keep_alive = 'timeout=3600'\n    elif isinstance(keep_alive, float):\n      keep_alive = f'timeout={keep_alive:.0f}'\n    elif not isinstance(keep_alive, str):\n      raise TypeError('keep_alive must be a float or a string')\n\n    if stream:\n      return self._stream_generate(\n        model,\n        prompt,\n        system,\n        template,\n        context,\n        format,\n        images,\n        options,\n        keep_alive,\n      )\n\n    if raw:\n      return self._raw_generate(\n        model,\n        prompt,\n        system,\n        template,\n        context,\n        format,\n        images,\n        options,\n        keep_alive,\n      )\n\n    return self._generate(\n      model,\n      prompt,\n      system,\n      template,\n      context,\n      format,\n      images,\n      options,\n      keep_alive,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if model is None:\n      raise Exception('model is required')\n\n    if prompt is None:\n      prompt = ''\n\n    if system is None:\n      system = ''\n\n    if template is None:\n      template = ''\n\n    if context is None:\n      context = []\n\n    if images is None:\n      images = []\n\n    if options is None:\n      options = {}\n\n    if keep_alive is None:\n      keep_alive = None\n\n    if stream is True:\n      return self._stream_response(\n        model,\n        prompt,\n        system,\n        template,\n        context,\n        format,\n        images,\n        options,\n        keep_alive,\n      )\n\n    if raw is True:\n      return self._raw_response(\n        model,\n        prompt,\n        system,\n        template,\n        context,\n        format,\n        images,\n        options,\n        keep_alive,\n      )\n\n    return self._response(\n      model,\n      prompt,\n      system,\n      template,\n      context,\n      format,\n      images,\n      options,\n      keep_alive,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if model is None:\n      raise ValueError('model is required')\n\n    if format not in ('', 'json'):\n      raise ValueError('format must be one of (\"\", \"json\")')\n\n    if context is None:\n      context = []\n\n    if images is None:\n      images = []\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is None:\n      keep_alive = self.keep_alive\n\n    if isinstance(keep_alive, str):\n      keep_alive = float(keep_alive)\n\n    if keep_alive is not None:\n      keep_alive = max(0.0, keep_alive)\n\n    if not isinstance(context, list):\n      context = list(context)\n\n    if not isinstance(images, list):\n      images = list(images)\n\n    if not isinstance(options, Options):\n      options = Options(**options)\n\n    if not isinstance(keep_alive, float):\n      keep_alive = None\n\n    if not isinstance(context, list):\n      context = list(context)\n\n    if not isinstance(images, list):\n      images = list(images)\n\n    if not isinstance(options, Options):\n      options = Options(**options)\n\n    if not isinstance(keep_alive, float):\n      keep_alive = None\n\n    if not isinstance(context, list):\n      context = list(context)\n\n    if not isinstance(images, list):\n      images = list(images)\n\n    if not isinstance(options, Options):\n      options = Options(**options)\n\n    if not isinstance(keep_alive, float):\n      keep_alive = None\n\n    if not isinstance(context, list):\n      context = list(context)\n\n    if not isinstance(images, list):\n      images = list(images)\n\n    if not isinstance(options, Options):\n      options = Options(**options)\n\n    if not isinstance(keep_alive, float):\n     "}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise Exception('model is required')\n    if not isinstance(context, list):\n      context = []\n    if not isinstance(images, list):\n      images = []\n    if not isinstance(options, dict):\n      options = {}\n\n    if keep_alive is not None:\n      options['keep_alive'] = keep_alive\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n    }\n\n    if stream:\n      return self._stream(data)\n    else:\n      return self._request(data)\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise Exception('model is required')\n\n    if not isinstance(context, list):\n      context = list(context or [])\n\n    if not isinstance(images, list):\n      images = list(images or [])\n\n    if not isinstance(options, dict):\n      options = dict(options or {})\n\n    if keep_alive is None:\n      keep_alive = '0'\n    elif isinstance(keep_alive, float):\n      keep_alive = str(keep_alive)\n\n    params = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    if stream:\n      return self._stream(params)\n    else:\n      return self._request(params)\n"}
{"namespace": "ollama._client.Client.push", "completion": "    params = {\n      'model': model,\n      'insecure': insecure,\n    }\n\n    if stream:\n      return self._stream_request(\n        method='POST',\n        path='/api/push',\n        params=params,\n      )\n\n    return self._request(\n      method='POST',\n      path='/api/push',\n      params=params,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self._stream_request(\n        method='POST',\n        url='/api/push',\n        params={\n          'model': model,\n          'insecure': insecure,\n        },\n      )\n    else:\n      return self._request(\n        method='POST',\n        url='/api/push',\n        params={\n          'model': model,\n          'insecure': insecure,\n        },\n      )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self._stream(\n        self.post,\n        '/api/push',\n        model=model,\n        insecure=insecure,\n      )\n    return self._get_response(\n      self.post,\n      '/api/push',\n      model=model,\n      insecure=insecure,\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request(\n      method='POST',\n      endpoint='/api/push',\n      params={\n        'model': model,\n        'insecure': insecure,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    # Send the request\n    response = self._send_request(\n      method=\"POST\",\n      path=\"/api/push\",\n      params={\n        \"model\": model,\n        \"insecure\": insecure,\n      },\n      stream=stream,\n    )\n\n    # Handle the response\n    if stream:\n      return response\n    else:\n      return next(response)\n"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self._stream_request(\n        endpoint='/api/push',\n        method='POST',\n        data={\n          'model': model,\n          'insecure': insecure,\n        },\n      )\n\n    return self._request(\n      endpoint='/api/push',\n      method='POST',\n      data={\n        'model': model,\n        'insecure': insecure,\n      },\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self.request_generator(\n        method=\"POST\",\n        path=\"/api/push\",\n        json={\n          \"model\": model,\n          \"insecure\": insecure,\n        },\n        stream=True,\n      )\n    else:\n      return self.request(\n        method=\"POST\",\n        path=\"/api/push\",\n        json={\n          \"model\": model,\n          \"insecure\": insecure,\n        },\n      )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self.request_generator(\n        method=\"POST\",\n        endpoint=\"/api/push\",\n        params={\n          \"model\": model,\n          \"insecure\": insecure,\n        },\n        expected_status=[200],\n        stream=True,\n      )\n    else:\n      return self.request(\n        method=\"POST\",\n        endpoint=\"/api/push\",\n        params={\n          \"model\": model,\n          \"insecure\": insecure,\n        },\n        expected_status=[200],\n      )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self._stream(\n        self._post,\n        '/api/push',\n        model=model,\n        insecure=insecure,\n      )\n    return self._get_response(\n      self._post,\n      '/api/push',\n      model=model,\n      insecure=insecure,\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self._stream(\n        self._post(\n          \"/api/push\",\n          json={\n            \"model\": model,\n            \"insecure\": insecure,\n          },\n        ),\n        expected=ProgressResponse,\n        path=[\"progress\"],\n      )\n    else:\n      return self._execute(\n        self._post(\n          \"/api/push\",\n          json={\n            \"model\": model,\n            \"insecure\": insecure,\n          },\n        ),\n        expected=ProgressResponse,\n        path=[\"progress\"],\n      )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self._stream(\n        self._post(\n          f\"{self.url}/api/push\",\n          json={\n            \"model\": model,\n            \"insecure\": insecure,\n          },\n        ),\n        expected=(ProgressResponse,),\n        path=\"push\",\n      )\n    else:\n      return self._execute(\n        self._post(\n          f\"{self.url}/api/push\",\n          json={\n            \"model\": model,\n            \"insecure\": insecure,\n          },\n        ),\n        expected=ProgressResponse,\n        path=\"push\",\n      )"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self._api_request(\n        \"POST\",\n        \"/api/push\",\n        json={\n          \"model\": model,\n          \"insecure\": insecure,\n        },\n        stream=True,\n      )\n\n    return self._api_request(\n      \"POST\",\n      \"/api/push\",\n      json={\n        \"model\": model,\n        \"insecure\": insecure,\n      },\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self._streamed_request(\n        self._post,\n        '/api/push',\n        model=model,\n        insecure=insecure,\n      )\n    else:\n      return self._request(\n        self._post,\n        '/api/push',\n        model=model,\n        insecure=insecure,\n      )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    url = self.base_url + '/api/push'\n    params = {\n        'model': model,\n        'insecure': insecure,\n    }\n    if stream:\n      return self.request_generator(\n        method='POST',\n        url=url,\n        params=params,\n        expected_status=200,\n        response_type=ProgressResponse,\n      )\n    else:\n      return self.request(\n        method='POST',\n        url=url,\n        params=params,\n        expected_status=200,\n        response_type=ProgressResponse,\n      )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    data = {\n      'model': model,\n      'insecure': insecure,\n    }\n    if stream:\n      return self._stream_request(\n        method='POST',\n        path='/api/push',\n        data=data,\n        deserialize=lambda x: ProgressResponse.from_json(x),\n      )\n    else:\n      return self._request(\n        method='POST',\n        path='/api/push',\n        data=data,\n        deserialize=lambda x: ProgressResponse.from_json(x),\n      )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    # Send the request\n    response = self._request(\n      method='POST',\n      path='/api/push',\n      params={\n        'model': model,\n        'insecure': insecure,\n      },\n      stream=stream,\n    )\n\n    # Handle the response\n    if stream:\n      return self._handle_response_stream(response)\n    else:\n      return self._handle_response(response)\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self._stream_request(\n        url=f\"{self.base_url}/api/push\",\n        json={\n          \"model\": model,\n          \"insecure\": insecure,\n        },\n        cls=ProgressResponse,\n      )\n    else:\n      return self._request(\n        url=f\"{self.base_url}/api/push\",\n        json={\n          \"model\": model,\n          \"insecure\": insecure,\n        },\n        cls=ProgressResponse,\n      )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self._request_stream(\n        method='POST',\n        url=self.base_url + '/api/push',\n        params={\n          'model': model,\n          'insecure': insecure,\n        },\n      )\n\n    return self._request(\n      method='POST',\n      url=self.base_url + '/api/push',\n      params={\n        'model': model,\n        'insecure': insecure,\n      },\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    # Create a dictionary with the parameters to be sent in the request\n    params = {\n      'model': model,\n      'insecure': insecure,\n    }\n\n    # Send a POST request to the '/api/push' endpoint with the specified parameters\n    response = self.post('/api/push', params=params, stream=stream)\n\n    # If the request was successful, return the response\n    if response.ok:\n      if stream:\n        return self._stream_response(response)\n      else:\n        return response.json()\n    # If the request was not successful, raise a 'ResponseError' with the response status code and content\n    else:\n      raise ResponseError(response.status_code, response.content)\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    # Check if the model name is valid\n    if not model:\n      raise ValueError('Model name is required')\n\n    # Prepare the request body\n    data = {\n      'model': model,\n      'insecure': insecure,\n    }\n\n    # Send the request and get the response\n    response = self._post('/api/push', data=data, stream=stream)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n      # If 'stream' is False, return a single 'ProgressResponse'\n      if not stream:\n        return response.json()\n      # If 'stream' is True, return a generator yielding 'ProgressResponse' objects\n      else:\n        return (line.decode('utf-8') for line in response.iter_lines())\n    else:\n      # If the request was not successful, raise a 'ResponseError'\n      raise ResponseError(response.status_code, response.content)\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\n        \"Either `path` or `modelfile` is required for the operation.\"\n      )\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    data = {\n      \"model\": model,\n      \"modelfile\": modelfile,\n    }\n\n    if stream:\n      return self._stream_request(\n        path=\"/create\",\n        data=data,\n      )\n    else:\n      return self._request(\n        path=\"/create\",\n        data=data,\n      )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\n        'Either `path` or `modelfile` is required for this operation.'\n      )\n    if path is not None:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n    request = CreateRequest(model=model, modelfile=modelfile)\n    if stream:\n      return self._stream_create(request)\n    return self._create(request)"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError('Either path or modelfile is required')\n\n    if path is not None:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request(\n      'POST',\n      '/v1/models',\n      json={\n        'model': model,\n        'modelfile': modelfile,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile is required\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      return self._stream(\n        \"POST\",\n        f\"/models/{model}\",\n        data=modelfile,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n      )\n    else:\n      return self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=modelfile,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n      )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\n        \"You must provide either a path or a modelfile to create a model\"\n      )\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      return self._request_stream(\n        \"POST\",\n        f\"/models/{model}\",\n        data=modelfile,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n      )\n    else:\n      return self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=modelfile,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n      )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError(\n        \"Either path or modelfile is required to create a model\"\n      )\n\n    if path and modelfile:\n      raise RequestError(\n        \"Either path or modelfile can be provided, but not both\"\n      )\n\n    if path:\n      with open(path, \"r\") as f:\n        modelfile = f.read()\n\n    request = CreateRequest(model=model, modelfile=modelfile)\n    if stream:\n      return self._stream_request(request)\n    else:\n      return self._request(request)\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError('Either path or modelfile is required')\n\n    if path is not None:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    if stream:\n      return self._stream(\n        self.post(\n          '/create',\n          data={\n            'model': model,\n            'modelfile': modelfile,\n          },\n        ),\n      )\n    else:\n      return self._single(\n        self.post(\n          '/create',\n          data={\n            'model': model,\n            'modelfile': modelfile,\n          },\n        ),\n      )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if modelfile is None:\n      if path is None:\n        raise RequestError(\"either path or modelfile must be provided\")\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n    if stream:\n      return self._stream_request(\n        \"create\",\n        model=model,\n        modelfile=modelfile,\n      )\n    return self._request(\n      \"create\",\n      model=model,\n      modelfile=modelfile,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\n        \"either path or modelfile must be provided\"\n      )\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n    req = CreateRequest(\n      model=model,\n      modelfile=modelfile,\n    )\n    return self._request(req, stream=stream)\n\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n    if modelfile is None:\n      raise RequestError(\n        \"Either `path` or `modelfile` is required for the operation.\"\n      )\n    return self._request(\n      \"POST\",\n      \"/models/{}\".format(model),\n      json={\n        \"model\": modelfile,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n    elif modelfile is None:\n      raise RequestError(\"path or modelfile is required\")\n\n    return self._request(\n      \"POST\",\n      f\"{self.url}/create/{model}\",\n      data=modelfile,\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is not None:\n      with open(path, \"r\") as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError(\"path or modelfile is required\")\n\n    return self._request(\n      method=\"POST\",\n      path=f\"/models/{model}\",\n      json={\"modelfile\": modelfile},\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if modelfile is None:\n      if path is None:\n        raise RequestError(\"either path or modelfile is required\")\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n    response = self._request(\n      method=\"POST\",\n      path=f\"/{model}\",\n      body=modelfile,\n      headers={\"Content-Type\": \"application/octet-stream\"},\n      stream=stream,\n    )\n    if stream:\n      return response\n    return next(response)\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"either path or modelfile is required\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      return self._request_stream(\n        \"POST\",\n        \"/models\",\n        data={\n          \"model\": model,\n          \"modelfile\": modelfile,\n        },\n      )\n    else:\n      return self._request(\n        \"POST\",\n        \"/models\",\n        data={\n          \"model\": model,\n          \"modelfile\": modelfile,\n        },\n      )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\n        \"Either 'path' or 'modelfile' is required\",\n        status_code=400,\n        method=\"POST\",\n        endpoint=f\"/{self.api_version}/{model}/create\",\n      )\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    response = self.request(\n      method=\"POST\",\n      endpoint=f\"/{self.api_version}/{model}/create\",\n      data=modelfile,\n      stream=stream,\n    )\n\n    if stream:\n      return response\n\n    return next(response)"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\n        'Either `path` or `modelfile` is required to create a model.',\n        status_code=400,\n      )\n\n    if modelfile is None:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    if stream:\n      return self._post_stream(\n        f'{self.url}/models/{model}',\n        data=modelfile,\n        headers={'Content-Type': 'application/octet-stream'},\n      )\n    else:\n      return self._post(\n        f'{self.url}/models/{model}',\n        data=modelfile,\n        headers={'Content-Type': 'application/octet-stream'},\n      )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\n        'Either `path` or `modelfile` is required to create a model.'\n      )\n\n    if path is not None:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    if stream:\n      return self._stream_create(model, modelfile)\n\n    return self._single_create(model, modelfile)\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"One of path or modelfile is required\")\n\n    if path is not None:\n      with open(path, \"r\") as f:\n        modelfile = f.read()\n\n    req = CreateRequest(model=model, modelfile=modelfile)\n\n    if stream:\n      return self._create_stream(req)\n    else:\n      return self._create(req)\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\n        \"Neither path nor modelfile is provided, one is required\"\n      )\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    # Create a new model\n    response = self._request(\n      \"POST\",\n      \"/models\",\n      json={\n        \"model\": model,\n        \"modelfile\": base64.b64encode(modelfile).decode(\"utf-8\"),\n      },\n    )\n\n    if not stream:\n      return response\n\n    return (\n      response\n      for response in self._gen_stream_request(\n        \"POST\",\n        \"/models\",\n        json={\n          \"model\": model,\n          \"modelfile\": base64.b64encode(modelfile).decode(\"utf-8\"),\n        },\n      )\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError('path or modelfile required')\n\n    if modelfile is None:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    req = Request(\n      method='POST',\n      path=f'/models/{model}',\n      headers={\n        'Content-Type': 'application/octet-stream',\n      },\n      body=modelfile,\n    )\n\n    if stream:\n      return self._request_stream(req, self._handle_progress)\n    else:\n      return self._request(req, self._handle_progress)\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      digest = hashlib.sha256(f.read()).hexdigest()\n    url = f'{self.base_url}/v2/{self.repository}/blobs/sha256:{digest}'\n    response = self.session.head(url)\n    if response.status_code == 404:\n      with open(path, 'rb') as f:\n        response = self.session.post(f'{self.base_url}/v2/{self.repository}/blobs/uploads/', headers={'Content-Type': 'application/octet-stream'}, data=f)\n      response.raise_for_status()\n    return f'sha256:{digest}'\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Calculate the SHA-256 checksum of the file\n    with open(path, \"rb\") as f:\n      file_hash = hashlib.sha256()\n      while True:\n        chunk = f.read(4096)\n        if not chunk:\n          break\n        file_hash.update(chunk)\n    file_hash = file_hash.hexdigest()\n\n    # Check if the blob already exists on the server\n    response = self.session.head(f\"{self.base_url}/v2/{self.repository}/blobs/{file_hash}\")\n    if response.status_code == 200:\n      # Blob already exists, return the digest\n      return f\"sha256:{file_hash}\"\n\n    # Upload the file as a new blob\n    with open(path, \"rb\") as f:\n      response = self.session.post(\n        f\"{self.base_url}/v2/{self.repository}/blobs/uploads/\",\n        headers={\"Content-Type\": \"application/octet-stream\"},\n        data=f,\n      )\n    response.raise_for_status()\n    location = response.headers[\"Location\"]\n    upload_uuid = location.split(\"/\")[-1]\n    response = self.session.put(\n      f\"{self.base_url}/v2/{self.repository}/blobs/uploads/{upload_uuid}?digest=sha256:{file_hash}\",\n      headers={\"Content-Type\": \"application/octet-stream\"},\n      data=f,\n    )\n    response.raise_for_status()\n\n    # Return the digest of the file\n    return f\"sha256:{file_hash}\"\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      data = f.read()\n    checksum = hashlib.sha256(data).hexdigest()\n    headers = {'Accept': 'application/vnd.docker.distribution.manifest.v2+json'}\n    url = f'{self.base_url}/v2/{self.repository}/blobs/{checksum}'\n    response = self.session.head(url, headers=headers)\n    if response.status_code == 404:\n      headers = {'Content-Type': 'application/octet-stream'}\n      response = self.session.post(url, headers=headers, data=data)\n      response.raise_for_status()\n    return f'sha256:{checksum}'\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Open the file and calculate its SHA-256 checksum\n    with open(path, \"rb\") as f:\n      file_bytes = f.read()\n      file_sha256 = hashlib.sha256(file_bytes).hexdigest()\n\n    # Check if the blob already exists on the server\n    response = self.session.head(f\"{self.base_url}/v2/{self.repository}/blobs/{file_sha256}\")\n    if response.status_code == 404:\n      # Upload the file as a new blob\n      response = self.session.post(\n        f\"{self.base_url}/v2/{self.repository}/blobs/uploads/\",\n        headers={\n          \"Content-Type\": \"application/octet-stream\"\n        },\n        data=file_bytes\n      )\n      if response.status_code != 201:\n        raise Exception(f\"Failed to upload blob: {response.text}\")\n\n    # Return the digest of the file\n    return f\"sha256:{file_sha256}\"\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Open the file in binary mode and calculate its SHA-256 checksum\n    with open(path, 'rb') as f:\n      content = f.read()\n      digest = 'sha256:' + hashlib.sha256(content).hexdigest()\n\n    # Check if the blob already exists on the server\n    url = self.url + '/v2/' + self.name + '/blobs/' + digest\n    response = requests.head(url, headers=self.headers)\n\n    # If the blob does not exist, upload it as a new blob\n    if response.status_code == 404:\n      url = self.url + '/v2/' + self.name + '/blobs/uploads/'\n      response = requests.post(url, headers=self.headers)\n      location = response.headers['Location']\n      response = requests.put(location, data=content, headers=self.headers)\n\n    # Return the digest of the file\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, \"rb\") as f:\n      hexdigest = hashlib.sha256(f.read()).hexdigest()\n\n    url = f\"{self.base_url}/v2/{self.repository}/blobs/sha256:{hexdigest}\"\n    response = self.session.head(url)\n    if response.status_code == 404:\n      with open(path, \"rb\") as f:\n        response = self.session.post(\n          f\"{self.base_url}/v2/{self.repository}/blobs/uploads/\",\n          headers={\"Content-Type\": \"application/octet-stream\"},\n          data=f,\n        )\n        response.raise_for_status()\n\n    return f\"sha256:{hexdigest}\"\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      file_bytes = f.read()\n    file_sha256 = hashlib.sha256(file_bytes).hexdigest()\n    file_digest = 'sha256:' + file_sha256\n\n    # check if blob already exists\n    r = self.session.head(self.base_url + '/v2/' + self.name + '/blobs/' + file_sha256)\n    if r.status_code == 404:\n      # upload blob\n      headers = {\n        'Content-Type': 'application/octet-stream'\n      }\n      r = self.session.post(self.base_url + '/v2/' + self.name + '/blobs/uploads/', headers=headers)\n      upload_location = r.headers['Location']\n      r = self.session.put(upload_location, data=file_bytes)\n      if r.status_code != 201:\n        raise Exception('Error uploading blob: ' + r.text)\n\n    return file_digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      file_hash = hashlib.sha256()\n      while True:\n        data = f.read(8192)\n        if not data:\n          break\n        file_hash.update(data)\n\n    digest = 'sha256:{}'.format(file_hash.hexdigest())\n    url = self._url('/v2/{}/blobs/{}'.format(self._repository, digest))\n    try:\n      self._session.head(url)\n    except requests.exceptions.HTTPError as e:\n      if e.response.status_code == 404:\n        with open(path, 'rb') as f:\n          self._session.post(url, data=f, headers={\n            'Content-Type': 'application/octet-stream'\n          })\n      else:\n        raise\n\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      data = f.read()\n    sha256 = hashlib.sha256()\n    sha256.update(data)\n    digest = 'sha256:%s' % sha256.hexdigest()\n    url = self._url('/v2/%s/blobs/%s' % (self._repository, digest))\n    response = self._head(url)\n    if response.status_code == 404:\n      self._post(url, data=data, headers={\n        'Content-Type': 'application/octet-stream'\n      })\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, \"rb\") as f:\n      data = f.read()\n    checksum = hashlib.sha256(data).hexdigest()\n    response = self.session.head(\n      self.base_url + f\"/v2/{self.repository}/blobs/{checksum}\"\n    )\n    if response.status_code == 404:\n      response = self.session.post(\n        self.base_url + f\"/v2/{self.repository}/blobs/uploads/\",\n        headers={\"Content-Type\": \"application/octet-stream\"},\n        data=data,\n      )\n      response.raise_for_status()\n    return f\"sha256:{checksum}\"\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, \"rb\") as f:\n      file_hash = hashlib.sha256()\n      while chunk := f.read(8192):\n        file_hash.update(chunk)\n\n    digest = \"sha256:\" + file_hash.hexdigest()\n\n    url = self._get_url(f\"{self.base_url}/v2/{self.repository}/blobs/{digest}\")\n    response = self.session.head(url)\n    if response.status_code == 404:\n      with open(path, \"rb\") as f:\n        response = self.session.post(\n          url,\n          data=f,\n          headers={\n            \"Content-Type\": \"application/octet-stream\"\n          }\n        )\n      response.raise_for_status()\n\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      file_hash = hashlib.sha256()\n      while True:\n        data = f.read(4096)\n        if not data:\n          break\n        file_hash.update(data)\n      digest = 'sha256:{}'.format(file_hash.hexdigest())\n      url = '{}/v2/{}/blobs/{}'.format(self.url, self.name, digest)\n      r = requests.head(url)\n      if r.status_code == 404:\n        with open(path, 'rb') as f:\n          data = f.read()\n        r = requests.post(url, data=data, headers=self.headers)\n        r.raise_for_status()\n      return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      file_bytes = f.read()\n      file_sha256 = hashlib.sha256(file_bytes).hexdigest()\n\n    response = self.session.head(f'{self.base_url}/v2/{self.repository}/blobs/{file_sha256}')\n    if response.status_code == 404:\n      response = self.session.post(f'{self.base_url}/v2/{self.repository}/blobs/uploads/', headers={'Content-Type': 'application/octet-stream'})\n      upload_url = response.headers['Location']\n      response = self.session.put(upload_url, data=file_bytes, headers={'Content-Type': 'application/octet-stream'})\n      response.raise_for_status()\n\n    return f'sha256:{file_sha256}'\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      data = f.read()\n      sha256 = hashlib.sha256(data).hexdigest()\n      digest = f'sha256:{sha256}'\n      url = f'{self.base_url}/v2/{self.repository}/blobs/{digest}'\n      response = requests.head(url, headers=self.headers)\n      if response.status_code != 200:\n        response = requests.post(url, data=data, headers=self.headers)\n        response.raise_for_status()\n      return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      sha256 = hashlib.sha256()\n      while True:\n        chunk = f.read(4096)\n        if not chunk:\n          break\n        sha256.update(chunk)\n      digest = f'sha256:{sha256.hexdigest()}'\n\n    # Check if the blob already exists on the server\n    response = self._session.head(f'{self._registry_url}/v2/{self._repository}/blobs/{digest}')\n    if response.status_code == 404:\n      # If the blob does not exist, upload it as a new blob\n      with open(path, 'rb') as f:\n        response = self._session.post(\n          f'{self._registry_url}/v2/{self._repository}/blobs/uploads/',\n          data=f,\n          headers={'Content-Type': 'application/octet-stream'}\n        )\n        response.raise_for_status()\n        location = response.headers['Location']\n\n      # Update the upload status\n      response = self._session.post(\n        f'{self._registry_url}{location}',\n        json={'digest': digest}\n      )\n      response.raise_for_status()\n\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Open the file in binary mode and calculate its SHA-256 checksum\n    with open(path, \"rb\") as f:\n      # Calculate the SHA-256 checksum of the file\n      file_hash = hashlib.sha256()\n      while True:\n        # Read the file in chunks of 4096 bytes\n        chunk = f.read(4096)\n        if not chunk:\n          # End of file\n          break\n        file_hash.update(chunk)\n\n    # Get the hexadecimal representation of the checksum\n    hexdigest = file_hash.hexdigest()\n\n    # Check if a blob with the same checksum already exists on the server\n    response = self.session.head(f\"{self.base_url}/v2/{self.name}/blobs/{hexdigest}\")\n\n    if response.status_code == 404:\n      # Blob does not exist, upload it\n      with open(path, \"rb\") as f:\n        # Upload the file as a new blob\n        response = self.session.post(\n          f\"{self.base_url}/v2/{self.name}/blobs/uploads/\",\n          headers={\n            \"Content-Type\": \"application/octet-stream\"\n          },\n          data=f\n        )\n\n      # Check the response status code\n      if response.status_code != 201:\n        raise Exception(\"Failed to upload blob\")\n\n    # Return the digest of the file\n    return f\"sha256:{hexdigest}\"\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, \"rb\") as f:\n      content = f.read()\n\n    digest = f\"sha256:{hashlib.sha256(content).hexdigest()}\"\n\n    try:\n      self._request(f\"HEAD\", f\"{self.base_url}/v2/{self.repository}/blobs/{digest}\")\n    except requests.exceptions.HTTPError as e:\n      if e.response.status_code == 404:\n        self._request(\n          f\"POST\",\n          f\"{self.base_url}/v2/{self.repository}/blobs/uploads/\",\n          headers={\"Content-Type\": \"application/octet-stream\"},\n          data=content,\n        )\n      else:\n        raise e\n\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      hasher = hashlib.sha256()\n      while True:\n        data = f.read(8192)\n        if not data:\n          break\n        hasher.update(data)\n      digest = 'sha256:' + hasher.hexdigest()\n      response = self._session.head(f'{self._base_url}/v2/{self._name}/blobs/{digest}')\n      if response.status_code == 404:\n        with open(path, 'rb') as f:\n          response = self._session.post(f'{self._base_url}/v2/{self._name}/blobs/uploads/', headers={'Content-Type': 'application/octet-stream'}, data=f)\n          if response.status_code != 201:\n            raise ValueError(f'Error uploading blob: {response.status_code}')\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      data = f.read()\n    digest = 'sha256:' + hashlib.sha256(data).hexdigest()\n    url = self.url + '/v2/' + self.repository + '/blobs/' + digest\n    r = self.session.head(url)\n    if r.status_code == 404:\n      r = self.session.post(url, data=data)\n      r.raise_for_status()\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      file_hash = hashlib.sha256()\n      while True:\n        data = f.read(8192)\n        if not data:\n          break\n        file_hash.update(data)\n      digest = 'sha256:' + file_hash.hexdigest()\n      url = f'{self.url}/v2/{self.repository}/blobs/{digest}'\n      try:\n        r = self.session.head(url)\n        r.raise_for_status()\n      except requests.HTTPError as e:\n        if e.response.status_code == 404:\n          with open(path, 'rb') as f:\n            data = f.read()\n            r = self.session.post(f'{self.url}/v2/{self.repository}/blobs/uploads/',\n                                  headers={'Content-Type': 'application/octet-stream'},\n                                  data=data)\n            r.raise_for_status()\n            location = r.headers['Location']\n            r = self.session.put(location,\n                                 headers={'Content-Type': 'application/octet-stream',\n                                          'Content-Length': str(len(data))},\n                                 data=data)\n            r.raise_for_status()\n            return digest\n        else:\n          raise\n      return digest\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    params = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    async for response in self._request('generate', params):\n      yield response\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not stream:\n      return await self._request(\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        raw=raw,\n        format=format,\n        images=images,\n        options=options,\n        keep_alive=keep_alive,\n      )\n\n    async def stream_response():\n      async for chunk in self._request_stream(\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        raw=raw,\n        format=format,\n        images=images,\n        options=options,\n        keep_alive=keep_alive,\n      ):\n        yield chunk\n\n    return stream_response()"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not stream:\n      return await self.request(\n        'generate',\n        model,\n        prompt,\n        system,\n        template,\n        context,\n        raw,\n        format,\n        images,\n        options,\n        keep_alive,\n      )\n\n    async def _generator():\n      async for chunk in self.request_stream(\n        'generate',\n        model,\n        prompt,\n        system,\n        template,\n        context,\n        raw,\n        format,\n        images,\n        options,\n        keep_alive,\n      ):\n        yield chunk\n\n    return _generator()"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not prompt and not images:\n      raise ValueError('prompt or images is required')\n\n    if stream:\n      return self._generate_stream(\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        raw=raw,\n        format=format,\n        images=images,\n        options=options,\n        keep_alive=keep_alive,\n      )\n\n    return await self._generate(\n      model=model,\n      prompt=prompt,\n      system=system,\n      template=template,\n      context=context,\n      raw=raw,\n      format=format,\n      images=images,\n      options=options,\n      keep_alive=keep_alive,\n    )\n\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    async with self.session.post(f'{self.base_url}/generate', json=data) as response:\n      if response.status == 200:\n        if stream:\n          async for chunk in response.content.iter_chunked(1024):\n            yield json.loads(chunk)\n        else:\n          return await response.json()\n      else:\n        raise ValueError(await response.text())\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ModelNotProvidedError()\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n    }\n\n    if keep_alive is not None:\n      data['keep-alive'] = keep_alive\n\n    async for chunk in self.request(\n      method='POST',\n      path='/generate',\n      data=data,\n      stream=stream,\n    ):\n      yield chunk\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not self.is_model(model):\n      raise ValueError(f'model {model} not found')\n\n    if not self.is_model_ready(model):\n      raise ValueError(f'model {model} is not ready')\n\n    if not self.is_model_running(model):\n      raise ValueError(f'model {model} is not running')\n\n    if self.is_model_stopped(model):\n      raise ValueError(f'model {model} is stopped')\n\n    if self.is_model_stopping(model):\n      raise ValueError(f'model {model} is stopping')\n\n    if self.is_model_stopping(model):\n      raise ValueError(f'model {model} is stopping')\n\n    if self.is_model_stopping(model):\n      raise ValueError(f'model {model} is stopping')\n\n    if self.is_model_stopping(model):\n      raise ValueError(f'model {model} is stopping')\n\n    if self.is_model_stopping(model):\n      raise ValueError(f'model {model} is stopping')\n\n    if self.is_model_stopping(model):\n      raise ValueError(f'model {model} is stopping')\n\n    if self.is_model_stopping(model):\n      raise ValueError(f'model {model} is stopping')\n\n    if self.is_model_stopping(model):\n      raise ValueError(f'model {model} is stopping')\n\n    if self.is_model_stopping(model):\n      raise ValueError(f'model {model} is stopping')\n\n    if self.is_model_stopping(model):\n      raise ValueError(f'model {model} is stopping')\n\n    if self.is_model_stopping(model):\n      raise ValueError(f'model {model} is stopping')\n\n    if self.is_model_stopping(model):\n      raise ValueError(f'model {model} is stopping')\n\n   "}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n    }\n\n    if options:\n      data.update(options.dict(exclude_unset=True))\n\n    async with self.session.stream_request(\n      method='POST',\n      url=self.url,\n      data=data,\n      keep_alive=keep_alive,\n    ) as response:\n      async for chunk in response.aiter_lines():\n        if not chunk:\n          continue\n        yield json.loads(chunk)"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model required')\n\n    data = {\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n    }\n\n    if keep_alive is not None:\n      data['keep_alive'] = keep_alive\n\n    async for chunk in self._stream(\n      'POST',\n      f'{self.url}/generate/{model}',\n      json=data,\n    ):\n      yield chunk\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('Model is required')\n\n    if not self.url:\n      raise ValueError('URL is required')\n\n    if not self.api_key:\n      raise ValueError('API key is required')\n\n    if not self.organization:\n      raise ValueError('Organization is required')\n\n    if not self.session:\n      self.session = aiohttp.ClientSession()\n\n    if not self.headers:\n      self.headers = {\n        'Authorization': f'Bearer {self.api_key}',\n        'X-OpenAI-Organization': self.organization,\n        'Content-Type': 'application/json',\n      }\n\n    if not self.timeout:\n      self.timeout = aiohttp.ClientTimeout(total=None)\n\n    if not self.keep_alive:\n      self.keep_alive = aiohttp.TCPConnector(keepalive_timeout=30)\n\n    if not self.proxy:\n      self.proxy = None\n\n    if not self.proxy_auth:\n      self.proxy_auth = None\n\n    if not self.proxy_headers:\n      self.proxy_headers = None\n\n    if not self.proxy_orig_auth:\n      self.proxy_orig_auth = None\n\n    if not self.proxy_auth:\n      self.proxy_auth = None\n\n    if not self.proxy_headers:\n      self.proxy_headers = None\n\n    if not self.proxy_orig_auth:\n      self.proxy_orig_auth = None\n\n    if not self.proxy_auth:\n      self.proxy_auth = None\n\n    if not self.proxy_headers:\n      self.proxy_headers = None\n\n    if not self.proxy_orig_auth:\n      self.proxy_orig_auth = None\n\n    if not self.proxy_auth:\n      self.proxy_auth = None\n\n    if not self.proxy_headers:\n      self.proxy_headers = None\n\n    if not self.proxy_orig_auth:\n      self.proxy_orig_"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model required')\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    async with self.session.post(self.url, json=data, stream=stream) as r:\n      if r.status != 200:\n        raise Exception(await r.text())\n\n      if stream:\n        async for line in r.iter_lines():\n          yield json.loads(line)\n      else:\n        return await r.json()\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if model is None:\n      raise ValueError('model is required')\n\n    if images is not None:\n      images = [\n        image if isinstance(image, (str, bytes)) else json.dumps(image)\n        for image in images\n      ]\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    async for chunk in self._request(\n      'POST',\n      '/generate',\n      data={\n        k: v\n        for k, v in data.items()\n        if v is not None and (isinstance(v, (bool, str)) or len(v) > 0)\n      },\n      stream=stream,\n    ):\n      yield chunk\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not self.server:\n      raise ValueError('server is required')\n\n    if not self.api_key:\n      raise ValueError('api_key is required')\n\n    if self.api_key.startswith('sk-'):\n      raise ValueError('api_key is invalid')\n\n    if not self.organization:\n      raise ValueError('organization is required')\n\n    if not self.api_version:\n      raise ValueError('api_version is required')\n\n    if not self.api_base:\n      raise ValueError('api_base is required')\n\n    if not self.api_type:\n      raise ValueError('api_type is required')\n\n    if not self.api_route:\n      raise ValueError('api_route is required')\n\n    if not self.headers:\n      raise ValueError('headers are required')\n\n    if not self.proxy:\n      raise ValueError('proxy is required')\n\n    if not self.timeout:\n      raise ValueError('timeout is required')\n\n    if not self.max_retries:\n      raise ValueError('max_retries is required')\n\n    if not self.max_backoff:\n      raise ValueError('max_backoff is required')\n\n    if not self.max_retries:\n      raise ValueError('max_retries is required')\n\n    if not self.max_backoff:\n      raise ValueError('max_backoff is required')\n\n    if not self.max_retries:\n      raise ValueError('max_retries is required')\n\n    if not self.max_backoff:\n      raise ValueError('max_backoff is required')\n\n    if not self.max_retries:\n      raise ValueError('max_retries is required')\n\n    if not self.max_backoff:\n      raise ValueError('max_backoff is required')\n\n    if not self.max_retries:\n      raise ValueError('max_retries is required')\n\n    if not self.max_backoff:\n      raise Value"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('Model not provided')\n\n    if not isinstance(prompt, str):\n      raise TypeError('Prompt must be a string')\n\n    if not isinstance(system, str):\n      raise TypeError('System must be a string')\n\n    if not isinstance(template, str):\n      raise TypeError('Template must be a string')\n\n    if context is not None and not isinstance(context, (list, tuple)):\n      raise TypeError('Context must be a list or tuple')\n\n    if not isinstance(stream, bool):\n      raise TypeError('Stream must be a boolean')\n\n    if not isinstance(raw, bool):\n      raise TypeError('Raw must be a boolean')\n\n    if format not in ('', 'json'):\n      raise ValueError('Format must be either empty or \"json\"')\n\n    if images is not None and not isinstance(images, (list, tuple)):\n      raise TypeError('Images must be a list or tuple')\n\n    if options is not None and not isinstance(options, Options):\n      raise TypeError('Options must be an instance of Options')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError('Keep-alive must be a float or a string')\n\n    if keep_alive is not None:\n      try:\n        keep_alive = float(keep_alive)\n      except ValueError:\n        raise ValueError('Invalid keep-alive value')\n\n    if keep_alive is not None and keep_alive <= 0:\n      raise ValueError('Keep-alive must be a positive number')\n\n    if keep_alive is not None:\n      self.session.keep_alive = keep_alive\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options':"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not isinstance(prompt, str):\n      raise TypeError('prompt must be a string')\n\n    if not isinstance(system, str):\n      raise TypeError('system must be a string')\n\n    if not isinstance(template, str):\n      raise TypeError('template must be a string')\n\n    if context is not None and not isinstance(context, Sequence):\n      raise TypeError('context must be a sequence')\n\n    if not isinstance(stream, bool):\n      raise TypeError('stream must be a boolean')\n\n    if not isinstance(raw, bool):\n      raise TypeError('raw must be a boolean')\n\n    if format not in ('', 'json'):\n      raise ValueError('format must be either \"\" or \"json\"')\n\n    if images is not None and not isinstance(images, Sequence):\n      raise TypeError('images must be a sequence')\n\n    if options is not None and not isinstance(options, Options):\n      raise TypeError('options must be an instance of Options')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError('keep_alive must be a float or a string')\n\n    headers = {\n      'Content-Type': 'application/json',\n      'Accept': 'application/json',\n      'User-Agent': 'OpenAI/Python/0.0.1',\n    }\n\n    if keep_alive is not None:\n      headers['Connection'] = 'keep-alive'\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n    }\n\n    if stream:\n      async for chunk in self.stream(\n        method='POST',\n        url=f'{self.base_url}/generate',\n        headers"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if images:\n      images = [\n        image.decode() if isinstance(image, bytes) else image\n        for image in images\n      ]\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    response = await self.request(\n      'generate',\n      data=data,\n      stream=stream,\n    )\n\n    if stream:\n      return response\n\n    try:\n      response = await response.json()\n    except Exception as e:\n      raise ResponseError(f'invalid response: {e}') from e\n\n    return response\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if model is None:\n      raise ValueError('model is required')\n\n    if images is not None:\n      images = [encode_image(image) for image in images]\n\n    if stream:\n      return self.generate_stream(\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        images=images,\n        options=options,\n        keep_alive=keep_alive,\n      )\n\n    response = await self.post(\n      '/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context,\n        'images': images,\n        'options': options,\n        'keep_alive': keep_alive,\n      },\n    )\n\n    if format == 'json':\n      return response.json()\n\n    return response.json()\n\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if stream:\n      return self.generate_stream(\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        raw=raw,\n        format=format,\n        images=images,\n        options=options,\n        keep_alive=keep_alive,\n      )\n\n    if not prompt and not images:\n      raise ValueError('prompt or images is required')\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n    }\n\n    async with self.http.post(\n      f'{self.base_url}/generate',\n      json=data,\n      headers=self.headers,\n      stream=True,\n      timeout=self.timeout,\n      keepalive_expiry=keep_alive,\n    ) as response:\n      if response.status_code == 200:\n        return await self.process_response(response, raw, format)\n      else:\n        raise ValueError(await response.text())\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n    }\n    if options:\n      data.update(options.dict())\n\n    async with self.session.post(\n      f'{self.base_url}/generate',\n      json=data,\n      headers=self.headers,\n      params={'keep-alive': keep_alive},\n      stream=stream,\n    ) as response:\n      if not response.ok:\n        raise HTTPError(response.status_code, await response.text())\n\n      if stream:\n        async def stream_generator():\n          async for line in response.content:\n            yield json.loads(line)\n        return stream_generator()\n\n      return await response.json()\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('Model is required')\n\n    if not isinstance(images, (list, tuple)):\n      images = [images]\n\n    if not isinstance(context, (list, tuple)):\n      context = [context]\n\n    if not isinstance(images, list):\n      images = list(images)\n\n    if not isinstance(context, list):\n      context = list(context)\n\n    if images:\n      images = [\n        image.encode('utf-8') if isinstance(image, str) else image\n        for image in images\n      ]\n\n    if context:\n      context = [\n        context.encode('utf-8') if isinstance(context, str) else context\n        for context in context\n      ]\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    async with self.session.post(\n      self.url,\n      json=data,\n      headers=self.headers,\n      timeout=self.timeout,\n      stream=stream,\n    ) as response:\n      if response.status == 200:\n        if stream:\n          async for chunk in response.content.iter_chunked(1024):\n            yield json.loads(chunk)\n        else:\n          return await response.json()\n      else:\n        raise ValueError(await response.text())\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    url = f'{self.base_url}/{model}'\n    if insecure:\n      url += '/insecure'\n    if stream:\n      url += '/stream'\n    async with self.session.get(url) as response:\n      if response.status != 200:\n        raise ResponseError(response.status, await response.text())\n      if stream:\n        async for line in response.content:\n          yield json.loads(line)\n      else:\n        return await response.json()"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    async with self._client.stream(\n      \"POST\",\n      f\"{self._base_url}/models/{model}/pull\",\n      headers=self._headers,\n      params={\n        \"insecure\": insecure,\n        \"stream\": stream,\n      },\n      timeout=None,\n    ) as response:\n      response = await response.json()\n      if response.get(\"error\"):\n        raise ResponseError(response[\"error\"])\n      if stream:\n        async for data in response.iter_content():\n          yield ProgressResponse(**json.loads(data))\n      else:\n        return ProgressResponse(**response)"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if not isinstance(model, str):\n      raise TypeError(f\"model must be a string, not {type(model)}\")\n\n    if not isinstance(insecure, bool):\n      raise TypeError(f\"insecure must be a boolean, not {type(insecure)}\")\n\n    if not isinstance(stream, bool):\n      raise TypeError(f\"stream must be a boolean, not {type(stream)}\")\n\n    if not self.api_key:\n      raise ValueError(\"api_key must be set\")\n\n    if not self.api_base:\n      raise ValueError(\"api_base must be set\")\n\n    url = f\"{self.api_base}/{model}\"\n\n    headers = {\n      \"Authorization\": f\"Bearer {self.api_key}\",\n      \"Content-Type\": \"application/json\",\n    }\n\n    async with self.session.get(url, headers=headers, stream=stream) as response:\n      if response.status != 200:\n        raise ResponseError(response.status, await response.text())\n\n      if stream:\n        async for chunk in response.content.iter_chunked(1024):\n          yield ProgressResponse(chunk)\n      else:\n        return ProgressResponse(await response.text())\n\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    endpoint = f\"{self.api_url}/pull\"\n    headers = {\"Content-Type\": \"application/json\"}\n    data = {\"model\": model}\n\n    if insecure:\n      headers[\"X-Insecure\"] = \"true\"\n\n    async with self.session.post(endpoint, headers=headers, data=json.dumps(data)) as response:\n      if response.status != 200:\n        raise ResponseError(response.status, await response.text())\n\n      if stream:\n        async for chunk in response.content.iter_chunked(1024):\n          yield ProgressResponse.parse_raw(chunk)\n      else:\n        return ProgressResponse.parse_raw(await response.text())"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    raise NotImplementedError\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    params = {\n      \"model\": model,\n      \"insecure\": insecure,\n      \"stream\": stream,\n    }\n    response = await self._request(\n      method=\"POST\",\n      path=\"/v1/pull\",\n      params=params,\n      stream=stream,\n    )\n    if not stream:\n      return response.json()\n    return response.iter_json()\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if stream:\n      return self.stream_pull(model, insecure=insecure)\n    else:\n      return await self.pull_once(model, insecure=insecure)\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    url = f'{self.base_url}/pull/{model}'\n    headers = self.headers\n\n    async with self.session.get(url, headers=headers, stream=stream) as response:\n      if response.status != 200:\n        raise ResponseError(response.status, response.reason)\n\n      if stream:\n        async for chunk in response.content.iter_chunked(1024):\n          yield ProgressResponse(\n            model=model,\n            progress=response.headers.get('X-Progress'),\n            data=chunk,\n          )\n      else:\n        data = await response.json()\n        return ProgressResponse(\n          model=model,\n          progress=response.headers.get('X-Progress'),\n          data=data,\n        )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    url = self._url_for_model(model)\n    if not insecure:\n      url = self._url_for_model(model, \"pull\")\n\n    async with self._session.get(url) as response:\n      if response.status != 200:\n        raise ResponseError(response.status, await response.text())\n\n      if stream:\n        async for chunk in response.content.iter_chunked(1024):\n          yield ProgressResponse.from_json(chunk)\n      else:\n        return await response.json()"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    url = f'{self.api_url}/{model}/pull'\n    headers = {\n      'Authorization': f'Bearer {self.token}',\n      'Content-Type': 'application/json',\n    }\n    params = {\n      'insecure': insecure,\n      'stream': stream,\n    }\n    response = await self.session.post(url, headers=headers, params=params)\n\n    if response.status_code != 200:\n      raise ResponseError(response)\n\n    if not stream:\n      return await response.json()\n\n    async def generator():\n      async for line in response.content:\n        yield json.loads(line)\n\n    return generator()"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    try:\n      async with self._session.get(\n        url=f'{self._url}/models/{model}/pull',\n        params={\n          'insecure': insecure,\n          'stream': stream,\n        },\n        headers=self._headers,\n        timeout=self._timeout,\n        stream=stream,\n      ) as response:\n        if response.status != 200:\n          raise ResponseError(\n            status_code=response.status,\n            message=response.reason,\n          )\n        if stream:\n          async for line in response.content:\n            yield ProgressResponse.parse_raw(line)\n        else:\n          return ProgressResponse.parse_raw(await response.read()).dict()\n    except asyncio.TimeoutError:\n      raise ResponseError(\n        status_code=408,\n        message='Request timed out',\n      )\n\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if insecure:\n      url = self.api_url\n    else:\n      url = self.api_url_secure\n\n    async with self.session.get(\n      url=f\"{url}/v1/models/{model}/pull\",\n      headers=self.headers,\n      timeout=self.timeout,\n      stream=stream,\n    ) as response:\n      if response.status == 200:\n        if stream:\n          async for chunk in response.aiter_json():\n            yield chunk\n        else:\n          return await response.json()\n      else:\n        raise ResponseError(\n          status_code=response.status,\n          message=await response.text(),\n        )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    url = self.url_for(model)\n    headers = self.headers\n    if insecure:\n      url = url.replace(\"https://\", \"http://\")\n      headers[\"host\"] = headers[\"host\"].replace(\"https://\", \"http://\")\n    async with self.session.get(url, headers=headers, stream=stream) as response:\n      if response.status != 200:\n        raise ResponseError(response.status, response.reason)\n      if stream:\n        async for chunk in response.aiter_lines():\n          yield ProgressResponse.parse_raw(chunk)\n      else:\n        return ProgressResponse.parse_raw(await response.text())\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if insecure:\n      url = f\"http://{self.host}:{self.port}/v1/{model}/pull\"\n    else:\n      url = f\"https://{self.host}:{self.port}/v1/{model}/pull\"\n\n    async with self.session.get(url) as resp:\n      if resp.status != 200:\n        raise ResponseError(resp.status, await resp.text())\n\n      if not stream:\n        return await resp.json()\n\n      async def gen():\n        async for line in resp.content:\n          yield json.loads(line)\n\n      return gen()\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if not insecure:\n      url = f'{self.url}/{model}/pull'\n    else:\n      url = f'{self.url}/{model}/pull?insecure=true'\n\n    async with self.session.get(url, stream=stream) as resp:\n      if resp.status != 200:\n        raise ResponseError(resp.status, await resp.json())\n\n      if stream:\n        async for line in resp.content:\n          yield ProgressResponse.parse_raw(line)\n      else:\n        return await resp.json()\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    # Send a request to the API endpoint to pull data for the specified model\n    async with self.stream(\n      \"POST\",\n      f\"/v1/models/{model}/pull\",\n      data={\n        \"insecure\": insecure,\n      },\n      stream=stream,\n    ) as response:\n      # If the request is not successful, raise a ResponseError\n      if not response.ok:\n        raise ResponseError(response)\n      # If the request is successful, yield a ProgressResponse object for each response received\n      async for data in response.aiter_json():\n        yield ProgressResponse.from_dict(data)\n\n    # If stream is False, return a single ProgressResponse object\n    return ProgressResponse.from_dict(await response.json())"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    url = f\"{self.base_url}/{model}/pull\"\n    params = {\n      \"insecure\": insecure,\n      \"stream\": stream,\n    }\n\n    try:\n      async for response in self.client.get(url, params=params):\n        yield response\n    except httpx.ResponseError as e:\n      raise ResponseError(e.response)\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    # Set the URL for the request based on whether the request is insecure or not\n    url = f\"{self.base_url}/{model}\" if insecure else f\"{self.base_url}/{model}/pull\"\n\n    # Set the headers for the request\n    headers = {\n      \"Accept\": \"application/json\",\n      \"Content-Type\": \"application/json\",\n      \"Authorization\": f\"Bearer {self.api_key}\",\n    }\n\n    # Send the request and get the response\n    async with self.session.get(url, headers=headers, stream=stream) as response:\n      # If the response is not successful, raise a ResponseError\n      if response.status != 200:\n        raise ResponseError(response.status, await response.json())\n\n      # If the response is not streaming, return the response as a dictionary\n      if not stream:\n        return await response.json()\n\n      # If the response is streaming, yield a ProgressResponse for each chunk of data\n      async for chunk in response.content.iter_chunked(1024):\n        yield ProgressResponse(chunk)\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    url = self.url / model\n    headers = {\n      'Authorization': f'Bearer {self.token}',\n      'Accept': 'application/json',\n      'Content-Type': 'application/json',\n    }\n    response = await self.session.get(url, headers=headers, stream=stream)\n    if response.status_code != 200:\n      raise ResponseError(response.status_code, await response.text())\n    if not stream:\n      return await response.json()\n    return stream_response(response)\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    # Set the endpoint for the request\n    endpoint = f\"{self.base_url}/v1/models/{model}/pull\"\n\n    # Set the headers for the request\n    headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n\n    # Set the parameters for the request\n    params = {\"insecure\": insecure}\n\n    # Make the request and get the response\n    async with self.session.get(endpoint, headers=headers, params=params, stream=stream) as response:\n\n      # If the response is not successful, raise an error\n      if response.status != 200:\n        raise ResponseError(response)\n\n      # If the response is not streaming, return the response as a dictionary\n      if not stream:\n        return await response.json()\n\n      # If the response is streaming, return a generator that yields progress responses\n      async def _generator():\n        async for chunk in response.content.iter_chunked(1024):\n          yield ProgressResponse(chunk)\n\n      return _generator()\n\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if messages is not None:\n      for message in messages:\n        if not isinstance(message, dict):\n          raise TypeError('messages must be a list of dictionaries')\n        if 'role' not in message:\n          raise ValueError('messages must have a role')\n        if 'content' not in message:\n          raise ValueError('messages must have content')\n        if 'images' in message:\n          if not isinstance(message['images'], list):\n            raise TypeError('images must be a list')\n          for image in message['images']:\n            if not isinstance(image, dict):\n              raise TypeError('images must be a list of dictionaries')\n            if 'url' not in image:\n              raise ValueError('images must have a url')\n\n    if format not in ('', 'json'):\n      raise ValueError('format must be either \"\" or \"json\"')\n\n    if options is not None:\n      if not isinstance(options, dict):\n        raise TypeError('options must be a dictionary')\n      if 'temperature' in options:\n        if not isinstance(options['temperature'], (int, float)):\n          raise TypeError('temperature must be a number')\n        if options['temperature'] < 0 or options['temperature'] > 2:\n          raise ValueError('temperature must be between 0 and 2')\n      if 'top_p' in options:\n        if not isinstance(options['top_p'], (int, float)):\n          raise TypeError('top_p must be a number')\n        if options['top_p'] < 0 or options['top_p'] > 1:\n          raise ValueError('top_p must be between 0 and 1')\n      if 'n' in options:\n        if not isinstance(options['n'], int):\n          raise TypeError('n must be an integer')\n        if options['n'] < 1:\n          raise ValueError('n must be greater than 0')\n      if 'max_tokens' in options:\n        if not isinstance(options['max_tokens'], int):\n          raise TypeError"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate the input parameters\n    if not model:\n      raise ValueError('model is required')\n    if messages is None:\n      messages = []\n    if not isinstance(messages, Sequence):\n      raise TypeError('messages must be a sequence')\n    for message in messages:\n      if not isinstance(message, Mapping):\n        raise TypeError('messages must be a sequence of dictionaries')\n      if 'role' not in message:\n        raise ValueError('message must contain \"role\"')\n      if 'content' not in message:\n        raise ValueError('message must contain \"content\"')\n      if 'images' in message and not isinstance(message['images'], Sequence):\n        raise TypeError('message[\"images\"] must be a sequence')\n\n    # Raise errors for invalid inputs\n    if not model:\n      raise ValueError('model is required')\n    if not isinstance(messages, Sequence):\n      raise TypeError('messages must be a sequence')\n    for message in messages:\n      if not isinstance(message, Mapping):\n        raise TypeError('messages must be a sequence of dictionaries')\n      if 'role' not in message:\n        raise ValueError('message must contain \"role\"')\n      if 'content' not in message:\n        raise ValueError('message must contain \"content\"')\n      if 'images' in message and not isinstance(message['images'], Sequence):\n        raise TypeError('message[\"images\"] must be a sequence')\n\n    # Make the asynchronous request to generate a chat response\n    async for response in self.chat_stream(\n      model=model,\n      messages=messages,\n      format=format,\n      options=options,\n      keep_alive=keep_alive,\n    ):\n      yield response"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if model == '':\n      raise ValueError('model is required')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, Sequence):\n      raise TypeError('messages must be a sequence')\n\n    for message in messages:\n      if not isinstance(message, Mapping):\n        raise TypeError('message must be a mapping')\n\n      if 'role' not in message:\n        raise ValueError('message must have a role')\n\n      if 'content' not in message:\n        raise ValueError('message must have content')\n\n      if 'images' in message:\n        if not isinstance(message['images'], Sequence):\n          raise TypeError('message images must be a sequence')\n\n        for image in message['images']:\n          if not isinstance(image, Mapping):\n            raise TypeError('image must be a mapping')\n\n          if 'url' not in image:\n            raise ValueError('image must have a url')\n\n          if 'data' in image:\n            if not isinstance(image['data'], str):\n              raise TypeError('image data must be a string')\n\n    if options is not None:\n      if not isinstance(options, Mapping):\n        raise TypeError('options must be a mapping')\n\n      if 'temperature' in options:\n        if not isinstance(options['temperature'], (int, float)):\n          raise TypeError('temperature must be a number')\n\n        if options['temperature'] < 0:\n          raise ValueError('temperature must be greater than 0')\n\n      if 'top_p' in options:\n        if not isinstance(options['top_p'], (int, float)):\n          raise TypeError('top_p must be a number')\n\n        if options['top_p'] < 0 or options['top_p'] > 1:\n          raise ValueError('top_p must be between 0 and 1')\n\n      if 'n' in options:\n        if not isinstance(options['n'], int):\n          raise TypeError('n must be an integer')\n\n        if options['n'] < 1:\n          raise ValueError('n must"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n    if messages:\n      for message in messages:\n        if not isinstance(message, dict):\n          raise ValueError(f'message {message} is not a dict')\n        if 'role' not in message or 'content' not in message:\n          raise ValueError(f'message {message} has no role or content')\n        if message.get('images'):\n          for image in message['images']:\n            if not isinstance(image, dict):\n              raise ValueError(f'image {image} is not a dict')\n            if 'url' not in image:\n              raise ValueError(f'image {image} has no url')\n    if format not in ['', 'json']:\n      raise ValueError(f'format {format} is not supported')\n\n    return await self._request(\n      model=model,\n      messages=messages,\n      stream=stream,\n      format=format,\n      options=options,\n      keep_alive=keep_alive,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n    if messages is None:\n      messages = []\n    if format not in ('', 'json'):\n      raise ValueError('format must be either \"\" or \"json\"')\n\n    for message in messages:\n      if not isinstance(message, dict):\n        raise ValueError('messages must be a sequence of dictionaries')\n      if 'role' not in message:\n        raise ValueError('message must contain \"role\"')\n      if 'content' not in message:\n        raise ValueError('message must contain \"content\"')\n      if 'images' in message and not isinstance(message['images'], list):\n        raise ValueError('message[\"images\"] must be a list')\n\n    if stream:\n      async def stream_chat():\n        async for chunk in self._stream_chat(\n          model=model,\n          messages=messages,\n          format=format,\n          options=options,\n          keep_alive=keep_alive,\n        ):\n          yield chunk\n      return stream_chat()\n    else:\n      return await self._chat(\n        model=model,\n        messages=messages,\n        format=format,\n        options=options,\n        keep_alive=keep_alive,\n      )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not messages:\n      raise ValueError('messages is required')\n\n    if not isinstance(messages, Sequence):\n      raise TypeError('messages must be a sequence')\n\n    for message in messages:\n      if not isinstance(message, Mapping):\n        raise TypeError('message must be a mapping')\n\n      if 'role' not in message:\n        raise ValueError('message must have a role')\n\n      if 'content' not in message:\n        raise ValueError('message must have content')\n\n      if message['role'] not in ['system', 'user', 'assistant']:\n        raise ValueError('message role must be one of system, user, or assistant')\n\n      if 'images' in message:\n        if not isinstance(message['images'], Sequence):\n          raise TypeError('message images must be a sequence')\n\n        for image in message['images']:\n          if not isinstance(image, Mapping):\n            raise TypeError('image must be a mapping')\n\n          if 'url' not in image:\n            raise ValueError('image must have a url')\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be one of \"\", \"json\"')\n\n    if options is not None:\n      if not isinstance(options, Mapping):\n        raise TypeError('options must be a mapping')\n\n      for key in options:\n        if key not in [\n          'temperature', 'top_p', 'n', 'stream', 'stop', 'max_tokens', 'presence_penalty', 'frequency_penalty', 'logit_bias', 'user',\n        ]:\n          raise ValueError(f'invalid option: {key}')\n\n    if keep_alive is not None:\n      if not isinstance(keep_alive, (float, str)):\n        raise TypeError('keep_alive must be a float or str')\n\n      if isinstance(keep_alive, str):\n        keep_alive = float(keep_alive)\n\n    if not stream:"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n\n    for message in messages:\n      if not isinstance(message, dict):\n        raise TypeError('message must be a dict')\n\n      if 'role' not in message:\n        raise ValueError('message must contain role')\n\n      if 'content' not in message:\n        raise ValueError('message must contain content')\n\n      if 'images' in message and not isinstance(message['images'], list):\n        raise TypeError('message.images must be a list')\n\n    if not isinstance(stream, bool):\n      raise TypeError('stream must be a bool')\n\n    if not isinstance(format, str):\n      raise TypeError('format must be a string')\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be one of [\"\", \"json\"]')\n\n    if options is not None and not isinstance(options, dict):\n      raise TypeError('options must be a dict')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError('keep_alive must be a float or a string')\n\n    if keep_alive is not None and isinstance(keep_alive, str):\n      try:\n        keep_alive = float(keep_alive)\n      except ValueError:\n        raise ValueError('keep_alive must be a float or a string')\n\n    if keep_alive is not None and keep_alive <= 0:\n      raise ValueError('keep_alive must be greater than 0')\n\n    if stream:\n      return self.stream_chat(\n        model=model,\n        messages=messages,\n        format=format,\n        options=options,\n        keep_alive=keep_alive,\n      )\n\n    return await self.request(\n      method='POST',\n      path='/chat',\n      json={\n        '"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n    if messages is None:\n      messages = []\n    for message in messages:\n      if 'role' not in message:\n        raise ValueError('message must have a role')\n      if 'content' not in message:\n        raise ValueError('message must have content')\n      if 'images' in message:\n        if not isinstance(message['images'], list):\n          raise ValueError('images must be a list')\n        for image in message['images']:\n          if not isinstance(image, dict):\n            raise ValueError('image must be a dict')\n          if 'url' not in image:\n            raise ValueError('image must have a url')\n          if 'b64_json' not in image:\n            raise ValueError('image must have a b64_json')\n    if format not in ('', 'json'):\n      raise ValueError(f'format must be \"\", \"json\" or None, not {format}')\n    if options is None:\n      options = {}\n    if keep_alive is not None:\n      if isinstance(keep_alive, str):\n        keep_alive = float(keep_alive)\n      if not isinstance(keep_alive, float):\n        raise ValueError('keep_alive must be a float or None')\n      if keep_alive < 0:\n        raise ValueError('keep_alive must be positive')\n    async for response in self.chat_stream(\n      model=model,\n      messages=messages,\n      format=format,\n      options=options,\n      keep_alive=keep_alive,\n    ):\n      yield response"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n    if not messages:\n      raise ValueError('messages is required')\n    if format and format != 'json':\n      raise ValueError(f'format {format} is not supported')\n    if not isinstance(messages, list):\n      raise TypeError(f'messages must be a list, not {type(messages)}')\n    if not all(isinstance(m, dict) for m in messages):\n      raise TypeError('messages must be a list of dicts')\n    if not all('role' in m and 'content' in m for m in messages):\n      raise ValueError('messages must have role and content keys')\n    if not all(m['role'] in ['system', 'user', 'assistant'] for m in messages):\n      raise ValueError('messages must have role of system, user, or assistant')\n    if not all(isinstance(m['content'], str) for m in messages):\n      raise TypeError('messages content must be a string')\n    if not all(m['role'] != 'assistant' or 'images' not in m for m in messages):\n      raise ValueError('messages with role \"assistant\" cannot have images')\n    if not all(isinstance(m['images'], list) for m in messages if 'images' in m):\n      raise TypeError('messages images must be a list')\n    if not all(isinstance(m['images'], list) and all(isinstance(i, str) for i in m['images']) for m in messages if 'images' in m):\n      raise TypeError('messages images must be a list of strings')\n    if options and not isinstance(options, dict):\n      raise TypeError(f'options must be a dict, not {type(options)}')\n    if keep_alive and not isinstance(keep_alive, (float, str)):\n      raise TypeError(f'keep_alive must be a float or str, not {type(keep_alive)}')\n\n    params = {\n      'model': model,\n      'messages': messages,\n      'stream': stream,\n      'format': format,"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n    if messages is None:\n      messages = []\n    for message in messages:\n      if 'role' not in message or 'content' not in message:\n        raise ValueError('message must have role and content')\n      if message.get('images') and not isinstance(message['images'], list):\n        raise ValueError('message.images must be a list')\n    if format not in ('', 'json'):\n      raise ValueError(f'format must be \"\", \"json\", or None, not {format}')\n    if options is None:\n      options = {}\n    if keep_alive is not None:\n      if isinstance(keep_alive, str):\n        keep_alive = float(keep_alive)\n      if not isinstance(keep_alive, float):\n        raise ValueError(f'keep_alive must be a float or None, not {keep_alive}')\n      options['keep_alive'] = keep_alive\n    if format == 'json':\n      options['format'] = 'json'\n    if stream:\n      return self.chat_stream(model, messages, options)\n    else:\n      return await self.chat_complete(model, messages, options)\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if messages:\n      for message in messages:\n        if not isinstance(message, dict):\n          raise ValueError(f'message must be a dict, not {type(message)}')\n        if 'role' not in message:\n          raise ValueError('message must have a \"role\" key')\n        if 'content' not in message:\n          raise ValueError('message must have a \"content\" key')\n        if message.get('images') and not isinstance(message['images'], list):\n          raise ValueError('message[\"images\"] must be a list')\n\n    if format and format != 'json':\n      raise ValueError(f'format must be \"\", not \"{format}\"')\n\n    if options is None:\n      options = {}\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, float):\n        keep_alive = f'{keep_alive}s'\n      if not isinstance(keep_alive, str):\n        raise ValueError(f'keep_alive must be a float or a string, not {type(keep_alive)}')\n\n    if stream:\n      return self._stream_chat(model, messages, options, keep_alive)\n    else:\n      return await self._request_chat(model, messages, options, keep_alive)\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n    if messages:\n      for message in messages:\n        if not isinstance(message, dict):\n          raise ValueError('messages must be a sequence of dicts')\n        if 'role' not in message:\n          raise ValueError('messages must have a role key')\n        if 'content' not in message:\n          raise ValueError('messages must have a content key')\n        if 'images' in message:\n          if not isinstance(message['images'], list):\n            raise ValueError('images must be a list')\n          for image in message['images']:\n            if not isinstance(image, dict):\n              raise ValueError('images must be a list of dicts')\n            if 'url' not in image:\n              raise ValueError('images must have a url key')\n    if format not in ('', 'json'):\n      raise ValueError('format must be either \"\" or \"json\"')\n    if options and not isinstance(options, dict):\n      raise ValueError('options must be a dict')\n    if keep_alive and not isinstance(keep_alive, (float, str)):\n      raise ValueError('keep_alive must be a float or a string')\n\n    async for chunk in self.chat_stream(model, messages, options, keep_alive):\n      if format == 'json':\n        yield chunk\n      else:\n        yield self.convert_to_chat_response(chunk)\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n\n    if not all(isinstance(m, dict) for m in messages):\n      raise TypeError('messages must be a list of dicts')\n\n    for m in messages:\n      if 'role' not in m:\n        raise ValueError('role is required for each message')\n      if 'content' not in m:\n        raise ValueError('content is required for each message')\n      if 'images' in m and not isinstance(m['images'], list):\n        raise TypeError('images must be a list')\n\n    if not isinstance(stream, bool):\n      raise TypeError('stream must be a boolean')\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be either \"\" or \"json\"')\n\n    if options is not None and not isinstance(options, dict):\n      raise TypeError('options must be a dict')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError('keep_alive must be a float or a string')\n\n    if stream:\n      async def generator():\n        async for message in self.chat_stream(\n          model,\n          messages,\n          format,\n          options,\n          keep_alive,\n        ):\n          yield message\n      return generator()\n\n    return await self.chat_complete(\n      model,\n      messages,\n      format,\n      options,\n      keep_alive,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if messages is None:\n      messages = []\n    else:\n      for message in messages:\n        if not isinstance(message, dict):\n          raise ValueError('messages must be a sequence of dictionaries')\n        if 'role' not in message or 'content' not in message:\n          raise ValueError('messages must contain at least \"role\" and \"content\" keys')\n        if 'images' in message:\n          if not isinstance(message['images'], list):\n            raise ValueError('\"images\" must be a list of dictionaries')\n          for image in message['images']:\n            if not isinstance(image, dict):\n              raise ValueError('\"images\" must be a list of dictionaries')\n            if 'url' not in image:\n              raise ValueError('\"images\" must contain \"url\" key')\n\n    if format not in ('', 'json'):\n      raise ValueError(f'invalid format: {format}')\n\n    if options is not None:\n      if not isinstance(options, dict):\n        raise ValueError('options must be a dictionary')\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, str):\n        try:\n          keep_alive = float(keep_alive)\n        except ValueError:\n          raise ValueError('keep_alive must be a float or None')\n      elif not isinstance(keep_alive, float):\n        raise ValueError('keep_alive must be a float or None')\n\n    if not stream:\n      return await self.request(\n        'chat',\n        model,\n        messages,\n        format,\n        options,\n        keep_alive,\n      )\n\n    return self.stream_request(\n      'chat',\n      model,\n      messages,\n      format,\n      options,\n      keep_alive,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if messages is None:\n      messages = []\n\n    for message in messages:\n      if not isinstance(message, dict):\n        raise ValueError(f'message must be a dict, got {type(message)}')\n      if 'role' not in message:\n        raise ValueError('message must contain a role')\n      if 'content' not in message:\n        raise ValueError('message must contain content')\n      if 'images' in message and not isinstance(message['images'], list):\n        raise ValueError('message images must be a list')\n\n    if format and format not in ('', 'json'):\n      raise ValueError(f'format must be \"\", \"json\", or None, got {format}')\n\n    if options is None:\n      options = {}\n\n    if keep_alive is None:\n      keep_alive = self.keep_alive\n    elif isinstance(keep_alive, str):\n      keep_alive = float(keep_alive)\n\n    if not isinstance(keep_alive, (float, int)):\n      raise ValueError(f'keep_alive must be a float or int, got {type(keep_alive)}')\n\n    if keep_alive <= 0:\n      raise ValueError(f'keep_alive must be positive, got {keep_alive}')\n\n    if not isinstance(options, dict):\n      raise ValueError(f'options must be a dict, got {type(options)}')\n\n    if not stream:\n      response = await self.request(\n        'POST',\n        f'/chat/{model}',\n        data={\n          'messages': messages,\n          'format': format,\n          'options': options,\n          'keep_alive': keep_alive,\n        },\n      )\n      return response.json()\n\n    async def gen():\n      async with self.request_stream(\n        'POST',\n        f'/chat/{model}',\n        data={\n          'messages': messages,\n          'format': format,\n          'options':"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if messages:\n      for message in messages:\n        if 'role' not in message:\n          raise ValueError('message must have a role')\n        if 'content' not in message:\n          raise ValueError('message must have content')\n        if 'images' in message:\n          if not isinstance(message['images'], list):\n            raise ValueError('message.images must be a list')\n          for image in message['images']:\n            if 'url' not in image:\n              raise ValueError('image must have a url')\n\n    if format not in ['', 'json']:\n      raise ValueError(f'format must be one of [\\'\\', \\'json\\'] (got {format})')\n\n    if format == 'json':\n      if not isinstance(options, dict):\n        raise ValueError('options must be a dict')\n\n    if keep_alive:\n      try:\n        float(keep_alive)\n      except ValueError:\n        if not isinstance(keep_alive, str):\n          raise ValueError('keep_alive must be a float or a string')\n\n    if not stream:\n      return await self.request(\n        'POST',\n        '/chat',\n        data={\n          'model': model,\n          'messages': messages,\n          'format': format,\n          'options': options,\n          'keep_alive': keep_alive,\n        },\n      )\n    else:\n      async for chunk in self.stream(\n        'POST',\n        '/chat',\n        data={\n          'model': model,\n          'messages': messages,\n          'format': format,\n          'options': options,\n          'keep_alive': keep_alive,\n        },\n      ):\n        yield chunk\n\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('Model is required')\n\n    if messages is None:\n      messages = []\n\n    for message in messages:\n      if 'role' not in message:\n        raise ValueError('Messages must have a role')\n      if 'content' not in message:\n        raise ValueError('Messages must have content')\n      if 'images' in message and not isinstance(message['images'], list):\n        raise ValueError('Messages must have a list of images')\n\n    if format not in ('', 'json'):\n      raise ValueError(f'Invalid format: {format}')\n\n    if options is None:\n      options = {}\n\n    if keep_alive is not None:\n      options['keep_alive'] = keep_alive\n\n    if stream:\n      async def stream_response():\n        async for chunk in self.stream_chat(model, messages, options):\n          yield chunk\n      return stream_response()\n    else:\n      return await self.chat_request(model, messages, options)\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n    if messages:\n      validate_messages(messages)\n    if format and format not in ('', 'json'):\n      raise ValueError('format must be one of [\"\", \"json\"]')\n    if options:\n      validate_options(options)\n    if keep_alive:\n      validate_keep_alive(keep_alive)\n\n    data = {\n      'model': model,\n      'messages': messages or [],\n      'stream': stream,\n      'format': format,\n      'options': options or {},\n      'keep_alive': keep_alive,\n    }\n\n    if stream:\n      async def stream_response():\n        async for chunk in self._stream_chat(data):\n          yield chunk\n      return stream_response()\n    else:\n      return await self._request('chat', data)\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not messages:\n      raise ValueError('messages is required')\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be one of [\\'\\', \\'json\\']')\n\n    if options is None:\n      options = {}\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, float):\n        keep_alive = f'{keep_alive}s'\n      elif not isinstance(keep_alive, str):\n        raise ValueError('keep_alive must be a float or a string')\n\n    for message in messages:\n      if not isinstance(message, dict):\n        raise ValueError('messages must be a sequence of dictionaries')\n\n      if 'role' not in message:\n        raise ValueError('message must contain a role')\n\n      if 'content' not in message:\n        raise ValueError('message must contain content')\n\n      if 'images' in message:\n        if not isinstance(message['images'], list):\n          raise ValueError('message.images must be a list')\n\n        for image in message['images']:\n          if not isinstance(image, dict):\n            raise ValueError('message.images must be a list of dictionaries')\n\n          if 'url' not in image:\n            raise ValueError('message.images must contain a url')\n\n          if 'caption' in image and not isinstance(image['caption'], str):\n            raise ValueError('message.images.caption must be a string')\n\n    if format == 'json':\n      options['response_format'] = 'json'\n\n    response = await self.request(\n      'POST',\n      f'/v1/chat/completions',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'options': options,\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n    if stream:\n      return response\n\n    return response.json()"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if messages is None:\n      messages = []\n\n    if format and format not in ('', 'json'):\n      raise ValueError('format must be either \"\" or \"json\"')\n\n    if options is None:\n      options = {}\n\n    if keep_alive is None:\n      keep_alive = self.keep_alive\n\n    if isinstance(keep_alive, str):\n      keep_alive = float(keep_alive)\n\n    if keep_alive < 0:\n      raise ValueError('keep_alive must be a positive number or None')\n\n    for message in messages:\n      if 'role' not in message:\n        raise ValueError('role is required for each message')\n      if 'content' not in message:\n        raise ValueError('content is required for each message')\n      if 'images' in message and not isinstance(message['images'], list):\n        raise ValueError('images must be a list for each message')\n\n    headers = {\n      'Content-Type': 'application/json',\n      'Accept': 'text/event-stream' if stream else 'application/json',\n      'X-Keep-Alive': str(keep_alive),\n    }\n\n    params = {\n      'model': model,\n      'format': format,\n      **options,\n    }\n\n    async with self.session.stream('POST', '/chat', json=messages, params=params, headers=headers) as response:\n      if response.status == 400:\n        raise ValueError(await response.text())\n      elif response.status == 404:\n        raise ValueError('model not found')\n      elif response.status == 500:\n        raise ValueError('server error')\n\n      if stream:\n        async for line in response.content:\n          line = line.decode('utf-8')\n          if not line.strip():\n            continue\n          data = json.loads(line.split('data: ')[-1])\n          yield ChatResponse(data)\n      else:\n        data = await"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    params = {\n      'model': model,\n      'insecure': insecure,\n    }\n    response = await self._request(\n      'POST',\n      '/api/push',\n      params=params,\n      stream=stream,\n    )\n    if stream:\n      async for line in response.iter_lines():\n        yield ProgressResponse.parse_raw(line)\n    else:\n      return ProgressResponse.parse_raw(await response.text())\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self.request(\n      method=\"POST\",\n      path=\"/api/push\",\n      model=model,\n      insecure=insecure,\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    async with self._client.stream(\n      'POST',\n      '/api/push',\n      data={\n        'model': model,\n        'insecure': insecure,\n      },\n      stream=stream,\n    ) as response:\n      if not stream:\n        return await response.json()\n      async for line in response.aiter_lines():\n        yield json.loads(line)"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    params = {\n        'model': model,\n        'insecure': insecure,\n    }\n    async with self._request(\n        'POST',\n        '/api/push',\n        params=params,\n    ) as resp:\n        if stream:\n            async for line in resp.aiter_lines():\n                yield json.loads(line)\n        else:\n            return await resp.json()\n\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    data = {\n      'model': model,\n      'insecure': insecure,\n    }\n    async for response in self._stream_request(\n      'POST',\n      '/api/push',\n      data=data,\n      stream=stream,\n    ):\n      yield ProgressResponse(**response)\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    url = '/api/push'\n    params = {\n        'model': model,\n        'insecure': insecure,\n    }\n    async for response in self._request_generator(url, params, stream=stream):\n      yield ProgressResponse(**response)\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    async def _gen():\n      async for resp in self._stream_request(\n        method='POST',\n        path='/api/push',\n        params={\n          'model': model,\n          'insecure': insecure,\n        },\n        stream=True,\n      ):\n        yield resp\n\n    if stream:\n      return _gen()\n\n    resp = await self._request(\n      method='POST',\n      path='/api/push',\n      params={\n        'model': model,\n        'insecure': insecure,\n      },\n    )\n    return resp\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    payload = {\n      \"model\": model,\n      \"insecure\": insecure,\n    }\n\n    async with self.post(\"/api/push\", json=payload) as response:\n      if stream:\n        async for line in response.iter_lines():\n          yield json.loads(line)\n      else:\n        return await response.json()\n\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    async def _generator():\n      async for response in self.stream(\n        endpoint='/api/push',\n        model=model,\n        insecure=insecure,\n      ):\n        yield response\n\n    if stream:\n      return _generator()\n    else:\n      response = await self.request(\n        endpoint='/api/push',\n        model=model,\n        insecure=insecure,\n      )\n      return response\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    url = self.url_for(f\"api/push\")\n    async with self.request(\n      \"POST\",\n      url,\n      json={\n        \"model\": model,\n        \"insecure\": insecure,\n      },\n      stream=stream,\n    ) as response:\n      if stream:\n        async for line in response.aiter_lines():\n          yield ProgressResponse.parse_raw(line)\n      else:\n        return ProgressResponse.parse_raw(await response.text())\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    response = await self._post(\n      '/api/push',\n      model=model,\n      insecure=insecure,\n      stream=stream,\n    )\n\n    if stream:\n      async def gen() -> AsyncIterator[Mapping[str, Any]]:\n        async for progress in response:\n          yield progress\n      return gen()\n    else:\n      return response\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    async for response in self._api_request(\n      method='POST',\n      path='/api/push',\n      data={\n        'model': model,\n        'insecure': insecure,\n      },\n      stream=stream,\n    ):\n      yield response\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    # Send the POST request to the '/api/push' endpoint with the specified model name, insecure flag, and stream flag.\n    response = await self.post(\n      '/api/push',\n      params={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n    )\n\n    # If the response is a single object, return it as a ProgressResponse object.\n    if not stream:\n      return ProgressResponse(**response.json())\n\n    # If the response is a stream, return a generator yielding ProgressResponse objects.\n    async for line in response.aiter_lines():\n      yield ProgressResponse(**json.loads(line))"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    endpoint = \"/api/push\"\n    params = {\"model\": model, \"insecure\": insecure}\n    async for response in self.stream_request(endpoint, params=params, stream=stream):\n      yield response\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    url = f'/api/push/{model}'\n    if insecure:\n      url += '?insecure=true'\n    async with self._client.stream(\n      'POST',\n      url,\n      timeout=None,\n    ) as response:\n      if stream:\n        async for line in response.aiter_lines():\n          yield json.loads(line)\n      else:\n        return await response.json()"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    if not model:\n      raise ValueError(\"model is required\")\n\n    url = \"/api/push\"\n    params = {\n      \"model\": model,\n      \"insecure\": insecure,\n    }\n\n    async for response in self.stream_request(url, params=params):\n      yield response\n\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    # Send the POST request to the '/api/push' endpoint with the specified model name, insecurity flag, and stream flag\n    response = await self.post(\n      url='/api/push',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n    )\n\n    # If the response is not streaming, return a single ProgressResponse object\n    if not stream:\n      return ProgressResponse(response.json())\n\n    # If the response is streaming, return a generator yielding ProgressResponse objects\n    async for line in response.aiter_lines():\n      yield ProgressResponse(json.loads(line))\n\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    # Send a POST request to the '/api/push' endpoint with the specified model name, insecurity flag, and stream flag\n    async with self.post(\n        '/api/push',\n        params={\n            'model': model,\n            'insecure': insecure,\n            'stream': stream,\n        },\n        stream=True,\n    ) as response:\n        # Handle the response based on the value of the `stream` parameter\n        if stream:\n            # If `stream` is True, yield `ProgressResponse` objects from the response stream\n            async for chunk in response.aiter_lines():\n                yield ProgressResponse.parse_raw(chunk)\n        else:\n            # If `stream` is False, return a single `ProgressResponse` object\n            return ProgressResponse.parse_raw(await response.text())\n\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    if insecure:\n      raise ValueError(\"Insecure mode is not supported for this method.\")\n\n    if stream:\n      return self._stream_request(\n        method=\"POST\",\n        path=\"/api/push\",\n        body={\n          \"model\": model,\n        },\n        response_class=ProgressResponse,\n      )\n    else:\n      return await self._request(\n        method=\"POST\",\n        path=\"/api/push\",\n        body={\n          \"model\": model,\n        },\n        response_class=ProgressResponse,\n      )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    # Send the POST request to the '/api/push' endpoint with the specified model name, insecure flag, and stream flag\n    async with self._client.stream(\n      'POST',\n      '/api/push',\n      params={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      data=None,\n      files=None,\n      timeout=None,\n    ) as response:\n      # If the response status code is not 200, raise an exception\n      if response.status_code != 200:\n        raise Exception(response.text)\n      # If the stream flag is True, yield a generator of ProgressResponse objects\n      if stream:\n        async for chunk in response.aiter_lines():\n          yield json.loads(chunk)\n      # Otherwise, return a single ProgressResponse object\n      else:\n        return json.loads(await response.text())\n\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = sha256()\n    with open(path, 'rb') as f:\n      while True:\n        chunk = f.read(4096)\n        if not chunk:\n          break\n        checksum.update(chunk)\n    digest = 'sha256:' + checksum.hexdigest()\n\n    # Check if a blob with the same checksum already exists on the server\n    response = await self._head(f'{self.base_url}/v2/{self.name}/blobs/{digest}')\n    if response.status_code == 200:\n      return digest\n\n    # Upload the file in chunks to the server\n    async with aiofiles.open(path, 'rb') as f:\n      position = 0\n      while True:\n        chunk = await f.read(4096)\n        if not chunk:\n          break\n        headers = {\n          'Content-Type': 'application/octet-stream',\n          'Content-Length': str(len(chunk)),\n          'Content-Range': f'bytes {position}-{position + len(chunk) - 1}/{os.path.getsize(path)}'\n        }\n        response = await self._post(f'{self.base_url}/v2/{self.name}/blobs/uploads/', headers=headers, data=chunk)\n        if response.status_code != 202:\n          raise Exception(f'Failed to upload chunk: {response.text}')\n        position += len(chunk)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        while True:\n            data = f.read(4096)\n            if not data:\n                break\n            sha256.update(data)\n    checksum = sha256.hexdigest()\n\n    # Check if a blob with the same checksum already exists on the server\n    headers = {'Accept': 'application/vnd.docker.distribution.manifest.v2+json'}\n    response = await self.head(f'/v2/{self.repository}/blobs/{checksum}', headers=headers)\n    if response.status_code == 200:\n        return f'sha256:{checksum}'\n\n    # Upload the file in chunks to the server\n    with open(path, 'rb') as f:\n        headers = {'Content-Type': 'application/octet-stream'}\n        response = await self.post(f'/v2/{self.repository}/blobs/uploads/', headers=headers)\n        location = response.headers['Location']\n        chunk_size = 4096\n        while True:\n            data = f.read(chunk_size)\n            if not data:\n                break\n            response = await self.patch(location, data=data, headers=headers)\n            location = response.headers['Location']\n\n    return f'sha256:{checksum}'"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n      while True:\n        data = f.read(65536)\n        if not data:\n          break\n        sha256.update(data)\n    digest = 'sha256:' + sha256.hexdigest()\n\n    # Check if the blob already exists on the server\n    headers = {'Authorization': f'Bearer {self.token}'}\n    url = f'{self.base_url}/v2/{self.repository}/blobs/{digest}'\n    async with self.session.head(url, headers=headers) as response:\n      if response.status == 200:\n        return digest\n\n    # If the blob does not exist, upload it to the server\n    url = f'{self.base_url}/v2/{self.repository}/blobs/uploads/'\n    async with self.session.post(url, headers=headers) as response:\n      response.raise_for_status()\n      location = response.headers['Location']\n\n    with open(path, 'rb') as f:\n      while True:\n        data = f.read(65536)\n        if not data:\n          break\n        headers['Content-Length'] = str(len(data))\n        async with self.session.patch(location, headers=headers, data=data) as response:\n          response.raise_for_status()\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n    checksum = sha256.hexdigest()\n\n    # Check if a blob with the same checksum already exists\n    url = f\"{self.base_url}/v2/{self.name}/blobs/sha256:{checksum}\"\n    async with self.session.head(url) as response:\n        if response.status == 200:\n            # Blob already exists, no need to upload\n            return f\"sha256:{checksum}\"\n\n    # Upload the file in chunks\n    url = f\"{self.base_url}/v2/{self.name}/blobs/uploads/\"\n    headers = {\n        \"Authorization\": f\"Bearer {self.token}\"\n    }\n    async with self.session.post(url, headers=headers) as response:\n        response.raise_for_status()\n        location = response.headers[\"Location\"]\n\n    with open(path, \"rb\") as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            headers = {\n                \"Authorization\": f\"Bearer {self.token}\",\n                \"Content-Length\": str(len(data)),\n                \"Content-Range\": f\"bytes 0-{len(data)-1}/{os.path.getsize(path)}\"\n            }\n            async with self.session.patch(location, headers=headers, data=data) as response:\n                response.raise_for_status()\n\n    # Return the digest of the file\n    return f\"sha256:{checksum}\"\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n      while True:\n        data = f.read(65536)\n        if not data:\n          break\n        sha256.update(data)\n    digest = 'sha256:' + sha256.hexdigest()\n\n    # Check if the blob already exists\n    try:\n      await self.head(f'{self.base_url}/v2/{self.name}/blobs/{digest}')\n      return digest\n    except HTTPStatusError as e:\n      if e.response.status_code != 404:\n        raise e\n\n    # Upload the file in chunks\n    with open(path, 'rb') as f:\n      while True:\n        data = f.read(65536)\n        if not data:\n          break\n        await self.post(\n          f'{self.base_url}/v2/{self.name}/blobs/uploads/',\n          data=data,\n          headers={\n            'Content-Type': 'application/octet-stream',\n            'Content-Length': str(len(data)),\n          },\n        )\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        while True:\n            data = f.read(1024)\n            if not data:\n                break\n            sha256.update(data)\n    checksum = sha256.hexdigest()\n\n    # Check if the blob already exists on the server\n    response = await self.head(f'v2/{self.repository}/blobs/{checksum}')\n    if response.status_code == 404:\n        # If the blob does not exist, upload it to the server\n        with open(path, 'rb') as f:\n            response = await self.post(\n                f'v2/{self.repository}/blobs/uploads/',\n                data=f,\n                headers={\n                    'Content-Type': 'application/octet-stream',\n                    'Content-Length': str(os.path.getsize(path)),\n                },\n            )\n            if response.status_code != 201:\n                raise Exception(f'Failed to upload blob: {response.status_code}')\n\n    # Return the digest of the file\n    return f'sha256:{checksum}'\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    with open(path, 'rb') as f:\n      file_hash = hashlib.sha256()\n      while True:\n        chunk = f.read(8192)\n        if not chunk:\n          break\n        file_hash.update(chunk)\n    digest = f'sha256:{file_hash.hexdigest()}'\n\n    # Check if the blob already exists on the server\n    async with self.session.head(f'{self.base_url}/v2/{self.repository}/blobs/{digest}') as resp:\n      if resp.status == 404:\n        # Upload the file to the server\n        async with self.session.post(f'{self.base_url}/v2/{self.repository}/blobs/uploads/') as resp:\n          resp.raise_for_status()\n          location = resp.headers['Location']\n          async with self.session.put(location, data=f) as resp:\n            resp.raise_for_status()\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        while True:\n            data = f.read(4096)\n            if not data:\n                break\n            sha256.update(data)\n    digest = 'sha256:' + sha256.hexdigest()\n\n    # Check if the blob already exists on the server\n    url = f'{self.base_url}/v2/{self.repository}/blobs/{digest}'\n    async with self.session.head(url, headers=self.headers) as response:\n        if response.status == 200:\n            return digest\n\n    # Upload the file to the server in chunks\n    with open(path, 'rb') as f:\n        while True:\n            chunk = f.read(4096)\n            if not chunk:\n                break\n            data = chunk\n            while True:\n                async with self.session.post(url, data=data, headers=self.headers) as response:\n                    if response.status == 201:\n                        break\n                    elif response.status == 413:\n                        # The chunk is too large, split it in half and try again\n                        data = chunk[:len(chunk) // 2]\n                        chunk = chunk[len(chunk) // 2:]\n                    else:\n                        raise Exception(f'Failed to upload blob: {response.status} {response.reason}')\n\n    return digest\n\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = hashlib.sha256()\n    with open(path, 'rb') as f:\n        while True:\n            chunk = f.read(65536)\n            if not chunk:\n                break\n            checksum.update(chunk)\n    checksum = checksum.hexdigest()\n\n    # Check if the blob already exists on the server\n    url = f'{self.base_url}/v2/{self.repository}/blobs/{checksum}'\n    headers = {'Authorization': f'Bearer {self.token}'}\n    async with self.session.head(url, headers=headers) as response:\n        if response.status == 200:\n            # The blob already exists on the server, so we can return its digest\n            return f'sha256:{checksum}'\n\n    # Upload the file to the server in chunks\n    with open(path, 'rb') as f:\n        while True:\n            chunk = f.read(65536)\n            if not chunk:\n                break\n            headers = {'Authorization': f'Bearer {self.token}', 'Content-Type': 'application/octet-stream'}\n            async with self.session.post(url, headers=headers, data=chunk) as response:\n                if response.status != 201:\n                    raise Exception(f'Failed to upload chunk of file {path} to {url}: {response.status} {response.reason}')\n\n    # Return the digest of the file\n    return f'sha256:{checksum}'\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    path = Path(path)\n\n    # Calculate the SHA256 checksum of the file\n    with path.open(\"rb\") as f:\n      file_hash = hashlib.sha256()\n      while True:\n        chunk = f.read(4096)\n        if not chunk:\n          break\n        file_hash.update(chunk)\n      digest = f\"sha256:{file_hash.hexdigest()}\"\n\n    # Check if the blob already exists on the server\n    response = await self.session.head(f\"{self.base_url}/v2/{self.name}/blobs/{digest}\")\n    if response.status == 200:\n      return digest\n\n    # Upload the file to the server in chunks\n    with path.open(\"rb\") as f:\n      while True:\n        chunk = f.read(4096)\n        if not chunk:\n          break\n        response = await self.session.post(\n          f\"{self.base_url}/v2/{self.name}/blobs/uploads/\",\n          headers={\n            \"Content-Type\": \"application/octet-stream\",\n            \"Content-Length\": str(len(chunk)),\n          },\n          data=chunk,\n        )\n        response.raise_for_status()\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    async with aiofiles.open(path, 'rb') as f:\n      checksum = hashlib.sha256()\n      while True:\n        chunk = await f.read(4096)\n        if not chunk:\n          break\n        checksum.update(chunk)\n    digest = f'sha256:{checksum.hexdigest()}'\n    headers = {'Docker-Content-Digest': digest}\n    async with self.session.head(f'{self.registry_url}/v2/{self.repository}/blobs/{digest}', headers=headers) as resp:\n      if resp.status == 404:\n        async with aiofiles.open(path, 'rb') as f:\n          while True:\n            chunk = await f.read(4096)\n            if not chunk:\n              break\n            async with self.session.post(f'{self.registry_url}/v2/{self.repository}/blobs/uploads/', headers=headers) as resp:\n              if resp.status == 201:\n                upload_url = resp.headers['Location']\n                async with self.session.put(upload_url, data=chunk) as resp:\n                  if resp.status != 202:\n                    raise Exception(f'Failed to upload chunk: {resp.status}')\n              else:\n                raise Exception(f'Failed to start upload: {resp.status}')\n        async with self.session.post(f'{self.registry_url}/v2/{self.repository}/blobs/uploads/{upload_id}') as resp:\n          if resp.status != 201:\n            raise Exception(f'Failed to complete upload: {resp.status}')\n      elif resp.status != 200:\n        raise Exception(f'Failed to check blob existence: {resp.status}')\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = hashlib.sha256()\n    with open(path, 'rb') as f:\n      for chunk in iter(lambda: f.read(4096), b''):\n        checksum.update(chunk)\n    digest = f'sha256:{checksum.hexdigest()}'\n\n    # Check if a blob with the same digest already exists on the server\n    response = await self._get(f'v2/{self._repository}/blobs/{digest}')\n    if response.status_code == 200:\n      return digest\n\n    # If the blob does not exist, upload the file in chunks to the server\n    with open(path, 'rb') as f:\n      response = await self._post(f'v2/{self._repository}/blobs/uploads/', headers={'Content-Type': 'application/octet-stream'})\n      location = response.headers['Location']\n      offset = 0\n      while True:\n        chunk = f.read(4096)\n        if not chunk:\n          break\n        response = await self._patch(location, data=chunk, headers={'Content-Range': f'bytes {offset}-{offset + len(chunk) - 1}/{os.path.getsize(path)}'})\n        offset += len(chunk)\n\n    # Return the digest of the file\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    async with aiofiles.open(path, 'rb') as f:\n      data = await f.read()\n    checksum = hashlib.sha256(data).hexdigest()\n    headers = {'Content-Type': 'application/octet-stream', 'Digest': f'sha256:{checksum}'}\n    async with self.session.head(f'{self.registry_url}/v2/{self.repository}/blobs/{checksum}') as resp:\n      if resp.status != 404:\n        return f'sha256:{checksum}'\n    async with self.session.post(f'{self.registry_url}/v2/{self.repository}/blobs/uploads/', headers=headers) as resp:\n      location = resp.headers['Location']\n    async with self.session.patch(location, data=data) as resp:\n      return f'sha256:{checksum}'\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = hashlib.sha256()\n    with open(path, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b''):\n            checksum.update(chunk)\n    checksum = checksum.hexdigest()\n\n    # Check if a blob with the same checksum already exists on the server\n    try:\n        await self.head(f'v2/{self.repository}/blobs/{checksum}')\n        return f'sha256:{checksum}'\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code != 404:\n            raise e\n\n    # Upload the file in chunks to the server\n    with open(path, 'rb') as f:\n        data = f.read()\n        for i in range(0, len(data), 4096):\n            chunk = data[i:i+4096]\n            await self.post(f'v2/{self.repository}/blobs/uploads/', data=chunk)\n\n    # Return the digest of the file\n    return f'sha256:{checksum}'\n\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    with open(path, 'rb') as f:\n        sha256 = hashlib.sha256()\n        while True:\n            data = f.read(4096)\n            if not data:\n                break\n            sha256.update(data)\n    checksum = sha256.hexdigest()\n\n    # Check if the blob already exists on the server\n    url = f'{self.base_url}/v2/{self.repository}/blobs/{checksum}'\n    response = await self.session.head(url, headers=self.headers)\n    if response.status == 404:\n        # If the blob does not exist, upload it to the server\n        with open(path, 'rb') as f:\n            data = f.read()\n        url = f'{self.base_url}/v2/{self.repository}/blobs/uploads/'\n        response = await self.session.post(url, headers=self.headers, data=data)\n        location = response.headers['Location']\n        response = await self.session.put(location, headers=self.headers, data=data)\n        if response.status != 201:\n            raise Exception(f'Failed to upload blob: {response.status}')\n\n    # Return the digest of the file\n    return f'sha256:{checksum}'\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    with open(path, 'rb') as f:\n        file_bytes = f.read()\n        file_hash = hashlib.sha256(file_bytes).hexdigest()\n\n    # Check if the blob already exists on the server\n    url = f'{self.url}/v2/{self.name}/blobs/{file_hash}'\n    async with self.session.head(url) as response:\n        if response.status == 200:\n            return f'sha256:{file_hash}'\n\n    # If the blob doesn't exist, upload it to the server\n    url = f'{self.url}/v2/{self.name}/blobs/uploads/'\n    async with self.session.post(url) as response:\n        if response.status != 201:\n            raise Exception(f'Failed to create upload session: {response.status} {response.reason}')\n        location = response.headers['Location']\n\n    # Upload the file in chunks to the server\n    with open(path, 'rb') as f:\n        while True:\n            chunk = f.read(self.chunk_size)\n            if not chunk:\n                break\n            headers = {'Content-Type': 'application/octet-stream'}\n            async with self.session.patch(location, data=chunk, headers=headers) as response:\n                if response.status != 202:\n                    raise Exception(f'Failed to upload chunk: {response.status} {response.reason}')\n\n    # Return the digest of the file\n    return f'sha256:{file_hash}'\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    checksum = await self.get_checksum(path)\n    url = f\"{self.url}/v2/{self.name}/blobs/{checksum}\"\n    headers = {\n      \"Authorization\": f\"Bearer {self.token}\"\n    }\n    async with self.session.head(url, headers=headers) as response:\n      if response.status == 404:\n        async with aiofiles.open(path, \"rb\") as file:\n          while True:\n            chunk = await file.read(self.chunk_size)\n            if not chunk:\n              break\n            async with self.session.post(url, headers=headers, data=chunk) as response:\n              response.raise_for_status()\n    return f\"sha256:{checksum}\"\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    async with aiofiles.open(path, 'rb') as f:\n      file_data = await f.read()\n\n    checksum = hashlib.sha256(file_data).hexdigest()\n    url = f'{self.base_url}/v2/{self.name}/blobs/{checksum}'\n\n    async with self.session.head(url) as response:\n      if response.status == 200:\n        return f'sha256:{checksum}'\n\n    async with self.session.post(url, data=file_data) as response:\n      response.raise_for_status()\n      return f'sha256:{checksum}'\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    checksum = self._calculate_checksum(path)\n    headers = {\"Content-Type\": \"application/octet-stream\"}\n    async with aiofiles.open(path, \"rb\") as f:\n      async with self.session.head(self.url + f\"/v2/{self.repository}/blobs/{checksum}\") as r:\n        if r.status == 404:\n          async with self.session.post(self.url + f\"/v2/{self.repository}/blobs/uploads/\", headers=headers) as r:\n            location = r.headers[\"Location\"]\n            while True:\n              chunk = await f.read(1024)\n              if not chunk:\n                break\n              async with self.session.patch(location, data=chunk, headers=headers) as r:\n                if r.status != 202:\n                  raise Exception(f\"Error uploading blob: {r.status} {r.reason}\")\n      return f\"sha256:{checksum}\"\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = self._calculate_checksum(path)\n\n    # Check if a blob with the same checksum already exists on the server\n    response = await self.head(f\"{self.base_url}/v2/{self.repository}/blobs/{checksum}\")\n    if response.status_code == 200:\n      # Blob already exists, return the digest\n      return f\"sha256:{checksum}\"\n\n    # Upload the file to the server in chunks\n    with open(path, \"rb\") as file:\n      while True:\n        chunk = file.read(self.chunk_size)\n        if not chunk:\n          break\n        response = await self.post(\n          f\"{self.base_url}/v2/{self.repository}/blobs/uploads/\",\n          headers={\"Content-Type\": \"application/octet-stream\"},\n          data=chunk,\n        )\n        if response.status_code != 200:\n          raise Exception(f\"Failed to upload chunk: {response.text}\")\n\n    # Return the digest of the file\n    return f\"sha256:{checksum}\"\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        combined_code = f\"{user_code}\\n{test_code}\"\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as temp_file:\n            temp_file.write(combined_code.encode())\n            temp_file.flush()\n\n            # Run Pyright on the temporary file\n            pyright_result = subprocess.run(\n                [\"pyright\", temp_file.name], capture_output=True, text=True\n            )\n\n            # Delete the temporary file\n            os.unlink(temp_file.name)\n\n        # Extract the relevant lines from the Pyright output\n        relevant_lines = cls._extract_relevant_lines(pyright_result.stdout)\n\n        # Check if the type check passed\n        passed = cls._check_type_check_passed(relevant_lines)\n\n        # Create the result object\n        result = TypeCheckResult(\n            passed=passed,\n            message=cls._generate_result_message(relevant_lines),\n        )\n\n        return result\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code into a single string\n        combined_code = user_code + \"\\n\" + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as temp_file:\n            temp_file.write(combined_code)\n            temp_file.flush()\n\n            # Run Pyright on the temporary file\n            result = subprocess.run(\n                [\"pyright\", temp_file.name, \"--outputjson\"],\n                capture_output=True,\n                text=True,\n            )\n\n        # Delete the temporary file\n        os.unlink(temp_file.name)\n\n        # Parse the Pyright output\n        pyright_output = json.loads(result.stdout)\n\n        # Check if Pyright found any errors\n        if len(pyright_output[\"generalDiagnostics\"]) > 0:\n            # Initialize a list to store the expected type errors\n            expected_type_errors = []\n\n            # Iterate over the general diagnostics\n            for diagnostic in pyright_output[\"generalDiagnostics\"]:\n                # Check if the diagnostic is an expected type error\n                if diagnostic[\"message\"] in cls.expected_type_errors:\n                    # Append the line number to the list of expected type errors\n                    expected_type_errors.append(diagnostic[\"range\"][\"start\"][\"line\"])\n\n            # Check if the expected type errors match the expected type errors\n            if expected_type_errors == cls.expected_type_errors:\n                # Return a success message\n                return TypeCheckResult(\n                    message=\"Type check passed\", passed=True\n                )\n            else:\n                # Return a failure message with the expected type errors\n                return TypeCheckResult(\n                    message=\"Type check failed, expected type errors: \"\n                    + str(cls.expected_type_errors),\n                    passed=False,\n                )\n        else:\n            # Return a success message\n            return TypeCheckResult(message=\"Type check passed\", passed=True)"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        combined_code = user_code + \"\\n\" + test_code\n\n        # Write the combined code to a temporary file\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as temp_file:\n            temp_file.write(combined_code)\n            temp_file.flush()\n\n            # Run Pyright on the temporary file\n            pyright_process = subprocess.Popen(\n                [\"pyright\", temp_file.name],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                universal_newlines=True,\n            )\n\n            # Wait for Pyright to finish and get the output\n            stdout, stderr = pyright_process.communicate()\n\n            # Check if Pyright reported any errors\n            if \"error\" in stdout or \"error\" in stderr:\n                # Parse the Pyright output to identify lines with expected type errors\n                expected_errors = cls._parse_pyright_output(stdout)\n\n                # Check if the expected type errors were reported\n                if expected_errors:\n                    return TypeCheckResult(\n                        message=\"Type check passed.\", passed=True\n                    )\n                else:\n                    return TypeCheckResult(\n                        message=\"Type check failed.\", passed=False\n                    )\n            else:\n                return TypeCheckResult(\n                    message=\"Type check passed.\", passed=True\n                )\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        code = user_code + \"\\n\" + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as tmp:\n            tmp.write(code.encode())\n            tmp_file_path = tmp.name\n\n        # Run Pyright on the combined code and capture the output\n        pyright_output = subprocess.run(\n            [\"pyright\", tmp_file_path], capture_output=True, text=True\n        )\n\n        # Delete the temporary file\n        os.remove(tmp_file_path)\n\n        # Check if the type check passed or failed\n        if \"error\" in pyright_output.stdout:\n            # If there are errors, identify the expected lines with type errors\n            expected_lines = cls._identify_expected_lines(test_code)\n\n            # Extract the error messages from the Pyright output\n            error_messages = cls._extract_error_messages(pyright_output.stdout)\n\n            # Check if the expected lines with type errors were found\n            if expected_lines:\n                # If the expected lines were found, check if the error messages match the expected lines\n                if cls._check_error_messages(error_messages, expected_lines):\n                    # If the error messages match the expected lines, return a success result\n                    return TypeCheckResult(\n                        message=\"Type check passed\", passed=True\n                    )\n                else:\n                    # If the error messages do not match the expected lines, return a failure result with the error messages\n                    return TypeCheckResult(\n                        message=\"Type check failed\",\n                        passed=False,\n                        error_messages=error_messages,\n                    )\n            else:\n                # If the expected lines were not found, return a failure result with the error messages\n                return TypeCheckResult(\n                    message=\"Type check failed\",\n                    passed=False,\n                    error_messages=error_messages,\n                )\n        else:\n            # If there are no errors, return a success result\n            return TypeCheckResult(message=\""}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        combined_code = user_code + \"\\n\" + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as temp_file:\n            temp_file.write(combined_code.encode(\"utf-8\"))\n            temp_file.flush()\n\n            # Run Pyright on the temporary file\n            result = subprocess.run(\n                [\"pyright\", temp_file.name],\n                capture_output=True,\n                text=True,\n            )\n\n        # Delete the temporary file\n        os.unlink(temp_file.name)\n\n        # Check if Pyright found any errors\n        if result.returncode == 0:\n            # If no errors were found, return a successful result\n            return TypeCheckResult(\n                message=\"Type check passed\", passed=True\n            )\n        else:\n            # If errors were found, extract the lines with expected type errors\n            error_lines = cls._extract_expected_type_error_lines(\n                result.stderr\n            )\n\n            # If no lines with expected type errors were found, return a failed result\n            if not error_lines:\n                return TypeCheckResult(\n                    message=\"Type check failed\", passed=False\n                )\n\n            # If lines with expected type errors were found, return a successful result with the lines\n            return TypeCheckResult(\n                message=f\"Type check passed, but expected type errors on lines {error_lines}\",\n                passed=True,\n            )\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        code = f\"{user_code}\\n{test_code}\"\n\n        # Run the type check using Pyright\n        result = cls._run_pyright_type_check(code)\n\n        # Check if the type check passed\n        if result.passed:\n            # If the type check passed, return a TypeCheckResult object with a message indicating success\n            return TypeCheckResult(\n                message=\"Type check passed\", passed=True\n            )\n\n        # If the type check failed, extract the expected type errors\n        expected_type_errors = cls._extract_expected_type_errors(\n            result.output\n        )\n\n        # If there are no expected type errors, return a TypeCheckResult object with a message indicating failure\n        if not expected_type_errors:\n            return TypeCheckResult(\n                message=\"Type check failed\", passed=False\n            )\n\n        # If there are expected type errors, return a TypeCheckResult object with a message indicating success\n        return TypeCheckResult(\n            message=\"Type check passed\", passed=True\n        )\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code into a single string\n        combined_code = f\"{user_code}\\n{test_code}\"\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as temp_file:\n            temp_file.write(combined_code.encode())\n            temp_file.flush()\n\n            # Run Pyright on the temporary file and capture the output\n            pyright_output = subprocess.run(\n                [\"pyright\", temp_file.name], capture_output=True\n            )\n\n        # Delete the temporary file\n        os.remove(temp_file.name)\n\n        # Extract the lines with expected type errors from the Pyright output\n        lines_with_expected_errors = [\n            int(line.split(\":\")[1])\n            for line in pyright_output.stdout.decode().splitlines()\n            if \"error:\" in line\n        ]\n\n        # Check if the type check passed or failed based on the presence of expected errors\n        if lines_with_expected_errors:\n            result_message = \"Type check failed.\"\n            passed = False\n        else:\n            result_message = \"Type check passed.\"\n            passed = True\n\n        # Create a TypeCheckResult object and return it\n        return TypeCheckResult(passed, result_message)\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        code = user_code + \"\\n\" + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as temp_file:\n            temp_file.write(code)\n            temp_file.flush()\n            temp_file.close()\n\n            # Run Pyright on the temporary file\n            result = subprocess.run(\n                [\"pyright\", temp_file.name], capture_output=True, text=True\n            )\n\n            # Delete the temporary file\n            os.remove(temp_file.name)\n\n            # Extract the relevant error messages\n            error_messages = cls._extract_error_messages(result.stdout)\n\n            # Check if the type check passed\n            passed = cls._type_check_passed(error_messages)\n\n            # Create the result object\n            result = TypeCheckResult(passed=passed, message=error_messages)\n\n        return result\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        combined_code = user_code + \"\\n\" + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(\n            mode=\"w\", suffix=\".py\", delete=False\n        ) as temp_file:\n            temp_file.write(combined_code)\n            temp_file.flush()\n\n            # Run Pyright type check on the temporary file\n            pyright_process = subprocess.Popen(\n                [\"pyright\", temp_file.name],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n            )\n            stdout, stderr = pyright_process.communicate()\n\n            # Delete the temporary file\n            os.remove(temp_file.name)\n\n        # Parse Pyright output to identify lines with expected type errors\n        expected_errors = cls._parse_pyright_output(stdout.decode(\"utf-8\"))\n\n        # Check if any expected type errors were found\n        if expected_errors:\n            # Return a result indicating that the type check failed\n            return TypeCheckResult(\n                message=\"Type check failed\", passed=False\n            )\n        else:\n            # Return a result indicating that the type check passed\n            return TypeCheckResult(\n                message=\"Type check passed\", passed=True\n            )\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        combined_code = user_code + \"\\n\" + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as temp_file:\n            temp_file.write(combined_code.encode())\n            temp_file.flush()\n\n            # Run Pyright on the temporary file\n            result = subprocess.run(\n                [\"pyright\", temp_file.name], capture_output=True, text=True\n            )\n\n        # Delete the temporary file\n        os.remove(temp_file.name)\n\n        # Extract the relevant lines with expected type errors from the Pyright output\n        lines_with_errors = cls._extract_lines_with_errors(result.stdout)\n\n        # Determine if the type check passed or failed based on the presence of expected type errors\n        passed = len(lines_with_errors) == 0\n\n        # Create a message detailing the outcome of the type check\n        message = (\n            \"Type check passed\" if passed else f\"Type check failed on lines: {lines_with_errors}\"\n        )\n\n        # Return the result of the type check\n        return TypeCheckResult(passed, message)\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        combined_code = f\"{user_code}\\n{test_code}\"\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(\n            mode=\"w\", suffix=\".py\", delete=False\n        ) as temp_file:\n            temp_file.write(combined_code)\n            temp_file.flush()\n\n            # Run Pyright on the temporary file\n            pyright_process = subprocess.run(\n                [\"pyright\", temp_file.name],\n                capture_output=True,\n                text=True,\n            )\n\n        # Read the Pyright output\n        pyright_output = pyright_process.stdout\n\n        # Check for expected type errors\n        expected_errors = cls._get_expected_type_errors(test_code)\n        type_check_passed = True\n        error_messages = []\n\n        for line_number, error_message in expected_errors.items():\n            if error_message not in pyright_output:\n                type_check_passed = False\n                error_messages.append(\n                    f\"Expected type error '{error_message}' not found on line {line_number}\"\n                )\n\n        # Clean up the temporary file\n        os.unlink(temp_file.name)\n\n        # Return the type check result\n        result = TypeCheckResult(\n            type_check_passed,\n            \"\\n\".join(error_messages),\n        )\n        return result"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code into a single string\n        combined_code = f\"{user_code}\\n{test_code}\"\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as temp_file:\n            temp_file.write(combined_code.encode())\n            temp_file.flush()\n\n            # Run Pyright on the temporary file\n            result = subprocess.run(\n                [\n                    \"pyright\",\n                    temp_file.name,\n                    \"--outputjson\",\n                    \"--typeshed-path\",\n                    \"pyright_venv/lib/python3.10/site-packages/typeshed-master\",\n                ],\n                capture_output=True,\n            )\n\n        # Read the Pyright output from the temporary file\n        pyright_output = result.stdout.decode()\n\n        # Parse the Pyright output as JSON\n        pyright_result = json.loads(pyright_output)\n\n        # Check if Pyright found any errors\n        if \"generalDiagnostics\" in pyright_result:\n            # Initialize a list to store the expected type errors\n            expected_type_errors = []\n\n            # Iterate over the general diagnostics\n            for diagnostic in pyright_result[\"generalDiagnostics\"]:\n                # Check if the diagnostic is an expected type error\n                if diagnostic[\"message\"] in cls.expected_type_errors:\n                    # Add the diagnostic to the list of expected type errors\n                    expected_type_errors.append(diagnostic)\n\n            # Check if the expected type errors match the expected type errors in the test code\n            if expected_type_errors == cls.expected_type_errors:\n                # Return a success result\n                return TypeCheckResult(\n                    message=\"Type check passed\",\n                    passed=True,\n                )\n            else:\n                # Return a failure result with the expected and actual type errors\n                return TypeCheckResult(\n                    message=\"Type check failed\",\n                    passed=False,\n                    expected_type_errors=cls.expected_type_errors,"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        combined_code = user_code + \"\\n\" + test_code\n\n        # Create a temporary file with the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as temp_file:\n            temp_file.write(combined_code)\n            temp_file.flush()\n            temp_file_path = temp_file.name\n\n        # Run Pyright on the temporary file\n        pyright_result = subprocess.run(\n            [\"pyright\", temp_file_path], capture_output=True, text=True\n        )\n\n        # Delete the temporary file\n        os.unlink(temp_file_path)\n\n        # Extract the expected type errors from the test code\n        expected_type_errors = cls._extract_expected_type_errors(test_code)\n\n        # Check if the Pyright output contains the expected type errors\n        type_check_passed = cls._check_type_errors(\n            pyright_result.stdout, expected_type_errors\n        )\n\n        # Prepare the result message\n        result_message = cls._prepare_result_message(\n            pyright_result.stdout, type_check_passed\n        )\n\n        # Return the result\n        return TypeCheckResult(result_message, type_check_passed)\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        combined_code = user_code + \"\\n\" + test_code\n\n        # Write the combined code to a temporary file\n        with tempfile.NamedTemporaryFile(\n            mode=\"w\", suffix=\".py\", delete=False\n        ) as temp_file:\n            temp_file.write(combined_code)\n            temp_file.flush()\n\n        # Create a temporary directory for Pyright's cache\n        with tempfile.TemporaryDirectory() as cache_dir:\n            # Run Pyright on the temporary file with the cache directory\n            pyright_process = subprocess.Popen(\n                [\n                    \"pyright\",\n                    temp_file.name,\n                    \"--cache\",\n                    \"--cacheDirectory\",\n                    cache_dir,\n                ],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n            )\n\n            # Wait for Pyright to finish\n            pyright_process.wait()\n\n            # Read the output and error streams\n            output, error = pyright_process.communicate()\n\n            # Decode the output and error streams\n            output = output.decode(\"utf-8\")\n            error = error.decode(\"utf-8\")\n\n            # Check if Pyright encountered any errors\n            if pyright_process.returncode != 0:\n                # If there are errors, return a TypeCheckResult object with the error message and a failed status\n                return TypeCheckResult(\n                    message=f\"Pyright encountered errors:\\n{error}\",\n                    passed=False,\n                )\n\n            # If there are no errors, check the output for expected type errors\n            expected_errors = cls._identify_expected_type_errors(\n                user_code, test_code\n            )\n            if expected_errors:\n                # If there are expected type errors, return a TypeCheckResult object with the error message and a failed status\n                return TypeCheckResult(\n                    message=f\"Expected type errors not found:\\n{expected_errors}\",\n                    passed=False,\n                )\n\n        # If the type check passed, return a TypeCheck"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        combined_code = f\"{user_code}\\n{test_code}\"\n\n        # Use Pyright to perform the type check\n        type_check_result = cls._run_pyright(combined_code)\n\n        # Check if the type check passed\n        passed = type_check_result.passed\n\n        # Identify the lines with expected type errors\n        expected_type_errors = cls._identify_expected_type_errors(test_code)\n\n        # Prepare the result message\n        message = cls._prepare_result_message(\n            passed, type_check_result, expected_type_errors\n        )\n\n        # Return the result\n        return TypeCheckResult(passed, message)\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        combined_code = user_code + \"\\n\" + test_code\n\n        # Save combined code to a temporary file\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as temp_file:\n            temp_file.write(combined_code.encode(\"utf-8\"))\n            temp_file.flush()\n\n            # Run Pyright on the temporary file\n            try:\n                subprocess.run(\n                    [\"pyright\", temp_file.name],\n                    check=True,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                )\n            except subprocess.CalledProcessError as e:\n                # Pyright returned an error\n                output = e.stdout.decode(\"utf-8\")\n                error_lines = cls._extract_error_lines(output)\n                return TypeCheckResult(\n                    message=f\"Type check failed. Pyright reported errors on lines: {error_lines}\",\n                    passed=False,\n                )\n\n        # Pyright did not return an error\n        return TypeCheckResult(message=\"Type check passed\", passed=True)\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Initialize the result object with default values\n        result = TypeCheckResult(\n            passed=False,\n            message=\"Type check failed\",\n            expected_errors=[],\n            actual_errors=[],\n        )\n\n        # Combine the user code and test code\n        combined_code = user_code + \"\\n\" + test_code\n\n        # Write the combined code to a temporary file\n        with tempfile.NamedTemporaryFile(\n            suffix=\".py\", delete=False\n        ) as temp_file:\n            temp_file.write(combined_code.encode())\n            temp_file.flush()\n\n            # Run Pyright on the temporary file\n            pyright_output = subprocess.run(\n                [\"pyright\", temp_file.name],\n                capture_output=True,\n                text=True,\n            )\n\n            # Extract the expected errors from the test code\n            expected_errors = cls._extract_expected_errors(test_code)\n\n            # Check if the type check passed\n            if pyright_output.returncode == 0:\n                result.passed = True\n                result.message = \"Type check passed\"\n            else:\n                # Extract the actual errors from the Pyright output\n                actual_errors = cls._extract_actual_errors(\n                    pyright_output.stdout\n                )\n\n                # Check if the actual errors match the expected errors\n                if actual_errors == expected_errors:\n                    result.passed = True\n                    result.message = \"Type check passed\"\n                else:\n                    result.message = \"Type check failed\"\n\n            # Store the actual and expected errors in the result object\n            result.actual_errors = actual_errors\n            result.expected_errors = expected_errors\n\n        # Delete the temporary file\n        os.remove(temp_file.name)\n\n        return result\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary directory to store the code files\n        with tempfile.TemporaryDirectory() as tmpdir:\n            # Create a file for the user code\n            user_code_file = os.path.join(tmpdir, \"user_code.py\")\n            with open(user_code_file, \"w\") as f:\n                f.write(user_code)\n\n            # Create a file for the test code\n            test_code_file = os.path.join(tmpdir, \"test_code.py\")\n            with open(test_code_file, \"w\") as f:\n                f.write(test_code)\n\n            # Run the type check using Pyright\n            result = subprocess.run(\n                [\n                    \"pyright\",\n                    \"--project\",\n                    tmpdir,\n                    \"--outputjson\",\n                    \"--pythonversion\",\n                    \"3.10\",\n                ],\n                capture_output=True,\n            )\n\n            # Parse the Pyright output\n            pyright_output = json.loads(result.stdout.decode())\n\n            # Initialize variables to track the number of type errors and the expected line numbers\n            num_type_errors = 0\n            expected_line_numbers = set()\n\n            # Iterate over the Pyright output and extract the expected line numbers\n            for diagnostic in pyright_output[\"generalDiagnostics\"]:\n                if diagnostic[\"severity\"] == \"error\":\n                    # Check if the diagnostic message contains the expected text\n                    if (\n                        \"Expected type\" in diagnostic[\"message\"]\n                        or \"Expected type(s)\" in diagnostic[\"message\"]\n                    ):\n                        expected_line_numbers.add(diagnostic[\"range\"][\"start\"][\"line\"])\n                        num_type_errors += 1\n\n            # Check if the number of type errors matches the expected number\n            if num_type_errors != len(expected_line_numbers):\n                # If the number of type errors does not match, return a failure result\n                return TypeCheckResult(\n                    passed=False,\n                    message=\"The number of type errors does not match the expected number.\",\n                )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        combined_code = user_code + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as temp_file:\n            temp_file.write(combined_code.encode())\n            temp_file.flush()\n\n            # Run Pyright on the combined code\n            result = subprocess.run(\n                [\"pyright\", temp_file.name], capture_output=True, text=True\n            )\n\n            # Remove the temporary file\n            os.unlink(temp_file.name)\n\n            # Check if there were any type errors\n            if \"error\" in result.stdout:\n                # Split the output into lines\n                lines = result.stdout.split(\"\\n\")\n\n                # Find the line numbers with expected type errors\n                expected_errors = set()\n                for line in lines:\n                    if \"error:\" in line:\n                        line_number = int(line.split(\":\")[1].split()[0])\n                        expected_errors.add(line_number)\n\n                # Check if the expected type errors were found\n                if expected_errors:\n                    # Create the result object with a message indicating the expected type errors\n                    result = TypeCheckResult(\n                        True,\n                        f\"Type check passed, but expected type errors on lines: {expected_errors}\",\n                    )\n                else:\n                    # Create the result object with a message indicating that the type check passed\n                    result = TypeCheckResult(\n                        True, \"Type check passed, but no expected type errors were found.\"\n                    )\n            else:\n                # Create the result object with a message indicating that the type check passed\n                result = TypeCheckResult(True, \"Type check passed.\")\n\n        return result\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        code = user_code + \"\\n\" + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n            f.write(code)\n            f.flush()\n            file_path = f.name\n\n        # Run Pyright on the combined code\n        try:\n            result = subprocess.run(\n                [\"pyright\", file_path],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                check=True,\n            )\n        except subprocess.CalledProcessError as e:\n            # If Pyright fails, return a result indicating failure and the error message\n            return TypeCheckResult(\n                result=False, message=e.stderr.strip()\n            )\n\n        # If Pyright succeeds, check if there are expected type errors\n        expected_errors = cls._find_expected_type_errors(test_code)\n        if expected_errors:\n            # If there are expected type errors, check if they were reported by Pyright\n            for line_number in expected_errors:\n                if f\"{file_path}:{line_number}\" not in result.stdout:\n                    # If a type error was expected but not reported, return a result indicating failure and the expected line number\n                    return TypeCheckResult(\n                        result=False,\n                        message=f\"Expected type error on line {line_number}\",\n                    )\n\n        # If all expected type errors were reported, return a result indicating success\n        return TypeCheckResult(result=True, message=\"\")\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\n        \"Either `path` or `modelfile` must be specified\"\n      )\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    data = {\n      \"model\": model,\n      \"modelfile\": modelfile,\n    }\n\n    if stream:\n      return self._stream_request(\n        \"create\",\n        data=data,\n      )\n    else:\n      return await self._request(\n        \"create\",\n        data=data,\n      )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError('Either `path` or `modelfile` must be specified')\n\n    if path is not None:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return await self._request('POST', f'{self._url}/{model}', modelfile, stream=stream)"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError('either path or modelfile must be specified')\n\n    if path is not None:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    if stream:\n      return self._stream_response(\n        await self._post(\n          f'{self.url}/models/{model}',\n          data=modelfile,\n          headers={'Content-Type': 'application/octet-stream'},\n        ),\n      )\n    else:\n      return await self._post(\n        f'{self.url}/models/{model}',\n        data=modelfile,\n        headers={'Content-Type': 'application/octet-stream'},\n      )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError(\"Neither path nor modelfile provided\")\n\n    if stream:\n      return self.stream(\n        method=\"POST\",\n        endpoint=f\"{self.endpoint}/{model}\",\n        data=modelfile,\n      )\n\n    return await self.request(\n      method=\"POST\",\n      endpoint=f\"{self.endpoint}/{model}\",\n      data=modelfile,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n    elif modelfile is None:\n      raise RequestError(\n        'Either `path` or `modelfile` must be specified for the request.'\n      )\n\n    headers = {\n      'Content-Type': 'application/octet-stream',\n      'Accept': 'application/octet-stream',\n    }\n\n    async for response in self._stream_request(\n      method='POST',\n      path=f'/models/{model}',\n      headers=headers,\n      data=modelfile,\n      stream=stream,\n    ):\n      yield response\n\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError(\"either path or modelfile must be specified\")\n\n    if stream:\n      async for response in self.stream_request(\n        method=\"POST\",\n        path=f\"{self.url}/{model}\",\n        data=modelfile,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n      ):\n        yield response\n    else:\n      return await self.request(\n        method=\"POST\",\n        path=f\"{self.url}/{model}\",\n        data=modelfile,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n      )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n    if modelfile is None:\n      raise RequestError('either path or modelfile must be specified')\n    return await self._request(\n      method='POST',\n      path=f'/models/{model}',\n      data=modelfile,\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be specified\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      return self._stream_response(\n        self.post(\n          f\"/v1/models/{model}/versions\",\n          data=modelfile,\n          headers={\"Content-Type\": \"application/octet-stream\"},\n        )\n      )\n    else:\n      return await self.post(\n        f\"/v1/models/{model}/versions\",\n        data=modelfile,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n      )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n    if modelfile is None:\n      raise RequestError('Either path or modelfile must be specified')\n    return await self._request(\n      'POST',\n      '/models/{}'.format(model),\n      data=modelfile,\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    if not modelfile:\n      raise RequestError(\n        'You must provide either a path or modelfile'\n      )\n\n    if stream:\n      return self._stream(\n        method='POST',\n        path=f'/{model}',\n        body=modelfile,\n      )\n\n    return await self._request(\n      method='POST',\n      path=f'/{model}',\n      body=modelfile,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError(\"Either path or modelfile must be specified\")\n\n    url = f\"{self.url}/{model}\"\n    headers = {\"Content-Type\": \"application/octet-stream\"}\n    async with self.session.post(url, headers=headers, data=modelfile) as response:\n      if response.status != 200:\n        raise ResponseError(response.status, await response.text())\n\n      if stream:\n        async for line in response.content:\n          yield json.loads(line)\n      else:\n        return await response.json()\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError('either path or modelfile must be specified')\n\n    if path is not None:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    if stream:\n      return self._stream_request(\n        method='POST',\n        path=f'{self.path}/{model}',\n        body=modelfile,\n        headers={'Content-Type': 'application/octet-stream'},\n      )\n\n    return await self._request(\n      method='POST',\n      path=f'{self.path}/{model}',\n      body=modelfile,\n      headers={'Content-Type': 'application/octet-stream'},\n    )\n\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError(\n        \"Either `path` or `modelfile` must be specified for the request\"\n      )\n\n    data = {\n      \"model\": model,\n      \"modelfile\": modelfile,\n    }\n\n    if stream:\n      return self._stream_request(\n        \"post\",\n        \"/create\",\n        data=data,\n        headers={\n          \"Content-Type\": \"application/octet-stream\",\n        },\n      )\n\n    return await self._request(\n      \"post\",\n      \"/create\",\n      data=data,\n      headers={\n        \"Content-Type\": \"application/octet-stream\",\n      },\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided\")\n\n    headers = {\"Content-Type\": \"application/octet-stream\"}\n    params = {\"model\": model}\n\n    async for chunk in self.post(\n      \"/create\",\n      params=params,\n      headers=headers,\n      data=modelfile,\n      stream=stream,\n    ):\n      yield chunk\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      if not os.path.exists(path):\n        raise RequestError(f\"File {path} does not exist\")\n\n      if os.path.isdir(path):\n        raise RequestError(f\"Path {path} is a directory\")\n\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided\")\n\n    request = {\n      \"action\": \"create\",\n      \"model\": model,\n      \"modelfile\": modelfile,\n    }\n\n    if stream:\n      async for response in self._stream_request(request):\n        yield response\n    else:\n      return await self._request(request)\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be specified\")\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n    params = {\"model\": model}\n    headers = {\"Content-Type\": \"application/octet-stream\"}\n    response = await self._request(\n      \"POST\",\n      \"create\",\n      params=params,\n      headers=headers,\n      data=modelfile,\n    )\n    if stream:\n      return self._stream_response(response)\n    return response.json()\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError('either path or modelfile must be specified')\n\n    params = {\n      'model': model,\n      'modelfile': modelfile,\n    }\n\n    if stream:\n      return self._stream_request(self.create_url, params)\n\n    return await self._request(self.create_url, params)\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n    if modelfile is None:\n      raise RequestError(\n        'Either `path` or `modelfile` must be specified'\n      )\n    headers = {'Content-Type': 'application/octet-stream'}\n    params = {'model': model}\n    async for response in self.post(\n      '/models',\n      data=modelfile,\n      headers=headers,\n      params=params,\n      stream=stream,\n    ):\n      yield response\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be specified\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    params = {\"model\": model, \"modelfile\": modelfile}\n    if stream:\n      return self._stream_request(\n        \"create\",\n        params,\n        self.create_parser,\n        self.create_serializer,\n      )\n    else:\n      return await self._request(\n        \"create\",\n        params,\n        self.create_parser,\n        self.create_serializer,\n      )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, 'rb') as f:\n        modeldata = f.read()\n    elif modelfile is not None:\n      modeldata = modelfile.encode('utf-8')\n    else:\n      raise RequestError('either path or modelfile must be provided')\n\n    headers = {\n      'Content-Type': 'application/octet-stream',\n      'Content-Length': str(len(modeldata)),\n    }\n\n    response = await self._request(\n      'POST',\n      f'/{model}',\n      headers=headers,\n      data=modeldata,\n    )\n\n    if stream:\n      return self._stream_response(response, self.decode_response)\n\n    return self.decode_response(response)\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module_printer(fn)\n    else:\n        return aot_function_printer(fn)"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return compile_module(fn, forward_compiler, backward_compiler)\n    else:\n        return compile_function(fn, forward_compiler, backward_compiler)"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return compile_module(fn, forward_compiler, backward_compiler)\n    else:\n        return compile_function(fn, forward_compiler, backward_compiler)"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return compile_module(fn, forward_compiler=forward_compiler, backward_compiler=backward_compiler)\n    else:\n        return compile_function(fn, forward_compiler=forward_compiler, backward_compiler=backward_compiler)\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        # Compile the module using the forward and backward compilers specific for modules\n        compiled_fn = torch.compile(fn, mode=\"reduce-overhead\")\n    else:\n        # Compile the function using the forward and backward compilers specific for functions\n        compiled_fn = torch.compile(fn, mode=\"reduce-overhead\")\n\n    return compiled_fn"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        # Compile the module using a forward and backward compiler specific for modules\n        compiled_fn = compile_module(fn)\n    else:\n        # Compile the function using a forward and backward compiler specific for functions\n        compiled_fn = compile_function(fn)\n\n    return compiled_fn\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if hasattr(fn, 'forward'):\n        return _compile_module(fn)\n    else:\n        return _compile_function(fn)\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    def inner(*args, **kwargs):\n        if hasattr(fn, \"forward\"):\n            return fn.forward(*args, **kwargs)\n        else:\n            return fn(*args, **kwargs)\n\n    return inner\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return compile_module(fn, forward_compiler, backward_compiler)\n    else:\n        return compile_function(fn, forward_compiler, backward_compiler)\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return compile_module(fn)\n    else:\n        return compile_function(fn)\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return compile_module(fn)\n    else:\n        return compile_function(fn)\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if hasattr(fn, \"forward\"):\n        return aot_module_printer(fn)\n    else:\n        return aot_function_printer(fn)\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    import torch\n    import torch._dynamo\n\n    def aot_printer_wrapper(x):\n        if isinstance(fn, torch.nn.Module):\n            # Compile the module using a forward and backward compiler specific for modules\n            return torch._dynamo.optimize(fn, x)\n        else:\n            # Compile the function using a forward and backward compiler specific for functions\n            return torch._dynamo.optimize(fn, x)\n\n    return aot_printer_wrapper"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module_simplifier(fn)\n    else:\n        return aot_function_simplifier(fn)\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        # Compile the module using a forward and backward compiler specific for modules\n        compiled_fn = torch.compile(fn, mode=\"reduce-overhead\")\n        return compiled_fn\n    else:\n        # Compile the function using a forward and backward compiler specific for functions\n        compiled_fn = torch.compile(fn, mode=\"reduce-overhead\")\n        return compiled_fn\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return torch.compile(fn, mode=\"aot_eager\", backend=\"aot_inductor\", fullgraph=True)\n    else:\n        return torch.compile(fn, mode=\"aot_eager\", backend=\"aot_inductor\", fullgraph=True)\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    # Importing the necessary modules\n    import torch\n    import torch._dynamo\n    import torch._dynamo.optimizations as optimizations\n    import torch._inductor.compile_fx as compile_fx\n    import torch._inductor.config as config\n    import torch._inductor.utils as utils\n    import torch.nn as nn\n    import torch.utils._pytree as pytree\n    import torch.fx as fx\n    import torch.fx.experimental.fx_acc.acc_ops as acc_ops\n    import torch.fx.experimental.fx_acc.acc_tracer as acc_tracer\n    import torch.fx.experimental.fx_acc.acc_utils as acc_utils\n    import torch.fx.experimental.fx_acc.acc_tracer.acc_ops as acc_tracer_ops\n    import torch.fx.experimental.fx_acc.acc_tracer.acc_tracing as acc_tracing\n    import torch.fx.experimental.fx_acc.acc_tracer.acc_utils as acc_tracer_utils\n    import torch.fx.experimental.fx_acc.acc_tracer.acc_tracer as acc_tracer_tracer\n    import torch.fx.experimental.fx_acc.acc_tracer.acc_tracing as acc_tracer_tracing\n    import torch.fx.experimental.fx_acc.acc_tracer.acc_tracing_mode as acc_tracer_tracing_mode\n    import torch.fx.experimental.fx_acc.acc_tracer.acc_tracing_utils as acc_tracer_tracing_utils\n    import torch.fx.experimental.fx_acc.acc_tracer.acc_tracing_utils as acc_tracing_utils\n    import torch.fx.experimental.fx_acc.acc_tracer.acc_tracing_mode as acc_tracing_mode\n    import torch.fx.experimental.fx_acc.acc_tracer.acc_tracing as acc"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    def _print_fn(*args, **kwargs):\n        if hasattr(fn, \"forward\"):\n            print(\"Compiling module\")\n            # Compile the module using a forward and backward compiler specific for modules\n            compiled_module = compile_module(fn)\n            return compiled_module(*args, **kwargs)\n        else:\n            print(\"Compiling function\")\n            # Compile the function using a forward and backward compiler specific for functions\n            compiled_function = compile_function(fn)\n            return compiled_function(*args, **kwargs)\n\n    return _print_fn\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module_printer(fn)\n    else:\n        return aot_function_printer(fn)\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return torch.compile(fn, backend=\"aot_eager\", mode=\"reduce-overhead\")\n    else:\n        return torch.compile(fn, backend=\"aot_eager\", mode=\"reduce-overhead\")\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary.csv file to find the best pipeline configuration\n    summary_path = os.path.join(trial_path, 'summary.csv')\n    summary_df = pd.read_csv(summary_path)\n    best_row = summary_df.loc[summary_df['score'].idxmax()]\n    best_config = best_row.to_dict()\n\n    # Extract the pipeline configuration from the best row\n    pipeline_config = best_config['pipeline']\n    pipeline_config = yaml.safe_load(pipeline_config)\n\n    # Save the extracted pipeline configuration to a YAML file if an output path is provided\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(pipeline_config, f)\n\n    return pipeline_config\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary file\n    summary_file = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_file)\n\n    # Find the row with the best validation loss\n    best_row = summary_df.loc[summary_df[\"validation_loss\"].idxmin()]\n\n    # Extract the configuration from the summary file\n    config_file = os.path.join(trial_path, \"config.yaml\")\n    with open(config_file, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    # Extract the best configuration\n    best_config = {}\n    for key, value in config.items():\n        if isinstance(value, dict):\n            best_config[key] = {}\n            for subkey, subvalue in value.items():\n                best_config[key][subkey] = subvalue[best_row[subkey]]\n        else:\n            best_config[key] = value[best_row[key]]\n\n    # Save the best configuration to a YAML file if output_path is specified\n    if output_path is not None:\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            raise ValueError(\"The output file must have a .yaml or .yml extension.\")\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config, f)\n\n    return best_config\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Load the summary.csv file from the trial directory\n    summary_path = os.path.join(trial_path, 'summary.csv')\n    summary_df = pd.read_csv(summary_path)\n\n    # Find the index of the row with the highest 'score'\n    best_index = summary_df['score'].idxmax()\n\n    # Extract the corresponding pipeline configuration from the summary.csv file\n    best_config = summary_df.loc[best_index].to_dict()\n\n    # Remove the 'score' and 'duration' keys from the dictionary\n    best_config.pop('score', None)\n    best_config.pop('duration', None)\n\n    # Load the configuration.yaml file from the trial directory\n    config_path = os.path.join(trial_path, 'configuration.yaml')\n    with open(config_path, 'r') as f:\n        config = yaml.safe_load(f)\n\n    # Update the configuration dictionary with the best pipeline configuration\n    config.update(best_config)\n\n    # Save the extracted pipeline configuration to a YAML file if output_path is provided\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(config, f)\n\n    return config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Check if the trial_path is a valid directory\n    if not os.path.isdir(trial_path):\n        raise ValueError(f\"Invalid trial directory path: {trial_path}\")\n\n    # Check if the summary.csv file exists in the trial directory\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    if not os.path.isfile(summary_path):\n        raise FileNotFoundError(f\"Summary file not found in the trial directory: {summary_path}\")\n\n    # Read the summary.csv file into a pandas DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Find the row with the minimum validation loss\n    min_loss_row = summary_df.loc[summary_df[\"validation_loss\"].idxmin()]\n\n    # Extract the pipeline configuration from the row\n    config = min_loss_row.to_dict()\n\n    # Check if the output_path is provided and ends with .yaml or .yml\n    if output_path is not None and not (output_path.endswith(\".yaml\") or output_path.endswith(\".yml\")):\n        raise ValueError(\"Invalid output file path. The file extension must be .yaml or .yml.\")\n\n    # Save the extracted pipeline configuration to a YAML file if output_path is provided\n    if output_path is not None:\n        with open(output_path, \"w\") as f:\n            yaml.dump(config, f)\n\n    return config\n\n\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary.csv file\n    summary_path = os.path.join(trial_path, 'summary.csv')\n    summary_df = pd.read_csv(summary_path)\n\n    # Find the row with the highest value in the 'value' column\n    best_row = summary_df.loc[summary_df['value'].idxmax()]\n\n    # Extract the pipeline configuration from the 'pipeline_config' column\n    best_config = best_row['pipeline_config']\n\n    # Convert the pipeline configuration string to a dictionary\n    best_config_dict = ast.literal_eval(best_config)\n\n    # Save the pipeline configuration to a YAML file if output_path is specified\n    if output_path is not None:\n        # Check if the output path has a valid file extension\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"Invalid file extension. The output path must have a '.yaml' or '.yml' extension.\")\n\n        # Save the pipeline configuration to a YAML file\n        with open(output_path, 'w') as file:\n            yaml.dump(best_config_dict, file)\n\n    return best_config_dict\n\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary file\n    summary_df = pd.read_csv(os.path.join(trial_path, \"summary.csv\"))\n\n    # Find the row with the best score\n    best_row = summary_df.loc[summary_df[\"score\"].idxmax()]\n\n    # Extract the best configuration\n    best_config = {}\n    for key, value in best_row.items():\n        if key in [\"score\", \"duration\"]:\n            continue\n        best_config[key] = value\n\n    # Save the configuration to a YAML file if output_path is provided\n    if output_path:\n        # Check if the output path is a valid file path\n        if not os.path.isfile(output_path):\n            raise ValueError(\"Invalid output path. Please provide a valid file path.\")\n\n        # Check if the file extension is .yaml or .yml\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            raise ValueError(\"Invalid output file extension. Please provide a file path with .yaml or .yml extension.\")\n\n        # Save the configuration to the file\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Check if the trial path exists\n    if not os.path.exists(trial_path):\n        raise ValueError(f\"Trial path '{trial_path}' does not exist.\")\n\n    # Check if the summary.csv file exists in the trial directory\n    summary_file = os.path.join(trial_path, 'summary.csv')\n    if not os.path.exists(summary_file):\n        raise ValueError(f\"Summary file '{summary_file}' does not exist in the trial directory.\")\n\n    # Read the summary.csv file\n    summary_df = pd.read_csv(summary_file)\n\n    # Check if the summary.csv file is empty\n    if summary_df.empty:\n        raise ValueError(f\"Summary file '{summary_file}' is empty.\")\n\n    # Find the row with the best evaluation result\n    best_row = summary_df.iloc[summary_df['evaluation_result'].argmax()]\n\n    # Extract the pipeline configuration from the summary.csv file\n    config_file = os.path.join(trial_path, 'config.yaml')\n    if not os.path.exists(config_file):\n        raise ValueError(f\"Configuration file '{config_file}' does not exist in the trial directory.\")\n    with open(config_file, 'r') as f:\n        config = yaml.safe_load(f)\n\n    # Extract the best pipeline configuration\n    best_config = {}\n    for key, value in config.items():\n        if isinstance(value, dict):\n            best_config[key] = value[best_row[key]]\n        else:\n            best_config[key] = value\n\n    # Save the best pipeline configuration to a YAML file if output_path is specified\n    if output_path is not None:\n        # Check if the output path has a valid file extension\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(f\"Output path '{output_path}' must have a '.yaml' or '."}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary.csv file\n    summary_df = pd.read_csv(os.path.join(trial_path, \"summary.csv\"))\n\n    # Find the row with the highest score\n    best_row = summary_df.loc[summary_df[\"score\"].idxmax()]\n\n    # Extract the best pipeline configuration from the row\n    best_config = {\n        \"pipeline_name\": best_row[\"pipeline_name\"],\n        \"model_name\": best_row[\"model_name\"],\n        \"model_params\": best_row[\"model_params\"],\n        \"preprocessor_name\": best_row[\"preprocessor_name\"],\n        \"preprocessor_params\": best_row[\"preprocessor_params\"],\n        \"scaler_name\": best_row[\"scaler_name\"],\n        \"scaler_params\": best_row[\"scaler_params\"],\n        \"feature_selector_name\": best_row[\"feature_selector_name\"],\n        \"feature_selector_params\": best_row[\"feature_selector_params\"],\n        \"imputer_name\": best_row[\"imputer_name\"],\n        \"imputer_params\": best_row[\"imputer_params\"],\n        \"encoder_name\": best_row[\"encoder_name\"],\n        \"encoder_params\": best_row[\"encoder_params\"],\n        \"sampler_name\": best_row[\"sampler_name\"],\n        \"sampler_params\": best_row[\"sampler_params\"],\n        \"score\": best_row[\"score\"],\n    }\n\n    # Read the configuration.yaml file\n    config_path = os.path.join(trial_path, \"configuration.yaml\")\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    # Update the best_config dictionary with the configuration from the configuration.yaml file\n    best_config.update(config)\n\n    # Save the best_config dictionary to a YAML file if output_path is specified\n    if output_path:\n        if not output_path.endswith(\".yaml"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary file\n    summary_df = pd.read_csv(os.path.join(trial_path, \"summary.csv\"))\n\n    # Find the row with the highest score\n    best_row = summary_df.iloc[summary_df[\"score\"].idxmax()]\n\n    # Extract the pipeline configuration from the best row\n    best_config = {\n        \"pipeline\": best_row[\"pipeline\"],\n        \"params\": best_row[\"params\"],\n        \"score\": best_row[\"score\"],\n        \"duration\": best_row[\"duration\"],\n    }\n\n    # Save the pipeline configuration to a YAML file if output_path is specified\n    if output_path is not None:\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config, f)\n\n    return best_config\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary file to get the best pipeline configuration\n    summary = pd.read_csv(os.path.join(trial_path, \"summary.csv\"))\n    best_pipeline_config = summary.iloc[summary[\"score\"].idxmax()][\"pipeline_config\"]\n\n    # Read the configuration file to get the pipeline configuration\n    config = read_config(os.path.join(trial_path, \"config.yaml\"))\n\n    # Extract the optimal pipeline configuration from the configuration file\n    best_config = {}\n    for key, value in config.items():\n        if key in best_pipeline_config:\n            best_config[key] = value\n\n    # Save the optimal pipeline configuration to a YAML file if an output path is provided\n    if output_path is not None:\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config, f)\n\n    return best_config\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary file\n    summary_file = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_file)\n\n    # Find the row with the minimum error\n    min_error_row = summary_df.loc[summary_df[\"error\"].idxmin()]\n\n    # Extract the pipeline configuration from the config.yaml file\n    config_file = os.path.join(trial_path, \"config.yaml\")\n    with open(config_file, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    # Construct the optimal pipeline configuration\n    optimal_config = {\n        \"pipeline\": config[\"pipeline\"],\n        \"input_data\": config[\"input_data\"],\n        \"output_data\": config[\"output_data\"],\n        \"hyperparameters\": min_error_row.to_dict()\n    }\n\n    # Save the optimal pipeline configuration to a YAML file if output_path is provided\n    if output_path is not None:\n        # Check if the output_path has a valid file extension\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            raise ValueError(\"Invalid output file extension. Please provide a file with a .yaml or .yml extension.\")\n        with open(output_path, \"w\") as f:\n            yaml.dump(optimal_config, f)\n\n    return optimal_config\n\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Check if the input file exists\n    if not os.path.exists(trial_path):\n        raise ValueError(f\"The input file {trial_path} does not exist.\")\n\n    # Check if the output file extension is valid\n    if output_path is not None and not (output_path.endswith(\".yaml\") or output_path.endswith(\".yml\")):\n        raise ValueError(\"The output file must have a .yaml or .yml extension.\")\n\n    # Read the summary.csv file\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n\n    # Find the row with the minimum value in the \"value\" column\n    min_row = summary_df.loc[summary_df[\"value\"].idxmin()]\n\n    # Extract the pipeline configuration from the summary.csv file\n    config = {\n        \"pipeline\": min_row[\"pipeline\"],\n        \"dataset\": min_row[\"dataset\"],\n        \"optimizer\": min_row[\"optimizer\"],\n        \"batch_size\": min_row[\"batch_size\"],\n        \"epochs\": min_row[\"epochs\"],\n        \"learning_rate\": min_row[\"learning_rate\"],\n        \"loss\": min_row[\"loss\"],\n        \"metrics\": min_row[\"metrics\"],\n        \"callbacks\": min_row[\"callbacks\"],\n    }\n\n    # Save the configuration to a YAML file if output_path is specified\n    if output_path is not None:\n        with open(output_path, \"w\") as f:\n            yaml.dump(config, f)\n\n    return config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Check if the trial path exists\n    if not os.path.exists(trial_path):\n        raise ValueError(f\"The trial path '{trial_path}' does not exist.\")\n\n    # Check if the trial path is a directory\n    if not os.path.isdir(trial_path):\n        raise ValueError(f\"The trial path '{trial_path}' is not a directory.\")\n\n    # Check if the output path is a valid file path if provided\n    if output_path is not None:\n        if not isinstance(output_path, str):\n            raise TypeError(f\"The output path must be a string.\")\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            raise ValueError(f\"The output path must have a .yaml or .yml extension.\")\n\n    # Read the summary file\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    if not os.path.exists(summary_path):\n        raise ValueError(f\"The summary file '{summary_path}' does not exist.\")\n    summary = pd.read_csv(summary_path)\n\n    # Find the index of the row with the best evaluation result\n    best_index = summary[\"evaluation\"].idxmax()\n\n    # Read the configuration file for the best pipeline\n    config_path = os.path.join(trial_path, f\"config_{best_index}.yaml\")\n    if not os.path.exists(config_path):\n        raise ValueError(f\"The configuration file '{config_path}' does not exist.\")\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    # Extract the optimal pipeline configuration\n    optimal_config = {}\n    for step in config[\"pipeline\"]:\n        for name, params in step.items():\n            optimal_config[name] = params\n\n    # Save the optimal pipeline configuration to a file if provided\n    if output_path is not None:\n        with open(output_path, \""}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Check if the trial path is a valid directory\n    if not os.path.isdir(trial_path):\n        raise ValueError(f\"Invalid trial path: {trial_path}\")\n\n    # Check if the output path is provided and has a valid extension\n    if output_path is not None and not output_path.endswith((\".yaml\", \".yml\")):\n        raise ValueError(f\"Invalid output path: {output_path}. The file extension must be .yaml or .yml.\")\n\n    # Construct the path to the summary file\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n\n    # Check if the summary file exists\n    if not os.path.isfile(summary_path):\n        raise ValueError(f\"Summary file not found in the trial directory: {trial_path}\")\n\n    # Read the summary file into a DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Find the row with the best score\n    best_row = summary_df.loc[summary_df[\"score\"].idxmax()]\n\n    # Construct the path to the configuration file\n    config_path = os.path.join(trial_path, \"config.yaml\")\n\n    # Check if the configuration file exists\n    if not os.path.isfile(config_path):\n        raise ValueError(f\"Configuration file not found in the trial directory: {trial_path}\")\n\n    # Read the configuration file into a dictionary\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    # Extract the best pipeline configuration from the configuration dictionary\n    best_config = {\n        \"pipeline\": config[\"pipeline\"],\n        \"hyperparameters\": {\n            \"data_preprocessor\": config[\"hyperparameters\"][\"data_preprocessor\"],\n            \"classifier\": config[\"hyperparameters\"][\"classifier\"],\n        },\n    }\n\n    # Save the best pipeline configuration to a YAML file if an output path is provided\n    if output_path is not None:\n       "}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_path = os.path.join(trial_path, \"summary.csv\")\n    config_path = os.path.join(trial_path, \"config.yaml\")\n\n    if not os.path.exists(summary_path):\n        raise ValueError(f\"Summary file not found at {summary_path}\")\n\n    if not os.path.exists(config_path):\n        raise ValueError(f\"Config file not found at {config_path}\")\n\n    summary_df = pd.read_csv(summary_path)\n    best_config = summary_df.loc[summary_df[\"score\"].idxmax()].to_dict()\n\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    for key, value in best_config.items():\n        if key in config:\n            config[key] = value\n\n    if output_path:\n        if not output_path.endswith((\".yaml\", \".yml\")):\n            raise ValueError(f\"Output path must have a .yaml or .yml extension\")\n        with open(output_path, \"w\") as f:\n            yaml.dump(config, f)\n\n    return config\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Load the summary file\n    summary_file = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_file)\n\n    # Find the index of the row with the highest score\n    best_row_index = summary_df['score'].idxmax()\n\n    # Get the corresponding row from the summary dataframe\n    best_row = summary_df.iloc[best_row_index]\n\n    # Get the pipeline configuration from the summary row\n    pipeline_config = best_row['pipeline_config']\n\n    # Load the configuration file\n    config_file = os.path.join(trial_path, \"config.yaml\")\n    with open(config_file, 'r') as f:\n        config = yaml.safe_load(f)\n\n    # Extract the best pipeline configuration\n    best_config = config[pipeline_config]\n\n    # Save the best configuration to a file if an output path is provided\n    if output_path is not None:\n        # Check if the output path has a valid file extension\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"Invalid file extension for output path. Please provide a file path with a '.yaml' or '.yml' extension.\")\n\n        # Save the best configuration to the output path\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Load the summary.csv file\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n\n    # Find the row with the highest \"score\" value\n    best_row = summary_df.loc[summary_df[\"score\"].idxmax()]\n\n    # Extract the pipeline configuration from the summary.csv file\n    best_config = {\n        \"pipeline\": best_row[\"pipeline\"],\n        \"scaler\": best_row[\"scaler\"],\n        \"estimator\": best_row[\"estimator\"],\n        \"n_neighbors\": best_row[\"n_neighbors\"],\n        \"weights\": best_row[\"weights\"],\n        \"p\": best_row[\"p\"],\n        \"C\": best_row[\"C\"],\n        \"epsilon\": best_row[\"epsilon\"],\n        \"gamma\": best_row[\"gamma\"],\n        \"coef0\": best_row[\"coef0\"],\n        \"degree\": best_row[\"degree\"],\n        \"kernel\": best_row[\"kernel\"],\n        \"max_depth\": best_row[\"max_depth\"],\n        \"min_samples_split\": best_row[\"min_samples_split\"],\n        \"min_samples_leaf\": best_row[\"min_samples_leaf\"],\n        \"bootstrap\": best_row[\"bootstrap\"],\n        \"max_features\": best_row[\"max_features\"],\n        \"n_estimators\": best_row[\"n_estimators\"],\n        \"criterion\": best_row[\"criterion\"],\n    }\n\n    # Save the best configuration to a YAML file if output_path is specified\n    if output_path is not None:\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            raise ValueError(\n                \"The output_path must have a .yaml or .yml extension.\"\n            )\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary.csv file to get the best pipeline configuration\n    summary_df = pd.read_csv(os.path.join(trial_path, \"summary.csv\"))\n    best_config = summary_df.loc[summary_df[\"evaluation_metric\"].idxmax()]\n\n    # Extract the pipeline configuration from the summary dataframe\n    pipeline_config = {\n        \"pipeline_name\": best_config[\"pipeline_name\"],\n        \"model_name\": best_config[\"model_name\"],\n        \"model_params\": ast.literal_eval(best_config[\"model_params\"]),\n        \"preprocessing_steps\": ast.literal_eval(best_config[\"preprocessing_steps\"]),\n        \"feature_selection_method\": best_config[\"feature_selection_method\"],\n        \"feature_selection_params\": ast.literal_eval(best_config[\"feature_selection_params\"]),\n        \"scaling_method\": best_config[\"scaling_method\"],\n        \"scaling_params\": ast.literal_eval(best_config[\"scaling_params\"]),\n        \"evaluation_metric\": best_config[\"evaluation_metric\"],\n        \"evaluation_metric_value\": best_config[\"evaluation_metric_value\"],\n    }\n\n    # Save the pipeline configuration to a YAML file if output_path is specified\n    if output_path is not None:\n        # Check if the output path is a valid file path\n        if not os.path.isfile(output_path):\n            raise ValueError(\"Invalid output path. Please provide a valid file path.\")\n\n        # Check if the output file extension is .yaml or .yml\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            raise ValueError(\"Invalid output file extension. Please use .yaml or .yml.\")\n\n        # Write the pipeline configuration to the output file\n        with open(output_path, \"w\") as f:\n            yaml.dump(pipeline_config, f)\n\n    return pipeline_config\n\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary.csv file\n    summary_df = pd.read_csv(os.path.join(trial_path, \"summary.csv\"))\n\n    # Find the index of the row with the lowest validation loss\n    best_row_idx = summary_df[\"validation_loss\"].idxmin()\n\n    # Read the configuration file for the best row\n    config_file = os.path.join(trial_path, \"config.yaml\")\n    with open(config_file, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    # Extract the optimal pipeline configuration\n    best_config = {\n        \"pipeline_name\": config[\"pipeline_name\"],\n        \"pipeline_description\": config[\"pipeline_description\"],\n        \"data_source\": config[\"data_source\"],\n        \"data_source_version\": config[\"data_source_version\"],\n        \"data_source_date\": config[\"data_source_date\"],\n        \"data_source_location\": config[\"data_source_location\"],\n        \"data_source_type\": config[\"data_source_type\"],\n        \"data_source_size\": config[\"data_source_size\"],\n        \"data_source_md5\": config[\"data_source_md5\"],\n        \"data_source_download_url\": config[\"data_source_download_url\"],\n        \"data_source_citation\": config[\"data_source_citation\"],\n        \"data_source_license\": config[\"data_source_license\"],\n        \"data_source_license_url\": config[\"data_source_license_url\"],\n        \"data_source_license_type\": config[\"data_source_license_type\"],\n        \"data_source_license_copyright_holder\": config[\"data_source_license_copyright_holder\"],\n        \"data_source_license_year\": config[\"data_source_license_year\"],\n        \"data_source_license_outbound_use_agreement\": config[\"data_source_license_outbound_use_agreement\"],\n        \"data_source_license_terms\": config[\"data_source_license_terms"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary.csv file to extract the best pipeline configuration\n    summary_file = os.path.join(trial_path, 'summary.csv')\n    summary_df = pd.read_csv(summary_file)\n    best_row = summary_df.loc[summary_df['score'].idxmax()]\n\n    # Construct the pipeline configuration dictionary\n    config = {\n        'model': best_row['model'],\n        'parameters': {\n            'n_estimators': int(best_row['n_estimators']),\n            'max_depth': int(best_row['max_depth']),\n            'min_samples_split': int(best_row['min_samples_split']),\n            'min_samples_leaf': int(best_row['min_samples_leaf']),\n            'max_features': best_row['max_features'],\n            'random_state': int(best_row['random_state'])\n        }\n    }\n\n    # Save the configuration to a YAML file if output_path is specified\n    if output_path:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"Invalid output file extension. Please provide a YAML file (.yaml or .yml)\")\n        with open(output_path, 'w') as file:\n            yaml.dump(config, file)\n\n    return config\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    def wrapper(*args, **kwargs):\n        nonlocal ts_compiler\n        if ts_compiler is not None:\n            ts_compiler = ts_compiler(**kwargs_)\n        else:\n            ts_compiler = torch.jit.trace\n        if isinstance(func, torch.nn.Module):\n            func = func.forward\n        with lock:\n            if func not in cache:\n                cache[func] = ts_compiler(func, *args, **kwargs)\n            return cache[func](*args, **kwargs)\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import functools\n    import threading\n    import torch\n    import torch._dynamo\n\n    cache = {}\n    lock = threading.Lock()\n\n    def wrapper(*args, **kwargs):\n        nonlocal cache\n        nonlocal lock\n        with lock:\n            key = (args, frozenset(kwargs.items()))\n            if key not in cache:\n                if isinstance(func, torch.nn.Module):\n                    func_ = func.forward\n                else:\n                    func_ = func\n                if ts_compiler is None:\n                    cache[key] = torch.compile(func_, **kwargs_)\n                else:\n                    cache[key] = ts_compiler(torch.compile(func_, **kwargs_))\n            return cache[key](*args, **kwargs)\n\n    return functools.update_wrapper(wrapper, func)"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    import functools\n    import threading\n\n    lock = threading.Lock()\n    cache = {}\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        nonlocal cache\n        with lock:\n            if func not in cache:\n                if isinstance(func, torch.nn.Module):\n                    trace_func = torch.jit.trace\n                else:\n                    trace_func = torch.jit.trace_module\n\n                if ts_compiler:\n                    kwargs_[\"_compiler\"] = ts_compiler\n\n                cache[func] = trace_func(func, *args, **kwargs_, **kwargs)\n\n            return cache[func](*args, **kwargs)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import functools\n    import threading\n    import torch\n    import torch._dynamo\n\n    lock = threading.Lock()\n    cache = {}\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        nonlocal cache\n        with lock:\n            if func not in cache:\n                if isinstance(func, torch.nn.Module):\n                    traced_module = torch.compile(func, backend=\"inductor\", fullgraph=True)\n                else:\n                    traced_module = torch.compile(func, backend=\"inductor\", fullgraph=True)\n                if ts_compiler is not None:\n                    traced_module = ts_compiler(traced_module, **kwargs_)\n                cache[func] = traced_module\n            return cache[func](*args, **kwargs)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import threading\n    import torch\n\n    lock = threading.Lock()\n    cache = {}\n\n    def wrapper(*args, **kwargs):\n        nonlocal cache\n        nonlocal lock\n\n        with lock:\n            if func not in cache:\n                if isinstance(func, torch.nn.Module):\n                    cache[func] = torch.jit.trace(func, example_inputs=args)\n                else:\n                    cache[func] = torch.jit.trace(func, example_inputs=args, **kwargs_)\n\n            if ts_compiler is not None:\n                cache[func] = ts_compiler(cache[func])\n\n            return cache[func](*args, **kwargs)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    def wrapper(*args, **kwargs):\n\n        \"\"\"\n        This function wraps the original function or module's forward method, traces it with optional compiler enhancements, and caches the result for identical future calls to reduce overhead.\n\n        Input-Output Arguments\n        :param args: Tuple. The positional arguments to be passed to the original function or module's forward method.\n        :param kwargs: Dict. The keyword arguments to be passed to the original function or module's forward method.\n        :return: Any. The result of calling the original function or module's forward method with the provided arguments.\n\n        Note: The function uses a lock to ensure thread safety when accessing or updating the cache of traced modules.\n        \"\"\"\n\n        with lock:\n            nonlocal cache\n            key = (args, tuple(kwargs.items()))\n            if key not in cache:\n                traced_func = torch.jit.trace(func, args, **kwargs_)\n                if ts_compiler is not None:\n                    traced_func = ts_compiler(traced_func, **kwargs_)\n                cache[key] = traced_func\n            return cache[key](*args, **kwargs)\n\n    lock = threading.Lock()\n    cache = {}\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    def wrapper(*args, **kwargs):\n        nonlocal kwargs_\n        if \"ts_compiler\" in kwargs_:\n            ts_compiler = kwargs_.pop(\"ts_compiler\")\n        if \"kwargs_\" in kwargs_:\n            kwargs_ = kwargs_.pop(\"kwargs_\")\n        kwargs_ = kwargs_ or {}\n        kwargs_.update(kwargs)\n        with lock:\n            if func not in cache:\n                cache[func] = {}\n            key = (args, tuple(kwargs_.items()))\n            if key not in cache[func]:\n                if isinstance(func, torch.nn.Module):\n                    traced_func = torch.jit.trace(func, *args, **kwargs_)\n                else:\n                    traced_func = torch.jit.trace(func, *args, **kwargs_)\n                if ts_compiler:\n                    traced_func = ts_compiler(traced_func)\n                cache[func][key] = traced_func\n            return cache[func][key](*args, **kwargs_)\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    def wrapper(*args, **kwargs):\n        nonlocal ts_compiler\n        if ts_compiler is None:\n            ts_compiler = torch.jit.trace\n        with lock:\n            if not isinstance(func, torch.nn.Module):\n                func = torch.jit.trace(func, *args, **kwargs)\n            else:\n                func = torch.jit.trace(func.forward, *args, **kwargs)\n            func = ts_compiler(func, *args, **kwargs)\n            func = func.call_on_forward\n        return func(*args, **kwargs)\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    def trace_and_compile(func, *, ts_compiler=None, **kwargs_):\n        with torch.inference_mode():\n            traced_module = torch.jit.trace(func, *kwargs_)\n            if ts_compiler is not None:\n                traced_module = ts_compiler(traced_module)\n        return traced_module\n\n    def wrapper(*args, **kwargs):\n        with lock:\n            key = (args, tuple(kwargs.items()))\n            if key not in cache:\n                cache[key] = trace_and_compile(func, *args, **kwargs)\n            return cache[key](*args, **kwargs)\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    def wrapper(self, *args, **kwargs):\n        # Get the arguments\n        args_ = args\n        kwargs_[\"self\"] = self\n\n        # Get the hash of the arguments\n        hash_ = hash(tuple(args_) + tuple(kwargs_.items()))\n\n        # Get the traced module from the cache\n        traced_module = _traced_modules.get(hash_, None)\n\n        # If the traced module is not in the cache, trace it\n        if traced_module is None:\n            # Trace the module\n            traced_module = trace(func, *args_, **kwargs_)\n\n            # Compile the traced module\n            if ts_compiler is not None:\n                traced_module = ts_compiler(traced_module)\n\n            # Cache the traced module\n            _traced_modules[hash_] = traced_module\n\n        # Call the traced module\n        return traced_module(*args_, **kwargs_)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Import necessary modules\n    import functools\n    import torch\n    import threading\n    import weakref\n\n    # Initialize the cache for traced modules\n    cache = weakref.WeakKeyDictionary()\n\n    # Create a lock for thread safety\n    lock = threading.RLock()\n\n    # Define the wrapper function\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n\n        # Get the traced module from the cache or create a new one\n        with lock:\n            if func in cache:\n                traced_module = cache[func]\n            else:\n                traced_module = torch.jit.trace(func, *args, **kwargs)\n                cache[func] = traced_module\n\n        # Compile the traced module if a compiler is provided\n        if ts_compiler is not None:\n            traced_module = ts_compiler(traced_module, **kwargs_)\n\n        # Get the call helper for the traced module\n        call_helper = traced_module.forward\n\n        # Compile the call helper if a compiler is provided\n        if ts_compiler is not None:\n            call_helper = ts_compiler(call_helper, **kwargs_)\n\n        # Call the call helper with the provided arguments and return the result\n        return call_helper(*args, **kwargs)\n\n    # Return the wrapped function or module\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Import necessary modules\n    import functools\n    import torch\n    import threading\n\n    # Define a decorator function\n    def decorator(func):\n\n        # Initialize a cache for traced modules\n        cache = {}\n\n        # Define a lock to ensure thread safety\n        lock = threading.Lock()\n\n        # Define a wrapper function to trace and cache the module\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n\n            # Create a key for the cache based on the function and arguments\n            key = (func, args, tuple(kwargs.items()))\n\n            # Acquire the lock before accessing the cache\n            with lock:\n\n                # Check if the key is already in the cache\n                if key in cache:\n\n                    # If the key is in the cache, return the cached traced module\n                    return cache[key]\n\n                # If the key is not in the cache, trace the module\n                traced_module = torch.jit.trace(func, args, **kwargs)\n\n                # If a compiler function is provided, compile the traced module\n                if ts_compiler is not None:\n                    traced_module = ts_compiler(traced_module)\n\n                # Add the traced module to the cache\n                cache[key] = traced_module\n\n            # Return the traced module\n            return traced_module\n\n        # Return the wrapper function\n        return wrapper\n\n    # If the input is a function, apply the decorator directly\n    if callable(func):\n        return decorator(func)\n\n    # If the input is a module, return the wrapped forward method\n    return decorator(func.forward)"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Importing required modules\n    import functools\n    import torch\n    import threading\n\n    # Creating a lock to ensure thread safety\n    lock = threading.Lock()\n\n    # Creating a cache to store traced modules\n    cache = {}\n\n    # Defining the wrapper function\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n\n        # Creating a key based on the function's name and arguments\n        key = (func.__name__,) + args + tuple(kwargs.items())\n\n        # Acquiring the lock to access the cache\n        with lock:\n\n            # Checking if the key is already in the cache\n            if key in cache:\n\n                # If the key is in the cache, return the cached traced module\n                return cache[key]\n\n            # If the key is not in the cache, trace the module and add it to the cache\n            module = torch.jit.trace(func, args, **kwargs)\n            cache[key] = module\n\n            # If a compiler function is provided, compile the traced module\n            if ts_compiler is not None:\n                module = ts_compiler(module)\n\n            # Return the traced module\n            return module\n\n    # Returning the wrapper function\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if isinstance(func, torch.nn.Module):\n        forward_func = func.forward\n    else:\n        forward_func = func\n\n    def wrapper(*args, **kwargs):\n        with _lock:\n            nonlocal _cache\n            nonlocal _keys\n            nonlocal _cache_size\n            nonlocal _cache_freq\n            nonlocal _cache_miss\n            key = (forward_func, args, frozenset(kwargs.items()))\n            if key in _cache:\n                _cache_freq[key] += 1\n                return _cache[key](*args, **kwargs)\n            else:\n                _cache_miss[key] += 1\n                _cache[key] = ts_compiler(forward_func, *args, **kwargs) if ts_compiler else torch.jit.trace(forward_func, args, **kwargs)\n                _keys.append(key)\n                if _cache_size > 0 and len(_keys) > _cache_size:\n                    del _cache[_keys.pop(0)]\n                return _cache[key](*args, **kwargs)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    from torch.fx import Tracer\n    from torch.fx.proxy import TraceError\n    import torch\n    import functools\n    import threading\n\n    lock = threading.Lock()\n    cache = {}\n\n    def trace(func, *args, **kwargs):\n        # Use the lock to ensure thread safety\n        with lock:\n            # Check if the function is already in the cache\n            if func in cache:\n                return cache[func]\n\n            # Trace the function and cache the result\n            try:\n                tracer = Tracer()\n                traced = tracer.trace(func)\n                cache[func] = traced\n                return traced\n            except TraceError as e:\n                print(f\"Error tracing function {func.__name__}: {e}\")\n                return None\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Check if the function is a module\n        if isinstance(func, torch.nn.Module):\n            # Trace the module's forward method\n            traced = trace(func.forward, *args, **kwargs)\n            if traced is None:\n                return func(*args, **kwargs)\n            return traced(*args, **kwargs)\n\n        # Trace the function\n        traced = trace(func, *args, **kwargs)\n        if traced is None:\n            return func(*args, **kwargs)\n        return traced(*args, **kwargs)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    def wrapper(*args, **kwargs):\n        with lock:\n            if func not in cache:\n                cache[func] = {}\n\n            # Get the cache key based on the arguments\n            key = (args, tuple(kwargs.items()))\n\n            # Check if the traced module is already cached\n            if key in cache[func]:\n                traced_module = cache[func][key]\n            else:\n                # Trace the module\n                traced_module = trace_module(func, *args, **kwargs)\n\n                # Compile the traced module with the given compiler function\n                if ts_compiler is not None:\n                    traced_module = ts_compiler(traced_module)\n\n                # Cache the traced module\n                cache[func][key] = traced_module\n\n        # Call the traced module with the given arguments\n        return traced_module(*args, **kwargs)\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Import torch and torch._C\n    import torch\n    import torch._C\n\n    # Import functools and weakref\n    import functools\n    import weakref\n\n    # Import threading\n    import threading\n\n    # Define a decorator function for tracing\n    def decorator(fn):\n\n        # Define a wrapper function for the original function or module's forward method\n        @functools.wraps(fn)\n        def wrapper(*args, **kwargs):\n\n            # Check if the function is a torch.nn.Module\n            if isinstance(fn, torch.nn.Module):\n\n                # Get the original forward method\n                forward = fn.forward\n\n                # If ts_compiler is provided, use it to compile the forward method\n                if ts_compiler is not None:\n                    forward = ts_compiler(forward, **kwargs_)\n\n                # Set the compiled forward method as the new forward method\n                fn.forward = forward\n\n                # Get the traced module\n                traced_module = torch._C._jit_pass_lower_graph(fn, *args)\n\n            else:\n\n                # If ts_compiler is provided, use it to compile the function\n                if ts_compiler is not None:\n                    fn = ts_compiler(fn, **kwargs_)\n\n                # Get the traced module\n                traced_module = torch._C._jit_pass_lower_graph(fn, *args)\n\n            # Return the traced module\n            return traced_module\n\n        # Return the wrapper function\n        return wrapper\n\n    # Return the decorator function\n    return decorator\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    from .trace import trace\n    from .utils import is_module, is_function\n    from .cache import get_cache, set_cache\n    from .lock import lock\n    from .utils import get_call_helper\n\n    def wrapper(*args, **kwargs):\n\n        \"\"\"\n        This function is a wrapper around the original function or module's forward method. It first checks if the traced module is already cached, and if not, it traces the module and caches it. It then calls the call helper of the traced module with the original arguments.\n\n        Input-Output Arguments\n        :param args: Tuple. Positional arguments passed to the original function or module's forward method.\n        :param kwargs: Dict. Keyword arguments passed to the original function or module's forward method.\n        :return: Any. The result of calling the call helper of the traced module with the original arguments.\n        \"\"\"\n\n        with lock:\n            cache = get_cache()\n            if func not in cache:\n                if is_module(func):\n                    set_cache(func, trace(func, **kwargs_))\n                elif is_function(func):\n                    set_cache(func, trace(func, *args, **kwargs_))\n                else:\n                    raise TypeError(\"func must be a function or a torch.nn.Module\")\n            traced_module = get_cache()[func]\n            call_helper = get_call_helper(traced_module)\n            if ts_compiler is not None:\n                traced_module = ts_compiler(traced_module)\n                call_helper = get_call_helper(traced_module)\n            return call_helper(*args, **kwargs)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import functools\n    import torch\n    import threading\n\n    # Cache for traced modules\n    _traced_modules = {}\n\n    # Lock for thread safety\n    _lock = threading.Lock()\n\n    # Wrapper function for the original function or module's forward method\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n\n        # Combine the provided arguments\n        combined_args = args + tuple(kwargs.values())\n\n        # Compute the cache key using the combined arguments\n        cache_key = (func, combined_args)\n\n        # Use the lock to ensure thread safety\n        with _lock:\n\n            # Check if the module is already traced\n            if cache_key in _traced_modules:\n\n                # If traced, retrieve the traced module from the cache\n                traced_module = _traced_modules[cache_key]\n\n            else:\n\n                # If not traced, trace the module\n                traced_module = torch.jit.trace(func, combined_args, **kwargs_)\n\n                # Compile the traced module with the provided compiler\n                if ts_compiler is not None:\n                    traced_module = ts_compiler(traced_module)\n\n                # Cache the traced module for future use\n                _traced_modules[cache_key] = traced_module\n\n        # Call the traced module with the combined arguments\n        return traced_module(*combined_args)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Importing required libraries\n    import torch\n    import functools\n    import threading\n\n    # Define a lock to ensure thread safety when accessing or updating the cache of traced modules\n    lock = threading.Lock()\n\n    # Define a dictionary to store the cached traced modules\n    cache = {}\n\n    # Define a wrapper function that traces the function or module and caches the result\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Get the non-keyword arguments as a tuple\n        non_kwargs = args[1:]\n\n        # Get the keyword arguments as a tuple\n        kwargs = tuple(kwargs.items())\n\n        # Get the keyword arguments as a tuple\n        kwargs = tuple(kwargs.items())\n\n        # Create a key for the cache based on the non-keyword arguments and keyword arguments\n        key = (func, non_kwargs, kwargs)\n\n        # Acquire the lock to ensure thread safety\n        with lock:\n            # Check if the key is already in the cache\n            if key in cache:\n                # If the key is in the cache, get the cached traced module\n                trace = cache[key]\n            else:\n                # If the key is not in the cache, trace the function or module and cache the result\n                if isinstance(func, torch.nn.Module):\n                    # If the function is a PyTorch module, trace its forward method\n                    trace = torch.jit.trace(func.forward, *args, **kwargs)\n                else:\n                    # If the function is not a PyTorch module, trace it directly\n                    trace = torch.jit.trace(func, *args, **kwargs)\n\n                # If a compiler function is provided, compile the traced module\n                if ts_compiler is not None:\n                    trace = ts_compiler(trace)\n\n                # Cache the traced module\n                cache[key] = trace\n\n        # Call the cached traced module with the original arguments\n        return trace(*args)\n\n    # Return the wrapped function\n    return wrapper"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Extract the best configuration from the trial folder\n        config = js['class_name'].extract_best_config(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the extracted configuration and project directory\n        return cls(config, project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Load the best configuration from the trial folder\n        best_config = cls.load_best_config(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        trial_path = Path(trial_path)\n        project_path = trial_path.parent\n        config_path = trial_path / 'config.json'\n\n        with open(config_path, 'r') as f:\n            config = json.load(f)\n\n        return cls(config, project_path)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        trial_path = os.path.abspath(trial_path)\n        project_dir = os.path.dirname(trial_path)\n\n        best_config_path = os.path.join(trial_path, 'best_config.json')\n        with open(best_config_path, 'r') as f:\n            best_config = json.load(f)\n\n        return cls(best_config, project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = cls.extract_configuration(trial_path)\n        project_dir = os.path.abspath(os.path.join(trial_path, os.pardir))\n        return cls(config, project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Load the configuration from the trial folder\n        config = js['class_name'].load_config_from_trial_folder(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_directory = os.path.dirname(os.path.dirname(trial_path))\n\n        # Initialize the Runner with the extracted configuration and project directory\n        return cls(config, project_directory)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Extract the best configuration from the trial folder\n        best_config = js['class_name'].extract_best_config(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        import os\n        import json\n        from .runner import Runner\n\n        # Load the best configuration from the trial folder\n        with open(os.path.join(trial_path, \"best_config.json\"), \"r\") as f:\n            best_config = json.load(f)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_directory = os.path.abspath(os.path.join(trial_path, os.pardir))\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_directory)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Load the best configuration from the trial folder\n        config = cls.load_best_config(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_directory = os.path.dirname(os.path.dirname(trial_path))\n\n        # Initialize the Runner with the extracted configuration and project directory\n        return cls(config, project_directory)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        import os\n        import json\n\n        # Load the best configuration from the trial folder\n        config_path = os.path.join(trial_path, \"best_config.json\")\n        with open(config_path, \"r\") as f:\n            best_config = json.load(f)\n\n        # Get the project directory from the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = cls.get_best_config(trial_path)\n        return cls(config, os.path.dirname(trial_path))\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Load the trial folder and extract the best configuration\n        trial_folder = js.load_trial_folder(trial_path)\n        best_config = trial_folder.best_config\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Load the best configuration from the trial folder\n        config = cls.load_best_config(trial_path)\n\n        # Get the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the extracted configuration and project directory\n        return cls(config=config, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        import json\n        import os\n\n        trial_path = os.path.abspath(trial_path)\n        project_dir = os.path.abspath(os.path.join(trial_path, '..'))\n\n        with open(os.path.join(trial_path, 'best_config.json'), 'r') as f:\n            config = json.load(f)\n\n        return cls(config, project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Load the best configuration from the trial folder\n        best_config = js['class_name'].load_best_config(trial_path)\n\n        # Get the parent directory of the trial folder\n        project_dir = os.path.abspath(os.path.join(trial_path, os.pardir))\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        import json\n        import os\n\n        # Extract the best configuration from the trial folder\n        with open(os.path.join(trial_path, \"best_config.json\"), \"r\") as f:\n            best_config = json.load(f)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_directory = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_directory)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = cls.extract_best_config(trial_path)\n        project_path = Path(trial_path).parent\n\n        return cls(config=config, project_path=project_path)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Load the best configuration from the trial folder\n        best_config = cls.load_best_config(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        import os\n        import json\n\n        from . import Runner\n\n        # Get the path of the best configuration file\n        best_config_path = os.path.join(trial_path, 'best_config.json')\n\n        # Load the best configuration\n        with open(best_config_path, 'r') as f:\n            best_config = json.load(f)\n\n        # Get the project directory\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        import json\n        import os\n        from pathlib import Path\n        from . import Runner\n\n        with open(os.path.join(trial_path, \"config.json\")) as f:\n            config = json.load(f)\n\n        project_dir = Path(trial_path).parent\n        return cls(config, project_dir)\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize variables\n    best_result = None\n    best_time = float('inf')\n    best_module = None\n    best_module_index = None\n    best_module_params = None\n    best_module_time = None\n    best_module_result = None\n    best_module_result_columns = None\n    best_module_result_columns_len = None\n    best_module_result_columns_len_diff = None\n    best_module_result_columns_len_diff_percent = None\n    best_module_result_columns_len_diff_percent_str = None\n    best_module_result_columns_len_diff_percent_str_len = None\n    best_module_result_columns_len_diff_percent_str_len_diff = None\n    best_module_result_columns_len_diff_percent_str_len_diff_str = None\n    best_module_result_columns_len_diff_percent_str_len_diff_str_len = None\n    best_module_result_columns_len_diff_percent_str_len_diff_str_len_diff = None\n    best_module_result_columns_len_diff_percent_str_len_diff_str_len_diff_str = None\n    best_module_result_columns_len_diff_percent_str_len_diff_str_len_diff_str_len = None\n    best_module_result_columns_len_diff_percent_str_len_diff_str_len_diff_str_len_diff = None\n    best_module_result_columns_len_diff_percent_str_len_diff_str_len_diff_str_len_diff_str = None\n    best_module_result_columns_len_diff_percent_str_len_diff_str_len_diff_str_len_diff_str_len = None\n    best_module_result_columns_len_diff_percent_str_len_diff_str_len_diff_str_len_diff_str_len_diff = None\n    best_module_result_columns_len_diff_percent_str"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create a directory for this node line if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize the best result dataframe with the previous result\n    best_result = previous_result.copy()\n\n    # Initialize the best result metrics to None\n    best_result_metrics = None\n\n    # Initialize the best result module name to None\n    best_result_module_name = None\n\n    # Initialize the best result module index to None\n    best_result_module_index = None\n\n    # Initialize the best result speed to None\n    best_result_speed = None\n\n    # Initialize the best result speed threshold to None\n    best_result_speed_threshold = None\n\n    # Initialize the best result speed threshold index to None\n    best_result_speed_threshold_index = None\n\n    # Initialize the best result metrics threshold to None\n    best_result_metrics_threshold = None\n\n    # Initialize the best result metrics threshold index to None\n    best_result_metrics_threshold_index = None\n\n    # Initialize the best result speed threshold index to None\n    best_result_speed_threshold_index = None\n\n    # Initialize the best result metrics threshold index to None\n    best_result_metrics_threshold_index = None\n\n    # Initialize the best result speed threshold index to None\n    best_result_speed_threshold_index = None\n\n    # Initialize the best result metrics threshold index to None\n    best_result_metrics_threshold_index = None\n\n    # Initialize the best result speed threshold index to None\n    best_result_speed_threshold_index = None\n\n    # Initialize the best result metrics threshold index to None\n    best_result_metrics_threshold_index = None\n\n    # Initialize the best result speed threshold index to None\n    best_result_speed_threshold_index = None\n\n    # Initialize the best result metrics threshold index to None\n    best_result_metrics_threshold_index = None\n\n    # Initialize the best result"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize an empty list to store the results\n    results = []\n\n    # Loop through each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n        # Run the module with the specified parameters\n        result = module(**params)\n\n        # Evaluate the result using the specified strategies\n        result = evaluate_result(result, previous_result, strategies)\n\n        # Append the result to the list\n        results.append(result)\n\n    # Select the best result from the list\n    best_result = select_best_result(results, strategies)\n\n    # Save the best result to disk\n    save_result(best_result, node_line_dir)\n\n    # Save the summary of the execution times and evaluation metrics to disk\n    save_summary(results, node_line_dir)\n\n    # Combine the previous result columns with the selected retrieval node's result columns\n    combined_result = pd.concat([previous_result, best_result], axis=1)\n\n    return combined_result"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize the best result and evaluation metrics\n    best_result = None\n    best_metrics = None\n\n    # Iterate over each module\n    for module, params in zip(modules, module_params):\n        # Run the module with the given parameters\n        result = module(**params)\n\n        # Evaluate the module result using the specified strategies\n        metrics = evaluate_result(result, previous_result, strategies)\n\n        # Check if the current module result is better than the previous best result\n        if best_result is None or metrics['score'] > best_metrics['score']:\n            best_result = result\n            best_metrics = metrics\n\n    # Save the best result and evaluation metrics to disk\n    save_result(best_result, node_line_dir, 'best_result.csv')\n    save_metrics(best_metrics, node_line_dir, 'best_metrics.json')\n\n    # Combine the previous result columns with the best result columns\n    combined_result = pd.concat([previous_result, best_result], axis=1)\n\n    # Return the combined result\n    return combined_result"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the directory for this node line if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Initialize the best result and best execution time\n    best_result = None\n    best_execution_time = float('inf')\n\n    # Initialize the summary DataFrame\n    summary = pd.DataFrame(columns=['Module', 'Execution Time', 'Evaluation Metrics'])\n\n    # Iterate over the modules and their parameters\n    for module, params in zip(modules, module_params):\n        # Get the module name\n        module_name = module.__name__\n\n        # Create the directory for this module if it doesn't exist\n        module_dir = os.path.join(node_line_dir, module_name)\n        if not os.path.exists(module_dir):\n            os.makedirs(module_dir)\n\n        # Run the module with the given parameters\n        result, execution_time = run_module(module, params, previous_result)\n\n        # Evaluate the result using the specified strategies\n        evaluation_metrics = evaluate_result(result, previous_result, strategies)\n\n        # Save the result and evaluation metrics to disk\n        result.to_csv(os.path.join(module_dir, 'result.csv'), index=False)\n        evaluation_metrics.to_csv(os.path.join(module_dir, 'evaluation_metrics.csv'), index=False)\n\n        # Update the summary DataFrame\n        summary = summary.append({'Module': module_name, 'Execution Time': execution_time, 'Evaluation Metrics': evaluation_metrics}, ignore_index=True)\n\n        # Check if this result is the best so far\n        if execution_time < best_execution_time:\n            best_result = result\n            best_execution_time = execution_time\n\n    # Save the summary to disk\n    summary.to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n\n    # Combine the previous result columns with"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize the best result DataFrame with the columns from the previous result\n    best_result = previous_result.copy()\n\n    # Initialize a dictionary to store the results for each module\n    results = {}\n\n    # Initialize a dictionary to store the execution times for each module\n    execution_times = {}\n\n    # Initialize a dictionary to store the evaluation metrics for each module\n    evaluation_metrics = {}\n\n    # Initialize a dictionary to store the number of rows for each module\n    num_rows = {}\n\n    # Loop through each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n        # Get the name of the module\n        module_name = module.__name__\n\n        # Initialize a dictionary to store the results for this module\n        results[module_name] = {}\n\n        # Initialize a dictionary to store the execution times for this module\n        execution_times[module_name] = {}\n\n        # Initialize a dictionary to store the evaluation metrics for this module\n        evaluation_metrics[module_name] = {}\n\n        # Initialize a dictionary to store the number of rows for this module\n        num_rows[module_name] = {}\n\n        # Loop through each strategy\n        for strategy in strategies:\n            # Get the name of the strategy\n            strategy_name = strategy['name']\n\n            # Initialize a dictionary to store the results for this strategy\n            results[module_name][strategy_name] = {}\n\n            # Initialize a dictionary to store the execution times for this strategy\n            execution_times[module_name][strategy_name] = {}\n\n            # Initialize a dictionary to store the evaluation metrics for this strategy\n            evaluation_metrics[module_name][strategy_name] = {}\n\n            # Initialize a dictionary to store the number of rows for this strategy\n            num_rows[module_name][strategy_name] = {}\n\n            # Loop through each threshold for the strategy\n            for threshold in strategy['thresholds']:\n                # Get the threshold value\n                threshold_value = threshold['value']\n\n                # Initialize a dictionary to store the results for this"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize the best result as the previous result\n    best_result = previous_result\n\n    # Initialize a dictionary to store the execution times of each module\n    execution_times = {}\n\n    # Iterate through each module and its parameters\n    for module, params in zip(modules, module_params):\n\n        # Run the module with the given parameters\n        result = module(**params)\n\n        # Measure the execution time of the module\n        execution_time = time.time() - start_time\n        execution_times[module.__name__] = execution_time\n\n        # Evaluate the result using the specified metrics and speed thresholds\n        result_evaluation = evaluate_result(result, previous_result, strategies)\n\n        # If the result passes the evaluation, update the best result\n        if result_evaluation:\n            best_result = result\n\n    # Save the results and execution times to disk\n    save_results(best_result, execution_times, node_line_dir)\n\n    # Return the best result\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Evaluate and select the best result\n    best_result = evaluate_and_select_best_result(\n        modules=modules,\n        module_params=module_params,\n        previous_result=previous_result,\n        strategies=strategies,\n    )\n\n    # Save the results and summary\n    save_results_and_summary(\n        best_result=best_result,\n        node_line_dir=node_line_dir,\n    )\n\n    # Combine the previous result columns with the best result columns\n    best_result = pd.concat([previous_result, best_result], axis=1)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize variables\n    best_result = None\n    best_result_summary = None\n    best_result_time = float('inf')\n    best_result_name = None\n\n    # Iterate through each module\n    for module, params in zip(modules, module_params):\n        # Get the module name\n        module_name = module.__name__\n\n        # Run the module with the given parameters\n        result, result_time = run_module(module, params, previous_result)\n\n        # Evaluate the result using the specified metrics\n        result_summary = evaluate_result(result, strategies['metrics'], previous_result)\n\n        # Check if the result time is within the speed threshold\n        if result_time <= strategies['speed_threshold']:\n            # Check if the result is better than the current best result\n            if best_result is None or result_summary['score'] > best_result_summary['score']:\n                best_result = result\n                best_result_summary = result_summary\n                best_result_time = result_time\n                best_result_name = module_name\n\n    # Save the best result and its summary to disk\n    save_result(best_result, node_line_dir, 'result')\n    save_result(best_result_summary, node_line_dir, 'result_summary')\n\n    # Combine the previous result columns with the best result columns\n    best_result = pd.concat([previous_result, best_result], axis=1)\n\n    # Return the best result\n    return best_result\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize the best result and best result index\n    best_result = None\n    best_result_index = None\n\n    # Initialize the best result speed\n    best_result_speed = float('inf')\n\n    # Initialize the best result metrics\n    best_result_metrics = {}\n\n    # Initialize the results list\n    results = []\n\n    # Initialize the execution times list\n    execution_times = []\n\n    # Iterate over the modules\n    for i, (module, params) in enumerate(zip(modules, module_params)):\n        # Initialize the result and execution time\n        result = None\n        execution_time = None\n\n        # Check if the module has a 'run' function\n        if hasattr(module, 'run'):\n            # Measure the execution time of the module\n            start_time = time.time()\n            result = module.run(**params)\n            execution_time = time.time() - start_time\n\n            # Check if the execution time is within the specified speed threshold\n            if execution_time < strategies['speed_threshold']:\n                # Evaluate the result using the specified metrics\n                result_metrics = evaluate_result(result, previous_result, strategies['metrics'])\n\n                # Check if the result is better than the current best result\n                if best_result is None or result_metrics > best_result_metrics:\n                    # Update the best result and best result index\n                    best_result = result\n                    best_result_index = i\n\n                    # Update the best result speed\n                    best_result_speed = execution_time\n\n                    # Update the best result metrics\n                    best_result_metrics = result_metrics\n\n        # Add the result and execution time to the lists\n        results.append(result)\n        execution_times.append(execution_time)\n\n    # Save the results and execution times to disk\n    save_results_and_execution_times(results, execution_times, node_line_dir)\n\n    # Save the best result and best result index to disk\n    save_best_result_and_index(best_result, best_result_"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize variables\n    best_result = None\n    best_module = None\n    best_params = None\n    best_metrics = None\n    best_speed = None\n    best_time = None\n    best_module_index = None\n\n    # Iterate over each module\n    for i, module in enumerate(modules):\n        # Get the parameters for this module\n        params = module_params[i]\n\n        # Run the module with the given parameters\n        result, time = run_module(module, params, previous_result)\n\n        # Evaluate the result using the specified metrics and speed thresholds\n        metrics, speed = evaluate_result(result, previous_result, strategies)\n\n        # Check if this is the best result so far\n        if best_result is None or metrics > best_metrics or (metrics == best_metrics and speed > best_speed):\n            best_result = result\n            best_module = module\n            best_params = params\n            best_metrics = metrics\n            best_speed = speed\n            best_time = time\n            best_module_index = i\n\n    # Save the best result to disk\n    best_result.to_csv(f\"{node_line_dir}/best_result.csv\", index=False)\n\n    # Save a summary of the execution times and evaluation metrics to disk\n    with open(f\"{node_line_dir}/summary.txt\", \"w\") as f:\n        f.write(f\"Best module: {best_module.__name__}\\n\")\n        f.write(f\"Best params: {best_params}\\n\")\n        f.write(f\"Best metrics: {best_metrics}\\n\")\n        f.write(f\"Best speed: {best_speed}\\n\")\n        f.write(f\"Best time: {best_time}\\n\")\n\n    # Return the best result dataframe, which combines the previous result columns with the selected retrieval node's result columns\n    return pd.concat([previous_result, best_result], axis=1)\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize variables\n    results = []\n    best_result = None\n    best_result_index = None\n    best_result_time = float('inf')\n\n    # Loop through each module and its parameters\n    for i, (module, params) in enumerate(zip(modules, module_params)):\n        # Get the module name\n        module_name = module.__name__\n\n        # Run the module with the given parameters\n        result = module(**params)\n\n        # Measure the execution time\n        start_time = time.time()\n        result = module(**params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Evaluate the result using the specified metrics and speed thresholds\n        evaluation_context = {'previous_result': previous_result}\n        evaluation_result = evaluate_result(result, strategies, evaluation_context)\n\n        # Append the result and evaluation to the results list\n        results.append((result, evaluation_result))\n\n        # Update the best result if applicable\n        if evaluation_result['is_best']:\n            best_result = result\n            best_result_index = i\n            best_result_time = execution_time\n\n    # Save the results and a summary of the execution times and evaluation metrics to disk\n    save_results_and_summary(results, best_result, best_result_index, best_result_time, node_line_dir)"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Define the metrics and speed thresholds from the strategies\n    metrics = strategies['metrics']\n    speed_thresholds = strategies['speed_thresholds']\n\n    # Initialize variables to track the best result and its index\n    best_result = None\n    best_index = None\n\n    # Initialize a dictionary to store the execution times of each module\n    execution_times = {}\n\n    # Iterate over each module and its parameters\n    for i, (module, params) in enumerate(zip(modules, module_params)):\n\n        # Run the module with the given parameters\n        result = module(**params)\n\n        # Measure the execution time of the module\n        execution_times[i] = time.time() - start_time\n\n        # Evaluate the result using the specified metrics\n        evaluation_result = evaluate_result(result, previous_result, metrics)\n\n        # Check if the result is better than the current best result\n        if best_result is None or evaluation_result > best_result:\n            best_result = evaluation_result\n            best_index = i\n\n    # Get the best module and its parameters\n    best_module = modules[best_index]\n    best_params = module_params[best_index]\n\n    # Run the best module with the best parameters\n    result = best_module(**best_params)\n\n    # Save the result and execution times to disk\n    save_result(result, execution_times, node_line_dir)\n\n    # Return the best result dataframe\n    return result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize variables\n    best_result = None\n    best_result_index = None\n    best_result_summary = None\n\n    # Create a directory for the node line if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Iterate over each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Get the module name\n        module_name = module.__name__\n\n        # Create a directory for the module if it doesn't exist\n        module_dir = os.path.join(node_line_dir, module_name)\n        if not os.path.exists(module_dir):\n            os.makedirs(module_dir)\n\n        # Run the module with the given parameters\n        result, result_summary = run_module(module, params, previous_result, module_dir)\n\n        # Evaluate the result using the specified strategies\n        eval_result = evaluate_result(result, strategies)\n\n        # Save the result and summary to disk\n        result.to_csv(os.path.join(module_dir, \"result.csv\"))\n        result_summary.to_csv(os.path.join(module_dir, \"result_summary.csv\"))\n\n        # Update the best result if the current result is better\n        if best_result is None or eval_result > best_result:\n            best_result = eval_result\n            best_result_index = module_name\n            best_result_summary = result_summary\n\n    # Save the best result and summary to disk\n    best_result_df = pd.concat([previous_result, best_result], axis=1)\n    best_result_df.to_csv(os.path.join(node_line_dir, \"best_result.csv\"))\n    best_result_summary.to_csv(os.path.join(node_line_dir, \"best_result_summary.csv\"))\n\n    return best_result_df\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize variables\n    best_result = None\n    best_result_summary = None\n    best_result_time = float('inf')\n    best_result_name = None\n    best_result_params = None\n    best_result_metrics = None\n\n    # Create the directory for this node line if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Iterate through each module\n    for module, params in zip(modules, module_params):\n        # Get the module name\n        module_name = module.__name__\n\n        # Create the directory for this module if it doesn't exist\n        module_dir = os.path.join(node_line_dir, module_name)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Initialize the module's result and summary variables\n        module_result = None\n        module_result_summary = None\n\n        # Iterate through each strategy\n        for strategy in strategies:\n            # Get the strategy name\n            strategy_name = strategy['name']\n\n            # Create the directory for this strategy if it doesn't exist\n            strategy_dir = os.path.join(module_dir, strategy_name)\n            os.makedirs(strategy_dir, exist_ok=True)\n\n            # Initialize the strategy's result and summary variables\n            strategy_result = None\n            strategy_result_summary = None\n\n            # Run the module with the given parameters\n            result, result_summary, result_time = run_module(module, params, previous_result)\n\n            # Check if the result meets the speed threshold\n            if result_time < strategy['speed_threshold']:\n                # Save the result and summary to disk\n                result.to_csv(os.path.join(strategy_dir, 'result.csv'), index=False)\n                result_summary.to_csv(os.path.join(strategy_dir, 'result_summary.csv'), index=False)\n\n                # Evaluate the result using the specified metrics\n                metrics = evaluate_result(result,"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize variables\n    best_result = None\n    best_result_index = -1\n    best_result_time = float('inf')\n    best_result_metric = float('-inf')\n    result_dfs = []\n    result_times = []\n    result_metrics = []\n\n    # Iterate through each module\n    for i, (module, params) in enumerate(zip(modules, module_params)):\n        # Get the module name\n        module_name = module.__name__\n\n        # Run the module with the given parameters\n        result = module(**params)\n\n        # Measure the execution time\n        start_time = time.time()\n        result = module(**params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Evaluate the result using the specified metrics and speed thresholds\n        metric_score = evaluate_result(result, strategies['metrics'], previous_result)\n        speed_score = evaluate_speed(execution_time, strategies.get('speed_threshold', float('inf')))\n\n        # Save the result and execution time\n        result_dfs.append(result)\n        result_times.append(execution_time)\n        result_metrics.append(metric_score)\n\n        # Update the best result if the current result is better\n        if metric_score > best_result_metric and speed_score < strategies.get('speed_threshold', float('inf')):\n            best_result = result\n            best_result_index = i\n            best_result_time = execution_time\n            best_result_metric = metric_score\n\n    # Save the best result to disk\n    best_result_path = os.path.join(node_line_dir, f'{best_result_index}_best_result.csv')\n    best_result.to_csv(best_result_path, index=False)\n\n    # Save a summary of the execution times and evaluation metrics to disk\n    summary_path = os.path.join(node_line_dir, 'summary.csv')\n    summary_df = pd"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize variables\n    best_result = None\n    best_module = None\n    best_module_idx = None\n    best_module_params = None\n    best_execution_time = float('inf')\n    best_evaluation_score = float('-inf')\n    best_evaluation_context = None\n\n    # Create a directory for the node line if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Loop through each module and its parameters\n    for module_idx, (module, module_param) in enumerate(zip(modules, module_params)):\n        # Initialize variables for this module\n        module_result = None\n        module_execution_time = None\n        module_evaluation_score = None\n        module_evaluation_context = None\n\n        # Run the module with the specified parameters\n        module_result, module_execution_time, module_evaluation_score, module_evaluation_context = run_retrieval_module(\n            module, module_param, previous_result)\n\n        # Check if the execution time is within the speed threshold\n        if module_execution_time <= strategies['speed_threshold']:\n            # Check if the evaluation score is better than the current best score\n            if module_evaluation_score > best_evaluation_score:\n                # Update the best result and evaluation metrics\n                best_result = module_result\n                best_module = module\n                best_module_idx = module_idx\n                best_module_params = module_param\n                best_execution_time = module_execution_time\n                best_evaluation_score = module_evaluation_score\n                best_evaluation_context = module_evaluation_context\n\n    # Save the best result to disk\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n\n    # Save the summary of execution times and evaluation metrics to disk\n    with open(os.path.join(node_line_dir, 'summary.txt'), 'w') as f:\n        f.write(f"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize variables for best result and best result index\n    best_result = None\n    best_result_index = None\n\n    # Initialize a list to store all results\n    all_results = []\n\n    # Loop through each module and its parameters\n    for i, (module, params) in enumerate(zip(modules, module_params)):\n        # Get the module name\n        module_name = module.__name__\n\n        # Create a directory for this module\n        module_dir = os.path.join(node_line_dir, module_name)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Get the result from the module\n        result = module(**params)\n\n        # Check if the result is a DataFrame\n        if not isinstance(result, pd.DataFrame):\n            raise ValueError(f\"Module {module_name} returned a non-DataFrame result.\")\n\n        # Add the result to the list of all results\n        all_results.append(result)\n\n        # Evaluate the result\n        evaluation_result = evaluate_result(result, previous_result, strategies)\n\n        # Save the result to disk\n        result_path = os.path.join(module_dir, f\"{module_name}_result.csv\")\n        result.to_csv(result_path, index=False)\n\n        # Save the evaluation result to disk\n        evaluation_result_path = os.path.join(module_dir, f\"{module_name}_evaluation_result.csv\")\n        evaluation_result.to_csv(evaluation_result_path, index=False)\n\n        # Check if this is the first module\n        if i == 0:\n            # Set the best result and best result index to the first module\n            best_result = result\n            best_result_index = i\n        else:\n            # Compare the evaluation result with the current best result\n            if strategies[\"metric\"](evaluation_result) > strategies[\"metric\"](best_result):\n                # Update the best result and best result index\n                best_result = result\n                best_result_index = i"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize the best result as None\n    best_result = None\n\n    # Initialize the best module name as an empty string\n    best_module_name = \"\"\n\n    # Initialize a dictionary to store the evaluation results for each module\n    evaluation_results = {}\n\n    # Initialize a dictionary to store the execution times for each module\n    execution_times = {}\n\n    # Iterate over each module in the list\n    for module, params in zip(modules, module_params):\n        # Get the module name\n        module_name = module.__name__\n\n        # Initialize the module result as None\n        module_result = None\n\n        # Initialize the module execution time as 0\n        module_execution_time = 0\n\n        # Try to execute the module\n        try:\n            # Measure the execution time of the module\n            start_time = time.time()\n            module_result = module(**params)\n            end_time = time.time()\n\n            # Calculate the execution time of the module\n            module_execution_time = end_time - start_time\n\n            # Evaluate the module result using the specified evaluation metrics\n            evaluation_results[module_name] = evaluate_result(module_result, strategies[\"metrics\"], previous_result)\n\n        # If an error occurs during execution, handle it\n        except Exception as e:\n            # Print the error message\n            print(f\"Error occurred during execution of module {module_name}: {str(e)}\")\n\n        # Save the module result to disk\n        save_result(module_result, node_line_dir, module_name)\n\n        # Save the execution time to the dictionary\n        execution_times[module_name] = module_execution_time\n\n        # If the module execution time is within the specified speed threshold, update the best result\n        if module_execution_time <= strategies[\"speed_threshold\"]:\n            best_result = module_result\n            best_module_name = module_name\n\n    # Save the evaluation results to disk\n    save_evaluation_results(evaluation_results, node_line_dir)\n\n    # Save the execution times to disk"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize the best result dataframe\n    best_result = pd.DataFrame()\n\n    # Initialize the best module index\n    best_module_index = -1\n\n    # Initialize the best evaluation score\n    best_evaluation_score = -1\n\n    # Initialize the best speed\n    best_speed = -1\n\n    # Initialize the best execution time\n    best_execution_time = -1\n\n    # Initialize the best module name\n    best_module_name = \"\"\n\n    # Iterate over each module and its parameters\n    for module_index, (module, params) in enumerate(zip(modules, module_params)):\n\n        # Get the module name\n        module_name = module.__name__\n\n        # Initialize the module result dataframe\n        module_result = pd.DataFrame()\n\n        # Check if the module is in the strategies dictionary\n        if module_name in strategies:\n\n            # Get the module's strategies\n            module_strategies = strategies[module_name]\n\n            # Check if the module has a speed threshold\n            if \"speed_threshold\" in module_strategies:\n\n                # Get the speed threshold\n                speed_threshold = module_strategies[\"speed_threshold\"]\n\n                # Check if the module's execution time is faster than the speed threshold\n                if speed_threshold != -1 and module_execution_time > speed_threshold:\n\n                    # Skip this module\n                    continue\n\n            # Check if the module has a metrics threshold\n            if \"metrics_threshold\" in module_strategies:\n\n                # Get the metrics threshold\n                metrics_threshold = module_strategies[\"metrics_threshold\"]\n\n                # Check if the module's evaluation score is higher than the metrics threshold\n                if metrics_threshold != -1 and module_evaluation_score < metrics_threshold:\n\n                    # Skip this module\n                    continue\n\n        # Run the module with the given parameters\n        module_result = module(**params)\n\n        # Measure the execution time of the module\n        module_execution_time"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Evaluate and select the best module among query expansion node results\n    results = []\n    for module, params in zip(modules, module_params):\n        result = evaluate_query_expansion_module(module, params, previous_result)\n        results.append(result)\n\n    # Save the results and a summary to the specified directory\n    save_results_and_summary(results, node_line_dir, strategies)\n\n    # Select and save the best result based on the evaluation\n    best_result = select_best_result(results, strategies)\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize variables\n    best_result = None\n    best_score = float('-inf')\n    best_module_index = None\n    best_module_name = None\n    best_module_params = None\n    best_execution_time = None\n    best_execution_time_score = None\n    best_execution_time_speed_threshold = None\n    best_execution_time_speed_threshold_score = None\n    best_execution_time_speed_threshold_speed_score = None\n    best_execution_time_speed_threshold_speed_score_score = None\n    best_execution_time_speed_threshold_speed_score_speed_threshold = None\n    best_execution_time_speed_threshold_speed_score_speed_threshold_score = None\n    best_execution_time_speed_threshold_speed_score_speed_threshold_score_score = None\n    best_execution_time_speed_threshold_speed_score_speed_threshold_score_score_score = None\n    best_execution_time_speed_threshold_speed_score_speed_threshold_score_score_score_score = None\n    best_execution_time_speed_threshold_speed_score_speed_threshold_score_score_score_score_score = None\n    best_execution_time_speed_threshold_speed_score_speed_threshold_score_score_score_score_score_score = None\n    best_execution_time_speed_threshold_speed_score_speed_threshold_score_score_score_score_score_score_score = None\n    best_execution_time_speed_threshold_speed_score_speed_threshold_score_score_score_score_score_score_score_score = None\n    best_execution_time_speed_threshold_speed_score_speed_threshold_score_score_score_score_score_score_score_score_score = None\n    best_execution_time_speed_threshold_speed_score_speed_threshold_score_score_score_score_score_score_score_score_score_"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize variables to store the best result and its evaluation\n    best_result = None\n    best_evaluation = None\n\n    # Iterate over each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Run the module with the given parameters and measure its execution time\n        start_time = time.time()\n        result = module(previous_result, **params)\n        execution_time = time.time() - start_time\n\n        # Evaluate the module's performance based on the specified strategies\n        evaluation = evaluate_module_performance(result, strategies)\n\n        # Save the result and its evaluation to the specified directory\n        save_result_and_evaluation(result, evaluation, node_line_dir, module.__name__, execution_time)\n\n        # Update the best result and its evaluation if the current module's evaluation is better\n        if best_evaluation is None or is_better_evaluation(evaluation, best_evaluation, strategies):\n            best_result = result\n            best_evaluation = evaluation\n\n    # Save the best result to the specified directory\n    save_result_and_evaluation(best_result, best_evaluation, node_line_dir, 'best_result', execution_time)\n\n    # Return the best result\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate over each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n\n        # Create a directory for the current module\n        module_dir = os.path.join(node_line_dir, module.__name__)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Create a directory for the current module's results\n        module_results_dir = os.path.join(module_dir, \"results\")\n        os.makedirs(module_results_dir, exist_ok=True)\n\n        # Create a directory for the current module's summaries\n        module_summaries_dir = os.path.join(module_dir, \"summaries\")\n        os.makedirs(module_summaries_dir, exist_ok=True)\n\n        # Run the current module with the given parameters and measure its execution time\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Save the result to a file in the module's results directory\n        result_file = os.path.join(module_results_dir, f\"result_{module.__name__}.csv\")\n        result.to_csv(result_file, index=False)\n\n        # Save the execution time to a file in the module's summaries directory\n        summary_file = os.path.join(module_summaries_dir, f\"summary_{module.__name__}.txt\")\n        with open(summary_file, \"w\") as f:\n            f.write(f\"Execution time: {execution_time:.2f} seconds\\n\")\n\n        # Append the result, module name, and execution time to the results list\n        results.append((result, module.__name__, execution_time))\n\n    # Select the best result based on the specified strategies\n    best_result = select_best_result(results, strategies)\n\n    # Save the best result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize variables\n    best_result = None\n    best_module = None\n    best_time = float('inf')\n    best_metrics = None\n    best_params = None\n\n    # Iterate over each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n        # Run the module with the given parameters and measure its execution time\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Evaluate the module's performance based on the specified strategies\n        metrics = evaluate_module(result, strategies)\n\n        # Save the module's results and summary to the specified directory\n        save_module_results(result, metrics, execution_time, params, node_line_dir, module.__name__)\n\n        # Select the best module based on the evaluation criteria\n        if execution_time < best_time and metrics['score'] > best_metrics['score']:\n            best_result = result\n            best_module = module\n            best_time = execution_time\n            best_metrics = metrics\n            best_params = params\n\n    # Save the best result to the specified directory\n    save_best_result(best_result, best_module.__name__, best_time, best_metrics, best_params, node_line_dir)\n\n    # Return the best result\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize an empty dictionary to store the results\n    results = {}\n\n    # Initialize an empty dictionary to store the execution times\n    execution_times = {}\n\n    # Loop through each module in the modules list\n    for module, params in zip(modules, module_params):\n\n        # Get the name of the module\n        module_name = module.__name__\n\n        # Run the module with the given parameters and measure its execution time\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n\n        # Calculate the execution time\n        execution_time = end_time - start_time\n\n        # Save the result and execution time in the corresponding dictionaries\n        results[module_name] = result\n        execution_times[module_name] = execution_time\n\n        # Save the result to a CSV file in the specified directory\n        result.to_csv(f\"{node_line_dir}/{module_name}.csv\", index=False)\n\n    # Save the execution times to a JSON file in the specified directory\n    with open(f\"{node_line_dir}/execution_times.json\", \"w\") as f:\n        json.dump(execution_times, f)\n\n    # Initialize an empty list to store the evaluation results\n    evaluation_results = []\n\n    # Loop through each module in the modules list\n    for module, params in zip(modules, module_params):\n\n        # Get the name of the module\n        module_name = module.__name__\n\n        # Get the result for the current module\n        result = results[module_name]\n\n        # Initialize an empty dictionary to store the evaluation metrics for the current module\n        evaluation_metrics = {}\n\n        # Loop through each metric in the metrics list\n        for metric in strategies[\"metrics\"]:\n\n            # Calculate the evaluation metric for the current module and append it to the evaluation_metrics dictionary\n            evaluation_metrics[metric.__name__] = metric(result)\n\n        # Append the evaluation metrics for the current module to the evaluation_results list\n        evaluation_results."}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize variables to track best results\n    best_result = None\n    best_result_summary = None\n    best_result_time = None\n    best_result_name = None\n\n    # Loop through each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Get the name of the module\n        module_name = module.__name__\n\n        # Initialize variables to track current results\n        current_result = None\n        current_result_summary = None\n        current_result_time = None\n\n        # Run the module with the given parameters and measure execution time\n        start_time = time.time()\n        current_result, current_result_summary = module(previous_result, **params)\n        end_time = time.time()\n        current_result_time = end_time - start_time\n\n        # Save the results and summary to the specified directory\n        current_result.to_csv(os.path.join(node_line_dir, f\"{module_name}_result.csv\"), index=False)\n        with open(os.path.join(node_line_dir, f\"{module_name}_summary.txt\"), \"w\") as f:\n            f.write(current_result_summary)\n\n        # Evaluate the performance of the module based on specified strategies\n        if best_result is None:\n            best_result = current_result\n            best_result_summary = current_result_summary\n            best_result_time = current_result_time\n            best_result_name = module_name\n        else:\n            if \"speed_threshold\" in strategies and current_result_time > strategies[\"speed_threshold\"]:\n                continue\n            if \"metrics\" in strategies:\n                current_result_eval = evaluate_result(current_result, **strategies[\"metrics\"])\n                best_result_eval = evaluate_result(best_result, **strategies[\"metrics\"])\n                if current_result_eval > best_result_eval:\n                    best_result = current_result\n                    best_result_summary = current_result_summary\n                    best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a directory for the current node\n    node_line_dir = os.path.join(node_line_dir, \"query_expansion\")\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize an empty dataframe to store all results\n    all_results = pd.DataFrame()\n\n    # Initialize an empty dataframe to store the summary of all results\n    all_results_summary = pd.DataFrame()\n\n    # Iterate over each module\n    for i, module in enumerate(modules):\n        # Get the parameters for the current module\n        params = module_params[i]\n\n        # Create a directory for the current module\n        module_dir = os.path.join(node_line_dir, f\"module_{i}\")\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Initialize an empty dataframe to store the results for the current module\n        module_results = pd.DataFrame()\n\n        # Initialize an empty dataframe to store the summary of the results for the current module\n        module_results_summary = pd.DataFrame()\n\n        # Iterate over each parameter combination for the current module\n        for j, param in enumerate(params):\n            # Create a directory for the current parameter combination\n            param_dir = os.path.join(module_dir, f\"param_{j}\")\n            os.makedirs(param_dir, exist_ok=True)\n\n            # Initialize an empty dataframe to store the results for the current parameter combination\n            param_results = pd.DataFrame()\n\n            # Initialize an empty dataframe to store the summary of the results for the current parameter combination\n            param_results_summary = pd.DataFrame()\n\n            # Iterate over each strategy\n            for strategy in strategies:\n                # Initialize an empty dataframe to store the results for the current strategy\n                strategy_results = pd.DataFrame()\n\n                # Initialize an empty dataframe to store the summary of the results for the current strategy\n                strategy_results_summary = pd.DataFrame()"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory for the node line if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize an empty list to store the results\n    results = []\n\n    # Loop through each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Get the module name\n        module_name = module.__name__.split(\".\")[-1]\n\n        # Create a directory for the module if it doesn't exist\n        module_dir = os.path.join(node_line_dir, module_name)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Run the module with the specified parameters\n        result = module(previous_result, **params)\n\n        # Measure the execution time of the module\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Evaluate the module's performance based on the specified strategies\n        evaluation_metrics = evaluate_module(result, strategies)\n\n        # Save the results and a summary of the module's execution time and evaluation metrics\n        result.to_csv(os.path.join(module_dir, \"result.csv\"))\n        with open(os.path.join(module_dir, \"summary.txt\"), \"w\") as f:\n            f.write(f\"Execution time: {execution_time} seconds\\n\")\n            f.write(f\"Evaluation metrics: {evaluation_metrics}\\n\")\n\n        # Append the module's results to the list\n        results.append((module_name, execution_time, evaluation_metrics, result))\n\n    # Select the best result based on the specified strategies\n    best_result = select_best_result(results, strategies)\n\n    # Save the best result to the directory\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"))\n\n    # Return the best result\n   "}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize variables\n    best_result = None\n    best_result_name = None\n    best_result_time = None\n    best_result_metric = None\n    best_result_metric_value = None\n\n    # Initialize a dictionary to store execution times\n    execution_times = {}\n\n    # Iterate over each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Get the name of the module\n        module_name = module.__name__\n\n        # Run the module with the given parameters and measure execution time\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Save the result and execution time\n        result.to_csv(os.path.join(node_line_dir, f\"{module_name}.csv\"), index=False)\n        execution_times[module_name] = execution_time\n\n        # Evaluate the result based on the specified strategies\n        metric_value = evaluate_result(result, strategies)\n\n        # Update the best result if the current result is better\n        if best_result is None or metric_value > best_result_metric_value:\n            best_result = result\n            best_result_name = module_name\n            best_result_time = execution_time\n            best_result_metric = strategies[\"metric\"]\n            best_result_metric_value = metric_value\n\n    # Save the execution times and the best result summary\n    with open(os.path.join(node_line_dir, \"execution_times.txt\"), \"w\") as f:\n        for module_name, execution_time in execution_times.items():\n            f.write(f\"{module_name}: {execution_time:.4f} seconds\\n\")\n\n    with open(os.path.join(node_line_dir, \"best_result_summary.txt\"), \"w\") as f:\n        f.write(f\"Best result: {best_result_name}\\n\")\n        f.write(f\"Execution time"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize variables for best result and best evaluation\n    best_result = None\n    best_eval = None\n\n    # Loop through each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Get the name of the module\n        module_name = module.__name__.split('.')[-1]\n\n        # Run the module with the given parameters\n        result = module(previous_result, **params)\n\n        # Check if the result is a pandas dataframe\n        if not isinstance(result, pd.DataFrame):\n            raise ValueError(f\"Module {module_name} did not return a pandas dataframe\")\n\n        # Measure the execution time of the module\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Check if the execution time is within the specified speed threshold\n        if execution_time > strategies['speed_threshold']:\n            continue\n\n        # Evaluate the performance of the module using the specified metrics\n        eval_results = {}\n        for metric_name, metric_func in strategies['metrics'].items():\n            eval_results[metric_name] = metric_func(result)\n\n        # Save the results and evaluation to the specified directory\n        result_dir = os.path.join(node_line_dir, module_name)\n        os.makedirs(result_dir, exist_ok=True)\n        result.to_csv(os.path.join(result_dir, 'result.csv'), index=False)\n        with open(os.path.join(result_dir, 'eval.json'), 'w') as f:\n            json.dump(eval_results, f)\n\n        # Check if the current result is the best based on the evaluation criteria\n        if best_result is None or all(eval_results[metric_name] > best_eval[metric_name] for metric_name in strategies['metrics']):\n            best_result = result\n            best_eval = eval_results"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a directory for the query expansion node\n    node_line_dir = os.path.join(node_line_dir, 'query_expansion')\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize variables to track the best result and its corresponding module\n    best_result = None\n    best_module = None\n\n    # Initialize a dictionary to store the execution times of each module\n    execution_times = {}\n\n    # Iterate over each query expansion module\n    for module, params in zip(modules, module_params):\n        # Create a directory for the current module\n        module_dir = os.path.join(node_line_dir, module.__name__)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Measure the execution time of the current module\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n        execution_times[module.__name__] = execution_time\n\n        # Evaluate the performance of the current module based on the specified strategies\n        for strategy in strategies:\n            if strategy == 'speed':\n                # Check if the execution time is within the specified threshold\n                if execution_time < strategies[strategy]:\n                    # If the execution time is within the threshold, save the result and summary\n                    result.to_csv(os.path.join(module_dir, 'result.csv'), index=False)\n                    pd.DataFrame.from_dict(execution_times, orient='index', columns=['Execution Time']).to_csv(os.path.join(module_dir, 'summary.csv'))\n\n                    # Update the best result and module if the current result is better than the previous best\n                    if best_result is None or strategies[strategy] < best_result:\n                        best_result = result\n                        best_module = module\n            elif strategy == 'metrics':\n                # Evaluate the performance of the current result based on the specified metrics\n                for metric in strategies[strategy]:"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize variables\n    best_result = None\n    best_result_summary = None\n    best_result_name = None\n    best_result_time = None\n    best_result_metrics = None\n    best_result_index = None\n\n    # Iterate over each module\n    for i, module in enumerate(modules):\n\n        # Get the module name\n        module_name = module.__name__\n\n        # Get the parameters for the current module\n        module_param = module_params[i]\n\n        # Create a directory for the current module\n        module_dir = os.path.join(node_line_dir, module_name)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Initialize variables for the current module\n        module_result = None\n        module_result_summary = None\n        module_result_time = None\n        module_result_metrics = None\n\n        # Check if the current module has already been executed and saved\n        if os.path.exists(os.path.join(module_dir, 'result.csv')):\n\n            # Load the saved result and summary\n            module_result = pd.read_csv(os.path.join(module_dir, 'result.csv'))\n            module_result_summary = pd.read_csv(os.path.join(module_dir, 'summary.csv'))\n\n            # Extract execution time and evaluation metrics from the summary\n            module_result_time = module_result_summary['Execution Time'].values[0]\n            module_result_metrics = module_result_summary.iloc[0, 1:].to_dict()\n\n        else:\n\n            # Initialize variables for measuring execution time\n            start_time = time.time()\n\n            # Execute the current module\n            module_result = module(previous_result, **module_param)\n\n            # Calculate execution time\n            end_time = time.time()\n            module_result_time = end_time - start_time\n\n            # Evaluate the performance of the current module\n            module_result_metrics = evaluate_performance"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate over each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n        # Get the name of the module\n        module_name = module.__name__\n        # Create a directory for the module in the specified directory\n        module_dir = os.path.join(node_line_dir, module_name)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Initialize a dictionary to store the results for the current module\n        module_results = {\n            \"module\": module_name,\n            \"params\": params,\n            \"results\": []\n        }\n\n        # Iterate over each strategy\n        for strategy in strategies:\n            # Get the name of the strategy\n            strategy_name = strategy[\"name\"]\n            # Create a directory for the strategy in the module directory\n            strategy_dir = os.path.join(module_dir, strategy_name)\n            os.makedirs(strategy_dir, exist_ok=True)\n\n            # Initialize a dictionary to store the results for the current strategy\n            strategy_results = {\n                \"strategy\": strategy_name,\n                \"results\": []\n            }\n\n            # Iterate over the number of iterations specified in the strategy\n            for i in range(strategy[\"iterations\"]):\n                # Initialize a dictionary to store the results for the current iteration\n                iteration_results = {\n                    \"iteration\": i + 1,\n                    \"time\": 0,\n                    \"metrics\": {}\n                }\n\n                # Start measuring the execution time of the module\n                start_time = time.time()\n                # Execute the module with the given parameters\n                result = module(**params)\n                # Stop measuring the execution time\n                end_time = time.time()\n                # Calculate the execution time\n                execution_time = end_time - start_time\n\n                # Save the execution time in the iteration results\n                iteration_results[\"time\"] = execution_time\n\n                # Iterate over each metric specified in the strategy\n                for metric in strategy[\"met"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize an empty list to store the results from each module\n    results = []\n\n    # Initialize an empty list to store the execution times of each module\n    execution_times = []\n\n    # Loop through each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n\n        # Run the module with the given parameters and measure its execution time\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n\n        # Calculate the execution time\n        execution_time = end_time - start_time\n\n        # Append the result, execution time, and parameters to the respective lists\n        results.append(result)\n        execution_times.append(execution_time)\n\n    # Create a dataframe to store the results, execution times, and parameters\n    results_df = pd.DataFrame({'result': results,\n                               'execution_time': execution_times,\n                               'params': module_params})\n\n    # Save the results dataframe to a CSV file in the node directory\n    results_df.to_csv(f'{node_line_dir}/results.csv', index=False)\n\n    # Create a summary dataframe to store the evaluation metrics and execution times\n    summary_df = pd.DataFrame({'execution_time': execution_times})\n\n    # Loop through each evaluation metric in the strategies\n    for metric in strategies['metrics']:\n\n        # Initialize an empty list to store the evaluation scores for each result\n        scores = []\n\n        # Loop through each result and calculate the evaluation score using the specified metric\n        for result in results:\n            score = metric(result, previous_result)\n            scores.append(score)\n\n        # Add the evaluation scores to the summary dataframe\n        summary_df[metric.__name__] = scores\n\n    # Save the summary dataframe to a CSV file in the node directory\n    summary_df.to_csv(f'{node_line_dir}/summary.csv', index=False)\n\n    # Select the best result based on the evaluation metrics and execution time thresh"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Check if the directory exists, create it if not\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Initialize a dictionary to store the results of each module\n    results = {}\n\n    # Initialize a list to store the execution times of each module\n    execution_times = []\n\n    # Iterate over the modules and their corresponding parameters\n    for module, params in zip(modules, module_params):\n\n        # Get the name of the module\n        module_name = module.__name__\n\n        # Initialize a dictionary to store the results for the current module\n        results[module_name] = {}\n\n        # Initialize a list to store the execution times for the current module\n        execution_times_module = []\n\n        # Initialize a list to store the evaluation metrics for the current module\n        evaluation_metrics = []\n\n        # Iterate over the number of iterations specified in the strategies dictionary\n        for i in range(strategies[\"iterations\"]):\n\n            # Start the timer\n            start_time = time.time()\n\n            # Run the current module with the given parameters\n            result = module(**params)\n\n            # Stop the timer\n            end_time = time.time()\n\n            # Calculate the execution time\n            execution_time = end_time - start_time\n\n            # Append the execution time to the list of execution times for the current module\n            execution_times_module.append(execution_time)\n\n            # Append the result to the list of results for the current module\n            results[module_name][i] = result\n\n            # If the current module is not the first one, compare it to the previous best result\n            if i > 0:\n\n                # Get the previous best result\n                previous_best_result = results[module_name][i - 1]\n\n                # Calculate the evaluation metrics for the current result compared to the previous best result\n                evaluation_metric = evaluate_results(result, previous_best_result)\n\n                # Append the evaluation metric to the list of evaluation metrics for the current module\n                evaluation_metrics"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize variables\n    best_result = None\n    best_result_summary = None\n    best_result_name = None\n    best_result_execution_time = None\n    best_result_speed = None\n    best_result_metrics = None\n    best_result_execution_time_diff = None\n    best_result_speed_diff = None\n    best_result_metrics_diff = None\n\n    # Iterate through modules and execute each with given parameters\n    for module, params in zip(modules, module_params):\n        module_name = module.__name__\n        print(f\"Running {module_name}...\")\n        module_result, module_execution_time, module_speed, module_metrics = run_module(module, params, previous_result)\n\n        # Save results and summary\n        module_result_path = os.path.join(node_line_dir, f\"{module_name}_result.csv\")\n        module_summary_path = os.path.join(node_line_dir, f\"{module_name}_summary.txt\")\n        module_result.to_csv(module_result_path, index=False)\n        with open(module_summary_path, \"w\") as f:\n            f.write(f\"Execution time: {module_execution_time} seconds\\n\")\n            f.write(f\"Speed: {module_speed} records/second\\n\")\n            f.write(f\"Metrics: {module_metrics}\\n\")\n\n        # Evaluate performance based on strategies\n        if best_result is None or evaluate_module_performance(module_result, module_execution_time, module_speed, module_metrics, strategies) > evaluate_module_performance(best_result, best_result_execution_time, best_result_speed, best_result_metrics, strategies):\n            best_result = module_result\n            best_result_summary = f\"Execution time: {module_execution_time} seconds\\nSpeed: {module_speed} records/second\\nMetrics: {module_metrics}\\n\"\n            best_result_name = module_name\n           "}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize a dictionary to store the results\n    results = {}\n\n    # Iterate over each query expansion module\n    for i, (module, params) in enumerate(zip(modules, module_params)):\n\n        # Get the name of the module\n        module_name = module.__name__\n\n        # Get the parameters for the module\n        params = params\n\n        # Initialize a dictionary to store the results for the current module\n        module_results = {}\n\n        # Run the module with the given parameters and measure the execution time\n        start_time = time.time()\n        module_results['result'] = module(previous_result, **params)\n        end_time = time.time()\n        module_results['execution_time'] = end_time - start_time\n\n        # Add the results for the current module to the dictionary of results\n        results[module_name] = module_results\n\n    # Initialize a dictionary to store the evaluation results\n    evaluation_results = {}\n\n    # Iterate over each strategy\n    for strategy, params in strategies.items():\n\n        # Initialize a dictionary to store the evaluation results for the current strategy\n        strategy_results = {}\n\n        # Iterate over each query expansion module\n        for module_name, module_results in results.items():\n\n            # Get the results for the current module\n            result = module_results['result']\n\n            # Get the execution time for the current module\n            execution_time = module_results['execution_time']\n\n            # Initialize a dictionary to store the evaluation metrics for the current module\n            module_metrics = {}\n\n            # Iterate over each metric\n            for metric, metric_params in params['metrics'].items():\n\n                # Get the parameters for the metric\n                metric_params = metric_params\n\n                # Evaluate the metric and add the result to the dictionary of metrics for the current module\n                module_metrics[metric] = metric(result, **metric_params)\n\n            # Add the execution time and evaluation metrics for the current module to the dictionary of results for the current strategy\n            strategy_results[module_name] = {'execution_time': execution_time,"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize a dictionary to store the results of each module\n    results = {}\n\n    # Loop through each module in the modules list\n    for i, module in enumerate(modules):\n\n        # Get the parameters for the current module from the module_params list\n        module_param = module_params[i]\n\n        # Run the current module with the given parameters and measure its execution time\n        start_time = time.time()\n        result = module(previous_result, **module_param)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Save the results and execution time to the results dictionary\n        results[module.__name__] = {\n            'result': result,\n            'execution_time': execution_time\n        }\n\n        # Save the results and execution time to a JSON file in the specified directory\n        with open(f'{node_line_dir}/{module.__name__}_results.json', 'w') as f:\n            json.dump(results[module.__name__], f)\n\n    # Initialize a dictionary to store the evaluation results for each module\n    evaluation_results = {}\n\n    # Loop through each module in the modules list\n    for module_name, module_results in results.items():\n\n        # Initialize a dictionary to store the evaluation metrics for the current module\n        module_evaluation = {}\n\n        # Loop through each evaluation metric in the strategies dictionary\n        for metric, value in strategies['metrics'].items():\n\n            # Calculate the evaluation metric for the current module and store it in the module_evaluation dictionary\n            module_evaluation[metric] = value(module_results['result'], previous_result)\n\n        # Calculate the speed of the current module based on the execution time and store it in the module_evaluation dictionary\n        module_evaluation['speed'] = module_results['execution_time']\n\n        # Store the module_evaluation dictionary in the evaluation_results dictionary\n        evaluation_results[module_name] = module_evaluation\n\n    # Initialize a dictionary to store the evaluation summary for each module\n    evaluation_summary = {}\n\n    #"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize variables\n    best_result = None\n    best_module_index = None\n    best_execution_time = float('inf')\n    best_metrics = None\n\n    # Evaluate each module\n    for module_index, (module, params) in enumerate(zip(modules, module_params)):\n        # Run the module with given parameters\n        module_result, execution_time = run_query_expansion_module(module, params, previous_result)\n\n        # Save the result\n        result_file = os.path.join(node_line_dir, f\"result_{module_index}.csv\")\n        module_result.to_csv(result_file, index=False)\n\n        # Evaluate the result based on specified strategies\n        metrics = evaluate_query_expansion_result(module_result, strategies)\n\n        # Save the evaluation summary\n        summary_file = os.path.join(node_line_dir, f\"summary_{module_index}.txt\")\n        save_query_expansion_summary(module_result, execution_time, metrics, summary_file)\n\n        # Check if the module meets the speed threshold\n        if execution_time <= strategies['speed_threshold']:\n            # Check if the module has the best metrics so far\n            if best_metrics is None or metrics > best_metrics:\n                best_result = module_result\n                best_module_index = module_index\n                best_execution_time = execution_time\n                best_metrics = metrics\n\n    # Select and save the best result\n    if best_result is not None:\n        best_result_file = os.path.join(node_line_dir, f\"best_result_{best_module_index}.csv\")\n        best_result.to_csv(best_result_file, index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create a directory for the node's output\n    node_output_dir = os.path.join(node_line_dir, 'node_output')\n    os.makedirs(node_output_dir, exist_ok=True)\n\n    # Create a directory for the prompt maker's output\n    prompt_maker_output_dir = os.path.join(node_output_dir, 'prompt_maker_output')\n    os.makedirs(prompt_maker_output_dir, exist_ok=True)\n\n    # Initialize a dictionary to store execution times\n    execution_times = {}\n\n    # Initialize a dictionary to store evaluation metrics\n    evaluation_metrics = {}\n\n    # Initialize a dictionary to store the best prompt maker's output\n    best_prompt_maker_output = {}\n\n    # Initialize a dictionary to store the best prompt maker's output\n    best_prompt_maker_output = {}\n\n    # Initialize a dictionary to store the best prompt maker's output\n    best_prompt_maker_output = {}\n\n    # Initialize a dictionary to store the best prompt maker's output\n    best_prompt_maker_output = {}\n\n    # Initialize a dictionary to store the best prompt maker's output\n    best_prompt_maker_output = {}\n\n    # Initialize a dictionary to store the best prompt maker's output\n    best_prompt_maker_output = {}\n\n    # Initialize a dictionary to store the best prompt maker's output\n    best_prompt_maker_output = {}\n\n    # Initialize a dictionary to store the best prompt maker's output\n    best_prompt_maker_output = {}\n\n    # Initialize a dictionary to store the best prompt maker's output\n    best_prompt_maker_output = {}\n\n    # Initialize a dictionary to store the best prompt maker's output\n    best_prompt_maker_output = {}\n\n    # Initialize a dictionary to store the best prompt maker's output\n    best_prompt_maker_output = {}\n\n    # Initialize a dictionary to store the best prompt maker's output"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    create_necessary_directories(node_line_dir)\n\n    # Initialize variables\n    best_prompt_maker = None\n    best_prompt_maker_result = None\n    best_prompt_maker_time = float('inf')\n    best_prompt_maker_eval_result = None\n\n    # Iterate through each prompt maker module\n    for module, params in zip(modules, module_params):\n        # Run the prompt maker module\n        prompt_maker_result, prompt_maker_time = run_prompt_maker(module, params)\n\n        # Evaluate the prompt maker module if it meets the speed threshold\n        if prompt_maker_time <= strategies['speed_threshold']:\n            # Use the default generator module if not specified in the strategies\n            if 'generator_module' not in strategies:\n                generator_module = default_generator_module\n            else:\n                generator_module = strategies['generator_module']\n\n            # Evaluate the prompt maker module using the generator module\n            prompt_maker_eval_result = evaluate_prompt_maker(prompt_maker_result, generator_module)\n\n            # Check if the prompt maker module is the best so far based on evaluation metrics\n            if best_prompt_maker is None or all(\n                    prompt_maker_eval_result[metric] > best_prompt_maker_eval_result[metric]\n                    for metric in strategies['evaluation_metrics']):\n                best_prompt_maker = module\n                best_prompt_maker_result = prompt_maker_result\n                best_prompt_maker_time = prompt_maker_time\n                best_prompt_maker_eval_result = prompt_maker_eval_result\n\n    # Save the best prompt maker's result and evaluation summary\n    save_prompt_maker_result(best_prompt_maker_result, node_line_dir)\n    save_prompt_maker_eval_summary(best_prompt_maker_eval_result, node_line_dir)\n\n    # Combine the previous operation's result with the best prompt maker's result\n    combined_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    node_dir = node_line_dir + '/node_' + str(len(os.listdir(node_line_dir)))\n    os.makedirs(node_dir)\n\n    # Initialize variables\n    best_module = None\n    best_module_params = None\n    best_result = None\n    best_time = float('inf')\n    best_eval = float('-inf')\n\n    # Iterate over modules and their parameters\n    for module, params in zip(modules, module_params):\n        # Create a directory for the current module\n        module_dir = node_dir + '/' + module.__name__\n        os.makedirs(module_dir)\n\n        # Run the module and measure execution time\n        start_time = time.time()\n        result = module(**params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Evaluate the module if specified\n        if 'evaluation_metric' in strategies:\n            eval_metric = strategies['evaluation_metric']\n            eval_result = eval_metric(result)\n        else:\n            eval_result = None\n\n        # Save the module's output and execution time\n        result.to_csv(module_dir + '/result.csv', index=False)\n        with open(module_dir + '/execution_time.txt', 'w') as f:\n            f.write(str(execution_time))\n\n        # Save the module's evaluation result if available\n        if eval_result is not None:\n            with open(module_dir + '/evaluation_result.txt', 'w') as f:\n                f.write(str(eval_result))\n\n        # Update best module and result if the current module is faster and better than the previous best\n        if execution_time < best_time and (eval_result is None or eval_result > best_eval):\n            best_module = module\n            best_module_params = params\n            best_result = result\n            best_time = execution_time\n            best_eval = eval_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(f'{node_line_dir}/prompt_maker_results', exist_ok=True)\n    os.makedirs(f'{node_line_dir}/prompt_maker_results/prompt_maker_results_csv', exist_ok=True)\n    os.makedirs(f'{node_line_dir}/prompt_maker_results/prompt_maker_results_txt', exist_ok=True)\n\n    # Initialize best_result and best_module\n    best_result = None\n    best_module = None\n\n    # Initialize generator module\n    if \"generator\" in strategies:\n        generator = strategies[\"generator\"]\n    else:\n        generator = default_generator\n\n    # Initialize speed_threshold\n    if \"speed_threshold\" in strategies:\n        speed_threshold = strategies[\"speed_threshold\"]\n    else:\n        speed_threshold = None\n\n    # Initialize evaluation_metrics\n    if \"evaluation_metrics\" in strategies:\n        evaluation_metrics = strategies[\"evaluation_metrics\"]\n    else:\n        evaluation_metrics = None\n\n    # Initialize evaluation_metrics_thresholds\n    if \"evaluation_metrics_thresholds\" in strategies:\n        evaluation_metrics_thresholds = strategies[\"evaluation_metrics_thresholds\"]\n    else:\n        evaluation_metrics_thresholds = None\n\n    # Initialize evaluation_metrics_thresholds_comparison_operators\n    if \"evaluation_metrics_thresholds_comparison_operators\" in strategies:\n        evaluation_metrics_thresholds_comparison_operators = strategies[\"evaluation_metrics_thresholds_comparison_operators\"]\n    else:\n        evaluation_metrics_thresholds_comparison_operators = None\n\n    # Initialize evaluation_metrics_weights\n    if \"evaluation_metrics_weights\" in strategies:\n        evaluation_met"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create directories\n    node_dir = os.path.join(node_line_dir, 'prompt_maker_node')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Initialize variables\n    best_result = None\n    best_module = None\n    best_params = None\n    best_time = float('inf')\n    best_eval_metrics = None\n\n    # Iterate over modules and parameters\n    for module, params in zip(modules, module_params):\n        # Create module directory\n        module_dir = os.path.join(node_dir, module.__name__)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Run module\n        start_time = time.time()\n        result = module(**params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Evaluate module if specified\n        if 'eval_metrics' in strategies:\n            eval_metrics = evaluate_prompt_maker(result, strategies['eval_metrics'])\n        else:\n            eval_metrics = None\n\n        # Save result\n        result_path = os.path.join(module_dir, 'result.csv')\n        result.to_csv(result_path, index=False)\n\n        # Save execution time\n        time_path = os.path.join(module_dir, 'time.txt')\n        with open(time_path, 'w') as f:\n            f.write(str(execution_time))\n\n        # Save evaluation metrics if available\n        if eval_metrics is not None:\n            eval_path = os.path.join(module_dir, 'eval.txt')\n            with open(eval_path, 'w') as f:\n                f.write(str(eval_metrics))\n\n        # Update best result if applicable\n        if execution_time < best_time:\n            best_result = result\n            best_module = module\n            best_params = params\n            best_time = execution_time\n            best_eval_met"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    create_necessary_directories(node_line_dir)\n\n    # Initialize variables\n    best_prompt_maker = None\n    best_prompt_maker_output = None\n    best_prompt_maker_execution_time = float('inf')\n    best_prompt_maker_index = None\n    best_prompt_maker_metrics = None\n\n    # Iterate over prompt maker modules\n    for i, (module, params) in enumerate(zip(modules, module_params)):\n        # Run prompt maker module\n        start_time = time.time()\n        prompt_maker_output = module(**params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Evaluate prompt maker module based on specified strategies\n        if 'speed_threshold' in strategies and execution_time > strategies['speed_threshold']:\n            continue\n\n        if 'generator_module' in strategies:\n            generator_module = strategies['generator_module']\n        else:\n            generator_module = default_generator_module\n\n        if 'metrics' in strategies:\n            metrics = strategies['metrics']\n        else:\n            metrics = default_metrics\n\n        generator_module_params = strategies.get('generator_module_params', {})\n        evaluation_results = evaluate_prompt_maker_module(prompt_maker_output, generator_module, metrics, **generator_module_params)\n\n        # Select best prompt maker module\n        if 'best_prompt_maker_selection_strategy' in strategies:\n            if strategies['best_prompt_maker_selection_strategy'] == 'min_execution_time':\n                if execution_time < best_prompt_maker_execution_time:\n                    best_prompt_maker = module\n                    best_prompt_maker_output = prompt_maker_output\n                    best_prompt_maker_execution_time = execution_time\n                    best_prompt_maker_index = i\n                    best_prompt_maker_metrics = evaluation_results\n            elif strategies['best_prompt_maker_selection"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker_outputs'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker_outputs', 'prompt_maker_results'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker_outputs', 'prompt_maker_results', 'summary'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker_outputs', 'prompt_maker_results', 'summary', 'evaluation_results'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker_outputs', 'prompt_maker_results', 'summary', 'speed_results'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker_outputs', 'prompt_maker_results', 'summary', 'generator_results'), exist_ok=True)\n\n    # Initialize variables\n    best_module = None\n    best_params = None\n    best_result = None\n    best_time = float('inf')\n    best_eval_result = None\n    best_speed_result = None\n    best_generator_result = None\n\n    # Loop through each prompt maker module and its parameters\n    for module, params in zip(modules, module_params):\n        # Create a directory for the current prompt maker module\n        module_dir = os.path.join(node_line_dir, 'prompt_maker_outputs', 'prompt_maker_results', module.__name__)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Initialize the prompt maker module\n        prompt_maker = module(**params)\n\n        # Initialize the generator module if specified"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize variables\n    best_module_index = None\n    best_module_result = None\n    best_module_time = None\n    best_module_name = None\n    best_module_params = None\n    best_module_evaluation = None\n\n    # Iterate over each module and its parameters\n    for module_index, (module, params) in enumerate(zip(modules, module_params)):\n\n        # Set the generator module based on the strategy\n        if 'generator_module' in strategies:\n            generator_module = strategies['generator_module']\n        else:\n            generator_module = default_generator_module\n\n        # Create a directory for the current module\n        module_dir = os.path.join(node_line_dir, f\"module_{module_index}\")\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Run the module and measure execution time\n        start_time = time.time()\n        module_result = module(**params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Save the module's result to a CSV file\n        module_result.to_csv(os.path.join(module_dir, \"result.csv\"), index=False)\n\n        # Evaluate the module's performance\n        evaluation_result = generator_module(module_result)\n\n        # Save the evaluation result to a CSV file\n        evaluation_result.to_csv(os.path.join(module_dir, \"evaluation.csv\"), index=False)\n\n        # Check if the module's execution time is within the speed threshold\n        if 'speed_threshold' in strategies and execution_time > strategies['speed_threshold']:\n            continue\n\n        # Check if the module's evaluation result meets the specified metrics\n        if 'metrics' in strategies:\n            if not all(evaluation_result[metric] >= strategies['metrics'][metric] for metric in strategies['metrics']"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create a directory for the node's output\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Initialize variables to track the best prompt maker and its performance\n    best_prompt_maker = None\n    best_performance = float('inf')\n    best_performance_index = None\n\n    # Initialize a list to store the performance of each prompt maker\n    performance_list = []\n\n    # Iterate through each prompt maker module\n    for i, (module, params) in enumerate(zip(modules, module_params)):\n        # Create a directory for the current prompt maker\n        module_dir = os.path.join(node_dir, f'{module.__name__}')\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Initialize variables to track the performance of the current prompt maker\n        performance = float('inf')\n        performance_index = None\n\n        # Initialize a list to store the execution times of the current prompt maker\n        execution_times = []\n\n        # Run the prompt maker module with the given parameters\n        start_time = time.time()\n        result = module(**params)\n        end_time = time.time()\n\n        # Calculate the execution time of the current prompt maker\n        execution_time = end_time - start_time\n        execution_times.append(execution_time)\n\n        # Evaluate the performance of the current prompt maker based on the specified strategies\n        if 'metrics' in strategies:\n            performance = evaluate_performance(result, strategies['metrics'])\n        if 'speed_threshold' in strategies:\n            if execution_time > strategies['speed_threshold']:\n                performance = float('inf')\n\n        # Update the best prompt maker and its performance if necessary\n        if performance < best_performance:\n            best_performance = performance\n            best_performance_index = i\n            best_prompt_maker = module\n\n        # Add the performance of the current prompt maker to the list\n        performance_list.append(performance"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize variables\n    best_module = None\n    best_result = None\n    best_time = float('inf')\n    best_score = float('-inf')\n\n    # Iterate over modules and parameters\n    for module, params in zip(modules, module_params):\n        # Run the module with the given parameters\n        result, time = run_prompt_maker_module(module, params)\n\n        # Evaluate the module's performance based on specified strategies\n        if 'metrics' in strategies:\n            score = evaluate_performance(result, strategies['metrics'])\n        else:\n            score = 0\n\n        # Update the best module and result if the current module meets the speed threshold and has the best performance\n        if time <= strategies.get('speed_threshold', float('inf')) and score > best_score:\n            best_module = module\n            best_result = result\n            best_time = time\n            best_score = score\n\n    # Save the best result to a CSV file\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n\n    # Create a summary of the node's execution\n    summary = {\n        'best_module': best_module.__name__,\n        'best_time': best_time,\n        'best_score': best_score,\n    }\n\n    # Save the summary to a JSON file\n    with open(os.path.join(node_line_dir, 'summary.json'), 'w') as f:\n        json.dump(summary, f)\n\n    # Combine the previous result with the best result\n    combined_result = pd.concat([previous_result, best_result], axis=1)\n\n    # Save the combined result to a CSV file\n    combined_result.to_csv(os.path.join(node_line_dir, 'combined_result.csv'), index=False)\n\n    return combined_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Initialize variables\n    best_module = None\n    best_result = None\n    best_time = float('inf')\n    best_metrics = None\n\n    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(f\"{node_line_dir}/prompt_maker_results\", exist_ok=True)\n    os.makedirs(f\"{node_line_dir}/prompt_maker_results/summaries\", exist_ok=True)\n    os.makedirs(f\"{node_line_dir}/prompt_maker_results/results\", exist_ok=True)\n\n    # Iterate over prompt maker modules\n    for module, params in zip(modules, module_params):\n        # Get the module name\n        module_name = module.__name__.split('.')[-1]\n\n        # Run the prompt maker module\n        start_time = time.time()\n        result = module(**params)\n        end_time = time.time()\n\n        # Calculate execution time\n        execution_time = end_time - start_time\n\n        # Save the result\n        result.to_csv(f\"{node_line_dir}/prompt_maker_results/results/{module_name}_result.csv\", index=False)\n\n        # Evaluate the result using the specified generator module\n        generator_module = strategies.get('generator_module', default_generator_module)\n        metrics = generator_module(result)\n\n        # Save the summary\n        summary = {\n            'module_name': module_name,\n            'execution_time': execution_time,\n            'metrics': metrics\n        }\n        with open(f\"{node_line_dir}/prompt_maker_results/summaries/{module_name}_summary.json\", 'w') as f:\n            json.dump(summary, f)\n\n        # Update best module if applicable\n        if execution_time <= strategies.get('speed_threshold', float('inf')) and metrics >= best_metrics:\n            best_module = module_name\n           "}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize the best result and best module\n    best_result = None\n    best_module = None\n\n    # Initialize the best result and best module\n    best_result = None\n    best_module = None\n\n    # Initialize the summary dataframe\n    summary_df = pd.DataFrame(columns=['module', 'params', 'time', 'evaluation_metrics'])\n\n    # Iterate over the modules and their parameters\n    for module, params in zip(modules, module_params):\n        # Create a subdirectory for the module\n        module_dir = os.path.join(node_line_dir, module.__name__)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Initialize the module's result\n        result = None\n\n        # Run the module with the given parameters\n        start_time = time.time()\n        result = module(**params)\n        end_time = time.time()\n\n        # Calculate the execution time\n        execution_time = end_time - start_time\n\n        # Evaluate the module's result using the specified strategies\n        evaluation_metrics = evaluate_result(result, strategies)\n\n        # Save the module's result and evaluation metrics to the summary dataframe\n        summary_df = summary_df.append({'module': module.__name__, 'params': params, 'time': execution_time, 'evaluation_metrics': evaluation_metrics}, ignore_index=True)\n\n        # Check if the module's result is better than the current best result\n        if best_result is None or evaluation_metrics['score'] > best_evaluation_metrics['score']:\n            best_result = result\n            best_module = module\n            best_evaluation_metrics = evaluation_metrics\n\n    # Save the best result to the node's output directory\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n\n    # Save"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # create necessary directories\n    create_necessary_directories(node_line_dir)\n\n    # initialize variables\n    best_module = None\n    best_module_params = None\n    best_module_result = None\n    best_module_time = float('inf')\n    best_module_eval = None\n    best_module_eval_time = None\n\n    # iterate over prompt maker modules\n    for module, params in zip(modules, module_params):\n        # run prompt maker module\n        result, time = run_prompt_maker_module(module, params, node_line_dir)\n\n        # check if prompt maker module is within speed threshold\n        if time <= strategies['speed_threshold']:\n            # evaluate prompt maker module\n            eval_result, eval_time = evaluate_prompt_maker_module(module, params, strategies['generator_module'], strategies['metrics'], node_line_dir)\n\n            # update best prompt maker module if necessary\n            if eval_result > best_module_eval:\n                best_module = module\n                best_module_params = params\n                best_module_result = result\n                best_module_time = time\n                best_module_eval = eval_result\n                best_module_eval_time = eval_time\n\n    # save best prompt maker module's result\n    if best_module_result is not None:\n        save_result(best_module_result, best_module_time, best_module_eval, best_module_eval_time, node_line_dir)\n\n        # combine best prompt maker module's result with previous operation's result\n        combined_result = pd.concat([previous_result, best_module_result], axis=1)\n        return combined_result\n    else:\n        return previous_result\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    node_dir = os.path.join(node_line_dir, 'node_outputs')\n    os.makedirs(node_dir, exist_ok=True)\n    node_output_dir = os.path.join(node_dir, 'node_output')\n    os.makedirs(node_output_dir, exist_ok=True)\n\n    # Initialize variables\n    best_module = None\n    best_params = None\n    best_result = None\n    best_time = float('inf')\n    best_eval = float('-inf')\n\n    # Iterate over modules and parameters\n    for module, params in zip(modules, module_params):\n        # Create module directory\n        module_dir = os.path.join(node_dir, module.__name__)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Run module with parameters\n        start_time = time.time()\n        result = module(**params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Evaluate module performance\n        if 'evaluation_metric' in strategies:\n            eval_metric = strategies['evaluation_metric']\n            if eval_metric == 'mean_similarity':\n                evaluator = MeanSimilarityEvaluator()\n            elif eval_metric == 'bert_score':\n                evaluator = BertScoreEvaluator()\n            else:\n                raise ValueError(f\"Unsupported evaluation metric: {eval_metric}\")\n\n            evaluator.evaluate(result, previous_result)\n            module_eval = evaluator.get_score()\n        else:\n            module_eval = None\n\n        # Update best module if applicable\n        if execution_time <= strategies['speed_threshold'] and (module_eval is None or module_eval > best_eval):\n            best_module = module\n            best_params = params\n            best_result = result\n            best_time = execution_time\n            best_eval = module_eval\n\n    # Save best module's result\n    best"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary subdirectories in the node directory\n    create_necessary_directories(node_line_dir)\n\n    # Initialize variables for best prompt maker module and its result\n    best_prompt_maker_module = None\n    best_prompt_maker_result = None\n\n    # Initialize a dictionary to store execution times for each prompt maker module\n    execution_times = {}\n\n    # Iterate over each prompt maker module\n    for prompt_maker_module, params in zip(modules, module_params):\n        # Start measuring execution time for the current prompt maker module\n        start_time = time.time()\n\n        # Run the current prompt maker module with the given parameters\n        prompt_maker_result = prompt_maker_module(**params)\n\n        # End measuring execution time for the current prompt maker module\n        end_time = time.time()\n\n        # Calculate the execution time for the current prompt maker module\n        execution_time = end_time - start_time\n\n        # Store the execution time for the current prompt maker module\n        execution_times[prompt_maker_module.__name__] = execution_time\n\n        # Check if the current prompt maker module meets the speed threshold\n        if 'speed_threshold' in strategies and execution_time > strategies['speed_threshold']:\n            # Skip the current prompt maker module if it exceeds the speed threshold\n            continue\n\n        # Check if the current prompt maker module has a generator module specified\n        if 'generator_module' in strategies:\n            # Use the specified generator module for evaluation\n            generator_module = strategies['generator_module']\n        else:\n            # Use the default generator module for evaluation\n            generator_module = generator_module\n\n        # Evaluate the current prompt maker module using the specified or default generator module\n        evaluation_result = generator_module(prompt_maker_result, **strategies['generator_module_params'])\n\n        # Check if the current prompt maker module has the best evaluation result so far\n        if best_prompt_maker_result is None or evaluation_result > best_prompt_maker_result:\n            # Update the best prompt maker module and its result\n            best_prom"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize variables to track the best result and the corresponding module\n    best_result = None\n    best_module = None\n\n    # Loop through each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Run the module with the specified parameters\n        result = module(**params)\n\n        # Evaluate the result using the specified evaluation metrics\n        evaluation_metrics = evaluate_result(result, strategies[\"evaluation_metrics\"])\n\n        # Check if the result meets the speed threshold\n        if strategies[\"speed_threshold\"] is not None:\n            if \"speed\" not in evaluation_metrics or evaluation_metrics[\"speed\"] > strategies[\"speed_threshold\"]:\n                continue\n\n        # If this is the first result or the current result is better than the best result so far, update the best result and module\n        if best_result is None or evaluation_metrics[\"score\"] > strategies[\"evaluation_metrics\"](best_result)[\"score\"]:\n            best_result = result\n            best_module = module\n\n    # If no module passed the speed threshold, use the default generator module\n    if best_result is None:\n        default_generator = strategies[\"default_generator\"]\n        best_result = default_generator(**params)\n        best_module = default_generator\n\n    # Save the results to a CSV file in the node's output directory\n    best_result.to_csv(os.path.join(node_line_dir, \"result.csv\"), index=False)\n\n    # Evaluate the best result using the specified evaluation metrics\n    best_evaluation_metrics = evaluate_result(best_result, strategies[\"evaluation_metrics\"])\n\n    # Save the evaluation metrics to a JSON file in the node's output directory\n    with open(os.path.join(node_line_dir, \"evaluation_metrics.json\"), \"w\") as f:\n        json.dump(best_evaluation_metrics, f)"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    create_necessary_directories(node_line_dir)\n\n    # Initialize variables\n    best_result = None\n    best_module = None\n    best_params = None\n    best_time = float('inf')\n    best_eval = float('-inf')\n\n    # Iterate over each module\n    for i, module in enumerate(modules):\n        module_name = module.__name__\n\n        # Create a directory for the current module\n        module_dir = os.path.join(node_line_dir, module_name)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Get parameters for the current module\n        params = module_params[i]\n\n        # Run the module\n        result, time = run_module(module, params, module_dir)\n\n        # Evaluate the module\n        if 'generator' in strategies:\n            generator_params = strategies['generator']\n            generator_module = generator_params['module']\n            generator_params = generator_params['params']\n            eval_result = evaluate_module(generator_module, result, generator_params)\n        else:\n            eval_result = None\n\n        # Update best result if necessary\n        if time <= strategies['speed_threshold'] and eval_result > best_eval:\n            best_result = result\n            best_module = module\n            best_params = params\n            best_time = time\n            best_eval = eval_result\n\n    # Save the best result\n    if best_result is not None:\n        best_module_name = best_module.__name__\n        best_result.to_csv(os.path.join(node_line_dir, best_module_name, 'result.csv'), index=False)\n\n    # Save the summary\n    summary = {\n        'best_module': best_module.__name__ if best_module is not None else None,\n        'best_params': best_params,\n        'best_time': best_time,\n        'best_eval': best_eval\n    }\n    with open(os.path."}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create directories for storing results and summaries\n    node_line_dir = os.path.join(node_line_dir, \"prompt_maker\")\n    os.makedirs(node_line_dir, exist_ok=True)\n    result_dir = os.path.join(node_line_dir, \"results\")\n    os.makedirs(result_dir, exist_ok=True)\n    summary_dir = os.path.join(node_line_dir, \"summaries\")\n    os.makedirs(summary_dir, exist_ok=True)\n\n    # Initialize variables for tracking best performance\n    best_performance = float('inf')\n    best_module = None\n    best_module_params = None\n\n    # Initialize variables for tracking execution times\n    execution_times = []\n\n    # Initialize generator module for evaluation purposes\n    generator_module = strategies.get(\"generator_module\", default_generator_module)\n\n    # Iterate over each prompt maker module\n    for module, params in zip(modules, module_params):\n        # Run the prompt maker module\n        result, execution_time = run_prompt_maker_module(module, params, previous_result)\n\n        # Evaluate the performance of the prompt maker module\n        performance = evaluate_prompt_maker_performance(result, generator_module)\n\n        # Update the best performance and module if a better performance is achieved\n        if performance < best_performance:\n            best_performance = performance\n            best_module = module\n            best_module_params = params\n\n        # Add the execution time to the list\n        execution_times.append(execution_time)\n\n        # Save the result to a file\n        result_file = os.path.join(result_dir, f\"{module.__name__}.csv\")\n        result.to_csv(result_file, index=False)\n\n    # Check if the speed threshold is exceeded\n    if any(execution_time > strategies[\"speed_threshold\"] for execution_time in execution_times):\n        # If the speed threshold is exceeded, run the best prompt maker module with the best parameters\n        best"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    node_dir = os.path.join(node_line_dir, \"node\")\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Initialize best_result and best_module\n    best_result = None\n    best_module = None\n\n    # Initialize speed_threshold and generator_module\n    speed_threshold = strategies.get(\"speed_threshold\", 1000)\n    generator_module = strategies.get(\"generator_module\", None)\n\n    # Initialize evaluation_metrics\n    evaluation_metrics = strategies.get(\"evaluation_metrics\", {})\n\n    # Iterate through modules and module_params\n    for module, params in zip(modules, module_params):\n        # Run the module and get the result\n        result = module(**params)\n\n        # Check if the result meets the speed threshold\n        if result[\"execution_time\"] > speed_threshold:\n            continue\n\n        # Evaluate the result using the generator module\n        evaluation_result = evaluate_prompt_maker_result(result, generator_module, evaluation_metrics)\n\n        # Check if the evaluation result is better than the current best result\n        if best_result is None or evaluation_result[\"score\"] > best_result[\"score\"]:\n            best_result = evaluation_result\n            best_module = module\n\n    # Save the best result\n    best_result.to_csv(os.path.join(node_dir, \"best_result.csv\"))\n\n    # Combine the previous result and the best result\n    combined_result = pd.concat([previous_result, best_result], axis=1)\n\n    # Save the combined result\n    combined_result.to_csv(os.path.join(node_dir, \"combined_result.csv\"))\n\n    # Save the summary\n    save_summary(modules, module_params, best_module, best_result, combined_result, node_dir)\n\n    return combined_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create directories for the node's output\n    create_node_dir(node_line_dir)\n\n    # Initialize variables for tracking best results\n    best_result = None\n    best_result_eval = None\n    best_result_time = None\n    best_result_module = None\n    best_result_module_params = None\n\n    # Iterate over each prompt maker module\n    for module, params in zip(modules, module_params):\n\n        # Run the prompt maker module with the specified parameters\n        result = module(**params)\n\n        # Check if the result is not empty\n        if result is not None:\n\n            # Evaluate the result using the specified evaluation metrics\n            result_eval = evaluate_result(result, strategies['evaluation_metrics'])\n\n            # Check if the result meets the speed threshold\n            if result_eval['time'] < strategies['speed_threshold']:\n\n                # If the result is better than the previous best, update the best result variables\n                if best_result_eval is None or result_eval['score'] > best_result_eval['score']:\n                    best_result = result\n                    best_result_eval = result_eval\n                    best_result_time = result_eval['time']\n                    best_result_module = module\n                    best_result_module_params = params\n\n    # If a best result is found, save it to a file\n    if best_result is not None:\n        best_result_eval['module'] = best_result_module.__name__\n        best_result_eval['params'] = best_result_module_params\n        best_result_eval['time'] = best_result_time\n        save_result(best_result, best_result_eval, node_line_dir)\n\n    # Combine the previous result with the best result and save it to a file\n    if best_result is not None and previous_result is not None:\n        combined_result = pd.concat([previous_result, best_result], axis=0)\n        save_result(combined_result, best_result_eval, node_line_dir)\n\n    # If no"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            if key in module.module_params:\n                value = module.module_params[key]\n                if value not in values:\n                    values.append(value)\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            for param in module.module_params:\n                if param.key == key:\n                    values.append(param.value)\n    return list(set(values))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        if key in node.module_params:\n            value = node.module_params[key]\n            if value not in values:\n                values.append(value)\n    return values\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            for module_param in module.module_params:\n                if module_param.key == key:\n                    values.append(module_param.value)\n    return list(set(values))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            if key in module.module_params:\n                value = module.module_params[key]\n                if value not in values:\n                    values.append(value)\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        if key in node.module_params:\n            value = node.module_params[key]\n            if value not in values:\n                values.append(value)\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        if key in node.module_params:\n            value = node.module_params[key]\n            if value not in values:\n                values.append(value)\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        if key in node.module_params:\n            value = node.module_params[key]\n            if value not in values:\n                values.append(value)\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        module_params = node.module_params\n        if key in module_params:\n            value = module_params[key]\n            if value not in values:\n                values.append(value)\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            if module.module_params.get(key):\n                value = module.module_params.get(key)\n                if value not in values:\n                    values.append(value)\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        if key in node.module_params:\n            values.append(node.module_params[key])\n    return list(set(values))\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            if key in module.module_params:\n                value = module.module_params[key]\n                if value not in values:\n                    values.append(value)\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            if key in module.module_params:\n                value = module.module_params[key]\n                if value not in values:\n                    values.append(value)\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    # Initialize an empty list to store the extracted values\n    values = []\n\n    # Iterate over each node in the list\n    for node in nodes:\n\n        # Check if the node has the specified key in its module_param\n        if key in node.module_param:\n\n            # Append the value associated with the key to the values list\n            values.append(node.module_param[key])\n\n    # Remove duplicates from the values list using set() and convert it back to a list\n    values = list(set(values))\n\n    # Return the list of unique values\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        if node.module_params is not None:\n            if key in node.module_params:\n                value = node.module_params[key]\n                if value not in values:\n                    values.append(value)\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            if key in module.module_params:\n                value = module.module_params[key]\n                if value not in values:\n                    values.append(value)\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        if node.module_params is not None:\n            if key in node.module_params:\n                value = node.module_params[key]\n                if value not in values:\n                    values.append(value)\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            if key in module.module_params:\n                value = module.module_params[key]\n                if value not in values:\n                    values.append(value)\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            for module_param in module.module_params:\n                if module_param.key == key:\n                    if module_param.value not in values:\n                        values.append(module_param.value)\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    # Initialize an empty list to store the extracted values\n    values = []\n\n    # Iterate over the nodes\n    for node in nodes:\n        # Iterate over the modules in each node\n        for module in node.modules:\n            # Iterate over the module_params in each module\n            for module_param in module.module_params:\n                # Check if the current module_param has the specified key\n                if module_param.key == key:\n                    # Extract the value associated with the key\n                    value = module_param.value\n                    # Check if the value is not already in the list\n                    if value not in values:\n                        # Add the value to the list\n                        values.append(value)\n\n    # Return the list of unique values\n    return values\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(model_name=\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.encode(pred)\n    generation_gt_embedding = embedding_model.encode(generation_gt)\n\n    similarity_scores = cosine_similarity(\n        [pred_embedding],\n        generation_gt_embedding\n    )[0]\n\n    return max(similarity_scores)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.get_embedding(pred)\n    gt_embeddings = embedding_model.get_embeddings(generation_gt)\n    similarity_scores = [cosine_similarity(pred_embedding, gt_embedding) for gt_embedding in gt_embeddings]\n    return max(similarity_scores)\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.get_embedding(pred)\n    gt_embeddings = embedding_model.get_embedding(generation_gt)\n\n    # Compute the cosine similarity between the predicted embedding and each ground truth embedding\n    similarity_scores = np.dot(gt_embeddings, pred_embedding) / (np.linalg.norm(gt_embeddings, axis=1) * np.linalg.norm(pred_embedding))\n\n    # Find the maximum similarity score\n    max_similarity = np.max(similarity_scores)\n\n    return max_similarity"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if not embedding_model:\n        embedding_model = BaseEmbedding(model_name=\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.encode(pred)\n    generation_gt_embeddings = embedding_model.encode(generation_gt)\n\n    scores = cosine_similarity(generation_gt_embeddings, pred_embedding)\n    return np.max(scores)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.get_embedding(pred)\n    generation_gt_embeddings = embedding_model.get_embeddings(generation_gt)\n\n    similarity_scores = [\n        cosine_similarity(pred_embedding, gt_embedding)\n        for gt_embedding in generation_gt_embeddings\n    ]\n\n    return max(similarity_scores)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if not embedding_model:\n        embedding_model = BaseEmbedding(model_name=\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.encode(pred)\n    gt_embeddings = embedding_model.encode(generation_gt)\n\n    return max(cosine_similarity([pred_embedding], gt_embedding)[0][0] for gt_embedding in gt_embeddings)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(model_name_or_path='all-mpnet-base-v2')\n\n    pred_embedding = embedding_model.encode(pred)\n    gt_embeddings = embedding_model.encode(generation_gt)\n\n    similarity_scores = np.array([cosine_similarity(pred_embedding, gt_embedding) for gt_embedding in gt_embeddings])\n    max_score = np.max(similarity_scores)\n\n    return max_score\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if not embedding_model:\n        embedding_model = BaseEmbedding()\n\n    embedding_gt = embedding_model.get_embedding(generation_gt)\n    embedding_pred = embedding_model.get_embedding(pred)\n\n    cosine_similarities = cosine_similarity(embedding_gt, embedding_pred)\n\n    return np.max(cosine_similarities)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(model_name='all-mpnet-base-v2')\n\n    pred_embedding = embedding_model.encode(pred)\n    generation_gt_embedding = embedding_model.encode(generation_gt)\n    similarity = cosine_similarity(generation_gt_embedding, pred_embedding).max()\n\n    return similarity\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if not embedding_model:\n        embedding_model = BaseEmbedding(model_name='all-mpnet-base-v2')\n\n    pred_embedding = embedding_model.encode(pred)\n    gt_embeddings = embedding_model.encode(generation_gt)\n\n    scores = [cosine_similarity(pred_embedding, gt_embedding) for gt_embedding in gt_embeddings]\n    return max(scores)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.embedding(pred)\n    gt_embeddings = embedding_model.embedding(generation_gt)\n    scores = np.array([cosine_similarity(pred_embedding, gt_embedding) for gt_embedding in gt_embeddings])\n    return scores.max()\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(model_name_or_path=\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.encode(pred)\n    generation_gt_embeddings = embedding_model.encode(generation_gt)\n\n    # Compute cosine similarity between the predicted string and each ground truth string\n    cosine_similarities = cosine_similarity(\n        [pred_embedding], generation_gt_embeddings\n    ).flatten()\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return max(cosine_similarities)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(model_name_or_path='all-mpnet-base-v2')\n\n    embedding_model.embedding_model.eval()\n\n    with torch.no_grad():\n        pred_embedding = embedding_model.get_embedding(pred)\n        gt_embeddings = embedding_model.get_embedding(generation_gt)\n        scores = torch.mm(pred_embedding, gt_embeddings.transpose(0, 1))\n        max_score = torch.max(scores).item()\n\n    return max_score"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.embed_documents([pred])[0]\n    generation_gt_embeddings = embedding_model.embed_documents(generation_gt)\n\n    max_score = 0\n    for i in range(len(generation_gt)):\n        score = cosine_similarity(\n            [pred_embedding], [generation_gt_embeddings[i]]\n        )[0][0]\n        max_score = max(max_score, score)\n\n    return max_score\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.embed_documents([pred])[0]\n    generation_gt_embeddings = embedding_model.embed_documents(generation_gt)\n    scores = [cosine_similarity(pred_embedding, gt_embedding) for gt_embedding in generation_gt_embeddings]\n    return max(scores)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(model_name_or_path=\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.encode(pred)\n    generation_gt_embeddings = embedding_model.encode(generation_gt)\n    sem_score = max(cosine_similarity(generation_gt_embeddings, pred_embedding).squeeze())\n\n    return sem_score\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(model_name=\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.get_embedding(pred)\n    gt_embeddings = [embedding_model.get_embedding(gt) for gt in generation_gt]\n    scores = [cosine_similarity(pred_embedding, gt_embedding) for gt_embedding in gt_embeddings]\n    return max(scores)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.get_embedding(pred)\n    gt_embeddings = [embedding_model.get_embedding(gt) for gt in generation_gt]\n\n    return max(\n        [\n            cosine_similarity(pred_embedding, gt_embedding)\n            for gt_embedding in gt_embeddings\n        ]\n    )\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(model_name=\"all-mpnet-base-v2\")\n\n    embedding_pred = embedding_model.get_embedding(pred)\n    embeddings_gt = embedding_model.get_embedding(generation_gt)\n\n    scores = []\n    for embedding_gt in embeddings_gt:\n        score = cosine_similarity(embedding_pred, embedding_gt)\n        scores.append(score)\n\n    return max(scores)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.get_embedding(pred)\n    gt_embeddings = [embedding_model.get_embedding(gt) for gt in generation_gt]\n    similarity_scores = [cosine_similarity(pred_embedding, gt_embedding) for gt_embedding in gt_embeddings]\n    max_similarity_score = max(similarity_scores)\n\n    return max_similarity_score\n\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        import gfpgan\n    except:\n        print(\"GFPGAN not found, please install it first.\")\n        return np_image\n\n    try:\n        _, _, restored_image = gfpgan.gfpgan.restore_faces(np_image)\n        return restored_image\n    except:\n        print(\"GFPGAN failed to restore faces, returning original image.\")\n        return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    # Check if the GFPGAN face restorer is set up\n    if \"gfpgan_model\" not in st.session_state:\n        # If not, log a warning and return the original image\n        logger.warning(\"GFPGAN is not set up. Returning original image.\")\n        return np_image\n\n    # Restore faces in the image using the GFPGAN face restorer\n    _, _, restored_image = st.session_state[\"gfpgan_model\"].enhance(\n        np_image, has_aligned=False, only_center_face=False, paste_back=True\n    )\n\n    # Return the restored image\n    return restored_image\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    # Check if the GFPGAN face restorer is set up\n    if not hasattr(gfpgan_model, \"gfpgan_model\"):\n        # If not, log a warning and return the original image\n        logger.warning(\"GFPGAN face restorer not set up. Returning original image.\")\n        return np_image\n\n    # Attempt to restore faces in the image using the GFPGAN face restorer\n    try:\n        _, _, restored_image = gfpgan_model.gfpgan_model(\n            np_image,\n            num_upsampling_steps=gfpgan_model.gfpgan_steps,\n            strength=gfpgan_model.gfpgan_strength,\n        )\n        # If successful, return the restored image\n        return restored_image\n    except Exception as e:\n        # If not successful, log a warning and return the original image\n        logger.warning(\n            f\"GFPGAN face restoration failed with error: {e}. Returning original image.\"\n        )\n        return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        from gfpgan import GFPGANer\n        from basicsr.archs.rrdbnet_arch import RRDBNet\n        from realesrgan import RealESRGANer\n        from facexlib.utils.face_restore_helper import FaceRestoreHelper\n    except:\n        return np_image\n\n    # GFPGAN\n    gfpgan_model_path = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\"\n    restorer = GFPGANer(\n        model_path=gfpgan_model_path,\n        upscale=2,\n        arch=\"clean\",\n        channel_multiplier=2,\n        bg_upsampler=RRDBNet(\n            num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2\n        ),\n    )\n\n    # ESRGAN\n    esrgan_model_path = \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth\"\n    esrgan_tile = 400\n    esrgan_bg_tile = None\n    upsampler = RealESRGANer(\n        scale=2,\n        model_path=esrgan_model_path,\n        dni_weight=[0.75, 0.25],\n        model=RRDBNet(\n            num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4\n        ),\n        tile=esrgan_tile,\n        tile_pad=10,\n        pre_pad=0,\n        half=True,\n    )\n\n    # Face Restore Helper\n    face_helper = FaceRestoreHelper(\n        upscale"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    import logging\n    import torch\n    from gfpgan import GFPGANer\n\n    try:\n        # Set up the GFPGAN face restorer\n        gfpgan = GFPGANer(\n            model_path='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth',\n            upscale=2,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None\n        )\n\n        # Move the GFPGAN face restorer to the GPU if available\n        if torch.cuda.is_available():\n            gfpgan.cuda()\n\n        # Restore the faces in the image using the GFPGAN face restorer\n        _, _, restored_image = gfpgan.enhance(np_image, has_aligned=False, only_center_face=False, paste_back=True)\n\n        return restored_image\n\n    except Exception as e:\n        logging.warning(f\"GFPGAN face restoration failed: {e}\")\n        return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    from modules.gfpgan_model import gfpgan_model\n\n    # If the GFPGAN face restorer is not set up, log a warning and return the original image\n    if gfpgan_model is None:\n        return np_image\n\n    # Attempt to restore faces in the image using the GFPGAN face restorer\n    try:\n        _, _, np_image = gfpgan_model.enhance(\n            np_image, has_aligned=False, only_center_face=False, paste_back=True\n        )\n    except Exception:\n        # If the restoration fails, log a warning and return the original image\n        print(\"GFPGAN error\")\n        pass\n\n    # Return the restored image or the original image\n    return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    # Check if the GFPGAN face restorer is set up\n    if not hasattr(gfpgan_model, \"gfpgan\"):\n        # If not, log a warning and return the original image\n        logger.warning(\n            \"GFPGAN is not set up. Skipping face restoration.\"\n        )\n        return np_image\n\n    # Restore faces in the image using the GFPGAN face restorer\n    _, _, restored_image = gfpgan_model.gfpgan(\n        np_image,\n        num_upsample=1,\n        strength=0.5,\n        guidance_scale=0,\n    )\n\n    # Return the restored image\n    return restored_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        import gfpgan\n        from basicsr.utils.download_util import load_file_from_url\n        from realesrgan import RealESRGANer\n        from facexlib.utils.face_restoration_helper import FaceRestoreHelper\n\n        # Initialize face restoration helper object\n        face_restore_helper = FaceRestoreHelper(\n            upscale_factor=2,\n            face_size=512,\n            crop_ratio=(1, 1),\n            det_model='retinaface_resnet50',\n            save_ext='png',\n            use_parse=True,\n            device='cuda',\n        )\n\n        # Load GFPGAN from Nvidia's NGC\n        url = 'https://api.ngc.nvidia.com/v2/models/nvidia/research/nfnet_f64/versions/1/files/GFPGANv1.4.pth'\n        model_path = load_file_from_url(url=url)\n\n        # Initialize GFPGAN restorer\n        restorer = gfpgan.GFPGANer(\n            model_path=model_path,\n            upscale=2,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=RealESRGANer(\n                scale=2,\n                model_path='https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth',\n                tile=400,\n                tile_pad=10,\n                pre_pad=0,\n                half=True,\n            ),\n        )\n\n        # Restore the image using GFPGAN\n        _, _, restored_img = restorer.enhance(\n            np_image,\n            has_aligned=False,\n            only_center_face=False,\n            paste_back=True,\n        )\n\n        return restored_img\n\n    except Exception as e:\n        # Log a warning if the GFPG"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    from gfpgan import GFPGANer\n    from modules.shared import opts, state\n\n    # If the GFPGAN face restorer is not set up, log a warning and return the original image.\n    if opts.face_restoration_model is None:\n        state.warn(\"GFPGAN is not set up, please set it up in the settings tab.\")\n        return np_image\n\n    # Use the GFPGAN face restorer to restore faces in the image.\n    _, _, restored_image = GFPGANer(\n        model_path=opts.face_restoration_model,\n        upscale=1,\n        arch=\"clean\",\n        channel_multiplier=2,\n        bg_upsampler=None,\n    ).enhance(np_image, has_aligned=False, only_center_face=False, paste_back=True)\n\n    # Return the restored image.\n    return restored_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    from gfpgan import GFPGANer\n    import cv2\n    import numpy as np\n    import torch\n\n    # Set up the GFPGAN face restorer\n    gfpgan_model_path = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\"\n    try:\n        gfpganer = GFPGANer(\n            model_path=gfpgan_model_path,\n            upscale=2,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n    except:\n        print(\"GFPGAN model not found. Please download it from https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\")\n        return np_image\n\n    # Restore faces in the image\n    try:\n        _, _, restored_image = gfpganer.enhance(\n            np_image, has_aligned=False, only_center_face=False, paste_back=True\n        )\n    except:\n        print(\"GFPGAN failed to restore faces in the image.\")\n        return np_image\n\n    # Convert the restored image to a NumPy array\n    restored_image = cv2.cvtColor(restored_image, cv2.COLOR_RGB2BGR)\n    restored_image = np.array(restored_image)\n\n    # Return the restored image\n    return restored_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    # Import the necessary modules\n    import numpy as np\n    from PIL import Image\n    from gfpgan import GFPGANer\n    import torch\n\n    # Check if the GFPGAN face restorer is set up\n    if not hasattr(gfpgan_fix_faces, \"restorer\"):\n        gfpgan_fix_faces.restorer = None\n\n    # If the GFPGAN face restorer is not set up, log a warning and return the original image\n    if gfpgan_fix_faces.restorer is None:\n        print(\n            \"WARNING: GFPGAN face restorer not set up. Returning original image.\"\n        )\n        return np_image\n\n    # Convert the NumPy array to a PIL image\n    pil_image = Image.fromarray(np_image)\n\n    # Restore the faces in the PIL image using the GFPGAN face restorer\n    _, _, restored_image = gfpgan_fix_faces.restorer.enhance(\n        pil_image, has_aligned=False, only_center_face=False, paste_back=True\n    )\n\n    # Convert the restored PIL image to a NumPy array\n    np_image = np.array(restored_image)\n\n    # Return the restored image as a NumPy array\n    return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    import logging\n    import numpy as np\n    from modules.gfpgan_model import gfpgan_model\n\n    if gfpgan_model is None:\n        logging.warning(\n            \"GFPGAN face restorer is not available. Face restoration is not possible.\")\n        return np_image\n\n    _, _, restored_image = gfpgan_model.enhance(np_image, has_aligned=False, only_center_face=False, paste_back=True)\n\n    return restored_image\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    # Check if the GFPGAN face restorer is set up\n    if not hasattr(gfpgan_model, \"gfpgan\"):\n        # Log a warning if the GFPGAN face restorer is not set up\n        logging.warning(\"GFPGAN face restorer not set up. Skipping face restoration.\")\n        # Return the original image\n        return np_image\n\n    # Attempt to restore faces in the image using the GFPGAN face restorer\n    try:\n        # Create a copy of the original image\n        restored_image = np_image.copy()\n        # Restore faces in the copy of the image using the GFPGAN face restorer\n        restored_image, _, _ = gfpgan_model.gfpgan.enhance(\n            restored_image, has_aligned=False, only_center_face=False, paste_back=True\n        )\n        # Return the restored image\n        return restored_image\n    except Exception as e:\n        # Log a warning if there is an error during face restoration\n        logging.warning(f\"GFPGAN face restoration failed: {e}\")\n        # Return the original image\n        return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    # Import the necessary modules\n    import cv2\n    import numpy as np\n    import torch\n    from gfpgan import GFPGANer\n    from modules import shared\n\n    # Check if the GFPGAN face restorer is set up\n    if not shared.cmd_opts.gfpgan_dir:\n        # If not, log a warning and return the original image\n        logging.warning(\"GFPGAN not set up, using original image\")\n        return np_image\n\n    # Set up the GFPGAN face restorer\n    gfpgan = GFPGANer(\n        model_path=shared.cmd_opts.gfpgan_dir,\n        upscale=1,\n        arch=\"clean\",\n        channel_multiplier=2,\n        bg_upsampler=None,\n    )\n\n    # Move the GFPGAN face restorer to the GPU if available\n    if torch.cuda.is_available():\n        gfpgan.cuda()\n\n    # Restore faces in the image\n    _, _, restored_img = gfpgan.enhance(np_image, has_aligned=False, only_center_face=False, paste_back=True)\n\n    # Return the restored image\n    return restored_img\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    import logging\n    import numpy as np\n    import torch\n\n    from gfpgan import GFPGANer\n    from modules.paths import script_path\n\n    logger = logging.getLogger(__name__)\n\n    if not torch.cuda.is_available():\n        logger.warning(\"GFPGAN: No CUDA device found. Face restoration will be skipped.\")\n        return np_image\n\n    # Initialize the GFPGAN face restorer\n    gfpgan = GFPGANer(\n        model_path=script_path / \"gfpgan\" / \"GFPGANv1.4.pth\",\n        upscale=2,\n        arch=\"clean\",\n        channel_multiplier=2,\n        bg_upsampler=None,\n    )\n\n    # Restore faces in the image\n    _, _, np_image = gfpgan.enhance(\n        np_image,\n        has_aligned=False,\n        only_center_face=False,\n        paste_back=True,\n    )\n\n    # Return the restored image\n    return np_image\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    import torch\n    from gfpgan import GFPGANer\n    from modules.shared import opts, state\n\n    if opts.gfpgan_model is None:\n        return np_image\n\n    # TODO: This is a hack. We should be able to do this with a single global variable.\n    if torch.cuda.is_available():\n        gfpgan = GFPGANer(\n            model_path=opts.gfpgan_model,\n            upscale=1,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None)\n    else:\n        gfpgan = GFPGANer(\n            model_path=opts.gfpgan_model,\n            upscale=1,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None,\n            device=torch.device('cpu'))\n\n    _, _, restored_img = gfpgan.enhance(np_image, has_aligned=False, only_center_face=False, paste_back=True)\n\n    if restored_img is not None:\n        if opts.gfpgan_fix_faces:\n            # update grid after restoring each face\n            state.job_count = state.job_count + 1\n            state.sampling_steps = state.sampling_steps + 1\n\n    return restored_img"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        import gfpgan\n        from basicsr.utils.download_util import load_file_from_url\n        from modules.upscaler import Upscaler\n        from modules.paths import models_path\n        from modules.shared import opts\n\n        model_url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\"\n        model_path = models_path / \"gfpgan\" / \"GFPGANv1.3.pth\"\n        if not model_path.exists():\n            model_path.parent.mkdir(exist_ok=True, parents=True)\n            load_file_from_url(url=model_url, file_path=model_path)\n\n        upscaler = Upscaler(\"GFPGAN\", None)\n        upscaler.path = model_path\n        upscaler.model = gfpgan.GFPGANer(\n            model_path=upscaler.path,\n            upscale=1,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=upscaler.bg_upsampler,\n        )\n\n        _, _, np_image = upscaler.upscale(np_image, np_image)\n    except Exception as e:\n        opts.logging.warning(\n            f\"GFPGAN face restorer failed to restore faces. This is expected if you are using the free version of the tool. Please consider supporting the project on https://github.com/sponsors/d8ahazard to unlock this feature. Error message: {e}\"\n        )\n\n    return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    import logging\n    import torch\n    import cv2\n    import numpy as np\n    from gfpgan import GFPGANer\n\n    # Initialize the GFPGAN face restorer\n    gfpgan = GFPGANer(\n        model_path='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth',\n        upscale=2,\n        arch='clean',\n        channel_multiplier=2,\n        bg_upsampler=None\n    )\n\n    # Set the GFPGAN face restorer to use the GPU if it is available\n    if torch.cuda.is_available():\n        gfpgan.cuda()\n\n    # Set the GFPGAN face restorer to use the CPU if it is not available\n    else:\n        gfpgan.cpu()\n\n    # Set the GFPGAN face restorer to use a 32-bit floating point precision\n    gfpgan.fp32()\n\n    # Restore faces in the image using the GFPGAN face restorer\n    cropped_faces, restored_faces, restored_img = gfpgan.enhance(\n        np_image,\n        has_aligned=False,\n        only_center_face=False,\n        paste_back=True,\n        weight=0.5\n    )\n\n    # Log a warning if the GFPGAN face restorer was not able to restore any faces in the image\n    if len(restored_faces) == 0:\n        logging.warning(\"GFPGAN was unable to restore any faces in the image.\")\n\n    # Return the original image if the GFPGAN face restorer was not able to restore any faces in the image\n    if len(restored_faces) == 0:\n        return np_image\n\n    # Return the restored image if the GFPGAN face restorer was able to restore at least one face in the image\n    return restored_img\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    from modules.gfpgan_model import gfpgan_model\n\n    # Check if the GFPGAN face restorer is set up\n    if gfpgan_model is None:\n        # If the GFPGAN face restorer is not set up, log a warning and return the original image\n        logger.warning(\n            \"GFPGAN face restorer is not set up. Returning original image.\"\n        )\n        return np_image\n\n    # Attempt to restore faces in the image using the GFPGAN face restorer\n    try:\n        # Convert the image from a NumPy array to a PIL image\n        img = Image.fromarray(np_image)\n        # Restore faces in the image using the GFPGAN face restorer\n        _, _, output = gfpgan_model.enhance(\n            np.array(img), has_aligned=False, only_center_face=False, paste_back=True\n        )\n        # Convert the restored image from a NumPy array to a PIL image\n        output = Image.fromarray(output)\n        # Convert the restored image from a PIL image to a NumPy array\n        np_image = np.array(output)\n    except Exception as e:\n        # If an error occurs while attempting to restore faces in the image, log a warning and return the original image\n        logger.warning(\n            f\"GFPGAN face restorer failed to restore faces in the image. Returning original image. Error: {e}\"\n        )\n\n    # Return the image as a NumPy array\n    return np_image\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    from gfpgan import GFPGANer\n    from modules.shared import opts, cmd_opts, state\n    from modules import images\n    import cv2\n    import numpy as np\n    import os\n    import logging\n\n    # Check if the GFPGAN face restorer is set up\n    if not \"gfpgan_model\" in state.sd_model.sd_checkpoint_info.filename:\n        logging.warning(\"GFPGAN is not used in the current model, so faces won't be restored\")\n        return np_image\n\n    # Check if the GFPGAN face restorer is already loaded\n    if \"gfpgan_model\" not in state.sd_model.sd_checkpoint_info.filename:\n        # Load the GFPGAN face restorer\n        state.sd_model.reload_model_weights(\n            state.sd_model.sd_checkpoint_info.filename,\n            state.sd_model.sd_checkpoint,\n            is_half=not cmd_opts.no_half,\n        )\n        state.sd_model.gfpgan = GFPGANer(\n            model_path=opts.gfpgan_model,\n            upscale=1,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n\n    # Restore the faces in the image using the GFPGAN face restorer\n    _, _, restored_image = state.sd_model.gfpgan.enhance(\n        np_image,\n        has_aligned=False,\n        only_center_face=False,\n        paste_back=True,\n    )\n\n    # Return the restored image\n    return restored_image\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        print(f\"Error in setup_model: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    from modules.upscaler import Upscaler\n\n    try:\n        Upscaler(dirname)\n    except Exception as e:\n        print(f\"Error during setup: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from basicsr.archs.codeformer_arch import CodeFormerModel\n        from realesrgan import RealESRGANer\n        from facexlib.utils.face_restoration_helper import FaceRestorerCodeFormer\n        from gfpgan import GFPGANer\n\n        model = CodeFormerModel(\n            model_path=dirname,\n            upscale=2,\n            arch=\"CodeFormer\",\n            channel_multiplier=2,\n            bg_upsampler=RealESRGANer(\n                scale=2,\n                model_path=\"https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/RealESRGAN_x2plus.pth\",\n                tile=400,\n                tile_pad=10,\n                pre_pad=0,\n                half=True,\n            ),\n        )\n        # face restoration helper\n        face_restorer = FaceRestorerCodeFormer(\n            upscale_factor=2,\n            face_size=512,\n            crop_ratio=(1, 1),\n            det_model=\"retinaface_resnet50\",\n            save_ext=\"png\",\n            use_parse=True,\n            device=\"cuda\",\n            model=model,\n            # bg_upsampler=RealESRGANer(\n            #     scale=2,\n            #     model_path=\"https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/RealESRGAN_x2plus.pth\",\n            #     tile=400,\n            #     tile_pad=10,\n            #     pre_pad=0,\n            #     half=True,\n            # ),\n            bg_tile=400,\n        )\n        face_restorers.append(face_restorer)\n        if \"GFPGAN\" in dirname:\n            bg_upsampler = RealESRGANer(\n                scale=2,\n                model_path=\"https://github."}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from modules.face_restoration_codeformer import FaceRestorerCodeFormer\n\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        print(f\"Error setting up face restoration: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from basicsr.archs.codeformer_arch import CodeFormerModel\n        from realesrgan import RealESRGANer\n        from facexlib.utils.face_restoration_helper import FaceRestorerCodeFormer\n\n        global face_restorers\n\n        model = CodeFormerModel(\n            dim_embd=512,\n            codebook_size=1024,\n            n_head=8,\n            n_layers=9,\n            connect_list=['32', '64', '128', '256'],\n        )\n        model_path = os.path.join(dirname, 'CodeFormer.pth')\n        if not os.path.isfile(model_path):\n            raise FileNotFoundError(f'Model file not found: {model_path}')\n        model.load_state_dict(torch.load(model_path)['params_ema'])\n        model = model.to(device)\n        model.eval()\n\n        face_restorer = FaceRestorerCodeFormer(\n            upscale_factor=1,\n            face_size=512,\n            crop_ratio=(1, 1),\n            det_model='retinaface_resnet50',\n            save_ext='png',\n            use_parse=True,\n            device=device,\n            model_path=dirname,\n        )\n        face_restorer.model = model\n        face_restorer.gfpgan = RealESRGANer(\n            scale=1,\n            model_path=os.path.join(dirname, 'GFPGAN.pth'),\n            model=None,\n            tile=0,\n            tile_pad=10,\n            pre_pad=0,\n            half=True)\n        face_restorers.append(face_restorer)\n    except Exception as error:\n        print(f'\\tFailed to setup CodeFormer: {error}')\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from gfpgan import GFPGANer\n        from basicsr.archs.rrdbnet_arch import RRDBNet\n        from realesrgan import RealESRGANer\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n        from basicsr.utils.download_util import load_file_from_url\n\n        model = RRDBNet(\n            num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4\n        )\n        url = \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth\"\n        model_path = load_file_from_url(\n            url=url, model_dir=\"weights/realesr-gan\", progress=True, file_name=None\n        )\n        netscale = 4\n        bg_upsampler = RealESRGANer(\n            scale=netscale,\n            model_path=model_path,\n            model=model,\n            tile=0,\n            tile_pad=10,\n            pre_pad=0,\n            half=True,\n        )  # need to set False in CPU mode\n        # determine model paths\n        model_name = dirname.split(\"/\")[-1]\n        model_path = f\"weights/facelib/{model_name}/detection_Resnet50_Final.pth\"\n        if not os.path.isfile(model_path):\n            model_path = f\"weights/facelib/{model_name}/detection_Resnet50_Final.onnx\"\n        if not os.path.isfile(model_path):\n            raise FileNotFoundError(f\"Cannot find model in {dirname}.\")\n\n        # initialize FaceRestorer\n        face_restorer = FaceRestorerCodeFormer(\n            model_name=model_name,\n            model_path=model_path"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from modules.face_restoration_codeformer import FaceRestorerCodeFormer\n\n        global face_restorers\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n\n    except Exception as e:\n        print(f\"Error during setup: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global face_restorers\n        if dirname.startswith(\"http\"):\n            face_restorers.append(\n                FaceRestorerCodeFormer(dirname, sess_options={\"action\": \"download\"})\n            )\n        else:\n            face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as error:\n        print(f\"Error setting up {dirname}: {error}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from . import facerestorer\n        facerestorer.face_restorers.append(facerestorer.FaceRestorerCodeFormer(dirname))\n    except Exception:\n        import traceback\n        print(\"Error in setting up the model:\", file=sys.stderr)\n        print(traceback.format_exc(), file=sys.stderr)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from modules.face_restoration_codeformer import FaceRestorerCodeFormer\n        global face_restorers\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        print(f\"Error setting up face restoration: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from modules.face_restoration import FaceRestorerCodeFormer\n\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n\n    except Exception:\n        import traceback\n\n        print(\"Error in setup_model:\", traceback.format_exc())\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global face_restorers\n        restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(restorer)\n        return\n    except Exception as e:\n        report(f\"Failed to setup {dirname} - {e}\", \"error\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from face_restorer_codeformer import FaceRestorerCodeFormer\n\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        print(f\"Error setting up CodeFormer model: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    from . import face_restoration\n\n    try:\n        face_restoration.face_restorers.append(\n            face_restoration.FaceRestorerCodeFormer(dirname)\n        )\n    except Exception:\n        print(\"Error setting up the model\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global face_restorers\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        print(\"Error during setup:\", e)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        report(f\"Error in setup_model: {e}\", \"error\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    from modules.face_restoration_codeformer import FaceRestorerCodeFormer\n\n    try:\n        model = FaceRestorerCodeFormer(dirname)\n        g.face_restorers.append(model)\n    except Exception as e:\n        print(f\"Error in setup_model: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        report(f\"An error occurred during setup: {e}\", \"error\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from face_restoration.face_restorer import FaceRestorerCodeFormer\n\n        global face_restorers\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as error:\n        print(f\"Error during setup: {error}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from face_restorer import FaceRestorerCodeFormer\n\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as error:\n        print(f\"Error setting up face restorer: {error}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import os\n    import sys\n    import torch\n    from basicsr.utils.download_util import load_file_from_url\n    from gfpgan import GFPGANer\n    from facexlib.utils.face_restore_helper import FaceRestoreHelper\n\n    url = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth'\n    model_path = os.path.join(dirname, 'GFPGANv1.3.pth')\n    if not os.path.isfile(model_path):\n        model_path = load_file_from_url(url=url, model_dir=dirname, progress=True, file_name=None)\n    restorer = GFPGANer(\n        model_path=model_path,\n        upscale=2,\n        arch='clean',\n        channel_multiplier=2,\n        bg_upsampler=None)\n\n    # ------------------------ set up the background upsampler ------------------------\n    if bg_upsampler_type == 'realesrgan':\n        if torch.cuda.is_available():\n            # torch.cuda.set_device(0)\n            bg_upsampler = RealESRGANer(\n                scale=2,\n                model_path='https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth',\n                model='RealESRGAN_x2plus',\n                tile=400,\n                tile_pad=10,\n                pre_pad=0,\n                half=True)  # need to set False in CPU mode\n        else:\n            bg_upsampler = RealESRGANer(\n                scale=2,\n                model_path='https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth',\n               "}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        from gfpgan import GFPGANer\n\n        model = GFPGANer(\n            model_path=dirname,\n            upscale=2,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n        _, _, _ = model.restore_faces(None)\n        print(f\"Successfully setup GFPGAN in {dirname}\")\n    except Exception as e:\n        import traceback\n        import sys\n\n        print(\"Failed to setup GFPGAN:\", file=sys.stderr)\n        print(traceback.format_exc(), file=sys.stderr)\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        from gfpgan import GFPGANer\n        from basicsr.archs.rrdbnet_arch import RRDBNet\n\n        from realesrgan import RealESRGANer\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact_deblur\n\n        from facexlib.utils.face_restore_helper import FaceRestoreHelper\n\n        # Patch the facexlib with the given directory\n        FaceRestoreHelper.face_restore_helper.face_det = None\n        FaceRestoreHelper.face_restore_helper.face_restore = None\n        FaceRestoreHelper.face_restore_helper.face_restore = GFPGANer(\n            model_path=dirname,\n            upscale=2,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=RealESRGANer(\n                model_path=f\"{dirname}/RealESRGAN_x2plus.pth\",\n                model=SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=2, act_type=\"prelu\"),\n                tile=400,\n                tile_pad=10,\n                pre_pad=0,\n                half=True,\n            ),\n            bg_tile=400,\n            version=\"1.3\",\n            model=RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2),\n        )\n\n    except Exception as e:\n        import traceback\n        print('Failed to setup GFPGAN:', e)\n        print(traceback.format_exc())"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import os\n    from basicsr.utils.download_util import load_file_from_url\n    from gfpgan import GFPGANer\n    from facexlib.utils.face_restoration_helper import FaceRestoreHelper\n\n    url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\"\n    model_path = os.path.join(dirname, \"GFPGANv1.3.pth\")\n    if not os.path.isfile(model_path):\n        model_path = load_file_from_url(url=url, model_dir=dirname, progress=True, file_name=None)\n    restorer = GFPGANer(\n        model_path=model_path,\n        upscale=2,\n        arch=\"clean\",\n        channel_multiplier=2,\n        bg_upsampler=None,\n    )\n    # initialize face helper\n    face_helper = FaceRestoreHelper(\n        upscale_factor=2,\n        face_size=512,\n        crop_ratio=(1, 1),\n        det_model=\"retinaface_resnet50\",\n        save_ext=\"png\",\n        use_parse=True,\n        device=\"cuda\",\n    )\n    # update face helper\n    face_helper.face_det.detect_model.detect_kwargs[\"upsample_scale\"] = 2.0\n    face_helper.face_det.detect_model.detect_kwargs[\"input_size\"] = 640\n    face_helper.face_restore = restorer\n    face_helper.face_restore.upscale = 2\n    face_helper.face_restore.tile_pad = 10\n    face_helper.face_restore.pre_pad = 0\n    face_helper.face_restore.threshold = 0.6\n    face_helper.face_restore.det_size = 640\n    face_helper.face_restore.save"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        from gfpgan import GFPGANer\n        from basicsr.archs.rrdbnet_arch import RRDBNet\n        from realesrgan import RealESRGANer\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n        from facexlib.utils.face_restore_helper import FaceRestoreHelper\n        from facexlib.parsing import parsing_helper\n        import torch\n        import os\n        import sys\n\n        # Patch the facexlib with the given directory\n        sys.modules[\"gfpgan\"] = __import__(\"gfpgan\")\n        sys.modules[\"gfpgan\"].__dict__[\"GFPGANer\"] = GFPGANer\n        sys.modules[\"gfpgan\"].__dict__[\"RRDBNet\"] = RRDBNet\n        sys.modules[\"realesrgan\"] = __import__(\"realesrgan\")\n        sys.modules[\"realesrgan\"].__dict__[\"RealESRGANer\"] = RealESRGANer\n        sys.modules[\"realesrgan\"].__dict__[\"SRVGGNetCompact\"] = SRVGGNetCompact\n        sys.modules[\"facexlib\"] = __import__(\"facexlib\")\n        sys.modules[\"facexlib\"].__dict__[\"FaceRestoreHelper\"] = FaceRestoreHelper\n        sys.modules[\"facexlib\"].__dict__[\"parsing_helper\"] = parsing_helper\n\n        # Initialize the GFPGAN face restorer with the model located in the specified directory\n        restorer = GFPGANer(\n            model_path=os.path.join(dirname, \"GFPGANv1.4.pth\"),\n            upscale=2,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n\n        # Set the GFPGAN face restorer as an attribute of the FaceRestoreHelper class\n        FaceRestoreHelper.face_restorers[\"GFPGANer\"] = restorer\n\n    except Exception as e:\n        # Report any exceptions that"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import os\n    from basicsr.utils.download_util import load_file_from_url\n\n    from gfpgan import GFPGANer\n\n    url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\"\n    model_path = os.path.join(dirname, \"GFPGANv1.3.pth\")\n\n    if not os.path.isfile(model_path):\n        try:\n            load_file_from_url(url, model_dir=dirname)\n        except Exception as e:\n            print(f\"Error downloading GFPGAN model from {url}: {e}\")\n\n    try:\n        restorer = GFPGANer(\n            model_path=model_path,\n            upscale=2,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n    except Exception as e:\n        print(f\"Error initializing GFPGAN face restorer: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import os\n    import sys\n    import torch\n    from facexlib.utils.face_restore_helper import FaceRestoreHelper\n    from gfpgan import GFPGANer\n    from torch.hub import download_url_to_file\n    from basicsr.utils.download_util import load_file_from_url\n\n    # Patch the facexlib with the given directory\n    sys.path.insert(0, dirname)\n\n    # Initialize the GFPGAN face restorer with the model located in the specified directory\n    try:\n        model_path = os.path.join(dirname, 'GFPGANCleanv1-NoCE-C2.pth')\n        if not os.path.isfile(model_path):\n            for model_name in ['GFPGANv1.3.pth', 'GFPGANCleanv1-NoCE-C2.pth']:\n                load_file_from_url(\n                    url='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/' + model_name,\n                    model_dir=dirname,\n                    progress=True,\n                    file_name=model_name)\n        restorer = GFPGANer(\n            model_path=model_path,\n            upscale=2,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None)\n        # Create the face restore helper object\n        face_restore_helper = FaceRestoreHelper(\n            upscale_factor=2,\n            face_size=512,\n            crop_ratio=(1, 1),\n            det_model='retinaface_resnet50',\n            save_ext='png',\n            use_parse=True,\n            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n    except Exception as e:\n        # Report any exceptions that occur during the setup process\n        print(f'Failed to setup GFPGAN: {e}')"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import os\n    from facexlib.utils.face_restore_helper import FaceRestoreHelper\n    from gfpgan import GFPGANer\n    from modules import errors\n\n    # Patch the facexlib with the given directory\n    try:\n        FaceRestoreHelper.patch_facexlib(dirname)\n    except Exception as e:\n        errors.display(e, f\"Error patching facexlib with directory {dirname}\")\n\n    # Initialize the GFPGAN face restorer\n    try:\n        model_path = os.path.join(dirname, \"GFPGANv1.4.pth\")\n        restorer = GFPGANer(\n            model_path=model_path,\n            upscale=2,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n    except Exception as e:\n        errors.display(e, f\"Error initializing GFPGAN face restorer with model {model_path}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import os\n    import sys\n    import shutil\n    import cv2\n    import torch\n    import numpy as np\n    import facexlib\n    from basicsr.utils.download_util import load_file_from_url\n    from facexlib.utils.face_restore_helper import FaceRestoreHelper\n    from gfpgan import GFPGANer\n\n    url = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth'\n    model_path = os.path.join(dirname, 'GFPGANv1.3.pth')\n    if os.path.exists(model_path):\n        print('GFPGAN model already exists.')\n    else:\n        print('Downloading GFPGAN model...')\n        load_file_from_url(url, model_dir=dirname)\n\n    # Replace facexlib with the given directory\n    facexlib_path = os.path.join(dirname, 'facexlib')\n    if os.path.exists(facexlib_path):\n        shutil.rmtree(facexlib_path)\n    shutil.copytree(dirname, facexlib_path)\n\n    # Patch facexlib with the given directory\n    sys.modules['facexlib'] = facexlib\n    facexlib.utils.face_restore_helper.face_restore_helper = FaceRestoreHelper(\n        upscale_factor=1, face_size=512, crop_ratio=(1, 1), det_model='retinaface_resnet50', save_ext='png', use_parse=True, device='cuda')\n\n    # Initialize the GFPGAN face restorer\n    restorer = GFPGANer(\n        model_path=model_path,\n        upscale=1,\n        arch='clean',\n        channel_multiplier=2,\n        bg_upsampler=None)\n\n    # Handle exceptions that occur during setup\n    try:"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        import os\n        import sys\n        import torch\n        import numpy as np\n        import cv2\n\n        from gfpgan import GFPGANer\n        from basicsr.archs.rrdbnet_arch import RRDBNet\n        from basicsr.utils.download_util import load_file_from_url\n        from basicsr.utils import img2tensor, tensor2img\n\n        from modules.upscaler import upscale_image\n        from modules.images import save_image\n        from modules.paths import models_path\n\n        # Check and download model\n        model_path = os.path.join(models_path, 'GFPGAN', dirname)\n        model_url = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/'\n\n        if not os.path.isfile(os.path.join(model_path, 'GFPGANv1.3.pth')):\n            model_path = os.path.join(models_path, 'GFPGAN')\n            if not os.path.isfile(os.path.join(model_path, 'GFPGANv1.3.pth')):\n                os.makedirs(model_path, exist_ok=True)\n                load_file_from_url(url=model_url + 'GFPGANv1.3.pth', model_dir=model_path, progress=True, file_name=None)\n            model_path = os.path.join(models_path, 'GFPGAN', dirname)\n            os.makedirs(model_path, exist_ok=True)\n            load_file_from_url(url=model_url + 'GFPGANv1.3.pth', model_dir=model_path, progress=True, file_name='GFPGANv1.3.pth')\n\n        # Patch facexlib\n        sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), 'facexlib')))"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import os\n    import sys\n    import torch\n    from basicsr.utils.download_util import load_file_from_url\n    from facexlib.utils.face_restoration_helper import FaceRestoreHelper\n\n    try:\n        import lpips\n    except ImportError:\n        import pip\n        pip.main(['install', 'lpips'])\n\n    url = 'https://github.com/xinntao/facexlib/releases/download/v0.2.5/detection_Resnet50_Final.pth'\n    model_path = load_file_from_url(url=url, model_dir=dirname, progress=True, file_name=None)\n    net = torch.load(model_path, map_location='cpu')\n    net = net.eval()\n    net = net.to('cuda')\n\n    # ------------------------ set up GFPGAN ------------------------\n    if not os.path.isfile(os.path.join(dirname, 'GFPGANv1.3.pth')):\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth'\n        model_path = load_file_from_url(url=url, model_dir=dirname, progress=True, file_name='GFPGANv1.3.pth')\n\n    restorer = FaceRestoreHelper(\n        upscale_factor=1, face_size=512, model_path=os.path.join(dirname, 'GFPGANv1.3.pth'), net='cpu',\n        use_sr=True, sr_model='realesrnet', sr_scale=2, sr_tile=512, sr_tile_pad=10,\n        det_model='retinaface_resnet50', save_ext='png', use_fp16=True,\n        gfpgan_model_path=os.path.join(dirname, '"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        import sys\n        import os\n        import torch\n        from basicsr.utils.download_util import load_file_from_url\n        from facexlib.utils.face_restore_helper import FaceRestoreHelper\n\n        # Check if the directory exists\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n        # Check if the model file exists in the directory\n        model_url = \"https://github.com/xinntao/facexlib/releases/download/v0.2.2/detection_Resnet50_Final.pth\"\n        model_path = os.path.join(dirname, \"detection_Resnet50_Final.pth\")\n        if not os.path.exists(model_path):\n            # Download the model file from the URL and save it to the directory\n            load_file_from_url(url=model_url, model_dir=dirname, progress=True, file_name=None)\n\n        # Patch the facexlib\n        sys.path.append(os.path.abspath(dirname))\n        from facexlib.parsing import ParsingNet\n\n        # Initialize the GFPGAN face restorer\n        restorer = FaceRestoreHelper(\n            upscale_factor=2,\n            face_size=512,\n            crop_ratio=(1, 1),\n            det_model=\"retinaface_resnet50\",\n            save_ext=\"png\",\n            use_parse=True,\n            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n        )\n\n        # Set the GFPGAN model path\n        restorer.load_face_restore_helpers(\n            det_model_path=None,  # Set to None to use the default model path\n            model_path=model_path,\n            bg_upsampler_name=\"realesrgan\",\n            bg_tile=400,\n            version=\"1.3\",\n            up"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        import sys\n        import os\n        import torch\n        import cv2\n        import numpy as np\n        from basicsr.utils.download_util import load_file_from_url\n        from facexlib.utils.face_restore_helper import FaceRestoreHelper\n        from gfpgan import GFPGANer\n\n        # patch facexlib\n        facexlib_path = os.path.join(dirname, \"facexlib\")\n        sys.path.append(facexlib_path)\n\n        # initialize face restore helper\n        face_restore_helper = FaceRestoreHelper(\n            upscale_factor=2,\n            face_size=512,\n            crop_ratio=(1, 1),\n            det_model=\"retinaface_resnet50\",\n            save_ext=\"png\",\n            use_parse=True,\n            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n        )\n\n        # initialize gfpgan restorer\n        url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\"\n        model_path = load_file_from_url(\n            url=url, model_dir=\"pretrained_models\", progress=True, file_name=None\n        )\n        restorer = GFPGANer(\n            model_path=model_path,\n            upscale=2,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n        restorer.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n        restorer.eval()\n\n    except Exception as e:\n        print(\"Error occurred while setting up the model:\", e)\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        import sys\n        import os\n        import torch\n        from basicsr.utils.download_util import load_file_from_url\n        from facexlib.utils.face_restore_helper import FaceRestoreHelper\n\n        # Patch the facexlib with the given directory\n        sys.path.insert(0, dirname)\n\n        # Initialize the GFPGAN face restorer\n        url = 'https://github.com/xinntao/facexlib/releases/download/v0.2.5/detection_Resnet50_Final.pth'\n        model_path = load_file_from_url(url=url, model_dir=os.path.join(dirname, 'weights'), progress=True, file_name=None)\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        face_helper = FaceRestoreHelper(upscale_factor=1, face_size=512, crop_ratio=(1, 1), det_model='retinaface_resnet50', save_ext='png', device=device)\n        face_helper.clean_all()\n        face_helper.read_face_det_model(model_path)\n\n    except Exception as e:\n        print(f\"Error occurred while setting up the GFPGAN model: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        import facexlib\n\n        from gfpgan import GFPGANer\n\n        url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\"\n        model_path = facexlib.utils.download_pretrained_models(url, dirname)\n\n        restorer = GFPGANer(\n            model_path=model_path,\n            upscale=2,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n\n        # initialize face helper\n        facexlib.utils.face_helper.FaceRestoreHelper.clear()\n        facexlib.utils.face_helper.FaceRestoreHelper.add_face_restore_helper(\n            \"GFPGAN\", restorer\n        )\n\n    except Exception:\n        import traceback\n        import sys\n\n        print(\"Error in setting up the GFPGAN:\", file=sys.stderr)\n        print(traceback.format_exc(), file=sys.stderr)\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import logging\n    import os\n    import sys\n    import torch\n    import torch.hub\n    import torch.nn as nn\n    from basicsr.utils.download_util import load_file_from_url\n\n    from modules.upscaler import Upscaler, UpscalerData\n    from modules.restorers import GFPGANer\n\n    class GFPGANv1(nn.Module):\n        \"\"\"\n        Args:\n            use_gpu (bool): Whether to use GPU.\n            device (torch.device): Device to use for inference.\n            model_path (str): Path to the pre-trained GFPGAN model.\n        \"\"\"\n\n        def __init__(self, use_gpu=True, device=None, model_path=None):\n            super().__init__()\n            # ------------------------ load GFPGAN for face restoration ------------------------\n            self.use_gpu = use_gpu\n            self.device = device\n\n            # initialize model\n            arch = 'clean'\n            channel_multiplier = 2\n            model = torch.hub.load(\n                \"TencentARC/GFPGAN\",\n                \"GFPGANv1\",\n                model_name=\"GFPGANv1\",\n                pretrained=\"celebs\",\n                channel_multiplier=channel_multiplier,\n                bg_upsampler=arch,\n                device=self.device,\n                model_root=model_path,\n            )\n            self.gfpgan = GFPGANer(\n                model,\n                None,\n                [0, 1, 2, 3],\n                [128, 256, 512, 1024],\n                0.6,\n                0.6,\n                1,\n                0.7,\n                1,\n                4,\n            )\n\n        def enhance(self, img, has_aligned=False, only_center_face=False, paste_back=True):\n            # ------------------------ restore ------------------------\n            # from [0, 1] to [-"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        from gfpgan import GFPGANer\n\n        import sys\n        import os\n        import torch\n\n        # Patch the facexlib with the given directory\n        sys.path.insert(0, dirname)\n\n        # Initialize the GFPGAN face restorer with the model located in the specified directory\n        restorer = GFPGANer(\n            model_path=os.path.join(dirname, 'experiments/pretrained_models/GFPGANv1.4.pth'),\n            upscale=2,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None)\n\n        # Move the GFPGAN face restorer to the GPU if available\n        if torch.cuda.is_available():\n            restorer = restorer.cuda()\n\n        # Print a message indicating that the GFPGAN face restorer is ready\n        print('GFPGAN face restorer is ready')\n\n    except Exception as e:\n        # Print an error message if an exception occurs during the setup process\n        print('Error during setup:', e)\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        import sys\n        import os\n        import torch\n        from basicsr.utils.download_util import load_file_from_url\n        from facexlib.utils.face_restore_helper import FaceRestoreHelper\n\n        # Patch the facexlib with the given directory\n        sys.path.insert(0, dirname)\n\n        # Initialize the GFPGAN face restorer\n        restorer = FaceRestoreHelper(\n            512,\n            512,\n            3,\n            5,\n            0.5,\n            0.5,\n            weight_path=os.path.join(dirname, 'weights', 'GFPGANv1.4.pth'),\n            model='original',\n            use_gpu=torch.cuda.is_available(),\n            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n        )\n\n        # Report that the model was successfully set up\n        print(\"GFPGAN model setup complete.\")\n\n    except Exception as e:\n        # Report any exceptions that occur during the setup process\n        print(\"GFPGAN model setup failed:\", e)\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    from facexlib.utils import load_file_from_url\n    from gfpgan import GFPGANer\n    import torch\n    from basicsr.utils.download_util import load_file_from_url\n\n    url = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth'\n    model_path = load_file_from_url(url=url, model_dir=dirname, progress=True, file_name=None)\n    restorer = GFPGANer(\n        model_path=model_path,\n        upscale=2,\n        arch='clean',\n        channel_multiplier=2,\n        bg_upsampler=None)\n\n    try:\n        restorer.model.eval()\n        restorer.model = restorer.model.to(torch.device('cuda'))\n    except Exception as e:\n        print(f\"Error setting up GFPGAN: {e}\")\n        raise e\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    from facexlib.utils.face_restore_helper import FaceRestoreHelper\n    from facexlib.utils.misc import set_device\n    from gfpgan import GFPGANer\n    import os\n    import sys\n\n    # Set the device to GPU if available\n    set_device(\"cuda\")\n\n    # Patch the facexlib with the given directory\n    sys.path.insert(0, dirname)\n    from basicsr.utils.download_util import load_file_from_url\n\n    # Initialize the GFPGAN face restorer with the model located in the specified directory\n    url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\"\n    model_path = load_file_from_url(\n        url=url, model_dir=os.path.join(dirname, \"experiments/pretrained_models\"), progress=True, file_name=None\n    )\n    restorer = GFPGANer(\n        model_path=model_path,\n        upscale=2,\n        arch=\"clean\",\n        channel_multiplier=2,\n        bg_upsampler=None,\n    )\n\n    # Set the GFPGAN face restorer to the face restore helper\n    FaceRestoreHelper.face_restorers[\"GFPGANer\"] = restorer\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  v_quat = [0, v[0], v[1], v[2]]\n\n  # Perform the rotation\n  rotated_quat = quat_mult(quat_mult(q, v_quat), quat_conjugate(q))\n\n  # Convert the rotated quaternion back to a vector format\n  rotated_v = [rotated_quat[1], rotated_quat[2], rotated_quat[3]]\n\n  return rotated_v\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  v_quat = np.array([0, v[0], v[1], v[2]])\n\n  # Apply the rotation using quaternion multiplication\n  rotated_quat = quaternion_multiply(quaternion_multiply(q, v_quat), quaternion_conjugate(q))\n\n  # Convert the rotated quaternion back to a vector format\n  rotated_v = rotated_quat[1:]\n\n  return rotated_v\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  v = np.append(0, v)\n\n  # Perform the rotation\n  rotated_v = quaternion_multiply(quaternion_multiply(q, v), quaternion_conjugate(q))\n\n  # Convert the rotated vector back to a vector format\n  rotated_v = rotated_v[1:]\n\n  return rotated_v\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  v_quat = [0, v[0], v[1], v[2]]\n\n  # Apply the rotation using quaternion multiplication\n  rotated_quat = quaternion_multiply(quaternion_multiply(q, v_quat), quaternion_conjugate(q))\n\n  # Convert the rotated quaternion back to a vector format\n  rotated_v = [rotated_quat[1], rotated_quat[2], rotated_quat[3]]\n\n  return rotated_v\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector into a quaternion format\n  v_quat = [0, v[0], v[1], v[2]]\n\n  # Apply the rotation using quaternion multiplication\n  rotated_quat = quat_mult(quat_mult(q, v_quat), quat_conjugate(q))\n\n  # Convert the rotated quaternion back to a vector format\n  rotated_v = [rotated_quat[1], rotated_quat[2], rotated_quat[3]]\n\n  return rotated_v\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  v_q = [0, v[0], v[1], v[2]]\n\n  # Apply the rotation using quaternion multiplication\n  rotated_q = quaternion_multiply(quaternion_multiply(q, v_q), quaternion_conjugate(q))\n\n  # Convert the rotated quaternion back to a vector format\n  rotated_v = [rotated_q[1], rotated_q[2], rotated_q[3]]\n\n  return rotated_v\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  v = np.array([0, v[0], v[1], v[2]])\n\n  # Apply the rotation using quaternion multiplication\n  q_inv = np.array([q[0], -q[1], -q[2], -q[3]])\n  q_rot = np.array([0, v[1], v[2], v[3]])\n  q_rot = quaternion_multiply(q_rot, q_inv)\n  q_rot = quaternion_multiply(q, q_rot)\n\n  # Convert the rotated quaternion back to a vector format\n  v_rot = np.array([q_rot[1], q_rot[2], q_rot[3]])\n\n  return v_rot\n\n"}
{"namespace": "quaternion.rotate", "completion": "  v = np.array([0, v[0], v[1], v[2]])\n  q_inv = np.array([q[0], -q[1], -q[2], -q[3]])\n  v_rot = quat_mult(quat_mult(q, v), q_inv)\n  return v_rot[1:]\n"}
{"namespace": "quaternion.rotate", "completion": "  q = np.array(q)\n  v = np.array(v)\n\n  qv = np.array([0, v[0], v[1], v[2]])\n\n  qv = qmult(qmult(q, qv), qconj(q))\n\n  return qv[1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  v_quat = np.append(0, v)\n\n  # Apply the quaternion rotation\n  rotated_quat = quat_multiply(quat_multiply(q, v_quat), quat_conjugate(q))\n\n  # Convert the quaternion back to a vector format\n  rotated_v = rotated_quat[1:]\n\n  return rotated_v\n\n"}
{"namespace": "quaternion.rotate", "completion": "  v = np.append(v, 0)\n  q_conj = np.array([-q[0], -q[1], -q[2], q[3]])\n  q_v = np.append(v, 0)\n  q_res = quat_mul(quat_mul(q, q_v), q_conj)\n  return q_res[:3]\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion\n  v_quat = np.array([0, v[0], v[1], v[2]])\n\n  # Perform the quaternion multiplication\n  rotated_quat = quaternion_multiply(quaternion_multiply(q, v_quat), quaternion_conjugate(q))\n\n  # Convert the rotated quaternion back to a vector\n  rotated_v = rotated_quat[1:]\n\n  return rotated_v\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  v = np.append(0, v)\n\n  # Perform the quaternion rotation\n  q_inv = np.append(q[0], -q[1:])\n  q_rotated = quaternion_multiply(quaternion_multiply(q, v), q_inv)\n\n  # Convert the quaternion back to a vector format\n  return q_rotated[1:]\n\n\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  v = np.array([0, v[0], v[1], v[2]])\n\n  # Apply the rotation using quaternion multiplication\n  qv = quat_mult(q, v)\n  q_conj = np.array([q[0], -q[1], -q[2], -q[3]])\n  qv = quat_mult(qv, q_conj)\n\n  # Convert the rotated quaternion back to a vector format\n  v_rotated = qv[1:]\n\n  return v_rotated\n\n\n"}
{"namespace": "quaternion.rotate", "completion": "  v = np.append(0, v)\n  v = np.matmul(quaternion_product(quaternion_product(q, v), quaternion_conjugate(q)), 1)\n  return v[1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  v = np.array([0, v[0], v[1], v[2]])\n  q = np.array([q[0], q[1], q[2], q[3]])\n  q_conj = np.array([q[0], -q[1], -q[2], -q[3]])\n  qvq_conj = np.multiply(np.multiply(q, v), q_conj)\n  return qvq_conj[1:4]\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector into a quaternion format\n  v = np.array([0, v[0], v[1], v[2]])\n\n  # Apply the rotation\n  q_conj = np.array([q[0], -q[1], -q[2], -q[3]])\n  q_rotated = quat_mult(quat_mult(q, v), q_conj)\n\n  # Convert the rotated quaternion back to a vector format\n  v_rotated = q_rotated[1:]\n\n  return v_rotated\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  v = np.append(0, v)\n\n  # Perform the rotation using quaternion multiplication\n  q_inv = q_conjugate(q)\n  q_v = quaternion_multiply(q, v)\n  q_v = quaternion_multiply(q_v, q_inv)\n\n  # Convert the rotated quaternion back to a vector format\n  v_rotated = q_v[1:]\n\n  return v_rotated\n\n"}
{"namespace": "quaternion.rotate", "completion": "  v = np.array(v)\n  q = np.array(q)\n\n  v_quat = np.append(0, v)\n  v_rotated = quat_mult(quat_mult(q, v_quat), quat_conjugate(q))\n  v_rotated = v_rotated[1:]\n\n  return v_rotated\n"}
{"namespace": "quaternion.rotate", "completion": "  v = np.array(v)\n  q = np.array(q)\n  q = q / np.sqrt(np.sum(q**2))\n  v = np.hstack((0, v))\n  v = np.dot(qinv(q), np.dot(v, q))\n  return v[1:]\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle)\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n  if angle < eps:\n    return jnp.array([0.0, 0.0, 0.0, 1.0])\n  else:\n    axis = axis_angle / angle\n    return jnp.concatenate([jnp.sin(angle / 2) * axis, jnp.cos(angle / 2)])\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis, angle = axis_angle[:3], axis_angle[3]\n  axis = axis / jnp.linalg.norm(axis)\n  half_angle = angle / 2.0\n  return jnp.array(\n      [\n          *jnp.sin(half_angle) * axis,\n          jnp.cos(half_angle),\n      ],\n      dtype=axis_angle.dtype,\n  )"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Extract the axis and angle from the axis-angle vector\n  axis, angle = axis_angle[:3], axis_angle[3]\n\n  # Normalize the axis\n  axis = axis / jnp.linalg.norm(axis)\n\n  # Compute the sine and cosine of half the angle\n  half_angle = angle / 2\n  sin_half_angle = jnp.sin(half_angle)\n  cos_half_angle = jnp.cos(half_angle)\n\n  # Compute the quaternion components\n  qw = cos_half_angle\n  qx = sin_half_angle * axis[0]\n  qy = sin_half_angle * axis[1]\n  qz = sin_half_angle * axis[2]\n\n  # Ensure numerical stability for small angles\n  if jnp.abs(sin_half_angle) < eps:\n    qx = qy = qz = 0.0\n  elif jnp.abs(cos_half_angle) < eps:\n    qw = 1.0\n\n  # Return the quaternion\n  return jnp.array([qw, qx, qy, qz])\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle)\n  assert axis_angle.shape[-1] == 3\n\n  axis_angle_norm = jnp.linalg.norm(axis_angle, axis=-1, keepdims=True)\n  axis_angle_normalized = axis_angle / jnp.maximum(axis_angle_norm, eps)\n  angle = axis_angle_norm[..., 0]\n  axis_angle_normalized = axis_angle_normalized * jnp.sin(angle / 2)[..., None]\n\n  c = jnp.cos(angle / 2)\n  q = jnp.concatenate([c[..., None], axis_angle_normalized], axis=-1)\n  return q\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Check if the axis_angle is a 3-vector\n  if len(axis_angle) != 3:\n    raise ValueError(\"The input axis_angle must be a 3-vector.\")\n\n  # Calculate the angle from the axis_angle vector\n  angle = jnp.linalg.norm(axis_angle)\n\n  # If the angle is close to zero, use a small epsilon value to avoid division by zero\n  if angle < eps:\n    axis = axis_angle / eps\n    angle = eps\n  else:\n    axis = axis_angle / angle\n\n  # Calculate the quaternion from the axis and angle\n  q = jnp.concatenate([jnp.cos(angle / 2), axis * jnp.sin(angle / 2)])\n\n  return q\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle)\n  axis_angle_norm = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / axis_angle_norm\n  angle = axis_angle_norm\n\n  # Handle small angles\n  if angle < eps:\n    return jnp.array([0.0, 0.0, 0.0, 1.0])\n\n  # Calculate the quaternion\n  half_angle = angle / 2.0\n  sin_half_angle = jnp.sin(half_angle)\n  cos_half_angle = jnp.cos(half_angle)\n  quat = jnp.concatenate([axis * sin_half_angle, [cos_half_angle]])\n\n  return quat\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (angle + eps)\n  quat = jnp.concatenate([[jnp.cos(angle / 2.0)], axis * jnp.sin(angle / 2.0)])\n  return quat\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Extract the axis and angle from the input axis-angle vector\n  axis, angle = axis_angle[:3], axis_angle[3]\n\n  # Calculate the sin and cos of the half angle\n  half_angle = angle / 2\n  sin_half_angle = jnp.sin(half_angle)\n  cos_half_angle = jnp.cos(half_angle)\n\n  # Calculate the quaternion components using the axis and sin and cos of the half angle\n  qw = cos_half_angle\n  qx = sin_half_angle * axis[0]\n  qy = sin_half_angle * axis[1]\n  qz = sin_half_angle * axis[2]\n\n  # Ensure numerical stability for small angles by using a small epsilon value\n  if jnp.abs(sin_half_angle) < eps:\n    qx = qy = qz = 0.0\n  else:\n    qx /= sin_half_angle\n    qy /= sin_half_angle\n    qz /= sin_half_angle\n\n  # Return the quaternion as a 4-vector\n  return jnp.array([qw, qx, qy, qz])\n\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Normalize the axis of rotation\n  axis_angle = jnp.asarray(axis_angle, dtype=jnp.float32)\n  axis_angle_norm = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (axis_angle_norm + eps)\n\n  # Calculate the angle of rotation\n  angle = axis_angle_norm\n\n  # Convert the angle to a quaternion\n  q = jnp.zeros(4, dtype=jnp.float32)\n  q = q.at[0].set(jnp.cos(angle / 2))\n  q = q.at[1:].set(axis * jnp.sin(angle / 2))\n\n  return q\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Unpack the axis-angle vector\n  axis, angle = axis_angle[..., :3], axis_angle[..., 3:]\n\n  # Compute the sine and cosine of the half angle\n  half = angle * 0.5\n  s, c = jnp.sin(half), jnp.cos(half)\n\n  # Normalize the axis vector\n  axis_normalized = axis / jnp.linalg.norm(axis + eps, axis=-1, keepdims=True)\n\n  # Construct the quaternion\n  quat = jnp.concatenate([c, s * axis_normalized], axis=-1)\n\n  return quat\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Extract the axis and angle from the axis-angle vector\n  axis, angle = axis_angle[..., :3], axis_angle[..., 3:]\n\n  # Compute the sine and cosine of half the angle\n  half = angle * 0.5\n  s, c = jnp.sin(half), jnp.cos(half)\n\n  # Normalize the axis and compute the quaternion components\n  axis = axis / jnp.linalg.norm(axis, axis=-1, keepdims=True)\n  w, x, y, z = c, s * axis[..., 0], s * axis[..., 1], s * axis[..., 2]\n\n  # Handle the case where the angle is close to zero\n  mask = jnp.abs(angle) < eps\n  w = jnp.where(mask, 1.0, w)\n  x = jnp.where(mask, 0.0, x)\n  y = jnp.where(mask, 0.0, y)\n  z = jnp.where(mask, 0.0, z)\n\n  # Concatenate the quaternion components and return the quaternion\n  return jnp.concatenate([w, x, y, z], axis=-1)\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Extract the angle from the axis-angle vector\n  angle = jnp.linalg.norm(axis_angle)\n\n  # If the angle is close to zero, use a small epsilon value to avoid division by zero\n  if angle < eps:\n    axis = axis_angle / eps\n    angle = eps\n  else:\n    axis = axis_angle / angle\n\n  # Calculate the quaternion components using the axis and angle\n  sin_half_angle = jnp.sin(angle / 2)\n  cos_half_angle = jnp.cos(angle / 2)\n  q = jnp.concatenate([cos_half_angle, sin_half_angle * axis])\n\n  return q\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Extract the angle and axis from the axis-angle vector\n  angle = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (angle + eps)\n\n  # Calculate the quaternion components using the angle and axis\n  qw = jnp.cos(angle / 2)\n  qx, qy, qz = jnp.sin(angle / 2) * axis\n\n  # Return the quaternion as a 4-vector\n  return jnp.array([qw, qx, qy, qz])\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Unpack the axis-angle vector\n  a0, a1, a2 = axis_angle\n\n  # Calculate the norm of the axis-angle vector\n  theta_sq = a0 ** 2 + a1 ** 2 + a2 ** 2\n\n  # Calculate the sine and cosine of half the angle of rotation\n  theta = jnp.sqrt(theta_sq)\n  half_theta = 0.5 * theta\n  sin_half_theta = jnp.sin(half_theta)\n  cos_half_theta = jnp.cos(half_theta)\n\n  # Avoid division by zero for small angles\n  if theta < eps:\n    half_theta_eps = eps * 0.5\n    sin_half_theta = jnp.sin(half_theta_eps)\n    cos_half_theta = jnp.cos(half_theta_eps)\n\n  # Calculate the components of the quaternion\n  q0 = cos_half_theta\n  q1 = sin_half_theta * a0 / theta\n  q2 = sin_half_theta * a1 / theta\n  q3 = sin_half_theta * a2 / theta\n\n  # Return the quaternion\n  return jnp.array([q0, q1, q2, q3])\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Extract the axis and angle from the input axis-angle vector\n  axis, angle = axis_angle[..., :3], axis_angle[..., 3:]\n\n  # Calculate the half angle\n  ha = angle * 0.5\n\n  # Calculate the sine and cosine of the half angle\n  s, c = jnp.sin(ha), jnp.cos(ha)\n\n  # Normalize the axis and calculate the quaternion components\n  axis = axis / jnp.sqrt(jnp.sum(axis * axis, axis=-1, keepdims=True))\n  q = jnp.concatenate([c, s * axis], axis=-1)\n\n  # Ensure numerical stability for small angles by using a small epsilon value\n  q = jnp.where(ha > eps, q, jnp.tile(jnp.array([1.0, 0.0, 0.0, 0.0]), q.shape[:-1] + (1,)))\n\n  return q\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Extract the angle and axis from the axis-angle representation\n  angle, axis = axis_angle[..., 0], axis_angle[..., 1:]\n\n  # Calculate the half angle and its sin and cosine\n  ha = angle / 2.0\n  s, c = jnp.sin(ha), jnp.cos(ha)\n\n  # Normalize the axis to avoid numerical instability\n  axis_norm = jnp.linalg.norm(axis, axis=-1, keepdims=True)\n  axis_norm = jnp.where(axis_norm < eps, jnp.ones_like(axis_norm), axis_norm)\n  axis = axis / axis_norm\n\n  # Construct the quaternion\n  q = jnp.concatenate([c, s * axis], axis=-1)\n\n  return q\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Extract the axis and angle from the axis-angle representation\n  axis = axis_angle[:3]\n  angle = axis_angle[3]\n\n  # Compute the sine and cosine of the half angle\n  half_angle = 0.5 * angle\n  sin_half_angle = jnp.sin(half_angle)\n  cos_half_angle = jnp.cos(half_angle)\n\n  # Compute the quaternion components using the axis and half angle values\n  qw = cos_half_angle\n  qx = sin_half_angle * axis[0]\n  qy = sin_half_angle * axis[1]\n  qz = sin_half_angle * axis[2]\n\n  # Ensure numerical stability for small angles\n  if jnp.abs(sin_half_angle) < eps:\n    qx = qx * eps\n    qy = qy * eps\n    qz = qz * eps\n\n  # Return the quaternion\n  return jnp.array([qw, qx, qy, qz])\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Extract the axis and angle from the axis-angle vector\n  axis, angle = axis_angle[:3], axis_angle[3]\n\n  # Calculate the sine and cosine of the half angle\n  half_angle = 0.5 * angle\n  s = jnp.sin(half_angle)\n  c = jnp.cos(half_angle)\n\n  # Normalize the axis and calculate the quaternion components\n  axis_norm = jnp.sqrt(jnp.sum(axis ** 2)) + eps\n  w = c\n  xyz = s * axis / axis_norm\n\n  # Concatenate the quaternion components and return the quaternion\n  return jnp.concatenate([w[None], xyz], axis=0)\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle = jnp.where(jnp.linalg.norm(axis_angle) > 0, axis_angle, axis_angle + eps)\n  angle = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / angle\n  return jnp.concatenate([jnp.cos(angle / 2)[..., None], axis * jnp.sin(angle / 2)[..., None]], -1)\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Get the magnitude of the rotation (angle) from the axis-angle vector.\n  magnitude = jnp.linalg.norm(axis_angle)\n\n  # If the magnitude is smaller than epsilon, return the identity quaternion.\n  if magnitude < eps:\n    return jnp.array([1, 0, 0, 0], dtype=jnp.float32)\n\n  # Normalize the axis-angle vector.\n  axis_angle_normalized = axis_angle / magnitude\n\n  # Calculate the sin and cos of half the angle.\n  sin_half_angle = jnp.sin(magnitude / 2)\n  cos_half_angle = jnp.cos(magnitude / 2)\n\n  # Construct the quaternion from the axis-angle vector.\n  quat = jnp.concatenate([cos_half_angle, sin_half_angle * axis_angle_normalized])\n\n  # Return the quaternion.\n  return quat\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    low = -40\n    while low < high:\n        mid = (low + high) // 2\n        logits, calls = model.topk(prefix, k, bias=mid)\n        if idx in logits:\n            high = mid\n        else:\n            low = mid + 1\n    logits, calls = model.topk(prefix, k, bias=low)\n    return logits[idx], calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    low = -1\n    while low < high - 1:\n        mid = (low + high) // 2\n        topk = model.topk(prefix, k, bias=mid)\n        topk_idx = [i for _, i in topk]\n        if idx in topk_idx:\n            high = mid\n        else:\n            low = mid\n    topk = model.topk(prefix, k, bias=high)\n    topk_idx = [i for _, i in topk]\n    return (high, topk_idx.index(idx))\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk = model.get_topk(prefix, k)\n\n    # if idx is in topk, return\n    if idx in topk:\n        return np.log(topk[idx]), 1\n\n    # if not, adjust search bias\n    low = 0\n    calls = 0\n    while high - low > 1:\n        calls += 1\n        mid = (low + high) // 2\n        topk = model.get_topk(prefix, k, bias=mid)\n        if idx in topk:\n            high = mid\n        else:\n            low = mid\n\n    # get log prob of idx being top result\n    topk = model.get_topk(prefix, k, bias=high)\n    return np.log(topk[idx]), calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk\n    topk = model.topk(prefix, k=k)\n\n    # adjust search bias until idx is the most probable\n    low = 0\n    while low < high:\n        mid = (low + high) // 2\n        model.adjust_search_bias(mid)\n        topk = model.topk(prefix, k=k)\n        if topk[0][0] == idx:\n            high = mid\n        else:\n            low = mid + 1\n\n    # get log probability of idx being the top result\n    log_prob = model.log_prob(prefix, idx)\n\n    # return log probability and number of calls\n    return log_prob, model.num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk_words, topk_probs = model.get_topk(prefix, k=k, high=high)\n\n    # get index of target word\n    target_idx = topk_words.index(idx)\n\n    # get log probability of target word being top result\n    log_prob = np.log(topk_probs[target_idx])\n\n    # return log probability and number of calls made to model\n    return log_prob, 1\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk\n    topk = model.topk(prefix, k)\n\n    # check if idx is in topk\n    if idx in topk:\n        return 0, 1\n\n    # adjust search bias\n    low = -40\n    while high - low > 1:\n        mid = (low + high) // 2\n        topk = model.topk(prefix, k, bias={idx: mid})\n        if idx in topk:\n            high = mid\n        else:\n            low = mid\n\n    # calculate log probability\n    topk = model.topk(prefix, k, bias={idx: high})\n    log_prob = math.log(topk[idx])\n\n    # return log probability and number of calls\n    return log_prob, model.num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk\n    topk = model.get_topk(prefix, k)\n\n    # get topk words and their indices\n    words = [x[0] for x in topk]\n    indices = [x[1] for x in topk]\n\n    # get topk probabilities\n    probs = [x[2] for x in topk]\n\n    # adjust search bias\n    low = 0\n    calls = 0\n    while high - low > 1:\n        mid = (high + low) // 2\n        calls += 1\n        model.adjust_search_bias(prefix, idx, mid)\n        topk = model.get_topk(prefix, k)\n        probs = [x[2] for x in topk]\n        if idx in indices:\n            low = mid\n        else:\n            high = mid\n\n    # get log probability of target index being top result\n    log_prob = np.log(probs[indices.index(idx)])\n\n    return log_prob, calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # set high to high enough to get idx in topk\n    low = 0\n    while True:\n        # get topk\n        topk, topk_indices = model.get_topk(prefix, k=k, high=high)\n        if idx in topk_indices:\n            break\n        high *= 2\n\n    # binary search\n    while low < high:\n        mid = (low + high) // 2\n        topk, topk_indices = model.get_topk(prefix, k=k, high=mid)\n        if idx in topk_indices:\n            high = mid\n        else:\n            low = mid + 1\n\n    # get log prob\n    log_prob = model.get_log_prob(prefix, topk[0])\n\n    return log_prob, model.calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk_words, topk_indices, topk_logprobs = model.get_topk(prefix, k=k)\n    # get index\n    idx_in_topk = topk_indices.index(idx)\n    # get logprob\n    logprob = topk_logprobs[idx_in_topk]\n    # get number of calls\n    num_calls = 1\n\n    # adjust bias\n    bias = high\n    while True:\n        # adjust bias\n        bias -= 1\n        # get topk\n        topk_words, topk_indices, topk_logprobs = model.get_topk(prefix, k=k, bias=bias)\n        # get index\n        idx_in_topk = topk_indices.index(idx)\n        # get logprob\n        logprob = topk_logprobs[idx_in_topk]\n        # increment number of calls\n        num_calls += 1\n        # check if target index is the top result\n        if idx_in_topk == 0:\n            break\n        # check if bias is too low\n        if bias < 0:\n            break\n\n    return logprob, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk = model.get_topk(prefix, k)\n    # if the target index is in topk, return\n    if idx in topk:\n        return math.log(topk[idx]), 1\n    # else, adjust the bias for the target index\n    low = 0\n    while low < high:\n        mid = (low + high) // 2\n        model.adjust_bias(prefix, idx, mid)\n        topk = model.get_topk(prefix, k)\n        if idx in topk:\n            high = mid\n        else:\n            low = mid + 1\n    # return the log probability of the target index being the top result\n    return math.log(topk[idx]), model.calls\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk\n    topk = model.topk(prefix, k)\n\n    # check if idx is in topk\n    if idx in topk[1]:\n        return np.log(1 / k), 1\n\n    # binary search\n    low = 0\n    calls = 0\n    while high - low > 1:\n        mid = (low + high) // 2\n        calls += 1\n        if idx in model.topk(prefix, k, bias=mid)[1]:\n            high = mid\n        else:\n            low = mid\n\n    # get final topk\n    topk = model.topk(prefix, k, bias=high)\n\n    # calculate log probability\n    log_prob = np.log(1 / k) + high * np.log(1 / topk[0][0])\n\n    return log_prob, calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk = model.get_topk(prefix, k=k)\n\n    # get the index of the target word\n    target_idx = topk.index(idx)\n\n    # initialize low and high\n    low = 0\n    high = high\n\n    # initialize the number of calls to the model\n    calls = 0\n\n    # perform binary search\n    while low < high:\n        mid = (low + high) // 2\n\n        # adjust search bias\n        model.adjust_search_bias(prefix, idx, mid)\n\n        # get the topk again\n        topk = model.get_topk(prefix, k=k)\n\n        # increment the number of calls\n        calls += 1\n\n        # check if the target word is in the topk\n        if idx in topk:\n            # get the index of the target word\n            target_idx = topk.index(idx)\n\n            # if the target word is in the topk, update the high bias\n            high = mid\n        else:\n            # if the target word is not in the topk, update the low bias\n            low = mid + 1\n\n    # get the topk again\n    topk = model.get_topk(prefix, k=k)\n\n    # calculate the log probability of the target index being the top result\n    log_prob = model.get_log_prob(prefix, topk[0])\n\n    # return the log probability and the number of calls\n    return log_prob, calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk = model.get_topk(prefix, k=k)\n    # get topk words and indices\n    topk_words = [w for w, _ in topk]\n    topk_indices = [i for _, i in topk]\n    # get the index of the target word in the topk indices\n    target_idx = topk_indices.index(idx)\n    # initialize low and high values\n    low = -100\n    high = high\n    # initialize call counter\n    calls = 0\n    # loop until low and high are close enough\n    while high - low > 1:\n        # calculate midpoint\n        mid = (low + high) // 2\n        # adjust search bias for the target word\n        model.adjust_search_bias(topk_words[target_idx], mid)\n        # get topk with adjusted bias\n        topk = model.get_topk(prefix, k=k)\n        # get topk words and indices\n        topk_words = [w for w, _ in topk]\n        topk_indices = [i for _, i in topk]\n        # get the index of the target word in the topk indices\n        target_idx = topk_indices.index(idx)\n        # update low and high values\n        if target_idx == 0:\n            high = mid\n        else:\n            low = mid\n        # increment call counter\n        calls += 1\n    # return log probability of target index being the top result and number of calls\n    return topk[0][1], calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk_words, topk_indices, topk_logprobs = model.get_topk(prefix, k=k)\n\n    # get index of target word in topk\n    target_idx = topk_indices[idx]\n\n    # initialize low and high bias values\n    low = -40\n    high = high\n\n    # initialize calls counter\n    calls = 0\n\n    # loop until target index is in topk\n    while target_idx not in topk_indices:\n\n        # calculate new bias values\n        low = low * 2\n        high = high * 2\n\n        # adjust search bias for target index\n        model.adjust_search_bias([(target_idx, high)])\n\n        # get topk again\n        topk_words, topk_indices, topk_logprobs = model.get_topk(prefix, k=k)\n\n        # increment calls counter\n        calls += 1\n\n    # calculate log probability of target index being the top result\n    logprob = topk_logprobs[topk_indices.index(target_idx)]\n\n    # return log probability and number of calls\n    return logprob, calls"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk\n    raw_topk = model.get_topk(prefix, k=k)\n    # get the topk words and their indices\n    topk_words = [word for word, _ in raw_topk]\n    topk_indices = [index for _, index in raw_topk]\n\n    # adjust the search bias until the desired index is the most probable\n    while True:\n        # get the topk words and their indices with the adjusted search bias\n        adjusted_topk = model.get_topk(prefix, k=k, high=high)\n        adjusted_topk_words = [word for word, _ in adjusted_topk]\n        adjusted_topk_indices = [index for _, index in adjusted_topk]\n\n        # check if the desired index is in the topk\n        if idx in adjusted_topk_indices:\n            # get the index of the desired index in the topk\n            idx_in_topk = adjusted_topk_indices.index(idx)\n            # calculate the log probability of the desired index being the top result\n            log_prob = adjusted_topk[idx_in_topk][1]\n            # return the log probability and the number of calls made to the model\n            return log_prob, model.num_calls\n        else:\n            # adjust the search bias\n            high += 1\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk\n    raw_topk = model.get_topk(prefix, k)\n    # print(raw_topk)\n    # if raw_topk[0][0] == idx:\n    #     return np.log(1), 1\n\n    # set initial values\n    low = -40\n    calls = 0\n\n    # iterate until low and high are close enough\n    while high - low > 1:\n        # adjust search bias\n        model.adjust_search_bias([(idx, (high + low) / 2)])\n\n        # get topk with adjusted search bias\n        topk = model.get_topk(prefix, k)\n        calls += 1\n\n        # if target index is in topk, adjust high to be the middle of current range\n        if topk[0][0] == idx:\n            high = (high + low) / 2\n        # otherwise, adjust low to be the middle of current range\n        else:\n            low = (high + low) / 2\n\n    # calculate log probability of target index being top result\n    log_prob = np.log(topk[0][1])\n\n    # return log probability and number of calls\n    return log_prob, calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk\n    raw_topk = model.topk(prefix, k)\n    topk = [(idx, prob) for idx, prob in raw_topk]\n\n    # get target idx\n    target_idx = topk[idx][0]\n\n    # calculate log prob of target idx being top result\n    log_prob = topk[idx][1]\n\n    # adjust search bias until target idx is top result\n    low = -1000\n    while True:\n        mid = (low + high) // 2\n        model.set_search_bias(target_idx, mid)\n        new_topk = model.topk(prefix, k)\n        new_idx = new_topk[0][0]\n        if new_idx == target_idx:\n            high = mid\n        else:\n            low = mid\n        if high - low <= 1:\n            break\n\n    # calculate log prob of target idx being top result\n    model.set_search_bias(target_idx, high)\n    new_topk = model.topk(prefix, k)\n    new_idx = new_topk[0][0]\n    if new_idx == target_idx:\n        log_prob = high\n    else:\n        log_prob = low\n\n    # return log prob and number of calls made to model\n    return log_prob, model.call_count\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk = model.get_topk(prefix, k)\n    topk_words = [word for word, _ in topk]\n    topk_indices = [index for _, index in topk]\n\n    # check if idx is in topk\n    if idx in topk_indices:\n        return 0, 1\n\n    # set low and high\n    low = -40\n    high = 40\n\n    # set initial bias\n    bias = {word: 0 for word in topk_words}\n    bias[prefix] = -100\n\n    # set initial log probability\n    log_prob = -1000\n\n    # set initial call count\n    call_count = 0\n\n    # binary search for bias that maximizes log probability of idx being top result\n    while high - low > 1:\n        # get midpoint\n        mid = (low + high) // 2\n\n        # adjust bias for idx\n        bias[prefix] = mid\n\n        # get topk with adjusted bias\n        topk = model.get_topk(prefix, k, bias)\n        topk_words = [word for word, _ in topk]\n        topk_indices = [index for _, index in topk]\n\n        # check if idx is in topk\n        if idx in topk_indices:\n            # get log probability of idx being top result\n            log_prob = model.log_prob(prefix, topk_words[topk_indices.index(idx)])\n\n            # update call count\n            call_count += 1\n\n            # update high\n            high = mid\n        else:\n            # update low\n            low = mid\n\n    # return log probability and call count\n    return log_prob, call_count\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk = model.get_topk(prefix, k=k)\n    # get topk words\n    topk_words = [word for word, _ in topk]\n    # get topk indices\n    topk_indices = [index for _, index in topk]\n\n    # check if idx is in topk\n    if idx not in topk_indices:\n        return -1, 0\n\n    # set low and high\n    low = 0\n    high = high\n\n    # set initial bias\n    bias = (low + high) / 2\n\n    # set initial calls\n    calls = 0\n\n    # loop until low and high are close enough\n    while high - low > 1:\n        # adjust search bias\n        topk = model.get_topk(prefix, k=k, bias=bias)\n        # get topk words\n        topk_words = [word for word, _ in topk]\n        # get topk indices\n        topk_indices = [index for _, index in topk]\n        # get topk probs\n        topk_probs = [prob for _, prob in topk]\n\n        # check if idx is in topk\n        if idx in topk_indices:\n            # get index of idx in topk\n            idx_idx = topk_indices.index(idx)\n            # get prob of idx\n            idx_prob = topk_probs[idx_idx]\n            # calculate log probability of idx being the top result\n            log_prob = np.log(idx_prob)\n            # return log probability and number of calls\n            return log_prob, calls\n        else:\n            # get index of topk\n            topk_idx = topk_indices.index(topk_indices[0])\n            # get prob of topk\n            topk_prob = topk_probs[topk_idx]\n            # calculate log probability of topk being the top result\n            log_prob = np.log(topk_prob)\n            # if log probability is less than 0\n            if log_prob < 0:"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk, topk_idx, topk_logprobs = model.get_topk(prefix, k=k)\n    topk_idx = topk_idx[0]\n    topk_logprobs = topk_logprobs[0]\n\n    # adjust topk\n    topk_logprobs = topk_logprobs - topk_logprobs.max()\n\n    # get the probability of the target index being the top result\n    p_idx = topk_logprobs[topk_idx == idx]\n\n    # adjust the bias of the target index\n    bias = np.ones_like(topk_logprobs) * high\n    bias[topk_idx == idx] = p_idx\n\n    # adjust the topk\n    topk_logprobs = topk_logprobs + bias\n\n    # get the log probability of the target index being the top result\n    p_idx = topk_logprobs[topk_idx == idx]\n\n    return p_idx, 1\n\n"}
{"namespace": "resample.resample_3d", "completion": "  # Check if the input data is a tensor\n  if not tf.is_tensor(data):\n    raise ValueError(\"Input data must be a tensor.\")\n\n  # Check if the input locations are a tensor\n  if not tf.is_tensor(locations):\n    raise ValueError(\"Input locations must be a tensor.\")\n\n  # Check if the input data has rank 4\n  if data.shape.ndims != 4:\n    raise ValueError(\"Input data must have rank 4.\")\n\n  # Check if the input locations have rank 2 or higher\n  if locations.shape.ndims < 2:\n    raise ValueError(\"Input locations must have rank 2 or higher.\")\n\n  # Check if the input data and locations have the same number of channels\n  if data.shape[-1] != locations.shape[-1]:\n    raise ValueError(\"Input data and locations must have the same number of channels.\")\n\n  # Check if the edge behavior is valid\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(\"Invalid edge behavior.\")\n\n  # Check if the coordinate order is valid\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError(\"Invalid coordinate order.\")\n\n  # Check if the interpolation method is valid\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError(\"Invalid interpolation method.\")\n\n  # Check if the input data and locations have the same number of dimensions\n  if data.shape.ndims != locations.shape.ndims:\n    raise ValueError(\"Input data and locations must have the same number of dimensions.\")\n\n  # Check if the input data and locations have the same batch size\n  if data.shape[0] != locations.shape[0]:\n    raise ValueError(\"Input data and locations must have the same batch size.\")\n\n  # Check if the input data and locations have the same spatial dimensions\n  if data.shape[1] != locations.shape[1] or data.shape[2] != locations.shape[2] or data.shape[3] != locations.shape[3"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order == 'xyz':\n    data = tf.transpose(data, [2, 1, 0, 3])\n  elif coordinate_order == 'zyx':\n    data = tf.transpose(data, [1, 0, 2, 3])\n  else:\n    raise ValueError(f\"Invalid coordinate order: {coordinate_order}\")\n\n  if method == 'TRILINEAR':\n    interpolation_method = 'linear'\n  elif method == 'NEAREST':\n    interpolation_method = 'nearest'\n  else:\n    raise ValueError(f\"Invalid interpolation method: {method}\")\n\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    padding_mode = 'constant'\n  elif edge_behavior == 'CLAMP':\n    padding_mode = 'edge'\n  else:\n    raise ValueError(f\"Invalid edge behavior: {edge_behavior}\")\n\n  if half_pixel_center:\n    align_corners = True\n  else:\n    align_corners = False\n\n  resampled_data = tf.contrib.resampler.resampler(\n      data,\n      locations,\n      method=interpolation_method,\n      padding_mode=padding_mode,\n      constant_values=constant_values,\n      align_corners=align_corners,\n  )\n\n  return resampled_data\n\n"}
{"namespace": "resample.resample_3d", "completion": "  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('Unknown edge_behavior: %s' % edge_behavior)\n\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Unknown method: %s' % method)\n\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Unknown coordinate_order: %s' % coordinate_order)\n\n  if len(data.shape) != 4:\n    raise ValueError('data must be 4D')\n\n  if len(locations.shape) < 2:\n    raise ValueError('locations must be at least 2D')\n\n  if locations.shape[-1] != 3:\n    raise ValueError('locations must have shape [..., 3]')\n\n  if len(data.shape) != len(locations.shape):\n    raise ValueError('data and locations must have same number of dimensions')\n\n  if len(data.shape) > 2 and data.shape[0] != locations.shape[0]:\n    raise ValueError('data and locations must have same batch dimension')\n\n  if len(data.shape) > 3 and data.shape[1] != locations.shape[1]:\n    raise ValueError('data and locations must have same channel dimension')\n\n  if len(data.shape) > 4 and data.shape[2] != locations.shape[2]:\n    raise ValueError('data and locations must have same spatial dimension')\n\n  if len(data.shape) > 5 and data.shape[3] != locations.shape[3]:\n    raise ValueError('data and locations must have same spatial dimension')\n\n  if len(data.shape) > 6 and data.shape[4] != locations.shape[4]:\n    raise ValueError('data and locations must have same spatial dimension')\n\n  if len(data.shape) > 7 and data.shape[5] != locations.shape[5]:\n    raise ValueError('data and locations must have same spatial dimension')\n\n  if len(data."}
{"namespace": "resample.resample_3d", "completion": "  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('Unknown edge_behavior: %s' % edge_behavior)\n\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Unknown method: %s' % method)\n\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Unknown coordinate_order: %s' % coordinate_order)\n\n  if len(data.shape) != 4:\n    raise ValueError('data must have 4 dimensions, not %d' % len(data.shape))\n\n  if len(locations.shape) < 2:\n    raise ValueError('locations must have at least 2 dimensions, not %d' % len(locations.shape))\n\n  if data.shape[0] != 1:\n    raise ValueError('data must have exactly 1 batch dimension, not %d' % data.shape[0])\n\n  if locations.shape[-1] != 3:\n    raise ValueError('locations must have exactly 3 spatial dimensions, not %d' % locations.shape[-1])\n\n  if coordinate_order == 'xyz':\n    x, y, z = tf.unstack(locations, axis=-1)\n  else:\n    z, y, x = tf.unstack(locations, axis=-1)\n\n  x = tf.cast(x, tf.float32)\n  y = tf.cast(y, tf.float32)\n  z = tf.cast(z, tf.float32)\n\n  if half_pixel_center:\n    x += 0.5\n    y += 0.5\n    z += 0.5\n\n  x0 = tf.floor(x)\n  x1 = x0 + 1\n  y0 = tf.floor(y)\n  y1 = y0 + 1\n  z0 = tf.floor(z)\n  z1 = z0 + 1\n\n  x0 = tf.cast(x"}
{"namespace": "resample.resample_3d", "completion": "  # Get the shape of the input data\n  data_shape = data.shape\n\n  # Get the number of dimensions in the input data\n  ndims = len(data_shape)\n\n  # Get the number of channels in the input data\n  nchannels = data_shape[-1]\n\n  # Get the shape of the locations tensor\n  locations_shape = locations.shape\n\n  # Get the number of locations\n  nlocations = locations_shape[1]\n\n  # Get the number of dimensions in the locations tensor\n  ndims_locations = len(locations_shape)\n\n  # Check if the number of dimensions in the locations tensor is less than or equal to 3\n  if ndims_locations <= 3:\n\n    # If the number of dimensions in the locations tensor is less than or equal to 3, set the number of dimensions to 3\n    ndims_locations = 3\n\n  # Check if the number of dimensions in the locations tensor is greater than 3\n  if ndims_locations > 3:\n\n    # If the number of dimensions in the locations tensor is greater than 3, raise an error\n    raise ValueError('locations can have at most 3 dimensions.')\n\n  # Check if the number of dimensions in the input data is not equal to 4\n  if ndims != 4:\n\n    # If the number of dimensions in the input data is not equal to 4, raise an error\n    raise ValueError('data must have 4 dimensions.')\n\n  # Check if the number of channels in the input data is not equal to 1\n  if nchannels != 1:\n\n    # If the number of channels in the input data is not equal to 1, raise an error\n    raise ValueError('data must have 1 channel.')\n\n  # Check if the edge behavior is not 'CONSTANT_OUTSIDE' or 'CLAMP'\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n\n    # If the edge behavior is not 'CONSTANT_OUTSIDE' or 'CLAMP', raise an error\n    raise ValueError('edge"}
{"namespace": "resample.resample_3d", "completion": "  # Check if the input data is a tensor\n  if not tf.is_tensor(data):\n    raise ValueError(\"Input data must be a tensor.\")\n\n  # Check if the input locations are a tensor\n  if not tf.is_tensor(locations):\n    raise ValueError(\"Input locations must be a tensor.\")\n\n  # Check if the input edge behavior is valid\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(\"Invalid edge behavior. Must be 'CONSTANT_OUTSIDE' or 'CLAMP'.\")\n\n  # Check if the input method is valid\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError(\"Invalid method. Must be 'TRILINEAR' or 'NEAREST'.\")\n\n  # Check if the input coordinate order is valid\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError(\"Invalid coordinate order. Must be 'xyz' or 'zyx'.\")\n\n  # Check if the input data has 4 dimensions\n  if data.shape.ndims != 4:\n    raise ValueError(\"Input data must have 4 dimensions.\")\n\n  # Check if the input locations have at least 2 dimensions\n  if locations.shape.ndims < 2:\n    raise ValueError(\"Input locations must have at least 2 dimensions.\")\n\n  # Check if the last dimension of the input locations is 3\n  if locations.shape[-1] != 3:\n    raise ValueError(\"Last dimension of input locations must be 3.\")\n\n  # Get the input data shape\n  data_shape = data.shape.as_list()\n\n  # Get the input locations shape\n  locations_shape = locations.shape.as_list()\n\n  # Get the number of input locations\n  num_locations = locations_shape[-2]\n\n  # Get the number of channels in the input data\n  num_channels = data_shape[-1]\n\n  # Check if the number of channels in the input data is valid\n  if num_channels < 1:\n    raise ValueError(\"Number of"}
{"namespace": "resample.resample_3d", "completion": "  # Get the shape of the input data\n  data_shape = data.shape\n\n  # Get the shape of the locations tensor\n  location_shape = locations.shape\n\n  # Check if the coordinate order is 'xyz' or 'zyx'\n  if coordinate_order == 'xyz':\n    # If 'xyz', reshape the locations tensor to have shape [D, ..., 3]\n    locations = locations.reshape(data_shape[0], -1, 3)\n  elif coordinate_order == 'zyx':\n    # If 'zyx', reshape the locations tensor to have shape [D, ..., 3]\n    locations = locations.reshape(data_shape[0], -1, 3)\n    # Swap the first and third dimensions of the locations tensor\n    locations = locations[:, :, [2, 1, 0]]\n  else:\n    # Raise an error if the coordinate order is invalid\n    raise ValueError(\n        f\"Invalid coordinate_order: {coordinate_order}. Must be 'xyz' or 'zyx'.\"\n    )\n\n  # Get the shape of the locations tensor after reshaping\n  location_shape = locations.shape\n\n  # Check if the edge behavior is 'CONSTANT_OUTSIDE' or 'CLAMP'\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    # If 'CONSTANT_OUTSIDE', pad the input data with a constant value\n    data = tf.pad(\n        data,\n        [[1, 1], [1, 1], [1, 1], [0, 0]],\n        mode='CONSTANT',\n        constant_values=constant_values,\n    )\n  elif edge_behavior == 'CLAMP':\n    # If 'CLAMP', do nothing\n    pass\n  else:\n    # Raise an error if the edge behavior is invalid\n    raise ValueError(\n        f\"Invalid edge_behavior: {edge_behavior}. Must be 'CONSTANT_OUTSIDE' or 'CLAMP'.\"\n    )\n\n  # Check if the method is '"}
{"namespace": "resample.resample_3d", "completion": "  # Check if the input data has the correct shape\n  if len(data.shape) != 4:\n    raise ValueError(\"Input data must have shape [D, H, W, C]\")\n\n  # Check if the locations have the correct shape\n  if len(locations.shape) < 2 or locations.shape[-1] != 3:\n    raise ValueError(\"Locations must have shape [D, ..., 3]\")\n\n  # Check if the edge behavior is valid\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(\"Invalid edge behavior\")\n\n  # Check if the coordinate order is valid\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError(\"Invalid coordinate order\")\n\n  # Check if the interpolation method is valid\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError(\"Invalid interpolation method\")\n\n  # Get the shape of the input data\n  depth, height, width, channels = data.shape\n\n  # Get the shape of the locations tensor\n  num_samples = locations.shape[0]\n\n  # Convert the locations to integer coordinates\n  locations = locations.astype(int)\n\n  # Adjust the locations for half-pixel centering if necessary\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  # Check if the locations are within the bounds of the input data\n  if (locations < 0).any() or (locations >= [depth, height, width]).any():\n    if edge_behavior == 'CONSTANT_OUTSIDE':\n      # Pad the input data with constant values outside the tensor\n      data = np.pad(\n          data,\n          ((1, 1), (1, 1), (1, 1), (0, 0)),\n          mode='constant',\n          constant_values=constant_values,\n      )\n\n      # Adjust the locations to account for the padding\n      locations = locations + 1\n    elif edge_behavior == 'CLAMP':\n      # Clamp the locations to"}
{"namespace": "resample.resample_3d", "completion": "  # Get the shape of the input data\n  data_shape = data.shape\n\n  # Get the number of dimensions in the input data\n  num_dims = len(data_shape)\n\n  # Check if the number of dimensions is 4\n  if num_dims != 4:\n    raise ValueError(\"The input data must have 4 dimensions.\")\n\n  # Get the number of channels in the input data\n  num_channels = data_shape[-1]\n\n  # Get the shape of the sample locations\n  sample_locations_shape = locations.shape\n\n  # Get the number of dimensions in the sample locations\n  num_sample_locations_dims = len(sample_locations_shape)\n\n  # Check if the number of dimensions in the sample locations is greater than 2\n  if num_sample_locations_dims < 2:\n    raise ValueError(\"The sample locations must have at least 2 dimensions.\")\n\n  # Check if the number of dimensions in the sample locations is greater than 3\n  if num_sample_locations_dims > 3:\n    raise ValueError(\"The sample locations must have at most 3 dimensions.\")\n\n  # Check if the number of dimensions in the sample locations is equal to 3\n  if num_sample_locations_dims == 3:\n    # Check if the number of channels in the input data is equal to the number of dimensions in the sample locations\n    if num_channels != num_sample_locations_dims:\n      raise ValueError(\"The number of channels in the input data must be equal to the number of dimensions in the sample locations.\")\n\n  # Check if the number of dimensions in the sample locations is equal to 2\n  if num_sample_locations_dims == 2:\n    # Check if the number of channels in the input data is equal to 1\n    if num_channels != 1:\n      raise ValueError(\"The number of channels in the input data must be 1 when the sample locations have 2 dimensions.\")\n\n  # Check if the edge behavior is valid\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n   "}
{"namespace": "resample.resample_3d", "completion": "  # Check if the input data is a tensor\n  assert tf.is_tensor(data), \"Input data must be a tensor.\"\n\n  # Check if the input locations are a tensor\n  assert tf.is_tensor(locations), \"Locations must be a tensor.\"\n\n  # Check if the edge behavior is valid\n  assert edge_behavior in ['CONSTANT_OUTSIDE', 'CLAMP'], \"Invalid edge behavior.\"\n\n  # Check if the coordinate order is valid\n  assert coordinate_order in ['xyz', 'zyx'], \"Invalid coordinate order.\"\n\n  # Check if the interpolation method is valid\n  assert method in ['TRILINEAR', 'NEAREST'], \"Invalid interpolation method.\"\n\n  # Check if the input data has rank 4\n  assert data.shape.ndims == 4, \"Input data must have rank 4.\"\n\n  # Check if the input locations have rank at least 2\n  assert locations.shape.ndims >= 2, \"Locations must have rank at least 2.\"\n\n  # Check if the number of dimensions in the input data and locations match\n  assert data.shape.ndims == locations.shape.ndims, \"Input data and locations must have the same number of dimensions.\"\n\n  # Check if the last dimension of the input data is 1\n  assert data.shape[-1] == 1, \"Input data must have a channel dimension of 1.\"\n\n  # Check if the last dimension of the input locations is 3\n  assert locations.shape[-1] == 3, \"Locations must have a dimension of 3.\"\n\n  # Check if the input locations are in the range of the input data\n  assert tf.reduce_all(tf.reduce_min(locations, axis=-1) >= 0), \"Locations must be in the range of the input data.\"\n  assert tf.reduce_all(tf.reduce_max(locations, axis=-1) < tf.cast(tf.shape(data)[:-1], locations.dtype)), \"Locations must be in the range of the input data.\"\n\n  # Check if the input data and locations have the same dtype"}
{"namespace": "resample.resample_3d", "completion": "  # Check if the input data has the correct number of dimensions\n  if len(data.shape) != 4:\n    raise ValueError(\"Input data must have 4 dimensions: [D, H, W, C]\")\n\n  # Check if the locations tensor has the correct number of dimensions\n  if len(locations.shape) < 2 or locations.shape[-1] != 3:\n    raise ValueError(\"Locations tensor must have at least 2 dimensions and the last dimension must be of size 3\")\n\n  # Check if the edge behavior is valid\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(\"Invalid edge behavior. Must be either 'CONSTANT_OUTSIDE' or 'CLAMP'\")\n\n  # Check if the coordinate order is valid\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError(\"Invalid coordinate order. Must be either 'xyz' or 'zyx'\")\n\n  # Check if the method is valid\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError(\"Invalid method. Must be either 'TRILINEAR' or 'NEAREST'\")\n\n  # Get the dimensions of the input data\n  D, H, W, C = data.shape\n\n  # Get the number of sample points\n  num_samples = locations.shape[0]\n\n  # Pad the input data if necessary\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    # Pad the input data with the constant value\n    data = tf.pad(data, [[1, 1], [1, 1], [1, 1], [0, 0]], mode='CONSTANT', constant_values=constant_values)\n\n    # Update the dimensions of the input data\n    D, H, W, C = data.shape\n\n  # Convert the sample locations to integer coordinates\n  if half_pixel_center:\n    locations = locations + 0.5\n  locations = tf.cast(tf.round(locations), tf.int32)\n\n  #"}
{"namespace": "resample.resample_3d", "completion": "  # Check if the input data has the correct number of dimensions\n  if len(data.shape) != 4:\n    raise ValueError(\n      \"Input data must have 4 dimensions, but got shape {}\".format(data.shape)\n    )\n\n  # Check if the locations tensor has the correct number of dimensions\n  if len(locations.shape) < 2 or locations.shape[-1] != 3:\n    raise ValueError(\n      \"Locations must have at least 2 dimensions and the last dimension must be 3, but got shape {}\".format(\n        locations.shape\n      )\n    )\n\n  # Check if the coordinate order is valid\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError(\n      \"Invalid coordinate order. Must be either 'xyz' or 'zyx', but got {}\".format(\n        coordinate_order\n      )\n    )\n\n  # Check if the method is valid\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError(\n      \"Invalid method. Must be either 'TRILINEAR' or 'NEAREST', but got {}\".format(\n        method\n      )\n    )\n\n  # Check if the edge behavior is valid\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(\n      \"Invalid edge behavior. Must be either 'CONSTANT_OUTSIDE' or 'CLAMP', but got {}\".format(\n        edge_behavior\n      )\n    )\n\n  # Get the dimensions of the input data\n  d, h, w, c = data.shape\n\n  # Get the number of sample points\n  n = locations.shape[0]\n\n  # Get the batch size\n  b = locations.shape[1]\n\n  # Get the sample locations\n  x, y, z = locations[:, :, 0], locations[:, :, 1], locations[:, :, 2]\n\n  # Convert the sample locations to integer coordinates\n  if half_pixel_center:\n    x = x + 0.5\n    y ="}
{"namespace": "resample.resample_3d", "completion": "  # Check input data shape\n  assert data.ndim == 4, \"Input data must have shape [D, H, W, C]\"\n\n  # Check input locations shape\n  assert locations.ndim >= 2 and locations.shape[-1] == 3, \"Locations must have shape [D, ..., 3]\"\n\n  # Check edge behavior\n  assert edge_behavior in ['CONSTANT_OUTSIDE', 'CLAMP'], \"Invalid edge behavior\"\n\n  # Check coordinate order\n  assert coordinate_order in ['xyz', 'zyx'], \"Invalid coordinate order\"\n\n  # Check method\n  assert method in ['TRILINEAR', 'NEAREST'], \"Invalid method\"\n\n  # Check half_pixel_center\n  assert isinstance(half_pixel_center, bool), \"half_pixel_center must be a boolean\"\n\n  # Get input data shape\n  batch_size, height, width, channels = data.shape\n\n  # Get sample locations shape\n  sample_locations_shape = locations.shape\n\n  # Reshape sample locations to [D, ..., 3]\n  sample_locations = locations.reshape(-1, 3)\n\n  # Convert sample locations to integer coordinates\n  if half_pixel_center:\n    sample_locations = sample_locations + 0.5\n\n  # Convert sample locations to integer coordinates\n  sample_locations = sample_locations.astype(np.int32)\n\n  # Clamp sample locations to the volume\n  if edge_behavior == 'CLAMP':\n    sample_locations = np.clip(sample_locations, 0, np.array([height - 1, width - 1, channels - 1]))\n\n  # Get sample locations\n  sample_locations_x = sample_locations[:, 0]\n  sample_locations_y = sample_locations[:, 1]\n  sample_locations_z = sample_locations[:, 2]\n\n  # Get sample values\n  if method == 'TRILINEAR':\n    # Get sample values using trilinear interpolation\n    sample_values"}
{"namespace": "resample.resample_3d", "completion": "  # Get the shape of the input volume\n  data_shape = tf.shape(data)\n\n  # Get the shape of the locations tensor\n  location_shape = tf.shape(locations)\n\n  # Get the batch size\n  batch_size = location_shape[0]\n\n  # Get the number of sample points\n  num_samples = location_shape[1]\n\n  # Get the number of channels\n  num_channels = data_shape[3]\n\n  # Reshape the locations tensor to [batch_size * num_samples, 3]\n  locations = tf.reshape(locations, [batch_size * num_samples, 3])\n\n  # Get the x, y, and z coordinates of the sample points\n  x = locations[:, 0]\n  y = locations[:, 1]\n  z = locations[:, 2]\n\n  # If the coordinate order is 'zyx', swap the x and z coordinates\n  if coordinate_order == 'zyx':\n    x, z = z, x\n\n  # If half_pixel_center is True, add 0.5 to the sample point coordinates\n  if half_pixel_center:\n    x += 0.5\n    y += 0.5\n    z += 0.5\n\n  # Compute the x0, y0, z0 coordinates of the bottom-left corner of the voxel containing each sample point\n  x0 = tf.floor(x)\n  y0 = tf.floor(y)\n  z0 = tf.floor(z)\n\n  # Compute the x1, y1, z1 coordinates of the top-right corner of the voxel containing each sample point\n  x1 = x0 + 1\n  y1 = y0 + 1\n  z1 = z0 + 1\n\n  # Compute the xd, yd, zd distances from the sample point to the bottom-left corner of the voxel\n  xd = x - x0\n  yd = y - y0\n  zd = z - z0\n\n  # Compute the xd_inv,"}
{"namespace": "resample.resample_3d", "completion": "  # Get the shape of the input data\n  data_shape = data.get_shape().as_list()\n\n  # Get the shape of the locations tensor\n  locations_shape = locations.get_shape().as_list()\n\n  # Check if the data and locations tensors have the correct number of dimensions\n  assert len(data_shape) == 4, \"Data must be 4D tensor\"\n  assert len(locations_shape) >= 2, \"Locations must be at least 2D tensor\"\n\n  # Check if the coordinate order is valid\n  assert coordinate_order in ['xyz', 'zyx'], \"Coordinate order must be 'xyz' or 'zyx'\"\n\n  # Check if the edge behavior is valid\n  assert edge_behavior in ['CONSTANT_OUTSIDE', 'CLAMP'], \"Edge behavior must be 'CONSTANT_OUTSIDE' or 'CLAMP'\"\n\n  # Check if the method is valid\n  assert method in ['TRILINEAR', 'NEAREST'], \"Method must be 'TRILINEAR' or 'NEAREST'\"\n\n  # Get the number of batch dimensions\n  batch_dims = len(data_shape) - 3\n\n  # Get the number of sample points\n  num_samples = locations_shape[-1]\n\n  # Check if the number of sample points is valid\n  assert num_samples == 3, \"Locations must be a tensor of shape [D, ..., 3]\"\n\n  # Check if the number of batch dimensions is valid\n  assert batch_dims >= 0 and batch_dims <= 2, \"Data must be a tensor of shape [D, H, W, C] or [H, W, C]\"\n\n  # Check if the number of dimensions in the locations tensor is valid\n  assert len(locations_shape) == batch_dims + 2, \"Locations must be a tensor of shape [D, ..., 3]\"\n\n  # Check if the number of dimensions in the data tensor is valid\n  assert len(data_shape) == batch_dims + 3, \"Data must be a tensor of shape [D, H, W, C] or [H"}
{"namespace": "resample.resample_3d", "completion": "  # Get the shape of the input data\n  data_shape = tf.shape(data)\n  # Get the shape of the input locations\n  location_shape = tf.shape(locations)\n\n  # Pad the data volume with zeros if necessary\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = tf.pad(\n      data,\n      [[1, 1], [1, 1], [1, 1], [0, 0]],\n      mode='CONSTANT',\n      constant_values=constant_values,\n    )\n\n  # Get the coordinates of the data volume\n  x, y, z = tf.meshgrid(\n    tf.range(0, data_shape[0]),\n    tf.range(0, data_shape[1]),\n    tf.range(0, data_shape[2]),\n    indexing='ij',\n  )\n\n  # Calculate the coordinates of the data volume\n  x = tf.cast(x, tf.float32)\n  y = tf.cast(y, tf.float32)\n  z = tf.cast(z, tf.float32)\n\n  # Get the coordinates of the input locations\n  x_locations = locations[..., 0]\n  y_locations = locations[..., 1]\n  z_locations = locations[..., 2]\n\n  # Calculate the coordinates of the input locations\n  x_locations = tf.cast(x_locations, tf.float32)\n  y_locations = tf.cast(y_locations, tf.float32)\n  z_locations = tf.cast(z_locations, tf.float32)\n\n  # Check if half-pixel centering is enabled\n  if half_pixel_center:\n    # Add 0.5 to the coordinates of the input locations\n    x_locations += 0.5\n    y_locations += 0.5\n    z_locations += 0.5\n\n "}
{"namespace": "resample.resample_3d", "completion": "  # Get input shape\n  input_shape = data.get_shape().as_list()\n\n  # Get output shape\n  output_shape = locations.get_shape().as_list()\n  output_shape[0] = input_shape[0]\n  output_shape[-1] = input_shape[-1]\n\n  # Convert locations to normalized coordinates\n  if coordinate_order == 'xyz':\n    normalized_coordinates = locations / tf.cast(tf.stack(input_shape[:3]), dtype=tf.float32)\n  else:\n    normalized_coordinates = locations / tf.cast(tf.stack(input_shape[1:4]), dtype=tf.float32)\n\n  # Convert locations to grid coordinates\n  grid_coordinates = normalized_coordinates * tf.cast(tf.stack(input_shape[:3]), dtype=tf.float32)\n\n  # Adjust grid coordinates for half-pixel centering\n  if half_pixel_center:\n    grid_coordinates = grid_coordinates + 0.5\n\n  # Adjust grid coordinates for edge behavior\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    grid_coordinates = tf.maximum(grid_coordinates, 0.0)\n    grid_coordinates = tf.minimum(grid_coordinates, tf.cast(tf.stack(input_shape[:3]), dtype=tf.float32) - 1.0)\n  elif edge_behavior == 'CLAMP':\n    grid_coordinates = tf.clip_by_value(grid_coordinates, 0.0, tf.cast(tf.stack(input_shape[:3]), dtype=tf.float32) - 1.0)\n\n  # Resample data using trilinear interpolation\n  if method == 'TRILINEAR':\n    resampled_data = trilinear_resampling(data, grid_coordinates)\n  elif method == 'NEAREST':\n    resampled_data = nearest_neighbor_resampling("}
{"namespace": "resample.resample_3d", "completion": "  # Convert the input data and locations to TensorFlow tensors\n  data = tf.convert_to_tensor(data)\n  locations = tf.convert_to_tensor(locations)\n\n  # Check the shape of the input data and locations\n  assert data.shape.ndims == 4, \"data must have shape [D, H, W, C]\"\n  assert locations.shape.ndims >= 2, \"locations must have shape [D, ..., 3]\"\n\n  # Determine the number of samples and sample locations\n  num_samples = locations.shape[1:-1].num_elements()\n  sample_locations = tf.reshape(locations, [-1, 3])\n\n  # Determine the data type of the input data\n  data_type = data.dtype\n\n  # Determine the output shape based on the number of samples and the input data shape\n  output_shape = tf.concat([tf.shape(data)[:1], [num_samples], tf.shape(data)[-1:]], axis=0)\n\n  # Create a TensorFlow variable to store the output data\n  output_data = tf.Variable(tf.zeros(output_shape, dtype=data_type), trainable=False)\n\n  # Create a TensorFlow variable to store the sample indices\n  sample_indices = tf.Variable(tf.zeros([num_samples, 8], dtype=tf.int32), trainable=False)\n\n  # Create a TensorFlow variable to store the sample weights\n  sample_weights = tf.Variable(tf.zeros([num_samples, 8], dtype=data_type), trainable=False)\n\n  # Create a TensorFlow variable to store the sample locations\n  sample_locations = tf.Variable(tf.zeros([num_samples, 8, 3], dtype=data_type), trainable=False)\n\n  # Create a TensorFlow variable to store the sample values\n  sample_values = tf.Variable(tf.zeros([num_samples, 8], dtype="}
{"namespace": "resample.resample_3d", "completion": "  # Get the shape of the input data and the sample locations\n  data_shape = tf.shape(data)\n  sample_locations_shape = tf.shape(locations)\n\n  # Get the number of dimensions of the input data and the sample locations\n  data_ndims = tf.rank(data)\n  sample_locations_ndims = tf.rank(locations)\n\n  # Get the batch size of the input data\n  batch_size = data_shape[0]\n\n  # Get the spatial dimensions of the input data\n  data_spatial_dims = data_shape[1:4]\n\n  # Get the number of sample locations\n  num_samples = tf.reduce_prod(sample_locations_shape[1:-1])\n\n  # Reshape the sample locations to have shape [batch_size, num_samples, 3]\n  sample_locations = tf.reshape(locations, [batch_size, num_samples, 3])\n\n  # Convert the sample locations to voxel coordinates\n  voxel_coordinates = sample_locations\n  if coordinate_order == 'zyx':\n    voxel_coordinates = tf.reverse(voxel_coordinates, axis=[2])\n\n  # Convert the voxel coordinates to pixel coordinates\n  pixel_coordinates = voxel_coordinates + 0.5\n\n  # Apply half-pixel centering if enabled\n  if half_pixel_center:\n    pixel_coordinates = pixel_coordinates - 0.5\n\n  # Get the minimum and maximum pixel coordinates\n  min_pixel_coordinates = tf.constant([0.0, 0.0, 0.0])\n  max_pixel_coordinates = tf.cast(data_spatial_dims, tf.float32) - 1.0\n\n  # Clamp the pixel coordinates to the minimum and maximum values\n  clamped_pixel_coordinates = tf.clip_by_value(pixel_coordinates, min_pixel_coordinates, max_pixel"}
{"namespace": "resample.resample_3d", "completion": "  # Get the shape of the input data\n  data_shape = data.shape\n\n  # Get the number of dimensions in the input data\n  ndims = data.ndim\n\n  # Get the number of channels in the input data\n  nchannels = data.shape[-1]\n\n  # Get the number of samples to resample\n  nsamples = locations.shape[0]\n\n  # Get the number of points to resample\n  npoints = locations.shape[1]\n\n  # Create a constant tensor of zeros with shape [D, ..., C]\n  zeros = tf.zeros([nsamples, npoints, nchannels])\n\n  # Create a constant tensor of ones with shape [D, ..., C]\n  ones = tf.ones([nsamples, npoints, nchannels])\n\n  # Check if the coordinate order is 'xyz' or 'zyx'\n  if coordinate_order == 'xyz':\n\n    # Get the x, y, and z coordinates from the locations tensor\n    x = locations[:, :, 0]\n    y = locations[:, :, 1]\n    z = locations[:, :, 2]\n\n  elif coordinate_order == 'zyx':\n\n    # Get the x, y, and z coordinates from the locations tensor\n    x = locations[:, :, 2]\n    y = locations[:, :, 1]\n    z = locations[:, :, 0]\n\n  else:\n\n    # Raise a ValueError if the coordinate order is not 'xyz' or 'zyx'\n    raise ValueError('Coordinate order must be \"xyz\" or \"zyx\".')\n\n  # Check if the method is 'TRILINEAR' or 'NEAREST'\n  if method == 'TRILINEAR':\n\n    # Check if half_pixel_center is True\n    if half_pixel_center:\n\n      # Add 0.5 to the x, y, and z coordinates\n      x += 0.5\n      y += 0.5\n      z += 0.5\n\n    # Check if edge_behavior is 'CONST"}
{"namespace": "math.plus_eps", "completion": "  import numpy as np\n\n  tiny = np.finfo(np.float64).tiny\n\n  if x < tiny:\n    x = tiny\n  else:\n    x = np.nextafter(x, np.inf)\n\n  return x"}
{"namespace": "math.plus_eps", "completion": "  tiny = 1.0e-30\n  if abs(x) < tiny:\n    x = tiny\n  return np.nextafter(x, np.inf)"}
{"namespace": "math.plus_eps", "completion": "  import numpy as np\n\n  tiny = np.finfo(np.float64).tiny\n\n  if abs(x) < tiny:\n    x = tiny\n  else:\n    x = np.nextafter(x, np.inf)\n\n  return x"}
{"namespace": "math.plus_eps", "completion": "  eps = 1.0e-15\n  if abs(x) < eps:\n    x = eps\n  else:\n    x = nextafter(x, inf)\n  return x"}
{"namespace": "math.plus_eps", "completion": "  import numpy as np\n\n  eps = np.finfo(float).eps\n\n  if x < eps:\n    return eps\n  else:\n    return np.nextafter(x, np.inf)"}
{"namespace": "math.plus_eps", "completion": "  import numpy as np\n\n  eps = np.finfo(np.float64).eps\n  tiny = np.finfo(np.float64).tiny\n\n  if x < tiny:\n    x = tiny\n  else:\n    x = np.nextafter(x, np.inf)\n\n  return x"}
{"namespace": "math.plus_eps", "completion": "  tiny = 1e-12\n  if abs(x) < tiny:\n    x = tiny\n  else:\n    x = nextafter(x, inf)\n  return x"}
{"namespace": "math.plus_eps", "completion": "  # Define a tiny threshold value\n  tiny = 1.0e-30\n\n  # Check if x is smaller than the threshold\n  if abs(x) < tiny:\n    # If x is smaller than the threshold, return the threshold value\n    return tiny\n  else:\n    # If x is not smaller than the threshold, compute the next representable floating-point value towards positive infinity\n    return nextafter(x, inf)"}
{"namespace": "math.plus_eps", "completion": "  tiny = 1e-15\n  if abs(x) < tiny:\n    x = tiny\n  else:\n    x = nextafter(x, inf)\n  return x\n\n"}
{"namespace": "math.plus_eps", "completion": "  eps = 1e-15\n  if abs(x) < eps:\n    x = eps\n  else:\n    x = nextafter(x, inf)\n  return x\n"}
{"namespace": "math.plus_eps", "completion": "  import numpy as np\n\n  eps = np.finfo(x.dtype).eps\n  tiny = np.finfo(x.dtype).tiny\n\n  if x.ndim == 0:\n    if np.abs(x) < tiny:\n      return tiny\n    else:\n      return np.nextafter(x, np.inf)\n  else:\n    x[np.abs(x) < tiny] = tiny\n    return np.nextafter(x, np.inf)"}
{"namespace": "math.plus_eps", "completion": "  tiny = 1.0e-300\n  if abs(x) < tiny:\n    x = tiny\n  else:\n    x = nextafter(x, inf)\n  return x\n\n"}
{"namespace": "math.plus_eps", "completion": "  import numpy as np\n\n  # Set the threshold value for x\n  x_threshold = 1e-10\n\n  # Check if x is smaller than the threshold\n  if x < x_threshold:\n    # If x is smaller than the threshold, return the threshold value\n    return x_threshold\n  else:\n    # If x is not smaller than the threshold, return the next representable floating-point value towards positive infinity\n    return np.nextafter(x, np.inf)"}
{"namespace": "math.plus_eps", "completion": "  import numpy as np\n  return np.nextafter(x, np.inf) if x < np.finfo(float).eps else x\n\n"}
{"namespace": "math.plus_eps", "completion": "  tiny = 1.0e-10\n  if abs(x) < tiny:\n    x = tiny\n  return x + abs(x) * tiny\n\n"}
{"namespace": "math.plus_eps", "completion": "  eps = 1.0e-15\n  if abs(x) < eps:\n    x = eps\n  else:\n    x = nextafter(x, inf)\n  return x\n"}
{"namespace": "math.plus_eps", "completion": "  tiny = 1e-10\n  if abs(x) < tiny:\n    return tiny\n  else:\n    return nextafter(x, inf)\n"}
{"namespace": "math.plus_eps", "completion": "  # Define a tiny threshold value\n  tiny = 1.0e-10\n\n  # Check if x is smaller than the threshold\n  if abs(x) < tiny:\n    # If x is smaller, return the threshold value\n    return tiny\n  else:\n    # If x is not smaller, return the next representable floating-point value towards positive infinity\n    return nextafter(x, inf)\n"}
{"namespace": "math.plus_eps", "completion": "  tiny = 1.0e-10\n  if abs(x) < tiny:\n    x = tiny\n  return x + abs(x) * 1.0e-10\n\n"}
{"namespace": "math.plus_eps", "completion": "  import numpy as np\n\n  tiny = np.finfo(x.dtype).tiny\n  x = np.where(np.abs(x) < tiny, tiny, x)\n  x = np.where(x < tiny, np.nextafter(x, np.inf), x)\n\n  return x\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-15\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-15\n\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x > tiny_val:\n    return x - tiny_val\n  else:\n    return -tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-16\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-15\n  if x > tiny_val:\n    return x - tiny_val\n  else:\n    return -tiny_val\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-15\n  if x < tiny_val:\n    x = -tiny_val\n  else:\n    x = x - tiny_val\n  return x\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-100\n\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - 1e-10\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - 1e-15\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-12\n\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1.e-10\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-15\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-15\n\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-15\n\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-15\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-15\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n\n"}
{"namespace": "math.safe_exp", "completion": "  # Define the safe range for the exponential function\n  safe_range = (-50, 50)\n\n  # Define the custom gradient function for backpropagation\n  def custom_grad(dy):\n    # Clip the input gradient to the safe range\n    dy = tf.clip_by_value(dy, safe_range[0], safe_range[1])\n    # Compute the gradient of the safe exponential function\n    return dy * tf.exp(tf.clip_by_value(x, safe_range[0], safe_range[1]))\n\n  # Compute the safe exponential function using the custom gradient function\n  return tf.custom_gradient(tf.exp(x), custom_grad)\n\n"}
{"namespace": "math.safe_exp", "completion": "  # Define a helper function to compute the safe exponential function\n  def safe_exp_helper(x):\n    # Define a custom gradient function for backpropagation\n    def safe_exp_grad(dy):\n      return dy * safe_exp_helper(x)\n    # Compute the safe exponential function using the custom gradient function\n    return tf.exp(tf.minimum(x, 88.0)) + 1e-8, safe_exp_grad\n\n  # Compute the safe exponential function using the helper function\n  return tf.custom_gradient(safe_exp_helper)(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  # Define the safe range for the exponential function\n  safe_range = 30\n\n  # Define the custom gradient function for backpropagation\n  def custom_grad(dy):\n    # Compute the gradient of the safe exponential function\n    return dy * jnp.exp(jnp.minimum(x, safe_range))\n\n  # Compute the safe exponential function using jax.custom_vjp\n  return jax.custom_vjp(lambda x: jnp.exp(jnp.minimum(x, safe_range)), custom_grad)\n"}
{"namespace": "math.safe_exp", "completion": "  # Define the safe range for the exponential function\n  safe_range = [-50, 50]\n\n  # Define the safe exponential function\n  def safe_exp_helper(x):\n      # Clip the input value to the safe range\n      x_clipped = tf.clip_by_value(x, safe_range[0], safe_range[1])\n      # Compute the exponential function\n      return tf.exp(x_clipped)\n\n  # Define the custom gradient function for backpropagation\n  def safe_exp_grad(dy, variables):\n      # Compute the gradient of the safe exponential function\n      return dy * safe_exp_helper(x)\n\n  # Apply the safe exponential function to the input value\n  return tf.custom_gradient(safe_exp_helper)(x, safe_exp_grad)\n\n"}
{"namespace": "math.safe_exp", "completion": "  # Define the safe range for the input value\n  safe_range = 50\n\n  # Define the custom gradient function for backpropagation\n  def custom_grad(dy):\n    # Compute the gradient of the safe exponential function\n    return dy * jnp.exp(jnp.minimum(x, safe_range))\n\n  # Apply the safe exponential function to the input value\n  return jnp.exp(jnp.minimum(x, safe_range)), custom_grad\n\n"}
{"namespace": "math.safe_exp", "completion": "  def custom_gradient(dy):\n    return dy * (1 - dy)\n\n  return jax.custom_jvp(safe_exp_inner, safe_exp_inner_p, x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    # Define a safe range for the input to the exponential function\n    safe_range = (-50, 50)\n\n    # Clip the input to the safe range\n    x_clipped = jnp.clip(x, safe_range[0], safe_range[1])\n\n    # Compute the exponential function on the clipped input\n    y = jnp.exp(x_clipped)\n\n    # Define the custom gradient function\n    def grad_fn(dy):\n      # Compute the gradient of the exponential function\n      grad = y * dy\n\n      # Clip the gradient to the safe range\n      grad_clipped = jnp.clip(grad, safe_range[0], safe_range[1])\n\n      return grad_clipped\n\n    # Return the result and the custom gradient function\n    return y, grad_fn\n\n  # Apply the safe exponential function to the input\n  return safe_exp_helper(x)\n"}
{"namespace": "math.safe_exp", "completion": "  # Define a custom gradient function for the safe exponential function\n  def safe_exp_grad(dy):\n    # Compute the gradient of the safe exponential function with respect to its input\n    # The gradient is the product of the input gradient and the safe exponential function value\n    return dy * safe_exp(x)\n\n  # Define a helper function to generate the safe exponential function\n  def safe_exp_helper(x):\n    # Compute the safe exponential function using the custom gradient function\n    return x.exp(gradient=safe_exp_grad)\n\n  # Define the safe exponential function using the helper function\n  safe_exp_func = safe_exp_helper(x)\n\n  # Return the result of applying the safe exponential function to the input x\n  return safe_exp_func\n\n"}
{"namespace": "math.safe_exp", "completion": "  def grad(dy):\n    return dy * (1 - dy)\n\n  def exp_func(x):\n    return tf.exp(tf.minimum(x, 30))\n\n  return tf.custom_gradient(exp_func)(x)\n"}
{"namespace": "math.safe_exp", "completion": "  # Define the safe range for the exponential function\n  safe_range = (0, 50)\n\n  # Define the custom gradient function\n  def custom_grad(dy):\n    # Clip the gradient to prevent overflow\n    clipped_dy = tf.clip_by_value(dy, -1e3, 1e3)\n    return clipped_dy * tf.exp(x)\n\n  # Create the safe exponential function using the custom gradient function\n  safe_exp_fn = lambda x: tf.exp(x)\n  safe_exp_fn.gradient = custom_grad\n\n  # Apply the safe exponential function to the input\n  return safe_exp_fn(x)"}
{"namespace": "math.safe_exp", "completion": "  # Define the safe range for the input value\n  safe_range = (-20, 20)\n\n  # Define the safe exponential function\n  def safe_exp_helper(x):\n      # Clip the input value within the safe range\n      x_clipped = tf.clip_by_value(x, safe_range[0], safe_range[1])\n      # Compute the exponential of the clipped value\n      return tf.exp(x_clipped)\n\n  # Define the custom gradient function for backpropagation\n  @tf.custom_gradient\n  def safe_exp_grad(x):\n      # Compute the exponential of the input value\n      y = safe_exp_helper(x)\n      # Define the gradient function\n      def grad(dy):\n          # Compute the gradient of the exponential function\n          return dy * y\n      # Return the computed exponential value and the gradient function\n      return y, grad\n\n  # Compute the safe exponential value using the custom gradient function\n  return safe_exp_grad(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  # Define the safe range for the exponential function\n  safe_range = (-50, 50)\n\n  # Define the custom gradient function for backpropagation\n  def custom_grad(dy):\n    # Check if the input is within the safe range\n    if x < safe_range[0] or x > safe_range[1]:\n      # If outside the safe range, return 0 for the gradient\n      return 0.0\n    else:\n      # If within the safe range, return the gradient of the exponential function\n      return dy * tf.exp(x)\n\n  # Apply the custom gradient function to the input x\n  return tf.exp(x, custom_grad)\n\n"}
{"namespace": "math.safe_exp", "completion": "  return safe_exp_helper(x, 10.0)\n"}
{"namespace": "math.safe_exp", "completion": "  # Define the custom gradient function for the safe exponential function\n  def grad_safe_exp(dy):\n    # Compute the gradient of the safe exponential function using the input and output values\n    return dy * (1 - dy)\n\n  # Define the safe exponential function using the custom gradient function\n  return grad_safe_exp(safe_exp_inner(x))\n"}
{"namespace": "math.safe_exp", "completion": "  # Define a custom safe exponential function\n  def safe_exp_fn(x):\n    # Define the safe range for the exponential function\n    safe_range = [-50, 50]\n\n    # Clamp the input to the safe range\n    x_clamped = tf.clip_by_value(x, safe_range[0], safe_range[1])\n\n    # Compute the exponential function\n    return tf.exp(x_clamped)\n\n  # Define a custom gradient function for the safe exponential function\n  def safe_exp_grad(dy, variables):\n    # Get the input value\n    x = variables[0]\n\n    # Compute the gradient of the safe exponential function\n    return dy * safe_exp_fn(x)\n\n  # Apply the custom gradient function to the safe exponential function\n  return tf.custom_gradient(safe_exp_fn)(x, grad_fn=safe_exp_grad)\n"}
{"namespace": "math.safe_exp", "completion": "  # Define the threshold for the safe exponential function\n  threshold = 60.0\n\n  # Define the safe exponential function\n  def safe_exp_fn(x):\n    # Compute the safe exponential function using the threshold\n    return tf.where(x < threshold, tf.exp(x), tf.exp(threshold))\n\n  # Define the custom gradient function for the safe exponential function\n  def safe_exp_grad(x, grad):\n    # Compute the gradient of the safe exponential function using the threshold\n    return grad * tf.where(x < threshold, tf.exp(x), tf.exp(threshold))\n\n  # Create the safe exponential function using the custom gradient function\n  safe_exp_fn_with_grad = tf.custom_gradient(safe_exp_fn, safe_exp_grad)\n\n  # Apply the safe exponential function to the input x\n  return safe_exp_fn_with_grad(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  # Define the safe range for the exponential function\n  safe_range = (-100, 100)\n\n  # Define the custom gradient function for backpropagation\n  def custom_grad(dy):\n    # Check if the input is within the safe range\n    if x < safe_range[0] or x > safe_range[1]:\n      # If the input is outside the safe range, return 0 as the gradient\n      return 0.0\n    else:\n      # If the input is within the safe range, compute the gradient using the normal exponential function\n      return dy * tf.exp(x)\n\n  # Apply the safe exponential function to the input x\n  safe_exp_x = tf.where(x < safe_range[0], tf.constant(0.0, dtype=tf.float32),\n                        tf.where(x > safe_range[1], tf.constant(tf.exp(safe_range[1]), dtype=tf.float32), tf.exp(x)))\n\n  # Register the custom gradient function for the safe exponential function\n  safe_exp_x = tf.RegisterGradient(\"SafeExp\")(safe_exp_x, custom_grad)\n\n  # Return the result of the safe exponential function\n  return safe_exp_x"}
{"namespace": "math.safe_exp", "completion": "  import torch\n  import torch.nn as nn\n\n  # Define the custom gradient function for the safe exponential function\n  def safe_exp_grad(x):\n      return torch.exp(x)\n\n  # Define the safe exponential function using the custom gradient function\n  safe_exp_func = lambda x: torch.exp(x.clamp(-100, 100))\n  safe_exp_func.custom_grad = safe_exp_grad\n\n  # Create a PyTorch module that uses the safe exponential function\n  class SafeExp(nn.Module):\n      def forward(self, x):\n          return safe_exp_func(x)\n\n  # Create an instance of the SafeExp module and apply it to the input x\n  safe_exp_module = SafeExp()\n  safe_exp_output = safe_exp_module(x)\n\n  return safe_exp_output\n"}
{"namespace": "math.safe_exp", "completion": "  # Define a custom safe exponential function\n  def safe_exp_fn(x):\n    # Define the range of safe values for the exponential function\n    safe_min = -80.0\n    safe_max = 80.0\n    # Clamp the input value to the safe range\n    x_clamped = jnp.clip(x, safe_min, safe_max)\n    # Compute the exponential function on the clamped value\n    return jnp.exp(x_clamped)\n\n  # Define the custom gradient function for backpropagation\n  def safe_exp_grad_fn(x):\n    # Compute the gradient of the safe exponential function\n    return safe_exp_fn(x)\n\n  # Apply the safe exponential function to the input value\n  return jax.custom_vjp(safe_exp_fn)(x)\n"}
{"namespace": "math.safe_exp", "completion": "  def exp_safe(x):\n    \"\"\"\n    The function computes the exponential function for the input x, but with a limit to prevent overflow. It uses the exp function from the math module.\n\n    Input-Output Arguments\n    :param x: The input value for which the exponential function is computed.\n    :return: The result of applying the exponential function to the input x, limited to a specified range to prevent overflow.\n    \"\"\"\n    MAX_EXP = 100\n    return torch.exp(torch.minimum(x, torch.tensor(MAX_EXP, dtype=torch.float32)))\n\n  @custom_gradient\n  def safe_exp_backward(x):\n    \"\"\"\n    The function computes the gradient of the safe exponential function with respect to its input. It uses the exp_safe function to compute the gradient of the exponential function, but with a limit to prevent overflow.\n\n    Input-Output Arguments\n    :param x: The input value for which the gradient of the safe exponential function is computed.\n    :return: The gradient of the safe exponential function with respect to its input.\n    \"\"\"\n    exp = exp_safe(x)\n    def grad(dy):\n      return dy * exp\n    return exp, grad\n\n  return safe_exp_backward(x)\n"}
{"namespace": "math.safe_log", "completion": "  safe_log_fn = generate_safe_fn(jnp.log)\n  return safe_log_fn(x)\n"}
{"namespace": "math.safe_log", "completion": "  safe_log = generate_safe_fn(jnp.log, lambda x: 1 / x, lambda x: x > 0)\n  return safe_log(x)"}
{"namespace": "math.safe_log", "completion": "  safe_log = generate_safe_fn(jnp.log, jnp.log(1.0), jnp.log(1.0))\n  return safe_log(x)"}
{"namespace": "math.safe_log", "completion": "  safe_log = generate_safe_fn(jnp.log, lambda x: 1 / x, lambda x: x > 0)\n  return safe_log(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  safe_log = generate_safe_fn(jnp.log, jnp.zeros_like(x), lambda x: x > 0)\n  return safe_log(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  safe_log = generate_safe_fn(jnp.log, jnp.ones_like, 1e-8, 1e8)\n  return safe_log(x)"}
{"namespace": "math.safe_log", "completion": "  safe_log = generate_safe_fn(jnp.log, jnp.nan, jnp.log(1e-6), jnp.log(1e6))\n  return safe_log(x)"}
{"namespace": "math.safe_log", "completion": "  safe_log = generate_safe_fn(jnp.log, jnp.zeros_like, lambda x: x > 0)\n  return safe_log(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log)(x)\n"}
{"namespace": "math.safe_log", "completion": "  safe_log = generate_safe_fn(jnp.log, jnp.log(1e-10), jnp.log(1e10))\n  return safe_log(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  import jax.numpy as jnp\n  from jax import custom_jvp\n\n  safe_log_fn = generate_safe_fn(\n      jnp.log,\n      lambda x: jnp.log(x),\n      lambda x: 1.0 / x,\n      lambda x: -1.0 / x ** 2,\n      lambda x: 2.0 / x ** 3,\n      lambda x: -6.0 / x ** 4,\n      lambda x: 24.0 / x ** 5,\n      lambda x: -120.0 / x ** 6,\n      lambda x: 720.0 / x ** 7,\n      lambda x: -5040.0 / x ** 8,\n      lambda x: 40320.0 / x ** 9,\n      lambda x: -362880.0 / x ** 10,\n      lambda x: 3628800.0 / x ** 11,\n      lambda x: -39916800.0 / x ** 12,\n      lambda x: 479001600.0 / x ** 13,\n      lambda x: -6227020800.0 / x ** 14,\n      lambda x: 87178291200.0 / x ** 15,\n      lambda x: -1307674368000.0 / x ** 16,\n      lambda x: 20922789888000.0 / x ** 17,\n      lambda x: -355687428096000.0 / x ** 18,\n      lambda x: 6402373705728000.0 / x ** 19,\n      lambda x: -121645100408"}
{"namespace": "math.safe_log", "completion": "  safe_log = generate_safe_fn(jnp.log, jnp.ones_like(x), lambda x: x >= 0)\n  return safe_log(x)\n"}
{"namespace": "math.safe_log", "completion": "  safe_log = generate_safe_fn(jnp.log, x, fn_name=\"safe_log\", domain=\"pos\")\n\n  return safe_log"}
{"namespace": "math.safe_log", "completion": "  safe_log = generate_safe_fn(jnp.log, \"log\")\n  return safe_log(x)\n"}
{"namespace": "math.safe_log", "completion": "  # Define the safe log function using generate_safe_fn\n  safe_log_fn = generate_safe_fn(jnp.log, jnp.zeros_like, jnp.ones_like, jnp.ones_like, jnp.ones_like)\n\n  # Apply the safe log function to the input x\n  return safe_log_fn(x)"}
{"namespace": "math.safe_log", "completion": "  safe_log = generate_safe_fn(jnp.log, jnp.zeros_like(x), jnp.ones_like(x), jnp.log(1e-7), jnp.log(1e7))\n\n  return safe_log(x)\n"}
{"namespace": "math.safe_log", "completion": "  import jax.numpy as jnp\n  import jax\n\n  safe_log = generate_safe_fn(jnp.log, jnp.zeros_like, jnp.ones_like)\n  return safe_log(x)\n"}
{"namespace": "math.safe_log", "completion": "  safe_log_fn = generate_safe_fn(jnp.log, lambda x: 1/x)\n  return safe_log_fn(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, x, lambda x: jnp.log(x) - 1., 1e-8, 1e8)\n\n"}
{"namespace": "math.safe_log", "completion": "  import jax.numpy as jnp\n  from jax.scipy.special import logsumexp\n  from jax.scipy.special import xlogy\n\n  def safe_log_fn(x):\n\n    \"\"\"\n    This is a nested function that defines the core logic for the safe logarithm. It takes a single input `x` and applies various operations to ensure numerical stability and safety.\n\n    Input-Output Arguments\n    :param x: The input value or array for which the logarithm is to be calculated.\n    :return: The result of applying the safe logarithm function to `x`. This could be a single value or an array of values, depending on the input `x`.\n    \"\"\"\n\n    # Define the lower bound for the logarithm.\n    lower_bound = 1e-30\n\n    # Clip the input value to ensure it is within a reasonable range.\n    x = jnp.clip(x, lower_bound, None)\n\n    # Calculate the logarithm using the JAX numpy logarithm function.\n    log_x = jnp.log(x)\n\n    # Calculate the log of the lower bound.\n    log_lower_bound = jnp.log(lower_bound)\n\n    # Calculate the difference between the log of the input and the log of the lower bound.\n    log_x_minus_lower_bound = log_x - log_lower_bound\n\n    # Calculate the log of the input value plus a small value to avoid division by zero.\n    log_x_plus_eps = jnp.log(x + 1e-7)\n\n    # Calculate the log of the lower bound plus a small value to avoid division by zero.\n    log_lower_bound_plus_eps = jnp.log(lower_bound + 1e-7)\n\n    # Calculate the difference between the log of the input plus a small value and the log of the lower bound plus a small value.\n    log_x_plus_eps_minus_lower_bound_plus_eps = log_x_plus_eps - log_lower_bound_plus_eps\n\n    # Calculate the derivative of the"}
{"namespace": "math.safe_sqrt", "completion": "  def sqrt_grad(grad):\n    return grad / (2.0 * res)\n\n  def safe_sqrt_internal(x, eps=1e-7):\n    x = max(x, eps)\n    return x ** 0.5\n\n  res = safe_sqrt_internal(x)\n  res.register_hook(sqrt_grad)\n  return res"}
{"namespace": "math.safe_sqrt", "completion": "  import jax.numpy as jnp\n  from jax import custom_jvp\n\n  @custom_jvp\n  def safe_sqrt(x):\n    return jnp.sqrt(x)\n\n  @safe_sqrt.defjvp\n  def safe_sqrt_jvp(primals, tangents):\n    x, = primals\n    x_dot, = tangents\n    ans = safe_sqrt(x)\n    ans_dot = 0.5 * x_dot / jnp.sqrt(x)\n    return ans, ans_dot\n\n  return safe_sqrt\n"}
{"namespace": "math.safe_sqrt", "completion": "  import jax.numpy as jnp\n  from jax import custom_jvp\n\n  @custom_jvp\n  def safe_sqrt(x):\n    return jnp.sqrt(jnp.clip(x, a_min=0, a_max=1e30))\n\n  @safe_sqrt.defjvp\n  def safe_sqrt_jvp(primals, tangents):\n    x, = primals\n    x_dot, = tangents\n    ans = safe_sqrt(x)\n    ans_dot = (x_dot * 0.5 * jnp.reciprocal(ans))\n    return ans, ans_dot\n\n  return safe_sqrt(x)\n"}
{"namespace": "math.safe_sqrt", "completion": "  def sqrt_grad(grad):\n    return grad / (2.0 * result)\n\n  result = tf.math.sqrt(x)\n  result = tf.where(tf.math.is_nan(result), 0.0, result)\n  result = tf.clip_by_value(result, 0.0, 1e9)\n  result = tf.register_gradient(sqrt_grad)(result)\n  return result\n"}
{"namespace": "math.safe_sqrt", "completion": "  def sqrt_grad(grad):\n\n    \"\"\"\n    This function computes the gradient of the square root function with respect to its input. It is used as a custom gradient function for automatic differentiation purposes.\n\n    Input-Output Arguments\n    :param grad: The gradient of the output of the square root function with respect to the input.\n    :return: The gradient of the input with respect to the output of the square root function.\n    \"\"\"\n\n    return grad / (2.0 * y)\n\n  y = tf.math.sqrt(tf.clip_by_value(x, 0, 1e100))\n  return y + tf.cast(tf.experimental.numpy.is_nan(y), tf.float64) * (y + 1e-7)\n"}
{"namespace": "math.safe_sqrt", "completion": "  # Import the necessary libraries\n  import tensorflow as tf\n  import numpy as np\n\n  # Define the maximum value for clamping\n  max_value = np.sqrt(np.finfo(x.dtype.as_numpy_dtype).max)\n\n  # Define the custom gradient function for backpropagation\n  def grad(dy):\n    return dy * 0.5 * tf.math.rsqrt(tf.maximum(x, 0.0))\n\n  # Define the safe square root function using the tf.custom_gradient decorator\n  @tf.custom_gradient\n  def safe_sqrt(x):\n    # Clamp the input value between 0 and the maximum value\n    x = tf.clip_by_value(x, 0.0, max_value)\n    # Compute the square root of the clamped value\n    y = tf.math.sqrt(x)\n    # Return the square root and the custom gradient function\n    return y, grad\n\n  # Return the result of applying the safe square root function to the input x\n  return safe_sqrt(x)\n"}
{"namespace": "math.safe_sqrt", "completion": "  def grad(dy):\n    return dy / (2.0 * y)\n\n  y = jnp.sqrt(jnp.where(x > 0, x, 0))\n  return y, grad\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  import jax\n  import jax.numpy as jnp\n\n  @jax.custom_jvp\n  def safe_sqrt(x):\n    return jnp.sqrt(jnp.maximum(x, 0))\n\n  @safe_sqrt.defjvp\n  def safe_sqrt_jvp(primals, tangents):\n    x, = primals\n    x_dot, = tangents\n    ans = safe_sqrt(x)\n    ans_dot = 0.5 * x_dot / ans\n    return ans, ans_dot\n\n  return safe_sqrt(x)\n"}
{"namespace": "math.safe_sqrt", "completion": "  import jax\n  import jax.numpy as jnp\n\n  # Define a helper function to generate the safe square root function\n  def safe_sqrt_helper(x):\n\n    # Define the maximum value for the input\n    max_value = jnp.finfo(x.dtype).max\n\n    # Clamp the input between 0 and the maximum value\n    x = jnp.clip(x, 0, max_value)\n\n    # Compute the square root of the clamped input\n    y = jnp.sqrt(x)\n\n    # Define a custom gradient function for backpropagation\n    def grad_fn(dy):\n      return 0.5 * dy / jnp.sqrt(x)\n\n    # Return the square root result and the custom gradient function\n    return y, grad_fn\n\n  # Apply the helper function to the input x using vmap to handle batch inputs\n  return jax.vmap(safe_sqrt_helper)(x)\n"}
{"namespace": "math.safe_sqrt", "completion": "  import tensorflow as tf\n  import tensorflow.keras.backend as K\n\n  def _safe_sqrt(x, eps=K.epsilon()):\n    return K.sqrt(K.maximum(x, eps))\n\n  def _grad_safe_sqrt(op, grad):\n    x = op.inputs[0]\n    x_sqrt = K.sqrt(K.maximum(x, K.epsilon()))\n    return grad / (2.0 * x_sqrt)\n\n  safe_sqrt_op = tf.RegisterGradient(\"SafeSqrt\")(_safe_sqrt)\n  g = tf.get_default_graph()\n  g.gradient_override_map({\"SafeSqrt\": \"SafeSqrtGrad\"})\n\n  with g.gradient_override_map({\"PyFunc\": \"SafeSqrtGrad\"}):\n    safe_sqrt_x = tf.py_func(_safe_sqrt, [x], tf.float32, name=\"SafeSqrt\", grad=_grad_safe_sqrt)\n\n  return safe_sqrt_x\n"}
{"namespace": "math.safe_sqrt", "completion": "  import jax\n  import jax.numpy as jnp\n\n  # Define a helper function to compute the square root of a positive value\n  def safe_sqrt_helper(x):\n      return jnp.sqrt(x)\n\n  # Define the custom gradient function for backpropagation\n  def safe_sqrt_grad(dy, variables, parameters):\n      x, = variables\n      safe_x = jnp.maximum(x, 0)\n      return dy * 0.5 / jnp.sqrt(safe_x)\n\n  # Create a version of the square root function that is safe for backpropagation\n  safe_sqrt = jax.custom_vjp(safe_sqrt_helper)\n  safe_sqrt.defvjp(safe_sqrt_grad)\n\n  # Return the safe square root of the input value\n  return safe_sqrt(x)\n"}
{"namespace": "math.safe_sqrt", "completion": "  import tensorflow as tf\n\n  @tf.custom_gradient\n  def safe_sqrt(x):\n    \"\"\"\n    This function computes the safe square root of a given input tensor x. It ensures that the input is clamped between 0 and a maximum value to avoid invalid inputs like negative numbers.\n\n    Input-Output Arguments\n    :param x: The input tensor for which the safe square root will be computed.\n    :return: The result of applying the safe square root function to the input x.\n    \"\"\"\n    # Clamp the input tensor between 0 and a maximum value\n    eps = 1e-8\n    x = tf.clip_by_value(x, eps, tf.reduce_max(x))\n\n    # Compute the square root of the clamped input tensor\n    y = tf.sqrt(x)\n\n    def grad(dy):\n      \"\"\"\n      This function computes the gradient of the safe square root function with respect to its input tensor x. It is used for automatic differentiation during the backward pass of a computation graph.\n\n      Input-Output Arguments\n      :param dy: The gradient of the output tensor with respect to the output of the safe square root function.\n      :return: The gradient of the output tensor with respect to the input tensor x.\n      \"\"\"\n      # Compute the gradient of the safe square root function with respect to x\n      return dy / (2.0 * y)\n\n    return y, grad\n\n  return safe_sqrt(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  import torch\n\n  # Define a custom gradient function for the safe square root function\n  def safe_sqrt_grad(grad_output):\n    # Compute the gradient of the safe square root function with respect to its input\n    grad_input = grad_output / (2 * torch.sqrt(torch.clamp(x, min=0, max=1e10)))\n    # Set the gradient of the input to zero for any input values that are outside the valid range\n    grad_input[x < 0] = 0\n    return grad_input\n\n  # Define the safe square root function using the custom gradient function\n  safe_sqrt_func = torch.ops.torch_ipex.custom_sqrt_forward(x, safe_sqrt_grad)\n\n  # Define the safe square root function using the custom gradient function\n  def safe_sqrt(x):\n    # Compute the safe square root of the input using the custom gradient function\n    return safe_sqrt_func(x)\n\n  # Return the safe square root function\n  return safe_sqrt\n"}
{"namespace": "math.safe_sqrt", "completion": "  import tensorflow as tf\n\n  # Define the safe square root function and its custom gradient\n  def safe_sqrt_fn(x):\n    # Ensure the input is non-negative\n    x = tf.maximum(x, 0.0)\n    # Compute the square root\n    y = tf.sqrt(x)\n    # Return the result and the custom gradient function\n    return y, lambda dy: dy / (2.0 * y)\n\n  # Create a TensorFlow function with the safe square root function and its custom gradient\n  safe_sqrt_tf = tf.function(safe_sqrt_fn)\n\n  # Apply the safe square root function to the input x\n  y = safe_sqrt_tf(x)\n\n  return y\n"}
{"namespace": "math.safe_sqrt", "completion": "  from tensorflow.python.framework import ops\n  from tensorflow.python.ops import array_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import control_flow_ops\n\n  def _safe_sqrt(x, name=None):\n    with ops.name_scope(name, \"SafeSqrt\", [x]) as name:\n      x = ops.convert_to_tensor(x, name=\"x\")\n      # Use absolute value of x for sqrt to avoid NaN gradient where x < 0.\n      return math_ops.sqrt(math_ops.abs(x), name=name)\n\n  def _safe_sqrt_grad(op, grad):\n    x = op.inputs[0]\n    # Derivative of sqrt(x) is 0.5 / sqrt(x), but is not defined at x = 0.\n    # Grad for sqrt at 0 is 0, so add a branch to handle this case.\n    safe_x = array_ops.where(x == 0., 0., x)\n    return 0.5 * grad / _safe_sqrt(safe_x)\n\n  safe_sqrt_op = math_ops.sqrt\n  safe_sqrt_op._add_derivative(math_ops.equal, _safe_sqrt_grad)\n\n  return safe_sqrt_op(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  import torch\n\n  def sqrt_with_grad(x, eps=1e-8):\n\n    \"\"\"\n    This function computes the square root of a given input tensor x, while also computing the gradient of the square root function with respect to the input tensor x. It uses the power rule of differentiation to compute the gradient.\n\n    Input-Output Arguments\n    :param x: The input tensor for which the square root will be computed.\n    :param eps: A small value added to the input tensor x to avoid division by zero.\n    :return: The square root of the input tensor x and the gradient of the square root function with respect to x.\n    \"\"\"\n\n    # Compute the square root of x\n    sqrt_x = torch.sqrt(x + eps)\n\n    # Compute the gradient of the square root function with respect to x\n    grad = 0.5 * torch.pow(x + eps, -0.5)\n\n    return sqrt_x, grad\n\n  # Define the maximum value for the input tensor x\n  max_val = 1e8\n\n  # Clamp the input tensor x to the range [0, max_val]\n  x = torch.clamp(x, 0, max_val)\n\n  # Compute the square root of x and its gradient using the sqrt_with_grad function\n  sqrt_x, grad = sqrt_with_grad(x)\n\n  # Return the square root of x and its gradient\n  return sqrt_x, grad\n"}
{"namespace": "math.safe_sqrt", "completion": "  # Define a helper function to compute the square root of x\n  def sqrt_helper(x):\n    # Clamp the input value between 0 and a maximum value to avoid invalid inputs\n    x = jnp.clip(x, a_min=0, a_max=1e12)\n    # Compute the square root of x\n    return jnp.sqrt(x)\n\n  # Define the custom gradient function for the square root function\n  def sqrt_grad(dy, variables, parameters):\n    # Extract the input value from the variables\n    x = variables[0]\n    # Compute the gradient of the square root function with respect to x\n    return 0.5 * dy / jnp.sqrt(x)\n\n  # Create a safe version of the square root function using the helper function and custom gradient\n  safe_sqrt = make_custom_vjp(sqrt_helper)\n  # Set the custom gradient function for the safe square root function\n  safe_sqrt.defvjp(sqrt_grad)\n\n  # Return the result of applying the safe square root function to x\n  return safe_sqrt(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  import tensorflow as tf\n  import numpy as np\n\n  def safe_sqrt_helper(x, eps=1e-8):\n    \"\"\"\n    This is a helper function that computes the safe square root of a given input x. It uses a small epsilon value eps to avoid division by zero. The function returns the square root of x if x is greater than eps, otherwise it returns eps.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed.\n    :param eps: The small value used to avoid division by zero. It is set to 1e-8 by default.\n    :return: The safe square root of x, which is either the square root of x if x is greater than eps, or eps otherwise.\n    \"\"\"\n    return tf.sqrt(tf.maximum(x, eps))\n\n  def safe_sqrt_grad(op, grad):\n    \"\"\"\n    This function computes the gradient of the safe square root operation with respect to its input. It takes the operation op and its gradient grad as input and returns the gradient of the safe square root operation with respect to its input.\n\n    Input-Output Arguments\n    :param op: The operation for which the gradient is being computed. In this case, it is the safe square root operation.\n    :param grad: The gradient of the output of the safe square root operation with respect to the output.\n    :return: The gradient of the safe square root operation with respect to its input.\n    \"\"\"\n    x = op.inputs[0]\n    eps = 1e-8\n    sqrt_x = tf.sqrt(tf.maximum(x, eps))\n    grad_x = 0.5 * grad / sqrt_x\n    return grad_x\n\n  def safe_sqrt_op(x):\n    \"\"\"\n    This function creates a TensorFlow operation that computes the safe square root of a given input x. It uses the safe_sqrt_helper function to compute the square root and the safe_sqrt_grad function to compute the gradient. The function returns the output of the safe square root operation.\n\n    Input-Output Arguments"}
{"namespace": "math.safe_sqrt", "completion": "  from numpy import sqrt, clip, abs\n  from tensorflow import custom_gradient\n\n  @custom_gradient\n  def safe_sqrt_inner(x):\n\n    \"\"\"\n    This function is the inner function that computes the safe square root of the input x. It uses the numpy.sqrt function to compute the square root, and the numpy.clip function to ensure that the input is clamped between 0 and a maximum value to avoid invalid inputs like negative numbers.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function.\n    :return: The result of applying the safe square root function to the input x.\n    \"\"\"\n\n    y = sqrt(clip(x, 0, 1e128))\n\n    def grad(dy):\n\n      \"\"\"\n      This function computes the gradient of the safe square root function with respect to its input. It uses the chain rule to compute the gradient of the square root function with respect to the input and the clamping function.\n\n      Input-Output Arguments\n      :param dy: The gradient of the output with respect to the input. This is used to compute the gradient of the square root function with respect to the input.\n      :return: The gradient of the safe square root function with respect to the input.\n      \"\"\"\n\n      return dy / (2 * sqrt(clip(x, 0, 1e128)))\n\n    return y, grad\n\n  return safe_sqrt_inner(x)"}
{"namespace": "math.safe_sqrt", "completion": "  # Define a safe square root function\n  def safe_sqrt_fn(x):\n    # Clamp the input between 0 and a maximum value\n    x = tf.clip_by_value(x, 0, 1e30)\n    # Compute the square root\n    return tf.sqrt(x)\n\n  # Define the custom gradient function\n  def safe_sqrt_grad(dy, variables):\n    # Compute the gradient of the square root function\n    return dy / (2 * safe_sqrt_fn(variables[0]))\n\n  # Apply the safe square root function and its custom gradient\n  return tf.custom_gradient(safe_sqrt_fn, safe_sqrt_grad)(x)\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 0:\n    return 0\n  elif p == 0:\n    return 1\n  else:\n    return float(\"inf\")\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p > 0:\n    return 1\n  else:\n    return -1\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 1\n  elif p > 0:\n    return 0\n  else:\n    return None\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p < 0:\n    return -1\n  else:\n    return 1\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p > 0:\n    return 0\n  elif p < 0:\n    return \"Infinity\"\n  else:\n    return 1\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p < 0:\n    return 1\n  elif p > 0:\n    return None\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p > 0:\n    return 1\n  elif p < 0:\n    return 0\n  else:\n    return \"Invalid input\"\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p > 0:\n    return 0\n  elif p == 0:\n    return 1\n  else:\n    return \"undefined\"\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p > 0:\n    return 1\n  else:\n    return -1\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p > 1:\n    return 1\n  elif p == 1:\n    return 0\n  elif p < 1:\n    return -1\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 0:\n    return 0\n  elif p == 0:\n    return 1\n  else:\n    return float(\"inf\")\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 1\n  elif p > 0:\n    return 0\n  elif p < 0:\n    return \"Infinity\"\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p < 0:\n    return -1\n  elif p > 0:\n    return 1\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 0:\n    return 0\n  elif p > 1:\n    return 1\n  elif p == 0:\n    return 1\n  elif p == 1:\n    return 0\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p < 0:\n    return float(\"inf\")\n  elif p > 0:\n    return 1\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 1\n  elif p < 0:\n    return 0\n  else:\n    return \"undefined\"\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p > 1:\n    return 0\n  elif p == 1:\n    return 1\n  else:\n    return \"undefined\"\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 1\n  elif p > 0:\n    return 0\n  else:\n    return \"undefined\"\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 0:\n    return 0\n  elif p == 0:\n    return 1\n  elif p > 0:\n    return 'Infinity'\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p < 0:\n    return 1\n  elif p > 0:\n    return \"Infinity\"\n"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n\n  # Define the vertices of the starting polyhedron\n  if base_shape == \"tetrahedron\":\n    vertices = np.array(\n        [\n            [1, 1, 1],\n            [-1, -1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n        ]\n    )\n  elif base_shape == \"icosahedron\":\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, -1],\n        ]\n    )\n  elif base_shape == \"octahedron\":\n    vertices = np.array(\n        [\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1],\n        ]\n    )\n  else:\n    raise ValueError(\"Invalid base_shape. Must be 'tetrahedron', 'icosahedron', or 'octahedron'.\")\n\n  # Normalize the vertices to have unit length\n  vertices = vertices / np.linalg.norm(vertices, axis=1)[:, np.newaxis]\n\n  # Tessellate the polyhedron\n  for _ in range(angular_tesselation):\n    new_vertices = []\n    for i in range(len(vertices)):\n      for j in"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n\n  # Define the vertices of the starting polyhedron\n  if base_shape == \"tetrahedron\":\n    vertices = np.array(\n        [\n            [1, 1, 1],\n            [-1, -1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n        ]\n    )\n  elif base_shape == \"icosahedron\":\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, -1],\n        ]\n    )\n  elif base_shape == \"octahedron\":\n    vertices = np.array(\n        [\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1],\n        ]\n    )\n  else:\n    raise ValueError(\"Invalid base shape. Choose 'tetrahedron', 'icosahedron', or 'octahedron'.\")\n\n  # Normalize the vertices to have unit norm\n  vertices = vertices / np.linalg.norm(vertices, axis=1)[:, np.newaxis]\n\n  # Tessellate the polyhedron\n  for _ in range(angular_tesselation):\n    new_vertices = []\n    for v1, v2 in zip(vertices, np.roll(vertices"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n\n  # Define the vertices and faces of the starting polyhedron\n  if base_shape == \"tetrahedron\":\n    vertices = np.array(\n        [\n            [1, 1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n            [-1, -1, 1],\n        ]\n    )\n    faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == \"icosahedron\":\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n    faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 8],\n            [0, 8, 9],\n            [0, 9, 1],\n            [1, 9, 10],\n            [1, 10, 5],\n            [1, 5, 2],\n            [2, 5, 11],\n            [2, 11, 6],\n            [2, 6, 3],\n            [3, 6, 10],\n            [3, 10, 9],\n            [3, 9, 8],\n            [3, 8, 7],\n            [3, 7"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  import trimesh\n\n  if base_shape == \"tetrahedron\":\n    vertices = np.array([[1, 1, 1], [1, -1, -1], [-1, 1, -1], [-1, -1, 1]]) / np.sqrt(3)\n    faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == \"icosahedron\":\n    vertices = np.array(\n        [\n            [0, 1, 0.5257311121191336],\n            [0.8506508083520399, 0, -0.5257311121191336],\n            [0.8506508083520399, 0, 0.5257311121191336],\n            [-0.8506508083520399, 0, -0.5257311121191336],\n            [-0.8506508083520399, 0, 0.5257311121191336],\n            [0, -1, 0.5257311121191336],\n            [0.8506508083520399, 0.5257311121191336, 0],\n            [-0.8506508083520399, 0.5257311121191336, 0],\n            [0, -0.5257311121191336"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  import trimesh\n\n  if base_shape == \"tetrahedron\":\n    base_vertices = np.array(\n        [\n            [0, 0, 0],\n            [1, 0, 0],\n            [0.5, np.sqrt(3) / 2, 0],\n            [0.5, np.sqrt(3) / 6, np.sqrt(2 / 3)],\n        ]\n    )\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [1, 2, 3], [0, 2, 3]])\n  elif base_shape == \"icosahedron\":\n    base_vertices = np.array(\n        [\n            [0, 0, 1],\n            [0.85065081, 0, -0.52573111],\n            [0.85065081, 0, 0.52573111],\n            [-0.85065081, 0, 0.52573111],\n            [-0.85065081, 0, -0.52573111],\n            [0, 0.85065081, -0.52573111],\n            [0, 0.85065081, 0.52573111],\n            [0, -0.85065081, 0.52573111],\n            [0, -0.85065081, -0.52573111],\n            [0.52573111, 0.85065081, 0],\n            [-0.52573111, 0.85065081, 0],\n            [-0.52"}
{"namespace": "geopoly.generate_basis", "completion": "  # Check if the base_shape is valid\n  if base_shape not in ['tetrahedron', 'icosahedron', 'octahedron']:\n    raise ValueError(\"Invalid base_shape. Must be 'tetrahedron', 'icosahedron', or 'octahedron'.\")\n\n  # Check if angular_tesselation is valid\n  if not isinstance(angular_tesselation, int) or angular_tesselation < 1:\n    raise ValueError(\"Invalid angular_tesselation. Must be a positive integer.\")\n\n  # Check if remove_symmetries is valid\n  if not isinstance(remove_symmetries, bool):\n    raise ValueError(\"Invalid remove_symmetries. Must be a boolean.\")\n\n  # Check if eps is valid\n  if not isinstance(eps, float) or eps <= 0:\n    raise ValueError(\"Invalid eps. Must be a positive float.\")\n\n  # Generate the initial shape\n  if base_shape == 'tetrahedron':\n    vertices = np.array([[1, 1, 1], [-1, -1, 1], [1, -1, -1], [-1, 1, -1]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array([\n        [0, 1, phi],\n        [0, -1, phi],\n        [0, 1, -phi],\n        [0, -1, -phi],\n        [1, phi, 0],\n        [-1, phi, 0],\n        [1, -phi, 0],\n        [-1, -phi, 0],\n        [phi, 0, 1],\n        [-phi, 0, 1],\n        [phi, 0, -1],\n        [-phi, 0, -1]\n    ])\n  elif base_shape == 'octahedron':\n    vertices = np.array([\n        [1, 0, 0],\n        [-1, 0"}
{"namespace": "geopoly.generate_basis", "completion": "  # Import necessary libraries\n  import numpy as np\n  from scipy.spatial import ConvexHull\n\n  # Define the vertices of the starting polyhedron\n  if base_shape == \"tetrahedron\":\n    vertices = np.array(\n        [\n            [1, 1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n            [-1, -1, 1],\n        ]\n    )\n  elif base_shape == \"icosahedron\":\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n  elif base_shape == \"octahedron\":\n    vertices = np.array(\n        [\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1],\n        ]\n    )\n  else:\n    raise ValueError(\n        \"Invalid base shape. Supported shapes are 'tetrahedron', 'icosahedron', and 'octahedron'.\"\n    )\n\n  # Tessellate the polyhedron\n  for _ in range(angular_tesselation):\n    new_vertices = []\n    for v1, v2 in zip(vertices, vertices[1:] + [vertices[0]]):\n      midpoint ="}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  import math\n\n  # Define the vertices and faces of the starting polyhedron\n  if base_shape == \"tetrahedron\":\n    vertices = np.array(\n      [\n        [0.5, 0.0, 0.0],\n        [0.0, 0.5, 0.0],\n        [0.0, 0.0, 0.5],\n        [-0.5, -0.5, -0.5],\n      ]\n    )\n    faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == \"icosahedron\":\n    phi = (1.0 + math.sqrt(5.0)) / 2.0\n    vertices = np.array(\n      [\n        [0.0, 1.0, phi],\n        [0.0, -1.0, phi],\n        [0.0, 1.0, -phi],\n        [0.0, -1.0, -phi],\n        [1.0, phi, 0.0],\n        [-1.0, phi, 0.0],\n        [1.0, -phi, 0.0],\n        [-1.0, -phi, 0.0],\n        [phi, 0.0, 1.0],\n        [-phi, 0.0, 1.0],\n        [phi, 0.0, -1.0],\n        [-phi, 0.0, -1.0],\n      ]\n    )\n    faces = np.array(\n      [\n        [0, 4, 1],\n        [0, 9, 4],\n        [9, 5, 4],\n        [4, 5, 8],\n        [4, 8, 1],\n        [8, 10, 1],\n        [8, 3, 10],\n        [5, "}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  from scipy.spatial import Delaunay\n  from scipy.spatial.transform import Rotation as R\n  from scipy.spatial.transform import Slerp\n\n  if base_shape == \"tetrahedron\":\n    # Define the vertices of the tetrahedron\n    vertices = np.array([[0, 0, 0], [1, 0, 0], [0.5, np.sqrt(3) / 2, 0], [0.5, np.sqrt(3) / 6, np.sqrt(2 / 3)]])\n    # Define the faces of the tetrahedron\n    faces = np.array([[0, 1, 2], [0, 1, 3], [1, 2, 3], [0, 2, 3]])\n  elif base_shape == \"icosahedron\":\n    # Define the vertices of the icosahedron\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array(\n        [\n            [-1, phi, 0],\n            [1, phi, 0],\n            [-1, -phi, 0],\n            [1, -phi, 0],\n            [0, -1, phi],\n            [0, 1, phi],\n            [0, -1, -phi],\n            [0, 1, -phi],\n            [phi, 0, -1],\n            [phi, 0, 1],\n            [-phi, 0, -1],\n            [-phi, 0, 1],\n        ]\n    )\n    # Define the faces of the icosahedron\n    faces = np.array(\n        [\n            [0, 11, 5],\n            [0, 5, 1],\n            [0, 1, 7],\n            [0, 7, 10],\n            [0, 10, 11],\n            [1, 5, 9],\n            [5, 11, "}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  from scipy.spatial import Delaunay\n\n  # Define the vertices of the starting polyhedron\n  if base_shape == \"tetrahedron\":\n    vertices = np.array(\n        [\n            [1, 1, 1],\n            [-1, -1, 1],\n            [-1, 1, -1],\n            [1, -1, -1],\n        ]\n    )\n  elif base_shape == \"icosahedron\":\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, -1],\n        ]\n    )\n  elif base_shape == \"octahedron\":\n    vertices = np.array(\n        [\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1],\n        ]\n    )\n  else:\n    raise ValueError(\n        \"Invalid base_shape. Choose from 'tetrahedron', 'icosahedron', or 'octahedron'.\"\n    )\n\n  # Normalize the vertices to have unit length\n  vertices /= np.linalg.norm(vertices, axis=1)[:, np.newaxis]\n\n  # Tessellate the polyhedron\n  for _ in range(angular_tesselation):\n    # Create a Dela"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  import scipy.spatial\n  import scipy.sparse\n  import scipy.sparse.linalg\n\n  # Define the vertices of the starting polyhedron\n  if base_shape == \"tetrahedron\":\n    vertices = np.array(\n      [\n        [1, 1, 1],\n        [1, -1, -1],\n        [-1, 1, -1],\n        [-1, -1, 1],\n      ]\n    )\n  elif base_shape == \"icosahedron\":\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array(\n      [\n        [0, 1, phi],\n        [0, -1, phi],\n        [0, 1, -phi],\n        [0, -1, -phi],\n        [1, phi, 0],\n        [-1, phi, 0],\n        [1, -phi, 0],\n        [-1, -phi, 0],\n        [phi, 0, 1],\n        [phi, 0, -1],\n        [-phi, 0, 1],\n        [-phi, 0, -1],\n      ]\n    )\n  elif base_shape == \"octahedron\":\n    vertices = np.array(\n      [\n        [1, 0, 0],\n        [-1, 0, 0],\n        [0, 1, 0],\n        [0, -1, 0],\n        [0, 0, 1],\n        [0, 0, -1],\n      ]\n    )\n  else:\n    raise ValueError(\"Invalid base_shape parameter. Must be 'tetrahedron', 'icosahedron', or 'octahedron'.\")\n\n  # Tessellate the polyhedron\n  for _ in range(angular_tesselation):\n    new_vertices = []\n    for i in range(len(vertices)):\n      for j in range(i + 1, len("}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n\n  # Define the vertices and faces of the base shapes\n  tetrahedron_vertices = np.array(\n      [\n          [1, 1, 1],\n          [1, -1, -1],\n          [-1, 1, -1],\n          [-1, -1, 1],\n      ]\n  )\n  tetrahedron_faces = np.array(\n      [\n          [0, 1, 2],\n          [0, 1, 3],\n          [0, 2, 3],\n          [1, 2, 3],\n      ]\n  )\n\n  icosahedron_vertices = np.array(\n      [\n          [0, 1, 0.5257311121191336],\n          [0.8506508083520399, 0, -0.5257311121191336],\n          [0.8506508083520399, 0, 0.5257311121191336],\n          [-0.8506508083520399, 0, -0.5257311121191336],\n          [-0.8506508083520399, 0, 0.5257311121191336],\n          [0, -1, 0.5257311121191336],\n          [0.8506508083520399, 0, 0.5257311121191336],\n          [0.8506508083520399, 0, -0.5257311121191336],\n         "}
{"namespace": "geopoly.generate_basis", "completion": "  # Define the vertices and faces of the starting polyhedron\n  if base_shape == \"tetrahedron\":\n    vertices = np.array(\n        [\n            [1, 1, 1],\n            [-1, -1, 1],\n            [-1, 1, -1],\n            [1, -1, -1],\n        ]\n    )\n    faces = np.array(\n        [\n            [0, 1, 2],\n            [0, 1, 3],\n            [0, 2, 3],\n            [1, 2, 3],\n        ]\n    )\n  elif base_shape == \"icosahedron\":\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, -1],\n        ]\n    )\n    faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 9],\n            [0, 9, 5],\n            [0, 5, 1],\n            [1, 5, 11],\n            [1, 11, 6],\n            [1, 6, 4],\n            [4, 6, 10],\n            [4, 10, 9],\n            [5, 9, 11],\n            [5, 11, 7],\n            [5, 7, 1],\n            [6, 11, 8],\n            [6, 8, "}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n\n  # Define the vertices of the starting polyhedron\n  if base_shape == \"tetrahedron\":\n    vertices = np.array(\n        [\n            [0.5, 0.5, 0.5],\n            [0.5, -0.5, -0.5],\n            [-0.5, 0.5, -0.5],\n            [-0.5, -0.5, 0.5],\n        ]\n    )\n  elif base_shape == \"icosahedron\":\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n  elif base_shape == \"octahedron\":\n    vertices = np.array(\n        [\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1],\n        ]\n    )\n  else:\n    raise ValueError(\n        \"Invalid base_shape. Supported shapes are 'tetrahedron', 'icosahedron', and 'octahedron'.\"\n    )\n\n  # Tessellate the polyhedron\n  for _ in range(angular_tesselation):\n    new_vertices = []\n    for i in range(len(vertices)):\n      for j in range(i + 1, len(vertices)):\n        new_vertex = (vertices[i] + vertices[j]) / 2\n        new_vertices.append(new_vertex)\n    vertices = np.concatenate([vertices, np.array(new_vertices)])\n\n  # Remove symmetric"}
{"namespace": "geopoly.generate_basis", "completion": "  # Define the vertices and faces of the initial polyhedron\n  if base_shape == \"tetrahedron\":\n    vertices = np.array(\n        [[1, 1, 1], [-1, -1, 1], [1, -1, -1], [-1, 1, -1]]\n    ) / np.sqrt(3)\n    faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == \"icosahedron\":\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, -1],\n        ]\n    )\n    faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 8],\n            [0, 8, 9],\n            [0, 9, 1],\n            [1, 9, 10],\n            [1, 10, 5],\n            [1, 5, 6],\n            [1, 6, 0],\n            [2, 3, 7],\n            [2, 7, 11],\n            [2, 11, 10],\n            [2, 10, 6],\n            [3, 2, 6],\n            [3, 6, 5],\n            [3, 5, 7],\n            [3, 7, "}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  from scipy.spatial import Delaunay\n  from scipy.spatial.transform import Rotation\n\n  # Define the vertices of the starting polyhedron\n  if base_shape == \"tetrahedron\":\n    vertices = np.array(\n        [\n            [1, 1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n            [-1, -1, 1],\n        ]\n    )\n  elif base_shape == \"icosahedron\":\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, -1],\n        ]\n    )\n  elif base_shape == \"octahedron\":\n    vertices = np.array(\n        [\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1],\n        ]\n    )\n  else:\n    raise ValueError(\"Invalid base_shape parameter. Must be 'tetrahedron', 'icosahedron', or 'octahedron'.\")\n\n  # Tessellate the polyhedron\n  for i in range(angular_tesselation):\n    tri = Delaunay(vertices)\n    midpoints = []\n    for simplex in tri.simplices:\n      midpoint = np."}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  from scipy.spatial import ConvexHull\n\n  # Define the vertices of the starting polyhedron\n  if base_shape == 'tetrahedron':\n    vertices = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array([\n      [0, 1, phi], [0, -1, phi], [0, 1, -phi], [0, -1, -phi],\n      [1, phi, 0], [-1, phi, 0], [1, -phi, 0], [-1, -phi, 0],\n      [phi, 0, 1], [-phi, 0, 1], [phi, 0, -1], [-phi, 0, -1]\n    ])\n  elif base_shape == 'octahedron':\n    vertices = np.array([\n      [1, 0, 0], [-1, 0, 0], [0, 1, 0], [0, -1, 0],\n      [0, 0, 1], [0, 0, -1]\n    ])\n  else:\n    raise ValueError('Invalid base shape. Choose either \"tetrahedron\", \"icosahedron\", or \"octahedron\".')\n\n  # Tessellate the polyhedron\n  tessellated_vertices = vertices\n  for _ in range(angular_tesselation):\n    tessellated_vertices = np.concatenate([\n      tessellated_vertices + v for v in vertices\n    ])\n\n  # Remove symmetries\n  if remove_symmetries:\n    unique_vertices = []\n    for v in tessellated_vertices:\n      if not any(np.allclose(v, u, atol=eps) for u"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  import math\n\n  if base_shape == \"tetrahedron\":\n    vertices = np.array([[0.5, 0.5, 0.5], [0.5, 0.5, -0.5], [0.5, -0.5, 0.5], [0.5, -0.5, -0.5]])\n    faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == \"icosahedron\":\n    phi = (1 + math.sqrt(5)) / 2\n    vertices = np.array(\n        [\n            [0, 1, phi],\n            [0, 1, -phi],\n            [0, -1, phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [1, -phi, 0],\n            [-1, phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n    faces = np.array(\n        [\n            [0, 2, 8],\n            [0, 4, 10],\n            [0, 5, 11],\n            [0, 6, 9],\n            [0, 8, 4],\n            [0, 8, 6],\n            [0, 10, 5],\n            [0, 10, 9],\n            [0, 11, 6],\n            [0, 11, 4],\n            [1, 3, 7],\n            [1, 3, 9],\n            [1, 4, 8],\n            [1, 4, 11],\n            [1, 5,"}
{"namespace": "geopoly.generate_basis", "completion": "  # Import required libraries\n  import numpy as np\n  from scipy.spatial import Delaunay\n  from scipy.spatial.transform import Rotation as R\n  import trimesh\n\n  # Define the vertices and faces of the starting polyhedron\n  if base_shape == \"tetrahedron\":\n    vertices = np.array([[1, 1, 1], [-1, -1, 1], [1, -1, -1], [-1, 1, -1]])\n    faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == \"icosahedron\":\n    vertices, faces = trimesh.creation.icosahedron().vertices, trimesh.creation.icosahedron().faces\n  elif base_shape == \"octahedron\":\n    vertices, faces = trimesh.creation.octahedron().vertices, trimesh.creation.octahedron().faces\n  else:\n    raise ValueError(\"Invalid base_shape. Must be 'tetrahedron', 'icosahedron', or 'octahedron'.\")\n\n  # Tessellate the polyhedron\n  for i in range(angular_tesselation):\n    mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\n    mesh = mesh.subdivide()\n    vertices, faces = mesh.vertices, mesh.faces\n\n  # Normalize the vertices to unit length\n  vertices = vertices / np.linalg.norm(vertices, axis=1)[:, np.newaxis]\n\n  # Remove symmetric basis columns\n  if remove_symmetries:\n    vertices = np.unique(vertices, axis=0)\n    vertices = vertices[np.lexsort(np.rot90(vertices))]\n    vertices = np.vstack((vertices, -vertices))\n    vertices = np.unique(vertices, axis=0)\n    vertices = vertices"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  import trimesh\n\n  # Generate a base shape using trimesh\n  if base_shape == \"tetrahedron\":\n    mesh = trimesh.creation.icosahedron()\n  elif base_shape == \"icosahedron\":\n    mesh = trimesh.creation.icosahedron()\n  elif base_shape == \"octahedron\":\n    mesh = trimesh.creation.octahedron()\n  else:\n    raise ValueError(\"Invalid base_shape. Must be 'tetrahedron', 'icosahedron', or 'octahedron'.\")\n\n  # Tessellate the base shape\n  for _ in range(angular_tesselation):\n    mesh = mesh.subdivide()\n\n  # Get the vertices of the tessellated shape\n  vertices = mesh.vertices\n\n  # Remove symmetric vertices to avoid redundant negative copies in projections\n  if remove_symmetries:\n    unique_vertices = []\n    for i, vertex in enumerate(vertices):\n      is_symmetric = False\n      for other_vertex in unique_vertices:\n        if np.allclose(vertex, -other_vertex, atol=eps):\n          is_symmetric = True\n          break\n      if not is_symmetric:\n        unique_vertices.append(vertex)\n    vertices = np.array(unique_vertices)\n\n  # Normalize the basis vectors\n  basis = vertices / np.linalg.norm(vertices, axis=1, keepdims=True)\n\n  return basis\n\n\n\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return x"}
{"namespace": "math.safe_log1p", "completion": "  return tf.math.log1p(tf.clip_by_value(x, -1e30, 1e30))\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return jnp.where(x > -1.0, jnp.log1p(x), jnp.log1p(1.0e-8))\n"}
{"namespace": "math.safe_log1p", "completion": "  return jnp.log1p(jnp.clip(x, -1e30, 1e30))\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return np.log1p(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return np.log1p(x)\n"}
{"namespace": "math.safe_log1p", "completion": "  return jnp.where(x > -1, jnp.log1p(x), jnp.log1p(1e-7))\n"}
{"namespace": "math.safe_log1p", "completion": "  return tf.math.log1p(tf.where(tf.abs(x) < 1e-10, 1e-10, x))\n\n"}
{"namespace": "math.safe_log1p", "completion": "  eps = 1e-8\n  return tf.math.log1p(tf.clip_by_value(x, eps, 1 - eps))\n\n"}
{"namespace": "math.safe_log1p", "completion": "  import numpy as np\n  from mygrad.nnet.activations import log1p\n\n  x = np.asarray(x)\n\n  if np.any(x < -1):\n    raise ValueError(\"Input values must be >= -1 for safe_log1p\")\n\n  return log1p(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return x\n\n"}
{"namespace": "math.safe_log1p", "completion": "  # Compute the natural logarithm of 1 plus x\n  result = tf.math.log1p(x)\n\n  # Define the gradient function for automatic differentiation\n  def grad(dy):\n    # Compute the derivative of the log1p operation\n    return dy / (1 + x)\n\n  # Return the result and the gradient function\n  return result, grad\n"}
{"namespace": "math.safe_log1p", "completion": "  # Define a safe range for the input value\n  safe_range = 1e-8\n\n  # Check if the input value is within the safe range\n  if x < -safe_range:\n    # If the input value is outside the safe range, return a large negative value\n    return -1e10\n  elif x > safe_range:\n    # If the input value is outside the safe range, return the natural logarithm of 1 plus x\n    return jnp.log1p(x)\n  else:\n    # If the input value is within the safe range, return the natural logarithm of 1 plus x\n    return jnp.log1p(x)\n"}
{"namespace": "math.safe_log1p", "completion": "  return jnp.log1p(jnp.maximum(x, 1e-8))\n\n"}
{"namespace": "math.safe_log1p", "completion": "  # Define a safe range for the input value\n  safe_range = (-0.5, 0.5)\n\n  # Check if the input value is within the safe range\n  if x < safe_range[0]:\n    # If the input value is less than the lower bound of the safe range, set it to the lower bound\n    x = safe_range[0]\n  elif x > safe_range[1]:\n    # If the input value is greater than the upper bound of the safe range, set it to the upper bound\n    x = safe_range[1]\n\n  # Compute the natural logarithm of 1 plus the input value using the log1p function\n  result = np.log1p(x)\n\n  # Define the derivative of the log1p operation\n  def grad_fn(dy):\n    # Compute the derivative of the log1p operation using the chain rule\n    return dy / (1 + x)\n\n  # Return the result and the derivative function\n  return result, grad_fn\n\n"}
{"namespace": "math.safe_log1p", "completion": "  eps = np.finfo(x.dtype).eps\n  return tf.math.log1p(tf.clip_by_value(x, eps, 1 - eps))\n"}
{"namespace": "math.safe_log1p", "completion": "  # Check if x is close to zero\n  if abs(x) < 1e-7:\n    # If x is close to zero, use a small value to avoid numerical instability\n    x = 1e-7\n\n  # Calculate the natural logarithm of 1 plus x using the safe_log function\n  result = safe_log(1 + x)\n\n  # Define the derivative of the safe_log1p function\n  def derivative(x):\n    return 1 / (1 + x)\n\n  # Set the derivative attribute of the result variable\n  result.derivative = derivative\n\n  return result\n\n"}
{"namespace": "math.safe_log1p", "completion": "  # Import the necessary modules\n  import tensorflow as tf\n  import numpy as np\n\n  # Define the safe range for the input value\n  safe_range = (1e-50, 1e50)\n\n  # Check if the input value is within the safe range\n  if x < safe_range[0] or x > safe_range[1]:\n    # If the input value is outside the safe range, raise an error\n    raise ValueError(\"Input value is outside the safe range.\")\n\n  # Compute the natural logarithm of 1 plus x using the safe_log1p function\n  result = tf.math.log1p(x)\n\n  # Return the result\n  return result\n"}
{"namespace": "math.safe_log1p", "completion": "  # Import the necessary modules\n  import jax.numpy as jnp\n  from jax import custom_jvp\n\n  # Define the safe log1p function\n  def safe_log1p(x):\n    # Define the safe range for the input value\n    safe_range = 1e-30\n    # Clip the input value within the safe range\n    x = jnp.clip(x, a_min=safe_range, a_max=None)\n    # Compute the natural logarithm of 1 plus the clipped input value\n    return jnp.log1p(x)\n\n  # Define the derivative of the safe log1p function\n  @custom_jvp\n  def safe_log1p_jvp(x):\n    # Compute the derivative of the natural logarithm of 1 plus x\n    return safe_log1p(x), jnp.ones_like(x)\n\n  # Define the derivative of the safe log1p function with respect to the input value\n  @safe_log1p_jvp.defjvp\n  def safe_log1p_jvp_jvp(primals, tangents):\n    x, = primals\n    x_dot, = tangents\n    ans = safe_log1p(x)\n    ans_dot = (1 - 1 / (1 + x)) * x_dot\n    return ans, ans_dot\n\n  # Return the safe log1p function\n  return safe_log1p\n"}
{"namespace": "math.safe_log1p", "completion": "  # Import the necessary libraries\n  import tensorflow as tf\n  import numpy as np\n\n  # Define the safe range for the input value x\n  safe_range = [-0.9999999999999999, 0.9999999999999999]\n\n  # Clip the input value x to the safe range\n  x_clipped = tf.clip_by_value(x, safe_range[0], safe_range[1])\n\n  # Compute the natural logarithm of 1 plus the clipped input value\n  result = tf.math.log1p(x_clipped)\n\n  # Define the derivative of the log1p operation\n  def grad(dy):\n    return dy * (1.0 / (1.0 + x_clipped))\n\n  # Define the gradient function for automatic differentiation\n  grad_fn = grad\n\n  # Return the result of the safe log1p operation and the gradient function\n  return result, grad_fn\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    x = np.log(x)\n  elif p == 0:\n    x = np.log(x + 1)\n  elif p == -np.inf:\n    x = -np.log(x)\n  elif p == np.inf:\n    x = np.exp(x)\n  else:\n    x = x ** p\n\n  if postmult is not None:\n    x = x * postmult\n\n  return x"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    x = np.log(x)\n  elif p == 0:\n    x = np.log(x + 1)\n  elif p == -np.inf:\n    x = -np.log(-x)\n  elif p == np.inf:\n    x = -np.log(-x + 1)\n  else:\n    x = (x ** p - 1) / p\n\n  if postmult is not None:\n    x = x * postmult\n\n  return x\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is None:\n    x = x\n  else:\n    x = x * premult\n\n  if p == 1:\n    x = np.log(x)\n  elif p == 0:\n    x = np.log(x + 1)\n  elif p == -np.inf:\n    x = -np.log(x)\n  elif p == np.inf:\n    x = x\n  else:\n    x = (x ** p - 1) / p\n\n  if postmult is None:\n    x = x\n  else:\n    x = x * postmult\n\n  return x\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = np.log(x)\n  elif p == -np.inf:\n    y = np.sign(x) * np.log(np.abs(x))\n  elif p == np.inf:\n    y = np.sign(x)\n  else:\n    y = (np.sign(x) * np.abs(x) ** p)\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is None:\n    premult = 1\n  if postmult is None:\n    postmult = 1\n\n  if p == 1:\n    return x * premult * postmult\n  elif p == 0:\n    return np.log(x * premult + 1) * postmult\n  elif p == -np.inf:\n    return -np.log(-x * premult + 1) * postmult\n  elif p == np.inf:\n    return np.log(x * premult + 1) * postmult\n  else:\n    return ((x * premult + 1) ** p - 1) / p * postmult\n\n"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n\n  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    x = np.log(x)\n  elif p == 0:\n    x = np.log(x + 1)\n  elif p == -np.inf:\n    x = -np.log(-x + 1)\n  elif p == np.inf:\n    x = np.log(-x + 1)\n  else:\n    x = x ** p\n\n  if postmult is not None:\n    x = x * postmult\n\n  return x\n\n\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is None:\n    premult = 1\n  if postmult is None:\n    postmult = 1\n\n  if p == 1:\n    return (x + premult) ** p / (premult * postmult)\n  elif p == 0:\n    return np.log(x + premult) / np.log(premult * postmult)\n  elif p == -np.inf:\n    return -np.log(-x + premult) / np.log(premult * postmult)\n  elif p == np.inf:\n    return -np.log(x + premult) / np.log(premult * postmult)\n  else:\n    return ((x + premult) ** p - 1) / (p * (premult * postmult) ** (p - 1))\n\n"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n\n  if premult is None:\n    premult = 1\n\n  if postmult is None:\n    postmult = 1\n\n  x = np.asarray(x)\n\n  if p == 1:\n    return premult * x + postmult\n  elif p == 0:\n    return premult * np.log(x) + postmult\n  elif p == -np.inf:\n    return premult * np.exp(x) + postmult\n  elif p == np.inf:\n    return premult * np.exp(-x) + postmult\n  else:\n    return premult * (x ** p) + postmult"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n\n  if premult is None:\n    premult = 1\n\n  if postmult is None:\n    postmult = 1\n\n  if p == 1:\n    return x * premult * postmult\n\n  if p == 0:\n    return np.log(x * premult + 1) * postmult\n\n  if p == -np.inf:\n    return np.exp(x * premult) - 1 * postmult\n\n  if p == np.inf:\n    return np.exp(x * premult) * postmult\n\n  return (np.sign(x) * np.abs(x * premult) ** p) * postmult\n\n"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n\n  x = np.asarray(x)\n  x = x.astype(float)\n\n  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    x = np.log(x)\n  elif p == 0:\n    x = np.log(x + 1)\n  elif p == -np.inf:\n    x = -np.log(-x + 1)\n  elif p == np.inf:\n    x = -np.log(-x + 1)\n  else:\n    x = (x ** p - 1) / p\n\n  if postmult is not None:\n    x = x * postmult\n\n  return x\n\n"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n\n  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = np.log(x)\n  elif p == 0:\n    y = np.log(x + 1)\n  elif p == -np.inf:\n    y = -np.log(-x + 1)\n  elif p == np.inf:\n    y = np.log(-x + 1)\n  else:\n    y = (x ** p - 1) / p\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is None:\n    premult = 1\n  if postmult is None:\n    postmult = 1\n\n  if p == 1:\n    return np.log(premult * x + postmult)\n  elif p == 0:\n    return np.log(premult * x + postmult)\n  elif p == -np.inf:\n    return np.exp(premult * x + postmult)\n  elif p == np.inf:\n    return -np.exp(-(premult * x + postmult))\n  else:\n    return (premult * x + postmult) ** p\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is None:\n    premult = 1\n\n  if postmult is None:\n    postmult = 1\n\n  if p == 1:\n    return x\n  elif p == 0:\n    return np.log(premult * x + postmult)\n  elif p == -np.inf:\n    return np.exp(premult * x + postmult) - 1\n  elif p == np.inf:\n    return -np.log(premult * x + postmult)\n  else:\n    return (premult * x + postmult) ** p\n\n"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n\n  # Check if x is numeric or array-like\n  if not np.isscalar(x):\n    x = np.asarray(x)\n\n  # Check if p is numeric\n  if not np.isscalar(p):\n    raise TypeError(\"p must be a numeric value\")\n\n  # Check if premult is numeric or None\n  if premult is not None and not np.isscalar(premult):\n    raise TypeError(\"premult must be a numeric value or None\")\n\n  # Check if postmult is numeric or None\n  if postmult is not None and not np.isscalar(postmult):\n    raise TypeError(\"postmult must be a numeric value or None\")\n\n  # Apply pre-multiplication if specified\n  if premult is not None:\n    x = x * premult\n\n  # Apply power ladder transformation based on p value\n  if p == 1:\n    x = np.log(x)\n  elif p == 0:\n    x = np.log10(x)\n  elif p == -np.inf:\n    x = np.exp(x)\n  elif p == np.inf:\n    x = np.exp10(x)\n  else:\n    x = np.sign(x) * np.abs(x) ** p\n\n  # Apply post-multiplication if specified\n  if postmult is not None:\n    x = x * postmult\n\n  return x"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n\n  if premult is None:\n    premult = 1\n  if postmult is None:\n    postmult = 1\n\n  if p == 1:\n    return np.log(x * premult + 1) * postmult\n  elif p == 0:\n    return np.sqrt(x * premult + 1) * postmult\n  elif p == -np.inf:\n    return np.exp(x * premult - 1) * postmult\n  elif p == np.inf:\n    return np.exp(x * premult) * postmult\n  else:\n    return (np.sign(x) * np.abs(x * premult + 1) ** p - 1) / p * postmult\n\n"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n\n  # Check if x is numeric or array-like\n  if not np.isscalar(x):\n    x = np.array(x)\n\n  # Check if p is numeric\n  if not np.isscalar(p):\n    raise ValueError(\"p must be a numeric value.\")\n\n  # Check if premult is numeric or None\n  if premult is not None and not np.isscalar(premult):\n    raise ValueError(\"premult must be a numeric value or None.\")\n\n  # Check if postmult is numeric or None\n  if postmult is not None and not np.isscalar(postmult):\n    raise ValueError(\"postmult must be a numeric value or None.\")\n\n  # Apply pre-multiplication if specified\n  if premult is not None:\n    x = x * premult\n\n  # Handle special cases for p\n  if p == 1:\n    # No transformation is applied\n    pass\n  elif p == 0:\n    # Log transformation\n    x = np.log(x)\n  elif p == -np.inf:\n    # Inverse transformation\n    x = 1 / x\n  elif p == np.inf:\n    # Square root transformation\n    x = np.sqrt(x)\n  else:\n    # General case: Apply power transformation\n    x = x ** p\n\n  # Apply post-multiplication if specified\n  if postmult is not None:\n    x = x * postmult\n\n  return x"}
{"namespace": "math.power_ladder", "completion": "  if premult is None:\n    premult = 1\n\n  if postmult is None:\n    postmult = 1\n\n  if p == 1:\n    return x\n  elif p == 0:\n    return np.log(x * premult + 1) * postmult\n  elif p == -np.inf:\n    return -np.log(-x * premult + 1) * postmult\n  elif p == np.inf:\n    return np.exp(x * premult) * postmult - 1\n  else:\n    return (np.sign(x) * np.abs(x * premult) ** p) * postmult\n\n"}
{"namespace": "math.power_ladder", "completion": "  # Import necessary libraries\n  import numpy as np\n\n  # Check if x is numeric or array-like\n  if not np.isscalar(x):\n    x = np.array(x)\n\n  # Check if premult is numeric or None\n  if not np.isscalar(premult) and premult is not None:\n    premult = np.array(premult)\n\n  # Check if postmult is numeric or None\n  if not np.isscalar(postmult) and postmult is not None:\n    postmult = np.array(postmult)\n\n  # Check if x, premult, and postmult are all numeric or array-like\n  if not np.isscalar(x) and not np.isscalar(premult) and not np.isscalar(postmult):\n    # If x, premult, and postmult are all array-like, check if they have the same shape\n    if x.shape != premult.shape or x.shape != postmult.shape:\n      raise ValueError(\"x, premult, and postmult must have the same shape\")\n\n  # Check if p is numeric\n  if not np.isscalar(p):\n    raise ValueError(\"p must be a numeric value\")\n\n  # Check if p is not equal to 1, 0, -inf, or inf\n  if p != 1 and p != 0 and p != -np.inf and p != np.inf:\n    # If p is not equal to 1, 0, -inf, or inf, raise an error\n    raise ValueError(\"p must be equal to 1, 0, -inf, or inf\")\n\n  # Check if premult is not None\n  if premult is not None:\n    # If premult is not None, check if it is numeric\n    if not np.isscalar(premult):\n      raise ValueError(\"premult must be a numeric value or None\")\n    # If premult is numeric, multiply x by premult\n    x = x * premult\n\n  # Check if postmult is not None\n "}
{"namespace": "math.power_ladder", "completion": "  # Check if x is numeric or array-like\n  if not isinstance(x, (int, float, list, tuple, np.ndarray)):\n    raise TypeError(\"Input x must be numeric or array-like.\")\n\n  # Check if p is numeric\n  if not isinstance(p, (int, float)):\n    raise TypeError(\"Input p must be numeric.\")\n\n  # Check if premult and postmult are numeric or None\n  if premult is not None and not isinstance(premult, (int, float)):\n    raise TypeError(\"Input premult must be numeric or None.\")\n  if postmult is not None and not isinstance(postmult, (int, float)):\n    raise TypeError(\"Input postmult must be numeric or None.\")\n\n  # Handle special cases for p\n  if p == 1:\n    return x\n  elif p == 0:\n    return np.log(x)\n  elif p == -np.inf:\n    return 1 / x\n  elif p == np.inf:\n    return np.exp(x)\n\n  # Apply pre-multiplication if specified\n  if premult is not None:\n    x = x * premult\n\n  # Apply the power ladder transformation\n  if p > 0:\n    x = x ** (1 / p)\n  else:\n    x = np.sign(x) * np.abs(x) ** p\n\n  # Apply post-multiplication if specified\n  if postmult is not None:\n    x = x * postmult\n\n  return x\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is None:\n    premult = 1\n  if postmult is None:\n    postmult = 1\n\n  if p == 0:\n    return np.log(premult * x + postmult)\n  elif p == 1:\n    return x\n  elif p == -np.inf:\n    return -np.log(premult * x + postmult)\n  elif p == np.inf:\n    return np.exp(premult * x + postmult)\n  else:\n    return (premult * x + postmult) ** p\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 0:\n    return y\n  elif p == 1:\n    return y\n  elif p == 2:\n    return y * y\n  elif p == 3:\n    return y * y * y\n  elif p == 4:\n    return y * y * y * y\n  elif p == 5:\n    return y * y * y * y * y\n  elif p == 6:\n    return y * y * y * y * y * y\n  elif p == 7:\n    return y * y * y * y * y * y * y\n  elif p == 8:\n    return y * y * y * y * y * y * y * y\n  elif p == 9:\n    return y * y * y * y * y * y * y * y * y\n  elif p == 10:\n    return y * y * y * y * y * y * y * y * y * y\n  elif p == 11:\n    return y * y * y * y * y * y * y * y * y * y * y\n  elif p == 12:\n    return y * y * y * y * y * y * y * y * y * y * y * y\n  elif p == 13:\n    return y * y * y * y * y * y * y * y * y * y * y * y * y\n  elif p == 14:\n    return y * y * y * y * y * y * y * y * y * y * y * y * y * y\n  elif p == 15:\n    return y * y * y * y * y * y * y * y * y * y * y * y * y * y * y\n  elif p == 16:\n    return y * y * y * y * y * y * y * y * y * y * y * y * y * y * y * y\n  elif p == 17:\n    return y * y * y * y * y * y * y * y * y * y * y * y * y * y * y * y * y\n  elif p == 18:\n    return y * y * y * y * y *"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 0:\n    return y\n  elif p == 1:\n    return y\n  elif p == 2:\n    return y**2\n  elif p == 3:\n    return y**3\n  elif p == 4:\n    return y**4\n  elif p == 5:\n    return y**5\n  elif p == 6:\n    return y**6\n  elif p == 7:\n    return y**7\n  elif p == 8:\n    return y**8\n  elif p == 9:\n    return y**9\n  elif p == 10:\n    return y**10\n  elif p == 11:\n    return y**11\n  elif p == 12:\n    return y**12\n  elif p == 13:\n    return y**13\n  elif p == 14:\n    return y**14\n  elif p == 15:\n    return y**15\n  elif p == 16:\n    return y**16\n  elif p == 17:\n    return y**17\n  elif p == 18:\n    return y**18\n  elif p == 19:\n    return y**19\n  elif p == 20:\n    return y**20\n  elif p == 21:\n    return y**21\n  elif p == 22:\n    return y**22\n  elif p == 23:\n    return y**23\n  elif p == 24:\n    return y**24\n  elif p == 25:\n    return y**25\n  elif p == 26:\n    return y**26\n  elif p == 27:\n    return y**27\n  elif p == 28:\n    return y**28\n  elif p == 29:\n    return y**29\n  elif p == 30:\n    return y**30\n  elif p == 31:\n    return y**31\n  elif p == 32:\n    return y**32\n  elif p == 33:\n    return y**33\n  elif p == 34:\n    return y**"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 0:\n    return y\n  elif p == 1:\n    return y\n  elif p == 2:\n    return y ** 2\n  elif p == 3:\n    return y ** 3\n  elif p == 4:\n    return y ** 4\n  elif p == 5:\n    return y ** 5\n  elif p == 6:\n    return y ** 6\n  elif p == 7:\n    return y ** 7\n  elif p == 8:\n    return y ** 8\n  elif p == 9:\n    return y ** 9\n  elif p == 10:\n    return y ** 10\n  elif p == 11:\n    return y ** 11\n  elif p == 12:\n    return y ** 12\n  elif p == 13:\n    return y ** 13\n  elif p == 14:\n    return y ** 14\n  elif p == 15:\n    return y ** 15\n  elif p == 16:\n    return y ** 16\n  elif p == 17:\n    return y ** 17\n  elif p == 18:\n    return y ** 18\n  elif p == 19:\n    return y ** 19\n  elif p == 20:\n    return y ** 20\n  elif p == 21:\n    return y ** 21\n  elif p == 22:\n    return y ** 22\n  elif p == 23:\n    return y ** 23\n  elif p == 24:\n    return y ** 24\n  elif p == 25:\n    return y ** 25\n  elif p == 26:\n    return y ** 26\n  elif p == 27:\n    return y ** 27\n  elif p == 28:\n    return y ** 28\n  elif p == 29:\n    return y ** 29\n  elif p == 30:\n    return y ** 30\n  elif p == 31:\n    return y ** 31\n  elif p == 32:\n    return y **"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is None:\n    premult = 1.0\n  if postmult is None:\n    postmult = 1.0\n\n  if p == 1:\n    return y * premult\n  elif p == 2:\n    return (y * premult) ** 2\n  elif p == 3:\n    return (y * premult) ** 3\n  elif p == 4:\n    return (y * premult) ** 4\n  elif p == 5:\n    return (y * premult) ** 5\n  elif p == 6:\n    return (y * premult) ** 6\n  elif p == 7:\n    return (y * premult) ** 7\n  elif p == 8:\n    return (y * premult) ** 8\n  elif p == 9:\n    return (y * premult) ** 9\n  elif p == 10:\n    return (y * premult) ** 10\n  elif p == 11:\n    return (y * premult) ** 11\n  elif p == 12:\n    return (y * premult) ** 12\n  elif p == 13:\n    return (y * premult) ** 13\n  elif p == 14:\n    return (y * premult) ** 14\n  elif p == 15:\n    return (y * premult) ** 15\n  elif p == 16:\n    return (y * premult) ** 16\n  elif p == 17:\n    return (y * premult) ** 17\n  elif p == 18:\n    return (y * premult) ** 18\n  elif p == 19:\n    return (y * premult) ** 19\n  elif p == 20:\n    return (y * premult) ** 20\n  elif p == 21:\n    return (y * premult) ** 21\n  elif p == 22:\n    return (y * premult) ** 22\n  elif p == 23:\n    return (y * premult) ** 23\n  elif p == 2"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 0:\n    return y\n  elif p == 1:\n    return y\n  elif p == 2:\n    return y ** 2\n  elif p == 3:\n    return y ** 3\n  elif p == 4:\n    return y ** 4\n  elif p == 5:\n    return y ** 5\n  elif p == 6:\n    return y ** 6\n  elif p == 7:\n    return y ** 7\n  elif p == 8:\n    return y ** 8\n  elif p == 9:\n    return y ** 9\n  elif p == 10:\n    return y ** 10\n  elif p == 11:\n    return y ** 11\n  elif p == 12:\n    return y ** 12\n  elif p == 13:\n    return y ** 13\n  elif p == 14:\n    return y ** 14\n  elif p == 15:\n    return y ** 15\n  elif p == 16:\n    return y ** 16\n  elif p == 17:\n    return y ** 17\n  elif p == 18:\n    return y ** 18\n  elif p == 19:\n    return y ** 19\n  elif p == 20:\n    return y ** 20\n  elif p == 21:\n    return y ** 21\n  elif p == 22:\n    return y ** 22\n  elif p == 23:\n    return y ** 23\n  elif p == 24:\n    return y ** 24\n  elif p == 25:\n    return y ** 25\n  elif p == 26:\n    return y ** 26\n  elif p == 27:\n    return y ** 27\n  elif p == 28:\n    return y ** 28\n  elif p == 29:\n    return y ** 29\n  elif p == 30:\n    return y ** 30\n  elif p == 31:\n    return y ** 31\n  elif p == 32:\n    return y **"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 1:\n    return y\n  elif p == 2:\n    return y.sqrt()\n  elif p == 3:\n    return y.rsqrt()\n  elif p == 4:\n    return y.sqrt().sqrt()\n  elif p == 5:\n    return y.sqrt().rsqrt()\n  elif p == 6:\n    return y.rsqrt().sqrt()\n  elif p == 7:\n    return y.rsqrt().rsqrt()\n  elif p == 8:\n    return y.sqrt().sqrt().sqrt()\n  elif p == 9:\n    return y.sqrt().sqrt().rsqrt()\n  elif p == 10:\n    return y.sqrt().rsqrt().sqrt()\n  elif p == 11:\n    return y.sqrt().rsqrt().rsqrt()\n  elif p == 12:\n    return y.rsqrt().sqrt().sqrt()\n  elif p == 13:\n    return y.rsqrt().sqrt().rsqrt()\n  elif p == 14:\n    return y.rsqrt().rsqrt().sqrt()\n  elif p == 15:\n    return y.rsqrt().rsqrt().rsqrt()\n  elif p == 16:\n    return y.sqrt().sqrt().sqrt().sqrt()\n  elif p == 17:\n    return y.sqrt().sqrt().sqrt().rsqrt()\n  elif p == 18:\n    return y.sqrt().sqrt().rsqrt().sqrt()\n  elif p == 19:\n    return y.sqrt().sqrt().rsqrt().rsqrt()\n  elif p == 20:\n    return y.sqrt().rsqrt().sqrt().sqrt()\n  elif p == 21:\n    return y.sqrt().rsqrt().sqrt().rsqrt()\n  elif p == 22:\n    return y.sqrt().rsqrt().rsqrt().sqrt()\n  elif p == 23:\n    return y.sqrt().rsqrt().rsqrt().rsqrt()\n  elif p == 24:\n    return y.rsqrt().sqrt().sqrt().sqrt()\n  elif p == 25:\n    return y.rsqrt().sqrt().sqrt().rsqrt()\n  elif p"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 1:\n    return y\n  elif p == 2:\n    return (y ** 2) * premult\n  elif p == 3:\n    return (y ** 3) * premult\n  elif p == 4:\n    return (y ** 4) * premult\n  elif p == 5:\n    return (y ** 5) * premult\n  elif p == 6:\n    return (y ** 6) * premult\n  elif p == 7:\n    return (y ** 7) * premult\n  elif p == 8:\n    return (y ** 8) * premult\n  elif p == 9:\n    return (y ** 9) * premult\n  elif p == 10:\n    return (y ** 10) * premult\n  elif p == 11:\n    return (y ** 11) * premult\n  elif p == 12:\n    return (y ** 12) * premult\n  elif p == 13:\n    return (y ** 13) * premult\n  elif p == 14:\n    return (y ** 14) * premult\n  elif p == 15:\n    return (y ** 15) * premult\n  elif p == 16:\n    return (y ** 16) * premult\n  elif p == 17:\n    return (y ** 17) * premult\n  elif p == 18:\n    return (y ** 18) * premult\n  elif p == 19:\n    return (y ** 19) * premult\n  elif p == 20:\n    return (y ** 20) * premult\n  elif p == 21:\n    return (y ** 21) * premult\n  elif p == 22:\n    return (y ** 22) * premult\n  elif p == 23:\n    return (y ** 23) * premult\n  elif p == 24:\n    return (y ** 24) * premult\n  elif p == 25:\n    return (y ** 25) * premult"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 0:\n    return y\n  elif p == 1:\n    return y * premult\n  elif p == 2:\n    return y * premult * postmult\n  elif p == 3:\n    return y * premult * postmult ** 2\n  elif p == 4:\n    return y * premult * postmult ** 3\n  elif p == 5:\n    return y * premult * postmult ** 4\n  elif p == 6:\n    return y * premult * postmult ** 5\n  elif p == 7:\n    return y * premult * postmult ** 6\n  elif p == 8:\n    return y * premult * postmult ** 7\n  elif p == 9:\n    return y * premult * postmult ** 8\n  elif p == 10:\n    return y * premult * postmult ** 9\n  elif p == 11:\n    return y * premult * postmult ** 10\n  elif p == 12:\n    return y * premult * postmult ** 11\n  elif p == 13:\n    return y * premult * postmult ** 12\n  elif p == 14:\n    return y * premult * postmult ** 13\n  elif p == 15:\n    return y * premult * postmult ** 14\n  elif p == 16:\n    return y * premult * postmult ** 15\n  elif p == 17:\n    return y * premult * postmult ** 16\n  elif p == 18:\n    return y * premult * postmult ** 17\n  elif p == 19:\n    return y * premult * postmult ** 18\n  elif p == 20:\n    return y * premult * postmult ** 19\n  elif p == 21:\n    return y * premult * postmult ** 20\n  elif p == 22:\n    return y * premult * postmult ** 21\n  elif p == 23:\n    return y * premult * postmult ** 22\n  elif p == 24:\n   "}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is None:\n    premult = 1.0\n  if postmult is None:\n    postmult = 1.0\n  if p == 1:\n    return y * premult\n  elif p == 2:\n    return (y * premult) ** (1.0 / 2.0)\n  elif p == 3:\n    return (y * premult) ** (1.0 / 3.0)\n  elif p == 4:\n    return (y * premult) ** (1.0 / 4.0)\n  elif p == 5:\n    return (y * premult) ** (1.0 / 5.0)\n  elif p == 6:\n    return (y * premult) ** (1.0 / 6.0)\n  elif p == 7:\n    return (y * premult) ** (1.0 / 7.0)\n  elif p == 8:\n    return (y * premult) ** (1.0 / 8.0)\n  elif p == 9:\n    return (y * premult) ** (1.0 / 9.0)\n  elif p == 10:\n    return (y * premult) ** (1.0 / 10.0)\n  elif p == 11:\n    return (y * premult) ** (1.0 / 11.0)\n  elif p == 12:\n    return (y * premult) ** (1.0 / 12.0)\n  elif p == 13:\n    return (y * premult) ** (1.0 / 13.0)\n  elif p == 14:\n    return (y * premult) ** (1.0 / 14.0)\n  elif p == 15:\n    return (y * premult) ** (1.0 / 15.0)\n  elif p == 16:\n    return (y * premult) ** (1.0 / 16.0)\n  elif p == 17:\n    return (y * premult) ** (1.0 / 17.0)\n  elif p == "}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is None:\n    premult = 1\n  if postmult is None:\n    postmult = 1\n\n  if p == 1:\n    return y * premult\n  elif p == 2:\n    return (y ** (1 / 2) - 1) * premult\n  elif p == 3:\n    return (y ** (1 / 3) - 1) * premult\n  elif p == 4:\n    return (y ** (1 / 4) - 1) * premult\n  elif p == 5:\n    return (y ** (1 / 5) - 1) * premult\n  elif p == 6:\n    return (y ** (1 / 6) - 1) * premult\n  elif p == 7:\n    return (y ** (1 / 7) - 1) * premult\n  elif p == 8:\n    return (y ** (1 / 8) - 1) * premult\n  elif p == 9:\n    return (y ** (1 / 9) - 1) * premult\n  elif p == 10:\n    return (y ** (1 / 10) - 1) * premult\n  elif p == 11:\n    return (y ** (1 / 11) - 1) * premult\n  elif p == 12:\n    return (y ** (1 / 12) - 1) * premult\n  elif p == 13:\n    return (y ** (1 / 13) - 1) * premult\n  elif p == 14:\n    return (y ** (1 / 14) - 1) * premult\n  elif p == 15:\n    return (y ** (1 / 15) - 1) * premult\n  elif p == 16:\n    return (y ** (1 / 16) - 1) * premult\n  elif p == 17:\n    return (y ** (1 / 17) - 1) * premult\n  elif p == 18:\n    return (y ** (1 / 18) - 1"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is None:\n    premult = 1.0\n  if postmult is None:\n    postmult = 1.0\n\n  if p == 1:\n    return y * premult\n  elif p == -1:\n    return y * premult\n  elif p == 2:\n    return (y * premult) ** (1.0 / 2.0)\n  elif p == -2:\n    return (y * premult) ** (-1.0 / 2.0)\n  elif p == 3:\n    return (y * premult) ** (1.0 / 3.0)\n  elif p == -3:\n    return (y * premult) ** (-1.0 / 3.0)\n  elif p == 4:\n    return (y * premult) ** (1.0 / 4.0)\n  elif p == -4:\n    return (y * premult) ** (-1.0 / 4.0)\n  elif p == 5:\n    return (y * premult) ** (1.0 / 5.0)\n  elif p == -5:\n    return (y * premult) ** (-1.0 / 5.0)\n  elif p == 6:\n    return (y * premult) ** (1.0 / 6.0)\n  elif p == -6:\n    return (y * premult) ** (-1.0 / 6.0)\n  elif p == 7:\n    return (y * premult) ** (1.0 / 7.0)\n  elif p == -7:\n    return (y * premult) ** (-1.0 / 7.0)\n  elif p == 8:\n    return (y * premult) ** (1.0 / 8.0)\n  elif p == -8:\n    return (y * premult) ** (-1.0 / 8.0)\n  elif p == 9:\n    return (y * premult) ** (1.0 / 9.0)\n  elif p == -9:\n    return (y * premult) ** (-1.0 / 9"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is None:\n    premult = 1.0\n  if postmult is None:\n    postmult = 1.0\n  y = y * premult\n  if p == 0:\n    return y\n  elif p == 1:\n    return y\n  elif p == 2:\n    return y * y\n  elif p == -1:\n    return 1.0 / y\n  elif p == -2:\n    return 1.0 / (y * y)\n  else:\n    return (1.0 / p) * (y ** (1.0 - p) - 1.0) / (y ** (1.0 - p) - y ** -p) * postmult\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is None:\n    premult = 1.0\n\n  if postmult is None:\n    postmult = 1.0\n\n  if p == 1:\n    return y * premult / postmult\n  elif p == 2:\n    return (y / postmult) ** 2 * premult\n  elif p == 3:\n    return (y / postmult) ** 3 * premult\n  elif p == 4:\n    return (y / postmult) ** 4 * premult\n  elif p == 5:\n    return (y / postmult) ** 5 * premult\n  elif p == 6:\n    return (y / postmult) ** 6 * premult\n  elif p == 7:\n    return (y / postmult) ** 7 * premult\n  elif p == 8:\n    return (y / postmult) ** 8 * premult\n  elif p == 9:\n    return (y / postmult) ** 9 * premult\n  elif p == 10:\n    return (y / postmult) ** 10 * premult\n  elif p == 11:\n    return (y / postmult) ** 11 * premult\n  elif p == 12:\n    return (y / postmult) ** 12 * premult\n  elif p == 13:\n    return (y / postmult) ** 13 * premult\n  elif p == 14:\n    return (y / postmult) ** 14 * premult\n  elif p == 15:\n    return (y / postmult) ** 15 * premult\n  elif p == 16:\n    return (y / postmult) ** 16 * premult\n  elif p == 17:\n    return (y / postmult) ** 17 * premult\n  elif p == 18:\n    return (y / postmult) ** 18 * premult\n  elif p == 19:\n    return (y / postmult) ** 19 * premult\n  elif p == 20:\n    return (y / postmult) ** 20 * premult\n  elif p == 21:"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 0:\n    return y\n  elif p == 1:\n    return y * premult\n  elif p == -1:\n    return y * premult / (y * premult + postmult)\n  elif p == 2:\n    return y * premult / (y * premult + postmult) ** 2\n  elif p == -2:\n    return y * premult / (y * premult + postmult) ** 3\n  elif p == 3:\n    return y * premult / (y * premult + postmult) ** 4\n  elif p == -3:\n    return y * premult / (y * premult + postmult) ** 5\n  elif p == 4:\n    return y * premult / (y * premult + postmult) ** 6\n  elif p == -4:\n    return y * premult / (y * premult + postmult) ** 7\n  elif p == 5:\n    return y * premult / (y * premult + postmult) ** 8\n  elif p == -5:\n    return y * premult / (y * premult + postmult) ** 9\n  elif p == 6:\n    return y * premult / (y * premult + postmult) ** 10\n  elif p == -6:\n    return y * premult / (y * premult + postmult) ** 11\n  elif p == 7:\n    return y * premult / (y * premult + postmult) ** 12\n  elif p == -7:\n    return y * premult / (y * premult + postmult) ** 13\n  elif p == 8:\n    return y * premult / (y * premult + postmult) ** 14\n  elif p == -8:\n    return y * premult / (y * premult + postmult) ** 15\n  elif p == 9:\n    return y * premult / (y * premult + postmult) ** 16\n  elif p == -9:\n    return y * premult / (y * premult + postmult) ** 17"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is None:\n    premult = 1.0\n  if postmult is None:\n    postmult = 1.0\n  y = y * postmult\n  if p == 0:\n    return y\n  elif p == 1:\n    return y / premult\n  elif p == 2:\n    return y * y / premult\n  elif p == 3:\n    return y * y * y / premult\n  elif p == 4:\n    return y * y * y * y / premult\n  elif p == 5:\n    return y * y * y * y * y / premult\n  elif p == 6:\n    return y * y * y * y * y * y / premult\n  elif p == 7:\n    return y * y * y * y * y * y * y / premult\n  elif p == 8:\n    return y * y * y * y * y * y * y * y / premult\n  elif p == 9:\n    return y * y * y * y * y * y * y * y * y / premult\n  elif p == 10:\n    return y * y * y * y * y * y * y * y * y * y / premult\n  elif p == 11:\n    return y * y * y * y * y * y * y * y * y * y * y / premult\n  elif p == 12:\n    return y * y * y * y * y * y * y * y * y * y * y * y / premult\n  elif p == 13:\n    return y * y * y * y * y * y * y * y * y * y * y * y * y / premult\n  elif p == 14:\n    return y * y * y * y * y * y * y * y * y * y * y * y * y * y / premult\n  elif p == 15:\n    return y * y * y * y * y * y * y * y * y * y * y * y * y * y * y / premult\n  elif p == 16:\n    return y * y * y * y * y * y"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is None:\n    premult = 1.0\n  if postmult is None:\n    postmult = 1.0\n\n  if p == 1:\n    return y * premult\n  elif p == 2:\n    return y * y * premult\n  elif p == -1:\n    return 1.0 / y * premult\n  elif p == -2:\n    return 1.0 / (y * y) * premult\n  elif p == 0.5:\n    return y ** 0.5 * premult\n  elif p == -0.5:\n    return 1.0 / (y ** 0.5) * premult\n  elif p == 0:\n    return 1.0 / y * premult\n  else:\n    return (y ** (1.0 / p) - 1.0) / p * premult\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is None:\n    premult = 1.0\n  if postmult is None:\n    postmult = 1.0\n  if p == 1:\n    return y * premult\n  elif p == 2:\n    return (y * premult) ** 2\n  elif p == -1:\n    return 1 / (y * premult)\n  elif p == -2:\n    return 1 / (y * premult) ** 2\n  elif p > 0:\n    return (y * premult) ** (1 / p)\n  elif p < 0:\n    return (y * premult) ** (p)\n  else:\n    raise ValueError(\"p must be a real number\")"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 1:\n    return y\n  elif p == 2:\n    return y.sqrt()\n  elif p == 3:\n    return y.rsqrt()\n  elif p == 4:\n    return y.sqrt().sqrt()\n  elif p == 0.5:\n    return y.square()\n  elif p == -1:\n    return y.reciprocal()\n  elif p == -2:\n    return y.reciprocal().sqrt()\n  elif p == -3:\n    return y.reciprocal().rsqrt()\n  elif p == -4:\n    return y.reciprocal().sqrt().sqrt()\n  elif p == -0.5:\n    return y.rsqrt()\n  else:\n    if premult is not None:\n      y = y * premult\n    if postmult is not None:\n      y = y * postmult\n    return y.pow(1.0 / p)\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 1:\n    return y\n  elif p == 2:\n    return y * (y > 0)\n  elif p == -1:\n    return y * (y > 0) + (y <= 0)\n  elif p == -2:\n    return y * (y > 0) + (y <= 0) * y\n  elif p == 0.5:\n    return y * (y > 0) + (y <= 0) * y**2\n  elif p == -0.5:\n    return y * (y > 0) + (y <= 0) * y**0.5\n  elif p == 0:\n    return y * (y > 0) + (y <= 0) * 1 / (y**2 + 1)\n  else:\n    if premult is not None:\n      y = y * premult\n    if postmult is not None:\n      y = y**postmult\n    if p > 0:\n      return y**(1 / p)\n    else:\n      return -((-y) ** (1 / p))\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # Check if y is a scalar or a tensor\n  if y.ndim == 0:\n    # If y is a scalar, perform the inverse transformation based on the value of p\n    if p == 1:\n      return y\n    elif p == 2:\n      return np.sqrt(y)\n    elif p == 3:\n      return np.cbrt(y)\n    elif p == 0:\n      return np.exp(y)\n    elif p == -1:\n      return 1 / y\n    elif p == -2:\n      return 1 / np.sqrt(y)\n    elif p == -3:\n      return 1 / np.cbrt(y)\n    else:\n      return np.power(y, 1 / p)\n  else:\n    # If y is a tensor, perform the inverse transformation element-wise\n    if p == 1:\n      return y\n    elif p == 2:\n      return np.sqrt(y)\n    elif p == 3:\n      return np.cbrt(y)\n    elif p == 0:\n      return np.exp(y)\n    elif p == -1:\n      return 1 / y\n    elif p == -2:\n      return 1 / np.sqrt(y)\n    elif p == -3:\n      return 1 / np.cbrt(y)\n    else:\n      return np.power(y, 1 / p)\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * np.sin(\n        0.5 * np.pi * np.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  t = np.clip(step / max_steps, 0, 1)\n  log_lerp = np.exp(np.log(lr_init) * (1 - t) + np.log(lr_final) * t)\n  return delay_rate * log_lerp"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * np.sin(\n        0.5 * np.pi * np.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  t = np.clip(step / max_steps, 0, 1)\n  log_lerp = np.exp(np.log(lr_init) * (1 - t) + np.log(lr_final) * t)\n  return delay_rate * log_lerp"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * np.sin(\n        0.5 * np.pi * np.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  t = np.clip(step / max_steps, 0, 1)\n  log_lerp = np.exp(np.log(lr_init) * (1 - t) + np.log(lr_final) * t)\n  return delay_rate * log_lerp"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * np.sin(\n        0.5 * np.pi * np.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  t = np.clip(step / max_steps, 0, 1)\n  log_lerp = np.exp(np.log(lr_init) * (1 - t) + np.log(lr_final) * t)\n  return delay_rate * log_lerp"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  t = jnp.clip(step / max_steps, 0, 1)\n  log_lerp = jnp.exp(jnp.log(lr_init) * (1 - t) + jnp.log(lr_final) * t)\n  return delay_rate * log_lerp"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  t = jnp.clip(step / max_steps, 0, 1)\n  log_lerp = jnp.exp(jnp.log(lr_init) * (1 - t) + jnp.log(lr_final) * t)\n  return delay_rate * log_lerp\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    # https://www.desmos.com/calculator/sym23t5y1b\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * np.sin(\n        0.5 * np.pi * np.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n\n  t = np.clip(step / max_steps, 0, 1)\n  log_lerp = np.exp(np.log(lr_init) * (1 - t) + np.log(lr_final) * t)\n  return delay_rate * log_lerp"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    # https://arxiv.org/abs/1812.01187\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n    delay_rate_inv = 1 / delay_rate\n  else:\n    delay_rate = lr_delay_mult\n    delay_rate_inv = 1.0\n\n  t = jnp.clip(step / max_steps, 0, 1)\n  log_lerp = jnp.exp(jnp.log(lr_init) * (1 - t) + jnp.log(lr_final) * t)\n  return delay_rate_inv * log_lerp\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * np.sin(\n        0.5 * np.pi * np.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  t = np.clip(step / max_steps, 0, 1)\n  log_lerp = np.exp(np.log(lr_init) * (1 - t) + np.log(lr_final) * t)\n  return delay_rate * log_lerp\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    # https://www.desmos.com/calculator/rcmcf5jwe7\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n    delay_step = jnp.fmin(step, lr_delay_steps)\n    progress = (step - delay_step) / (max_steps - lr_delay_steps)\n    mult = jnp.pi / 2 * jnp.clip(1.0 - progress, 0, 1)\n    lr = lr_init * jnp.cos(mult) * delay_rate\n    lr = jnp.clip(lr, 0.01 * lr_init, lr_init)\n  else:\n    progress = step / max_steps\n    mult = jnp.pi / 2 * jnp.clip(1.0 - progress, 0, 1)\n    lr = lr_init * jnp.cos(mult)\n\n  if lr_final > 0:\n    # use a log curve at the end to finish the training\n    # https://www.desmos.com/calculator/4cbposyfty\n    lr = jnp.clip(lr, 1e-7, lr_init)\n    mult = jnp.exp(jnp.log(lr_final / lr_init) / (max_steps - step))\n    lr = mult * lr\n\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * np.sin(\n        0.5 * np.pi * np.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  t = np.clip(step / max_steps, 0, 1)\n  log_lerp = np.exp(np.log(lr_init) * (1 - t) + np.log(lr_final) * t)\n  return delay_rate * log_lerp\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    # https://www.desmos.com/calculator/rcmcf5jwe7\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * np.sin(\n        0.5 * np.pi * np.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  t = np.clip(step / max_steps, 0, 1)\n  log_lerp = np.exp(np.log(lr_init) * (1 - t) + np.log(lr_final) * t)\n  return delay_rate * log_lerp\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * np.sin(\n        0.5 * np.pi * np.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  t = np.clip(step / max_steps, 0, 1)\n  log_lerp = np.exp(np.log(lr_init) * (1 - t) + np.log(lr_final) * t)\n  return delay_rate * log_lerp\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    # https://www.desmos.com/calculator/rcmcf5jwe7\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n    delay_step = jnp.fmin(step, lr_delay_steps)\n    progress = (step - delay_step) / (max_steps - lr_delay_steps)\n    mult = jnp.pi / 2 * jnp.clip(1.0 - progress, 0, 1)\n    lr = lr_init * jnp.cos(mult) * delay_rate\n  else:\n    progress = step / max_steps\n    mult = jnp.pi / 2 * jnp.clip(1.0 - progress, 0, 1)\n    lr = lr_init * jnp.cos(mult)\n\n  return lr * lr_final\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n    delay_rate_inv = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1) + np.pi\n    )\n    assert 0.0 <= delay_rate <= 1.0\n    t = jnp.clip(step / max_steps, 0, 1)\n    log_lerp = jnp.exp(jnp.log(delay_rate) + t * (jnp.log(delay_rate_inv) - jnp.log(delay_rate)))\n    res = lr_init * log_lerp\n  else:\n    res = lr_init\n  return res\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    # https://www.desmos.com/calculator/tj6sfctefx\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n    delay_rate_inv = 1 / delay_rate\n    step_init = step - lr_delay_steps\n  else:\n    # No delay in the learning rate.\n    delay_rate = 1.0\n    delay_rate_inv = 1.0\n    step_init = step\n\n  lr = (\n      lr_init\n      * delay_rate\n      * 0.5\n      * (1.0 + jnp.cos(jnp.pi * jnp.clip(step_init / max_steps, 0, 1)))\n  )\n  return jnp.clip(lr, lr_init * 1e-3, lr_final) * delay_rate_inv\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * np.sin(\n        0.5 * np.pi * np.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  t = np.clip(step / max_steps, 0, 1)\n  log_lerp = np.exp(np.log(lr_init) * (1 - t) + np.log(lr_final) * t)\n  return delay_rate * log_lerp\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * np.sin(\n        0.5 * np.pi * np.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  t = np.clip(step / max_steps, 0, 1)\n  log_lerp = np.exp(np.log(lr_init) * (1 - t) + np.log(lr_final) * t)\n  return delay_rate * log_lerp\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    # https://www.desmos.com/calculator/symcdhlhwe\n    # lr_delay_mult changes the initial value of the decay.\n    lr_delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n    lr = lr_init * lr_delay_rate\n  else:\n    lr = lr_init\n\n  t = jnp.clip(step / max_steps, 0, 1)\n  lr = (1 - t) * lr + t * lr_final\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    # https://www.desmos.com/calculator/rcmcf5jwe7\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n    delay_rate_diff = (1 - lr_delay_mult) * jnp.pi / 2 / lr_delay_steps\n    delay_steps = lr_init / delay_rate_diff\n    steps_left = lr_delay_steps - step\n    lr = lr_init * delay_rate * steps_left / (steps_left + delay_steps)\n    progress = jnp.clip(step / lr_delay_steps, 0, 1)\n  else:\n    # Cosine decay.\n    progress = jnp.clip(step / max_steps, 0, 1)\n    lr = lr_final + (lr_init - lr_final) * (1 - jnp.cos(jnp.pi * progress)) / 2\n  return lr * progress\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n"}
{"namespace": "utils.dummy_rays", "completion": "  from .random_rays import generate_random_rays\n\n  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n"}
{"namespace": "utils.dummy_rays", "completion": "  # Call the generate_random_rays function with the specified options\n  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n"}
{"namespace": "utils.dummy_rays", "completion": "  from . import generate_random_rays\n  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n"}
{"namespace": "utils.dummy_rays", "completion": "  # TODO: This is a dummy function.\n  # We need to implement the actual function here.\n  # The function should generate random rays based on the specified parameters and options.\n  # The return value should be the result from `generate_random_rays`.\n  # The type and structure of the return value depend on the implementation of `generate_random_rays`.\n\n  # Example:\n  # result = generate_random_rays(include_exposure_idx, include_exposure_values, include_device_idx)\n  # return result\n\n  # Dummy return value\n  return None\n"}
{"namespace": "utils.dummy_rays", "completion": "  # Importing the necessary modules\n  import jax\n  import jax.numpy as jnp\n  from jax import random\n\n  # Defining the `generate_random_rays` function\n  def generate_random_rays(\n      key,\n      num_rays,\n      near,\n      far,\n      image_width,\n      image_height,\n      camera_idx,\n      camera_to_worlds,\n      fx,\n      fy,\n      cx,\n      cy,\n      use_pixel_centers,\n  ):\n\n    \"\"\"\n    Generates random rays based on the provided parameters and options.\n\n    Input-Output Arguments\n    :param key: A random key used for generating random numbers.\n    :param num_rays: The number of rays to generate.\n    :param near: The near plane of the camera frustum.\n    :param far: The far plane of the camera frustum.\n    :param image_width: The width of the image in pixels.\n    :param image_height: The height of the image in pixels.\n    :param camera_idx: The index of the camera.\n    :param camera_to_worlds: The transformation matrices for the camera poses.\n    :param fx: The focal length in the x-direction.\n    :param fy: The focal length in the y-direction.\n    :param cx: The principal point in the x-direction.\n    :param cy: The principal point in the y-direction.\n    :param use_pixel_centers: A boolean flag indicating whether to use the center of each pixel for generating rays.\n    :return: A tuple containing the following elements:\n      - origins: The origin points of the rays.\n      - directions: The direction vectors of the rays.\n      - viewdirs: The view directions of the rays.\n      - radii: The radii of the rays.\n      - lossmult: The loss multipliers for the rays.\n      - near: The near plane of the camera frustum.\n      - far: The far plane of the camera frustum.\n      - metadata: A dictionary containing metadata about the"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  assert camtype == ProjectionType.PERSPECTIVE, 'Only support perspective camera'\n  assert points.shape[-1] == 3, 'points must be (..., 3)'\n  assert pixtocams.shape[-2:] == (3, 3), 'pixtocams must be (..., 3, 3)'\n  assert camtoworlds.shape[-2:] == (3, 4), 'camtoworlds must be (..., 3, 4)'\n  if distortion_params is not None:\n    assert 'k1' in distortion_params, 'distortion_params must have k1'\n    assert 'k2' in distortion_params, 'distortion_params must have k2'\n    assert 'p1' in distortion_params, 'distortion_params must have p1'\n    assert 'p2' in distortion_params, 'distortion_params must have p2'\n    assert 'k3' in distortion_params, 'distortion_params must have k3'\n    assert 'k4' in distortion_params, 'distortion_params must have k4'\n    assert 'k5' in distortion_params, 'distortion_params must have k5'\n    assert 'k6' in distortion_params, 'distortion_params must have k6'\n    assert 's1' in distortion_params, 'distortion_params must have s1'\n    assert 's2' in distortion_params, 'distortion_params must have s2'\n    assert 's3' in distortion_params, 'distortion_params must have s3'\n    assert 's4' in distortion_params, 'distortion_params must have s4'\n    assert 'taux' in distortion_params, 'distortion_params must have taux'\n    assert 'tauy' in distortion_params, 'distortion_params must have tauy'\n    assert distortion_params['k1'].shape == points.shape[:-1], 'k1 must be (...,)'\n    assert distortion_params['k2'].shape == points.shape[:-1], 'k2"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  assert camtype == ProjectionType.PERSPECTIVE\n  assert distortion_params is None\n\n  batch_shape = points.shape[:-2]\n  points = points.reshape(-1, 3)\n  pixtocams = pixtocams.reshape(-1, 3, 3)\n  camtoworlds = camtoworlds.reshape(-1, 4, 4)\n\n  assert pixtocams.shape[0] == camtoworlds.shape[0]\n  assert points.shape[0] in [camtoworlds.shape[0], 1]\n\n  # Convert the points to homogeneous coordinates\n  points = xnp.concatenate([points, xnp.ones_like(points[Ellipsis, :1])], axis=-1)\n\n  # Transform the points from world to camera coordinates\n  points = xnp.matmul(camtoworlds, points[Ellipsis, None])[Ellipsis, 0]\n\n  # Project the points onto the 2D image plane\n  points = xnp.matmul(pixtocams, points[Ellipsis, None])[Ellipsis, 0]\n\n  # Normalize the points by the z-coordinate\n  points = points / points[Ellipsis, 2:]\n\n  # Convert the points to pixel coordinates\n  points = points[Ellipsis, :2]\n\n  # Reshape the points to the original batch shape\n  points = points.reshape(*batch_shape, -1, 2)\n\n  return points, points[Ellipsis, 2:]\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  assert len(points.shape) >= 2\n  assert points.shape[-1] == 3\n  assert len(pixtocams.shape) >= 2\n  assert pixtocams.shape[-2:] == (3, 3)\n  assert len(camtoworlds.shape) >= 2\n  assert camtoworlds.shape[-2:] == (3, 4)\n  assert camtype == ProjectionType.PERSPECTIVE\n  if distortion_params is not None:\n    assert 'k1' in distortion_params\n    assert 'k2' in distortion_params\n    assert 'k3' in distortion_params\n    assert 'p1' in distortion_params\n    assert 'p2' in distortion_params\n\n  def proj(points, pixtocam, camtoworld, distortion_params):\n    camcenter = camtoworld[:, :3, 3]\n    camcenter = camcenter[..., xnp.newaxis]\n    camcenter = xnp.tile(camcenter, points[..., :1].shape)\n    raydirs = points - camcenter\n    raydirs = xnp.squeeze(raydirs / xnp.linalg.norm(raydirs, axis=-1)[..., xnp.newaxis], -1)\n    raydirs = xnp.matmul(pixtocam, raydirs[..., xnp.newaxis])[..., 0]\n    raydirs = raydirs / xnp.linalg.norm(raydirs, axis=-1)[..., xnp.newaxis]\n    if distortion_params is not None:\n      raydirs = _radial_and_tangential_undistort(\n          raydirs,\n          xnp.squeeze(distortion_params['k1']),\n          xnp.squeeze(distortion_params['k2']),\n          xnp.squeeze(distortion_params['k3']),\n          xnp.squeeze(distortion_params['p1']),\n          xnp.squeeze(distortion"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  assert camtype == ProjectionType.PERSPECTIVE\n\n  # Convert points to float32\n  points = xnp.asarray(points, dtype=xnp.float32)\n\n  # Get the shape of points\n  shape = points.shape\n\n  # Check if points is 2D or 3D\n  if len(shape) == 2:\n    # Reshape points to 3D if it is 2D\n    points = points[..., None]\n\n  # Check if points is 3D\n  assert len(points.shape) == 3\n\n  # Get the number of points\n  num_points = points.shape[0]\n\n  # Check if pixtocams is 2D or 3D\n  if len(pixtocams.shape) == 2:\n    # Reshape pixtocams to 3D if it is 2D\n    pixtocams = pixtocams[None, ...]\n\n  # Check if pixtocams is 3D\n  assert len(pixtocams.shape) == 3\n\n  # Check if pixtocams is 3D with the correct number of points\n  assert pixtocams.shape[0] == num_points\n\n  # Check if camtoworlds is 2D or 3D\n  if len(camtoworlds.shape) == 2:\n    # Reshape camtoworlds to 3D if it is 2D\n    camtoworlds = camtoworlds[None, ...]\n\n  # Check if camtoworlds is 3D\n  assert len(camtoworlds.shape) == 3\n\n  # Check if camtoworlds is 3D with the correct number of points\n  assert camtoworlds.shape[0] == num_points\n\n  # Check if distortion_params is provided\n  if distortion_params is not None:\n    # Check if distortion_params is a dictionary\n    assert isinstance(distortion_params, dict)\n\n    # Check if distortion_params contains the required keys\n    assert \"k1\" in distortion_params\n   "}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Get the shape of the input points array\n  shape = points.shape\n\n  # Check if the input points array is 3D or 2D\n  if len(shape) == 2:\n    # If the input points array is 2D, add a new axis to make it 3D\n    points = points[:, None, :]\n\n  # Check if the input points array has the correct number of dimensions\n  assert len(shape) == 3\n\n  # Get the number of points and the number of cameras\n  num_points, num_cams = points.shape[:2]\n\n  # Check if the input pixtocams array has the correct number of dimensions\n  assert len(pixtocams.shape) == 3\n\n  # Check if the input pixtocams array has the correct number of cameras\n  assert pixtocams.shape[0] == num_cams\n\n  # Check if the input camtoworlds array has the correct number of dimensions\n  assert len(camtoworlds.shape) == 3\n\n  # Check if the input camtoworlds array has the correct number of cameras\n  assert camtoworlds.shape[0] == num_cams\n\n  # Check if the input pixtocams array and camtoworlds array have the same number of cameras\n  assert pixtocams.shape[0] == camtoworlds.shape[0]\n\n  # Check if the input pixtocams array and camtoworlds array have the same number of points\n  assert pixtocams.shape[1] == camtoworlds.shape[1]\n\n  # Check if the input pixtocams array and camtoworlds array have the same number of cameras\n  assert pixtocams.shape[2] == camtoworlds.shape[2]\n\n  # Check if the input pixtocams array and camtoworlds array have the same number of cameras\n  assert pixtocams.shape[0] == camtoworlds.shape[0]\n\n  # Check if the input pixtocams array and camtoworlds array have the same number of points\n  assert pixtocams.shape[1]"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  assert camtype == ProjectionType.PERSPECTIVE\n  assert len(points.shape) >= 2\n  assert points.shape[-1] == 3\n  assert len(pixtocams.shape) >= 2\n  assert pixtocams.shape[-2:] == (3, 3)\n  assert len(camtoworlds.shape) >= 2\n  assert camtoworlds.shape[-2:] == (3, 4)\n\n  # Get the shape of the points array.\n  batch_shape = points.shape[:-2]\n\n  # Reshape the points array to have a shape of (-1, 3).\n  points = points.reshape((-1, 3))\n\n  # Reshape the pixtocams array to have a shape of (-1, 3, 3).\n  pixtocams = pixtocams.reshape((-1, 3, 3))\n\n  # Reshape the camtoworlds array to have a shape of (-1, 3, 4).\n  camtoworlds = camtoworlds.reshape((-1, 3, 4))\n\n  # Transform the points from world coordinates to camera coordinates.\n  camcoords = xnp.matmul(\n      xnp.linalg.inv(camtoworlds),\n      xnp.concatenate([points, xnp.ones_like(points[..., :1])], axis=-1)[..., None],\n  )[..., 0]\n\n  # Apply radial and tangential distortion to the camera coordinates.\n  if distortion_params is not None:\n    camcoords[..., :2] = apply_distortion_to_points(\n        camcoords[..., :2], **distortion_params\n    )\n\n  # Project the camera coordinates onto the 2D image plane.\n  pixtocam_shape = pixtocams.shape[:-2]\n  pixtocams = pixtocams.reshape((-1, 3, 3))\n  pixels = xnp.matmul(pixtocams, camcoords["}
{"namespace": "camera_utils.points_to_pixels", "completion": "  assert camtype == ProjectionType.PERSPECTIVE, \"Only support perspective camera\"\n\n  batch_shape = pixtocams.shape[:-2]\n  num_points = points.shape[-2]\n  num_cameras = pixtocams.shape[-3]\n\n  assert pixtocams.shape == (*batch_shape, num_cameras, 3, 3)\n  assert camtoworlds.shape == (*batch_shape, num_cameras, 4, 4)\n  assert points.shape == (*batch_shape, num_points, 3)\n\n  if distortion_params is None:\n    distortion_params = {k: xnp.zeros_like(pixtocams[..., 0, 0]) for k in\n                         [\"k1\", \"k2\", \"k3\", \"p1\", \"p2\"]}\n\n  points_distorted = points\n  for _ in range(3):\n    points_distorted = _radial_and_tangential_undistort(\n      points_distorted,\n      **distortion_params,\n    )\n\n  # NOTE: docs say z is in front and y is up, but the code does x, y, z\n  camcoords = xnp.einsum(\"...ij,...nj->...ni\", camtoworlds[..., :3, :3],\n                         points_distorted) + camtoworlds[..., :3, 3:]\n\n  # [..., num_cameras, 3]\n  camcoords_hom = xnp.concatenate(\n    (camcoords, xnp.ones_like(camcoords[..., :1])), axis=-1)\n\n  pixels_hom = xnp.einsum(\"...ij,...nj->...ni\", pixtocams, camcoords_hom)\n  pixels = xnp.divide(pixels_hom[..., :2], pixels_hom[..., 2:])\n  depth = pixels_hom[..., 2]\n\n  return pixels, depth"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  if camtype != ProjectionType.PERSPECTIVE:\n    raise ValueError(\"Only perspective projection is supported.\")\n\n  if distortion_params is None:\n    distortion_params = {\n        \"k1\": 0.0,\n        \"k2\": 0.0,\n        \"k3\": 0.0,\n        \"p1\": 0.0,\n        \"p2\": 0.0,\n    }\n\n  # Compute the camera center in world coordinates\n  camcenter = camtoworlds[..., :3, 3]\n\n  # Compute the ray direction from the camera center to the 3D points\n  ray_dir = points - camcenter[..., None, :]\n\n  # Compute the ray direction in camera coordinates\n  ray_cam = xnp.matmul(camtoworlds[..., :3, :3], ray_dir)\n\n  # Compute the normalized ray direction\n  ray_norm = xnp.linalg.norm(ray_cam, axis=-1, keepdims=True)\n  ray_dir_norm = ray_cam / ray_norm\n\n  # Compute the 2D pixel coordinates in the normalized image plane\n  ray_pix = xnp.matmul(pixtocams, ray_dir_norm)\n\n  # Apply radial and tangential distortion\n  k1 = distortion_params[\"k1\"]\n  k2 = distortion_params[\"k2\"]\n  k3 = distortion_params[\"k3\"]\n  p1 = distortion_params[\"p1\"]\n  p2 = distortion_params[\"p2\"]\n\n  r2 = xnp.sum(ray_pix[..., :2] ** 2, axis=-1, keepdims=True)\n  radial_distortion = 1 + k1 * r2 + k2 * r2 ** 2 + k3 * r2 ** 3\n\n  tangential_distortion = xnp.array([[p2], [p1]]) * ray_pix[..., [1], :]\n  tangential_distortion += x"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  assert len(points.shape) == len(pixtocams.shape) == len(camtoworlds.shape)\n  assert points.shape[-1] == 3\n  assert pixtocams.shape[-1] == 3 and pixtocams.shape[-2] == 3\n  assert camtoworlds.shape[-1] == 4 and camtoworlds.shape[-2] == 4\n\n  # The leading dimensions of the input arrays.\n  batch_shape = points.shape[:-2]\n\n  # The number of points to project.\n  num_points = points.shape[-2]\n\n  # Reshape the input arrays to handle vectorized operations.\n  points = points.reshape((-1, num_points, 3))\n  pixtocams = pixtocams.reshape((-1, 3, 3))\n  camtoworlds = camtoworlds.reshape((-1, 4, 4))\n\n  # Transform points from world to camera coordinates.\n  camtoworlds_inv = xnp.linalg.inv(camtoworlds)\n  cam_points = xnp.einsum('bij,bkj->bki', camtoworlds_inv, points)\n\n  # Project points onto the 2D image plane.\n  if camtype == ProjectionType.PERSPECTIVE:\n    cam_points = xnp.concatenate([cam_points, xnp.ones_like(cam_points[..., :1])], axis=-1)\n    cam_points = xnp.einsum('bij,bkj->bki', pixtocams, cam_points)\n    cam_points = cam_points[..., :2] / cam_points[..., 2:]\n\n  # Correct for radial and tangential distortion.\n  if distortion_params is not None:\n    k1 = distortion_params['k1']\n    k2 = distortion_params['k2']\n    p1 = distortion_params['p1']\n    p2 = distortion_params['p2']\n    k3"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  if camtype != ProjectionType.PERSPECTIVE:\n    raise NotImplementedError(\"Only perspective camera supported.\")\n\n  if distortion_params is None:\n    distortion_params = {}\n\n  # Convert points to camera coordinates.\n  cam_coords = points_to_camcoords(points, camtoworlds, xnp=xnp)\n\n  # Project to pixels.\n  coordinates = camcoords_to_pixels(\n      cam_coords, pixtocams, distortion_params, xnp=xnp\n  )\n\n  # Compute depth values.\n  depth = xnp.linalg.norm(cam_coords, axis=-1)\n\n  return coordinates, depth\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  assert camtype == ProjectionType.PERSPECTIVE\n  assert points.shape[-1] == 3\n  assert len(pixtocams.shape) == 3\n  assert len(camtoworlds.shape) == 3\n  assert points.shape[0] == pixtocams.shape[1]\n  assert points.shape[0] == camtoworlds.shape[1]\n\n  xnp = np if xnp is None else xnp\n\n  assert xnp.all(camtoworlds[..., :3, :3] == xnp.transpose(camtoworlds[..., :3, :3], (0, 2, 1)))\n\n  camtoworlds = camtoworlds[:, :3]\n  pixtocams = pixtocams[:, :3, :3]\n  camcenter = xnp.broadcast_to(camtoworlds[:, :3, 3], list(points.shape[:-1]) + [3])\n  p_cam = xnp.squeeze(xnp.matmul(xnp.expand_dims(camtoworlds, axis=2),\n                                xnp.expand_dims(points - camcenter, axis=-1)), axis=2)\n\n  p_cam = xnp.concatenate([p_cam, xnp.ones_like(p_cam[Ellipsis, 0:1])], axis=-1)\n  p_img = xnp.squeeze(xnp.matmul(xnp.expand_dims(pixtocams, axis=2),\n                                 xnp.expand_dims(p_cam, axis=-1)), axis=2)\n  xy = p_img[Ellipsis, :2] / p_img[Ellipsis, 2:3]\n  depth = p_cam[Ellipsis, 2]\n\n  if distortion_params is not None:\n    xy = distort_points(xy, distortion_params, xnp=xnp)\n\n  return xy, depth\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  assert camtype == ProjectionType.PERSPECTIVE\n  assert points.shape[-1] == 3\n  assert len(pixtocams.shape) == 3\n  assert len(camtoworlds.shape) == 3\n  assert points.shape[0] == pixtocams.shape[1]\n  assert points.shape[0] == camtoworlds.shape[1]\n\n  xnp = jnp if isinstance(pixtocams, jnp.ndarray) else xnp\n\n  # should be B x 3 x N\n  assert len(points.shape) == 3\n  points = points.transpose((0, 2, 1))\n\n  # camera coordinate system to world coordinate system\n  points = xnp.matmul(camtoworlds, points)  # B x 3 x N\n\n  # world coordinate system to camera coordinate system\n  points = points[:, :3, :] / points[:, 2:3, :]  # B x 3 x N\n  points = points.transpose((0, 2, 1))  # B x N x 3\n\n  # camera coordinate system to image (pixel) coordinate system.\n  coordinates = xnp.matmul(points, pixtocams)  # B x N x 3\n  if distortion_params is None:\n    coordinates = coordinates[..., :2] / coordinates[..., 2:3]\n  else:\n    # The distortion is applied to the normalized coordinates\n    # (i.e. after projection)\n    k1 = distortion_params[\"k1\"]\n    k2 = distortion_params[\"k2\"]\n    k3 = distortion_params[\"k3\"]\n    p1 = distortion_params[\"p1\"]\n    p2 = distortion_params[\"p2\"]\n    coordinates = coordinates[..., :2] / coordinates[..., 2:3]  # B x N x 2\n    # To account for residual radial and tangential distortion: to the distorted\n    # normalized points, apply the distortion coefficients:\n    # (1 + k1 r^2 + k2 r"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  if camtype != ProjectionType.PERSPECTIVE:\n    raise NotImplementedError(\"Only perspective camera is supported.\")\n\n  if distortion_params is not None:\n    raise NotImplementedError(\"Distortion not implemented yet.\")\n\n  # Get the number of leading dimensions in the input arrays.\n  batch_shape = points.shape[:-2]\n\n  # Reshape the input arrays to have a single leading dimension.\n  points = points.reshape((-1, 3))\n  pixtocams = pixtocams.reshape((-1, 3, 3))\n  camtoworlds = camtoworlds.reshape((-1, 4, 4))\n\n  # Transform the points from world coordinates to camera coordinates.\n  cam_points = xnp.einsum('bij,bpj->bpi', camtoworlds, points[..., None])[..., 0]\n\n  # Project the points onto the image plane.\n  cam_points = xnp.concatenate([cam_points, xnp.ones_like(cam_points[..., :1])], axis=-1)\n  pixels = xnp.einsum('bij,bpj->bpi', pixtocams, cam_points[..., None])[..., 0]\n\n  # Compute the depth values of the points in the camera coordinate system.\n  depth = xnp.linalg.norm(cam_points, axis=-1)\n\n  # Reshape the output arrays to match the input dimensions.\n  pixels = pixels.reshape(batch_shape + (3,))\n  depth = depth.reshape(batch_shape)\n\n  return pixels, depth\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  assert camtype == ProjectionType.PERSPECTIVE, \"Only perspective camera is supported.\"\n\n  # Convert to numpy arrays if necessary\n  if not isinstance(points, xnp.ndarray):\n    points = xnp.array(points)\n  if not isinstance(pixtocams, xnp.ndarray):\n    pixtocams = xnp.array(pixtocams)\n  if not isinstance(camtoworlds, xnp.ndarray):\n    camtoworlds = xnp.array(camtoworlds)\n  if distortion_params is not None and not isinstance(distortion_params, xnp.ndarray):\n    distortion_params = xnp.array(distortion_params)\n\n  # Check input shapes\n  assert points.shape[-1] == 3, \"points must have shape (..., 3)\"\n  assert pixtocams.shape[-2:] == (3, 3), \"pixtocams must have shape (..., 3, 3)\"\n  assert camtoworlds.shape[-2:] == (3, 4), \"camtoworlds must have shape (..., 3, 4)\"\n  if distortion_params is not None:\n    assert distortion_params.shape[-1] == 2, \"distortion_params must have shape (..., 2)\"\n\n  # Convert points to homogeneous coordinates\n  points = xnp.concatenate([points, xnp.ones_like(points[..., :1])], axis=-1)\n\n  # Apply camera extrinsics\n  camtoworlds = camtoworlds[..., :3, :]\n  points = xnp.einsum(\"...ij,...j->...i\", camtoworlds, points)\n\n  # Apply camera intrinsics\n  pixtocams = pixtocams[..., :3, :3]\n  points = xnp.einsum(\"...ij,...j->...i\", pixtocams, points)\n\n  # Apply radial and tangential distortion\n  if distortion_params is not None:\n    r2 = xnp.sum(points"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert points to homogeneous coordinates\n  points = xnp.concatenate([points, xnp.ones_like(points[..., :1])], -1)\n\n  # Apply camera extrinsics\n  points = xnp.einsum('...ij,...j->...i', camtoworlds, points)\n\n  # Apply camera intrinsics\n  points = xnp.einsum('...ij,...j->...i', pixtocams, points)\n\n  # Apply distortion if provided\n  if distortion_params is not None:\n    points = distort_points(points, **distortion_params)\n\n  # Normalize by depth\n  points = points[..., :2] / points[..., 2:]\n\n  # Convert to pixel coordinates\n  points = points[..., ::-1]\n\n  # Compute depth\n  depth = xnp.linalg.norm(points[..., :2], axis=-1)\n\n  return points, depth\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Check that the input arrays have the correct shape\n  assert points.shape[-1] == 3\n  assert pixtocams.shape[-2:] == (3, 3)\n  assert camtoworlds.shape[-2:] == (3, 4)\n\n  # Get the number of leading dimensions in the input arrays\n  n_batch_dims = len(points.shape) - 2\n\n  # Reshape the input arrays to have a single leading dimension\n  points = points.reshape(-1, 3)\n  pixtocams = pixtocams.reshape(-1, 3, 3)\n  camtoworlds = camtoworlds.reshape(-1, 3, 4)\n\n  # Get the camera center in world coordinates\n  camorigin = camtoworlds[..., :3, 3]\n\n  # Transform points from world to camera coordinates\n  cam_points = xnp.einsum('ijk,ikl->ijl', camtoworlds, points[..., None])[..., 0]\n\n  # Calculate the depth of the points\n  depth = xnp.linalg.norm(cam_points - camorigin, axis=-1)\n\n  # Calculate the 2D pixel coordinates of the points\n  if camtype == ProjectionType.PERSPECTIVE:\n    cam_points = xnp.concatenate([cam_points, xnp.ones_like(cam_points[..., :1])], axis=-1)\n    cam_points = xnp.einsum('ijk,ikl->ijl', pixtocams, cam_points[..., None])[..., 0]\n    cam_points = cam_points / cam_points[..., 2:]\n    coordinates = cam_points[..., :2]\n  else:\n    raise NotImplementedError(f'{camtype} camera not yet supported.')\n\n  # Correct for radial and tangential distortion if distortion parameters are provided\n  if distortion_params is not None:\n    # Get the radial and tangential distortion parameters\n    k"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  assert len(points.shape) >= 2\n  assert points.shape[-1] == 3\n  assert len(pixtocams.shape) >= 2\n  assert pixtocams.shape[-2:] == (3, 3)\n  assert len(camtoworlds.shape) >= 2\n  assert camtoworlds.shape[-2:] == (3, 4)\n  assert len(points.shape[:-2]) == len(pixtocams.shape[:-3])\n  assert len(points.shape[:-2]) == len(camtoworlds.shape[:-2])\n\n  # Convert the points to camera coordinates by applying the extrinsic matrices.\n  camtoworlds = camtoworlds[..., :3, :]\n  camtoworlds_points = xnp.concatenate(\n      (camtoworlds[..., None, :, :], xnp.broadcast_to(points[..., None], points.shape[:-1] + (1, 3))),\n      axis=-1,\n  )\n  camerapoints = xnp.squeeze(\n      xnp.matmul(camtoworlds_points, xnp.moveaxis(pixtocams, -1, -2)), axis=-1\n  )\n\n  # Normalize the points by dividing them by their z-coordinate.\n  if camtype == ProjectionType.PERSPECTIVE:\n    valid_z = xnp.logical_and(camerapoints[..., 2] >= MIN_DEPTH, camerapoints[..., 2] <= MAX_DEPTH)\n    camerapoints = xnp.where(\n        xnp.logical_and(valid_z[..., None], xnp.abs(camerapoints) > MIN_NORM),\n        camerapoints / (camerapoints[..., 2:] + EPS),\n        xnp.zeros_like(camerapoints),\n    )\n\n    # Apply radial and tangential distortion if distortion parameters are provided.\n    if distortion_"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  assert camtype == ProjectionType.PERSPECTIVE\n\n  if distortion_params is not None:\n    points = undistort_points(points, distortion_params, xnp = xnp)\n\n  # should be B x 3 x ?\n  assert len(points.shape) == 3\n  assert points.shape[1] == 3\n\n  # should be B x 3 x 3\n  assert len(camtoworlds.shape) == 3\n  assert camtoworlds.shape[1] == 3\n  assert camtoworlds.shape[2] == 3\n\n  # should be B x 3 x 3\n  assert len(pixtocams.shape) == 3\n  assert pixtocams.shape[1] == 3\n  assert pixtocams.shape[2] == 3\n\n  # project points\n  # NOTE: for column vectors, the variables below don't need to be transposed\n  camcoords = xnp.matmul(camtoworlds, points)\n  pixcoords = xnp.matmul(pixtocams, camcoords)\n\n  # normalize via the last axis\n  pixcoords = pixcoords / pixcoords[..., -xnp.newaxis]\n  # NOTE: unconventional, but seems to work better\n  # pixcoords = pixcoords / pixcoords[..., xnp.newaxis, -1]\n\n  # NOTE: this is unconventional but it seems that\n  # there are some cases where the results are much better\n  # when dividing by the W component\n  # w = pixcoords[..., -1, :]\n  # pixcoords = pixcoords / w[..., xnp.newaxis]\n\n  # NOTE: another unconventional but seemingly better way\n  # w = pixcoords[..., -1:, :]\n  # pixcoords = pixcoords / w\n\n  # convert to from homogeneous\n  # NOTE: not sure whether it should be divided by the last channel or the second last channel\n  # pixcoords = pixcoords"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    cam_resolution = pixtocams.shape[1:-1]\n    batch_shape = points.shape[:-2]\n    homog_points = xnp.concatenate(\n        [points, xnp.ones_like(points[..., :1])], axis=-1\n    )\n    if distortion_params is None:\n      pixels = xnp.einsum(\"bij,...j->...bi\", pixtocams, homog_points)\n      pixels = pixels[..., :2] / pixels[..., 2:3]\n      depths = pixels[..., 2:3]\n    else:\n      pixels = xnp.einsum(\"bij,...j->...bi\", pixtocams, homog_points)\n      r2 = xnp.sum(pixels[..., :2] ** 2, axis=-1, keepdims=True)\n      radial = (\n          1\n          + distortion_params[\"k1\"] * r2\n          + distortion_params[\"k2\"] * r2 ** 2\n          + distortion_params[\"k3\"] * r2 ** 3\n      )\n      tan = xnp.sum(\n          xnp.tan(xnp.sqrt(r2) * xnp.arctan(xnp.sqrt(r2) * distortion_params[\"p1\"]))\n          + xnp.tan(xnp.sqrt(r2) * xnp.arctan(xnp.sqrt(r2) * distortion_params[\"p2\"]))\n      )\n      pixels = (\n          pixels[..., :2] * (radial + tan)\n          + pixels[..., 2:3] * distortion_params[\"k1\"] * r2\n      )\n      pixels = pixels[..., :2] / pixels[..., 2:3]\n      depths = pixels[..., 2:3]\n    pixels = pixels.reshape(batch_shape + (pixels.shape[-2], 2))\n    depths = depths.reshape(batch"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  assert camtype == ProjectionType.PERSPECTIVE\n\n  # The number of leading dimensions of the input arrays\n  n_batch_dims = len(points.shape) - 2\n\n  # Reshape the input arrays to have a leading batch dimension\n  points = points.reshape([-1] + list(points.shape[-2:]))\n  pixtocams = pixtocams.reshape([-1] + list(pixtocams.shape[-3:]))\n  camtoworlds = camtoworlds.reshape([-1] + list(camtoworlds.shape[-3:]))\n\n  # Transform the points from world coordinates to camera coordinates\n  points_cam = xnp.matmul(\n      camtoworlds[Ellipsis, :3, :3], points[Ellipsis, :, None]\n  ) + camtoworlds[Ellipsis, :3, 3:4]\n\n  # Apply the inverse of the camera intrinsics matrix to transform points from camera coordinates to pixel coordinates\n  points_cam = xnp.matmul(pixtocams, points_cam)\n  points_cam = points_cam[Ellipsis, :2] / points_cam[Ellipsis, 2:3]\n\n  # Apply radial and tangential distortion correction\n  if distortion_params is not None:\n    k1 = distortion_params[\"k1\"]\n    k2 = distortion_params[\"k2\"]\n    p1 = distortion_params[\"p1\"]\n    p2 = distortion_params[\"p2\"]\n    k3 = distortion_params.get(\"k3\", 0.0)\n\n    r2 = xnp.sum(points_cam**2, axis=-1, keepdims=True)\n    radial = 1.0 + r2 * (k1 + r2 * (k2 + r2 * k3))\n    tan = 2.0 * p1 * points_cam[Ellipsis, 0:1] * points_cam[Ellipsis, 1:2] + p2 * (\n       "}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4) + skew(screw_axis)\n  else:\n    w = screw_axis[:3] / theta\n    v = screw_axis[3:]\n    return jnp.eye(4) + \\\n      jnp.sin(theta) * skew(w) + \\\n      (1 - jnp.cos(theta)) * jnp.outer(w, w) + \\\n      (theta - jnp.sin(theta)) * jnp.outer(w, v) / theta\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the angle-axis rotation and translation from the screw axis\n  w = screw_axis[:3]\n  v = screw_axis[3:]\n\n  # Compute the magnitude of the angle-axis rotation\n  theta = jnp.linalg.norm(w)\n\n  # Compute the skew-symmetric matrix of the angle-axis rotation\n  w_hat = skew_3d(w)\n\n  # Compute the exponential map\n  if theta < eps:\n    # If the angle is small, use the first-order Taylor expansion\n    R = jnp.eye(3) + w_hat\n    p = v\n  else:\n    # Otherwise, use the Rodrigues formula\n    R = jnp.eye(3) + jnp.sin(theta) * w_hat / theta + (1 - jnp.cos(theta)) * w_hat @ w_hat / (theta ** 2)\n    p = (jnp.eye(3) - R) @ (w_hat @ v) + w_hat @ w_hat @ v * theta\n\n  # Construct the homogeneous transformation matrix\n  T = jnp.eye(4)\n  T[:3, :3] = R\n  T[:3, 3] = p\n\n  return T\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4) + skew_3d(screw_axis)\n  else:\n    w = screw_axis[:3] / theta\n    v = screw_axis[3:]\n    w_hat = skew_3d(w)\n    R = jnp.eye(3) + jnp.sin(theta) * w_hat + (1 - jnp.cos(theta)) * w_hat @ w_hat\n    p = (jnp.eye(3) - R) @ (w_hat @ v) + w * jnp.dot(w, v) * theta\n    return jnp.block([[R, p[:, None]], [0, 0, 0, 1]])\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  w = screw_axis[:3] / (theta + eps)\n  v = screw_axis[3:]\n  w_hat = skew_symmetric(w)\n  R = jnp.eye(3) + jnp.sin(theta) * w_hat + (1 - jnp.cos(theta)) * w_hat @ w_hat\n  p = (jnp.eye(3) - R) @ (w_hat @ v) + w * v.dot(w.T) * theta\n  return jnp.vstack((jnp.hstack((R, p[:, None])), jnp.array([0, 0, 0, 1])))\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the angle-axis rotation and translation components from the screw axis\n  w = screw_axis[:3]\n  v = screw_axis[3:]\n\n  # Compute the skew-symmetric matrix of the rotation component\n  w_hat = skew_symmetric(w)\n\n  # Compute the magnitude of the rotation component\n  theta = jnp.linalg.norm(w)\n\n  # Compute the exponential map using the Rodrigues formula\n  if theta < eps:\n    R = jnp.eye(3) + w_hat\n    p = v\n  else:\n    R = (\n        jnp.eye(3)\n        + (jnp.sin(theta) / theta) * w_hat\n        + ((1 - jnp.cos(theta)) / (theta**2)) * (w_hat @ w_hat)\n    )\n    p = (jnp.eye(3) + ((1 - jnp.cos(theta)) / (theta**2)) * w_hat\n         + ((theta - jnp.sin(theta)) / (theta**3)) * (w_hat @ w_hat)) @ v\n\n  # Construct the homogeneous transformation matrix\n  T = jnp.eye(4)\n  T[:3, :3] = R\n  T[:3, 3] = p\n\n  return T\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[:3]\n  v = screw_axis[3:]\n  theta = jnp.linalg.norm(w)\n  if theta < eps:\n    R = jnp.eye(3)\n    p = v\n  else:\n    R = exp_so3(w)\n    p = jnp.dot(jnp.eye(3) * theta + (1 - jnp.cos(theta)) * skew_symmetric(w) + (theta - jnp.sin(theta)) * jnp.dot(skew_symmetric(w), skew_symmetric(w)), v) / theta ** 2\n  return jnp.block([[R, p[:, None]], [jnp.zeros(3), 1]])\n\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  omega = screw_axis[:3] / (theta + eps)\n  v = screw_axis[3:]\n\n  omega_hat = skew(omega)\n  omega_hat_2 = omega_hat @ omega_hat\n\n  R = jnp.eye(3) + jnp.sin(theta) * omega_hat + (1 - jnp.cos(theta)) * omega_hat_2\n\n  V = (\n      jnp.eye(3) * theta\n      + (1 - jnp.cos(theta)) * omega_hat\n      + (theta - jnp.sin(theta)) * omega_hat_2\n  )\n\n  p = V @ v\n\n  T = jnp.eye(4)\n  T = T.at[:3, :3].set(R)\n  T = T.at[:3, 3].set(p)\n\n  return T\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the angle-axis rotation and translation from the screw axis\n  w = screw_axis[:3]\n  v = screw_axis[3:]\n\n  # Calculate the magnitude of the rotation\n  theta = jnp.linalg.norm(w)\n\n  # If the magnitude of the rotation is small, use a Taylor series expansion to approximate the exponential map\n  if theta < eps:\n    R = jnp.eye(3) + skew(w)\n    V = jnp.eye(3) + skew(w) + (1.0 / 2.0) * skew(w) @ skew(w)\n  else:\n    # Otherwise, use the Rodrigues formula to calculate the rotation matrix\n    R = rodrigues(w, theta)\n    V = (\n      jnp.eye(3)\n      + (1.0 - jnp.cos(theta)) / (theta**2) * skew(w)\n      + (theta - jnp.sin(theta)) / (theta**3) * skew(w) @ skew(w)\n    )\n\n  # Calculate the exponential map\n  T = jnp.eye(4)\n  T[:3, :3] = R\n  T[:3, 3] = V @ v\n\n  return T\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    theta = eps\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:]\n  w_hat = skew_3d(w)\n  R = jnp.eye(3) + jnp.sin(theta) * w_hat + (1 - jnp.cos(theta)) * w_hat @ w_hat\n  p = (jnp.eye(3) - R) @ (w_hat @ v) + w * w.T @ v * theta\n  T = jnp.eye(4)\n  T = T.at[:3, :3].set(R)\n  T = T.at[:3, 3].set(p)\n  return T\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4) + skew(screw_axis)\n  else:\n    w = screw_axis[:3] / theta\n    v = screw_axis[3:]\n    w_hat = skew(w)\n    R = jnp.eye(3) + jnp.sin(theta) * w_hat + (1 - jnp.cos(theta)) * (w_hat @ w_hat)\n    p = (jnp.eye(3) - R) @ (w.reshape((3, 1)) @ v.reshape((1, 3))) + (theta * jnp.sin(theta)) * (w.reshape((3, 1)) @ w.reshape((1, 3)))\n    return jnp.vstack([jnp.hstack([R, p]), jnp.array([0, 0, 0, 1])])\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Unpack the screw axis\n  w = screw_axis[:3]\n  v = screw_axis[3:]\n\n  # Compute the angle of rotation\n  theta = jnp.linalg.norm(w)\n\n  # Compute the skew-symmetric matrix of the axis of rotation\n  w_hat = skew_3d(w)\n\n  # Compute the exponential map\n  R = jnp.eye(3) + jnp.sin(theta) / (theta + eps) * w_hat + (1 - jnp.cos(theta)) / (theta**2 + eps) * w_hat @ w_hat\n  p = (jnp.eye(3) - R) @ (w_hat @ v) + (w @ v.T) * theta / (theta**2 + eps)\n\n  # Construct the homogeneous transformation matrix\n  T = jnp.eye(4)\n  T[:3, :3] = R\n  T[:3, 3] = p\n\n  return T\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:]\n\n  if theta < eps:\n    R = jnp.eye(3) + skew(w)\n    p = v\n  else:\n    R = (\n        jnp.eye(3)\n        + jnp.sin(theta) * skew(w)\n        + (1 - jnp.cos(theta)) * jnp.outer(w, w)\n    )\n    p = (\n        jnp.eye(3)\n        @ (theta * jnp.sin(theta))\n        @ w\n        + (1 - jnp.cos(theta)) * jnp.outer(w, w) @ v\n        + (theta - jnp.sin(theta)) * jnp.outer(w, v)\n    )\n\n  return jnp.vstack([jnp.hstack([R, jnp.expand_dims(p, axis=1)]), [0, 0, 0, 1]])\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4) + skew(screw_axis)\n  else:\n    w = screw_axis[:3] / theta\n    v = screw_axis[3:]\n    return (\n        jnp.eye(3)\n        + (jnp.sin(theta) / theta) * skew(w)\n        + ((1 - jnp.cos(theta)) / theta**2) * (skew(w) @ skew(w))\n    ) @ jnp.eye(3) + (\n        (1 / theta) * skew(v)\n        + (1 - jnp.cos(theta)) * (skew(w) @ skew(v))\n        + (theta - jnp.sin(theta)) * (skew(w) @ skew(w) @ skew(v))\n    )\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[:3]\n  v = screw_axis[3:]\n  theta = jnp.linalg.norm(w)\n  if theta < eps:\n    R = jnp.eye(3)\n    p = v\n  else:\n    R = exp_so3(w)\n    p = jnp.dot(\n      jnp.eye(3) * theta + (1 - jnp.cos(theta)) * skew(w) + (theta - jnp.sin(theta)) * jnp.dot(skew(w), skew(w)),\n      v / theta\n    )\n  return jnp.concatenate([jnp.concatenate([R, jnp.expand_dims(p, axis=1)], axis=1), jnp.expand_dims(jnp.array([0, 0, 0, 1]), axis=0)], axis=0)\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the angle-axis rotation and translation components from the screw axis\n  w = screw_axis[:3]\n  v = screw_axis[3:]\n\n  # Compute the skew-symmetric matrix of the rotation axis\n  w_hat = skew_symmetric(w)\n\n  # Compute the matrix exponential of the skew-symmetric matrix of the rotation axis\n  R = exp_so3(w, eps)\n\n  # Compute the matrix logarithm of the rotation matrix\n  theta = jnp.linalg.norm(w)\n  if theta < eps:\n    V = jnp.eye(3) + w_hat\n  else:\n    V = jnp.eye(3) + (1 - jnp.cos(theta)) / theta**2 * w_hat + (theta - jnp.sin(theta)) / theta**3 * w_hat @ w_hat\n\n  # Compute the translation component of the homogeneous transformation matrix\n  t = V @ v\n\n  # Construct the homogeneous transformation matrix\n  T = jnp.eye(4)\n  T[:3, :3] = R\n  T[:3, 3] = t\n\n  return T\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  w = jnp.zeros(3)\n  v = jnp.zeros(3)\n\n  if theta < eps:\n    w = jnp.array([0, 0, 0])\n    v = screw_axis[3:]\n  else:\n    w = screw_axis[:3] / theta\n    v = screw_axis[3:] / theta\n\n  Omega = skew_symmetric(w)\n  Omega_sq = Omega @ Omega\n\n  R = (\n      jnp.eye(3)\n      + (jnp.sin(theta) * Omega)\n      + ((1 - jnp.cos(theta)) * Omega_sq)\n  )\n  p = (\n      jnp.eye(3) @ v * theta\n      + (1 - jnp.cos(theta)) @ (Omega @ v)\n      + (theta - jnp.sin(theta)) @ (Omega_sq @ v)\n  )\n\n  return jnp.block([[R, p[:, None]], [jnp.zeros(3), 1]])\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the angle-axis rotation and translation components from the screw axis\n  w = screw_axis[:3]\n  v = screw_axis[3:]\n\n  # Compute the skew-symmetric matrix for the angle-axis rotation\n  Omega = skew_3d(w)\n\n  # Compute the translation component of the homogeneous transformation matrix\n  if jnp.linalg.norm(w) < eps:\n    # If the angle-axis rotation is small, use a first-order approximation\n    R = jnp.eye(3) + Omega\n    p = v\n  else:\n    # Otherwise, use the exponential map for the rotation\n    theta = jnp.linalg.norm(w)\n    u = w / theta\n    R = jnp.cos(theta) * jnp.eye(3) + (1 - jnp.cos(theta)) * jnp.outer(u, u) + jnp.sin(theta) * Omega\n    p = (jnp.sin(theta) * jnp.eye(3) + (1 - jnp.cos(theta)) * Omega + (1 - jnp.cos(theta) - theta * jnp.sin(theta)) * jnp.outer(u, u)) @ v\n\n  # Construct the homogeneous transformation matrix\n  T = jnp.eye(4)\n  T[:3, :3] = R\n  T[:3, 3] = p\n\n  return T\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    theta = eps\n  omega_hat = skew(screw_axis[:3] / theta)\n  exp_se3 = jnp.eye(4) + jnp.sin(theta) * omega_hat + (1 - jnp.cos(theta)) * omega_hat @ omega_hat\n  exp_se3[:3, 3] = (jnp.eye(3) - exp_se3[:3, :3]) @ (screw_axis[3:] / theta)\n  return exp_se3\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the angle-axis rotation and translation from the screw axis\n  w = screw_axis[:3]\n  v = screw_axis[3:]\n\n  # Compute the skew-symmetric matrix of the angle-axis rotation\n  w_hat = skew_symmetric(w)\n\n  # Compute the magnitude of the angle-axis rotation\n  theta = jnp.linalg.norm(w)\n\n  # Compute the exponential map\n  if theta < eps:\n    # If the angle is small, use the first-order approximation\n    R = jnp.eye(3) + w_hat\n    p = v\n  else:\n    # Otherwise, use the second-order approximation\n    R = jnp.eye(3) + jnp.sin(theta) / theta * w_hat + (1 - jnp.cos(theta)) / (theta ** 2) * w_hat @ w_hat\n    p = (jnp.eye(3) - R) @ w_hat @ v + (theta * jnp.sin(theta) / theta) * v\n\n  # Construct the homogeneous transformation matrix\n  T = jnp.eye(4)\n  T[:3, :3] = R\n  T[:3, 3] = p\n\n  return T\n\n\n\n\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4) + skew_3d(screw_axis[:3]) + skew_3d(screw_axis[:3]) @ skew_3d(screw_axis[:3])\n  else:\n    w = screw_axis[:3] / theta\n    v = screw_axis[3:]\n    return jnp.eye(4) + jnp.sin(theta) * skew_3d(w) + (1 - jnp.cos(theta)) * skew_3d(w) @ skew_3d(w) + (jnp.dot(jnp.eye(3) - jnp.cos(theta) * skew_3d(w) + (theta - jnp.sin(theta)) * skew_3d(w) @ skew_3d(w), v) / theta) * skew_3d(w)\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the angle and axis from the axis-angle representation\n  angle = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (angle + eps)\n\n  # Compute the skew-symmetric matrix of the axis\n  skew_axis = jnp.array([[0, -axis[2], axis[1]],\n                         [axis[2], 0, -axis[0]],\n                         [-axis[1], axis[0], 0]])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(angle) * skew_axis + (1 - jnp.cos(angle)) * jnp.matmul(skew_axis, skew_axis)\n\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the norm of the axis-angle vector\n  theta = jnp.linalg.norm(axis_angle)\n\n  # If the norm is smaller than eps, return the identity matrix\n  if theta < eps:\n    return jnp.eye(3)\n\n  # Normalize the axis-angle vector\n  axis = axis_angle / theta\n\n  # Compute the cross-product matrix of the axis vector\n  axis_cross = jnp.array([[0, -axis[2], axis[1]],\n                         [axis[2], 0, -axis[0]],\n                         [-axis[1], axis[0], 0]])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(theta) * axis_cross + (1 - jnp.cos(theta)) * axis_cross @ axis_cross\n\n  return R\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the norm of the axis-angle vector\n  theta = jnp.linalg.norm(axis_angle)\n\n  # Check if the norm is less than or equal to eps\n  if theta <= eps:\n    # If the norm is less than or equal to eps, return the identity matrix\n    return jnp.eye(3)\n\n  # Compute the skew-symmetric matrix of the axis-angle vector\n  axis_angle_skew = skew_symmetric(axis_angle)\n\n  # Compute the rotation matrix using Rodrigues' formula\n  rotation_matrix = (\n      jnp.eye(3)\n      + jnp.sin(theta) / theta * axis_angle_skew\n      + (1 - jnp.cos(theta)) / theta**2 * axis_angle_skew @ axis_angle_skew\n  )\n\n  return rotation_matrix\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the angle of rotation\n  angle = jnp.linalg.norm(axis_angle)\n\n  # If the angle is small, use a Taylor series expansion to compute the rotation matrix\n  if angle < eps:\n    skew_axis_angle = jnp.array([[0, -axis_angle[2], axis_angle[1]],\n                                 [axis_angle[2], 0, -axis_angle[0]],\n                                 [-axis_angle[1], axis_angle[0], 0]])\n    return jnp.eye(3) + skew_axis_angle\n\n  # Otherwise, use Rodrigues' formula to compute the rotation matrix\n  else:\n    axis = axis_angle / angle\n    skew_axis = jnp.array([[0, -axis[2], axis[1]],\n                           [axis[2], 0, -axis[0]],\n                           [-axis[1], axis[0], 0]])\n    return jnp.eye(3) + jnp.sin(angle) * skew_axis + (1 - jnp.cos(angle)) * jnp.dot(skew_axis, skew_axis)\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  if theta < eps:\n    return jnp.eye(3) + skew_symmetric(axis_angle)\n  else:\n    axis = axis_angle / theta\n    return jnp.cos(theta) * jnp.eye(3) + (1 - jnp.cos(theta)) * jnp.outer(axis, axis) + jnp.sin(theta) * skew_symmetric(axis)\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the norm of the axis-angle vector\n  angle = jnp.linalg.norm(axis_angle)\n\n  # If the angle is very small, use a Taylor expansion\n  if angle < eps:\n    skew_axis_angle = so3_hat(axis_angle)\n    return jnp.eye(3) + skew_axis_angle + 0.5 * jnp.linalg.matrix_power(\n      skew_axis_angle, 2\n    )\n\n  # Otherwise, use Rodrigues' formula\n  axis = axis_angle / angle\n  skew_axis = so3_hat(axis)\n  return (\n    jnp.eye(3)\n    + jnp.sin(angle) * skew_axis\n    + (1 - jnp.cos(angle)) * jnp.linalg.matrix_power(skew_axis, 2)\n  )\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the angle of rotation\n  angle = jnp.linalg.norm(axis_angle)\n\n  # If the angle is very small, use a Taylor expansion to avoid numerical instability\n  if angle < eps:\n    skew_axis_angle = skew(axis_angle)\n    return jnp.eye(3) + skew_axis_angle + 0.5 * jnp.linalg.matrix_power(skew_axis_angle, 2)\n\n  # Otherwise, use Rodrigues' formula to compute the rotation matrix\n  axis = axis_angle / angle\n  skew_axis = skew(axis)\n  return jnp.eye(3) + jnp.sin(angle) * skew_axis + (1 - jnp.cos(angle)) * jnp.linalg.matrix_power(skew_axis, 2)\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  if theta < eps:\n    return jnp.eye(3) + skew_symmetric(axis_angle)\n  else:\n    axis = axis_angle / theta\n    s = jnp.sin(theta)\n    c = jnp.cos(theta)\n    return (\n        c * jnp.eye(3)\n        + (1 - c) * jnp.outer(axis, axis)\n        + s * skew_symmetric(axis)\n    )\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the angle of rotation from the axis-angle representation\n  angle = jnp.linalg.norm(axis_angle)\n\n  # Check if the angle of rotation is small\n  if angle < eps:\n    # If the angle of rotation is small, use the first-order Taylor expansion of the exponential map\n    skew_axis_angle = jnp.array([\n        [0, -axis_angle[2], axis_angle[1]],\n        [axis_angle[2], 0, -axis_angle[0]],\n        [-axis_angle[1], axis_angle[0], 0]\n    ])\n    return jnp.eye(3) + skew_axis_angle\n\n  # Compute the axis of rotation\n  axis = axis_angle / angle\n\n  # Compute the cross product matrix of the axis\n  axis_cross = jnp.array([\n      [0, -axis[2], axis[1]],\n      [axis[2], 0, -axis[0]],\n      [-axis[1], axis[0], 0]\n  ])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(angle) * axis_cross + (1 - jnp.cos(angle)) * axis_cross @ axis_cross\n\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the norm of the axis-angle vector\n  theta = jnp.linalg.norm(axis_angle)\n\n  # If the norm is zero, return the identity matrix\n  if theta < eps:\n    return jnp.eye(3)\n\n  # Compute the normalized axis-angle vector\n  axis = axis_angle / theta\n\n  # Compute the skew-symmetric matrix of the normalized axis\n  axis_skew = jnp.array([[0, -axis[2], axis[1]],\n                         [axis[2], 0, -axis[0]],\n                         [-axis[1], axis[0], 0]])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(theta) * axis_skew + (1 - jnp.cos(theta)) * axis_skew @ axis_skew\n\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the angle of rotation from the axis-angle representation\n  theta = jnp.linalg.norm(axis_angle)\n\n  # Check if the angle is small enough to use a Taylor series approximation\n  if theta < eps:\n    # If the angle is small, use a Taylor series approximation\n    skew_axis_angle = skew_symmetric(axis_angle)\n    return jnp.eye(3) + skew_axis_angle + 0.5 * jnp.linalg.matrix_power(skew_axis_angle, 2)\n  else:\n    # If the angle is not small, use Rodrigues' formula\n    axis = axis_angle / theta\n    skew_axis = skew_symmetric(axis)\n    return jnp.eye(3) + jnp.sin(theta) * skew_axis + (1 - jnp.cos(theta)) * jnp.linalg.matrix_power(skew_axis, 2)\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the angle of rotation\n  angle = jnp.linalg.norm(axis_angle)\n\n  # If the angle is very small, use a Taylor expansion to avoid numerical instability\n  if angle < eps:\n    skew_axis_angle = hat_so3(axis_angle)\n    return jnp.eye(3) + skew_axis_angle + 0.5 * jnp.dot(skew_axis_angle, skew_axis_angle)\n\n  # Otherwise, use Rodrigues' formula\n  axis_angle_normalized = axis_angle / angle\n  skew_axis_angle_normalized = hat_so3(axis_angle_normalized)\n  return jnp.eye(3) + jnp.sin(angle) * skew_axis_angle_normalized + (1 - jnp.cos(angle)) * jnp.dot(skew_axis_angle_normalized, skew_axis_angle_normalized)\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  u = axis_angle\n  angle = jnp.linalg.norm(u)\n  skew_u = jnp.array(\n    [\n      [0, -u[2], u[1]],\n      [u[2], 0, -u[0]],\n      [-u[1], u[0], 0],\n    ]\n  )\n  R = (\n    jnp.eye(3)\n    + jnp.sin(angle) / (angle + eps) * skew_u\n    + (1 - jnp.cos(angle)) / (angle**2 + eps) * (skew_u @ skew_u)\n  )\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the norm of the axis-angle representation.\n  angle = jnp.linalg.norm(axis_angle)\n\n  # If the angle is very small, use a first-order Taylor approximation.\n  if angle < eps:\n    skew_axis_angle = cross_mat(axis_angle)\n    return jnp.eye(3) + skew_axis_angle + 0.5 * jnp.linalg.matrix_power(skew_axis_angle, 2)\n\n  # Otherwise, use Rodrigues' formula.\n  axis = axis_angle / angle\n  skew_axis = cross_mat(axis)\n  return jnp.eye(3) + jnp.sin(angle) * skew_axis + (1 - jnp.cos(angle)) * jnp.linalg.matrix_power(skew_axis, 2)\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the axis and angle from the input axis-angle representation\n  axis, angle = axis_angle[:3], axis_angle[3]\n\n  # Compute the skew-symmetric matrix of the axis\n  axis_skew = np.array([\n    [0, -axis[2], axis[1]],\n    [axis[2], 0, -axis[0]],\n    [-axis[1], axis[0], 0]\n  ])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = np.eye(3) + np.sin(angle) * axis_skew + (1 - np.cos(angle)) * np.dot(axis_skew, axis_skew)\n\n  # If the angle is very small, use the Taylor series approximation\n  if np.abs(angle) < eps:\n    R = np.eye(3) + axis_skew + np.dot(axis_skew, axis_skew) * (1 - angle**2 / 6)\n\n  return R\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the magnitude of rotation and the axis of rotation from the input axis-angle representation\n  theta = jnp.linalg.norm(axis_angle)\n  u = axis_angle / (theta + eps)\n\n  # Compute the skew-symmetric matrix of the axis of rotation\n  u_skew = jnp.array([[0, -u[2], u[1]],\n                      [u[2], 0, -u[0]],\n                      [-u[1], u[0], 0]])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(theta) * u_skew + (1 - jnp.cos(theta)) * u_skew @ u_skew\n\n  return R\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the norm of the axis-angle representation.\n  angle = jnp.linalg.norm(axis_angle)\n\n  # If the norm is smaller than the numerical stability threshold, return the identity matrix.\n  if angle < eps:\n    return jnp.eye(3)\n\n  # Compute the skew-symmetric matrix of the axis-angle representation.\n  axis = axis_angle / angle\n  skew_axis = jnp.array([\n      [0, -axis[2], axis[1]],\n      [axis[2], 0, -axis[0]],\n      [-axis[1], axis[0], 0]\n  ])\n\n  # Compute the rotation matrix using Rodrigues' formula.\n  rotation_matrix = jnp.eye(3) + jnp.sin(angle) * skew_axis + (1 - jnp.cos(angle)) * jnp.dot(skew_axis, skew_axis)\n\n  return rotation_matrix\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the components of the axis-angle representation\n  phi = jnp.linalg.norm(axis_angle)\n  n = axis_angle / (phi + eps)\n\n  # Compute the cross-product matrix of the axis vector\n  nx = jnp.array([\n      [0, -n[2], n[1]],\n      [n[2], 0, -n[0]],\n      [-n[1], n[0], 0]\n  ])\n\n  # Compute the exponential map using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(phi) * nx + (1 - jnp.cos(phi)) * nx @ nx\n\n  return R\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the norm of the axis-angle representation\n  theta = jnp.linalg.norm(axis_angle)\n\n  # If the norm of the axis-angle representation is less than the numerical stability threshold, return the identity matrix\n  if theta < eps:\n    return jnp.eye(3)\n\n  # Compute the skew-symmetric matrix corresponding to the axis-angle representation\n  K = skew_symmetric(axis_angle)\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(theta) * K + (1 - jnp.cos(theta)) * K @ K\n\n  # Return the rotation matrix\n  return R\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the magnitude of the rotation (angle) from the axis-angle representation\n  theta = jnp.linalg.norm(axis_angle)\n\n  # Check if the angle is small enough to use the approximation\n  if theta < eps:\n    # If the angle is small, use the approximation\n    skew_axis_angle = skew_so3(axis_angle)\n    return jnp.eye(3) + skew_axis_angle\n  else:\n    # If the angle is large, use the full formula\n    axis_angle_hat = skew_so3(axis_angle)\n    return (\n        jnp.eye(3)\n        + jnp.sin(theta) / theta * axis_angle_hat\n        + (1 - jnp.cos(theta)) / (theta**2) * (axis_angle_hat @ axis_angle_hat)\n    )\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  mu = ((t0 + 0.5 * (t1 - t0)) + mu) * d\n  h = t1 - t0\n  t_mean = t0 + 0.5 * h\n  r_mean = t_mean * base_radius\n  return conical_frustum_to_gaussian_shared(d, t0, t1, base_radius, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean of the Gaussian distribution\n  mu = ((t0 + 0.5) + t1) / (t1 - t0) * d\n\n  # Calculate the variance of the Gaussian distribution\n  h = t1 - t0\n  t_mean = (t0 + t1) / 2\n  r_mean = t_mean * base_radius\n  r_variance = (1 / 3) * h ** 2 * base_radius ** 2\n\n  # Calculate the covariance matrix of the Gaussian distribution\n  if diag:\n    D = jnp.eye(3) * (r_variance / r_mean ** 2 + h ** 2 / 12)\n  else:\n    D = jnp.eye(3) * r_variance\n\n  return mu, D"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean of the Gaussian\n  mu = ((t0 + 4 * t1) / 6) * d\n\n  # Calculate the covariance matrix of the Gaussian\n  h = t1 - t0\n  t2 = t0 * t0\n  t3 = h * t0\n  t4 = t0 * t2\n  t5 = t0 * h\n  t6 = t2 * h\n  t7 = t3 * h\n  t8 = t2 * t2\n  t9 = t3 * t3\n  t10 = t4 * h\n  t11 = t5 * h\n  t12 = t4 * t4\n  t13 = t5 * t5\n  t14 = t6 * h\n  t15 = t7 * h\n  t16 = t6 * t6\n  t17 = t7 * t7\n  t18 = t8 * h\n  t19 = t9 * h\n  t20 = t8 * t8\n  t21 = t9 * t9\n  t22 = t10 * h\n  t23 = t11 * h\n  t24 = t10 * t10\n  t25 = t11 * t11\n  t26 = t12 * h\n  t27 = t13 * h\n  t28 = t12 * t12\n  t29 = t13 * t13\n  t30 = t14 * h\n  t31 = t15 * h\n  t32 = t14 * t14\n  t33 = t15 * t15\n  t34 = t16 * h\n  t35 = t17 * h\n  t36 = t16 * t16\n  t37 = t17 * t17\n  t38 = t18 * h\n  t39 = t19 * h\n  t40 = t18 * t18\n  t41 = t19 * t19\n  t42 = t"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  mu = ((t0 + 0.5) + t1) * 0.5\n  hw = (t1 - t0) * 0.5\n  t_mean = mu + (2.0 * mu * hw**2.0) / (3.0 * mu**2.0 + hw**2.0)\n  t_var = (hw**2.0) / 3.0 - (4.0 / 15.0) * ((hw**4.0) / (mu**2.0 + hw**2.0)) + (\n      1.0 / 15.0) * ((hw**6.0) / (mu**2.0 + 2.0 * hw**2.0))\n  r_var = base_radius**2.0 * ((mu**2.0) / 4.0 + (5.0 / 12.0) * hw**2.0 -\n                             4.0 / 15.0 * (hw**4.0) / (mu**2.0 + hw**2.0) +\n                             (1.0 / 15.0) * (hw**6.0) / (mu**2.0 + 2.0 * hw**2.0))\n  r_mean = jnp.sqrt(r_var + base_radius**2.0)\n  return (d * t_mean,\n          jnp.diag(d * t_var) + jnp.outer(d * r_var, d * r_var)) if diag else (\n              d * t_mean,\n              jnp.diag(d * t_var) + jnp.outer(d, d) * r_var)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean of the Gaussian distribution\n  mu = ((t0 + 4/3 * t1) * d + (2/3 * t0 + 1/3 * t1) * jnp.array([0, 0, 0])) / (t0**0.5 + 2**0.5 * t1**0.5)\n\n  # Calculate the covariance matrix of the Gaussian distribution\n  h = t1 - t0\n  t_mean = (t0 * t1)**0.5\n  r_mean = (t0**0.5 * t1**0.5) / (t0**0.5 + t1**0.5)\n  sigma = (1/3) * (t_mean)**2 * jnp.eye(3) + r_mean**2 * jnp.outer(mu, mu)\n\n  if diag:\n    sigma = jnp.diag(jnp.diag(sigma))\n\n  return mu, sigma"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean of the Gaussian distribution\n  mu = ((t0 + 4/3*t1) + (t1 - t0)/3) * d\n\n  # Calculate the covariance matrix of the Gaussian distribution\n  h = t1 - t0\n  t = (t0 * t1)**0.5\n  r = base_radius * (t ** 0.5)\n  r_diff = r - base_radius * t0 ** 0.5\n  r_avg = (base_radius * (t0 ** 0.5) + r) / 2\n  cov_xx = t * d[2] * d[2] + h * r_diff * r_diff / 3 + d[0] * d[0] * r_avg * r_avg\n  cov_yy = t * d[2] * d[2] + h * r_diff * r_diff / 3 + d[1] * d[1] * r_avg * r_avg\n  cov_zz = t * d[2] * d[2] + h * r_diff * r_diff / 3 + d[2] * d[2] * r_avg * r_avg\n  cov_xy = d[0] * d[1] * r_avg * r_avg\n  cov_xz = d[0] * d[2] * r_avg * r_avg\n  cov_yz = d[1] * d[2] * r_avg * r_avg\n\n  # Create the covariance matrix\n  cov_diag = jnp.array([cov_xx, cov_yy, cov_zz])\n  cov_off_diag = jnp.array([cov_xy, cov_xz, cov_yz])\n  cov_diag = cov_diag[:, jnp.newaxis]\n  cov_off_diag = cov_off_diag[:, jnp.newaxis]\n  cov_diag = jnp.tile(cov_diag, (1, 3))\n  cov_off_diag ="}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean of the Gaussian distribution\n  mu = ((t0 + 0.5) + t1) * d / (t0 + t1)\n\n  # Calculate the variance of the radius of the Gaussian distribution\n  h = t1 - t0\n  t_mean = (t0 + t1) / 2\n  r_var = base_radius**2 * ((h**2 / 12) + (mu.dot(mu) - t_mean**2))\n\n  # Calculate the covariance matrix of the Gaussian distribution\n  if diag:\n    cov = jnp.diag(jnp.array([r_var, r_var, r_var * 1 / (1 + d[2]**2)]) + jnp.array([1e-10, 1e-10, 1e-10]))\n  else:\n    cov = jnp.array([[r_var, 0, 0], [0, r_var, 0], [0, 0, r_var * 1 / (1 + d[2]**2)]])\n\n  return mu, cov\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  mu = ((t0 + 0.5 * (t1 - t0)) + mu) * d\n  h = t1 - t0\n  t_mean = mu + (0.5 * h) * d\n  radius2 = fn.square(t_mean)\n  radius2 += jnp.dot(d[None, :], d[None, :]) * ((radius2 - fn.square(mu)) / jnp.dot(d, d))\n  sigma = radius2 * base_radius ** 2\n  sigma /= 3\n\n  epsilon = 1e-10\n  sigma = jnp.where(sigma >= epsilon, sigma, epsilon)\n\n  if diag:\n    return t_mean, jnp.diag(sigma)\n\n  l = jnp.sqrt(sigma)\n  u = jnp.linalg.qr(jnp.eye(3) - 2.0 * jnp.outer(d, d))[0] * l[..., None]\n  sigma = u @ u.T\n\n  return t_mean, sigma"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  import jax.numpy as jnp\n  from jax import vmap\n  from jax.numpy import linalg as jnpl\n\n  # Define the function to calculate the radius of the cone at a given distance\n  def radius_of_cone(t, base_radius):\n    return jnp.sqrt(1 + t ** 2) * base_radius\n\n  # Define the function to calculate the mean of the Gaussian distribution\n  def mean_fn(t):\n    return d * radius_of_cone(t, base_radius) / len(d)\n\n  # Define the function to calculate the covariance of the Gaussian distribution\n  def cov_fn(t):\n    outer_product = jnp.outer(d, d)\n    scaled_outer_product = outer_product * (t ** 2 / len(d))\n    return scaled_outer_product\n\n  # Calculate the mean using vmap\n  mean = vmap(mean_fn)((t0 + t1) / 2)\n\n  # Calculate the covariance using vmap\n  cov = vmap(cov_fn)((t1 - t0) / 2)\n\n  # Set the covariance to be diagonal or full based on the diag argument\n  cov = jnp.diag(jnp.diag(cov)) if diag else cov\n\n  return mean, cov\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  mu = ((t0 + 0.5) + t1) / (t1 - t0) * d\n  h = t1 - t0\n  t_mean = (t0 + t1) / 2.0\n  r_mean = t_mean * base_radius\n  mean = mu * r_mean / jnp.linalg.norm(mu)\n\n  # Cone radius as a function of distance.\n  sigma_r = (t1 * base_radius**2 - t0 * base_radius**2) / (t1 - t0)\n\n  # Covariance matrix computation.\n  magnitude = jnp.linalg.norm(d)\n  d = d / magnitude\n  d_cross_outer_d = jnp.outer(d, d)\n  id_min_d_cross = jnp.eye(3) - d_cross_outer_d\n  first_moment = jnp.matmul(id_min_d_cross, jnp.matmul(jnp.eye(3) * r_mean**2, id_min_d_cross))\n  second_moment = jnp.matmul(id_min_d_cross,\n                             jnp.matmul(jnp.eye(3) * sigma_r**2, id_min_d_cross))\n  sigma = first_moment + second_moment\n\n  # Set the scale such that h = (t1 - t0) when d, t0, t1 = 0, 0, 1.\n  sigma = sigma / (h**2)\n\n  if diag:\n    sigma = jnp.diag(jnp.diag(sigma))\n\n  return mean, sigma\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  import jax.numpy as jnp\n  from jax import vmap\n\n  # Define the function to calculate the radius of the cone at a given distance from the origin\n  def radius_fn(t):\n    return base_radius * t\n\n  # Calculate the mean of the Gaussian distribution\n  mean = (t0 + 0.5 * (t1 - t0)) * d\n\n  # Calculate the covariance of the Gaussian distribution\n  t_mid = 0.5 * (t0 + t1)\n  mean_a = mean + (t_mid - t1) * d\n  mean_b = mean - (t_mid - t0) * d\n\n  t_half = 0.5 * (t0 + t1)\n  d_half = d * (t_half / jnp.linalg.norm(d))\n\n  outer_a0 = jnp.outer(mean_a, d_half - mean)\n  outer_b0 = jnp.outer(mean_b, d_half - mean)\n  outer_a1 = jnp.outer(mean_a, d_half + mean)\n  outer_b1 = jnp.outer(mean_b, d_half + mean)\n\n  scale = 0.25 * (t1 - t0)\n  cov_diag = scale * (jnp.sum((d_half - mean)**2) + jnp.sum((d_half + mean)**2))\n\n  cov_a00 = jnp.dot(outer_a0, outer_a0)\n  cov_a01 = jnp.dot(outer_a0, outer_b0)\n  cov_a11 = jnp.dot(outer_a1, outer_a1)\n\n  cov_b00 = jnp.dot(outer_b0, outer_b0)\n  cov_b01 = jnp.dot(outer_b0, outer_b1)\n  cov_b11 = jnp.dot(outer_b1, outer_b1)\n\n  if diag:\n    cov_a00 *= cov"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  mu = ((t0 + 0.5 * (t1 - t0)) * d)\n  h = t1 - t0\n  t_mean = mu + (0.5 * h) * d\n  radius2 = base_radius**2 + mu @ d * ((t1 - t0) * 0.5)\n  d_mag_sq = jnp.dot(d, d)\n  if diag:\n    d_mag = jnp.sqrt(d_mag_sq)\n    D = jnp.sqrt(radius2 / d_mag)\n    cov_diag = d_mag_sq / (4.0 * h) * jnp.array([D, D, D])\n    return t_mean, jnp.diag(cov_diag)\n  else:\n    d_outer_prod = jnp.outer(d, d)\n    cov_full = jnp.eye(3, 3) * 0.5 * d_outer_prod + \\\n      jnp.eye(3, 3) * (t1 - t0) - d_outer_prod / (3.0 * h)\n    cov_full *= radius2 / (t1 - t0)\n    return t_mean, cov_full"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean of the Gaussian distribution\n  mu = ((t0 + 0.5) + t1) / (t0 + t1) * d\n\n  # Calculate the covariance matrix of the Gaussian distribution\n  h = t1 - t0\n  t0t0 = (t0 * t0)\n  t1t1 = (t1 * t1)\n  c0 = t1t1 * (3 * t0t0 + 2 * h * t0 + h * h)\n  c1 = t0t0 * (3 * t1t1 + 2 * h * t1 + h * h)\n  c01 = (t0t0 * (t0t0 + 3 * h * t0 + 3 * h * h) + t1t1 * (t1t1 + 3 * h * t1 + 3 * h * h))\n\n  det = c0 * c1 - c01 * c01\n  invdet = 1 / det\n  c0invdet = c1 * invdet\n  c1invdet = -c01 * invdet\n\n  def cov_func(r):\n    c0_ = c0invdet * r\n    c1_ = c1invdet * r\n    a = c0_ * c0_ + c1_ * c1_ + 1\n    b = c0_ * c1_\n    return base_radius * base_radius / a * jnp.array([[a, b, b], [b, a, b], [b, b, a]])\n\n  cov = cov_func(jnp.linalg.norm(mu))\n\n  if diag:\n    scale = 1 / jnp.sqrt(jnp.max(jnp.diag(cov)))\n    cov = cov * scale\n\n  return mu, cov\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean of the Gaussian distribution\n  mu = ((t0 + 0.5) * d + (t1 + 0.5) * d) / (t0 + t1)\n\n  # Calculate the height of the conical frustum\n  h = t1 - t0\n\n  # Calculate the time-scaled radius at the top and bottom of the frustum\n  r1 = base_radius * (t0 + h * 0.5)\n  r0 = base_radius * (t0 + 0.5)\n\n  # Calculate the slopes of the conical frustum\n  s1 = r1 / h\n  s0 = r0 / h\n\n  # Calculate the time-scaled mean of the slopes\n  mu_s = ((t1 * s1 - t0 * s0) / (t1 - t0))\n\n  # Calculate the time-scaled mean of the squared slopes\n  mu_ss = ((t0 ** 2 * (s1 - mu_s) ** 2 + t1 ** 2 * (s0 - mu_s) ** 2) / (t0 + t1))\n\n  # Calculate the covariance matrix of the Gaussian distribution\n  if diag:\n    cov = jnp.diag(jnp.array([mu_ss, mu_ss, mu_ss, mu_s ** 2, mu_s ** 2, mu_s ** 2]) * h / 3)\n  else:\n    cov = jnp.array([\n      [mu_ss, 0, 0, 0, 0, 0],\n      [0, mu_ss, 0, 0, 0, 0],\n      [0, 0, mu_ss, 0, 0, 0],\n      [0, 0, 0, mu_s ** 2, 0, 0],\n      [0, 0, 0, 0, mu_s ** 2, 0],\n      [0, 0, 0, 0, 0, mu_s ** 2]\n    ])"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean of the Gaussian distribution\n  mu = ((t0 + 0.5) + t1) / (t1 - t0) * d\n\n  # Calculate the covariance of the Gaussian distribution\n  h = t1 - t0\n  t = (t0 + t1) / (t1 - t0)\n  r2 = base_radius * base_radius\n  r_mean = (t0 * t0 + 3.0 * t0 * t1 + t1 * t1) / (3.0 * h)\n  sigma2 = r2 * (t1 - t0) / h - r_mean * r_mean\n  n = mu.dot(mu)\n  normalization = 1.0 / (2.0 * jnp.pi * sigma2) ** 1.5\n\n  cov_factor = sigma2 / r2 * (4.0 / 15.0 - (t - r_mean) / r2 * (7.0 / 5.0 - (t - r_mean) / r2))\n  cov_factor = jnp.where(r_mean < 0.0, cov_factor + r_mean / r2, cov_factor)\n\n  # If diag is True, return the diagonal covariance matrix\n  if diag:\n    return mu, cov_factor * jnp.eye(3) / normalization\n\n  # If diag is False, return the full covariance matrix\n  else:\n    d2 = n - mu.dot(mu)\n    cov_factor = cov_factor / d2 * (n * n - mu.dot(mu))\n    cov_factor = cov_factor.clip(1e-6)\n    cov_factor = cov_factor.at[2, 2].set(cov_factor[2, 2] + 2.0 / 15.0 * d2)\n    return mu, cov_factor / normalization"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean of the Gaussian distribution\n  mu = ((t0 + 0.5) + (t1 + 0.5)) * 0.5 * d\n\n  # Calculate the covariance matrix of the Gaussian distribution\n  h = (t1 - t0) * 0.5\n  t_mean = (t0 + t1) * 0.5\n  r_mean = t_mean * base_radius\n  r_var = (t0 ** 2 + t0 * t1 + t1 ** 2) / 3 * base_radius ** 2\n  h_var = h ** 2 / 12 * base_radius ** 2 - r_var\n  r_diag = jnp.diag(jnp.array([1, 1, r_var / r_mean ** 2]))\n  h_diag = jnp.diag(jnp.array([h_var / h ** 2, h_var / h ** 2, 1 / h ** 2]))\n  h_outer_diag = jnp.diag(jnp.array([1 / h ** 2, 1 / h ** 2, h_var / h ** 2]))\n  cov_diag = r_diag @ h_diag @ r_diag\n  cov_outer_diag = r_diag @ h_outer_diag\n  cov_diag = cov_diag + cov_diag.T - jnp.diag(jnp.diag(cov_diag))\n  cov_outer_diag = cov_outer_diag + cov_outer_diag.T - jnp.diag(jnp.diag(cov_outer_diag))\n  cov = cov_diag if diag else cov_outer_diag\n\n  return mu, cov\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  mu = ((t0 + 0.5) + (t1 - 0.5)) * d\n  h = t1 - t0\n  t_mean = mu + (0.5 * h)\n  r_mean = t_mean * base_radius\n  radius2 = r_mean ** 2\n  radius3 = r_mean * radius2\n  radius4 = r_mean ** 3\n  term_1 = (h ** 2) * (base_radius ** 2) * (1.0 / 3.0)\n  term_2 = -((mu ** 2) * (base_radius ** 2)) * (1.0 / 3.0)\n  approx_cov_diag = term_1 + term_2\n  approx_cov_diag = jnp.maximum(approx_cov_diag, 1e-10)\n  if diag:\n    return t_mean, jnp.stack([approx_cov_diag] * 3, axis=-1)\n  else:\n    d_mag_sq = jnp.maximum(jnp.dot(d, d), 1e-10)\n    d_outer_diag = jnp.stack([d ** 2] * 3, axis=-1)\n    eye = jnp.eye(3)\n    term_3 = -((radius3 * (4.0 / 5.0)) * ((1.0 / 3.0) * (d_outer_diag) - ((1.0 / 5.0) * (d_outer_diag - (d_mag_sq * eye)))))\n    term_4 = radius3 * ((-1.0 / 5.0) * (d_outer_diag - (d_mag_sq * eye)))\n    approx_cov_non_diag = term_3 + term_4\n    approx_cov_non_diag = jnp.maximum(approx_cov_non_diag, 1e-10)\n    return t_mean, approx_cov_diag + approx_cov_non_diag\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean of the Gaussian distribution\n  mu = ((t0 + 0.5) + (t1 + 0.5)) * d\n\n  # Calculate the magnitude of the mean vector\n  # This is used to determine the scale of the covariance matrix\n  # It is the distance from the origin to the mean of the Gaussian distribution\n  r_mag = jnp.sqrt(jnp.sum(mu ** 2))\n\n  # Calculate the radius of the frustum at the starting and ending distances\n  # This is used to determine the scale of the covariance matrix\n  # It is the distance from the origin to the surface of the frustum\n  r0 = base_radius * t0\n  r1 = base_radius * t1\n\n  # Calculate the slopes of the linear functions that describe the radius as a function of distance\n  # These slopes are used to determine the scale of the covariance matrix\n  # They represent the rate of change of the radius with respect to distance\n  dr0 = base_radius\n  dr1 = (r1 - r0) / (t1 - t0 + 1e-10)\n\n  # Calculate the variances of the Gaussian distribution along each axis (x, y, z)\n  # These variances represent the spread of the distribution along each axis\n  # They are calculated using the slopes of the linear functions that describe the radius as a function of distance\n  var_x = jnp.square(r0) + 0.5 * (r1 ** 2 - r0 ** 2) + (t0 ** 2 * dr0 ** 2 + t0 * dr0 * dr1 + t0 * dr1 ** 2) / 3.0 + t0 * (t1 - t0) * (dr1 ** 2) / 3.0\n  var_y = var_x\n  var_z = var_x\n\n  # Calculate the covariance matrix of the Gaussian distribution\n  # This matrix represents the relationship between the different axes of the distribution\n  # It is symmetric and positive-definite\n  # It is calculated using the variances calculated above\n  cov_xx = var_x\n  cov_yy = var"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean of the Gaussian distribution\n  mu = ((t0 + 0.5) + (t1 - 0.5)) * d\n\n  # Calculate the variance of the Gaussian distribution\n  h = t1 - t0\n  t_mean = (t0 + t1) / 2.0\n  r_mean = t_mean * base_radius\n  r_variance = (t0**2 + t0 * t1 + t1**2) / 3.0 * base_radius**2 - r_mean**2\n\n  # Calculate the covariance matrix of the Gaussian distribution\n  if diag:\n    D = jnp.eye(3) * r_variance\n  else:\n    d_mag_sq = jnp.dot(d, d) + 1e-10\n    d = d / jnp.sqrt(d_mag_sq)\n    d_cross_mat = jnp.array([\n      [    0, -d[2],  d[1] ],\n      [ d[2],     0, -d[0] ],\n      [-d[1],  d[0],     0 ],\n    ])\n    null_proj_mat = jnp.eye(3) - d[:, None] * d[None, :]\n    D = r_variance / d_mag_sq * (\n      h**2 / 4 * d_cross_mat @ d_cross_mat.T + \n      (h**2 / 3 - h**2 / 4) * null_proj_mat\n    )\n\n  return mu, D\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  import jax.numpy as jnp\n  from jax import lax\n\n  # Calculate the mean of the Gaussian distribution\n  mu = (t0 + 0.5 * (t1 - t0)) * d\n\n  # Calculate the covariance of the Gaussian distribution\n  h = t1 - t0\n  t = (t0 * t1) / (t0 + t1)\n  r = base_radius(t)\n  I = jnp.eye(3)\n  R = I * r\n  R_perp = I - jnp.outer(d, d)\n  cov_diag = (h / 12) * jnp.dot(R_perp, R_perp) + jnp.dot(R, R)\n\n  # If diag is True, return the diagonal covariance matrix\n  if diag:\n    return mu, jnp.diag(cov_diag)\n\n  # If diag is False, calculate the full covariance matrix\n  d_norm = d / jnp.linalg.norm(d)\n  d_norm = jnp.expand_dims(d_norm, -2)\n  d_norm_transpose = jnp.transpose(d_norm, (0, 2, 1))\n  outer_product = jnp.matmul(d_norm, d_norm_transpose)\n  eye = jnp.eye(3)\n  cov_non_diag = cov_diag - jnp.matmul(jnp.matmul(R_perp, eye - outer_product), R_perp)\n\n  # Return the mean and the full covariance matrix\n  return mu, cov_non_diag\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2.\n  r_var = radius ** 2 / 4.\n  t_var = (t1 - t0) ** 2 / 12.\n  mean = jnp.array([t_mean, 0., 0.])\n  if diag:\n    d = d / jnp.linalg.norm(d)\n    x, y, z = d\n    cov_diag = jnp.array([t_var,\n                          r_var * z ** 2,\n                          r_var * y ** 2,\n                          r_var * y ** 2,\n                          r_var * z ** 2,\n                          r_var * x ** 2])\n    return mean, cov_diag\n  else:\n    dxyz = jnp.eye(3) * radius / jnp.sqrt(3)\n    dxyz = dxyz @ rotation_matrix(d, jnp.eye(3))\n    x, y, z = dxyz\n    cov_full = jnp.array([[t_var, 0., 0., 0., 0., 0.],\n                          [0., r_var * z ** 2, 0., 0., 0., 0.],\n                          [0., 0., r_var * y ** 2, 0., 0., 0.],\n                          [0., 0., 0., r_var * y ** 2, 0., 0.],\n                          [0., 0., 0., 0., r_var * z ** 2, 0.],\n                          [0., 0., 0., 0., 0., r_var * x ** 2]])\n    return mean, cov_full\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  import jax.numpy as jnp\n  from .utils import lift_gaussian\n\n  t0 = t0 * jnp.linalg.norm(d)\n  t1 = t1 * jnp.linalg.norm(d)\n  mean = (t0 + t1) * 0.5\n  # timestamps = jnp.linspace(t0, t1, 100)\n  # samples = mean + jnp.sin(timestamps[..., None] * jnp.pi) * radius\n  # mean = jnp.mean(samples, axis=0)\n  # cov_samples = jnp.cov(samples, rowvar=False)\n  h = t1 - t0\n  t_mean = mean\n  t_var = (radius ** 2) / 4 * (h ** 2) / 3\n  if diag:\n    d_var = jnp.zeros_like(d)\n  else:\n    d_var = jnp.zeros(3)\n  return lift_gaussian(d, t_mean, t_var, d_var)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  import jax.numpy as jnp\n  from .utils import lift_gaussian\n\n  mean = (t0 + t1) / 2.0\n  diff = t1 - t0\n  var = jnp.square(radius) / 4.0 * diff\n\n  d = jnp.array(d)\n\n  if diag:\n    d = d / jnp.linalg.norm(d)\n    cov_diag = jnp.array([var, var, var])\n    cov_diag = cov_diag * jnp.abs(d)\n  else:\n    cov_diagonal = jnp.array([var, var, var])\n    cov_off_diag = jnp.array([0.0, 0.0, 0.0])\n    cov_diag = jnp.concatenate([cov_diagonal, cov_off_diag], axis=0)\n    cov_diag = cov_diag * jnp.abs(d)\n\n  return lift_gaussian(d, jnp.array([mean]), cov_diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  import jax.numpy as jnp\n  from .utils import lift_gaussian\n\n  # Calculate the mean of the Gaussian\n  mean = (t0 + t1) / 2.0\n\n  # Calculate the variance of the Gaussian\n  variance = (t1 - t0) ** 2 / 12.0\n\n  # Calculate the covariance matrix of the Gaussian\n  cov_diag = variance * jnp.eye(3)\n  cov_axis = variance * jnp.outer(d, d)\n  cov = cov_diag + cov_axis\n\n  # If diag is True, set the covariance matrix to the diagonal matrix\n  if diag:\n    cov = jnp.diag(jnp.diag(cov))\n\n  # Lift the Gaussian distribution to 4D\n  mean, cov = lift_gaussian(mean, cov, 1)\n\n  # Return the mean and covariance of the Gaussian\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  import jax.numpy as jnp\n\n  t_mean = (t0 + t1) / 2.0\n  r_mean = (t1 - t0) / 2.0\n  mean = jnp.concatenate([t_mean[None] * d, r_mean[None] * jnp.array([0.0, 0.0, 0.0])], axis=0)\n\n  l = jnp.linalg.norm(d)\n  t_var = (r_mean ** 2) / 3.0\n  r_var = (l ** 2) / 4.0\n  if diag:\n    d_var = jnp.array([l ** 2 / 12.0, radius ** 2 / 12.0, radius ** 2 / 12.0])\n    cov_diag = d_var[None, :] * jnp.eye(3, 3)\n    cov = cov_diag\n  else:\n    cov = jnp.eye(6, 6)\n    cov = cov.at[0, 0].set(t_var)\n    cov = cov.at[1, 1].set(r_var)\n    cov = cov.at[2, 2].set(r_var)\n    cov = cov.at[3, 3].set(d_var[0])\n    cov = cov.at[4, 4].set(d_var[1])\n    cov = cov.at[5, 5].set(d_var[2])\n    cov = cov.at[0, 3].set(l * radius * t_mean / (l ** 2 + radius ** 2))\n    cov = cov.at[0, 4].set(-radius * t_mean / (l ** 2 + radius ** 2))\n    cov = cov.at[1, 3].set(radius * l / (l ** 2 + radius ** 2))\n    cov = cov.at[1, 4].set(l / (l ** 2 + radius ** 2))"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  import jax.numpy as jnp\n  from jax import vmap\n\n  from .gaussian_utils import lift_gaussian\n\n  theta = jnp.arctan2(d[1], d[0])\n  phi = jnp.arctan2(jnp.sqrt(d[0] ** 2 + d[1] ** 2), d[2])\n  mu = jnp.stack([t0, theta, phi], axis=-1)\n\n  cos_theta = jnp.cos(theta)\n  sin_theta = jnp.sin(theta)\n  cos_phi = jnp.cos(phi)\n  sin_phi = jnp.sin(phi)\n\n  c1 = jnp.stack([cos_phi * cos_theta, cos_phi * sin_theta, -sin_phi], axis=-1)\n  c2 = jnp.stack([-sin_theta, cos_theta, jnp.zeros_like(theta)], axis=-1)\n  c3 = jnp.stack([sin_phi * cos_theta, sin_phi * sin_theta, cos_phi], axis=-1)\n\n  c = jnp.stack([c1, c2, c3], axis=-1)\n\n  if diag:\n    cov1 = jnp.stack([radius ** 2 / 4, (t1 - t0) ** 2 / 12, jnp.zeros_like(theta)], axis=-1)\n    cov2 = jnp.stack([(t1 - t0) ** 2 / 12, (t1 - t0) ** 2 / 36, jnp.zeros_like(theta)], axis=-1)\n    cov3 = jnp.stack([jnp.zeros_like(theta), jnp.zeros_like(theta), radius ** 2 / 4], axis=-1)\n    cov = jnp.stack([cov1, cov2, cov3], axis=-1)\n  else:\n    cov11 = radius ** 2 / 4 * (sin_"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  import jax.numpy as jnp\n  from .gaussian import lift_gaussian\n\n  mean, cov = cylinder_to_gaussian_private(d, t0, t1, radius, diag)\n\n  return lift_gaussian(mean, cov, diag)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  import jax.numpy as jnp\n  from .gaussian import Gaussian\n  from .gaussian import lift_gaussian\n\n  # Calculate the mean of the Gaussian based on the cylinder's axis, start and end distances\n  t_mean = (t0 + t1) / 2.\n  p_mean = t_mean * d\n\n  # Calculate the variance of the Gaussian based on the cylinder's radius\n  p_var = radius**2 * ((t1 - t0)**2 / 12.)\n\n  # Calculate the covariance matrix of the Gaussian based on the cylinder's axis and variance\n  axis = d / jnp.linalg.norm(d)\n  d_cov = jnp.eye(3, 3) * p_var\n  cov = d_cov - p_var * jnp.outer(axis, axis)\n\n  # Lift the Gaussian to the correct representation based on the diag flag\n  return lift_gaussian(p_mean, cov, diag)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # Calculate the mean of the Gaussian\n  mu = (t0 + t1) / 2 + d * radius**2\n\n  # Calculate the variance of the Gaussian\n  h = t1 - t0\n  t_mean = (t0 + t1) / 2\n  t_var = t_mean**2 + radius**2\n  r_var = radius**2 / 4\n  # Calculate the covariance matrix of the Gaussian\n  if diag:\n    # If the covariance matrix is diagonal, set the off-diagonal elements to zero\n    sigma = jnp.array([t_var, r_var, r_var])\n  else:\n    # If the covariance matrix is full, calculate the off-diagonal elements\n    sigma = jnp.array([\n        [t_var, 0, 0],\n        [0, r_var, 0],\n        [0, 0, r_var]\n    ])\n\n  # Call the `lift_gaussian` function to convert the mean and covariance matrix into a Gaussian distribution\n  return lift_gaussian(d, mu, sigma, diag)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  import jax.numpy as jnp\n  from .gaussian import lift_gaussian\n\n  # Calculate the mean of the Gaussian based on the cylinder's axis, start and end distances, and radius\n  mean = (t0 + t1) * 0.5 + d[0] * radius\n\n  # Calculate the variance of the Gaussian based on the cylinder's radius and the difference between t1 and t0\n  variance = (t1 - t0) / 12 * (jnp.dot(d, d) + 4 * radius * radius)\n\n  # Create a diagonal covariance matrix if diag is True, otherwise create a full-covariance matrix\n  if diag:\n    d_cov = jnp.diag(jnp.maximum(variance, 1e-10))\n  else:\n    d_cov = jnp.diag(jnp.maximum(variance, 1e-10))\n    d_cov = jnp.tile(d_cov[None, :], (d_cov.shape[0], 1, 1))\n\n  # Lift the Gaussian distribution to a higher dimension using the `lift_gaussian` function\n  return lift_gaussian(mean, d_cov, 1)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  import jax.numpy as jnp\n  from jax import vmap\n  from .utils import lift_gaussian\n\n  t_mean = (t0 + t1) / 2.\n  r_var = radius ** 2 / 4.\n  t_var = (t1 - t0) ** 2 / 12.\n  mean = jnp.array([t_mean, r_mean, 0.])\n  diag = jnp.array([t_var, r_var, r_var])\n  if diag:\n    d = jnp.diag(diag)\n  else:\n    d = jnp.diag(jnp.array([1., 1., 0.]))\n  phi = jnp.eye(3)\n  phi = jnp.array([d, jnp.zeros((3, 3))])\n  phi = jnp.array([phi, jnp.zeros((3, 6))])\n  phi = vmap(jnp.concatenate)(phi, axis=1)\n  phi = jnp.array([phi, jnp.zeros((3, 9))])\n  phi = vmap(jnp.concatenate)(phi, axis=1)\n  phi = jnp.array([[phi, jnp.zeros((3, 9))], [jnp.zeros((3, 9)), phi]])\n  phi = vmap(jnp.concatenate)(phi, axis=0)\n  xi = jnp.concatenate([jnp.eye(3), jnp.zeros((3, 6))], 1)\n  mu = jnp.dot(xi, mean)\n  sigma = jnp.dot(xi, jnp.dot(phi, xi.T))\n  return lift_gaussian(mu, sigma, diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  import jax.numpy as jnp\n  from .gaussian import lift_gaussian\n\n  t0 = jnp.array(t0)\n  t1 = jnp.array(t1)\n\n  mu = (t0 + t1) / 2\n  hw = (t1 - t0) / 2\n  t_mean = mu + d * hw * jnp.linalg.norm(d)\n  t_var = (hw ** 2) / 3\n\n  if diag:\n    d_var = jnp.array([0, 0, t_var])\n    d_mean = jnp.array([0, 0, t_mean])\n  else:\n    d_var = jnp.array(\n      [\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, t_var],\n      ]\n    )\n    d_mean = jnp.array([0, 0, t_mean])\n\n  xyz_mean = jnp.array([0, 0, 0])\n  xyz_var = jnp.array(\n    [\n      [radius ** 2, 0, 0],\n      [0, radius ** 2, 0],\n      [0, 0, radius ** 2],\n    ]\n  )\n\n  return lift_gaussian(d_mean, d_var, xyz_mean, xyz_var)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  import jax.numpy as jnp\n  from .utils import lift_gaussian\n\n  mean = (t0 + t1) / 2.0\n  diff = t1 - t0\n  var = jnp.square(radius / 2.0) * (diff * diff / 12.0 + jnp.square(t0 - mean) + jnp.square(t1 - mean))\n  return lift_gaussian(d, t0, t1, mean, var, diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # Calculate the mean of the Gaussian based on the cylinder's axis, start and end distances, and radius\n  mean = (t0 + t1) * 0.5 * d\n\n  # Calculate the variance of the Gaussian based on the cylinder's start and end distances and radius\n  variance = (t1 - t0) * radius ** 2 / 12\n\n  # Calculate the covariance matrix of the Gaussian based on the variance and the cylinder's axis\n  h = t1 - t0\n  t_mean = mean / h\n  t_var = variance / (h ** 2)\n  if diag:\n    d_mag_sq = jnp.maximum(1e-10, jnp.sum(d ** 2))\n    diag = variance * jnp.array([\n        1, 1, 1, t_var, t_var, t_var, t_var * d_mag_sq, t_var * d_mag_sq,\n        t_var * d_mag_sq\n    ])\n    return lift_gaussian(d, t_mean, diag)\n  else:\n    u = d / jnp.linalg.norm(d)\n    cov_diag = jnp.array([\n        variance, variance, variance, t_var, t_var, t_var, t_var * d_mag_sq,\n        t_var * d_mag_sq, t_var * d_mag_sq\n    ])\n    outer_prod = jnp.outer(u, u) * (1 - jnp.eye(3))\n    cov_off_diag = outer_prod * t_var\n    return lift_gaussian(d, t_mean, cov_diag, cov_off_diag)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # Calculate the mean of the Gaussian\n  mean = (t0 + t1) * 0.5\n\n  # Calculate the variance of the Gaussian\n  variance = (t1 - t0) * (t1 - t0) / 12.0\n\n  # Calculate the axis-angle representation of the cylinder's orientation\n  axis = d / jnp.linalg.norm(d)\n  angle = jnp.arccos(axis[1])\n\n  # Calculate the covariance matrix of the Gaussian\n  cov_diag = jnp.array([radius * radius, radius * radius, variance])\n  cov_rot = jnp.diag(cov_diag)\n  cov = rotation_matrix(angle, axis) @ cov_rot\n\n  # Lift the Gaussian to 5D\n  return lift_gaussian(mean, cov, diag)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  import jax.numpy as jnp\n  from .utils import lift_gaussian\n\n  # Calculate the mean of the Gaussian based on the cylinder's axis, start and end distances, and radius.\n  mean = (t0 + t1) / 2.0 + d[0] * 0.5\n\n  # Calculate the variance of the Gaussian based on the cylinder's radius and the difference between t1 and t0.\n  variance = (t1 - t0) / 2.0\n  variance = jnp.square(variance) + jnp.square(radius) / 12.0\n\n  # Create a diagonal covariance matrix if diag is True, otherwise create a full-covariance matrix.\n  if diag:\n    d = jnp.sqrt(variance)\n    cov_diag = jnp.stack([d, d, jnp.linalg.norm(d * d)], axis=-1)\n    cov_diag = cov_diag * 1e-5\n  else:\n    cov_off = jnp.eye(3) * (variance / 3.0)\n    cov_diag = jnp.concatenate([d[0] * cov_off, d[1] * cov_off, d[2] * cov_off], axis=0)\n\n  # Convert the cylinder to a Gaussian distribution using the `lift_gaussian` function.\n  return lift_gaussian(mean, cov_diag, diag)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # Calculate the mean of the Gaussian by finding the midpoint of the cylinder's axis and scaling it by t0 + t1.\n  mean = (t0 + t1) * 0.5 * d\n\n  # Calculate the variance of the Gaussian by using the cylinder's radius and scaling it by t1 - t0.\n  variance = (t1 - t0) * radius * radius / 4\n\n  # Create a diagonal covariance matrix with the calculated variance along the diagonal.\n  cov_diag = variance * jnp.ones(3)\n\n  # Create a full-covariance matrix by taking the outer product of the cylinder's axis with itself and scaling it by the calculated variance.\n  cov_full = jnp.outer(d, d) * variance\n\n  # Select the appropriate covariance matrix based on the diag argument.\n  cov = jax.lax.select(diag, cov_diag, cov_full)\n\n  # Return the mean and covariance as a tuple representing the Gaussian distribution.\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # Calculate the mean of the Gaussian based on the axis, start and end distances, and radius\n  mu = (t0 + t1) * 0.5 + d * radius\n\n  # Calculate the variance of the Gaussian based on the start and end distances and the radius\n  h = t1 - t0\n  hw = jnp.maximum(h, 1e-10)\n  var = radius * radius - (radius * radius) / (3.0 * hw * hw)\n  dvar = var * d * d\n\n  # If diag is True, return a diagonal covariance matrix with the calculated variance along the axis and zero along other axes\n  if diag:\n    return mu, dvar\n\n  # Otherwise, return a full-covariance matrix with the calculated variance along the axis and zero along other axes\n  return mu, jnp.diag(dvar)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # Calculate the mean of the Gaussian by taking the midpoint of the cylinder's start and end distances.\n  mu = (t0 + t1) / 2\n\n  # Calculate the variance of the Gaussian based on the cylinder's radius.\n  h = t1 - t0\n  t_var = radius ** 2 / (4 * h)\n  r2 = jnp.dot(d[:2], d[:2])\n  r_var = radius ** 2 / (4 * r2) if r2 > 0 else 1e5 * (mu ** 2)\n  var = t_var + r_var\n\n  # Calculate the covariance matrix of the Gaussian based on the variance and the cylinder's axis.\n  d = d / jnp.linalg.norm(d)\n  x, y = d[:2]\n  cov_diag = jnp.array([var, var, r_var])\n  cov_diag = cov_diag * (1 if diag else 4)\n\n  # Construct the full covariance matrix using the diagonal elements and the cylinder's axis.\n  cov_lower = jnp.array([[x, 0, 0], [y, x, 0], [0, y, x]]) * (var - r_var)\n  cov_lower = cov_lower * (1 if diag else 4)\n  cov = cov_diag + cov_lower\n\n  return lift_gaussian(d, mu, cov)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  import jax.numpy as jnp\n  from .lift_gaussian import lift_gaussian\n\n  mean, cov = lift_gaussian(d, t0, t1, radius)\n\n  if diag:\n    d = jnp.diag(cov)\n    cov = jnp.diag(d)\n\n  return mean, cov\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Get the shape of the input pixel coordinates.\n  sh = pix_x_int.shape\n\n  # Compute the x and y coordinates of the image plane in camera coordinates.\n  x = (pix_x_int + 0.5 - pixtocams[..., 0, 2]) / pixtocams[..., 0, 0]\n  y = (pix_y_int + 0.5 - pixtocams[..., 1, 2]) / pixtocams[..., 1, 1]\n\n  # Compute the ray directions in camera coordinates.\n  if camtype == ProjectionType.PERSPECTIVE:\n    directions = xnp.stack([x, y, xnp.ones_like(x)], -1)\n  elif camtype == ProjectionType.FISHEYE:\n    r = xnp.sqrt(x ** 2 + y ** 2)\n    theta = r * pixtocams[..., 0, 0]\n    sine = xnp.sin(theta)\n    cosine = xnp.cos(theta)\n    directions = xnp.stack([x * sine / r, y * sine / r, cosine], -1)\n  elif camtype == ProjectionType.PANORAMIC:\n    phi = xnp.arctan2(x, y)\n    theta = xnp.arctan2(xnp.sqrt(x ** 2 + y ** 2), 1)\n    sin_theta = xnp.sin(theta)\n    cos_theta = xnp.cos(theta)\n    sin_phi = xnp.sin(phi)\n    cos_phi = xnp.cos(phi)\n    directions = xnp.stack([sin_theta * sin_phi, sin_theta * cos_phi, cos_theta], -1)\n  else:\n    raise NotImplementedError(f\"Unknown camera type {camtype}\")\n\n  # Apply distortion correction if provided.\n  if distortion_params is not None:\n    directions = apply_distortion_to_dirs(directions, dist"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Get the shape of the input pixel coordinates.\n  sh = pix_x_int.shape\n\n  # Compute the camera coordinates of the pixels.\n  cam_x = (pix_x_int + 0.5 - pixtocams[..., 0, 2]) / pixtocams[..., 0, 0]\n  cam_y = (pix_y_int + 0.5 - pixtocams[..., 1, 2]) / pixtocams[..., 1, 1]\n  cam_z = xnp.ones_like(cam_x)\n  cam_coords = xnp.stack([cam_x, cam_y, cam_z], axis=-1)\n\n  # Apply distortion correction if provided.\n  if distortion_params is not None:\n    cam_coords = apply_distortion_to_points(cam_coords, distortion_params, xnp=xnp)\n\n  # Compute the ray directions based on the camera projection type.\n  if camtype == ProjectionType.PERSPECTIVE:\n    directions = cam_coords\n  elif camtype in [ProjectionType.FISHEYE, ProjectionType.PANORAMIC]:\n    directions = xnp.sqrt(cam_coords[..., :2]**2.0 + cam_coords[..., 2:]**2.0)\n    directions = xnp.concatenate([cam_coords[..., :2] / directions, cam_coords[..., 2:] / directions], axis=-1)\n  else:\n    raise ValueError(f\"Unknown camera type {camtype}\")\n\n  # Compute the ray origins and directions in world coordinates.\n  origins = xnp.broadcast_to(camtoworlds[..., :3, -1], directions.shape)\n  directions = xnp.squeeze(camtoworlds[..., :3, :3] @ directions[..., None], axis=-1)\n  viewdirs = directions / xnp.linalg.norm(directions, axis="}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Compute the camera coordinates of the pixels\n  cam_x, cam_y, cam_z = xnp.einsum(\n      '...ij,...j->...i', pixtocams, xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], axis=-1)\n  )\n\n  # Apply distortion correction if provided\n  if distortion_params is not None:\n    cam_x, cam_y = apply_distortion_to_points(cam_x, cam_y, distortion_params)\n\n  # Compute the world coordinates of the camera rays\n  world_x, world_y, world_z = xnp.einsum(\n      '...ij,...j->...i', camtoworlds, xnp.stack([cam_x, cam_y, cam_z], axis=-1)\n  )\n\n  # Compute the ray origins and directions\n  origins = camtoworlds[..., :3, 3]\n  directions = xnp.stack([world_x - origins[..., 0], world_y - origins[..., 1], world_z - origins[..., 2]], axis=-1)\n\n  # Compute the view directions\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute the differential radii for mip-NeRF cones\n  radii = xnp.linalg.norm(xnp.cross(directions[:, :, :2], viewdirs[:, :, :2]), axis=-1, keepdims=True)\n\n  # Compute the image plane coordinates\n  if pixtocam_ndc is not None:\n    imageplane = xnp.einsum(\n        '...ij,...j->...i', pixtocam_ndc, xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)],"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Assert that the input arrays have compatible shapes\n  assert pix_x_int.shape == pix_y_int.shape\n  assert len(pixtocams.shape) == 3 and pixtocams.shape[-2:] == (3, 3)\n  assert len(camtoworlds.shape) == 3 and camtoworlds.shape[-2:] == (3, 4)\n\n  # Get the shape of the input arrays\n  sh = pix_x_int.shape\n\n  # Create a meshgrid of pixel coordinates\n  pix_x, pix_y = xnp.meshgrid(pix_x_int, pix_y_int, indexing=\"ij\")\n  pix_x = pix_x.reshape([-1])\n  pix_y = pix_y.reshape([-1])\n\n  # Create a meshgrid of camera indices\n  cam_i, cam_j = xnp.meshgrid(xnp.arange(pixtocams.shape[0]), xnp.arange(pixtocams.shape[1]), indexing=\"ij\")\n  cam_i = cam_i.reshape([-1])\n  cam_j = cam_j.reshape([-1])\n\n  # Get the inverse intrinsics and extrinsics for each camera\n  pixtocam = pixtocams[cam_i, cam_j]\n  camtoworld = camtoworlds[cam_i, cam_j]\n\n  # Compute the camera coordinates from pixel coordinates\n  cam_x, cam_y, cam_z = camera_utils.pixels_to_camera(\n      pix_x,\n      pix_y,\n      pixtocam,\n      camtype=camtype,\n      xnp=xnp,\n  )\n\n  # Apply distortion correction if provided\n  if distortion_params is not None:\n    cam_x, cam_y = camera_utils.apply_distortion(\n        cam_x,\n        cam_y,\n        distortion_params,\n        xnp=xnp,\n    )\n\n  # Compute"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Compute the pixel coordinates in NDC space\n  if pixtocam_ndc is None:\n    pixtocam_ndc = pixtocams\n\n  # Compute the pixel coordinates in NDC space\n  x, y, z = xnp.tile(xnp.array([0, 0, 1]), [len(pix_x_int.shape), 1]).T\n  pix_ndc = xnp.stack([x[..., None], y[..., None], z[..., None]], axis=-2)\n  pix_ndc = pix_ndc @ xnp.swapaxes(pixtocam_ndc, -1, -2)\n  pix_ndc = pix_ndc[..., :2] / pix_ndc[..., 2:]\n  pix_ndc = xnp.concatenate([pix_ndc, xnp.ones_like(pix_ndc[..., :1])], axis=-1)\n\n  # Compute the ray directions in camera space\n  x = (pix_x_int + 0.5) / pixtocams[..., 0, 0] - pix_ndc[..., 0]\n  y = (pix_y_int + 0.5) / pixtocams[..., 1, 1] - pix_ndc[..., 1]\n  z = xnp.ones_like(x)\n  directions = xnp.stack([x, y, z], axis=-1)\n\n  # Apply distortion correction if provided\n  if distortion_params is not None:\n    directions = apply_distortion_to_dirs(directions, distortion_params)\n\n  # Transform the ray directions to world space\n  directions = directions[..., None, :]\n  camrot = camtoworlds[..., :3, :3]\n  directions = directions @ camrot\n  origins = xnp.broadcast_to(camtoworlds[..., :3, 3], directions.shape)\n  viewdirs = directions / x"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Get the shape of the input pixel coordinates\n  sh = pix_x_int.shape\n\n  # Compute the x and y coordinates in the image plane\n  x = pix_x_int + 0.5\n  y = pix_y_int + 0.5\n\n  # Compute the camera coordinates for the rays\n  cam_x, cam_y, _ = xnp.linalg.inv(pixtocams[..., :3, :3]).dot(\n    xnp.stack(\n      [\n        xnp.full(sh, fill_value=x, dtype=xnp.float32),\n        xnp.full(sh, fill_value=y, dtype=xnp.float32),\n        xnp.ones_like(x, dtype=xnp.float32),\n      ],\n      axis=-1,\n    )\n  )\n\n  # Compute the world coordinates for the rays\n  world_x, world_y, world_z = xnp.linalg.inv(camtoworlds[..., :3, :3]).dot(\n    xnp.stack(\n      [\n        cam_x,\n        cam_y,\n        xnp.ones_like(cam_x, dtype=xnp.float32),\n      ],\n      axis=-1,\n    )\n  )\n\n  # Compute the ray origins and directions\n  origins = camtoworlds[..., :3, -1]\n  directions = xnp.stack([world_x - origins[..., 0], world_y - origins[..., 1], world_z - origins[..., 2]], axis=-1)\n\n  # Normalize the ray directions\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute the differential radii for the rays\n  radii = xnp.linalg.norm(xnp.stack([cam_x, cam_y, xnp.zeros_like(cam_x)], axis=-"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Cast to the correct types.\n  pix_x_int = xnp.array(pix_x_int)\n  pix_y_int = xnp.array(pix_y_int)\n  pixtocams = xnp.array(pixtocams)\n  camtoworlds = xnp.array(camtoworlds)\n\n  # Derive the batch shape.\n  batch_shape = pix_x_int.shape\n\n  # Derive the camera shape.\n  cam_shape = pixtocams.shape[:-2]\n\n  # Derive the full shape.\n  full_shape = batch_shape + cam_shape\n\n  # Derive the pixel shape.\n  pix_shape = batch_shape + (1, 1)\n\n  # Derive the pixel coordinates.\n  pix_x = xnp.full(pix_shape, pix_x_int).astype(xnp.float32)\n  pix_y = xnp.full(pix_shape, pix_y_int).astype(xnp.float32)\n\n  # Derive the pixel coordinates in NDC space.\n  if pixtocam_ndc is not None:\n    pix_x_ndc, pix_y_ndc = convert_pixels_to_ndc(\n        pix_x,\n        pix_y,\n        pixtocam_ndc,\n        xnp = xnp,\n    )\n  else:\n    pix_x_ndc, pix_y_ndc = pix_x, pix_y\n\n  # Derive the camera coordinates.\n  cam_x, cam_y, cam_z = convert_pixels_to_camera_coordinates(\n      pix_x_ndc,\n      pix_y_ndc,\n      pixtocams,\n      xnp = xnp,\n  )\n\n  # Derive the rays.\n  if camtype == ProjectionType.PERSPECTIVE:\n    rays = xnp.stack([cam_x, cam_y, cam_z, xnp.zeros_like(cam_"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if distortion_params is None:\n    distortion_params = {}\n\n  if pixtocam_ndc is None:\n    pixtocam_ndc = xnp.eye(3)\n\n  # Convert to NDC\n  pix_x_int = pix_x_int.astype(xnp.float32)\n  pix_y_int = pix_y_int.astype(xnp.float32)\n  pix_x_int, pix_y_int = convert_to_ndc(pix_x_int, pix_y_int, pixtocam_ndc)\n\n  # Apply distortion\n  if 'k1' in distortion_params:\n    pix_x_int, pix_y_int = apply_distortion_to_pixels(pix_x_int, pix_y_int, **distortion_params)\n\n  # Convert to point in camera coordinates\n  cam_x, cam_y, _ = xnp.linalg.inv(pixtocams[..., :3, :3]) @ xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], axis=-1)\n  cam_x, cam_y, _ = xnp.broadcast_arrays(cam_x[..., None], cam_y[..., None], xnp.ones_like(cam_x[..., None]))\n\n  # Compute ray directions\n  if camtype == ProjectionType.PERSPECTIVE:\n    directions = xnp.stack([cam_x, cam_y, xnp.ones_like(cam_x)], axis=-1)\n  elif camtype == ProjectionType.FISHEYE:\n    directions = xnp.stack([cam_x, cam_y, xnp.zeros_like(cam_x)], axis=-1)\n  elif camtype == ProjectionType.PANORAMIC:\n    directions = xnp.stack([cam_x, cam_y, xnp.ones_like(cam_x"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Compute the xy coordinates in camera space\n  x = (pix_x_int - pixtocams[..., 0, 2]) / pixtocams[..., 0, 0]\n  y = (pix_y_int - pixtocams[..., 1, 2]) / pixtocams[..., 1, 1]\n  z = xnp.ones_like(x)\n\n  # Apply distortion correction if provided\n  if distortion_params is not None:\n    x, y = apply_distortion_to_dirs(x, y, distortion_params, xnp=xnp)\n\n  # Compute the direction vectors in camera space\n  directions = xnp.stack([x, y, z], axis=-1)\n\n  # Apply the camera's rotation to the direction vectors\n  directions = xnp.einsum('...ij,...j->...i', camtoworlds[..., :3, :3], directions)\n\n  # Compute the ray origins in world space\n  origins = camtoworlds[..., :3, 3]\n\n  # Compute the normalized direction vectors\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute the differential radii for mip-NeRF cones\n  radii = xnp.linalg.norm(xnp.stack([xnp.linalg.norm(directions[..., :2], axis=-1), directions[..., 2]], axis=-1), axis=-1, keepdims=True)\n\n  # Project the ray origins and directions into NDC space\n  if pixtocam_ndc is not None:\n    origins = xnp.einsum('...ij,...j->...i', pixtocam_ndc, origins)\n    directions = xnp.einsum('...ij,...j->...i', pixtocam_ndc, directions)\n\n  # Compute the xy coordinates on the image plane"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Compute the number of dimensions in the input arrays\n  ndims = len(pix_x_int.shape)\n\n  # Convert pixel coordinates to NDC coordinates\n  pix_x_ndc, pix_y_ndc = pixel_to_ndc(\n      pix_x_int,\n      pix_y_int,\n      pixtocam_ndc,\n      xnp = xnp,\n  )\n\n  # Compute the ray directions\n  directions = xnp.stack(\n      cam_pix_to_ray_directions(\n          pix_x_ndc,\n          pix_y_ndc,\n          pixtocams,\n          distortion_params = distortion_params,\n          xnp = xnp,\n      ),\n      axis = -1,\n  )\n\n  # Compute the ray origins and view directions\n  if camtype == ProjectionType.PERSPECTIVE:\n    # For perspective cameras, the origin is the camera center\n    origins = camtoworlds[..., :3, 3]\n    viewdirs = directions\n  elif camtype in [\n      ProjectionType.FISHEYE,\n      ProjectionType.PANORAMIC,\n  ]:\n    # For fisheye and panoramic cameras, the origin is the camera center plus the ray direction\n    origins = camtoworlds[..., :3, 3] + directions\n    viewdirs = xnp.broadcast_to(\n        xnp.array([0, 0, -1], dtype = xnp.float32),\n        directions.shape,\n    )\n  else:\n    raise NotImplementedError(\n        f\"{camtype} camera not yet supported...\",\n    )\n\n  # Compute the differential radii of the rays\n  dx = xnp.linalg.norm(\n      xnp.stack(\n          cam_pix_to_ray_directions(\n              pix_x_ndc + 1,\n              pix_y_ndc,\n              pixtocams,\n              distortion_params = distortion"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Get the shape of the input pixel coordinates\n  sh = pix_x_int.shape\n\n  # Get the camera projection type\n  camtype = camera_utils.ProjectionType(camtype)\n\n  # Calculate the camera coordinates using the pixel coordinates and the inverse intrinsics\n  cam_x = (pix_x_int + 0.5 - pixtocams[..., 0, 2]) / pixtocams[..., 0, 0]\n  cam_y = (pix_y_int + 0.5 - pixtocams[..., 1, 2]) / pixtocams[..., 1, 1]\n  cam_z = xnp.ones_like(cam_x)\n  cam_coords = xnp.stack([cam_x, cam_y, cam_z], axis=-1)\n\n  # Apply distortion correction if provided\n  if distortion_params is not None:\n    cam_coords = camera_utils.apply_distortion(cam_coords, distortion_params, camtype)\n\n  # Calculate the ray directions using the camera coordinates and the extrinsics\n  directions = xnp.sum(cam_coords[..., None, :] * camtoworlds[..., :3, :3], axis=-1)\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Calculate the ray origins using the extrinsics\n  origins = xnp.broadcast_to(camtoworlds[..., :3, -1], directions.shape)\n\n  # Calculate the normalized view directions\n  viewdirs = directions\n\n  # Calculate the differential radii of the rays\n  radii = xnp.linalg.norm(xnp.cross(cam_coords, cam_coords[..., ::-1, :]), axis=-1, keepdims=True)\n\n  # Calculate the xy coordinates on the image plane\n  imageplane = xnp.stack([cam_x, cam_y], axis=-"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if distortion_params is not None:\n    raise NotImplementedError('distortion is not implemented')\n\n  if pixtocam_ndc is None:\n    pixtocam_ndc = xnp.eye(3)\n\n  if camtype == ProjectionType.PERSPECTIVE:\n    # Sample point on the image plane\n    x = (pix_x_int + 0.5) / pixtocams[..., 0, 0] - 0.5\n    y = (pix_y_int + 0.5) / pixtocams[..., 1, 1] - 0.5\n    z = xnp.ones_like(x)\n\n    # Point in camera space\n    cam_x = xnp.stack([x, y, z], axis=-1)\n\n    # Point in world space\n    world_x = xnp.matmul(cam_x, xnp.linalg.inv(pixtocams))\n    world_x = world_x / xnp.linalg.norm(world_x, axis=-1, keepdims=True)\n    world_x = xnp.matmul(world_x, camtoworlds[..., :3, :3].T)\n\n    # Direction in world space\n    directions = xnp.matmul(world_x, xnp.linalg.inv(camtoworlds[..., :3, :3]))\n    directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n    # Origin in world space\n    origins = xnp.broadcast_to(camtoworlds[..., :3, -1], directions.shape)\n\n    # Direction in NDC space\n    directions_ndc = xnp.matmul(directions, xnp.linalg.inv(pixtocam_ndc))\n    directions_ndc = directions_ndc / xnp.linalg.norm(directions_ndc, axis=-1, keepdims="}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Get the shape of the input arrays\n  sh = pix_x_int.shape\n\n  # Get the camera intrinsics and extrinsics\n  pixtocam = pixtocams[..., 0:3, 0:3]\n  camtoworld = camtoworlds[..., 0:3, 0:4]\n\n  # Get the camera center in world coordinates\n  camcenter = camtoworld[..., :3, 3]\n\n  # Get the pixel coordinates in camera coordinates\n  pix_z = xnp.ones_like(pix_x_int)\n  camcoord = xnp.stack([pix_x_int, pix_y_int, pix_z], axis=-1)\n  camcoord = xnp.reshape(camcoord, sh + [3, 1])\n  camcoord = xnp.matmul(pixtocam, camcoord)[..., 0]\n\n  # Get the ray directions in camera coordinates\n  ray_d = camcoord\n  ray_d = ray_d / xnp.linalg.norm(ray_d, axis=-1, keepdims=True)\n\n  # Apply distortion correction if provided\n  if distortion_params is not None:\n    camcoord_dist = lens_undistort_points(\n      camcoord,\n      distortion_params,\n      xnp=xnp,\n    )\n    ray_d_dist = camcoord_dist / xnp.linalg.norm(camcoord_dist, axis=-1, keepdims=True)\n    ray_d = ray_d_dist\n\n  # Get the ray origins in world coordinates\n  ray_o = camcenter\n\n  # Get the ray directions in world coordinates\n  ray_d = xnp.matmul(camtoworld[..., :3, :3], ray_d)\n\n  # Get the view directions in world coordinates\n  viewdirs = ray_d / xnp.linalg.norm(ray_d, axis=-1, keepdims=True)\n\n "}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Get the shape of the input pixel coordinates.\n  sh = pix_x_int.shape\n\n  # Compute the x and y coordinates of the pixel centers.\n  pix_x = pix_x_int + 0.5\n  pix_y = pix_y_int + 0.5\n\n  # Compute the camera coordinates by applying the inverse intrinsics to the pixel coordinates.\n  cam_x = (pix_x - pixtocams[..., 0, 2]) / pixtocams[..., 0, 0]\n  cam_y = (pix_y - pixtocams[..., 1, 2]) / pixtocams[..., 1, 1]\n\n  # Compute the ray directions based on the camera coordinates, camera projection type, and distortion parameters.\n  directions = xnp.stack(\n    [cam_x, cam_y, xnp.ones_like(cam_x)],\n    -1,\n  )\n  if distortion_params is not None:\n    directions = distort_points(directions, **distortion_params)\n  if camtype == ProjectionType.PERSPECTIVE:\n    directions /= xnp.linalg.norm(directions, axis=-1, keepdims=True)\n  elif camtype in [ProjectionType.FISHEYE, ProjectionType.PANORAMIC]:\n    directions = xnp.sin(directions)\n  else:\n    raise NotImplementedError(f\"{camtype} not implemented\")\n  directions = directions @ xnp.swapaxes(pixtocams[..., :3, :3], -1, -2)\n\n  # Compute the ray origins by applying the extrinsics to the camera coordinates.\n  origins = xnp.broadcast_to(camtoworlds[..., :3, 3], directions.shape)\n\n  # Compute the normalized view directions.\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  #"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Get the shape of the input pixel coordinates.\n  sh = pix_x_int.shape\n\n  # Compute the xy coordinates on the image plane.\n  x = (pix_x_int + 0.5) / pixtocams[..., 0, 0] - pixtocams[..., 0, 2]\n  y = (pix_y_int + 0.5) / pixtocams[..., 1, 1] - pixtocams[..., 1, 2]\n  imageplane = xnp.stack([x, y], axis=-1)\n\n  # Compute the directions of the rays.\n  if camtype == ProjectionType.PERSPECTIVE:\n    # For perspective projection, compute the directions as a stack of the xy coordinates and a fixed z-coordinate of -1.\n    directions = xnp.stack([x, y, xnp.full_like(x, -1)], axis=-1)\n  elif camtype == ProjectionType.FISHEYE:\n    # For fisheye projection, compute the directions as a stack of the xy coordinates and a fixed z-coordinate of -1, then normalize them.\n    directions = xnp.stack([x, y, xnp.full_like(x, -1)], axis=-1)\n    directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n  elif camtype == ProjectionType.PANORAMIC:\n    # For panoramic projection, compute the spherical coordinates of the directions and convert them to cartesian coordinates.\n    theta = xnp.pi * (pix_x_int / pixtocams[..., 0, 0] - 1)\n    phi = xnp.pi * (pix_y_int / pixtocams[..., 1, 1] - 1)\n    directions = xnp.stack(\n      [\n        xnp.sin(phi) * xnp.sin(theta),\n        xnp.cos(phi),\n        -xnp."}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if distortion_params is None:\n    distortion_params = {}\n\n  # Shape of the broadcasted rays.\n  shape = pix_x_int.shape\n\n  # The broadcasted pixel coordinates.\n  pix_x = xnp.broadcast_to(pix_x_int[..., None], shape + (1,))\n  pix_y = xnp.broadcast_to(pix_y_int[..., None], shape + (1,))\n\n  # Pixel coordinates in camera space.\n  if camtype == ProjectionType.PERSPECTIVE:\n    cam_x = (pix_x - pixtocams[..., 0:1, 2]) / pixtocams[..., 0:1, 0] * pix_x.shape[0]\n    cam_y = (pix_y - pixtocams[..., 1:2, 2]) / pixtocams[..., 1:2, 1] * pix_y.shape[1]\n    cam_z = xnp.ones_like(pix_x)\n  elif camtype == ProjectionType.FISHEYE:\n    r = xnp.sqrt(pix_x ** 2 + pix_y ** 2)\n    theta = r * pixtocams[..., 0, 0]\n    cam_x = theta * xnp.sin(theta)\n    cam_y = -theta * xnp.cos(theta)\n    cam_z = xnp.ones_like(pix_x)\n  elif camtype == ProjectionType.SPHERICAL_PANORAMA:\n    theta = xnp.arctan2(pix_x, pix_y)\n    phi = r * pixtocams[..., 0, 0]\n    cam_x = xnp.sin(phi) * xnp.cos(theta)\n    cam_y = -xnp.sin(phi) * xnp.sin(theta)\n    cam_z = xnp.cos(phi)\n  else:\n    raise Value"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Compute the pixel coordinates in NDC space.\n  if pixtocam_ndc is not None:\n    # Apply the inverse intrinsics to transform pixel coordinates to NDC space.\n    pix_x = (pix_x_int + 0.5) / xnp.array(pixtocam_ndc[0, 0]) - 1.\n    pix_y = (pix_y_int + 0.5) / xnp.array(pixtocam_ndc[1, 1]) - 1.\n    z_ndc = -xnp.ones_like(pix_x)\n  else:\n    # If pixtocam_ndc is not provided, use the provided inverse intrinsics to transform pixel coordinates to camera space.\n    pix_x = (pix_x_int + 0.5) / xnp.array(pixtocams[0, 0]) - 1.\n    pix_y = (pix_y_int + 0.5) / xnp.array(pixtocams[1, 1]) - 1.\n    z_ndc = -xnp.ones_like(pix_x)\n\n  # Compute the ray directions based on the camera projection type.\n  if camtype == ProjectionType.PERSPECTIVE:\n    # For perspective projection, the ray directions are computed using the inverse intrinsics and the pixel coordinates.\n    directions = xnp.stack([pix_x, pix_y, z_ndc], -1)\n  elif camtype == ProjectionType.ORTHOGRAPHIC:\n    # For orthographic projection, the ray directions are simply the z-axis of the camera.\n    directions = xnp.array([0., 0., 1.])\n    directions = xnp.broadcast_to(directions, list(pix_x.shape) + [3])\n  elif camtype == ProjectionType.FISHEYE:\n    # For fisheye projection, the ray directions are computed using the inverse intrinsics, pixel coordinates, and distortion parameters.\n    x, y = distort_p"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Get the shape of the input pixel coordinates.\n  sh = pix_x_int.shape\n\n  # Calculate the camera coordinates of the pixel coordinates using the inverse intrinsics.\n  xcam = pix_x_int - pixtocams[..., 0, 2]\n  ycam = pix_y_int - pixtocams[..., 1, 2]\n  zcam = -xnp.ones_like(xcam)\n\n  # Apply distortion correction to the camera coordinates if distortion parameters are provided.\n  if distortion_params is not None:\n    r2 = xcam * xcam + ycam * ycam\n    r4 = r2 * r2\n    r6 = r4 * r2\n    coeff = (1.0 + distortion_params['k1'] * r2 + distortion_params['k2'] * r4 + distortion_params['k3'] * r6)\n    xcam = xcam * coeff\n    ycam = ycam * coeff\n    xcam = xcam + distortion_params['p1'] * 2.0 * xcam * ycam + distortion_params['p2'] * (r2 + 2.0 * xcam * xcam)\n    ycam = ycam + distortion_params['p2'] * 2.0 * xcam * ycam + distortion_params['p1'] * (r2 + 2.0 * ycam * ycam)\n    r2 = xcam * xcam + ycam * ycam\n\n  # Convert the camera coordinates to world coordinates using the extrinsics and the camera type.\n  if camtype == ProjectionType.PERSPECTIVE:\n    directions = xnp.stack([xcam, ycam, zcam], axis=-1)\n    viewdirs = directions\n    origins = xnp.reshape(camtoworlds[..., :3, 3], list(sh) + [3])\n  elif camtype == ProjectionType.FISHEYE:\n    directions = xnp.stack([xcam, ycam, zcam], axis=-1)\n    thetas"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # 1. Compute pixel coordinates in camera space\n  #   a. Apply distortion correction if provided\n  #   b. Compute pixel coordinates in camera space\n  if distortion_params is not None:\n    # Apply distortion correction\n    pix_x_corrected, pix_y_corrected = apply_distortion_to_pixels(\n        pix_x_int,\n        pix_y_int,\n        distortion_params,\n        xnp = xnp,\n    )\n  else:\n    pix_x_corrected = pix_x_int\n    pix_y_corrected = pix_y_int\n\n  # Compute pixel coordinates in camera space\n  pix_x_cam, pix_y_cam, _ = apply_pix_to_cam(\n      pix_x_corrected,\n      pix_y_corrected,\n      pixtocams,\n      xnp = xnp,\n  )\n\n  # 2. Compute ray directions in camera space\n  #   a. Compute ray directions based on camera projection type\n  #   b. Apply distortion correction if provided\n  #   c. Normalize ray directions\n  if camtype == ProjectionType.PERSPECTIVE:\n    # Compute ray directions for perspective projection\n    ray_d_cam = xnp.stack([pix_x_cam, pix_y_cam, xnp.ones_like(pix_x_cam)], axis = -1)\n  elif camtype == ProjectionType.FISHEYE:\n    # Compute ray directions for fisheye projection\n    r = xnp.sqrt(pix_x_cam ** 2 + pix_y_cam ** 2)\n    theta = r * 1.2\n    x = xnp.sin(theta) * pix_x_cam / r\n    y = xnp.sin(theta) * pix_y_cam / r\n    ray_d_cam = xnp.stack([x, y, xnp.cos(theta)], axis = -1)\n  else:\n    raise NotImplementedError(f\"Unsupported camera type: {"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Get the shape of the input arrays\n  sh = pix_x_int.shape\n\n  # Convert pixel coordinates to NDC coordinates\n  x, y = convert_to_ndc(\n      pix_x_int,\n      pix_y_int,\n      pixtocam_ndc,\n      xnp = xnp,\n  )\n\n  # Compute the ray directions\n  directions = xnp.stack([x, y, xnp.ones_like(x)], axis = -1)\n  if distortion_params is not None:\n    directions = distort_points(directions, distortion_params, xnp = xnp)\n  directions = xnp.squeeze(directions, axis = -1)\n\n  # Compute the ray origins and directions in camera coordinates\n  mat_mul = einops.reduce(pixtocams[..., None, :, :] * directions[..., None], '... ab, ... bc -> ... ac', 'sum')\n  mat_mul += camtoworlds[..., :3, 3]\n  origins = camtoworlds[..., :3, :3].transpose((0, 2, 1)) @ (mat_mul - camtoworlds[..., :3, 3:4])\n\n  # Compute the ray directions in camera coordinates\n  ray_dirs = directions[..., None, :] * camtoworlds[..., :3, :3]\n  ray_dirs = xnp.squeeze(ray_dirs, axis = -2)\n\n  # Compute the view directions\n  viewdirs = ray_dirs / xnp.linalg.norm(ray_dirs, axis = -1, keepdims = True)\n\n  # Compute the differential radii for mip-NeRF\n  dx = xnp.sqrt(xnp.sum((xnp.squeeze(directions, axis = -1) - xnp.squeeze(origins, axis = -1)) ** 2, axis = -1))\n  radii = 2 / xnp"}
{"namespace": "render.compute_alpha_weights", "completion": "  return compute_alpha_weights_helper(\n    density * tdist,\n    dirs,\n    **kwargs,\n  )"}
{"namespace": "render.compute_alpha_weights", "completion": "  return compute_alpha_weights_helper(\n    density * tdist,\n    dirs,\n    **kwargs,\n  )\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the product of density and the adjusted distance between points\n  adjusted_density = density * tdist\n\n  # Call the helper function to compute the alpha weights\n  return compute_alpha_weights_helper(adjusted_density, dirs, **kwargs)\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the product of density and the adjusted distance between points\n  density_tdist = density * tdist\n\n  # Compute the alpha weights using the helper function and the computed density_tdist\n  return compute_alpha_weights_helper(density_tdist, dirs, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  return compute_alpha_weights_helper(\n    density * tdist,\n    dirs,\n    **kwargs,\n  )\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the alpha compositing weights\n  return compute_alpha_weights_helper(density * tdist, dirs, **kwargs)\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  return compute_alpha_weights_helper(density * tdist, dirs, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  return compute_alpha_weights_helper(\n    density * (tdist / np.linalg.norm(dirs, axis=-1)),\n    **kwargs,\n  )\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  import numpy as np\n  from .compute_alpha_weights_helper import compute_alpha_weights_helper\n\n  # Compute the product of density and the adjusted distance between points\n  density_tdist_product = density * np.linalg.norm(tdist * dirs, axis=1)\n\n  # Call the helper function to compute the alpha weights\n  return compute_alpha_weights_helper(density_tdist_product, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  return compute_alpha_weights_helper(\n    density * tdist,\n    dirs,\n    **kwargs,\n  )\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the adjusted distance between points\n  adjusted_tdist = density * tdist\n\n  # Compute the alpha weights using the helper function\n  return compute_alpha_weights_from_adjusted_tdist(adjusted_tdist, dirs, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the product of density and the adjusted distance between points\n  adjusted_density = density * (tdist / np.linalg.norm(dirs, axis=1))\n\n  # Call the helper function to compute the alpha weights\n  return compute_alpha_weights_helper(adjusted_density, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the adjusted density values\n  adjusted_density = density * tdist\n\n  # Call the helper function to compute the alpha weights\n  return compute_alpha_weights_helper(adjusted_density, dirs, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  return compute_alpha_weights_helper(\n      density * tdist,\n      dirs,\n      **kwargs,\n  )\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  import numpy as np\n  from .compute_alpha_weights_helper import compute_alpha_weights_helper\n\n  # Compute the product of density and the adjusted distance between points\n  alpha_weights = density * np.linalg.norm(tdist * dirs, axis=1)\n\n  # Call the helper function to compute the alpha weights\n  return compute_alpha_weights_helper(alpha_weights, **kwargs)\n\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  alpha_weights = compute_alpha_weights_helper(\n    density * tdist,\n    dirs,\n    **kwargs,\n  )\n\n  return alpha_weights\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  return compute_alpha_weights_helper(\n    density * tdist,\n    dirs,\n    **kwargs,\n  )\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density = density * tdist\n  return compute_alpha_weights_helper(density, dirs, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  return compute_alpha_weights_helper(\n    density * tdist,\n    dirs,\n    **kwargs,\n  )\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  return compute_alpha_weights_helper(density * tdist, dirs, **kwargs)\n"}
{"namespace": "stepfun.sample", "completion": "  if rng is None:\n    # Deterministic sampling\n    if deterministic_center:\n      # Centered deterministic sampling\n      t_mids = 0.5 * (t[..., 1:] + t[..., :-1])\n      t_left = jnp.concatenate([t[..., :1], t_mids], axis=-1)\n      t_right = jnp.concatenate([t_mids, t[..., -1:]], axis=-1)\n      w_left = w_logits[..., :-1]\n      w_right = w_logits[..., 1:]\n      w_max = jnp.maximum(w_left, w_right)\n      cdf_left = jax.nn.sigmoid(w_left - w_max)\n      cdf_right = jax.nn.sigmoid(w_right - w_max)\n      u = jnp.linspace(0., 1., num_samples)\n      u = jnp.broadcast_to(u, list(cdf_left.shape[:-1]) + [num_samples])\n      indices = jnp.searchsorted(jnp.cumsum(cdf_right - cdf_left, axis=-1), u, side='right')\n      t0 = jnp.broadcast_to(t_left, indices.shape)\n      t1 = jnp.broadcast_to(t_right, indices.shape)\n      t0 = jnp.take_along_axis(t0, indices, axis=-1)\n      t1 = jnp.take_along_axis(t1, indices, axis=-1)\n      samples = t0 + (t1 - t0) * (u - jnp.take_along_axis(cdf_left, indices, axis=-1)) / (jnp.take_along_axis(cdf_right, indices, axis=-1) - jnp.take_along_axis(cdf_left, indices, axis=-1) + eps)\n    else:"}
{"namespace": "stepfun.sample", "completion": "  # Calculate the PDF using the weights and bin endpoints\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cdf = jnp.cumsum(w, axis=-1)\n  cdf = jnp.concatenate([jnp.zeros(list(cdf.shape[:-1]) + [1]), cdf], axis=-1)\n  cdf_delta = cdf[..., 1:] - cdf[..., :-1]\n  cdf_delta = jnp.clip(cdf_delta, a_min=1e-10, a_max=None)\n\n  # Determine the bin indices for each sample\n  if rng is None:\n    # Deterministic sampling\n    if deterministic_center:\n      # Sample from the center of each bin\n      u = jnp.linspace(0.0, 1.0, num_samples)\n      u = jnp.broadcast_to(u, list(cdf.shape[:-1]) + [num_samples])\n      u = u + jnp.zeros(list(cdf.shape[:-1]) + [num_samples])\n    else:\n      # Sample uniformly from the entire PDF\n      u = jnp.linspace(0.0, 1.0, num_samples)\n      u = jnp.broadcast_to(u, list(cdf.shape[:-1]) + [num_samples])\n  else:\n    # Random sampling\n    if single_jitter:\n      # Sample uniformly from the entire PDF and add the same jitter to all samples\n      u = jax.random.uniform(rng, list(cdf.shape[:-1]) + [num_samples])\n      u = u + jax.random.uniform(rng, list(cdf.shape[:-1]) + [1])\n    else:\n      # Sample uniformly from the entire PDF and add independent jitter to each sample\n      u = jax.random.uniform(rng, list(cdf.shape[:-1]) + [num_s"}
{"namespace": "stepfun.sample", "completion": "  # Get the bin weights and normalize them\n  w = jax.nn.softmax(w_logits)\n\n  # Compute the bin widths\n  bin_widths = t[1:] - t[:-1]\n\n  # Compute the cumulative weights\n  cdf = jnp.cumsum(w, axis=-1)\n\n  # Compute the total weight\n  total_weight = cdf[..., -1]\n\n  # Compute the bin boundaries\n  bin_boundaries = jnp.concatenate([t[..., :1], t[..., 1:]], axis=-1)\n\n  # Compute the bin centers\n  bin_centers = (bin_boundaries[..., 1:] + bin_boundaries[..., :-1]) / 2\n\n  # Compute the bin probabilities\n  bin_probs = w / total_weight\n\n  # Compute the bin indices\n  bin_indices = jnp.arange(w.shape[-1])\n\n  # Compute the bin intervals\n  bin_intervals = jnp.stack([bin_boundaries[..., :-1], bin_boundaries[..., 1:]], axis=-1)\n\n  # Compute the bin centers\n  bin_centers = (bin_boundaries[..., 1:] + bin_boundaries[..., :-1]) / 2\n\n  # Compute the bin probabilities\n  bin_probs = w / total_weight\n\n  # Compute the bin indices\n  bin_indices = jnp.arange(w.shape[-1])\n\n  # Compute the bin intervals\n  bin_intervals = jnp.stack([bin_boundaries[..., :-1], bin_boundaries[..., 1:]], axis=-1)\n\n  # Compute the bin centers\n  bin_centers = (bin_boundaries[..., 1:] + bin_boundaries[..., :-1]) / 2\n\n  # Compute the bin probabilities\n  bin_probs = w / total_weight\n\n  # Compute the bin"}
{"namespace": "stepfun.sample", "completion": "  # Get the weights from logits\n  w = jax.nn.softmax(w_logits, axis=-1)\n\n  # Compute the bin widths\n  bin_widths = t[..., 1:] - t[..., :-1]\n\n  # Compute the cumulative bin widths\n  cum_bin_widths = jnp.cumsum(bin_widths, axis=-1)\n\n  # Compute the total width of the step function\n  total_width = jnp.sum(bin_widths, axis=-1, keepdims=True)\n\n  # Compute the cumulative bin probabilities\n  cum_bin_probs = jnp.concatenate([jnp.zeros_like(cum_bin_widths[..., :1]), cum_bin_widths / total_width], axis=-1)\n\n  # Compute the inverse cumulative bin probabilities\n  inv_cum_bin_probs = 1.0 - cum_bin_probs\n\n  # Compute the bin probabilities\n  bin_probs = bin_widths / total_width\n\n  # Compute the cumulative bin probabilities\n  cum_bin_probs = jnp.concatenate([jnp.zeros_like(cum_bin_probs[..., :1]), cum_bin_probs], axis=-1)\n\n  # Compute the inverse cumulative bin probabilities\n  inv_cum_bin_probs = jnp.concatenate([inv_cum_bin_probs, jnp.zeros_like(inv_cum_bin_probs[..., :1])], axis=-1)\n\n  # Compute the bin widths\n  bin_widths = jnp.concatenate([bin_widths, jnp.zeros_like(bin_widths[..., :1])], axis=-1)\n\n  # Compute the cumulative bin widths\n  cum_bin_widths = jnp.concatenate([jnp.zeros_like(cum_bin_widths[..., :1]),"}
{"namespace": "stepfun.sample", "completion": "  # Convert logits to weights\n  w = jax.nn.softmax(w_logits, axis=-1)\n\n  # Compute bin widths\n  t0 = t[..., :-1]\n  t1 = t[..., 1:]\n  bin_widths = t1 - t0\n\n  # Compute bin centers\n  bin_centers = t0 + 0.5 * bin_widths\n\n  # Compute weight-integrated bin centers\n  w_integrated_bin_centers = jnp.cumsum(w * bin_centers, axis=-1)\n\n  # Compute the total mass of the step function\n  total_mass = jnp.sum(w * bin_widths, axis=-1, keepdims=True)\n\n  # Compute the cumulative mass distribution\n  cdf = jnp.cumsum(w * bin_widths, axis=-1)\n  cdf = cdf / total_mass\n\n  # Add a small value to the CDF for numerical stability\n  cdf = jnp.concatenate([jnp.zeros_like(cdf[..., :1]), cdf], axis=-1)\n\n  # Compute the inverse CDF\n  inv_cdf = 1.0 / (cdf[..., :-1] + eps)\n\n  # Compute the PDF\n  pdf = w * inv_cdf\n\n  # Normalize the PDF\n  pdf = pdf / jnp.sum(pdf, axis=-1, keepdims=True)\n\n  # Compute the CDF of the PDF\n  pdf_cdf = jnp.cumsum(pdf, axis=-1)\n  pdf_cdf = jnp.concatenate([jnp.zeros_like(pdf_cdf[..., :1]), pdf_cdf], axis=-1)\n\n  # Generate uniform samples\n  if rng is None:\n    # Deterministic sampling\n    if deterministic_center:\n      u = jnp.linspace(0.0, 1.0, num_s"}
{"namespace": "stepfun.sample", "completion": "  # Convert logits to weights\n  w = jax.nn.softmax(w_logits, axis=-1)\n\n  # Compute the cumulative sum of the weights\n  cdf = jnp.cumsum(w, axis=-1)\n\n  # Add a small value to the cumulative sum to avoid numerical issues\n  cdf = jnp.concatenate([jnp.zeros_like(cdf[..., :1]), cdf], axis=-1)\n\n  # Normalize the cumulative sum to ensure it ranges from 0 to 1\n  cdf = cdf / jnp.maximum(cdf[..., -1:], eps)\n\n  # Compute the bin boundaries\n  bin_bounds = jnp.concatenate([t[..., :1], (t[..., 1:] + t[..., :-1]) / 2, t[..., -1:]], axis=-1)\n\n  # Generate random samples\n  if rng is None:\n    # Deterministic sampling\n    if deterministic_center:\n      # Sample from the center of each bin\n      u = jnp.linspace(0.0, 1.0, num_samples)\n      u = u[None, ...]\n      u = u + jnp.zeros(list(cdf.shape[:-1]) + [num_samples], dtype=cdf.dtype)\n      u = jnp.clip(u, eps, 1.0 - eps)\n    else:\n      # Sample uniformly over the entire PDF\n      u = jnp.linspace(0.0, 1.0, num_samples + 2)\n      u = u[1:-1]\n      u = u[None, ...]\n      u = u + jnp.zeros(list(cdf.shape[:-1]) + [num_samples], dtype=cdf.dtype)\n  else:\n    # Random sampling\n    if single_jitter:\n      # Sample a single jitter value for all samples\n      u_jitter = jax.random."}
{"namespace": "stepfun.sample", "completion": "  # Compute the weights of each bin using the softmax function\n  weights = jax.nn.softmax(w_logits)\n\n  # Compute the CDF of the step function\n  cdf = jnp.concatenate([jnp.zeros_like(w_logits[:1]), jnp.cumsum(weights[:-1])], axis=0)\n\n  # Compute the bin widths\n  delta = t[1:] - t[:-1]\n\n  # Compute the normalization factor for the CDF\n  cdf_normalization = cdf[-1] + delta[-1] / 2\n\n  # Compute the inverse CDF\n  inv_cdf = jnp.clip(jnp.diff(cdf, prepend=0) / delta, eps, 1 - eps)\n\n  # Compute the PDF\n  pdf = inv_cdf / cdf_normalization\n\n  # Compute the CDF of the midpoints of each bin\n  cdf_midpoints = (cdf[:-1] + cdf[1:]) / 2\n\n  # Compute the inverse CDF of the midpoints\n  inv_cdf_midpoints = jnp.clip(cdf_midpoints / cdf_normalization, eps, 1 - eps)\n\n  # Compute the bin centers\n  bin_centers = (t[:-1] + t[1:]) / 2\n\n  # Compute the PDF of the bin centers\n  pdf_midpoints = jnp.diff(inv_cdf_midpoints, prepend=0) / delta\n\n  # Compute the CDF of the bin centers\n  cdf_midpoints = jnp.concatenate(\n      [jnp.zeros_like(cdf_midpoints[:1]), jnp.cumsum(pdf_midpoints[:-1])], axis=0\n  )\n\n  # Compute the inverse CDF of the bin centers\n  inv_cdf_midpoints = jnp.clip(cdf_midpoints / cdf_normalization, eps, 1 - eps)"}
{"namespace": "stepfun.sample", "completion": "  # Compute the PDF (pmf) using the provided bin endpoints and weights.\n  pmf = jnp.diff(jnp.exp(w_logits), prepend=0.0)\n  pmf = pmf / jnp.sum(pmf)\n\n  # Compute the CDF (cdf) from the PDF.\n  cdf = jnp.cumsum(pmf)\n  cdf = jnp.concatenate([jnp.zeros(1), cdf], axis=0)\n\n  # Compute the bin widths.\n  bin_widths = jnp.diff(t, prepend=t[0])\n\n  # Compute the bin centers.\n  bin_centers = (t[1:] + t[:-1]) / 2\n\n  # Compute the total length of the step function.\n  total_length = jnp.sum(bin_widths * pmf)\n\n  # Compute the cumulative bin widths.\n  cum_bin_widths = jnp.cumsum(bin_widths)\n\n  # Compute the cumulative bin centers.\n  cum_bin_centers = jnp.cumsum(bin_centers * pmf) / jnp.sum(pmf)\n\n  # Compute the offset for each bin.\n  offset = cum_bin_centers - cum_bin_widths / 2\n\n  # Compute the offset for each bin.\n  offset = cum_bin_centers - cum_bin_widths / 2\n\n  # Compute the offset for each bin.\n  offset = cum_bin_centers - cum_bin_widths / 2\n\n  # Compute the offset for each bin.\n  offset = cum_bin_centers - cum_bin_widths / 2\n\n  # Compute the offset for each bin.\n  offset = cum_bin_centers - cum_bin_widths / 2\n\n  # Compute the offset for each bin.\n  offset = cum_bin_centers - cum_bin_widths / 2\n\n  # Compute the offset for each"}
{"namespace": "stepfun.sample", "completion": "  # Compute the bin widths\n  t_diff = t[..., 1:] - t[..., :-1]\n  t_diff = jnp.concatenate([t_diff, jnp.array([eps])], axis=-1)\n\n  # Compute the cumulative sum of the weights\n  w_cumsum = jnp.cumsum(jax.nn.softmax(w_logits, axis=-1), axis=-1)\n\n  # Compute the mid-point of each bin\n  t_mid = t[..., :-1] + 0.5 * t_diff[..., :-1]\n\n  # Compute the width of each bin\n  t_width = t_diff[..., :-1]\n\n  # Compute the probability density function (PDF) of the step function\n  pdf = w_cumsum[..., 1:] - w_cumsum[..., :-1]\n\n  # Compute the cumulative distribution function (CDF) of the step function\n  cdf = jnp.concatenate([jnp.zeros_like(w_cumsum[..., :1]), w_cumsum], axis=-1)\n\n  # Compute the inverse CDF of the step function\n  inv_cdf = jnp.clip(jnp.diff(cdf, axis=-1) / pdf, a_min=eps, a_max=1.0 - eps)\n  inv_cdf = jnp.concatenate([jnp.zeros_like(cdf[..., :1]), inv_cdf], axis=-1)\n  inv_cdf = jnp.cumsum(inv_cdf, axis=-1)\n\n  # Compute the CDF of the mid-points of the bins\n  cdf_mid = jnp.concatenate([jnp.zeros_like(w_cumsum[..., :1]), w_cumsum[..., :-1]], axis=-1)\n\n  # Compute the inverse CDF of the mid-points of the bins"}
{"namespace": "stepfun.sample", "completion": "  if rng is None:\n    # Deterministic sampling.\n    if deterministic_center:\n      # Center the samples within each interval.\n      t_mids = 0.5 * (t[..., 1:] + t[..., :-1])\n      t_mids = jnp.concatenate([t_mids, t[..., -1:]], -1)\n      t_mids = jnp.concatenate([t[..., :1], t_mids], -1)\n      return t_mids\n    else:\n      # Uniformly sample from the PDF's support.\n      t_interval = t[..., -1] - t[..., 0]\n      t_interval = jnp.expand_dims(t_interval, -1)\n      return t[..., None, :] + jnp.linspace(0.0, 1.0, num_samples)[None, ...] * t_interval\n  else:\n    # Random sampling.\n    if single_jitter:\n      # Apply the same jitter to all samples.\n      jitter = jax.random.uniform(rng, [num_samples], minval=eps, maxval=1.0 - eps)\n      jitter = jnp.broadcast_to(jitter, t.shape[:-1] + jitter.shape)\n    else:\n      # Apply a different jitter to each sample.\n      jitter = jax.random.uniform(rng, t.shape[:-1] + [num_samples], minval=eps, maxval=1.0 - eps)\n\n    # Get the CDF at the jittered positions.\n    cdf_jitter = jnp.cumsum(jnp.exp(w_logits), axis=-1)\n    cdf_jitter = jnp.concatenate([jnp.zeros_like(cdf_jitter[..., :1]), cdf_jitter], axis=-1)\n    cdf_jitter = jnp.clip(cdf_jitter, a_min=0"}
{"namespace": "stepfun.sample", "completion": "  # Compute the bin widths\n  w = jax.nn.softmax(w_logits)\n  widths = t[..., 1:] - t[..., :-1]\n\n  # Compute the cumulative sum of the bin weights\n  cdf = jnp.cumsum(w, axis=-1)\n  cdf = jnp.concatenate([jnp.zeros(list(cdf.shape[:-1]) + [1]), cdf], axis=-1)\n\n  # Compute the total length of the step function\n  total_length = cdf[..., -1:]\n\n  # Compute the bin indices for each sample\n  if rng is not None:\n    # Generate random samples\n    if single_jitter:\n      # Generate a single jitter value for all samples\n      jitter = random.uniform(rng, list(cdf.shape[:-1]) + [1])\n    else:\n      # Generate a separate jitter value for each sample\n      jitter = random.uniform(rng, list(cdf.shape[:-1]) + [num_samples])\n    bin_idx = jnp.searchsorted(cdf, jitter, side=\"right\")\n  else:\n    # Generate deterministic samples\n    if deterministic_center:\n      # Sample from the center of each bin\n      bin_idx = jnp.searchsorted(cdf, 0.5, side=\"right\")\n    else:\n      # Sample uniformly from the entire step function\n      bin_idx = jnp.searchsorted(cdf, jnp.linspace(0.0, 1.0, num_samples), side=\"right\")\n\n  # Compute the bin boundaries for each sample\n  lower_bounds = t[..., :-1][..., bin_idx]\n  upper_bounds = t[..., 1:][..., bin_idx]\n\n  # Compute the length of each bin\n  bin_length = upper_bounds - lower_bounds\n\n  # Compute the offset for each sample\n  if r"}
{"namespace": "stepfun.sample", "completion": "  # Compute the bin weights by applying the softmax function to the logits\n  w = jax.nn.softmax(w_logits)\n\n  # Compute the bin widths\n  bin_widths = jnp.diff(t, axis=-1, append=jnp.inf)\n\n  # Compute the cumulative bin weights\n  cum_w = jnp.cumsum(w, axis=-1)\n\n  # Compute the total weight\n  total_w = cum_w[..., -1:]\n\n  # Normalize the bin weights\n  w = w / jnp.clip(total_w, a_min=eps)\n\n  # Compute the cumulative bin widths\n  cum_bin_widths = jnp.concatenate([jnp.zeros_like(w[..., :1]), jnp.cumsum(bin_widths, axis=-1)], axis=-1)\n\n  # Compute the step function values\n  cdf = jnp.clip(cum_w / total_w, a_min=0.0, a_max=1.0)\n\n  # Compute the bin indices for each sample\n  if rng is None:\n    # Deterministic sampling\n    if deterministic_center:\n      # Sample from the center of each bin\n      u = jnp.linspace(0.0, 1.0, num_samples)\n      bin_idx = jnp.searchsorted(cdf, u, side=\"right\")\n    else:\n      # Sample uniformly from the entire PDF\n      u = jnp.linspace(0.0, 1.0, num_samples + 2)\n      u = u[1:-1]\n      bin_idx = jnp.searchsorted(cdf, u, side=\"right\")\n  else:\n    # Random sampling\n    if single_jitter:\n      # Sample uniformly from the bin widths\n      u = jax.random.uniform(rng, shape=(num_samples,))\n      bin_idx = jnp.searchsorted(cum_w"}
{"namespace": "stepfun.sample", "completion": "  # Convert logits to weights\n  w = jax.nn.softmax(w_logits)\n\n  # Get the cumulative sum of the weights\n  cdf = jnp.cumsum(w, axis=-1)\n\n  # Pad the CDF to ensure broadcasting works correctly\n  cdf = jnp.pad(cdf, ((0, 0), (1, 0)), mode=\"constant\", constant_values=0)\n\n  # Get the bin boundaries\n  t0 = t[..., :-1]\n  t1 = t[..., 1:]\n\n  # Calculate the range of the PDF\n  pdf_range = t1 - t0\n\n  # Calculate the bin size for each bin\n  bin_size = w * pdf_range\n\n  # Calculate the cumulative sum of the bin sizes\n  bin_size_cumsum = jnp.cumsum(bin_size, axis=-1)\n\n  # Calculate the total range of the PDF\n  total_range = bin_size_cumsum[..., -1:]\n\n  # Calculate the step size for each bin\n  step = total_range / num_samples\n\n  # Generate samples using either random or deterministic sampling\n  if rng is not None:\n    # Random sampling\n    u = jax.random.uniform(rng, list(cdf.shape[:-1]) + [num_samples])\n    if single_jitter:\n      # Add the same jitter to all samples\n      u += jax.random.uniform(rng, t.shape[:-1] + (1,))[..., None]\n    else:\n      # Add different jitter to each sample\n      u += jax.random.uniform(rng, t.shape[:-1] + (num_samples,))[..., None]\n  else:\n    # Deterministic sampling\n    if deterministic_center:\n      # Sample around the center of each bin\n      u = jnp.linspace(0.5 / num_samples, 1.0 - 0.5 / num_samples,"}
{"namespace": "stepfun.sample", "completion": "  # Compute the bin widths\n  delta = t[..., 1:] - t[..., :-1]\n\n  # Compute the cumulative sum of the weights\n  cdf = jnp.cumsum(jax.nn.softmax(w_logits, axis=-1), axis=-1)\n\n  # Normalize the CDF\n  cdf = jnp.concatenate([jnp.zeros(list(cdf.shape[:-1]) + [1]), cdf], axis=-1)\n\n  # Compute the bin boundaries\n  lower_bound = cdf[..., :-1]\n  upper_bound = cdf[..., 1:]\n\n  # Compute the bin indices\n  indices = jnp.arange(t.shape[-1])\n\n  # Sample uniformly from the CDF\n  if rng is None:\n    u = jnp.linspace(0., 1., num_samples)\n    u = jnp.broadcast_to(u, list(cdf.shape[:-1]) + [num_samples])\n  else:\n    if single_jitter:\n      u = jax.random.uniform(rng, list(cdf.shape[:-1]) + [num_samples])\n    else:\n      u = jax.random.uniform(rng, list(cdf.shape[:-1]) + [num_samples])\n\n  # Find the bin indices corresponding to the sampled values\n  mask = (u[..., None, :] >= lower_bound) & (u[..., None, :] <= upper_bound)\n  mask = jnp.any(mask, axis=-2)\n  inds = jnp.argmax(mask, axis=-1)\n\n  # Compute the bin boundaries for the sampled indices\n  lower = jnp.take_along_axis(lower_bound, inds[..., None], axis=-1)[..., 0]\n  upper = jnp.take_along_axis(upper_bound, inds[..., None], axis=-1)[..., 0]"}
{"namespace": "stepfun.sample", "completion": "  # Determine the number of bins and the size of the last dimension of w_logits.\n  num_bins = t.shape[-1]\n  last_dim = w_logits.shape[-1]\n\n  # Compute the PDF by normalizing the weights.\n  pdf = jax.nn.softmax(w_logits)\n\n  # Compute the CDF by cumulatively summing the PDF.\n  cdf = jnp.cumsum(pdf, axis=-1)\n  cdf = jnp.concatenate([jnp.zeros(list(cdf.shape[:-1]) + [1]), cdf], axis=-1)\n\n  # Compute the bin widths.\n  delta = t[..., 1:] - t[..., :-1]\n\n  # Compute the normalization factor for the CDF.\n  normalization = cdf[..., -1:]\n\n  # Compute the bin indices for each sample.\n  if rng is None:\n    # Deterministic sampling.\n    if deterministic_center:\n      # Sample the center of each bin.\n      u = (t[..., :-1] + t[..., 1:]) / 2.0\n    else:\n      # Sample uniformly from the entire PDF.\n      u = jnp.linspace(0.0, 1.0, num_samples)\n      u = jnp.broadcast_to(u, list(t.shape[:-1]) + [num_samples])\n  else:\n    # Random sampling.\n    if single_jitter:\n      # Sample uniformly from [0, 1) and add the same value to all samples.\n      u = jax.random.uniform(rng, list(t.shape[:-1]) + [num_samples])\n      u += jax.random.uniform(rng, t.shape[:-1] + [1], minval=0.0, maxval=1.0)\n      u %= 1.0\n    else:\n      # Sample uniformly from [0, 1).\n      u = jax.random"}
{"namespace": "stepfun.sample", "completion": "  if w_logits.shape[0] + 1 != t.shape[0]:\n    raise ValueError(\n        f\"Incorrect shape: t.shape[0]={t.shape[0]} and w_logits.shape[0]={w_logits.shape[0]}.\"\n    )\n\n  pmf = jax.nn.softmax(w_logits)\n  cdf = jnp.cumsum(pmf)\n  cdf = jnp.pad(cdf, ((1, 0)), mode=\"constant\", constant_values=0)\n  cdf_delta = cdf[1:] - cdf[:-1]\n  valid_idx = jnp.logical_and(cdf_delta > 0, cdf_delta < 1)\n  t = jnp.concatenate([-jnp.ones((1,)), t[valid_idx], jnp.ones((1,))], axis=0)\n  cdf = jnp.concatenate([jnp.zeros((1,)), cdf[valid_idx], jnp.ones((1,))], axis=0)\n  pmf = jnp.concatenate([jnp.zeros((1,)), pmf[valid_idx], jnp.zeros((1,))], axis=0)\n  pmf = pmf / jnp.sum(pmf)\n  cdf = jnp.clip(cdf, a_min=0, a_max=1)\n\n  if rng is None:\n    if deterministic_center and not single_jitter:\n      raise ValueError(\n          \"Deterministic center sampling requires single jitter to be enabled.\"\n      )\n    shape = (num_samples,)\n    if deterministic_center:\n      u = jnp.linspace(0.0, 1.0, num_samples + 2)\n      u = u[1:-1:2]\n      u = jnp.broadcast_to(u, shape)\n    else:\n      u = jax.random.uniform(rng, shape)\n  else:\n    shape ="}
{"namespace": "stepfun.sample", "completion": "  # Compute the weights by applying the softmax function to the logits\n  weights = jax.nn.softmax(w_logits, axis=-1)\n\n  # Compute the cumulative sum of the weights along the last axis\n  cdf = jnp.cumsum(weights, axis=-1)\n\n  # Compute the bin boundaries by concatenating a zero at the beginning and the cumulative sum at the end\n  cdf = jnp.concatenate([jnp.zeros_like(cdf[..., :1]), cdf, jnp.ones_like(cdf[..., :1])], axis=-1)\n\n  # Compute the bin widths by subtracting adjacent elements in the cumulative sum\n  bin_widths = jnp.diff(cdf, axis=-1)\n\n  # Compute the bin centers by averaging adjacent elements in the cumulative sum\n  bin_centers = (cdf[..., 1:] + cdf[..., :-1]) / 2.0\n\n  # Compute the bin intervals by adding half the bin width to the bin centers\n  bin_intervals = bin_centers + bin_widths / 2.0\n\n  # Compute the normalization factor for the step function\n  normalization = jnp.sum(weights, axis=-1, keepdims=True) / num_samples\n\n  # Compute the sampled values using either random or deterministic sampling\n  if rng is not None:\n    # Generate random samples using the inverse transform sampling method\n    u = jax.random.uniform(rng, list(cdf.shape[:-1]) + [num_samples])\n    if single_jitter:\n      # Apply the same jitter to all samples\n      u += jax.random.uniform(rng, cdf.shape[:-1] + [1])\n    else:\n      # Apply a different jitter to each sample\n      u += jax.random.uniform(rng, cdf.shape[:-1] + [num_samples])\n    u = jnp.clip(u, eps, 1"}
{"namespace": "stepfun.sample", "completion": "  # Compute the weights and the cumulative sum of the weights\n  w = jax.nn.softmax(w_logits)\n  cdf = jnp.cumsum(w, axis=-1)\n\n  # Compute the bin boundaries for the piecewise constant PDF\n  t0 = jnp.pad(t[..., :-1], [[0, 0]] * (t.ndim - 1) + [[1, 0]], constant_values=t[..., 0])\n  t1 = jnp.pad(t[..., 1:], [[0, 0]] * (t.ndim - 1) + [[0, 1]], constant_values=t[..., -1])\n\n  # Compute the bin size for each bin\n  bin_size = t1 - t0\n\n  # Generate random samples\n  if rng is not None:\n    # Generate random samples using uniform distribution\n    u = jax.random.uniform(rng, list(cdf.shape[:-1]) + [num_samples])\n    if single_jitter:\n      # Apply the same jitter to all samples\n      u += jax.random.uniform(rng, cdf.shape[:-1])[..., None]\n    else:\n      # Apply different jitter to each sample\n      u += jax.random.uniform(rng, cdf.shape[:-1])[..., None]\n  else:\n    # Generate deterministic samples using linspace\n    if deterministic_center:\n      # Deterministically sample the center of each bin\n      u = cdf - 0.5 * w\n    else:\n      # Deterministically sample the entire bin\n      u = jnp.linspace(0.0 + eps, 1.0 - eps, num_samples)\n      if t.ndim == 1:\n        u = jnp.broadcast_to(u, cdf.shape[:-1] + (num_samples,))\n      else:\n        u = jnp.broadcast_to(u, cdf.shape[:"}
{"namespace": "stepfun.sample", "completion": "  # Compute the PDF (pmf) using the provided logits and normalize it\n  pmf = jax.nn.softmax(w_logits)\n  cdf = jnp.cumsum(pmf, axis=-1)\n  cdf = jnp.concatenate([jnp.zeros(list(cdf.shape[:-1]) + [1]), cdf], axis=-1)\n  cdf = cdf[..., :-1]\n\n  # Calculate the bin size for each interval\n  delta = t[..., 1:] - t[..., :-1]\n\n  # Prepare for sampling\n  cdf_delta = cdf[..., 1:] - cdf[..., :-1]\n  normalization_factor = cdf_delta.sum(-1, keepdims=True)\n  cdf_delta = cdf_delta / normalization_factor\n  pmf = pmf / normalization_factor\n\n  # If rng is None, use linspace instead of random sampling\n  if rng is None:\n    u = jnp.linspace(0., 1., num_samples)\n    u = jnp.broadcast_to(u, list(cdf.shape[:-1]) + [num_samples])\n  else:\n    if single_jitter:\n      # Generate a single jitter value for all samples\n      u_delta = jax.random.uniform(rng, list(cdf.shape[:-1]) + [num_samples])\n    else:\n      # Generate a unique jitter value for each sample\n      u_delta = jax.random.uniform(rng, list(cdf.shape[:-1]) + [num_samples])\n\n    # Add the jitter to the uniform samples\n    u = u_delta + cdf[..., None, :]\n\n  # Determine the indices of the bins for each sample\n  inds = jnp.searchsorted(cdf, u, side='right')\n\n  # Compute the bin boundaries for each sample\n  below = jnp.maximum(0,"}
{"namespace": "stepfun.sample", "completion": "  # Compute the bin widths\n  delta = t[:-1] - t[1:]\n\n  # Compute the cumulative sum of the weights\n  c_logits = jnp.cumsum(w_logits, axis=-1)\n\n  # Compute the normalization constant for the CDF\n  c_logits = jnp.log(jnp.exp(c_logits) / jnp.exp(c_logits[:, -1:]))\n\n  # Compute the bin probabilities\n  p = jnp.exp(c_logits)\n\n  # Compute the CDF values\n  c = jnp.concatenate([jnp.zeros_like(p[..., :1]), jnp.cumsum(p, axis=-1)], axis=-1)\n\n  # Compute the bin edges\n  edges = jnp.concatenate([t[..., :1], t + delta / 2.0, t[-1:]], axis=-1)\n\n  # Compute the normalization constant for the CDF\n  c_normalized = (c - c[..., :1]) / (c[..., -1:] - c[..., :1])\n\n  # Compute the bin widths\n  delta = edges[..., 1:] - edges[..., :-1]\n\n  # Compute the PDF values\n  p = p / (c[..., -1:] - c[..., :1])\n\n  # Compute the CDF values\n  c = jnp.minimum(c_normalized, 1.0)\n\n  # Compute the inverse CDF\n  c_inv = jnp.zeros_like(c)\n  c_inv = c_inv.at[..., 1:].set(jnp.diff(c, axis=-1) / delta)\n\n  # Compute the PDF values\n  p = jnp.zeros_like(c)\n  p = p.at[..., :-1].set(jnp.diff(c_inv, axis=-1) / delta)\n\n  # Compute the C"}
{"namespace": "stepfun.sample_intervals", "completion": "  if rng is None:\n    # Regular sampling, no randomness\n    u = jnp.linspace(0.0, 1.0, num_samples + 1)\n    if not single_jitter:\n      u += jax.random.uniform(make_rng(), shape=u.shape)\n      u %= 1.0\n    u = jnp.concatenate([u, jnp.array([0.0])])\n    return jnp.diff(jnp.interp(u, t, w_logits))\n\n  shape = t.shape[:-1] + (num_samples,)\n  u = jax.random.uniform(rng, shape)\n  if single_jitter:\n    u += jax.random.uniform(rng, ())\n    u %= 1.0\n  u = jnp.concatenate([u, jnp.array([0.0])], axis=-1)\n  return jnp.diff(jnp.interp(u, t, w_logits))\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Calculate the midpoints between adjacent bin endpoints\n  t_mid = (t[..., 1:] + t[..., :-1]) / 2\n\n  # Sample points from the step function using either 'linspace' or 'inverse CDF' sampling\n  if rng is None:\n    t_samples = jnp.linspace(t[..., 0], t[..., -1], num_samples)\n  else:\n    if single_jitter:\n      t_samples = sample_pdf(\n          rng,\n          t_mid,\n          w_logits,\n          num_samples,\n          single_jitter=True,\n          domain=domain,\n      )\n    else:\n      t_samples = sample_pdf(\n          rng,\n          t_mid,\n          w_logits,\n          num_samples,\n          single_jitter=False,\n          domain=domain,\n      )\n\n  # Calculate the midpoints between adjacent samples\n  t_samples_mid = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the domain\n  t_samples_mid = jnp.concatenate([\n      jnp.full_like(t_samples_mid[..., :1], domain[0]),\n      t_samples_mid,\n      jnp.full_like(t_samples_mid[..., :1], domain[1]),\n  ], axis=-1)\n\n  return t_samples_mid"}
{"namespace": "stepfun.sample_intervals", "completion": "  t = jnp.array(t)\n  w_logits = jnp.array(w_logits)\n\n  # Sample points from the step function\n  if rng is None:\n    # If rng is None, use linspace sampling\n    t_samples = jnp.linspace(t[0], t[-1], num_samples + 1)\n  else:\n    # Otherwise, use inverse CDF sampling\n    t_samples = inverse_cdf_sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples\n  t_mids = (t_samples[1:] + t_samples[:-1]) / 2\n\n  # Adjust the first and last intervals to fit within the specified domain\n  t_mids = jnp.concatenate([t_mids, [t[-1]]], axis=0)\n  t_mids = jnp.concatenate([[t[0]], t_mids], axis=0)\n  t_mids = jnp.clip(t_mids, domain[0], domain[1])\n\n  return t_mids\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  t = jnp.array(t)\n  w_logits = jnp.array(w_logits)\n\n  # Generate samples from the step function\n  if rng is None:\n    # If rng is None, use 'linspace' sampling\n    samples = jnp.linspace(t[0], t[-1], num_samples + 1)\n  else:\n    # Otherwise, use inverse CDF sampling\n    cdf = jnp.cumsum(jax.nn.softmax(w_logits))\n    cdf = jnp.pad(cdf, pad_width=((1, 0),), mode=\"constant\")\n    uniform_samples = jax.random.uniform(rng, shape=(num_samples,))\n    if single_jitter:\n      # If single_jitter is True, jitter every sample by the same amount\n      jitter = jax.random.uniform(rng, shape=())\n      samples = jnp.interp(uniform_samples + jitter, cdf, t)\n    else:\n      # Otherwise, jitter each sample independently\n      jitter = jax.random.uniform(rng, shape=(num_samples,))\n      samples = jnp.interp(uniform_samples + jitter, cdf, t)\n\n  # Calculate midpoints between adjacent samples\n  midpoints = 0.5 * (samples[..., 1:] + samples[..., :-1])\n\n  # Adjust the first and last intervals to fit within the specified domain\n  first_interval = jnp.array([t[0], midpoints[0]])\n  last_interval = jnp.array([midpoints[-1], t[-1]])\n  midpoints = jnp.clip(midpoints, domain[0], domain[1])\n\n  # Combine the adjusted first and last intervals with the midpoints\n  intervals = jnp.concatenate([first_interval[None], midpoints[:, None], last_interval[None]], axis=0)\n  intervals = intervals.clip(*domain)\n\n  return intervals\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Compute the midpoints between adjacent samples\n  t_mid = (t[..., :-1] + t[..., 1:]) / 2\n\n  # If rng is None, use 'linspace' sampling\n  if rng is None:\n    t_samples = jnp.linspace(t[..., 0], t[..., -1], num_samples)\n  else:\n    # Otherwise, use 'sample_along_step_f' to sample from the step function\n    t_samples = sample_along_step_f(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate the midpoints between adjacent samples\n  t_samples_mid = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first interval to fit within the domain\n  t_samples = jnp.concatenate([t_samples[..., :1], t_samples_mid], axis=-1)\n  t_samples = jnp.clip(t_samples, a_min=domain[0], a_max=domain[1])\n\n  # Adjust the last interval to fit within the domain\n  t_samples = jnp.concatenate([t_samples_mid, t_samples[..., -1:]], axis=-1)\n  t_samples = jnp.clip(t_samples, a_min=domain[0], a_max=domain[1])\n\n  return t_samples\n\n\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Calculate the midpoints between adjacent bin endpoints\n  t_mid = (t[..., 1:] + t[..., :-1]) / 2\n\n  # Calculate the weights of the midpoints\n  w_logits_mid = (w_logits[..., 1:] + w_logits[..., :-1]) / 2\n\n  # Calculate the CDF of the midpoints\n  cdf = jax.nn.softmax(w_logits_mid, axis=-1)\n\n  # Calculate the CDF of the left endpoint\n  cdf_left = jnp.zeros_like(cdf[..., :1])\n\n  # Calculate the CDF of the right endpoint\n  cdf_right = jnp.ones_like(cdf[..., -1:])\n\n  # Concatenate the CDFs\n  cdf = jnp.concatenate([cdf_left, cdf, cdf_right], axis=-1)\n\n  # Generate random samples from the CDF\n  if rng is not None:\n    u = jax.random.uniform(rng, list(cdf.shape[:-1]) + [num_samples])\n  else:\n    u = jnp.linspace(0., 1., num_samples)\n\n  # Calculate the inverse CDF of the samples\n  if single_jitter:\n    u = jnp.expand_dims(u, axis=-1)\n    inds = jnp.argmax(cdf > u, axis=-1) - 1\n    inds = jnp.clip(inds, a_min=0, a_max=cdf.shape[-1] - 1)\n    t_samples = jnp.take_along_axis(t_mid, inds[..., None], axis=-1)[..., 0]\n  else:\n    t_samples = jnp.interp(u, cdf, t_mid)\n\n  # Sort the samples\n  t_samples = jnp.sort(t_samples,"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Calculate the midpoints between adjacent bin endpoints\n  midpoints = (t[..., 1:] + t[..., :-1]) / 2\n\n  # Calculate the weights of the midpoints\n  w_logits_mid = (w_logits[..., 1:] + w_logits[..., :-1]) / 2\n\n  # If rng is None, use 'linspace' sampling\n  if rng is None:\n    t_samples = jnp.linspace(t[..., 0], t[..., -1], num_samples + 1)\n  else:\n    # Sample 'num_samples' points from the step function using inverse transform sampling\n    t_samples = inverse_transform_sampling(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate the midpoints between adjacent samples\n  midpoints_samples = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Calculate the weights of the midpoints of the samples\n  w_logits_mid_samples = (w_logits[..., 1:] + w_logits[..., :-1]) / 2\n\n  # Calculate the weights of the intervals\n  w_intervals = w_logits_mid_samples / w_logits_mid\n\n  # Calculate the intervals\n  intervals = midpoints_samples[..., 1:] - midpoints_samples[..., :-1]\n\n  # Adjust the first and last intervals to fit within the domain\n  intervals = jnp.concatenate([\n      intervals[..., :1] + midpoints_samples[..., :1] - domain[0],\n      intervals,\n      intervals[..., -1:] + domain[1] - midpoints_samples[..., -1:]\n  ], axis=-1)\n\n  # Return the intervals\n  return intervals\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  t_min, t_max = domain\n\n  if rng is None:\n    # sample uniformly from bins\n    t_samples = jnp.linspace(t_min, t_max, num_samples + 1)\n    w_samples = jnp.ones_like(t_samples)\n  else:\n    # sample from step function\n    t_samples = sample_from_step(rng, t, w_logits, num_samples, single_jitter)\n    w_samples = jnp.diff(jnp.concatenate([jnp.zeros((1,)), w_logits]))\n\n  # calculate midpoints\n  t_mids = 0.5 * (t_samples[1:] + t_samples[:-1])\n\n  # adjust first and last intervals to fit domain\n  t_mids = jnp.concatenate([\n    jnp.array([t_min]),\n    t_mids,\n    jnp.array([t_max]),\n  ])\n  w_samples = jnp.concatenate([\n    jnp.array([w_samples[0]]),\n    w_samples,\n    jnp.array([w_samples[-1]]),\n  ])\n\n  return t_mids, w_samples\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Convert the logits to weights\n  w = jax.nn.softmax(w_logits)\n\n  # Calculate the cumulative sum of the weights\n  cdf = jnp.cumsum(w, axis=-1)\n\n  # Normalize the CDF to the [0, 1] range\n  cdf = cdf / cdf[..., -1:]\n\n  # Generate 'num_samples' samples from the step function\n  if rng is not None:\n    # If 'rng' is provided, use it to generate random samples\n    u = random.uniform(rng, shape=list(w.shape[:-1]) + [num_samples])\n    if single_jitter:\n      # If 'single_jitter' is True, jitter every sample by the same amount\n      u = u + jnp.linspace(0.0, 1.0 - 1.0 / num_samples, num_samples)\n      u = (u % 1.0).reshape(u.shape[:-1] + [1, num_samples])\n    else:\n      # If 'single_jitter' is False, jitter each sample independently in the inverse CDF\n      u = jnp.interp(u, cdf, jnp.linspace(0.0, 1.0, w.shape[-1]))\n  else:\n    # If 'rng' is None, generate samples using 'linspace'\n    u = jnp.linspace(1.0 / num_samples / 2, 1.0 - 1.0 / num_samples / 2, num_samples)\n    u = u.reshape(u.shape + (1,)*len(w.shape[:-1]))\n\n  # Find the indices of the samples in the CDF\n  inds = jnp.searchsorted(cdf, u, side='right')\n\n  # Clamp the indices to the valid range\n  inds = jnp.clip(inds, 0, w.shape[-1] - 1)\n\n  # Calculate the lower and"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Get the bin midpoints\n  t_mid = (t[..., 1:] + t[..., :-1]) / 2\n\n  # Calculate the CDF of the step function\n  cdf = jax.nn.softmax(w_logits, axis=-1)\n  cdf = jnp.cumsum(cdf, axis=-1)\n  cdf = jnp.pad(cdf, ((0, 0), (1, 0)), mode='constant', constant_values=0)\n\n  # Calculate the inverse CDF\n  inv_cdf = jax.lax.inverse_permutation(jnp.argsort(cdf, axis=-1))\n\n  # Sample points from the step function\n  if rng is not None:\n    # Generate random samples\n    u = jax.random.uniform(rng, shape=(num_samples,))\n    if single_jitter:\n      # Jitter all samples by the same amount\n      jitter = jax.random.uniform(rng, shape=())\n      u = u + jitter\n    else:\n      # Jitter each sample independently\n      jitter = jax.random.uniform(rng, shape=(num_samples,))\n      u = u + jitter\n    u = u[..., None]\n    # Sample points from the inverse CDF\n    indices = jnp.argmax(u > inv_cdf, axis=-1)\n  else:\n    # Generate equidistant samples\n    indices = jnp.linspace(0, num_samples, num_samples + 1).astype(int)\n\n  # Calculate the midpoints of the sampled intervals\n  t_samples = jnp.take_along_axis(t_mid, indices[None], axis=-1)[0]\n\n  # Adjust the first and last intervals to fit within the specified domain\n  t_samples = jnp.concatenate([jnp.full_like(t_samples[:1], domain[0]), t_samples, jnp.full_like(t"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Compute the weights of each bin in the step function\n  w = jax.nn.softmax(w_logits)\n\n  # Calculate the midpoints between the bin endpoints\n  midpoints = (t[..., 1:] + t[..., :-1]) / 2\n\n  # Generate samples from the step function using either 'linspace' or random sampling\n  if rng is None:\n    # Use 'linspace' sampling\n    samples = jnp.linspace(0, 1, num_samples)\n  else:\n    # Use random sampling\n    samples = jax.random.uniform(rng, shape=(num_samples,))\n\n  # Calculate the cumulative sum of the bin weights\n  cdf = jnp.cumsum(w, axis=-1)\n  cdf = jnp.concatenate([jnp.zeros(list(cdf.shape[:-1]) + [1]), cdf], axis=-1)\n\n  # Calculate the inverse CDF of the samples\n  if single_jitter:\n    # Apply the same jitter to all samples\n    u = jnp.clip(samples[..., None] + jnp.arange(t.shape[-1]) / num_samples, 0, 1)\n  else:\n    # Apply a different jitter to each sample\n    u = jnp.clip(samples[..., None] + jnp.linspace(0, 1, t.shape[-1]) / num_samples, 0, 1)\n\n  # Find the indices of the samples in the CDF\n  inds = jnp.searchsorted(cdf, u, side='right') - 1\n\n  # Calculate the bin ranges for the samples\n  below = jnp.clip(inds - 1, 0, t.shape[-1] - 1)\n  above = jnp.clip(inds, 0, t.shape[-1] - 1)\n  inds_g = jnp.stack([below, above], axis=-1)\n\n  # Calculate"}
{"namespace": "stepfun.sample_intervals", "completion": "  t_min, t_max = domain\n\n  # Compute the weights of each bin in the step function\n  w = jax.nn.softmax(w_logits, axis=-1)\n\n  # Compute the midpoints between adjacent bins in the step function\n  t_mid = (t[..., 1:] + t[..., :-1]) / 2\n\n  # Compute the CDF of the step function\n  cdf = jnp.cumsum(w, axis=-1)\n  cdf = jnp.pad(cdf, ((0, 0), (1, 0)), mode='constant', constant_values=0)\n\n  # Compute the inverse CDF of the step function\n  inv_cdf = jax.lax.cumsum(w / cdf[..., :-1], axis=-1, reverse=True)\n\n  # Sample points from the step function\n  if rng is None:\n    u = jnp.linspace(0., 1., num_samples)\n    u = jnp.broadcast_to(u, t_mid.shape[:-1] + (num_samples,))\n  else:\n    if single_jitter:\n      u_delta = jax.random.uniform(rng, t_mid.shape[:-1] + (1,)) * (1. / num_samples)\n      u = jnp.linspace(0., 1., num_samples) + u_delta\n    else:\n      u = jax.random.uniform(rng, t_mid.shape[:-1] + (num_samples,))\n    u = jnp.clip(u, 0., 1.)\n\n  # Find the indices of the bins where each sample falls\n  inds = jnp.searchsorted(cdf, u, side='right') - 1\n\n  # Compute the midpoints of the bins where each sample falls\n  t_samples = jnp.clip(\n      jnp.take_along_axis(t_mid, inds, axis=-1), t_"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Calculate midpoints between adjacent samples\n  midpoints = (t[:-1] + t[1:]) / 2\n\n  # Adjust the first and last intervals to fit within the domain\n  first_interval = t[0] + (midpoints[0] - t[0]) * (t[0] < domain[0])\n  last_interval = t[-1] + (midpoints[-1] - t[-1]) * (t[-1] > domain[1])\n\n  # Combine the first interval, midpoints, and last interval into a single array\n  t = jnp.concatenate([jnp.array([first_interval]), midpoints, jnp.array([last_interval])], axis=0)\n\n  # Calculate the weights of the intervals\n  w_logits = jnp.log(t[1:] - t[:-1]) - jnp.log(jnp.concatenate([w_logits[1:], jnp.array([0.0])]))\n\n  # Normalize the weights to sum to 1\n  w_logits = w_logits - jax.scipy.special.logsumexp(w_logits)\n\n  # Sample interval indices using the weights\n  if rng is None:\n    # If rng is None, use linspace sampling\n    u = jnp.linspace(0.0, 1.0, num_samples + 1)[:-1]\n  else:\n    # Otherwise, use inverse transform sampling\n    if single_jitter:\n      # If single_jitter is True, jitter the entire set of u values by the same amount\n      u_jitter = jax.random.uniform(rng, shape=[num_samples + 1])\n      u = jnp.linspace(0.0, 1.0, num_samples + 1)[:-1] + u_jitter\n    else:\n      # Otherwise, jitter each u value independently\n      u = jax.random.uniform(rng, shape=[num_samples + 1])\n    u = jnp.clip(u, 0.0, 1"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Get the step function values and bin endpoints\n  w = jax.nn.softmax(w_logits)\n  t0, t1 = t[..., :-1], t[..., 1:]\n\n  # Sample points from the step function\n  if rng is None:\n    # If no random number generator is provided, use 'linspace' sampling\n    u = jnp.linspace(0., 1., num_samples + 1)\n  else:\n    # Otherwise, use inverse CDF sampling\n    if single_jitter:\n      # If single_jitter is True, jitter all samples by the same amount\n      u_jitter = jax.random.uniform(rng, (1,))\n      u = jnp.linspace(0., 1., num_samples + 1) + u_jitter\n    else:\n      # Otherwise, jitter each sample independently in the inverse CDF\n      u = jax.random.uniform(rng, (num_samples + 1,))\n    u = jnp.clip(u, 1e-5, 1. - 1e-5)\n    u = jnp.sort(u)\n\n  # Calculate the midpoints between adjacent samples\n  t_mids = .5 * (t0[..., :, None] + t1[..., :, None])\n\n  # Calculate the CDF values for each midpoint\n  cdf = jnp.clip(jnp.cumsum(w[..., :-1], axis=-1), 0, 1)\n\n  # Calculate the inverse CDF values for each midpoint\n  cdf = jnp.concatenate([jnp.zeros(list(cdf.shape[:-1]) + [1]), cdf, jnp.ones(list(cdf.shape[:-1]) + [1])], axis=-1)\n  inv_cdf = jax.lax.stop_gradient(jnp.interp(u, cdf, t_mids))\n\n  # Calculate the midpoints between the inverse CDF samples\n  t_m"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Calculate the midpoints between adjacent samples\n  midpoints = (t[..., 1:] + t[..., :-1]) / 2\n\n  # Add the midpoints to the sample points\n  samples = jnp.concatenate([t[..., :1], midpoints, t[..., -1:]], axis=-1)\n\n  # Adjust the first and last intervals to fit within the specified domain\n  samples = jnp.where(samples < domain[0], domain[0], samples)\n  samples = jnp.where(samples > domain[1], domain[1], samples)\n\n  # Calculate the weights of each bin\n  weights = jax.nn.softmax(w_logits)\n\n  # Calculate the CDF of the weights\n  cdf = jnp.cumsum(weights, axis=-1)\n\n  # Normalize the CDF\n  cdf = cdf / cdf[..., -1:]\n\n  # Calculate the inverse CDF\n  cdf_inv = jnp.clip(1.0 / cdf, 0.0, 1.0)\n\n  # Generate uniform samples\n  if rng is None:\n    u = jnp.linspace(0.0, 1.0, num_samples)\n  else:\n    u = jax.random.uniform(rng, shape=(num_samples,))\n\n  # Sample the intervals\n  if single_jitter:\n    # If single_jitter is True, jitter every sample by the same amount\n    jitter = jax.random.uniform(rng, shape=(1,))\n    u = u + jitter\n  else:\n    # If single_jitter is False, jitter each sample independently in the inverse CDF\n    u = u + jax.random.uniform(rng, shape=(num_samples,)) / num_samples\n\n  # Find the indices of the samples in the CDF\n  idx = jnp.searchsorted(cdf, u, side='right')\n\n  # Clip the indices to the valid range"}
{"namespace": "stepfun.sample_intervals", "completion": "  if rng is None:\n    # Sample linearly spaced intervals\n    t_samples = jnp.linspace(t[0], t[-1], num_samples + 1)\n    w_samples = jax.nn.softmax(w_logits)\n    w_samples = w_samples / jnp.diff(t)[..., None]\n    w_samples = w_samples.sum(axis=-1)\n    w_samples = w_samples / w_samples.sum()\n    w_samples = jnp.concatenate([jnp.array([0.0]), w_samples, jnp.array([0.0])])\n    cdf = jnp.cumsum(w_samples)\n    cdf = jnp.clip(cdf, 0.0, 1.0)\n    assert jnp.all(jnp.diff(cdf) >= 0.0)\n    return cdf\n  else:\n    # Sample non-linearly spaced intervals\n    t_samples = inverse_transform_sampling(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints\n  t_samples_mid = 0.5 * (t_samples[..., 1:] + t_samples[..., :-1])\n\n  # Padding\n  t_samples_mid = jnp.concatenate([t[0:1], t_samples_mid, t[-1:]], axis=0)\n\n  # Clip to domain\n  t_samples_mid = jnp.clip(t_samples_mid, domain[0], domain[1])\n\n  # Return intervals\n  return t_samples_mid\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Get the bin weights from logits\n  w = jax.nn.softmax(w_logits)\n\n  # Calculate the midpoints between adjacent bin endpoints\n  midpoints = (t[..., 1:] + t[..., :-1]) / 2\n\n  # If the domain is (-inf, inf), we can sample directly from the inverse CDF\n  if domain == (-jnp.inf, jnp.inf):\n    # Calculate the CDF of the step function\n    cdf = jnp.cumsum(w, axis=-1)\n    cdf = jnp.insert(cdf, 0, 0, axis=-1)\n\n    # Sample from the inverse CDF\n    if single_jitter:\n      u = jax.random.uniform(rng, shape=cdf.shape[:-1] + (num_samples,))\n      # Push u to the right to get the [u, u+1] interval\n      u = jnp.expand_dims(u, -1)\n      idx = jnp.searchsorted(cdf, u, side='right')\n      t_samples = t.take(idx, axis=-1)\n    else:\n      # Sample from the inverse CDF\n      u = jax.random.uniform(rng, shape=cdf.shape[:-1] + (num_samples,))\n      u = jnp.expand_dims(u, -1)\n      idx = jnp.searchsorted(cdf, u, side='right') - 1\n      t_samples = jnp.clip(t.take(idx, axis=-1) + jax.random.uniform(rng, shape=idx.shape), t[..., :-1], t[..., 1:])\n\n  # Otherwise, we need to sample from the step function and then adjust the samples\n  else:\n    # Sample from the step function\n    if rng is None:\n      # If rng is None, use linspace sampling\n      t_samples = jnp.linspace(domain[0], domain[1"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Check if 't' is sorted\n  if not jnp.all(jnp.diff(t) > 0):\n    raise ValueError(\"'t' must be sorted in ascending order.\")\n\n  # Check if 'domain' is valid\n  if domain[0] >= domain[1]:\n    raise ValueError(\"Invalid 'domain' specified. 'domain[0]' must be less than 'domain[1]'.\")\n\n  # Calculate midpoints between adjacent 't' values\n  t_mid = (t[..., :-1] + t[..., 1:]) / 2\n\n  # Generate random samples from the step function\n  if rng is None:\n    # Use 'linspace' sampling if 'rng' is None\n    u = jnp.linspace(0., 1., num_samples)\n  else:\n    # Use 'uniform' sampling if 'rng' is provided\n    u = random.uniform(rng, (num_samples,))\n\n  # Apply inverse CDF sampling\n  if single_jitter:\n    # Apply the same jitter to all samples\n    cdf_delta = 1e-5\n    u_ = jnp.clip(u + cdf_delta * random.normal(rng, u.shape), 0., 1.)\n    t_ = jnp.interp(u_, jnp.clip(w_logits, 0., 1.), t_mid)\n  else:\n    # Apply independent jitter to each sample\n    inds = jnp.searchsorted(jnp.cumsum(jnp.exp(w_logits)), u)\n    t0 = jnp.take(t[:-1], inds, mode='clip')\n    t1 = jnp.take(t[1:], inds, mode='clip')\n    t_ = random.uniform(rng, t0.shape) * (t1 - t0) + t0\n\n  # Sort the samples\n  t_ = jnp.sort(t_)\n\n  # Adjust the first and last intervals to fit within the specified domain\n  t_ = j"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Calculate the midpoints between adjacent bin endpoints\n  midpoints = (t[..., 1:] + t[..., :-1]) / 2\n\n  # Determine the range of values for 't'\n  t_min, t_max = domain\n\n  # Calculate the bin widths\n  bin_widths = t_max - t_min\n\n  # Calculate the bin probabilities using softmax\n  bin_probs = jax.nn.softmax(w_logits)\n\n  # Calculate the cumulative bin probabilities\n  cum_bin_probs = jnp.cumsum(bin_probs, axis=-1)\n\n  # Calculate the cumulative bin probabilities normalized to [0, 1]\n  cum_bin_probs_norm = cum_bin_probs / cum_bin_probs[..., -1:]\n\n  # Calculate the bin widths for each bin\n  bin_widths = t_max - t_min\n\n  # Calculate the midpoints of each bin\n  midpoints = (t[..., 1:] + t[..., :-1]) / 2\n\n  # Calculate the normalized cumulative bin widths\n  cum_bin_widths_norm = jnp.cumsum(bin_widths * bin_probs, axis=-1) / cum_bin_probs[..., -1:]\n\n  # Calculate the normalized cumulative bin widths normalized to [0, 1]\n  cum_bin_widths_norm_norm = cum_bin_widths_norm / cum_bin_widths_norm[..., -1:]\n\n  # Generate random samples from the step function\n  if rng is None:\n    # If 'rng' is None, use 'linspace' sampling\n    u = jnp.linspace(0., 1., num_samples)\n  else:\n    # Otherwise, use 'uniform' sampling\n    u = jax.random.uniform(rng, shape=(num_samples,))\n\n  # Calculate the inverse CDF of the step function\n  t_invcd"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Calculate the midpoints between adjacent samples\n  midpoints = (t[..., 1:] + t[..., :-1]) / 2\n\n  # Add midpoints to the endpoints to form the complete set of samples\n  t_samples = jnp.concatenate([t[..., :1], midpoints, t[..., -1:]], axis=-1)\n\n  # Calculate the weights of each sample\n  w_samples = jax.nn.softmax(w_logits, axis=-1)\n\n  # Calculate the CDF of the samples\n  cdf = jnp.cumsum(w_samples[..., :-1], axis=-1)\n  cdf = jnp.concatenate([jnp.zeros(list(cdf.shape[:-1]) + [1]), cdf], axis=-1)\n\n  # Add a small epsilon value to the CDF to ensure numerical stability\n  eps = jnp.finfo(jax.lax.dtype(t_samples)).eps\n  cdf = jnp.clip(cdf, eps, 1.0 - eps)\n\n  # Normalize the CDF to the range [0, 1]\n  cdf = cdf / cdf[..., -1:]\n\n  # Sample 'num_samples' intervals from the CDF\n  if rng is not None:\n    u = jax.random.uniform(rng, list(cdf.shape[:-1]) + [num_samples])\n  else:\n    u = jnp.linspace(0., 1., num_samples)\n\n  # Use inverse transform sampling to obtain the indices of the samples\n  inds = jnp.searchsorted(cdf, u, side='right')\n\n  # Clamp the indices to ensure they are within the valid range\n  inds = jnp.clip(inds, 0, cdf.shape[-1] - 1)\n\n  # Get the samples from the indices\n  t_samples = jnp.take_along_axis(t_s"}
{"namespace": "stepfun.weighted_percentile", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  assert np.abs(np.sum(w) - 1.0) < 1e-6, 'weights do not sum to 1 (%f)' % np.sum(w)\n  y = np.cumsum(w)\n  y /= y[-1]\n  f = interp1d(y, t)\n  return f(ps/100.0)"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Calculate the cumulative weights\n  cw = np.cumsum(w)\n\n  # Normalize the cumulative weights\n  cw /= cw[-1]\n\n  # Interpolate to find the x-values corresponding to the given percentiles\n  px = np.interp(ps / 100, cw, t)\n\n  return px"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  assert np.isclose(np.sum(w), 1)\n\n  # Compute the cumulative sum of the weights\n  w_cumsum = np.cumsum(w)\n\n  # Interpolate the cumulative sum of the weights at the given percentiles\n  ps_norm = ps / 100\n  w_cumsum_interp = np.interp(ps_norm, w_cumsum, t)\n\n  return w_cumsum_interp\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Check if the weights sum to 1\n  if not np.isclose(np.sum(w), 1):\n    raise ValueError(\"Weights must sum to 1.\")\n\n  # Sort the values and weights\n  idx = np.argsort(t)\n  t = t[idx]\n  w = w[idx]\n\n  # Compute the cumulative weights\n  cw = np.cumsum(w)\n\n  # Compute the percentiles\n  ps = np.array(ps) / 100.0\n  ps = np.clip(ps, 0, 1)\n  ps = np.interp(ps, cw, t)\n\n  return ps"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  assert abs(sum(w) - 1.0) < 1e-6, \"Weights do not sum to 1\"\n\n  # Sort the data and weights by the values in t\n  ind_sort = np.argsort(t)\n  t_sorted = t[ind_sort]\n  w_sorted = w[ind_sort]\n\n  # Compute the cumulative sum of the weights\n  w_cumsum = np.cumsum(w_sorted)\n\n  # Interpolate the cumulative sum of the weights to find the values at the given percentiles\n  ps_normed = ps / 100\n  w_interp = np.interp(ps_normed, w_cumsum, t_sorted)\n\n  return w_interp"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  assert abs(sum(w) - 1.0) < 1e-6, \"weights do not sum to 1\"\n\n  # Sort the input arrays\n  ind_sort = np.argsort(t)\n  t_sorted = t[ind_sort]\n  w_sorted = w[ind_sort]\n\n  # Compute the cumulative sum of the weights\n  w_cumsum = np.cumsum(w_sorted)\n\n  # Interpolate the cumulative sum of the weights at the given percentiles\n  ps_normed = ps / 100.0\n  t_interp = np.interp(ps_normed, w_cumsum, t_sorted)\n\n  return t_interp\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Sort the values and weights in ascending order\n  t, w = zip(*sorted(zip(t, w)))\n\n  # Calculate the cumulative sum of the weights\n  w_cumsum = np.cumsum(w)\n\n  # Normalize the cumulative sum to ensure it sums to 1\n  w_cumsum /= w_cumsum[-1]\n\n  # Interpolate the percentiles using the cumulative sum of the weights as the x-values\n  return np.interp(ps / 100, w_cumsum, t)"}
{"namespace": "stepfun.weighted_percentile", "completion": "  t = np.asarray(t)\n  w = np.asarray(w)\n  ps = np.asarray(ps)\n\n  assert np.all(np.diff(t) >= 0), \"t must be sorted\"\n  assert np.allclose(w.sum(), 1), \"weights must sum to 1\"\n\n  w = np.cumsum(w)\n  w /= w[-1]\n\n  pcts = np.interp(ps / 100, w, t)\n  return pcts"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  assert np.isclose(w.sum(), 1), \"Weights must sum to 1\"\n\n  # Sort the values and weights\n  idx = np.argsort(t)\n  t = t[idx]\n  w = w[idx]\n\n  # Compute the cumulative weights\n  w_cum = np.cumsum(w)\n\n  # Compute the percentiles of the cumulative weights\n  ps_cum = np.interp(ps, w_cum * 100, np.arange(len(w_cum)))\n\n  # Interpolate the values at the percentiles of the cumulative weights\n  return np.interp(ps_cum, np.arange(len(t)), t)\n\n\n\n\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  w = np.asarray(w)\n  assert np.isclose(w.sum(), 1), \"Weights must sum to 1\"\n\n  # Sort the values and weights in ascending order\n  ind_sort = np.argsort(t)\n  t_sorted = t[ind_sort]\n  w_sorted = w[ind_sort]\n\n  # Compute the cumulative sum of the weights\n  w_cumsum = np.cumsum(w_sorted)\n\n  # Interpolate into the integrated weights to find the values corresponding to the given percentiles\n  ps = np.asarray(ps)\n  ps_norm = ps / 100\n  t_interp = np.interp(ps_norm, w_cumsum, t_sorted)\n\n  return t_interp\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  assert abs(sum(w) - 1.0) < 1e-6, \"weights do not sum to 1\"\n  assert min(w) >= 0, \"weights have a minimum below 0\"\n\n  w_sort = np.sort(w)\n  w_integral = np.cumsum(w_sort)\n  ps_normed = ps / 100\n\n  t_sorted = t[np.argsort(w)]\n  p_t = np.interp(ps_normed, w_integral, t_sorted)\n\n  return p_t"}
{"namespace": "stepfun.weighted_percentile", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  t = np.asarray(t)\n  w = np.asarray(w)\n  ps = np.asarray(ps)\n\n  # Ensure that the weights sum to 1\n  assert np.isclose(w.sum(), 1), \"Weights must sum to 1\"\n\n  # Sort the values and weights\n  ind_sort = np.argsort(t)\n  t_sorted = t[ind_sort]\n  w_sorted = w[ind_sort]\n\n  # Compute the cumulative sum of the weights\n  w_cumsum = np.cumsum(w_sorted)\n\n  # Normalize the percentiles to the range [0, 1]\n  ps_norm = ps / 100\n\n  # Interpolate the cumulative sum of the weights to find the corresponding values\n  t_interp = interp1d(w_cumsum, t_sorted, kind='linear', bounds_error=False, fill_value=(t_sorted[0], t_sorted[-1]))\n  t_ps = t_interp(ps_norm)\n\n  return t_ps\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Ensure that the weights sum to 1\n  w = np.array(w) / np.sum(w)\n\n  # Sort the values and weights based on the values\n  ind_sort = np.argsort(t)\n  t_sorted = t[ind_sort]\n  w_sorted = w[ind_sort]\n\n  # Calculate the cumulative sum of the sorted weights\n  w_cumsum = np.cumsum(w_sorted)\n\n  # Create an interpolation function for the cumulative sum of the sorted weights\n  f = interp1d(t_sorted, w_cumsum, kind='linear', fill_value='extrapolate')\n\n  # Calculate the percentiles using the interpolated cumulative sum\n  percentiles = f(ps / 100)\n\n  return percentiles\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  w = np.array(w) / np.sum(w)\n\n  # Create a cumulative sum of the weights\n  cw = np.cumsum(w)\n\n  # Create a function to interpolate the cumulative sum of the weights\n  cw_interp = interp1d(t, cw, kind='linear', bounds_error=False, fill_value=(0, 1))\n\n  # Compute the percentiles by interpolating into the cumulative sum of the weights\n  percentiles = cw_interp(np.array(ps) / 100)\n\n  return percentiles\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  assert np.all(np.diff(t) >= 0), 't must be strictly increasing'\n  assert np.all(w >= 0), 'w must be non-negative'\n  assert np.isclose(np.sum(w), 1), 'w must sum to 1'\n  assert np.all(ps >= 0) and np.all(ps <= 100), 'ps must be between 0 and 100'\n\n  # Sort t and w by t\n  t, w = np.array(t), np.array(w)\n  idx = np.argsort(t)\n  t, w = t[idx], w[idx]\n\n  # Compute the cumulative weights\n  cw = np.cumsum(w)\n\n  # Compute the weighted percentiles by interpolating into the integrated weights\n  ps = np.array(ps) / 100\n  wp = np.interp(ps, cw, t)\n\n  return wp\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  assert np.allclose(w.sum(), 1)\n  w_int = np.cumsum(w)\n  w_int /= w_int[-1]\n  return np.interp(ps / 100, w_int, t)\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  t = np.asarray(t)\n  w = np.asarray(w)\n  ps = np.asarray(ps)\n\n  if np.sum(w) != 1:\n    w = w / np.sum(w)\n\n  if not np.all(np.diff(t) >= 0):\n    idx = np.argsort(t)\n    t = t[idx]\n    w = w[idx]\n\n  cw = np.cumsum(w)\n  cw = cw / cw[-1]\n  cw = np.concatenate([[0], cw])\n  tw = np.concatenate([[t[0]], t])\n\n  return np.interp(ps / 100, cw, tw)\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  assert np.isclose(np.sum(w), 1), \"The weights must sum to 1.\"\n\n  # Compute the cumulative weights\n  cw = np.cumsum(w)\n\n  # Interpolate the cumulative weights to find the values at the given percentiles\n  percentiles = np.interp(ps, cw, t)\n\n  return percentiles\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  t, w = np.asarray(t), np.asarray(w)\n  if t.shape != w.shape:\n    raise ValueError(\"`t` and `w` must have the same shape.\")\n  if t.ndim != 1:\n    raise ValueError(\"`t` and `w` must be 1-dimensional.\")\n  if not np.all(np.diff(t) >= 0):\n    raise ValueError(\"`t` must be strictly increasing.\")\n  if not np.isclose(w.sum(), 1):\n    raise ValueError(\"`w` must sum to 1.\")\n  if np.any(ps < 0) or np.any(ps > 100):\n    raise ValueError(\"All percentiles must be between 0 and 100.\")\n\n  # Sort the values and weights in ascending order\n  idx = np.argsort(t)\n  t, w = t[idx], w[idx]\n\n  # Compute the cumulative weights\n  cw = np.cumsum(w)\n\n  # Normalize the percentiles to the range [0, 1]\n  ps = np.asarray(ps) / 100\n\n  # Interpolate the percentiles using the cumulative weights\n  p_interp = np.interp(ps, cw, t)\n\n  return p_interp\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  assert np.allclose(np.sum(w), 1), \"Weights must sum to 1\"\n\n  # Sort the values and weights\n  idx = np.argsort(t)\n  t = t[idx]\n  w = w[idx]\n\n  # Compute the cumulative sum of the weights\n  w_cumsum = np.cumsum(w)\n\n  # Compute the percentiles of the cumulative sum of the weights\n  w_cumsum_percentiles = np.interp(ps, w_cumsum * 100, np.arange(len(t)))\n\n  # Interpolate the original values at the computed percentiles\n  weighted_percentiles = interp1d(np.arange(len(t)), t)(w_cumsum_percentiles)\n\n  return weighted_percentiles\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.stats import norm\n\n  # Convert the histogram to a probability density function (PDF)\n  pdf = np.zeros_like(tq)\n  for i in range(len(t)):\n    pdf += w[i] * norm.pdf(tq, loc=t[i], scale=blur_halfwidth)\n\n  # Resample the PDF to match the new time points\n  resampled_weights = np.interp(tq, tq, pdf)\n\n  return resampled_weights\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  pdf = hist_to_pdf(t, w)\n\n  # Blur the PDF\n  pdf_blurred = blur_pdf(pdf, blur_halfwidth)\n\n  # Resample the PDF to match the new time points\n  w_blurred = resample_pdf(tq, pdf_blurred)\n\n  return w_blurred"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = w / np.sum(w)\n\n  # Create a Gaussian kernel with the specified half-width\n  kernel = np.exp(-0.5 * np.linspace(-blur_halfwidth, blur_halfwidth, 2 * blur_halfwidth + 1)**2)\n\n  # Convolve the PDF with the Gaussian kernel\n  blurred_pdf = np.convolve(pdf, kernel, mode='same')\n\n  # Normalize the blurred PDF\n  blurred_pdf /= np.sum(blurred_pdf)\n\n  # Resample the blurred PDF to match the new time points\n  resampled_weights = np.interp(tq, t, blurred_pdf)\n\n  return resampled_weights"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.stats import norm\n  from scipy.interpolate import interp1d\n\n  # Compute the PDF\n  pdf = np.zeros_like(tq)\n  for i in range(len(t)):\n    pdf += w[i] * norm.pdf(tq, loc=t[i], scale=blur_halfwidth)\n\n  # Interpolate the PDF\n  f = interp1d(tq, pdf, kind='linear', fill_value='extrapolate')\n\n  # Resample the PDF\n  resampled_pdf = f(tq)\n\n  # Normalize the resampled PDF\n  resampled_pdf /= np.sum(resampled_pdf)\n\n  return resampled_pdf"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  pdf = w / np.sum(w)\n\n  # Create a Gaussian kernel for blurring\n  blur_kernel = np.exp(-0.5 * (np.arange(-blur_halfwidth, blur_halfwidth + 1) / blur_halfwidth) ** 2)\n  blur_kernel /= np.sum(blur_kernel)\n\n  # Convolve the PDF with the blur kernel\n  blurred_pdf = np.convolve(pdf, blur_kernel, mode='same')\n\n  # Resample the blurred PDF to match the new time points\n  resampled_weights = np.interp(tq, t, blurred_pdf)\n\n  return resampled_weights\n\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  from scipy.stats import norm\n  import numpy as np\n\n  # Convert the histogram to a PDF\n  pdf = np.zeros(tq.shape)\n  for i in range(len(t)):\n    pdf += w[i] * norm.pdf(tq, loc=t[i], scale=blur_halfwidth)\n\n  # Resample the PDF to match the new time points\n  resampled_weights = np.interp(tq, tq, pdf)\n\n  return resampled_weights"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = w / np.trapz(w, t)\n\n  # Blur the PDF using a Gaussian kernel\n  blur_kernel = gaussian_kernel(len(t), blur_halfwidth)\n  blurred_pdf = np.convolve(pdf, blur_kernel, mode='same')\n\n  # Resample the blurred PDF to match the new time points\n  resampled_weights = np.interp(tq, t, blurred_pdf)\n\n  return resampled_weights\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert histogram to PDF\n  pdf = w / np.sum(w)\n\n  # Create Gaussian kernel\n  kernel = np.exp(-np.arange(-5*blur_halfwidth, 5*blur_halfwidth+1)**2 / (2*blur_halfwidth**2))\n  kernel = kernel / np.sum(kernel)\n\n  # Convolve PDF with Gaussian kernel\n  pdf_blurred = np.convolve(pdf, kernel, mode='same')\n\n  # Resample PDF to match new time points\n  wq = np.interp(tq, t, pdf_blurred)\n\n  return wq\n\n\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n  from scipy.stats import norm\n\n  # Convert time points and weights to a probability density function (PDF)\n  pdf = np.zeros_like(t)\n  for i in range(len(t) - 1):\n    pdf[i] = (w[i] + w[i + 1]) / 2 * (t[i + 1] - t[i])\n\n  # Blur the PDF using a Gaussian filter\n  blurred_pdf = np.zeros_like(pdf)\n  for i in range(len(t)):\n    kernel = norm.pdf(t, loc=t[i], scale=blur_halfwidth)\n    kernel /= np.sum(kernel)\n    blurred_pdf[i] = np.sum(pdf * kernel)\n\n  # Resample the blurred PDF to match the new time points\n  f = interp1d(t, blurred_pdf, kind='linear', fill_value='extrapolate')\n  resampled_weights = f(tq)\n\n  return resampled_weights"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.signal import convolve\n\n  # Convert time points and weights to PDF\n  pdf = np.zeros_like(t)\n  pdf[1:] = np.diff(t) * w[:-1]\n\n  # Blur PDF\n  blur_kernel = np.exp(-np.arange(-blur_halfwidth, blur_halfwidth+1)**2 / (2*blur_halfwidth**2))\n  blurred_pdf = convolve(pdf, blur_kernel, mode='same')\n\n  # Resample blurred PDF to match new time points\n  resampled_weights = np.interp(tq, t, blurred_pdf)\n\n  return resampled_weights"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.stats import norm\n\n  # Calculate the bin width based on the first two time points\n  dt = t[1] - t[0]\n\n  # Calculate the blur standard deviation based on the bin width and blur half-width\n  blur_std = blur_halfwidth * dt\n\n  # Calculate the PDF by dividing the weights by the bin width\n  pdf = w / dt\n\n  # Calculate the blur kernel using a Gaussian distribution\n  blur_kernel = norm.pdf(t, loc=0, scale=blur_std)\n\n  # Convolve the PDF with the blur kernel\n  blurred_pdf = np.convolve(pdf, blur_kernel, mode='same')\n\n  # Normalize the blurred PDF\n  blurred_pdf /= np.sum(blurred_pdf)\n\n  # Resample the blurred PDF at the new time points\n  resampled_weights = np.interp(tq, t, blurred_pdf)\n\n  return resampled_weights"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert time points and weights to a probability density function (PDF)\n  pdf = w / np.sum(w)\n\n  # Define the blurring kernel\n  kernel = np.exp(-0.5 * (np.arange(-blur_halfwidth, blur_halfwidth + 1) / blur_halfwidth)**2)\n  kernel /= np.sum(kernel)\n\n  # Blur the PDF\n  pdf_blurred = np.convolve(pdf, kernel, mode='same')\n\n  # Resample the PDF to match the new time points\n  pdf_resampled = np.interp(tq, t, pdf_blurred)\n\n  # Normalize the resampled PDF to obtain the resampled weights\n  wq = pdf_resampled / np.sum(pdf_resampled)\n\n  return wq"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Convert the histogram to a probability density function (PDF)\n  pdf = w / np.sum(w)\n\n  # Create a new set of time points for the blurred PDF\n  t_blur = np.linspace(np.min(t) - blur_halfwidth, np.max(t) + blur_halfwidth, 1000)\n\n  # Create a Gaussian kernel for blurring\n  kernel = np.exp(-0.5 * (t_blur[:, np.newaxis] - t)**2 / blur_halfwidth**2)\n\n  # Convolve the PDF with the Gaussian kernel to create the blurred PDF\n  pdf_blur = np.sum(kernel * pdf[:, np.newaxis], axis=0)\n\n  # Normalize the blurred PDF\n  pdf_blur /= np.trapz(pdf_blur, t_blur)\n\n  # Create an interpolation function for the blurred PDF\n  f = interp1d(t_blur, pdf_blur, kind='linear', bounds_error=False, fill_value=0)\n\n  # Evaluate the interpolation function at the new time points\n  pdf_blur_interp = f(tq)\n\n  # Normalize the interpolated blurred PDF\n  pdf_blur_interp /= np.trapz(pdf_blur_interp, tq)\n\n  # Convert the blurred PDF back to weights\n  w_blur = pdf_blur_interp * np.sum(w)\n\n  return w_blur"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Convert the histogram to a PDF\n  pdf = w / np.trapz(w, t)\n\n  # Create a Gaussian kernel for blurring\n  kernel = np.exp(-0.5 * ((t[:, np.newaxis] - t[np.newaxis, :]) / blur_halfwidth) ** 2)\n  kernel /= np.trapz(kernel, t, axis=1)[:, np.newaxis]\n\n  # Blur the PDF using the kernel\n  blurred_pdf = np.dot(kernel, pdf)\n\n  # Interpolate the blurred PDF at the new time points\n  f = interp1d(t, blurred_pdf, kind='linear', bounds_error=False, fill_value=0)\n  blurred_resampled_pdf = f(tq)\n\n  # Normalize the blurred and resampled PDF to obtain the resampled weights\n  blurred_resampled_weights = blurred_resampled_pdf / np.trapz(blurred_resampled_pdf, tq)\n\n  return blurred_resampled_weights\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.stats import norm\n\n  # Convert the histogram to a probability density function (PDF)\n  pdf = np.zeros_like(t)\n  for i in range(len(t)):\n    pdf[i] = w[i] / np.sum(w)\n\n  # Blur the PDF using a Gaussian kernel\n  kernel = norm.pdf(t, loc=t, scale=blur_halfwidth)\n  pdf_blurred = np.convolve(pdf, kernel, mode='same')\n\n  # Resample the blurred PDF to match the new time points\n  pdf_resampled = np.interp(tq, t, pdf_blurred)\n\n  # Normalize the resampled PDF to obtain the resampled weights\n  wq = pdf_resampled / np.sum(pdf_resampled)\n\n  return wq\n\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.stats import norm\n\n  # Convert the histogram to a probability density function (PDF)\n  pdf = np.zeros_like(tq)\n  for i in range(len(t)):\n    pdf += w[i] * norm.pdf(tq, t[i], blur_halfwidth)\n\n  # Blur the PDF using a Gaussian filter\n  blurred_pdf = norm.pdf(tq, tq, blur_halfwidth) * pdf\n\n  # Resample the blurred PDF to match the new time points\n  resampled_weights = np.interp(tq, tq, blurred_pdf)\n\n  return resampled_weights\n\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  from numpy import exp, pi, sqrt\n  from scipy.interpolate import interp1d\n  from scipy.integrate import quad\n\n  # Convert the histogram to a PDF\n  pdf = w / sum(w)\n\n  # Define the blurring function\n  def blur_func(t_):\n    return 1 / (sqrt(2 * pi) * blur_halfwidth) * exp(-(t_ ** 2) / (2 * blur_halfwidth ** 2))\n\n  # Blur the PDF\n  blurred_pdf = [quad(lambda t_: blur_func(t_ - ti) * pdfi, -1000, 1000)[0] for ti, pdfi in zip(t, pdf)]\n\n  # Interpolate the blurred PDF\n  f = interp1d(t, blurred_pdf, kind='linear', fill_value='extrapolate')\n\n  # Resample the blurred PDF\n  resampled_weights = f(tq)\n\n  return resampled_weights\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.stats import norm\n\n  # Compute the histogram bin width\n  dt = t[1] - t[0]\n\n  # Convert the histogram to a probability density function (PDF)\n  pdf = w / np.sum(w * dt)\n\n  # Compute the standard deviation of the Gaussian kernel\n  sigma = blur_halfwidth / 2.0\n\n  # Create a Gaussian kernel\n  kernel = norm.pdf(t, loc=0, scale=sigma)\n\n  # Convolve the PDF with the Gaussian kernel\n  convolved = np.convolve(pdf, kernel, mode='same')\n\n  # Normalize the convolved PDF\n  convolved /= np.sum(convolved * dt)\n\n  # Resample the convolved PDF to match the new time points\n  resampled = np.interp(tq, t, convolved)\n\n  return resampled\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Convert time points and weights to a probability density function (PDF)\n  t_min, t_max = np.min(t), np.max(t)\n  t_range = t_max - t_min\n  t_edges = np.linspace(t_min - t_range / 10, t_max + t_range / 10, 1000)\n  pdf, _ = np.histogram(t, bins=t_edges, weights=w, density=True)\n\n  # Smooth the PDF using a Gaussian filter\n  blur_std = blur_halfwidth / 3  # Convert blur_halfwidth to Gaussian standard deviation\n  blur_kernel = np.exp(-0.5 * ((t_edges - t_edges[:, np.newaxis]) / blur_std) ** 2)\n  blur_kernel = blur_kernel / np.sum(blur_kernel, axis=1, keepdims=True)\n  pdf_blur = np.sum(pdf[:, np.newaxis] * blur_kernel, axis=0)\n\n  # Resample the blurred PDF to match the new time points\n  pdf_interp = interp1d(t_edges, pdf_blur, kind='linear', fill_value=0, bounds_error=False)\n  w_resampled = pdf_interp(tq)\n\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Create an interpolation function for the original time points and weights\n  f = interp1d(t, w, kind='linear', bounds_error=False, fill_value=0)\n\n  # Create a new set of time points between the minimum and maximum of the original time points\n  t_new = np.linspace(np.min(t), np.max(t), 1000)\n\n  # Calculate the PDF by interpolating the original weights at the new time points\n  pdf = f(t_new)\n\n  # Create a Gaussian kernel with the specified half-width\n  kernel = np.exp(-0.5 * ((t_new[:, np.newaxis] - t_new) / blur_halfwidth) ** 2)\n\n  # Normalize the kernel\n  kernel /= np.sum(kernel, axis=1, keepdims=True)\n\n  # Convolve the PDF with the kernel to obtain the blurred PDF\n  blurred_pdf = np.sum(kernel * pdf[:, np.newaxis], axis=0)\n\n  # Interpolate the blurred PDF at the new time points to obtain the resampled weights\n  f_blurred = interp1d(t_new, blurred_pdf, kind='linear', bounds_error=False, fill_value=0)\n  w_blurred = f_blurred(tq)\n\n  return w_blurred\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  vectors_hom = np.concatenate([vectors, np.ones((*vectors.shape[:-1], 1))], axis=-1)\n  return (transform @ vectors_hom.T).T[..., :-1]\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return (transform @ vectors.T).T\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return vectors @ transform[:-1,:-1].T + transform[-1,:-1]\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Apply homogeneous transformation to each vector\n  return (transform @ vectors.T).T\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return (transform @ vectors.T).T\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  import numpy as np\n\n  # Convert the input array to a homogeneous coordinate system\n  vectors_homogeneous = np.concatenate((vectors, np.ones((vectors.shape[0], 1))), axis=1)\n\n  # Apply the homogeneous transformation to the homogeneous coordinate system\n  vectors_homogeneous_transformed = np.dot(vectors_homogeneous, transform)\n\n  # Convert the transformed points back to a Cartesian coordinate system\n  vectors_transformed = vectors_homogeneous_transformed[:, :-1]\n\n  return vectors_transformed"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  vectors = vectors.T\n  vectors = np.dot(transform, vectors)\n  vectors = vectors.T\n\n  return vectors\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Convert the input array to a homogeneous coordinate system.\n  vectors = np.concatenate([vectors, np.ones((vectors.shape[0], 1))], axis=-1)\n\n  # Apply the homogeneous transformation.\n  vectors = np.matmul(vectors, transform.T)\n\n  # Convert the array back to a Cartesian coordinate system.\n  vectors = vectors[:, :-1]\n\n  return vectors"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return vectors @ transform[:-1, :-1].T + transform[-1, :-1]\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Apply the homogeneous transformation to the points\n  transformed_vectors = transform @ vectors\n\n  # Divide the points by their homogeneous coordinate to obtain the Euclidean coordinates\n  transformed_vectors = transformed_vectors[:-1, :] / transformed_vectors[-1, :]\n\n  return transformed_vectors\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Check if the transform matrix is homogeneous\n  if transform.shape[0] != transform.shape[1]:\n    raise ValueError(\"The transform matrix must be square.\")\n\n  # Check if the transform matrix is homogeneous\n  if transform[-1, :-1].sum() != 0 or transform[-1, -1] != 1:\n    raise ValueError(\"The transform matrix must be homogeneous.\")\n\n  # Check if the vectors array has the correct shape\n  if vectors.shape[-1] != transform.shape[0] - 1:\n    raise ValueError(\"The vectors array must have the same number of dimensions as the transform matrix.\")\n\n  # Apply the homogeneous transformation\n  vectors_homogeneous = np.concatenate((vectors, np.ones((vectors.shape[0], 1))), axis=-1)\n  transformed_vectors_homogeneous = np.dot(vectors_homogeneous, transform.T)\n  transformed_vectors = transformed_vectors_homogeneous[:, :-1]\n\n  return transformed_vectors\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Transform the points according to the homogeneous transformation\n  vectors_transformed = np.matmul(transform, np.transpose(vectors))\n\n  # Remove the homogeneous coordinate and return the result\n  return np.transpose(vectors_transformed[:-1])\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Transform the given vectors according to the specified transformation matrix\n  transformed_vectors = transform @ vectors.T\n\n  # Normalize the transformed vectors by dividing each component by the last component\n  transformed_vectors /= transformed_vectors[-1]\n\n  # Return the transformed vectors, excluding the last component (which is always 1)\n  return transformed_vectors[:-1].T\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Check if the transform is homogeneous\n  if transform.shape[0] != transform.shape[1]:\n    raise ValueError(\"The transformation matrix must be a square matrix.\")\n\n  # Check if the vectors have the correct shape\n  if vectors.shape[-1] != transform.shape[0] - 1:\n    raise ValueError(\"The vectors must have the same dimensionality as the transformation matrix.\")\n\n  # Add a column of ones to the vectors to make them homogeneous\n  ones = np.ones((vectors.shape[0], 1))\n  vectors_homogeneous = np.concatenate((vectors, ones), axis=1)\n\n  # Apply the homogeneous transformation\n  transformed_vectors = np.dot(vectors_homogeneous, transform.T)\n\n  # Remove the extra column of ones\n  transformed_vectors = transformed_vectors[:, :-1]\n\n  return transformed_vectors\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Check if the transform matrix is homogeneous\n  if transform.shape[0] != transform.shape[1]:\n    raise ValueError(\"The transform matrix must be a square matrix.\")\n\n  # Check if the vectors array has the correct shape\n  if vectors.shape[-1] != transform.shape[0] - 1:\n    raise ValueError(\"The last dimension of the vectors array must be equal to the number of rows in the transform matrix minus one.\")\n\n  # Convert the vectors array to homogeneous coordinates\n  vectors_hom = np.concatenate((vectors, np.ones((*vectors.shape[:-1], 1))), axis=-1)\n\n  # Apply the homogeneous transformation\n  transformed_vectors_hom = np.einsum('ij,kj->ki', transform, vectors_hom)\n\n  # Convert the transformed vectors back to Euclidean coordinates\n  transformed_vectors = transformed_vectors_hom[..., :-1] / transformed_vectors_hom[..., -1:]\n\n  return transformed_vectors\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  if transform.shape[0] != transform.shape[1]:\n    raise ValueError(\"The transformation matrix must be square.\")\n\n  if vectors.shape[-1] != transform.shape[0] - 1:\n    raise ValueError(\"The dimensionality of the points (%d) must be one less than the dimensionality of the transformation matrix (%d).\" % (vectors.shape[-1], transform.shape[0]))\n\n  # Add a column of ones to the points to make them homogeneous coordinates.\n  ones = np.ones((*vectors.shape[:-1], 1))\n  vectors = np.concatenate((vectors, ones), axis=-1)\n\n  # Apply the transformation to the points.\n  transformed_vectors = np.matmul(vectors, transform.T)\n\n  # Divide by the last coordinate to get the transformed points in Euclidean coordinates.\n  transformed_vectors = transformed_vectors[..., :-1] / transformed_vectors[..., -1:]\n\n  return transformed_vectors"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Check the input array shape\n  if vectors.shape[-1] != transform.shape[0] - 1:\n    raise ValueError(f\"The dimensionality of the vectors ({vectors.shape[-1]}) does not match the dimensionality of the transform ({transform.shape[0] - 1})\")\n\n  # Add a column of ones to the vectors\n  vectors_with_ones = np.hstack([vectors, np.ones((vectors.shape[0], 1))])\n\n  # Apply the transformation\n  transformed_vectors = np.dot(vectors_with_ones, transform)\n\n  # Remove the last column (which contains the ones)\n  transformed_vectors = transformed_vectors[:, :-1]\n\n  return transformed_vectors\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Transform the given vectors according to the given transformation matrix.\n  transformed_vectors = transform @ vectors.T\n\n  # Normalize the transformed vectors by dividing them by the last element of each vector.\n  transformed_vectors = transformed_vectors[:-1] / transformed_vectors[-1]\n\n  # Return the transformed vectors.\n  return transformed_vectors.T\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return (transform @ vectors.T).T\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # The following code is equivalent to:\n  # >>> return np.einsum('ij,kj->ki', transform, vectors)\n\n  # The above is a more efficient way of writing:\n  # >>> return np.matmul(transform, vectors.T).T\n\n  # The following is a more efficient way of writing:\n  # >>> return np.matmul(transform, vectors.T).T\n  # >>> return np.matmul(transform, vectors.T, out=np.empty((vectors.shape[0], transform.shape[1])))\n\n  # The following is a more efficient way of writing:\n  # >>> return np.matmul(transform, vectors.T, out=np.empty((vectors.shape[0], transform.shape[1])))\n  # >>> return np.matmul(transform, vectors.T, out=np.empty((vectors.shape[0], transform.shape[1]), dtype=transform.dtype))\n\n  # The following is a more efficient way of writing:\n  # >>> return np.matmul(transform, vectors.T, out=np.empty((vectors.shape[0], transform.shape[1]), dtype=transform.dtype))\n  # >>> return np.matmul(transform, vectors.T, out=np.empty((vectors.shape[0], transform.shape[1]), dtype=transform.dtype, order='F'))\n\n  # The following is a more efficient way of writing:\n  # >>> return np.matmul(transform, vectors.T, out=np.empty((vectors.shape[0], transform.shape[1]), dtype=transform.dtype, order='F'))\n  # >>> return np.matmul(transform, vectors.T, out=np.empty((vectors.shape[0], transform.shape[1]), dtype=transform.dtype, order='F', casting='same_kind'))\n\n  # The following is a more efficient way of writing:\n  # >>> return np.matmul(transform, vectors"}
{"namespace": "stepfun.resample", "completion": "  if use_avg:\n    v = (vp[:, None] * (tp[1:] - tp[:-1])).cumsum(0) / (t[1:] - t[:-1])\n  else:\n    v = (vp[:, None] * (t[1:] - t[:-1])).cumsum(0)\n  return v"}
{"namespace": "stepfun.resample", "completion": "  if use_avg:\n    return resample_avg(t, tp, vp)\n  else:\n    return resample_sum(t, tp, vp)\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Check if tp and vp have the same shape\n  if tp.shape != vp.shape:\n    raise ValueError(\"tp and vp must have the same shape\")\n\n  # Check if tp is sorted\n  if not torch.all(tp[:-1] <= tp[1:]):\n    raise ValueError(\"tp must be sorted\")\n\n  # Check if t is sorted\n  if not torch.all(t[:-1] <= t[1:]):\n    raise ValueError(\"t must be sorted\")\n\n  # Check if tp and t are compatible\n  if tp[-1] < t[0] or t[-1] > tp[0]:\n    raise ValueError(\"t and tp must have compatible ranges\")\n\n  # Resample the step function\n  if use_avg:\n    # Compute the average value of the step function for each interval in t\n    v = torch.zeros_like(t)\n    for i in range(len(t) - 1):\n      # Find the indices of the original time points that fall within the current interval\n      indices = torch.where((tp >= t[i]) & (tp < t[i + 1]))[0]\n      # Compute the width of each interval\n      widths = torch.zeros_like(indices)\n      widths[0] = t[i + 1] - t[i]\n      widths[1:] = tp[indices[1:]] - tp[indices[:-1]]\n      # Compute the average value of the step function for the current interval\n      v[i] = torch.sum(vp[indices] * widths) / torch.sum(widths)\n  else:\n    # Compute the sum of the values of the step function for each interval in t\n    v = torch.zeros_like(t)\n    for i in range(len(t) - 1):\n      # Find the indices of the original time points that fall within the current interval\n      indices = torch.where((tp >= t[i]) & (tp < t[i + 1]))["}
{"namespace": "stepfun.resample", "completion": "  # Check if tp is sorted\n  assert torch.all(tp[:-1] <= tp[1:]), \"tp is not sorted\"\n\n  # Check if t is sorted\n  assert torch.all(t[:-1] <= t[1:]), \"t is not sorted\"\n\n  # Check if tp and t have at least two elements\n  assert tp.shape[0] >= 2 and t.shape[0] >= 2, \"tp and t must have at least two elements\"\n\n  # Check if tp and vp have the same shape\n  assert tp.shape == vp.shape, \"tp and vp must have the same shape\"\n\n  # Check if t is within the range of tp\n  assert t[0] >= tp[0] and t[-1] <= tp[-1], \"t is not within the range of tp\"\n\n  # Check if tp and t have the same device\n  assert tp.device == t.device, \"tp and t must have the same device\"\n\n  # Check if tp and vp have the same device\n  assert tp.device == vp.device, \"tp and vp must have the same device\"\n\n  # Check if tp and t are of the same dtype\n  assert tp.dtype == t.dtype, \"tp and t must have the same dtype\"\n\n  # Check if tp and vp are of the same dtype\n  assert tp.dtype == vp.dtype, \"tp and vp must have the same dtype\"\n\n  # Check if tp and t are of the same type\n  assert tp.dtype == t.dtype, \"tp and t must have the same type\"\n\n  # Check if tp and vp are of the same type\n  assert tp.dtype == vp.dtype, \"tp and vp must have the same type\"\n\n  # Check if tp and t are of the same type\n  assert tp.dtype == t.dtype, \"tp and t must have the same type\"\n\n  # Check if tp and vp are of the"}
{"namespace": "stepfun.resample", "completion": "  t = t.unsqueeze(1)\n  tp = tp.unsqueeze(0)\n  vp = vp.unsqueeze(0)\n\n  if use_avg:\n    w = (t - tp).clamp(min=0)\n    w = w[:, 1:] - w[:, :-1]\n    w = w.clamp(min=0)\n    return (w * vp).sum(1) / w.sum(1)\n  else:\n    w = (t - tp).clamp(min=0)\n    w = w[:, 1:] - w[:, :-1]\n    w = w.clamp(min=0)\n    return (w * vp).sum(1)\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Compute the width of each interval in `t`\n  widths = t[1:] - t[:-1]\n\n  # Initialize a tensor to store the resampled values\n  v = torch.zeros_like(t)\n\n  # Iterate over each interval in `t`\n  for i in range(len(t) - 1):\n    # Find the indices of the original time points that fall within the current interval\n    indices = torch.where((tp >= t[i]) & (tp < t[i + 1]))[0]\n\n    # Compute the sum or average of the values of the step function in the current interval\n    if use_avg:\n      v[i] = torch.sum(vp[indices] * widths[i]) / widths[i]\n    else:\n      v[i] = torch.sum(vp[indices])\n\n  return v\n\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Check if tp and vp have the same shape\n  if tp.shape != vp.shape:\n    raise ValueError(\"tp and vp must have the same shape\")\n\n  # Check if tp is sorted\n  if not torch.all(tp[:-1] <= tp[1:]):\n    raise ValueError(\"tp must be sorted\")\n\n  # Check if t is sorted\n  if not torch.all(t[:-1] <= t[1:]):\n    raise ValueError(\"t must be sorted\")\n\n  # Check if tp is within the range of t\n  if not torch.all(t[0] <= tp) and torch.all(tp <= t[-1]):\n    raise ValueError(\"tp must be within the range of t\")\n\n  # Initialize the output tensor\n  v = torch.zeros_like(t)\n\n  # Iterate over the intervals in t\n  for i in range(t.shape[0] - 1):\n    # Find the indices of tp that are within the current interval\n    indices = torch.where((t[i] <= tp) & (tp < t[i + 1]))[0]\n\n    # If there are no indices within the current interval, skip to the next interval\n    if indices.shape[0] == 0:\n      continue\n\n    # If there is only one index within the current interval, assign the corresponding value to v\n    if indices.shape[0] == 1:\n      v[i] = vp[indices[0]]\n      continue\n\n    # If there are multiple indices within the current interval, compute the sum or average of the corresponding values\n    if use_avg:\n      v[i] = torch.mean(vp[indices])\n    else:\n      v[i] = torch.sum(vp[indices])\n\n  return v\n\n\n"}
{"namespace": "stepfun.resample", "completion": "  t = t.unsqueeze(1)\n  tp = tp.unsqueeze(0)\n  vp = vp.unsqueeze(0)\n\n  if use_avg:\n    delta_tp = tp[:, 1:] - tp[:, :-1]\n    delta_t = t[:, 1:] - t[:, :-1]\n    vp = vp * delta_tp\n    vp = torch.cat((vp, torch.zeros_like(vp[:, :1])), dim=1)\n    vp = 0.5 * (vp[:, :-1] + vp[:, 1:])\n    vp = vp * delta_t\n    v = torch.sum(vp, dim=1) / delta_t\n  else:\n    vp = torch.cat((vp, torch.zeros_like(vp[:, :1])), dim=1)\n    v = torch.sum((tp < t) * (t <= tp.roll(-1, dims=1)) * vp, dim=1)\n  return v\n\n"}
{"namespace": "stepfun.resample", "completion": "  assert t.shape == tp.shape, \"t and tp must have the same shape\"\n  assert t.shape == vp.shape, \"t and vp must have the same shape\"\n\n  t = t.reshape(-1, 1)\n  tp = tp.reshape(-1, 1)\n  vp = vp.reshape(-1, 1)\n\n  t = t.repeat(1, 2)\n  tp = tp.repeat(1, 2)\n  vp = vp.repeat(1, 2)\n\n  t[:, 1] = t[:, 0]\n  tp[:, 1] = tp[:, 0]\n  vp[:, 1] = vp[:, 0]\n\n  t_diff = t[:, 1] - t[:, 0]\n  tp_diff = tp[:, 1] - tp[:, 0]\n\n  mask = (t_diff > 0) & (tp_diff > 0)\n\n  t_min = torch.max(t[:, 0], tp[:, 0])\n  t_max = torch.min(t[:, 1], tp[:, 1])\n\n  t_diff = t_max - t_min\n\n  if use_avg:\n    v = t_diff * vp[:, 0]\n  else:\n    v = t_diff * vp[:, 0]\n\n  v[~mask] = 0\n\n  return v\n"}
{"namespace": "stepfun.resample", "completion": "  t = tf.convert_to_tensor(t, dtype=tf.float32)\n  tp = tf.convert_to_tensor(tp, dtype=tf.float32)\n  vp = tf.convert_to_tensor(vp, dtype=tf.float32)\n\n  if len(t.shape) != 1:\n    raise ValueError(\"t must be a 1D tensor\")\n  if len(tp.shape) != 1:\n    raise ValueError(\"tp must be a 1D tensor\")\n  if len(vp.shape) != 1:\n    raise ValueError(\"vp must be a 1D tensor\")\n  if t.shape[0] < 2:\n    raise ValueError(\"t must have at least 2 elements\")\n  if tp.shape[0] < 2:\n    raise ValueError(\"tp must have at least 2 elements\")\n  if vp.shape[0] < 2:\n    raise ValueError(\"vp must have at least 2 elements\")\n  if t.shape[0] != tp.shape[0]:\n    raise ValueError(\"t and tp must have the same length\")\n  if t.shape[0] != vp.shape[0]:\n    raise ValueError(\"t and vp must have the same length\")\n  if not tf.reduce_all(tf.math.is_non_decreasing(t)):\n    raise ValueError(\"t must be a non-decreasing sequence\")\n  if not tf.reduce_all(tf.math.is_non_decreasing(tp)):\n    raise ValueError(\"tp must be a non-decreasing sequence\")\n\n  tp_diff = tf.concat([[0], tf.math.diff(tp)], axis=0)\n  if not tf.reduce_all(tf.math.is_non_negative(tp_diff)):\n    raise ValueError(\"tp must be a non-decreasing sequence\")\n\n  t_diff = tf.concat([[0], tf.math.diff(t"}
{"namespace": "stepfun.resample", "completion": "  # Compute the width of each interval in t\n  t_width = t[:-1] - t[1:]\n  t_width = tf.concat([t_width, [t_width[-1]]], axis=0)\n\n  # Compute the width of each interval in tp\n  tp_width = tp[:-1] - tp[1:]\n  tp_width = tf.concat([tp_width, [tp_width[-1]]], axis=0)\n\n  # Compute the width of each interval in tp in the new time scale\n  tp_width_new = t_width / tp_width\n\n  # Compute the new values for each interval in t\n  vp_new = tf.expand_dims(vp, axis=0) * tp_width_new\n\n  # Sum the values for each interval in t if use_avg is False, otherwise compute the average\n  if not use_avg:\n    vp_new = tf.reduce_sum(vp_new, axis=1)\n  else:\n    vp_new = tf.reduce_mean(vp_new, axis=1)\n\n  return vp_new"}
{"namespace": "stepfun.resample", "completion": "  # Assert that t, tp, and vp have the same batch size\n  assert t.shape[0] == tp.shape[0] == vp.shape[0]\n\n  # Assert that tp and vp have the same number of time points\n  assert tp.shape[1] == vp.shape[1]\n\n  # Add a dimension to t and tp for broadcasting\n  t = t[:, None]\n  tp = tp[:, :, None]\n\n  # Calculate the intervals between consecutive time points\n  intervals = tp[:, 1:] - tp[:, :-1]\n\n  # Calculate the width of each interval\n  widths = intervals[:, :, 1] - intervals[:, :, 0]\n\n  # Calculate the values of the resampled step function\n  if use_avg:\n    # Calculate the average value of the step function for each interval\n    v = torch.sum(vp[:, :-1] * widths, dim=1) / torch.sum(widths, dim=1)\n  else:\n    # Calculate the sum of the values of the step function for each interval\n    v = torch.sum(vp[:, :-1] * widths, dim=1)\n\n  return v"}
{"namespace": "stepfun.resample", "completion": "  # Add a small epsilon to the time points to avoid numerical issues\n  eps = 1e-8\n  tp = tp + eps\n  t = t + eps\n\n  # Sort the time points and values in ascending order\n  sort_indices = tf.argsort(tp)\n  tp = tf.gather(tp, sort_indices)\n  vp = tf.gather(vp, sort_indices)\n\n  # Compute the differences between adjacent time points\n  tp_diff = tp[1:] - tp[:-1]\n\n  # Compute the cumulative sum of the differences\n  tp_diff_cumsum = tf.math.cumsum(tp_diff)\n\n  # Compute the new time points by adding the cumulative sum to the original time points\n  tp_new = tf.concat([tf.zeros((1,)), tp_diff_cumsum], axis=0) + tp[:1]\n\n  # Compute the values of the resampled step function at the new time points\n  vp_new = tf.concat([vp[:1], vp[1:] * tp_diff], axis=0)\n\n  # Compute the differences between adjacent new time points\n  tp_new_diff = tp_new[1:] - tp_new[:-1]\n\n  # Compute the cumulative sum of the differences\n  tp_new_diff_cumsum = tf.math.cumsum(tp_new_diff)\n\n  # Compute the new time points by adding the cumulative sum to the original time points\n  t_new = tf.concat([tf.zeros((1,)), tp_new_diff_cumsum], axis=0) + tp_new[:1]\n\n  # Compute the values of the resampled step function at the new time points\n  if use_avg:\n    v_new = tf.concat([vp_new[:1], vp_new[1:] * tp_new_diff], axis=0)\n  else:"}
{"namespace": "stepfun.resample", "completion": "  t = t.reshape(-1, 1)\n  tp = tp.reshape(1, -1)\n  vp = vp.reshape(1, -1)\n\n  t_diff = t - tp\n  t_diff_neg = t_diff < 0\n  t_diff[t_diff_neg] = 0\n  t_diff_min = t_diff.min(dim=1).values\n  t_diff_min_idx = t_diff.argmin(dim=1)\n\n  if use_avg:\n    t_diff_min_idx_shifted = torch.cat([t_diff_min_idx[1:], torch.tensor([0])])\n    t_diff_min_shifted = t_diff_min[1:]\n    t_diff_min_shifted = torch.cat([t_diff_min_shifted, torch.tensor([0])])\n    t_diff_min_shifted[t_diff_neg[:-1]] = 0\n    t_diff_min_shifted = torch.cat([t_diff_min_shifted, torch.tensor([0])])\n    vp_shifted = torch.cat([vp[:, 1:], vp[:, :1]], dim=1)\n    vp_shifted[t_diff_neg[:-1]] = 0\n    vp_shifted = torch.cat([vp_shifted, vp[:, :1]], dim=1)\n    v = (t_diff_min * vp[:, t_diff_min_idx] + t_diff_min_shifted * vp_shifted[:, t_diff_min_idx_shifted]) / (t_diff_min + t_diff_min_shifted)\n  else:\n    v = vp[:, t_diff_min_idx]\n\n  return v\n"}
{"namespace": "stepfun.resample", "completion": "  t = t.unsqueeze(0)\n  tp = tp.unsqueeze(0)\n  vp = vp.unsqueeze(0)\n\n  t_diff = t.diff(dim=-1)\n  t_diff_p = tp.diff(dim=-1)\n  t_diff_p = torch.cat([t_diff_p, torch.zeros_like(t_diff_p[..., -1:])], dim=-1)\n\n  if use_avg:\n    t_diff_p_cumsum = t_diff_p.cumsum(dim=-1)\n    t_diff_p_cumsum = torch.cat([torch.zeros_like(t_diff_p_cumsum[..., :1]), t_diff_p_cumsum], dim=-1)\n    t_diff_p_cumsum = torch.cat([t_diff_p_cumsum, torch.zeros_like(t_diff_p_cumsum[..., :1])], dim=-1)\n    t_diff_p_cumsum = t_diff_p_cumsum.diff(dim=-1)\n    t_diff_p_cumsum = t_diff_p_cumsum.diff(dim=-1)\n\n    vp_padded = torch.cat([torch.zeros_like(vp[..., :1]), vp], dim=-1)\n    vp_padded = torch.cat([vp_padded, torch.zeros_like(vp_padded[..., :1])], dim=-1)\n    vp_padded = vp_padded.diff(dim=-1)\n    vp_padded = vp_padded.diff(dim=-1)\n\n    vp_cumsum = vp_padded.cumsum(dim=-1)\n    vp_cumsum = torch.cat([torch.zeros_like"}
{"namespace": "stepfun.resample", "completion": "  if use_avg:\n    return _resample_avg(t, tp, vp)\n  else:\n    return _resample_sum(t, tp, vp)\n\n"}
{"namespace": "stepfun.resample", "completion": "  t = t.unsqueeze(-1)\n  tp = tp.unsqueeze(-1)\n  vp = vp.unsqueeze(-1)\n\n  # Find the indices of the time points in tp that are closest to the time points in t\n  idx = torch.searchsorted(tp, t, side='right') - 1\n\n  # Clip the indices to the range of tp\n  idx = torch.clamp(idx, 0, tp.shape[-1] - 1)\n\n  # Calculate the time differences between the time points in t and the time points in tp\n  t_diff = t - tp.gather(-1, idx)\n\n  # Calculate the time differences between the time points in tp\n  tp_diff = torch.cat([tp.gather(-1, idx), tp.gather(-1, idx + 1)], dim=-1)\n\n  # Calculate the time differences between the time points in tp\n  tp_diff = tp_diff[:, 1:] - tp_diff[:, :-1]\n\n  # Calculate the time differences between the time points in t\n  t_diff = t_diff.unsqueeze(-1)\n\n  # Calculate the time differences between the time points in tp\n  tp_diff = tp_diff.unsqueeze(-1)\n\n  # Calculate the values of the resampled step function\n  v = vp.gather(-1, idx)\n\n  # If use_avg is True, calculate the average value of the resampled step function\n  if use_avg:\n    v = v * t_diff / tp_diff\n\n  # Sum the values of the resampled step function\n  v = v.sum(-1)\n\n  return v\n\n"}
{"namespace": "stepfun.resample", "completion": "  # t and tp should be in ascending order\n  t = tf.sort(t)\n  tp = tf.sort(tp)\n\n  # Check if t is in the range of tp\n  assert tf.reduce_min(t) >= tf.reduce_min(tp), \"t should be in the range of tp\"\n  assert tf.reduce_max(t) <= tf.reduce_max(tp), \"t should be in the range of tp\"\n\n  # Calculate the width of each interval in t\n  t_width = tf.concat([[0], t[1:] - t[:-1]], axis=0)\n\n  # Find the indices of the intervals in tp where each interval in t starts\n  t_start_indices = tf.searchsorted(tp, t, side='left')\n\n  # Find the indices of the intervals in tp where each interval in t ends\n  t_end_indices = tf.searchsorted(tp, t, side='right')\n\n  # Calculate the width of each interval in tp that overlaps with each interval in t\n  tp_width = tf.gather(tp, t_end_indices) - tf.gather(tp, t_start_indices)\n\n  # Calculate the width of each interval in tp that overlaps with each interval in t, normalized by the width of the corresponding interval in t\n  tp_width_normalized = tp_width / t_width\n\n  # Calculate the values of the resampled step function at each interval in t\n  if use_avg:\n    # If use_avg is True, return the average value, weighted by the width of each interval in t\n    v = tf.reduce_sum(tf.gather(vp, t_start_indices) * tp_width_normalized, axis=0)\n  else:\n    # If use_avg is False, return the sum of the values of the step function for each interval in t\n    v = tf.reduce_sum(tf.gather(vp"}
{"namespace": "stepfun.resample", "completion": "  if len(tp.shape) == 1:\n    tp = tp.unsqueeze(0)\n\n  if len(vp.shape) == 1:\n    vp = vp.unsqueeze(0)\n\n  # Create a tensor of shape (batch_size, 1) filled with zeros\n  batch_size = tp.shape[0]\n  zeros = torch.zeros(batch_size, 1, device=t.device)\n\n  # Concatenate zeros and tp along the second dimension\n  tp_padded = torch.cat((zeros, tp), dim=1)\n\n  # Create a tensor of shape (batch_size, len(t)) filled with zeros\n  zeros_t = torch.zeros(batch_size, len(t), device=t.device)\n\n  # Concatenate zeros_t and t along the second dimension\n  t_padded = torch.cat((zeros_t, t), dim=1)\n\n  # Compute the difference between adjacent elements in tp_padded along the second dimension\n  diff = tp_padded[:, 1:] - tp_padded[:, :-1]\n\n  # Create a mask that selects elements in t_padded that are greater than or equal to the corresponding elements in tp_padded\n  mask = (t_padded.unsqueeze(2) >= tp_padded.unsqueeze(1)).float()\n\n  # Multiply the difference tensor by the mask and sum along the second dimension\n  res = (diff.unsqueeze(2) * mask).sum(dim=1)\n\n  # Multiply the result by the values tensor\n  res = res * vp\n\n  # Sum along the second dimension if use_avg is False, otherwise compute the mean along the second dimension\n  if not use_avg:\n    res = res.sum(dim=1)\n  else:\n    # Compute the width of each interval in t\n    width = t_padded[:, 1:] - t_padded"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input tensors have the correct shapes\n  assert t.shape[0] == tp.shape[0] == vp.shape[0], \"Input tensors must have the same number of elements\"\n  assert t.shape[1] == tp.shape[1] == vp.shape[1], \"Input tensors must have the same number of channels\"\n\n  # Get the number of elements and channels in the input tensors\n  n, c = t.shape\n\n  # Calculate the width of each interval in `t`\n  width = t[:, 1:] - t[:, :-1]\n\n  # Create a tensor `t_mid` that contains the midpoints of each interval in `t`\n  t_mid = (t[:, 1:] + t[:, :-1]) / 2\n\n  # Create a tensor `tp_mid` that contains the midpoints of each interval in `tp`\n  tp_mid = (tp[:, 1:] + tp[:, :-1]) / 2\n\n  # Calculate the difference between the midpoints of each interval in `t` and the midpoints of each interval in `tp`\n  diff = t_mid[:, :, None, :] - tp_mid[:, None, :, :]\n\n  # Calculate the maximum and minimum differences between the midpoints of each interval in `t` and the midpoints of each interval in `tp`\n  max_diff = torch.max(diff, dim=2)[0]\n  min_diff = torch.min(diff, dim=2)[0]\n\n  # Create a tensor `mask` that contains 1s for values where the difference is negative and 0s otherwise\n  mask = (diff > 0).float()\n\n  # Calculate the area of each interval in `t` and `tp`\n  area = width * (max_diff - min_diff)\n\n  # Calculate the value of the resampled step function at each interval in `t`\n  if use_avg:\n    # Calculate the weighted average of the values of the step function at each interval in `t`\n    value"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scaling the mean and variance\n  scaled_mean = mean[Ellipsis, None, :] / jnp.array([2**i for i in range(min_deg, max_deg)])\n  scaled_var = var[Ellipsis, None, :] / jnp.array([2**i for i in range(min_deg, max_deg)])\n\n  # Concatenating the scaled mean and variance\n  scaled_vars = jnp.concatenate([jnp.sin(scaled_mean), jnp.cos(scaled_var)], axis=-1)\n\n  return scaled_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scaling the mean and variance\n  scaled_mean = mean[Ellipsis, None, :] / (2 ** (jnp.arange(min_deg, max_deg)[None, :] / 2))\n  scaled_var = var[Ellipsis, None, :] / (2 ** (jnp.arange(min_deg, max_deg)[None, :] / 2))\n\n  # Concatenating the scaled mean and variance\n  scaled_vars = jnp.concatenate([jnp.sin(scaled_mean), jnp.cos(scaled_var)], axis=-1)\n\n  return scaled_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the scaling factor for the encoding\n  scale = 2 ** jnp.linspace(min_deg, max_deg, mean.shape[-1], dtype=mean.dtype)\n\n  # Scale the mean and variance\n  scaled_mean = mean[..., None, :] * scale[:, None]\n  scaled_var = var[..., None, :] * scale[:, None]\n\n  # Apply the sinusoidal encoding\n  encoded_vars = jnp.concatenate([jnp.sin(scaled_mean), jnp.cos(scaled_var)], axis=-1)\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  scaled_mean = mean[Ellipsis, None] / jnp.array([2**i for i in range(min_deg, max_deg)])\n  scaled_var = var[Ellipsis, None] / jnp.array([2**i for i in range(min_deg, max_deg)])\n\n  # Concatenate the scaled mean and variance along the last axis\n  scaled_mean_var = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply the sinusoidal encoding\n  encoded_vars = jnp.sin(jnp.concatenate([scaled_mean_var, scaled_mean_var + 0.5 * jnp.pi], axis=-1))\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  from jax import numpy as jnp\n\n  # Scale the mean and variance\n  scaled_mean = 2**min_deg * mean\n  scaled_var = 2**min_deg * var\n\n  # Concatenate the scaled mean and variance\n  scaled_vars = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply the sinusoidal encoding\n  encoded_vars = jnp.concatenate([jnp.sin(jnp.concatenate([scaled_vars, scaled_vars + 0.5 * jnp.pi], axis=-1)),\n                                  jnp.cos(jnp.concatenate([scaled_vars, scaled_vars + 0.5 * jnp.pi], axis=-1))], axis=-1)\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  scaled_mean = mean[Ellipsis, None, :] / (2 ** (jnp.arange(min_deg, max_deg) / 2))\n  scaled_var = var[Ellipsis, None, :] / (2 ** (jnp.arange(min_deg, max_deg) / 2))\n\n  # Concatenate the scaled mean and variance\n  scaled_vars = jnp.concatenate([jnp.sin(scaled_mean), jnp.cos(scaled_var)], axis=-1)\n\n  # Flatten the scaled variables\n  flat_vars = scaled_vars.reshape(*scaled_vars.shape[:-2], -1)\n\n  return flat_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance using powers of 2\n  scaled_mean = mean[..., None, :] / 2 ** np.arange(min_deg, max_deg)\n  scaled_var = var[..., None, :] / 2 ** np.arange(min_deg, max_deg)\n\n  # Concatenate the scaled mean and variance\n  scaled_vars = jnp.concatenate([scaled_mean, scaled_var], axis=-2)\n\n  # Apply a sinusoidal encoding to the scaled variables\n  encoded_vars = jnp.sin(jnp.concatenate([scaled_vars, scaled_vars + 0.5 * jnp.pi], axis=-3))\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance using the scaling factor\n  scaled_mean = mean[Ellipsis, None, :] / (2 ** (jnp.arange(min_deg, max_deg) / 2))\n  scaled_var = var[Ellipsis, None, :] / (2 ** (jnp.arange(min_deg, max_deg) / 2))\n\n  # Concatenate the scaled mean and variance along the last axis\n  scaled_vars = jnp.concatenate([jnp.sin(scaled_mean), jnp.cos(scaled_var)], axis=-1)\n\n  # Flatten the last two dimensions of the scaled variables\n  scaled_vars = scaled_vars.reshape(*scaled_vars.shape[:-2], -1)\n\n  # Return the scaled variables\n  return scaled_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scaling the mean and variance by powers of 2\n  scaled_mean = 2**min_deg * mean\n  scaled_var = 2**min_deg * var\n\n  # Concatenating the scaled mean and variance\n  scaled_vars = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Applying the sinusoidal encoding\n  encoded_vars = jnp.concatenate([jnp.sin((2**i) * scaled_vars) for i in range(min_deg, max_deg)], axis=-1)\n\n  return encoded_vars\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Calculate the scaling factor using the minimum and maximum degrees\n  scale = 2 ** jnp.linspace(min_deg, max_deg, mean.shape[-1], dtype=jnp.float32)\n\n  # Scale the mean and variance using the scaling factor\n  scaled_mean = mean[..., None, :] * scale\n  scaled_var = var[..., None, :] * scale\n\n  # Apply the sinusoidal encoding to the scaled mean and variance\n  x = jnp.concatenate([jnp.sin(scaled_mean), jnp.cos(scaled_var)], axis=-1)\n\n  # Return the encoded variables\n  return x"}
{"namespace": "coord.integrated_pos_enc", "completion": "  import jax.numpy as jnp\n  from jax import vmap\n\n  # Scale mean and variance\n  scaled_mean = mean[..., None, :] / jnp.array([2**i for i in range(min_deg, max_deg)])\n  scaled_var = var[..., None, :] / jnp.array([2**i for i in range(min_deg, max_deg)])\n\n  # Concatenate scaled mean and variance\n  scaled_mean_var = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply sinusoidal encoding\n  encoded_vars = jnp.sin(jnp.concatenate([scaled_mean_var, scaled_mean_var + 0.5 * jnp.pi], axis=-1))\n\n  return encoded_vars\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scales the mean and variance of the coordinates using powers of 2\n  scaled_mean = mean[Ellipsis, None] / 2**(jnp.arange(min_deg, max_deg))\n  scaled_var = var[Ellipsis, None] / 2**(jnp.arange(min_deg, max_deg))\n\n  # Concatenates the scaled mean and variance\n  scaled_vars = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Applies a sinusoidal encoding to the concatenated variables\n  encoded_vars = jnp.sin(jnp.concatenate([scaled_vars, scaled_vars + 0.5 * jnp.pi], axis=-1))\n\n  return encoded_vars\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance using powers of 2\n  scaled_mean = 2 ** min_deg * mean\n  scaled_var = 2 ** min_deg * var\n\n  # Concatenate the scaled mean and variance along the last axis\n  scaled_vars = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Create an array of degrees within the specified range\n  degs = jnp.arange(min_deg, max_deg)\n\n  # Apply the sinusoidal encoding to the scaled variables\n  encoded_vars = jnp.sin(jnp.einsum('...d, d -> ...d', 2 ** degs * scaled_vars, jnp.pi))\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the scaling factor based on the specified minimum and maximum degrees\n  scale = 2 ** jnp.linspace(min_deg, max_deg, mean.shape[-1])\n\n  # Scale the mean and variance using the computed scaling factor\n  scaled_mean = mean[..., None, :] * scale\n  scaled_var = var[..., None, :] * scale\n\n  # Concatenate the scaled mean and variance along the last axis\n  x = jnp.concatenate([jnp.sin(scaled_mean), jnp.cos(scaled_var)], axis=-1)\n\n  return x\n\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the scaling factor based on the specified degrees\n  deg_diff = max_deg - min_deg\n  scale = 2 ** jnp.linspace(min_deg, max_deg, deg_diff)\n\n  # Scale the mean and variance\n  scaled_mean = mean[..., None, :] * scale[:, None]\n  scaled_var = var[..., None, :] * scale[:, None]\n\n  # Apply the sinusoidal encoding to the scaled mean and variance\n  shape = scaled_mean.shape[:-1]\n  scaled_mean = jnp.reshape(scaled_mean, (*shape, -1))\n  scaled_var = jnp.reshape(scaled_var, (*shape, -1))\n  encoded = jnp.concatenate([jnp.sin(scaled_mean), jnp.cos(scaled_var)], axis=-1)\n\n  # Return the encoded variables\n  return encoded"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # The function first scales the mean and variance using a scaling factor of 2^i, where i ranges from min_deg to max_deg.\n  # The scaled mean and variance are then concatenated along the last axis to form a new tensor, which is then passed through a sinusoidal encoding function.\n  # Finally, the encoded variables are returned as a jnp.ndarray.\n\n  encoded_vars = []\n  for i in range(min_deg, max_deg):\n    scale = 2**i\n    encoded_vars.append(jnp.sin(scale * mean))\n    encoded_vars.append(jnp.cos(scale * mean))\n    encoded_vars.append(jnp.sin(scale * var))\n    encoded_vars.append(jnp.cos(scale * var))\n  encoded_vars = jnp.concatenate(encoded_vars, axis=-1)\n  return encoded_vars\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  import jax.numpy as jnp\n  import jax\n\n  # Scaling the mean and variance by powers of 2\n  scaled_mean = mean[..., jnp.newaxis] / 2 ** jnp.arange(min_deg, max_deg)\n  scaled_var = var[..., jnp.newaxis] / 2 ** jnp.arange(min_deg, max_deg)\n\n  # Concatenating the scaled mean and variance\n  scaled_vars = jnp.concatenate([jnp.sin(scaled_mean), jnp.cos(scaled_var)], axis=-1)\n\n  # Flattening the encoded variables\n  flattened_vars = jnp.reshape(scaled_vars, (-1, scaled_vars.shape[-1]))\n\n  # Applying sinusoidal encoding to the flattened variables\n  encoded_vars = jax.vmap(lambda x: jnp.concatenate([jnp.sin(x), jnp.cos(x)], axis=-1))(flattened_vars)\n\n  # Reshaping the encoded variables to the original shape\n  encoded_vars = jnp.reshape(encoded_vars, (*scaled_mean.shape[:-1], -1))\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scales the mean and variance by a power of 2 for each degree in the range [min_deg, max_deg)\n  scales = 2 ** jnp.arange(min_deg, max_deg)\n  shape = mean.shape\n  shape_dims = mean.ndim\n  perm = [shape_dims - 1, *range(0, shape_dims - 1)]\n  mean = mean.reshape(-1, 1) * scales\n  var = var.reshape(-1, 1) * scales\n\n  # Concatenates the scaled mean and variance along the last dimension\n  mean = jnp.reshape(mean, (mean.shape[0], -1))\n  var = jnp.reshape(var, (var.shape[0], -1))\n  encoded = jnp.concatenate([jnp.sin(mean), jnp.cos(mean),\n                             jnp.sin(var), jnp.cos(var)], axis=-1)\n  encoded = encoded.reshape(*shape[:-1], -1)\n  encoded = jnp.transpose(encoded, perm)\n  return encoded"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scaled_mean = 2**min_deg * mean\n  scaled_var = 2**min_deg * var\n  x = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n  x_proj = (2**jnp.arange(min_deg, max_deg))[None, :] * x[..., None]\n  return jnp.concatenate([jnp.sin(jnp.concatenate([x_proj, x_proj + 0.5 * jnp.pi], axis=-1)), jnp.cos(jnp.concatenate([x_proj, x_proj + 0.5 * jnp.pi], axis=-1))], axis=-1)\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance by factors of 2^i\n  scaled_mean = 2**min_deg * mean\n  scaled_var = 2**min_deg * var\n\n  # Concatenate the scaled mean and variance along the last axis\n  scaled_vars = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Create a list of degrees to apply to the encoded variables\n  degs = jnp.arange(min_deg, max_deg)\n\n  # Apply a sinusoidal encoding to the scaled mean and variance\n  encoded_vars = [jnp.sin(scaled_vars * 2**i) for i in degs]\n\n  # Concatenate the encoded variables along the last axis\n  encoded_vars = jnp.concatenate(encoded_vars, axis=-1)\n\n  return encoded_vars\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  def dir_enc_fn(x):\n\n    \"\"\"\n    Evaluates the directional encoding for a given 3D point (or points).\n\n    Input-Output Arguments\n    :param x: Float. The 3D point (or points) for which to evaluate the directional encoding.\n    :return: Float. The directional encoding for the input 3D point (or points).\n\n    \"\"\"\n\n    return eval_fn(x)\n\n  eval_fn = generate_integrated_dir_enc_fn(deg_view)\n\n  return dir_enc_fn\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  def dir_enc_fn(x):\n\n    \"\"\"\n    Evaluates the directional encoding function for the given input point(s).\n\n    Input-Output Arguments\n    :param x: Tensor. The input point(s) for which to evaluate the directional encoding. It should have a shape of [batch_size, 3] or [3].\n    :return: Tensor. The directional encoding for the input point(s). It has a shape of [batch_size, 2 * deg_view * (deg_view + 1)].\n\n    \"\"\"\n\n    return integrated_dir_enc_fn(x)\n\n  integrated_dir_enc_fn = generate_integrated_dir_enc_fn(deg_view)\n\n  return dir_enc_fn\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  import jax\n  import jax.numpy as jnp\n  from jax.tree_util import tree_map\n\n  from snerg.snerg import get_encoder\n\n  def dir_enc_fn(x):\n\n    \"\"\"\n    Evaluates the directional encoding for the given input points.\n\n    Input-Output Arguments\n    :param x: Float32[B, 3]. A batch of 3D points for which to evaluate the directional encoding.\n    :return: Float32[B, 2 * deg_view]. The directional encoding for the input points.\n\n    \"\"\"\n\n    return get_encoder(deg_view)(x)\n\n  return dir_enc_fn\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  import numpy as np\n  import torch\n  from scipy.special import sph_harm\n\n  def dir_enc_fn(x):\n    \"\"\"\n    Evaluates the directional encoding for the given input point(s) using the integrated directional encoding function.\n\n    Input-Output Arguments\n    :param x: Tensor. A 3D point (or points) to evaluate the directional encoding for.\n    :return: Tensor. The directional encoding for the input point(s).\n    \"\"\"\n    return dir_enc_fn_integrated(x)\n\n  def dir_enc_fn_integrated(x):\n    \"\"\"\n    Evaluates the integrated directional encoding function for the given input point(s).\n\n    Input-Output Arguments\n    :param x: Tensor. A 3D point (or points) to evaluate the integrated directional encoding function for.\n    :return: Tensor. The integrated directional encoding function evaluated for the input point(s).\n    \"\"\"\n    x = x / torch.norm(x, dim=-1, keepdim=True)\n    y = torch.stack([x[..., 0], x[..., 2], x[..., 1]], dim=-1)\n\n    sh_val = torch.stack([\n      sph_harm(m, l, y[..., 1], y[..., 2])\n      for l in range(deg_view + 1)\n      for m in range(-l, l + 1)\n    ], dim=-1)\n\n    return sh_val\n\n  return dir_enc_fn\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  import torch\n  import numpy as np\n  from .utils import positional_encoding\n\n  # Generate the directional encoding function with the specified degree of spherical harmonics\n  def dir_enc_fn(xyz):\n    return positional_encoding(xyz, L=deg_view)\n\n  # Return the generated directional encoding function\n  return dir_enc_fn\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  def dir_enc_fn(dirs):\n\n    \"\"\"\n    Evaluates the directional encoding for the given 3D point (or points) using the generated directional encoding function.\n\n    Input-Output Arguments\n    :param dirs: Tensor. A 3D point (or points) to evaluate the directional encoding for. It should have a shape of (3,) for a single point or (N, 3) for a batch of points.\n    :return: Tensor. The directional encoding for the given 3D point (or points). It has a shape of (2 * deg_view,) for a single point or (N, 2 * deg_view) for a batch of points.\n\n    \"\"\"\n\n    dirs = dirs / torch.norm(dirs, dim=-1, keepdim=True)\n    result = torch.stack([fn(dirs) for fn in dir_enc_fns], 1)\n    return result\n\n  # Generate a list of directional encoding functions\n  dir_enc_fns = [lambda x: x[..., 0], lambda x: x[..., 1], lambda x: x[..., 2]]\n  for i in range(deg_view):\n    for j in range(1, (i // 2) + 2):\n      fn = [\n        lambda x, fn=fn: fn(x[..., 0], x[..., 1]) * x[..., 2],\n        lambda x, fn=fn: fn(x[..., 0], x[..., 1]) * x[..., 2],\n        lambda x, fn=fn: fn(x[..., 1], x[..., 2]) * x[..., 0],\n        lambda x, fn=fn: fn(x[..., 1], x[..., 2]) * x[..., 0],\n        lambda x, fn=fn: fn(x[..., 2], x[..., 0]) * x[..., 1],\n        lambda x, fn=fn: fn(x[..., "}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  def dir_enc_fn(dirs):\n\n    \"\"\"\n    Evaluates the directional encoding function for the given 3D point(s).\n\n    Input-Output Arguments\n    :param dirs: Float32 Tensor of shape [batch_size, 3]. The 3D points for which the directional encoding is to be computed.\n    :return: Float32 Tensor of shape [batch_size, 2 * deg_view * deg_view]. The directional encoding of the input 3D points.\n\n    \"\"\"\n\n    dirs = tf.reshape(dirs, [-1, 3])\n    dirs = tf.nn.l2_normalize(dirs, axis=-1)\n    dirs = tf.reshape(dirs, [-1, 1, 3])\n    dirs = tf.tile(dirs, [1, deg_view * deg_view, 1])\n    dirs = tf.reshape(dirs, [-1, deg_view, deg_view, 3])\n\n    sh_dirs = tf.split(dirs, 3, axis=-1)\n    sh_dirs = tf.concat(sh_dirs, axis=0)\n\n    sh_dirs = tf.split(sh_dirs, deg_view, axis=0)\n    sh_dirs = tf.stack(sh_dirs, axis=1)\n\n    sh_dirs = tf.split(sh_dirs, 3, axis=-1)\n    sh_dirs = tf.concat(sh_dirs, axis=0)\n\n    sh_dirs = tf.split(sh_dirs, deg_view, axis=0)\n    sh_dirs = tf.stack(sh_dirs, axis=1)\n\n    sh_dirs = tf.split(sh_dirs, 3, axis=-1)\n    sh_dirs = tf.concat(sh_dirs, axis=0)\n\n    sh_dirs = tf.split(sh"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  import jax\n  import jax.numpy as jnp\n  import numpy as np\n\n  from sde_nerf.utils import posenc\n\n  # Generate the integrated directional encoding function with the specified degree of spherical harmonics\n  integrated_dir_enc_fn = posenc.generate_fourier_features(deg_view)\n\n  # Define a function that evaluates the directional encoding for given inputs\n  def dir_enc_fn(inputs):\n    # Apply the integrated directional encoding function to the input\n    return integrated_dir_enc_fn(inputs)\n\n  # Return the generated directional encoding function\n  return dir_enc_fn\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  import numpy as np\n  import torch\n  from torch import nn\n\n  # Define a helper function to compute the associated Legendre polynomials\n  def P(l, m, x):\n    if m == 0:\n      return torch.ones_like(x)\n    elif m > 0:\n      pmm = torch.ones_like(x)\n      somx2 = torch.sqrt((1.0 - x) * (1.0 + x))\n      fact = 1.0\n      for i in range(1, m + 1):\n        pmm = pmm * (-fact) * somx2\n        fact = fact + 2.0\n      if l == m:\n        return pmm\n      pmmp1 = x * (2.0 * m + 1.0) * pmm\n      if l == m + 1:\n        return pmmp1\n      else:\n        pll = torch.zeros_like(x)\n        for i in range(m + 2, l + 1):\n          pll = ((2.0 * i - 1.0) * x * pmmp1 - (i + m - 1.0) * pmm) / (i - m)\n          pmm = pmmp1\n          pmmp1 = pll\n        return pll\n    else:\n      m = torch.abs(m)\n      mm = m ** 2\n      if l == m:\n        return torch.sqrt(((m + 1.0) * torch.pow(-x, mm - 1.0)) / ((mm - m) * torch.pow(1.0 - x, mm - 1.0)))\n      pmmp1 = torch.sqrt(((m + 1.0) * torch.pow(-x, mm - 1.0)) / ((mm - m) * torch.pow(1.0 - x, mm - 1.0)))\n      if l == m + 1:\n        return pmmp1\n      else:\n        pll = torch.zeros_like(x)\n        for i in range(m + 2, l + 1):\n          pll"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  def dir_enc_fn(x):\n\n    \"\"\"\n    Evaluates the directional encoding for the given input point(s).\n\n    Input-Output Arguments\n    :param x: Float or numpy array. The input point(s) for which to compute the directional encoding.\n    :return: Float or numpy array. The directional encoding for the input point(s).\n\n    \"\"\"\n\n    # Importing required libraries\n    import numpy as np\n    import torch\n    from torch import nn\n\n    # Converting the input point(s) to a numpy array\n    x = np.array(x)\n\n    # Creating a tensor from the input point(s)\n    x = torch.tensor(x, dtype=torch.float32)\n\n    # Creating an instance of the integrated directional encoding function with the specified degree of spherical harmonics\n    integrated_dir_enc_fn = integrated_dir_enc_fn_generator(deg_view)\n\n    # Evaluating the integrated directional encoding function on the input point(s)\n    x_enc = integrated_dir_enc_fn(x)\n\n    # Converting the evaluated tensor to a numpy array\n    x_enc = x_enc.numpy()\n\n    # Returning the evaluated directional encoding\n    return x_enc\n\n  # Returning the generated directional encoding function\n  return dir_enc_fn\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Importing the necessary libraries\n  import numpy as np\n  import torch\n  import math\n\n  # Generating the integrated directional encoding function with the specified degree of spherical harmonics\n  integrated_dir_enc_fn = generate_integrated_dir_enc_fn(deg_view)\n\n  # Defining the directional encoding function\n  def dir_enc_fn(x):\n\n    # Extracting the batch size and the number of points in the input\n    batch_size, num_pts, _ = x.shape\n\n    # Converting the input to a PyTorch tensor\n    x = torch.tensor(x)\n\n    # Reshaping the input to have a single dimension of size 3\n    x = x.view(-1, 3)\n\n    # Computing the directional encoding using the integrated directional encoding function\n    dir_encoding = integrated_dir_enc_fn(x)\n\n    # Reshaping the directional encoding to match the original input shape\n    dir_encoding = dir_encoding.view(batch_size, num_pts, dir_encoding.shape[-1])\n\n    # Converting the directional encoding to a NumPy array\n    dir_encoding = dir_encoding.numpy()\n\n    # Returning the directional encoding\n    return dir_encoding\n\n  # Returning the directional encoding function\n  return dir_enc_fn\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  def dir_enc_fn(pts):\n\n    \"\"\"\n    Evaluates the directional encoding for the given 3D point (or points) using the internally generated integrated directional encoding function.\n\n    Input-Output Arguments\n    :param pts: Float32 Tensor of shape [B, 3]. The 3D point (or points) for which the directional encoding needs to be evaluated.\n    :return: Float32 Tensor of shape [B, 2 * deg_view]. The evaluated directional encoding for the input 3D point (or points).\n\n    \"\"\"\n\n    # Compute the spherical coordinates of the input points\n    theta = torch.acos(pts[:, 2])\n    phi = torch.atan2(pts[:, 1], pts[:, 0])\n\n    # Compute the spherical harmonics\n    sh = torch.zeros((pts.shape[0], deg_view), device=pts.device)\n    for l in range(deg_view):\n      for m in range(-l, l + 1):\n        sh[:, l * (l + 1) + m] = sph_harm(m, l, theta, phi)\n\n    # Compute the directional encoding\n    dir_enc = torch.cat([sh, torch.sin(theta)[:, None], torch.cos(theta)[:, None]], dim=1)\n\n    return dir_enc\n\n  return dir_enc_fn\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Import modules\n  import numpy as np\n  import torch\n  import torch.nn as nn\n  import torch.nn.functional as F\n  from torch import Tensor\n\n  # Generate the integrated directional encoding function with the specified degree of spherical harmonics\n  integrated_dir_enc_fn = generate_integrated_dir_enc_fn(deg_view)\n\n  # Define a function that takes a 3D point (or points) as input and returns its directional encoding\n  def dir_enc_fn(pts: Tensor) -> Tensor:\n\n    # Initialize the output tensor\n    out = torch.zeros(pts.shape[:-1] + (integrated_dir_enc_fn.deg_view * 2 + 1,), device=pts.device)\n\n    # Evaluate the integrated directional encoding function for each point in the input tensor\n    for i in range(pts.shape[0]):\n      for j in range(pts.shape[1]):\n        out[i, j] = integrated_dir_enc_fn(pts[i, j])\n\n    # Return the output tensor\n    return out\n\n  # Return the generated function\n  return dir_enc_fn\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Importing required libraries\n  import numpy as np\n  import torch\n  import torch.nn as nn\n  import torch.nn.functional as F\n\n  # Defining the encoding function\n  def encoding_fn(x):\n\n    \"\"\"\n    Evaluates the directional encoding for the given input point(s) using the integrated directional encoding function with the specified degree of spherical harmonics.\n\n    Input-Output Arguments\n    :param x: Float. A 3D point (or points) to evaluate the directional encoding for.\n    :return: Float. The evaluated directional encoding for the input point(s).\n\n    \"\"\"\n\n    # Initializing the output tensor\n    out = torch.empty((x.shape[0], 2 * deg_view * deg_view), device = x.device)\n\n    # Computing the directional encoding for each input point\n    for i, p in enumerate(x):\n\n      # Computing the spherical coordinates of the input point\n      theta = torch.acos(p[1])\n      phi = torch.atan2(p[2], p[0])\n\n      # Computing the directional encoding for the input point\n      for l in range(deg_view):\n        for m in range(-l, l + 1):\n          y_lm = sph_harm(m, l, theta, phi)\n          out[i, (l * (l + 1) + m)] = y_lm.real\n          out[i, (l * (l + 1) + m + deg_view * deg_view)] = y_lm.imag\n\n    # Returning the output tensor\n    return out\n\n  # Returning the directional encoding function\n  return encoding_fn\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  import numpy as np\n  import torch\n  import torch.nn as nn\n  from torch.autograd import Function\n\n  class DirEncFn(Function):\n    @staticmethod\n    def forward(ctx, input):\n      ctx.save_for_backward(input)\n      return torch.tensor(eval_dir_enc_fn(input.detach().numpy(), deg_view), dtype=input.dtype)\n    @staticmethod\n    def backward(ctx, grad_output):\n      input, = ctx.saved_tensors\n      grad_input = grad_output.clone()\n      return grad_input\n\n  def eval_dir_enc_fn(pts, deg_view):\n    \"\"\"\n    Evaluates the directional encoding function for the given 3D points.\n\n    Input-Output Arguments\n    :param pts: Numpy array of shape (..., 3). The 3D points for which to evaluate the directional encoding.\n    :param deg_view: Int. The number of spherical harmonics degrees to use for generating the directional encoding.\n    :return: Numpy array of shape (..., 2 * deg_view * (deg_view + 2)). The evaluated directional encoding for the given points.\n    \"\"\"\n    pts = pts.reshape(-1, 3)\n    embed_dirs = []\n    for i in range(pts.shape[0]):\n      embed_dirs.append(eval_dir_enc_fn_single(pts[i:i+1], deg_view))\n    embed_dirs = np.concatenate(embed_dirs, 0)\n    return embed_dirs\n\n  def eval_dir_enc_fn_single(pts, deg_view):\n    \"\"\"\n    Evaluates the directional encoding function for a single 3D point.\n\n    Input-Output Arguments\n    :param pts: Numpy array of shape (1, 3). The 3D point for which to evaluate the directional encoding.\n    :param deg_view: Int. The number of"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Importing required libraries\n  import jax.numpy as jnp\n  import jax\n  from jax import jit\n\n  # Defining the function that generates the directional encoding function\n  @jit\n  def dir_enc_fn(dirs):\n\n    \"\"\"\n    Generates a directional encoding function based on the specified number of spherical harmonics degrees. This function internally creates another function that evaluates the directional encoding for given inputs.\n\n    Input-Output Arguments\n    :param dirs: Float32[1, 3]. A 3D point (or points) to evaluate the directional encoding function.\n    :return: Float32[1, 2 * deg_view * (deg_view + 2)]. The directional encoding of the input 3D point.\n\n    \"\"\"\n\n    # Defining the function that evaluates the directional encoding for given inputs\n    def eval_fn(dirs):\n\n      \"\"\"\n      Evaluates the directional encoding function for the given 3D points.\n\n      Input-Output Arguments\n      :param dirs: Float32[1, 3]. A 3D point (or points) to evaluate the directional encoding function.\n      :return: Float32[1, 2 * deg_view * (deg_view + 2)]. The directional encoding of the input 3D point.\n\n      \"\"\"\n\n      # Defining the function that computes the associated Legendre polynomials\n      def P(l, x):\n\n        \"\"\"\n        Computes the associated Legendre polynomials for a given degree (l) and input value (x).\n\n        Input-Output Arguments\n        :param l: Int. The degree of the associated Legendre polynomials.\n        :param x: Float32[1, 3]. The input value(s) for which to compute the associated Legendre polynomials.\n        :return: Float32[1, 2 * deg_view * (deg_view + 2)]. The computed associated Legendre polynomials.\n\n        \"\"\"\n\n        # Computing the associated Legendre polynomials\n        if l == 0:\n          return jnp.ones_"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Importing required libraries\n  import numpy as np\n  import torch\n\n  # Generating the integrated directional encoding function\n  def integrated_dir_enc_fn(dirs):\n\n    \"\"\"\n    Generates integrated directional encoding for the given directions.\n\n    Input-Output Arguments\n    :param dirs: Tensor. A tensor of shape (..., 3), where ... represents any number of dimensions. It contains the directions for which the integrated directional encoding is to be computed.\n    :return: Tensor. A tensor of shape (..., 2 * deg_view * deg_view), where ... represents the same dimensions as the input tensor. It contains the integrated directional encoding for the given directions.\n\n    \"\"\"\n\n    # Computing the spherical coordinates\n    theta = torch.acos(dirs[..., 1])\n    phi = torch.atan2(dirs[..., 2], dirs[..., 0])\n\n    # Pre-computing the constants\n    sh_proj_matrix = np.zeros((deg_view, 2 * deg_view - 1))\n    for l in range(deg_view):\n      for i in range(2 * l + 1):\n        sh_proj_matrix[l, i] = np.sqrt((4.0 * np.pi * (2.0 * l + 1.0)) / (i + 1.0))\n\n    # Computing the spherical harmonics basis functions\n    sh_basis = np.zeros((dirs.shape[:-1] + (deg_view, 2 * deg_view - 1)))\n    for l in range(deg_view):\n      for m in range(-l, l + 1):\n        i = l + m\n        sh_basis[..., l, i] = sh_proj_matrix[l, i] * torch.sin(torch.tensor(m, dtype=torch.float32) * phi) * torch.exp(torch.tensor(m, dtype=torch.float32) * theta)\n\n    # Computing the integrated directional encoding\n    integrated"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Importing the necessary libraries\n  import jax\n  import jax.numpy as jnp\n  import numpy as np\n  import math\n  from jax import grad, jit\n  from functools import partial\n  from jax import random\n  from jax import vmap\n\n  # Defining the spherical harmonics function\n  def spherical_harmonics(deg_view, theta, phi):\n\n    \"\"\"\n    Computes the spherical harmonics for a given degree and angles.\n\n    Input-Output Arguments\n    :param deg_view: Int. The degree of the spherical harmonics.\n    :param theta: Float. The polar angle in radians.\n    :param phi: Float. The azimuthal angle in radians.\n    :return: Float. The value of the spherical harmonics for the given degree, polar angle, and azimuthal angle.\n\n    \"\"\"\n\n    # Computing the normalized associated Legendre polynomials\n    P = jnp.zeros((deg_view + 1, deg_view + 1))\n    P = P.at[0, 0].set(1)\n    P = P.at[1, 1].set(jnp.sqrt(3) * jnp.cos(theta))\n    for i in range(2, deg_view + 1):\n      for j in range(i + 1):\n        if j == 0:\n          P = P.at[i, j].set(jnp.sqrt(2 * jnp.pi) * jnp.sqrt(1 / (4 * jnp.pi)) * jnp.sqrt(1 / (2 * i + 1)) * (jnp.sqrt(i) * jnp.cos(theta)))\n        elif j == 1:\n          P = P.at[i, j].set(jnp.sqrt(2 * jnp.pi) * jnp.sqrt(1 / (4 * jnp.pi)) * jnp.sqrt(1 / (2 * i + 1)) * (jnp.sqrt(i) * jnp.cos(theta) * jnp.sqrt(1 - jnp.power(jnp"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  import numpy as np\n\n  # Generate the directional encoding function\n  dir_enc_fn = generate_dir_enc_fn_internal(deg_view)\n\n  # Define the function to evaluate the directional encoding\n  def evaluate_dir_enc(pts):\n\n    # Convert the input points to a numpy array\n    pts = np.array(pts)\n\n    # If the input is a single point, reshape it to have shape (1, 3)\n    if len(pts.shape) == 1:\n      pts = pts.reshape(1, 3)\n\n    # Evaluate the directional encoding for each point in the input\n    dir_enc = np.array([dir_enc_fn(x, y, z) for x, y, z in pts])\n\n    # Return the directional encoding as a numpy array\n    return dir_enc\n\n  # Return the function that evaluates the directional encoding\n  return evaluate_dir_enc\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Importing required libraries\n  import torch\n  import numpy as np\n  import math\n  import os\n  import sys\n\n  # Importing the required functions from the utils module\n  from .utils import get_spherical_harmonics, get_spherical_coords, get_dir_enc_fn\n\n  # Generating the integrated directional encoding function\n  dir_enc_fn = get_dir_enc_fn(deg_view)\n\n  # Defining the main function that returns the directional encoding for a given point\n  def dir_enc_fn_main(xyz):\n\n    # Converting the input point(s) to a tensor\n    xyz = torch.tensor(xyz)\n\n    # Ensuring that the input tensor has the correct shape\n    if len(xyz.shape) == 1:\n      xyz = xyz.unsqueeze(0)\n\n    # Getting the spherical coordinates of the input point(s)\n    r, theta, phi = get_spherical_coords(xyz)\n\n    # Getting the spherical harmonics of the input point(s)\n    sh = get_spherical_harmonics(deg_view, theta, phi)\n\n    # Evaluating the directional encoding for the input point(s)\n    dir_enc = dir_enc_fn(r, theta, phi, sh)\n\n    # Returning the directional encoding\n    return dir_enc\n\n  # Returning the main function\n  return dir_enc_fn_main\n"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    text_group = []\n    text_group_index = 0\n    text_group_type = None\n    text_group_header = None\n    text_group_indent = None\n    text_group_list = None\n    text_group_list_level = None\n    text_group_list_index = None\n    text_group_list_item = None\n    text_group_list_item_index = None\n    text_group_list_item_level = None\n    text_group_list_item_list = None\n    text_group_list_item_list_level = None\n    text_group_list_item_list_index = None\n    text_group_list_item_list_item = None\n    text_group_list_item_list_item_index = None\n    text_group_list_item_list_item_level = None\n    text_group_list_item_list_item_list = None\n    text_group_list_item_list_item_list_level = None\n    text_group_list_item_list_item_list_index = None\n    text_group_list_item_list_item_list_item = None\n    text_group_list_item_list_item_list_item_index = None\n    text_group_list_item_list_item_list_item_level = None\n    text_group_list_item_list_item_list_item_list = None\n    text_group_list_item_list_item_list_item_list_level = None\n    text_group_list_item_list_item_list_item_list_index = None\n    text_group_list_item_list_item_list_item_list_item = None\n    text_group_list_item_list_item_list_item_list_item_index = None\n    text_group_list_item_list_item_list_item_list_item_level = None\n    text_group_list_item_list_item_list_item_list_item_list = None\n    text_"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    text_group = []\n    text_group_index = -1\n    text_group_type = None\n    text_group_header = None\n    text_group_level = None\n    text_group_list = []\n    text_group_list_level = None\n    text_group_list_index = -1\n    text_group_list_type = None\n    text_group_list_header = None\n    text_group_list_level = None\n    text_group_list_list = []\n    text_group_list_list_level = None\n    text_group_list_list_index = -1\n    text_group_list_list_type = None\n    text_group_list_list_header = None\n    text_group_list_list_level = None\n    text_group_list_list_list = []\n    text_group_list_list_list_level = None\n    text_group_list_list_list_index = -1\n    text_group_list_list_list_type = None\n    text_group_list_list_list_header = None\n    text_group_list_list_list_level = None\n    text_group_list_list_list_list = []\n    text_group_list_list_list_list_level = None\n    text_group_list_list_list_list_index = -1\n    text_group_list_list_list_list_type = None\n    text_group_list_list_list_list_header = None\n    text_group_list_list_list_list_level = None\n    text_group_list_list_list_list_list = []\n    text_group_list_list_list_list_list_level = None\n    text_group_list_list_list_list_list_index = -1\n    text_group_list_list_list_list_list_type = None\n    text_group_list_list_list_list_list_header = None\n    text_group_list_list_list_list_list_level ="}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Remove duplicate lines\n    lines = list(set(lines))\n\n    # Remove lines with only numbers\n    lines = [line for line in lines if not line.isdigit()]\n\n    # Fix spaced characters\n    lines = [line.replace(\"  \", \" \") for line in lines]\n\n    # Connect incomplete lines\n    lines = [line.strip() for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]\n    lines = [line if line.endswith(\".\") else line + \".\" for line in lines]"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    block_index = 0\n    block_type = 'paragraph'\n    block_text = ''\n    block_start_index = 0\n    block_list = []\n    block_header_index = 0\n    block_level = 0\n    block_list_type = ''\n    block_list_item = ''\n    block_list_item_index = 0\n    block_list_item_level = 0\n    block_list_item_list = []\n    block_list_item_list_index = 0\n    block_list_item_list_level = 0\n    block_list_item_list_list = []\n    block_list_item_list_list_index = 0\n    block_list_item_list_list_level = 0\n    block_list_item_list_list_list = []\n    block_list_item_list_list_list_index = 0\n    block_list_item_list_list_list_level = 0\n    block_list_item_list_list_list_list = []\n    block_list_item_list_list_list_list_index = 0\n    block_list_item_list_list_list_list_level = 0\n    block_list_item_list_list_list_list_list = []\n    block_list_item_list_list_list_list_list_index = 0\n    block_list_item_list_list_list_list_list_level = 0\n    block_list_item_list_list_list_list_list_list = []\n    block_list_item_list_list_list_list_list_list_index = 0\n    block_list_item_list_list_list_list_list_list_level = 0\n    block_list_item_list_list_list_list_list_list_list = []\n    block_list_item_list_list_list_list_list_list_list_index = 0\n    block_list_item_list_list_list_list_list_list_list_level = 0\n    block_list_item_"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    current_block = None\n    current_block_index = None\n    current_block_type = None\n    current_block_start_index = None\n    current_block_list = None\n    current_block_header_index = None\n    current_block_level = None\n    current_block_indent = None\n    current_block_is_list = None\n    current_block_is_list_item = None\n    current_block_is_list_item_header = None\n    current_block_is_list_item_content = None\n    current_block_is_list_item_footer = None\n    current_block_is_list_item_header_footer = None\n    current_block_is_list_item_header_content = None\n    current_block_is_list_item_content_footer = None\n    current_block_is_list_item_header_content_footer = None\n    current_block_is_list_item_header_content_footer_header = None\n    current_block_is_list_item_header_content_footer_header_content = None\n    current_block_is_list_item_header_content_footer_header_content_footer = None\n    current_block_is_list_item_header_content_footer_header_content_footer_header = None\n    current_block_is_list_item_header_content_footer_header_content_footer_header_content = None\n    current_block_is_list_item_header_content_footer_header_content_footer_header_content_footer = None\n    current_block_is_list_item_header_content_footer_header_content_footer_header_content_footer_header = None\n    current_block_is_list_item_header_content_footer_header_content_footer_header_content_footer_header_content = None\n    current_block_is_list_item_header_content_footer_header_content_footer_header_content_footer_header_content_footer = None\n    current_block_is_list_item_header_"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Remove duplicate lines\n    lines = remove_duplicate_lines(lines)\n\n    # Fix spaced characters\n    lines = fix_spaced_characters(lines)\n\n    # Connect incomplete lines\n    lines = connect_incomplete_lines(lines)\n\n    # Initialize variables\n    paragraphs = []\n    headers = []\n    list_items = []\n    current_paragraph = []\n    current_header = None\n    current_list_item = []\n    current_list_item_level = 0\n    current_list_item_index = -1\n    current_list_item_type = None\n    current_list_item_parent = None\n    current_list_item_parent_index = -1\n    current_list_item_parent_type = None\n    current_list_item_parent_level = -1\n    current_list_item_parent_list_items = []\n    current_list_item_parent_list_items_index = -1\n    current_list_item_parent_list_items_type = None\n    current_list_item_parent_list_items_level = -1\n    current_list_item_parent_list_items_list_items = []\n    current_list_item_parent_list_items_list_items_index = -1\n    current_list_item_parent_list_items_list_items_type = None\n    current_list_item_parent_list_items_list_items_level = -1\n    current_list_item_parent_list_items_list_items_list_items = []\n    current_list_item_parent_list_items_list_items_list_items_index = -1\n    current_list_item_parent_list_items_list_items_list_items_type = None\n    current_list_item_parent_list_items_list_items_list_items_level = -1\n    current_list_item_parent_list_items_list_items_list_items_list_items = []\n    current_list_item_parent_list_items_list_items_list_items_list_items"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    block_index = 0\n    block_type = None\n    block_text = ''\n    block_start_index = 0\n    block_list = []\n    header_index = None\n    indent_level = 0\n    list_level = 0\n\n    # Iterate through the lines\n    for i, line in enumerate(lines):\n        # Remove duplicate lines\n        if line in [block['text'] for block in result]:\n            continue\n\n        # Fix spaced characters\n        line = line.replace('\\xa0', ' ')\n\n        # Connect incomplete lines\n        if line.endswith('-') and i < len(lines) - 1:\n            line = line[:-1] + lines[i + 1]\n\n        # Check if the line is a header\n        if line.startswith('#'):\n            # If the current block is not empty, append it to the result list\n            if block_text:\n                result.append({\n                    'block_index': block_index,\n                    'text': block_text,\n                    'type': block_type,\n                    'start_index': block_start_index,\n                    'list': block_list,\n                    'header_index': header_index,\n                    'indent_level': indent_level,\n                    'list_level': list_level\n                })\n                block_index += 1\n\n            # Reset the block variables\n            block_type = 'header'\n            block_text = line\n            block_start_index = i\n            block_list = []\n            header_index = block_index\n            indent_level = 0\n            list_level = 0\n\n        # Check if the line is a list item\n        elif line.startswith('-') or line.startswith('*'):\n            # If the current block is not empty, append it to the result list\n            if block_text:\n                result.append({\n                    'block_index': block_index,\n                    'text': block_text,\n                    'type': block_type,\n                    'start_index': block_start_index,"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Remove duplicate lines\n    lines = list(dict.fromkeys(lines))\n\n    # Fix spaced characters\n    lines = fix_spaced_characters(lines)\n\n    # Connect incomplete lines\n    lines = connect_incomplete_lines(lines)\n\n    # Remove empty lines\n    lines = [line for line in lines if line.strip()]\n\n    # Initialize variables\n    result = []\n    paragraph = []\n    header = None\n    header_index = None\n    list_item = None\n    list_item_index = None\n    list_item_level = None\n\n    # Process each line\n    for i, line in enumerate(lines):\n        # Check if the line is a header\n        if is_header(line):\n            # If a paragraph is being processed, append it to the result list\n            if paragraph:\n                result.append({\n                    'index': len(result),\n                    'text': ' '.join(paragraph),\n                    'type': 'paragraph',\n                    'header_index': header_index,\n                    'list_item_index': list_item_index,\n                    'list_item_level': list_item_level\n                })\n                paragraph = []\n            # If a list item is being processed, append it to the result list\n            if list_item:\n                result.append({\n                    'index': len(result),\n                    'text': ' '.join(list_item),\n                    'type': 'list_item',\n                    'header_index': header_index,\n                    'list_item_index': list_item_index,\n                    'list_item_level': list_item_level\n                })\n                list_item = []\n            # Append the header to the result list\n            result.append({\n                'index': len(result),\n                'text': line,\n                'type': 'header',\n                'header_index': len(result),\n                'list_item_index': list_item_index,\n                'list_item_level': list_item_level\n            })\n            # Update the header index and reset the list item index and level\n            header_index = len(result"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Remove duplicate lines\n    lines = list(set(lines))\n\n    # Fix spaced characters\n    lines = fix_spaces(lines)\n\n    # Connect incomplete lines\n    lines = connect_incomplete_lines(lines)\n\n    # Create a list of blocks\n    blocks = []\n\n    # Create a list of headers\n    headers = []\n\n    # Initialize the block index\n    block_index = 0\n\n    # Initialize the header index\n    header_index = 0\n\n    # Initialize the paragraph index\n    paragraph_index = 0\n\n    # Initialize the list index\n    list_index = 0\n\n    # Initialize the list level\n    list_level = 0\n\n    # Initialize the list item index\n    list_item_index = 0\n\n    # Initialize the list item level\n    list_item_level = 0\n\n    # Initialize the list item block\n    list_item_block = None\n\n    # Initialize the list item blocks\n    list_item_blocks = []\n\n    # Initialize the list item block index\n    list_item_block_index = 0\n\n    # Initialize the list item block level\n    list_item_block_level = 0\n\n    # Initialize the list item block list\n    list_item_block_list = []\n\n    # Initialize the list item block list index\n    list_item_block_list_index = 0\n\n    # Initialize the list item block list level\n    list_item_block_list_level = 0\n\n    # Initialize the list item block list item index\n    list_item_block_list_item_index = 0\n\n    # Initialize the list item block list item level\n    list_item_block_list_item_level = 0\n\n    # Initialize the list item block list item block\n    list_item_block_list_item_block = None\n\n    # Initialize the list item block list item blocks\n    list_item_block_list_item_blocks = []\n\n    # Initialize the list item block list item block index\n    list_item_block_list_item_block_index = 0"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    text = ''\n    text_group_index = -1\n    text_group_type = 'paragraph'\n    text_group_start_index = -1\n    text_group_list = []\n    header_index = -1\n    list_index = -1\n    indentation_index = -1\n    list_item_index = -1\n    list_item_start_index = -1\n    list_item_end_index = -1\n    list_item_text = ''\n    list_item_text_group_list = []\n    list_item_text_group_index = -1\n    list_item_text_group_type = 'paragraph'\n    list_item_text_group_start_index = -1\n    list_item_text_group_list = []\n    list_item_text_group_index = -1\n    list_item_text_group_type = 'paragraph'\n    list_item_text_group_start_index = -1\n    list_item_text_group_list = []\n    list_item_text_group_index = -1\n    list_item_text_group_type = 'paragraph'\n    list_item_text_group_start_index = -1\n    list_item_text_group_list = []\n    list_item_text_group_index = -1\n    list_item_text_group_type = 'paragraph'\n    list_item_text_group_start_index = -1\n    list_item_text_group_list = []\n    list_item_text_group_index = -1\n    list_item_text_group_type = 'paragraph'\n    list_item_text_group_start_index = -1\n    list_item_text_group_list = []\n    list_item_text_group_index = -1\n    list_item_text_group_type = 'paragraph'\n    list_item_text_group_start_index = -1\n    list_item_text_group_list = []\n    list_item_text_group_index"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    block_index = 0\n    block_type = None\n    block_text = \"\"\n    block_start_index = None\n    block_list = []\n    block_header_index = None\n    block_level = None\n    header_index = 0\n    header_text = \"\"\n    header_start_index = None\n    header_level = None\n    list_index = 0\n    list_text = \"\"\n    list_start_index = None\n    list_level = None\n\n    # Loop through each line\n    for i, line in enumerate(lines):\n        # Remove duplicate lines (ignoring numbers)\n        if line not in [l[\"text\"] for l in result] or not line.strip().isdigit():\n            # Fix spaced characters\n            line = fix_spaced_characters(line)\n            # Connect incomplete lines\n            if block_text and line.startswith(block_text[-1]):\n                block_text += line.strip()\n            else:\n                # Determine block type\n                if xml:\n                    if line.startswith(\"<p>\"):\n                        block_type = \"paragraph\"\n                    elif line.startswith(\"<h\"):\n                        block_type = \"header\"\n                    elif line.startswith(\"<li>\"):\n                        block_type = \"list\"\n                else:\n                    if line.startswith(\"<p>\"):\n                        block_type = \"paragraph\"\n                    elif line.startswith(\"<h\"):\n                        block_type = \"header\"\n                    elif line.startswith(\"<li>\"):\n                        block_type = \"list\"\n\n                # Append previous block to result if it exists\n                if block_text:\n                    result.append({\n                        \"index\": block_index,\n                        \"text\": block_text,\n                        \"type\": block_type,\n                        \"start_index\": block_start_index,\n                        \"list\": block_list,\n                        \"header_index\": block_header_index,\n                        \"level\": block_level\n                    })\n\n                # Res"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Remove duplicate lines\n    lines = list(set(lines))\n\n    # Fix spaced characters\n    lines = [line.replace('\\xa0', ' ') for line in lines]\n\n    # Connect incomplete lines\n    for i in range(len(lines) - 1):\n        if lines[i].endswith('-') and lines[i + 1].startswith('-'):\n            lines[i] = lines[i][:-1] + lines[i + 1]\n            lines[i + 1] = ''\n\n    # Initialize variables\n    result = []\n    current_block = {'text': '', 'type': 'paragraph', 'index': 0, 'list': [], 'header': 0, 'level': 0}\n    header_blocks = []\n    header_block_index = 0\n    list_block_index = 0\n    list_block_level = 0\n\n    # Process each line\n    for i, line in enumerate(lines):\n        line = line.strip()\n\n        # Check if the line is empty\n        if not line:\n            continue\n\n        # Check if the line is a header\n        if xml and line.startswith('<h'):\n            header_block_index += 1\n            header_blocks.append(i)\n            current_block['type'] = 'header'\n            current_block['header'] = header_block_index\n            current_block['level'] = int(line[2])\n            current_block['text'] += line\n        # Check if the line is a list item\n        elif xml and line.startswith('<li'):\n            list_block_index += 1\n            list_block_level = int(line[3])\n            current_block['type'] = 'list'\n            current_block['list'] = [i]\n            current_block['text'] += line\n        # Check if the line is a continuation of the previous block\n        elif current_block['type'] == 'header' and line.startswith('<'):\n            current_block['text'] += line\n        elif current_block['type'] == 'list' and line.start"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    current_block = None\n    current_header_block = None\n    current_list_block = None\n    current_list_item_block = None\n    current_paragraph_block = None\n    current_list_item_level = 0\n    current_paragraph_level = 0\n    current_list_item_index = 0\n    current_paragraph_index = 0\n    current_header_index = 0\n    current_list_index = 0\n    current_block_index = 0\n    current_block_type = None\n    current_block_text = ''\n    current_block_start_index = 0\n    current_block_list = []\n    current_block_header_index = None\n    current_block_indent_level = 0\n    current_block_list_level = 0\n\n    # Process each line\n    for i, line in enumerate(lines):\n\n        # Remove duplicate lines\n        if line in [block['text'] for block in result]:\n            continue\n\n        # Fix spaced characters\n        line = line.replace('\\xa0', ' ')\n\n        # Connect incomplete lines\n        if current_block_text and line.startswith(current_block_text[-1]):\n            current_block_text += line[1:]\n            continue\n\n        # Categorize lines\n        if xml and line.startswith('<'):\n            current_block_type = 'xml'\n        elif line.startswith('#'):\n            current_block_type = 'header'\n            current_header_index = current_block_index\n        elif line.startswith('-'):\n            current_block_type = 'list_item'\n            current_list_item_level = line.count('-')\n            current_list_item_index = current_block_index\n        elif line.startswith('*'):\n            current_block_type = 'list_item'\n            current_list_item_level = line.count('*')\n            current_list_item_index = current_block_index\n        else:\n            current_block_type"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n\n    # Create a dictionary to store the index of each header block\n    header_indices = {}\n\n    # Loop through each line in the input list\n    for i, line in enumerate(lines):\n\n        # Remove any leading or trailing whitespace characters\n        line = line.strip()\n\n        # Check if the line is empty\n        if not line:\n\n            # If the line is empty, skip to the next iteration\n            continue\n\n        # Check if the line is a header\n        if line.startswith('#'):\n\n            # If the line is a header, add it to the result list with the appropriate metadata\n            header_indices[i] = len(result)\n            result.append({'index': i, 'text': line, 'type': 'header', 'group': i, 'blocks': [], 'header': i, 'level': line.count('#')})\n\n        # Check if the line is a list item\n        elif line.startswith('-'):\n\n            # If the line is a list item, add it to the result list with the appropriate metadata\n            result.append({'index': i, 'text': line, 'type': 'list', 'group': i, 'blocks': [], 'header': None, 'level': line.count('-')})\n\n        # If the line is not a header or list item, it is assumed to be a paragraph\n        else:\n\n            # Check if the line is incomplete\n            if line.endswith('-'):\n\n                # If the line is incomplete, add it to the result list with the appropriate metadata\n                result.append({'index': i, 'text': line, 'type': 'paragraph', 'group': i, 'blocks': [], 'header': None, 'level': 0})\n\n            # If the line is not incomplete, add it to the result list with the appropriate metadata\n            else:\n\n                # Check if the line contains any numbers\n                if any(char.isdigit() for char in line):\n\n                    # If the line contains numbers, add it to the result list with the appropriate metadata\n                    result.append({'index': i, 'text': line"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Remove duplicate lines\n    lines = list(set(lines))\n\n    # Fix spaced characters\n    lines = fix_spaced_characters(lines)\n\n    # Connect incomplete lines\n    lines = connect_incomplete_lines(lines)\n\n    # Categorize lines\n    lines = categorize_lines(lines)\n\n    # Create a list of dictionaries with metadata for each block of text\n    result = []\n    for i, line in enumerate(lines):\n        block = {}\n        block['index'] = i\n        block['text'] = line['text']\n        block['type'] = line['type']\n        block['start'] = line['start']\n        block['blocks'] = []\n        block['header'] = None\n        block['level'] = None\n        if 'level' in line:\n            block['level'] = line['level']\n        result.append(block)\n\n    # Add metadata for related header blocks\n    for i, block in enumerate(result):\n        if block['type'] == 'paragraph':\n            for j in range(i, -1, -1):\n                if result[j]['type'] == 'header':\n                    block['header'] = j\n                    break\n\n    # Add metadata for list items\n    for i, block in enumerate(result):\n        if block['type'] == 'list_item':\n            for j in range(i, len(result)):\n                if result[j]['type'] == 'header':\n                    block['header'] = j\n                    break\n                elif result[j]['type'] == 'list_item':\n                    block['blocks'].append(j)\n                else:\n                    break\n\n    return result\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize empty lists and dictionaries\n    result = []\n    group = []\n    group_index = 0\n    group_type = 'paragraph'\n    group_header = None\n    group_indent = 0\n    group_list = []\n\n    # Define a function to remove duplicate lines\n    def remove_duplicate_lines(lines):\n        \"\"\"\n        Removes duplicate lines from a list of strings.\n\n        :param lines: List of strings. The lines to be processed.\n        :return: List of strings. The lines with duplicates removed.\n        \"\"\"\n        # Initialize an empty set to store unique lines\n        unique_lines = set()\n\n        # Initialize an empty list to store the result\n        result = []\n\n        # Iterate over the lines\n        for line in lines:\n            # Strip leading and trailing whitespace from the line\n            line = line.strip()\n\n            # If the line is not already in the set of unique lines, add it to the set and the result list\n            if line not in unique_lines:\n                unique_lines.add(line)\n                result.append(line)\n\n        # Return the result list\n        return result\n\n    # Define a function to fix spaced characters\n    def fix_spaced_characters(line):\n        \"\"\"\n        Fixes spaced characters in a string by replacing certain characters with their corresponding ASCII equivalents.\n\n        :param line: String. The line to be processed.\n        :return: String. The line with spaced characters fixed.\n        \"\"\"\n        # Define a dictionary of spaced characters and their corresponding ASCII equivalents\n        spaced_characters = {\n            '\u2013': '-',\n            '\u2014': '-',\n            '\u2018': \"'\",\n            '\u2019': \"'\",\n            '\u201c': '\"',\n            '\u201d': '\"',\n            '\u2026': '...',\n            '\u2022': '*',\n            '\u00b0': 'o',\n            '\u00bc': '1/4',\n            '\u00bd': '1/2',\n            '\u00be': '3/4',\n            '\u00d7': 'x',\n            '\u2122': '(TM)',"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    i = 0\n    header_index = 0\n    header_indices = []\n    list_index = 0\n    list_indices = []\n    paragraph_index = 0\n    paragraph_indices = []\n    header_blocks = []\n    list_blocks = []\n    paragraph_blocks = []\n    last_line = ''\n\n    # Loop through each line\n    for line in lines:\n\n        # Skip empty lines\n        if line.strip() == '':\n            continue\n\n        # Remove duplicate lines (ignoring numbers)\n        if line.strip() == last_line.strip() or (line.strip().isdigit() and last_line.strip().isdigit()):\n            continue\n\n        # Fix spaced characters\n        line = line.replace('\\xa0', ' ')\n\n        # Connect incomplete lines\n        if line.strip().startswith('-') and last_line.strip().endswith('-'):\n            line = last_line.strip() + line.strip()[1:]\n\n        # Categorize lines into paragraphs, headers, or list items\n        if line.strip().startswith('<') and line.strip().endswith('>'):\n            block_type = 'header'\n            header_index += 1\n            header_indices.append(i)\n        elif line.strip().startswith('-'):\n            block_type = 'list'\n            list_index += 1\n            list_indices.append(i)\n        else:\n            block_type = 'paragraph'\n            paragraph_index += 1\n            paragraph_indices.append(i)\n\n        # Append block to result list with metadata\n        result.append({\n            'index': i,\n            'text': line.strip(),\n            'type': block_type,\n            'group': i if block_type == 'header' else paragraph_index,\n            'list': list_index if block_type == 'list' else None,\n            'header': header_index if block_type == 'header' else None,\n            'level':"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize empty lists and variables\n    result = []\n    header_list = []\n    header_index = None\n    list_index = None\n    list_level = None\n    paragraph_index = None\n    paragraph_list = []\n    paragraph_text = \"\"\n\n    # Iterate over each line in the input list\n    for i, line in enumerate(lines):\n\n        # Remove duplicate lines (ignoring numbers)\n        if line not in [l[\"text\"] for l in result] or not line.strip().replace(\".\", \"\").isdigit():\n\n            # Fix spaced characters\n            line = line.replace(\"  \", \" \")\n            line = line.replace(\"  \", \" \")\n            line = line.replace(\"  \", \" \")\n\n            # Connect incomplete lines\n            if line.endswith(\"-\"):\n                line = paragraph_text + line\n            else:\n                line = paragraph_text + \" \" + line\n\n            # Categorize lines into paragraphs, headers, or list items\n            if line.startswith(\"<h\"):\n                if paragraph_text:\n                    result.append({\n                        \"index\": paragraph_index,\n                        \"text\": paragraph_text,\n                        \"type\": \"paragraph\",\n                        \"paragraph_index\": paragraph_index,\n                        \"list_index\": list_index,\n                        \"list_level\": list_level,\n                        \"header_index\": header_index,\n                        \"header_list\": header_list\n                    })\n                    paragraph_text = \"\"\n\n                header_list.append(line)\n                header_index = i\n                result.append({\n                    \"index\": i,\n                    \"text\": line,\n                    \"type\": \"header\",\n                    \"header_index\": i,\n                    \"header_list\": header_list\n                })\n\n            elif line.startswith(\"<li\"):\n                if paragraph_text:\n                    result.append({\n                        \"index\": paragraph_index,\n                        \"text\": paragraph_text,\n                        \"type\": \"paragraph\",\n                        \"paragraph_index\": paragraph_index,\n                        \"list_index\": list_index,\n                        \"list"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Clean the text lines by removing duplicate lines and fixing spaced characters\n    lines = clean_text(lines, xml)\n\n    # Initialize variables\n    result = []\n    block_index = 0\n    text_group_index = 0\n    text_group = []\n    header_index = None\n    header_block = None\n    list_level = None\n\n    # Iterate through the lines of text\n    for i, line in enumerate(lines):\n        # Check if the line is a header\n        if line.startswith('#'):\n            # If the current block is not empty, append it to the result list\n            if text_group:\n                result.append(create_block(block_index, text_group, text_group_index, header_index, list_level))\n                block_index += 1\n                text_group = []\n            # Append the header line to the result list with its index as the header block\n            result.append(create_block(block_index, [line], i, i, list_level))\n            header_index = block_index\n            block_index += 1\n        # Check if the line is a list item\n        elif line.startswith('-'):\n            # If the current block is not empty, append it to the result list\n            if text_group:\n                result.append(create_block(block_index, text_group, text_group_index, header_index, list_level))\n                block_index += 1\n                text_group = []\n            # Append the list item line to the result list with its index as the header block\n            result.append(create_block(block_index, [line], i, header_index, 1))\n            block_index += 1\n        # Check if the line is a continuation of the previous line\n        elif line.startswith(' ') and text_group:\n            # If the current block is not empty, append it to the result list\n            if text_group:\n                result.append(create_block(block_index, text_group, text_group_index, header_index, list_level))\n                block_index += 1"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    index = 0\n    text_group_index = None\n    text_group = []\n    text_group_type = None\n    list_index = None\n    list_level = None\n    header_index = None\n    header_level = None\n    indentation_level = None\n    current_indentation_level = None\n    current_list_level = None\n    current_list_index = None\n    current_header_level = None\n    current_header_index = None\n    current_text_group = None\n    current_text_group_index = None\n    current_text_group_type = None\n\n    # Process each line\n    for line in lines:\n\n        # Check if the line is empty\n        if not line.strip():\n            continue\n\n        # Check if the line is a list item\n        if line.startswith(\"-\"):\n            current_list_level = 1\n            current_list_index = index\n\n        # Check if the line is a header\n        if line.startswith(\"#\"):\n            current_header_level = line.count(\"#\")\n            current_header_index = index\n\n        # Check if the line is a list item or a header\n        if current_list_level is not None or current_header_level is not None:\n\n            # Check if the line is a list item\n            if current_list_level is not None:\n\n                # Check if the line is a list item of the same level\n                if line.startswith(\"-\"):\n\n                    # Check if the list level has changed\n                    if current_list_level != current_indentation_level:\n                        current_list_level = current_indentation_level\n                        current_list_index = index\n\n                # Check if the line is a list item of a lower level\n                elif line.startswith(\" \" * 4):\n\n                    # Check if the list level has changed\n                    if current_list_level != current_indentation_level + 1:\n                        current_list_level = current_indentation_level + 1\n                        current_list_index = index"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # Apply the tokenization rules\n    for rule in rules:\n        org_texts = space_rule(org_texts)\n        org_texts = bracket_rule(org_texts)\n        org_texts = rule(org_texts)\n\n    # Normalize quotation marks\n    org_texts = re.sub(quotation_pattern, r\"`` \\1 ''\", org_texts)\n\n    # Tokenize the text into sentences\n    sentences = nltk_tokenzier.tokenize(org_texts)\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or len(org_texts) == 0:\n        return org_texts\n\n    # apply space rule\n    space_rule = r\"\\s+\"\n    org_texts = re.sub(space_rule, \" \", org_texts)\n\n    # apply bracket rule\n    bracket_rule = r\"\\s+(?=\\))\"\n    org_texts = re.sub(bracket_rule, \"\", org_texts)\n\n    # apply rules\n    rules = [\n        r\"\\s+\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\",\n        r\"\\s+(?=\\))\","}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # Apply the tokenization rules\n    tokens = space_rule(org_texts)\n    tokens = bracket_rule(tokens)\n    tokens = rules(tokens)\n\n    # Normalize the quotation marks\n    tokens = [quotation_pattern.sub(r'\"', t) for t in tokens]\n\n    # Tokenize the sentences using the NLTK tokenizer\n    sentences = [\n        \" \".join(t) for t in nltk_tokenzier.tokenize_sents([tokens])\n    ]\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # If the input text is empty or None, return the input as is\n    if not org_texts:\n        return org_texts\n\n    # Apply the space rule to the input text\n    texts = space_rule(org_texts)\n\n    # Apply the bracket rule to the input text\n    texts = bracket_rule(texts)\n\n    # Apply the tokenization rules to the input text\n    texts = [rules(text) for text in texts]\n\n    # Replace quotation marks in the input text with a standard quotation mark\n    texts = [quotation_pattern.sub(\"\\\"\", text) for text in texts]\n\n    # Tokenize the input text using the nltk_tokenzier\n    texts = [nltk_tokenzier(text) for text in texts]\n\n    # Flatten the list of lists of tokenized sentences\n    texts = [sent for sent_list in texts for sent in sent_list]\n\n    # Return the tokenized sentences\n    return texts\n\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # Normalize quotation marks\n    org_texts = quotation_pattern.sub(r'\"', org_texts)\n\n    # Tokenize the text into sentences\n    sentences = []\n    for sent in nltk_tokenzier.tokenize(org_texts):\n        # Apply the tokenization rules\n        sent = space_rule(sent)\n        sent = bracket_rule(sent)\n        for rule in rules:\n            sent = rule(sent)\n\n        # Append the sentence to the list of sentences\n        sentences.append(sent)\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # Check if the input text is empty or None\n    if not org_texts:\n        return org_texts\n\n    # Apply the space rule to the original text\n    space_rule = re.sub(r\"\\s+\", \" \", org_texts)\n\n    # Apply the bracket rule to the space-processed text\n    bracket_rule = re.sub(r\"\\(\", \"( \", bracket_rule)\n    bracket_rule = re.sub(r\"\\)\", \" )\", bracket_rule)\n\n    # Apply the rules to the bracket-processed text\n    rules = [\n        (\"\\n\", \" \"),\n        (\"\\t\", \" \"),\n        (\"\\xa0\", \" \"),\n        (\"\\r\", \" \"),\n        (\"\\u200b\", \" \"),\n        (\"\\u200e\", \" \"),\n        (\"\\u200f\", \" \"),\n        (\"\\u202a\", \" \"),\n        (\"\\u202c\", \" \"),\n        (\"\\u202d\", \" \"),\n        (\"\\u202e\", \" \"),\n        (\"\\u2060\", \" \"),\n        (\"\\u3000\", \" \"),\n        (\"\\ufeff\", \" \"),\n        (\"\\u2009\", \" \"),\n        (\"\\u200a\", \" \"),\n        (\"\\u200c\", \" \"),\n        (\"\\u200d\", \" \"),\n        (\"\\u206a\", \" \"),\n        (\"\\u206b\", \" \"),\n        (\"\\u206c\", \" \"),\n        (\"\\u206d\", \" \"),\n        (\"\\u206e\", \" \"),\n        (\"\\u206f\", \" \"),\n        (\"\\u2028\", \" \"),\n        (\"\\u2029\", \" \"),\n        (\"\\u202f\", \" \"),\n        (\"\\u205f\", \" \"),\n        (\"\\u3"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # Check if the input text is empty or None\n    if not org_texts:\n        return org_texts\n\n    # Apply space rule to split the text into sentences\n    sentences = space_rule(org_texts)\n\n    # Apply bracket rule to handle sentences within brackets\n    sentences = bracket_rule(sentences)\n\n    # Apply rules to handle special cases\n    for rule in rules:\n        sentences = [rule(sentence) for sentence in sentences]\n\n    # Apply quotation pattern to normalize quotation marks\n    sentences = [quotation_pattern.sub('\"', sentence) for sentence in sentences]\n\n    # Tokenize the sentences using the nltk_tokenzier\n    sentences = [nltk_tokenzier(sentence) for sentence in sentences]\n\n    # Flatten the list of lists into a single list\n    sentences = [sentence for sublist in sentences for sentence in sublist]\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # Apply space rule\n    space_rule = re.compile(r'(\\s*\\n\\s*)+')\n    org_texts = space_rule.sub('\\n', org_texts)\n\n    # Apply bracket rule\n    bracket_rule = re.compile(r'(\\s*\\(\\s*)+')\n    org_texts = bracket_rule.sub('(', org_texts)\n\n    # Apply rules\n    rules = [\n        {r'([a-zA-Z])\\.([A-Z])': r'\\1. \\2'},\n        {r'([a-zA-Z])\\.\\s+([a-zA-Z])': r'\\1. \\2'},\n        {r'([a-zA-Z])\\.\\s+([0-9])': r'\\1. \\2'},\n        {r'([0-9])\\.\\s+([a-zA-Z])': r'\\1. \\2'},\n        {r'([0-9])\\.\\s+([0-9])': r'\\1. \\2'},\n        {r'([a-zA-Z])\\s+([a-zA-Z])': r'\\1 \\2'},\n        {r'([a-zA-Z])\\s+([0-9])': r'\\1 \\2'},\n        {r'([0-9])\\s+([a-zA-Z])': r'\\1 \\2'},\n        {r'([0-9])\\s+([0-9])': r'\\1 \\2'},\n        {r'([a-zA-Z])\\-([a-zA-Z])': r'\\1\\2'},\n        {r'([a-zA-Z])\\-([0-9])': r'\\1\\2'},\n        {r'([0-9])\\-([a-zA-Z])': r'\\1\\2'},\n        {r'([0-9])\\-([0-9])': r'\\1\\2'},\n        {r'"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or len(org_texts) == 0:\n        return org_texts\n\n    # Apply the space rule\n    space_rule = re.compile(r\"\\s+\")\n    org_texts = space_rule.sub(\" \", org_texts)\n\n    # Apply the bracket rule\n    bracket_rule = re.compile(r\"\\([^\\)]+\\)\")\n    org_texts = bracket_rule.sub(\"\", org_texts)\n\n    # Apply the rules\n    rules = [\n        {\"rule\": re.compile(r\"[^\\w\\s]\"), \"repl\": \"\"},\n        {\"rule\": re.compile(r\"\\s+\"), \"repl\": \" \"},\n        {\"rule\": re.compile(r\"^\\s+\"), \"repl\": \"\"},\n        {\"rule\": re.compile(r\"\\s+$\"), \"repl\": \"\"},\n    ]\n    for rule in rules:\n        org_texts = rule[\"rule\"].sub(rule[\"repl\"], org_texts)\n\n    # Normalize quotation marks\n    quotation_pattern = re.compile(r\"[\\u2018\\u2019\\u201C\\u201D\\u201E\\u201F\\u2039\\u203A\\u300C\\u300D\\u300E\\u300F\\uFF08\\uFF09\\uFF3B\\uFF3D\\uFF5B\\uFF5D\\uFF5F\\uFF62\\uFF63]\")\n    org_texts = quotation_pattern.sub(\"'\", org_texts)\n\n    # Tokenize the text\n    sentences = nltk_tokenzier.tokenize(org_texts)\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # Apply space rule\n    org_texts = space_rule(org_texts)\n\n    # Apply bracket rule\n    org_texts = bracket_rule(org_texts)\n\n    # Apply rules\n    for rule in rules:\n        org_texts = re.sub(rule[0], rule[1], org_texts)\n\n    # Normalize quotation marks\n    org_texts = re.sub(quotation_pattern, r\"``\\2''\", org_texts)\n\n    # Tokenize sentences\n    sentences = nltk_tokenzier.tokenize(org_texts)\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    space_rule = lambda x: re.sub(r'([.,])([^0-9])', r'\\1 \\2', x)\n    bracket_rule = lambda x: re.sub('([\\u4e00-\\u9fff])([)(\\]])', r'\\1\\2', x)\n    rules = [space_rule, bracket_rule]\n    quotation_pattern = re.compile(r'([\"\\'])(.*?)(\\1)')\n\n    tokenized_texts = []\n    for text in org_texts:\n        text = text.replace('\\n', '')\n        text = quotation_pattern.sub(r'\\2', text)\n        for rule in rules:\n            text = rule(text)\n        tokenized_texts.extend(nltk_tokenzier.tokenize(text))\n    return tokenized_texts\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts == None or org_texts == \"\":\n        return org_texts\n\n    # Apply space rule\n    space_rule = r\"(?<=[a-z])\\.(?=[A-Z])\"\n    org_texts = re.sub(space_rule, \". \", org_texts)\n\n    # Apply bracket rule\n    bracket_rule = r\"(?<=[a-z])\\.(?=[\\[|\\(])\"\n    org_texts = re.sub(bracket_rule, \". \", org_texts)\n\n    # Apply rules\n    rules = [\n        {r\"\\n\": \" \"},\n        {r\"\\s+\": \" \"},\n        {r\"\\t\": \" \"},\n        {r\"\\s+\\.\\s+\": \". \"},\n        {r\"\\s+,+\\s+\": \", \"},\n        {r\"\\s+\\?+\\s+\": \"? \"},\n        {r\"\\s+\\!+\\s+\": \"!\"},\n        {r\"\\s+;+\\s+\": \"; \"},\n        {r\"\\s+:+\\s+\": \": \"},\n        {r\"\\s+\\-+\\s+\": \"-\"},\n        {r\"\\s+\\++\\s+\": \"+\"},\n        {r\"\\s+\\*+\\s+\": \"*\"},\n        {r\"\\s+/+\\s+\": \"/\"},\n        {r\"\\s+\\|+\\s+\": \"|\"},\n        {r\"\\s+\\&+\\s+\": \"&\"},\n        {r\"\\s+\\^+\\s+\": \"^\"},\n        {r\"\\s+%+\\s+\": \"%\"},\n        {r\"\\s+\\$+\\s+\": \"$\"},\n        {r\"\\s+\\#+\\s+\": \"#\"},\n        {r\"\\s+@+\\s+\": \"@\"},\n        {r\"\\s+~+\\s+\": \"~\"},\n        {r\""}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None:\n        return org_texts\n\n    # Check if the input is a string\n    if not isinstance(org_texts, str):\n        return org_texts\n\n    # Check if the input is empty\n    if not org_texts:\n        return org_texts\n\n    # Apply space rule\n    space_rule = re.sub(r\"\\s+\", \" \", org_texts)\n\n    # Apply bracket rule\n    bracket_rule = re.sub(r\"\\(([^)]+)\\)\", r\"\\1\", space_rule)\n\n    # Apply rules\n    rules = [\n        (r'[\\.\\,\\?\\!\\:\\;]', r'\\1\\n'),\n        (r'[ ]+', ' '),\n        (r'[\\\"]', '\"'),\n        (r'[\\']', \"'\"),\n        (r'[\\u2018\\u2019]', \"'\"),\n        (r'[\\u201c\\u201d]', '\"'),\n        (r'[\\u2013\\u2014]', '-'),\n        (r'[\\u2026]', '...'),\n        (r'[\\u201C\\u201D]', '\"'),\n        (r'[\\u2014\\u2014]', '--'),\n        (r'[\\u2013\\u2014]', '--'),\n        (r'[\\u2026]', '...'),\n        (r'[\\u201C\\u201D]', '\"'),\n        (r'[\\u2014\\u2014]', '--'),\n        (r'[\\u2013\\u2014]', '--'),\n        (r'[\\u2026]', '...'),\n        (r'[\\u201C\\u201D]', '\"'),\n        (r'[\\u2014\\u2014]', '--'),\n        (r'[\\u20"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # If the input text is empty or None, return the input as is\n    if not org_texts:\n        return org_texts\n\n    # Apply the space rule to the input text\n    space_rule = re.sub(r\"([a-z])\\.([A-Z])\", r\"\\1. \\2\", org_texts)\n    # Apply the bracket rule to the space-rule-applied text\n    bracket_rule = re.sub(r\"([a-z])\\.([A-Z])\", r\"\\1. \\2\", space_rule)\n    # Apply the rules to the bracket-rule-applied text\n    rules = re.sub(r\"([a-z])\\.([A-Z])\", r\"\\1. \\2\", bracket_rule)\n    # Apply the quotation pattern to the rules-applied text\n    quotation_pattern = re.sub(r\"([a-z])\\.([A-Z])\", r\"\\1. \\2\", rules)\n\n    # Tokenize the quotation-pattern-applied text using the nltk_tokenizer\n    tokens = nltk_tokenizer.tokenize(quotation_pattern)\n\n    # Normalize the tokens by removing extra whitespaces and quotation marks\n    normalized_tokens = [re.sub(r\"\\s+\", \" \", token).strip() for token in tokens]\n\n    return normalized_tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # Apply space rule\n    space_rule = re.compile(r'(\\S+(?:\\s+\\S+)+)')\n    org_texts = space_rule.sub(lambda x: x.group(1).replace(' ', '\\n'), org_texts)\n\n    # Apply bracket rule\n    bracket_rule = re.compile(r'(\\([^\\)]+\\))')\n    org_texts = bracket_rule.sub(lambda x: x.group(1).replace('\\n', ' '), org_texts)\n\n    # Apply rules\n    for rule in rules:\n        org_texts = re.sub(rule[0], rule[1], org_texts)\n\n    # Normalize quotation marks\n    quotation_pattern = re.compile(r'[\u201c\u201d]')\n    org_texts = quotation_pattern.sub('\"', org_texts)\n\n    # Tokenize\n    sentences = nltk_tokenizer.tokenize(org_texts)\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # Normalize quotation marks\n    org_texts = quotation_pattern.sub('\"', org_texts)\n\n    # Apply tokenization rules\n    for rule in rules:\n        org_texts = rule(org_texts)\n\n    # Tokenize into sentences\n    sentences = nltk_tokenzier.tokenize(org_texts)\n\n    # Apply bracket rule\n    sentences = bracket_rule(sentences)\n\n    # Apply space rule\n    sentences = space_rule(sentences)\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # Apply the space rule to handle cases where sentences are separated by new lines\n    space_rule = re.compile(r\"(\\.|\\?|\\!)\\s+\")\n    org_texts = space_rule.sub(r\"\\1\\n\", org_texts)\n\n    # Apply the bracket rule to handle cases where sentences are separated by brackets\n    bracket_rule = re.compile(r\"\\n\\n+\")\n    org_texts = bracket_rule.sub(r\"\\n\", org_texts)\n\n    # Apply the rules to handle special cases\n    rules = [\n        {r\"^\\\"\": u\"\\\"\"},\n        {r\"(``)\": u\"\\\"\"},\n        {r\"([ (\\[{<])(\\\"|\\'{2})\": r\"\\1\\\"\"},\n        {r\"(.*)(\\.{2,})\": r\"\\1...\"},\n        {r\"(.*)(\\.{2,})\": r\"\\1...\"},\n        {r\"(.*)(\\.{2,})\": r\"\\1...\"},\n        {r\"([^\\.])(\\.)([\\]\\)}>\\\"\\']*)\\s*$\": r\"\\1\\2\\3\"},\n        {r\"(\\.)$\": r\"\\.\"},\n        {r\"\\s+\": u\" \"},\n        {r\"\\s+\": u\" \"},\n        {r\"\\s+\": u\" \"},\n        {r\"\\s+\": u\" \"},\n        {r\"\\s+\": u\" \"},\n        {r\"\\s+\": u\" \"},\n        {r\"\\s+\": u\" \"},\n        {r\"\\s+\": u\" \"},\n        {r\"\\s+\": u\" \"},\n        {r\"\\s+\": u\" \"},\n        {r\"\\s+\": u\" \"},\n        {r\"\\s+\": u\" \"},\n        {r\"\\s+\": u\" \"},\n        {r\"\\s+\": u"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # Check if the input text is empty or None\n    if not org_texts:\n        return org_texts\n\n    # Apply the space rule to the input text\n    space_rule = re.sub(r\"\\s+\", \" \", org_texts)\n\n    # Apply the bracket rule to the space-rule output\n    bracket_rule = re.sub(r\"([\\(\\[](.*?)[\\)\\]])\", r\"\\2\", space_rule)\n\n    # Apply the rules to the bracket-rule output\n    rules = re.compile(r\"[^\\.\\!\\?]+[\\.\\!\\?]\")\n    sentences = rules.findall(bracket_rule)\n\n    # Normalize quotation marks in the input text\n    quotation_pattern = re.compile(r\"[\u00ab\u00bb\u201c\u201d\u201e]\")\n    sentences = [quotation_pattern.sub(r'\"', s) for s in sentences]\n\n    # Tokenize the sentences using the nltk_tokenizer\n    sentences = [nltk_tokenizer.tokenize(s) for s in sentences]\n\n    # Flatten the list of tokenized sentences\n    sentences = [sent for sent_list in sentences for sent in sent_list]\n\n    # Return the list of tokenized sentences\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or len(org_texts) == 0:\n        return org_texts\n\n    space_rule = r\"\\s+\"\n    bracket_rule = r\"\\)\\s+\"\n    rules = [\n        {\n            \"rule\": space_rule,\n            \"repl\": \" \",\n        },\n        {\n            \"rule\": bracket_rule,\n            \"repl\": \") \",\n        },\n    ]\n\n    for rule in rules:\n        org_texts = re.sub(rule[\"rule\"], rule[\"repl\"], org_texts)\n\n    quotation_pattern = re.compile(r\"(?<=\\w)['\\\"](?=\\w)\")\n    org_texts = quotation_pattern.sub('\"', org_texts)\n\n    sentences = nltk_tokenzier.tokenize(org_texts)\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or len(org_texts) == 0:\n        return org_texts\n    else:\n        # Apply space rule\n        org_texts = space_rule(org_texts)\n\n        # Apply bracket rule\n        org_texts = bracket_rule(org_texts)\n\n        # Apply rules\n        for rule in rules:\n            org_texts = re.sub(rule, '', org_texts)\n\n        # Normalize quotation marks\n        org_texts = re.sub(quotation_pattern, r'\"', org_texts)\n\n        # Tokenize\n        org_texts = nltk_tokenzier.tokenize(org_texts)\n\n        return org_texts\n\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return [np.where(self.data[i] == token)[0] for i in range(self.data.shape[0])]\n        else:\n            return np.where(self.data[key] == token)[0]"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key:\n            return self.positions_by_key[token][key]\n        else:\n            return self.positions_by_key[token]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return [np.array(self.positions[token])]\n        else:\n            return [np.array(self.positions[token][key])]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return [self.positions(token, key=key) for key in self.keys()]\n        else:\n            return self.get_positions(token, key)\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key:\n            return self.positions_dict[token][key]\n        else:\n            return [pos for pos_list in self.positions_dict[token].values() for pos in pos_list]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            positions = [np.array(self.positions[token][key]) for key in self.positions[token]]\n        else:\n            positions = [np.array(self.positions[token][key])]\n        return positions\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return [self.positions(token, key=key) for key in self.keys()]\n        else:\n            return self.get_positions(token, key)\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            return self.positions_dict[token][key]\n        else:\n            return [pos for pos in self.positions_dict[token].values()]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return [self.positions_by_key[key][token] for key in self.positions_by_key]\n        else:\n            return self.positions_by_key[key][token]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            return self.positions_by_key[key][token]\n        else:\n            return self.positions[token]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            if key not in self.docs:\n                raise KeyError(f\"Key {key} not found in the documents.\")\n            return self.docs[key].positions(token)\n        else:\n            positions = []\n            for doc in self.docs.values():\n                positions.extend(doc.positions(token))\n            return positions\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            return self.doc_positions[key][token]\n        else:\n            return [\n                pos\n                for doc_positions in self.doc_positions.values()\n                for pos in doc_positions[token]\n            ]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        # Check if the token is present in the index\n        if token not in self.index:\n            return []\n\n        # If a specific document key is provided, retrieve the positions for that document\n        if key is not None:\n            if key in self.index[token]:\n                return [self.index[token][key]]\n            else:\n                return []\n\n        # If no specific document key is provided, retrieve positions across all documents\n        positions = []\n        for key, positions_list in self.index[token].items():\n            positions.extend(positions_list)\n        return positions\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return [self.positions(token, key=k) for k in self.keys]\n        else:\n            return self.get(key).positions(token)\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            return [np.array(self.search(token, key=key))]\n        else:\n            return [np.array(self.search(token, key=k)) for k in self.keys()]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            positions = []\n            for key in self.data:\n                positions.extend(self.data[key][token])\n            return positions\n        else:\n            return self.data[key][token]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            return self.positions_dict[key][token]\n        else:\n            return [positions for doc_positions in self.positions_dict.values() for positions in doc_positions[token]]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            positions = [self.positions(token, key) for key in self.keys]\n            return positions\n\n        # If the key is provided, retrieve the positions for that specific document\n        positions = self.data[key]['positions'][token]\n        return positions\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return [self.positions(token, key=k) for k in self.keys()]\n        else:\n            return self.get(key).positions(token)\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return [self.positions(token, key=key) for key in self.keys()]\n\n        if key not in self.keys():\n            raise ValueError(f\"Document key '{key}' not found in the SearchArray.\")\n\n        return self.array[key].positions(token)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith('%'):\n        percentage = float(spec[:-1])\n        return int(num_clauses * percentage / 100)\n    elif spec.startswith('<'):\n        threshold = int(spec[1:])\n        return num_clauses - threshold + 1\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith('%'):\n        percentage = float(spec[:-1])\n        return int(num_clauses * percentage / 100)\n    elif spec.startswith('<'):\n        threshold = int(spec[1:])\n        return min(num_clauses, threshold)\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith('%'):\n        percentage = float(spec[:-1]) / 100\n        return int(num_clauses * percentage)\n    elif spec.startswith('<'):\n        threshold = int(spec[1:])\n        return num_clauses - threshold + 1\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith('%'):\n        percentage = float(spec[:-1])\n        return int(num_clauses * percentage / 100)\n    elif spec.startswith('<'):\n        threshold = int(spec[1:])\n        return num_clauses - threshold + 1\n    else:\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Check if the spec is an absolute number\n    if spec.isdigit():\n        return int(spec)\n\n    # Check if the spec is a percentage\n    if spec.endswith('%'):\n        percentage = float(spec[:-1])\n        return int(num_clauses * percentage / 100)\n\n    # Check if the spec is a conditional expression\n    if '<' in spec:\n        clauses, min_clauses = spec.split('<')\n        clauses = int(clauses)\n        min_clauses = int(min_clauses)\n        return min(clauses, min_clauses)\n\n    # If the spec is not recognized, raise an error\n    raise ValueError(f\"Unrecognized 'mm' spec: {spec}\")"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith('%'):\n        percentage = int(spec[:-1])\n        return int(num_clauses * percentage / 100)\n    elif spec.startswith('<'):\n        threshold = int(spec[1:])\n        return min(num_clauses, threshold)\n    else:\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        percentage = float(spec[:-1]) / 100.0\n        return int(num_clauses * percentage)\n    elif spec.startswith(\"<\"):\n        threshold = int(spec[1:])\n        return max(num_clauses - threshold + 1, 0)\n    else:\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Check if spec is a percentage\n    if spec.endswith('%'):\n        percentage = float(spec[:-1]) / 100.0\n        return int(num_clauses * percentage)\n\n    # Check if spec is a conditional expression\n    if '<' in spec:\n        clauses_to_match, clauses_to_not_match = spec.split('<')\n        clauses_to_match = int(clauses_to_match)\n        clauses_to_not_match = int(clauses_to_not_match)\n        return num_clauses - clauses_to_not_match\n\n    # Otherwise, spec is an absolute number\n    return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        # Parse percentage-based specification\n        percentage = int(spec[:-1])\n        return int(num_clauses * percentage / 100)\n    elif spec.startswith(\"<\"):\n        # Parse conditional specification\n        threshold = int(spec[1:])\n        return num_clauses - threshold + 1\n    else:\n        # Parse absolute number specification\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Check if spec is a percentage\n    if spec.endswith('%'):\n        percentage = float(spec[:-1])\n        return int(num_clauses * percentage / 100)\n\n    # Check if spec is a conditional expression\n    if '<' in spec:\n        threshold, clauses = spec.split('<')\n        if num_clauses < int(threshold):\n            return int(clauses)\n        else:\n            return 0\n\n    # Otherwise, spec is an absolute number\n    return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith('%'):\n        percentage = float(spec[:-1])\n        return int(num_clauses * percentage / 100)\n    elif spec.startswith('<'):\n        clauses_to_match = int(spec[1:])\n        if clauses_to_match >= num_clauses:\n            return num_clauses\n        else:\n            return clauses_to_match\n    else:\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Check if the spec is a percentage\n    if spec.endswith('%'):\n        # Calculate the percentage of clauses to match\n        percentage = float(spec[:-1]) / 100\n        # Return the minimum number of clauses that must match\n        return int(num_clauses * percentage)\n\n    # Check if the spec is a conditional expression\n    elif '<' in spec:\n        # Split the expression into the clause count and the condition\n        clause_count, condition = spec.split('<')\n        # Convert the clause count to an integer\n        clause_count = int(clause_count)\n        # Check if the clause count is less than the total number of clauses\n        if clause_count < num_clauses:\n            # Return the clause count\n            return clause_count\n        else:\n            # Return the total number of clauses\n            return num_clauses\n\n    # If the spec is neither a percentage nor a conditional expression, it is assumed to be an absolute number\n    else:\n        # Convert the spec to an integer and return it\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith('%'):\n        percentage = float(spec[:-1]) / 100.0\n        return int(num_clauses * percentage)\n    elif spec.startswith('<'):\n        threshold = int(spec[1:])\n        return num_clauses - threshold + 1\n    else:\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        percentage = int(spec[:-1])\n        min_should_match = int((percentage * num_clauses) / 100)\n    elif spec.startswith(\"<\"):\n        min_should_match = int(spec[1:])\n    else:\n        min_should_match = int(spec)\n\n    return min_should_match\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Check if spec is a percentage\n    if spec.endswith('%'):\n        # Convert percentage to a float and divide by 100\n        percentage = float(spec[:-1]) / 100\n        # Calculate the minimum number of clauses that must match based on the percentage\n        return int(num_clauses * percentage)\n    # Check if spec is a conditional expression\n    elif '<' in spec:\n        # Split the expression into clauses and minimum match values\n        clauses, min_matches = spec.split('<')\n        # Convert clauses and minimum match values to integers\n        clauses, min_matches = int(clauses), int(min_matches)\n        # Calculate the minimum number of clauses that must match based on the conditional expression\n        return clauses if clauses < min_matches else min_matches\n    # If spec is neither a percentage nor a conditional expression, assume it's an absolute number\n    else:\n        # Convert spec to an integer and return it\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith('%'):\n        percentage = float(spec[:-1]) / 100.0\n        return int(num_clauses * percentage)\n    elif spec.startswith('<'):\n        threshold = int(spec[1:])\n        return num_clauses - threshold + 1\n    else:\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Check if the spec is a percentage\n    if spec.endswith('%'):\n        # Convert the percentage to a float and divide by 100 to get the fraction\n        fraction = float(spec[:-1]) / 100\n        # Calculate the minimum number of clauses that must match based on the fraction\n        return int(num_clauses * fraction)\n    # Check if the spec is a conditional expression\n    elif '<' in spec:\n        # Split the spec into the two parts and convert them to integers\n        left, right = spec.split('<')\n        left, right = int(left), int(right)\n        # Calculate the minimum number of clauses that must match based on the conditional expression\n        return left if num_clauses < right else right\n    # Otherwise, assume the spec is an absolute number\n    else:\n        # Convert the spec to an integer and return it\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Check if the spec is a percentage\n    if spec.endswith('%'):\n        # Calculate the percentage of clauses to match\n        percent = float(spec[:-1]) / 100.0\n        # Return the minimum number of clauses that must match\n        return int(num_clauses * percent)\n\n    # Check if the spec is a conditional expression\n    if spec.startswith('<'):\n        # Parse the threshold and clause count\n        threshold, clause_count = spec[1:].split('/', 1)\n        # Calculate the minimum number of clauses that must match\n        return int(clause_count) if num_clauses < int(threshold) else 0\n\n    # Otherwise, the spec is an absolute number\n    return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Check if the spec is a percentage\n    if spec.endswith('%'):\n        # Calculate the percentage of clauses to match\n        percentage = float(spec[:-1]) / 100\n        # Return the minimum number of clauses that must match based on the percentage\n        return int(num_clauses * percentage)\n\n    # Check if the spec is a conditional expression\n    elif '<' in spec:\n        # Split the expression into clauses\n        clauses = spec.split('<')\n        # Calculate the minimum number of clauses that must match based on the conditional expression\n        return int(num_clauses - len(clauses)) + 1\n\n    # Otherwise, the spec is an absolute number\n    else:\n        # Return the absolute number of clauses that must match\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Check if spec is a percentage\n    if spec.endswith('%'):\n        percentage = float(spec[:-1])\n        return int(num_clauses * percentage / 100)\n\n    # Check if spec is a conditional expression\n    if '<' in spec:\n        threshold, min_matches = spec.split('<')\n        threshold = int(threshold)\n        min_matches = int(min_matches)\n        return min_matches if num_clauses < threshold else min_matches + 1\n\n    # Otherwise, spec is an absolute number\n    return int(spec)\n\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_general(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_general(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            # If slop is 1 and all tokens are unique, calculate the phrase frequencies directly\n            return self._phrase_freq_direct(tokens)\n        else:\n            # If slop is not 1 or tokens are not unique, delegate the calculation to another method\n            return self._phrase_freq_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_general(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_adj(tokens)\n        else:\n            return self.phrase_freq_non_adj(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_adjacent(tokens)\n        else:\n            return self._phrase_freq_general(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            # Calculate phrase frequencies using positions of terms\n            return self.phrase_freq_pos(tokens)\n        else:\n            # Delegate phrase frequency calculation to another method\n            return self.phrase_freq_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_direct(tokens)\n        else:\n            return self._phrase_freq_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            # If slop is 1 and all tokens are unique, use the positions of terms to directly calculate phrase frequencies\n            positions = [self.positions[token] for token in tokens]\n            return self._phrase_freq_from_positions(positions)\n        else:\n            # If slop is not 1 or tokens are not unique, delegate the calculation to another method\n            return self._phrase_freq_from_tokens(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            # If slop is 1 and all tokens are unique, calculate phrase frequencies directly\n            return self._phrase_freq_unique(tokens)\n        else:\n            # If slop is not 1 or tokens are not unique, delegate to another method\n            return self._phrase_freq_general(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_fast(tokens)\n        else:\n            return self._phrase_freq_slow(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If the slop is 1 and all tokens are unique, calculate phrase frequencies directly\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_direct(tokens)\n        else:\n            # If the slop is not 1 or tokens are not unique, delegate to another method\n            return self._phrase_freq_non_direct(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            # If slop is 1 and tokens are unique, directly calculate phrase frequencies\n            # using the positions of terms\n            return self.phrase_freq_from_pos(tokens)\n        else:\n            # If slop is not 1 or tokens are not unique, delegate to another method\n            return self.phrase_freq_from_tokens(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(tokens) == len(set(tokens)):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_general(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            # If the slop is 1 and all tokens are unique, directly calculate the phrase frequencies using the positions of terms\n            positions = [self.positions[token] for token in tokens]\n            phrase_freq = np.prod(np.stack(positions), axis=0)\n        else:\n            # If the slop is not 1 or tokens are not unique, delegate the calculation to another method\n            phrase_freq = self._phrase_freq_slop(tokens, slop)\n\n        return phrase_freq\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # Check if tokens are unique and slop is 1\n        if len(set(tokens)) == len(tokens) and slop == 1:\n            # Get the positions of the tokens\n            positions = [self.pos[t] for t in tokens]\n            # Calculate the phrase frequencies directly\n            return np.prod(np.diff(positions, axis=1), axis=1)\n        else:\n            # Delegate to another method to handle different slops or non-unique tokens\n            return self._phrase_freq(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check if the array is empty\n        if not array:\n            return cls(np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),\n                       np.array([], dtype=np.int32),"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check if array is empty\n        if len(array) == 0:\n            raise ValueError('Array is empty')\n\n        # Check if array is a list\n        if not isinstance(array, list):\n            raise TypeError('Array is not a list')\n\n        # Check if array is a list of strings\n        if not all(isinstance(x, str) for x in array):\n            raise TypeError('Array is not a list of strings')\n\n        # Check if tokenizer is a function\n        if not callable(tokenizer):\n            raise TypeError('Tokenizer is not a function')\n\n        # Check if truncate is a boolean\n        if not isinstance(truncate, bool):\n            raise TypeError('Truncate is not a boolean')\n\n        # Check if batch_size is an integer\n        if not isinstance(batch_size, int):\n            raise TypeError('Batch size is not an integer')\n\n        # Check if batch_size is positive\n        if batch_size <= 0:\n            raise ValueError('Batch size must be positive')\n\n        # Check if avoid_copies is a boolean\n        if not isinstance(avoid_copies, bool):\n            raise TypeError('Avoid copies is not a boolean')\n\n        # Check if array is a list of strings\n        if not all(isinstance(x, str) for x in array):\n            raise TypeError('Array is not a list of strings')\n\n        # Check if array is empty\n        if len(array) == 0:\n            raise ValueError('Array is empty')\n\n        # Check if tokenizer is a function\n        if not callable(tokenizer):\n            raise TypeError('Tokenizer is not a function')\n\n        # Check if truncate is a boolean\n        if not isinstance(truncate, bool):\n            raise TypeError('Truncate is not a boolean')\n\n        # Check if batch_size is an integer\n        if not isinstance(batch_size, int):\n            raise TypeError('Batch size is not an integer')\n\n        # Check if batch_size is positive\n        if batch_size <= 0:\n            raise ValueError('Batch"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check if array is empty\n        if len(array) == 0:\n            raise ValueError(\"Array is empty\")\n\n        # Check if array is not iterable\n        if not isinstance(array, Iterable):\n            raise TypeError(\"Array is not iterable\")\n\n        # Check if array contains non-string elements\n        if not all(isinstance(item, str) for item in array):\n            raise TypeError(\"Array contains non-string elements\")\n\n        # Check if tokenizer is callable\n        if not callable(tokenizer):\n            raise TypeError(\"Tokenizer is not callable\")\n\n        # Check if truncate is boolean\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Truncate is not a boolean\")\n\n        # Check if batch_size is integer\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Batch size is not an integer\")\n\n        # Check if batch_size is positive\n        if batch_size <= 0:\n            raise ValueError(\"Batch size is not positive\")\n\n        # Check if avoid_copies is boolean\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Avoid copies is not a boolean\")\n\n        # Check if array is not empty\n        if len(array) == 0:\n            raise ValueError(\"Array is empty\")\n\n        # Check if array is not iterable\n        if not isinstance(array, Iterable):\n            raise TypeError(\"Array is not iterable\")\n\n        # Check if array contains non-string elements\n        if not all(isinstance(item, str) for item in array):\n            raise TypeError(\"Array contains non-string elements\")\n\n        # Check if tokenizer is callable\n        if not callable(tokenizer):\n            raise TypeError(\"Tokenizer is not callable\")\n\n        # Check if truncate is boolean\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Truncate is not a boolean\")\n\n        # Check if batch_size is integer\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Batch size is"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check if the input array is empty\n        if len(array) == 0:\n            raise ValueError(\"Array is empty\")\n\n        # Check if the batch size is valid\n        if batch_size < 1:\n            raise ValueError(\"Batch size must be greater than 0\")\n\n        # Check if the input array is a list or numpy array\n        if not isinstance(array, list) and not isinstance(array, np.ndarray):\n            raise TypeError(\"Array must be a list or numpy array\")\n\n        # Check if the tokenizer is a function\n        if not callable(tokenizer):\n            raise TypeError(\"Tokenizer must be a function\")\n\n        # Check if the truncate parameter is a boolean\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Truncate must be a boolean\")\n\n        # Check if the avoid_copies parameter is a boolean\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Avoid_copies must be a boolean\")\n\n        # Check if the batch_size parameter is an integer\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Batch_size must be an integer\")\n\n        # Check if the input array is a list of strings\n        if not all(isinstance(x, str) for x in array):\n            raise TypeError(\"Array must contain only strings\")\n\n        # Check if the input array is a list of strings\n        if not all(isinstance(x, str) for x in array):\n            raise TypeError(\"Array must contain only strings\")\n\n        # Check if the input array is a list of strings\n        if not all(isinstance(x, str) for x in array):\n            raise TypeError(\"Array must contain only strings\")\n\n        # Check if the input array is a list of strings\n        if not all(isinstance(x, str) for x in array):\n            raise TypeError(\"Array must contain only strings\")\n\n        # Check if the input array is a list of strings\n        if not all(isinstance(x, str) for x in array):\n            raise TypeError(\"Array must contain"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check if the array is empty\n        if len(array) == 0:\n            raise ValueError(\"Array must not be empty\")\n\n        # Check if the array is already a SearchArray\n        if isinstance(array, SearchArray):\n            return array\n\n        # Check if the array is a list of strings\n        if not isinstance(array[0], str):\n            raise TypeError(\"Array must be a list of strings\")\n\n        # Check if the tokenizer is callable\n        if not callable(tokenizer):\n            raise TypeError(\"Tokenizer must be callable\")\n\n        # Check if the batch size is a positive integer\n        if not isinstance(batch_size, int) or batch_size <= 0:\n            raise ValueError(\"Batch size must be a positive integer\")\n\n        # Check if the truncate flag is a boolean\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Truncate flag must be a boolean\")\n\n        # Check if the avoid_copies flag is a boolean\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Avoid_copies flag must be a boolean\")\n\n        # Check if the array is a list\n        if not isinstance(array, list):\n            raise TypeError(\"Array must be a list\")\n\n        # Check if the array is not empty\n        if not array:\n            raise ValueError(\"Array must not be empty\")\n\n        # Check if the tokenizer is a callable\n        if not callable(tokenizer):\n            raise TypeError(\"Tokenizer must be a callable\")\n\n        # Check if the batch_size is a positive integer\n        if not isinstance(batch_size, int) or batch_size <= 0:\n            raise ValueError(\"Batch size must be a positive integer\")\n\n        # Check if the truncate flag is a boolean\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Truncate flag must be a boolean\")\n\n        # Check if the avoid_copies flag is a boolean\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Avoid_copies"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # If the array is a list or numpy array, convert it to a list\n        if isinstance(array, (list, np.ndarray)):\n            array = list(array)\n\n        # If the array is a pandas series, convert it to a list\n        elif isinstance(array, pd.Series):\n            array = list(array.values)\n\n        # If the array is a generator, convert it to a list\n        elif isinstance(array, types.GeneratorType):\n            array = list(array)\n\n        # If the array is a dictionary, convert it to a list of values\n        elif isinstance(array, dict):\n            array = list(array.values())\n\n        # If the array is a tuple, convert it to a list\n        elif isinstance(array, tuple):\n            array = list(array)\n\n        # If the array is a set, convert it to a list\n        elif isinstance(array, set):\n            array = list(array)\n\n        # If the array is a range, convert it to a list\n        elif isinstance(array, range):\n            array = list(array)\n\n        # If the array is a map, convert it to a list\n        elif isinstance(array, map):\n            array = list(array)\n\n        # If the array is a filter, convert it to a list\n        elif isinstance(array, filter):\n            array = list(array)\n\n        # If the array is a zip, convert it to a list\n        elif isinstance(array, zip):\n            array = list(array)\n\n        # If the array is a frozenset, convert it to a list\n        elif isinstance(array, frozenset):\n            array = list(array)\n\n        # If the array is a bytearray, convert it to a list\n        elif isinstance(array, bytearray):\n            array = list(array)\n\n        # If the array is a memoryview, convert it to a list\n        elif isinstance(array, memoryview):\n            array = list(array)\n\n        # If the array is a bytes, convert it to a list\n        elif isinstance(array, bytes"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check if the array is empty\n        if len(array) == 0:\n            raise ValueError(\"Array is empty\")\n\n        # Check if the array is a list\n        if not isinstance(array, list):\n            raise TypeError(\"Array must be a list\")\n\n        # Check if the array contains only strings\n        if not all(isinstance(x, str) for x in array):\n            raise TypeError(\"Array must contain only strings\")\n\n        # Check if the tokenizer is a function\n        if not callable(tokenizer):\n            raise TypeError(\"Tokenizer must be a function\")\n\n        # Check if the truncate parameter is a boolean\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Truncate parameter must be a boolean\")\n\n        # Check if the batch size is an integer\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Batch size must be an integer\")\n\n        # Check if the batch size is positive\n        if batch_size <= 0:\n            raise ValueError(\"Batch size must be positive\")\n\n        # Check if the avoid_copies parameter is a boolean\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Avoid copies parameter must be a boolean\")\n\n        # Initialize the term matrix, positions, term dictionary, and document lengths\n        term_matrix = []\n        positions = []\n        term_dict = {}\n        doc_lengths = []\n\n        # Iterate over the array in batches\n        for i in range(0, len(array), batch_size):\n            # Get the current batch\n            batch = array[i:i + batch_size]\n\n            # Check if the batch is empty\n            if len(batch) == 0:\n                continue\n\n            # Tokenize the batch using the specified tokenizer\n            tokens = tokenizer(batch)\n\n            # Check if the tokens are empty\n            if len(tokens) == 0:\n                continue\n\n            # Check if the tokens are a list\n            if not isinstance(tokens, list):\n                raise TypeError(\"Tokens must be a list\")\n\n           "}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = array[:truncate]\n\n        # build the index\n        term_matrix, positions, term_dictionary, average_document_length, document_lengths = cls.build_index(\n            array, tokenizer, batch_size, avoid_copies)\n\n        # create the SearchArray instance\n        return cls(term_matrix, positions, term_dictionary, average_document_length, document_lengths)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = array[:10000]\n        term_matrix, positions, term_dict, avg_doc_len, doc_lens = cls.build_index(array, tokenizer, batch_size, avoid_copies)\n        return cls(term_matrix, positions, term_dict, avg_doc_len, doc_lens)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check if the array is empty\n        if not array:\n            raise ValueError(\"Array cannot be empty.\")\n\n        # Check if the array is a list\n        if not isinstance(array, list):\n            raise ValueError(\"Array must be a list.\")\n\n        # Check if the array contains only strings\n        if not all(isinstance(item, str) for item in array):\n            raise ValueError(\"Array must contain only strings.\")\n\n        # Check if the tokenizer is callable\n        if not callable(tokenizer):\n            raise ValueError(\"Tokenizer must be callable.\")\n\n        # Check if the batch size is a positive integer\n        if not isinstance(batch_size, int) or batch_size <= 0:\n            raise ValueError(\"Batch size must be a positive integer.\")\n\n        # Check if the avoid_copies flag is a boolean\n        if not isinstance(avoid_copies, bool):\n            raise ValueError(\"Avoid copies flag must be a boolean.\")\n\n        # Initialize the term matrix, positions, term dictionary, and document lengths\n        term_matrix = []\n        positions = []\n        term_dict = {}\n        doc_lengths = []\n\n        # Process the array in batches\n        for batch in batch_generator(array, batch_size):\n            # Tokenize the batch and update the term matrix, positions, term dictionary, and document lengths\n            term_matrix_batch, positions_batch, term_dict_batch, doc_lengths_batch = cls._process_batch(batch, tokenizer)\n            term_matrix.extend(term_matrix_batch)\n            positions.extend(positions_batch)\n            term_dict.update(term_dict_batch)\n            doc_lengths.extend(doc_lengths_batch)\n\n        # Compute the average document length\n        avg_doc_length = sum(doc_lengths) / len(doc_lengths)\n\n        # Truncate the data if necessary\n        if truncate:\n            term_matrix, positions, term_dict, doc_lengths = cls._truncate_data(term_matrix, positions, term_dict, doc"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Create a new instance of SearchArray\n        sa = cls()\n\n        # Check if the array is empty\n        if len(array) == 0:\n            return sa\n\n        # Check if the array is a list\n        if isinstance(array, list):\n            # Convert the list to a numpy array\n            array = np.array(array)\n\n        # Check if the array is a numpy array\n        if isinstance(array, np.ndarray):\n            # Check if the array is 1D\n            if len(array.shape) == 1:\n                # If the array is 1D, reshape it to 2D with a single column\n                array = array.reshape(-1, 1)\n            # Check if the array is 2D\n            elif len(array.shape) == 2:\n                # If the array is 2D, check if it has multiple columns\n                if array.shape[1] > 1:\n                    # If the array has multiple columns, concatenate them into a single column\n                    array = np.concatenate(array, axis=1)\n\n        # Check if the array is a pandas DataFrame\n        if isinstance(array, pd.DataFrame):\n            # Convert the DataFrame to a numpy array\n            array = array.values\n\n        # Check if the array is a Series\n        if isinstance(array, pd.Series):\n            # Convert the Series to a numpy array\n            array = array.values\n\n        # Check if the array is a list of lists\n        if isinstance(array[0], list):\n            # Convert the list of lists to a numpy array\n            array = np.array(array)\n\n        # Check if the array is a list of tuples\n        if isinstance(array[0], tuple):\n            # Convert the list of tuples to a numpy array\n            array = np.array(array)\n\n        # Check if the array is a list of dictionaries\n        if isinstance(array[0], dict):\n            # Convert the list of dictionaries to a numpy array\n            array = np.array(list(array))\n\n        #"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Initialize the index and document length arrays\n        index = {}\n        doc_lengths = []\n\n        # Process the array in batches\n        for batch in batch_array(array, batch_size):\n            # Tokenize the batch of strings\n            tokens = tokenizer(batch)\n\n            # Calculate the document length for each document in the batch\n            doc_lengths.extend([len(doc) for doc in tokens])\n\n            # Update the index with the tokens from the current batch\n            for i, doc in enumerate(tokens):\n                for term in doc:\n                    if term not in index:\n                        index[term] = []\n                    index[term].append(i)\n\n        # Calculate the average document length\n        avg_doc_length = sum(doc_lengths) / len(doc_lengths)\n\n        # Create a SearchArray instance with the indexed data\n        return cls(index, doc_lengths, avg_doc_length, truncate=truncate, avoid_copies=avy_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = array[:truncate]\n\n        if batch_size:\n            array = batch(array, batch_size)\n\n        if avoid_copies:\n            array = copyfree(array)\n\n        return cls(array, tokenizer)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not isinstance(array, Iterable):\n            raise TypeError('array must be an Iterable')\n\n        if not callable(tokenizer):\n            raise TypeError('tokenizer must be a function')\n\n        if not isinstance(truncate, bool):\n            raise TypeError('truncate must be a bool')\n\n        if not isinstance(batch_size, int):\n            raise TypeError('batch_size must be an int')\n\n        if not isinstance(avoid_copies, bool):\n            raise TypeError('avoid_copies must be a bool')\n\n        if truncate:\n            array = truncate_array(array)\n\n        array = array_to_array(array)\n\n        if len(array) > batch_size:\n            term_matrix = []\n            positions = []\n            for batch in tqdm(batch_array(array, batch_size), total=int(len(array) / batch_size)):\n                term_matrix_batch, positions_batch = cls.index_batch(batch, tokenizer, avoid_copies)\n                term_matrix.append(term_matrix_batch)\n                positions.append(positions_batch)\n            term_matrix = np.concatenate(term_matrix)\n            positions = np.concatenate(positions)\n        else:\n            term_matrix, positions = cls.index_batch(array, tokenizer, avoid_copies)\n\n        term_dict = cls.build_term_dict(term_matrix)\n\n        average_document_length = np.mean(term_matrix.sum(axis=1))\n\n        document_lengths = term_matrix.sum(axis=1)\n\n        return cls(term_matrix, positions, term_dict, average_document_length, document_lengths)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not isinstance(array, Iterable):\n            raise TypeError(\"The array must be an iterable.\")\n\n        if truncate:\n            array = array[:truncate]\n\n        if len(array) == 0:\n            raise ValueError(\"The array must not be empty.\")\n\n        if not callable(tokenizer):\n            raise TypeError(\"The tokenizer must be callable.\")\n\n        if not isinstance(batch_size, int):\n            raise TypeError(\"The batch size must be an integer.\")\n\n        if batch_size <= 0:\n            raise ValueError(\"The batch size must be positive.\")\n\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"The avoid_copies parameter must be a boolean.\")\n\n        # Initialize the SearchArray instance\n        search_array = cls(tokenizer=tokenizer)\n\n        # Process the array in batches\n        for i in range(0, len(array), batch_size):\n            batch = array[i:i + batch_size]\n\n            # Tokenize the batch\n            batch_tokens = [tokenizer(doc) for doc in batch]\n\n            # Add the batch to the search array\n            search_array.add_batch(batch_tokens, avoid_copies=avoid_copies)\n\n        return search_array\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check if the input array is already a SearchArray\n        if isinstance(array, cls):\n            return array\n\n        # Check if the input array is a list\n        if isinstance(array, list):\n            # Convert the list to a numpy array for better performance\n            array = np.array(array)\n\n        # Check if the input array is a numpy array\n        if isinstance(array, np.ndarray):\n            # Check if the array is already indexed\n            if isinstance(array[0], cls):\n                return array\n\n            # Check if the array is empty\n            if len(array) == 0:\n                return cls(np.zeros((0, 0), dtype=np.uint32), np.zeros(0, dtype=np.uint32), {}, 0, np.zeros(0, dtype=np.uint32))\n\n            # Check if the array has only one element\n            if len(array) == 1:\n                # Tokenize the single element\n                tokens = tokenizer(array[0])\n\n                # Create a term dictionary with the token as the key and its position as the value\n                term_dict = {token: 0 for token in tokens}\n\n                # Create a term matrix with a single row and a column for each token\n                term_matrix = np.zeros((1, len(term_dict)), dtype=np.uint32)\n\n                # Set the count of each token in the term matrix to 1\n                for token in tokens:\n                    term_matrix[0, term_dict[token]] = 1\n\n                # Create a positions array with a single element representing the position of the token in the term matrix\n                positions = np.zeros(len(tokens), dtype=np.uint32)\n                for i, token in enumerate(tokens):\n                    positions[i] = term_dict[token]\n\n                # Calculate the average document length as the number of tokens in the document\n                avgdl = len(tokens)\n\n                # Create a document lengths array with a single element representing the length of the document\n                doc_lengths"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check if the array is empty\n        if len(array) == 0:\n            raise ValueError(\"Array is empty\")\n\n        # Check if the batch size is valid\n        if batch_size <= 0:\n            raise ValueError(\"Batch size must be positive\")\n\n        # Check if the array is a list or numpy array\n        if isinstance(array, list):\n            array = np.array(array, dtype=object)\n        elif not isinstance(array, np.ndarray):\n            raise TypeError(\"Array must be a list or numpy array\")\n\n        # Check if the array contains strings\n        if not np.issubdtype(array.dtype, np.str_):\n            raise TypeError(\"Array must contain strings\")\n\n        # Check if the array contains only strings\n        if not np.all(np.char.isdecimal(array)):\n            raise TypeError(\"Array must contain only strings\")\n\n        # Check if the array is not too large\n        if array.size > 1e9:\n            raise ValueError(\"Array is too large\")\n\n        # Check if the tokenizer is callable\n        if not callable(tokenizer):\n            raise TypeError(\"Tokenizer must be callable\")\n\n        # Check if the truncate argument is boolean\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Truncate must be a boolean\")\n\n        # Check if the batch size is an integer\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Batch size must be an integer\")\n\n        # Check if the avoid_copies argument is boolean\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Avoid_copies must be a boolean\")\n\n        # Check if the array is too large to be indexed\n        if array.size > 1e9:\n            raise ValueError(\"Array is too large to be indexed\")\n\n        # Check if the batch size is too large\n        if batch_size > 1e9:\n            raise ValueError(\"Batch size is too large\")\n\n        # Check if the batch size is too small\n        if batch_size"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Initialize the SearchArray instance\n        search_array = cls()\n\n        # Check if array is empty\n        if not array:\n            return search_array\n\n        # Determine the number of documents and terms in the array\n        num_docs = len(array)\n        num_terms = sum(map(len, map(tokenizer, array)))\n\n        # Initialize the term matrix and positions\n        search_array.term_matrix = np.zeros((num_terms, num_docs), dtype=np.uint32)\n        search_array.positions = np.zeros(num_terms, dtype=np.uint32)\n\n        # Initialize the term dictionary\n        search_array.term_dict = {}\n\n        # Initialize the document lengths and average document length\n        search_array.doc_lengths = np.zeros(num_docs, dtype=np.uint32)\n        search_array.avg_doc_length = 0\n\n        # Initialize the current term and document indices\n        current_term = 0\n        current_doc = 0\n\n        # Initialize the document lengths and average document length\n        doc_lengths = np.zeros(num_docs, dtype=np.uint32)\n        avg_doc_length = 0\n\n        # Iterate over the array in batches\n        for batch in batches(array, batch_size):\n            # Initialize the term matrix and positions for the batch\n            term_matrix = np.zeros((len(batch), num_docs), dtype=np.uint32)\n            positions = np.zeros(len(batch), dtype=np.uint32)\n\n            # Iterate over the documents in the batch\n            for i, doc in enumerate(batch):\n                # Tokenize the document\n                tokens = tokenizer(doc)\n\n                # Update the term dictionary\n                for term in tokens:\n                    if term not in search_array.term_dict:\n                        search_array.term_dict[term] = current_term\n                        current_term += 1\n\n                # Update the term matrix and positions for the document\n               "}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = array[:truncate]\n\n        if not isinstance(array, list):\n            array = list(array)\n\n        if len(array) == 0:\n            raise ValueError(\"Array must not be empty.\")\n\n        if len(array) == 1:\n            raise ValueError(\"Array must contain more than one item.\")\n\n        # Build the index\n        index = {}\n        positions = {}\n        doc_lengths = []\n        for i, doc in enumerate(array):\n            tokens = tokenizer(doc)\n            doc_lengths.append(len(tokens))\n            for pos, term in enumerate(tokens):\n                if term not in index:\n                    index[term] = []\n                    positions[term] = []\n                index[term].append(i)\n                positions[term].append(pos)\n\n        # Convert the index to a term matrix\n        terms = sorted(index.keys())\n        term_dict = {term: i for i, term in enumerate(terms)}\n        term_matrix = np.zeros((len(array), len(terms)), dtype=int)\n        for term, doc_ids in index.items():\n            j = term_dict[term]\n            for doc_id in doc_ids:\n                term_matrix[doc_id, j] += 1\n\n        # Calculate the average document length\n        avgdl = np.mean(doc_lengths)\n\n        return cls(term_matrix, positions, term_dict, avgdl, doc_lengths)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = array[:len(array) // 2]\n\n        array = np.array(array)\n        if array.ndim == 0:\n            array = array.reshape(-1)\n\n        if array.ndim != 1:\n            raise ValueError('array must be 1-dimensional')\n\n        if array.dtype != np.object:\n            array = array.astype(np.object)\n\n        array = array.copy()\n\n        # tokenize\n        array = np.array([tokenizer(doc) for doc in array])\n\n        # build term matrix\n        term_matrix = []\n        for doc in array:\n            term_matrix.append(Counter(doc))\n        term_matrix = np.array(term_matrix)\n\n        # build positions\n        positions = []\n        for doc in array:\n            positions.append(np.array([[i, pos] for pos, i in enumerate(doc)]))\n        positions = np.array(positions)\n\n        # build term dictionary\n        term_dict = {}\n        for i, doc in enumerate(array):\n            for j, term in enumerate(doc):\n                if term not in term_dict:\n                    term_dict[term] = []\n                term_dict[term].append((i, j))\n\n        # build average document length\n        avg_doc_len = np.mean([len(doc) for doc in array])\n\n        # build document lengths\n        doc_lens = np.array([len(doc) for doc in array])\n\n        return cls(term_matrix, positions, term_dict, avg_doc_len, doc_lens)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(f\"Initializing {js['class_name']}\")\n        self.server = self.config.get_server()\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.server.start()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = None\n        self.config = None\n        self.logger = None\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            self.config['server_address'],\n            self.config['server_port'],\n            self.config['server_ssl'],\n            self.config['server_cert'],\n            self.config['server_key'],\n            self.config['server_ca'],\n            self.config['server_ciphers'],\n            self.config['server_dh'],\n            self.config['server_tls_version'],\n            self.config['server_cipher_suite'],\n            self.config['server_compression'],\n            self.config['server_compression_level'],\n            self.config['server_compression_min_size'],\n            self.config['server_compression_max_size'],\n            self.config['server_compression_mem_level'],\n            self.config['server_compression_strategy'],\n            self.config['server_compression_chunk_size'],\n            self.config['server_compression_window_bits'],\n            self.config['server_compression_level_strategy'],\n            self.config['server_compression_method'],\n            self.config['server_compression_dictionary'],\n            self.config['server_compression_dictionary_size'],\n            self.config['server_compression_dictionary_attack'],\n            self.config['server_compression_dictionary_attack_timeout'],\n            self.config['server_compression_dictionary_attack_probability'],\n            self.config['server_compression_dictionary_attack_interval'],\n            self.config['server_compression_dictionary_attack_threshold'],\n            self.config['server_compression_dictionary_attack_timeout_threshold'],\n            self.config['server_compression_dictionary_attack_probability_threshold'],\n            self.config['server_compression_dictionary_attack_interval_threshold'],\n            self.config['server_compression_dictionary_attack_threshold_threshold'],\n            self.config['server_compression_dictionary_attack_timeout_threshold_threshold"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = None\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.config = self.config\n        self.logger = self.logger\n        self.server = None\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = self.config.get_server()\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info('Initializing ProxifierMessageInterceptor')\n        self.server = ProxifierServer(self.config['server']['host'], self.config['server']['port'], self.config['server']['buffer_size'], self.config['server']['backlog'], self.config['server']['timeout'])\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.config = js['config']\n        self.logger = js['logger']\n\n        self.server = None\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = None\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        # Set up the server\n        self.server = self.config.get_server()\n\n        # Start the server\n        self.server.start()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = ThreadingTCPServer((self.config['host'], self.config['port']), self.config['handler'])\n        self.server.allow_reuse_address = True\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Set up the server with the specified configurations\n        self.server = HTTPServer((self.config['listen_host'], self.config['listen_port']), ProxifierMessageInterceptor.RequestHandler)\n        self.server.timeout = self.config['timeout']\n        self.server.config = self.config\n        self.server.logger = self.logger\n\n        # Initialize a dictionary to keep track of connections\n        self.connections = {}\n\n        # Initialize a lock for thread safety\n        self.lock = threading.Lock()\n\n        # Start the server\n        self.server.serve_forever()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.server.bind((self.config['server_ip'], self.config['server_port']))\n        self.server.listen(self.config['max_connections'])\n        self.logger.info(f\"Server listening on {self.config['server_ip']}:{self.config['server_port']}\")\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            self.config.get(\"host\", \"0.0.0.0\"),\n            self.config.get(\"port\", 8080),\n            self.config.get(\"backlog\", 5),\n            self.config.get(\"max_connections\", 10),\n            self.config.get(\"timeout\", 30),\n            self.config.get(\"buffer_size\", 1024),\n            self.config.get(\"encoding\", \"utf-8\"),\n            self.config.get(\"log_level\", \"info\"),\n        )\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.server.bind((self.config['host'], self.config['port']))\n        self.server.listen(10)\n        self.logger.info(f\"Server started on {self.config['host']}:{self.config['port']}\")\n\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config.host, self.config.port, self.config.max_connections, self.config.max_retries, self.config.retry_interval)\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.server.start()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.config = self.config['proxifier']\n        self.logger = self.logger['proxifier']\n\n        self.server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.server.bind((self.config['host'], self.config['port']))\n        self.server.listen(10)\n\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.start()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = None\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        # Set up the server with the specified configurations\n        self.server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.server.bind((self.config['host'], self.config['port']))\n        self.server.listen(self.config['max_connections'])\n\n        # Start the server in a separate thread\n        threading.Thread(target=self.start_server).start()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = HTTPServer(('', self.config.getint('server', 'port')), self.RequestHandler)\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.logger.info('Starting server on port ' + str(self.config.getint('server', 'port')))\n        self.server.serve_forever()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.config = self.config.get_config()\n        self.logger = self.logger.get_logger()\n        self.server = self.setup_server()\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = TCPServer((self.config['host'], self.config['port']), self.handle_message)\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & 0x5555555555555555\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n    arr = (arr + (arr >> 4)) & 0x0f0f0f0f0f0f0f0f\n    arr += arr >> 8\n    arr += arr >> 16\n    arr += arr >> 32\n    return arr & 0x7f\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & 0x5555555555555555\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n    arr = (arr + (arr >> 4)) & 0x0f0f0f0f0f0f0f0f\n    arr += arr >> 8\n    arr += arr >> 16\n    arr += arr >> 32\n    arr &= 0x7f\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & 0x5555555555555555\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n    arr = (arr + (arr >> 4)) & 0x0f0f0f0f0f0f0f0f\n    arr += arr >> 8\n    arr += arr >> 16\n    arr += arr >> 32\n    arr &= 0x7f\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & 0x5555555555555555\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n    arr = (arr + (arr >> 4)) & 0x0f0f0f0f0f0f0f0f\n    arr += arr >> 8\n    arr += arr >> 16\n    arr += arr >> 32\n    arr &= 0x7f\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & 0x5555555555555555\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n    arr = (arr + (arr >> 4)) & 0x0f0f0f0f0f0f0f0f\n    arr += arr >> 8\n    arr += arr >> 16\n    arr += arr >> 32\n    return arr & 0x7f\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & 0x5555555555555555\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n    arr = (arr + (arr >> 4)) & 0x0f0f0f0f0f0f0f0f\n    arr += arr >> 8\n    arr += arr >> 16\n    arr += arr >> 32\n    arr &= 0x7f\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & 0x5555555555555555\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n    arr = (arr + (arr >> 4)) & 0x0f0f0f0f0f0f0f0f\n    arr += arr >> 8\n    arr += arr >> 16\n    arr += arr >> 32\n    return arr & 0x7f\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & 0x5555555555555555\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n    arr = (arr + (arr >> 4)) & 0x0f0f0f0f0f0f0f0f\n    arr += arr >> 8\n    arr += arr >> 16\n    arr += arr >> 32\n    return arr & 0x7f\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & 0x5555555555555555\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n    arr = (arr + (arr >> 4)) & 0x0F0F0F0F0F0F0F0F\n    arr += arr >> 8\n    arr += arr >> 16\n    arr += arr >> 32\n    return arr & 0x7F\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & 0x5555555555555555\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n    arr = (arr + (arr >> 4)) & 0x0F0F0F0F0F0F0F0F\n    arr += arr >> 8\n    arr += arr >> 16\n    arr += arr >> 32\n    return arr & 0x0000007F\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & 0x5555555555555555\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n    arr = (arr + (arr >> 4)) & 0x0F0F0F0F0F0F0F0F\n    arr += arr >> 8\n    arr += arr >> 16\n    arr += arr >> 32\n    return arr & 0x7F\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & 0x5555555555555555\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n    arr = (arr + (arr >> 4)) & 0x0f0f0f0f0f0f0f0f\n    arr += arr >> 8\n    arr += arr >> 16\n    arr += arr >> 32\n    arr &= 0x7f\n    return arr\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Step 1: Count the number of bits set to 1 in each 2-bit group\n    arr = (arr & 0x5555555555555555) + ((arr >> 1) & 0x5555555555555555)\n\n    # Step 2: Count the number of bits set to 1 in each 4-bit group\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n\n    # Step 3: Count the number of bits set to 1 in each 8-bit group\n    arr = (arr & 0x0F0F0F0F0F0F0F0F) + ((arr >> 4) & 0x0F0F0F0F0F0F0F0F)\n\n    # Step 4: Count the number of bits set to 1 in each 16-bit group\n    arr = (arr & 0x00FF00FF00FF00FF) + ((arr >> 8) & 0x00FF00FF00FF00FF)\n\n    # Step 5: Count the number of bits set to 1 in each 32-bit group\n    arr = (arr & 0x0000FFFF0000FFFF) + ((arr >> 16) & 0x0000FFFF0000FFFF)\n\n    # Step 6: Count the number of bits set to 1 in each 64-bit group\n    arr = (arr & 0x00000000FFFFFFFF) + ((arr >> 32) & 0x00000000FFFFFFFF)\n\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Perform the first round of bit counting\n    arr = (arr & 0x5555555555555555) + ((arr >> 1) & 0x5555555555555555)\n\n    # Perform the second round of bit counting\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n\n    # Perform the third round of bit counting\n    arr = (arr & 0x0F0F0F0F0F0F0F0F) + ((arr >> 4) & 0x0F0F0F0F0F0F0F0F)\n\n    # Perform the fourth round of bit counting\n    arr = (arr & 0x00FF00FF00FF00FF) + ((arr >> 8) & 0x00FF00FF00FF00FF)\n\n    # Perform the fifth round of bit counting\n    arr = (arr & 0x0000FFFF0000FFFF) + ((arr >> 16) & 0x0000FFFF0000FFFF)\n\n    # Perform the sixth round of bit counting\n    arr = (arr & 0x00000000FFFFFFFF) + ((arr >> 32) & 0x00000000FFFFFFFF)\n\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize an array of 256 64-bit integers with the values 0 to 255\n    b = [0] * 256\n    for i in range(256):\n        b[i] = i\n\n    # Initialize an array of 256 64-bit integers with the values 0 to 255\n    b = [0] * 256\n    for i in range(256):\n        b[i] = i\n\n    # Initialize an array of 256 64-bit integers with the values 0 to 255\n    b = [0] * 256\n    for i in range(256):\n        b[i] = i\n\n    # Initialize an array of 256 64-bit integers with the values 0 to 255\n    b = [0] * 256\n    for i in range(256):\n        b[i] = i\n\n    # Initialize an array of 256 64-bit integers with the values 0 to 255\n    b = [0] * 256\n    for i in range(256):\n        b[i] = i\n\n    # Initialize an array of 256 64-bit integers with the values 0 to 255\n    b = [0] * 256\n    for i in range(256):\n        b[i] = i\n\n    # Initialize an array of 256 64-bit integers with the values 0 to 255\n    b = [0] * 256\n    for i in range(256):\n        b[i] = i\n\n    # Initialize an array of 256 64-bit integers with the values 0 to 255\n    b = [0] * 256\n    for i in range(256):\n        b[i] = i\n\n    # Initialize an array of 256 64-bit integers with"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize the bit count array with zeros\n    bit_count = [0] * len(arr)\n\n    # Iterate over each element in the input array\n    for i in range(len(arr)):\n\n        # Initialize a variable to store the bit count for the current element\n        count = 0\n\n        # Initialize a variable to store the current element\n        x = arr[i]\n\n        # Iterate over each bit in the current element\n        while x:\n\n            # Increment the bit count if the current bit is set to 1\n            count += x & 1\n\n            # Shift the current element to the right by one bit\n            x >>= 1\n\n        # Store the bit count for the current element in the bit count array\n        bit_count[i] = count\n\n    # Return the bit count array\n    return bit_count\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize a mask with the value 0x5555555555555555. This mask is used to extract the even bits of each element in the array.\n    mask = 0x5555555555555555\n\n    # Iterate over each element in the array and perform the following steps:\n    for i in range(len(arr)):\n\n        # Extract the even bits of the current element using the mask and store the result in the same element.\n        arr[i] = (arr[i] & mask) + ((arr[i] >> 1) & mask)\n\n    # Initialize a mask with the value 0x3333333333333333. This mask is used to extract the odd bits of each element in the array.\n    mask = 0x3333333333333333\n\n    # Iterate over each element in the array and perform the following steps:\n    for i in range(len(arr)):\n\n        # Extract the odd bits of the current element using the mask and store the result in the same element.\n        arr[i] = (arr[i] & mask) + ((arr[i] >> 2) & mask)\n\n    # Initialize a mask with the value 0x0f0f0f0f0f0f0f0f. This mask is used to extract the nibbles (4-bit groups) of each element in the array.\n    mask = 0x0f0f0f0f0f0f0f0f\n\n    # Iterate over each element in the array and perform the following steps:\n    for i in range(len(arr)):\n\n        # Extract the nibbles of the current element using the mask and store the result in the same element.\n        arr[i] = (arr[i] & mask) + ((arr[i] >> 4) & mask)\n\n    # Initialize a mask with the value 0x00ff00ff00ff00ff. This mask is used to extract the bytes ("}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize an array of 256 8-bit integers with the number of bits set to 1 in each possible 8-bit integer\n    BitsSetTable256 = [0] * 256\n    for i in range(256):\n        BitsSetTable256[i] = (i & 1) + BitsSetTable256[i // 2]\n\n    # Iterate over each element in the input array\n    for i in range(len(arr)):\n        # Split the element into 8-bit chunks\n        c = arr[i]\n        c = (c & 0x5555555555555555) + ((c >> 1) & 0x5555555555555555)\n        c = (c & 0x3333333333333333) + ((c >> 2) & 0x3333333333333333)\n        c = (c & 0x0F0F0F0F0F0F0F0F) + ((c >> 4) & 0x0F0F0F0F0F0F0F0F)\n        c = (c & 0x00FF00FF00FF00FF) + ((c >> 8) & 0x00FF00FF00FF00FF)\n        c = (c & 0x0000FFFF0000FFFF) + ((c >> 16) & 0x0000FFFF0000FFFF)\n        c = (c & 0x00000000FFFFFFFF) + ((c >> 32) & 0x00000000FFFFFFFF)\n        # Use the BitsSetTable256 array to calculate the number of bits set to 1 in each 8-bit chunk\n        arr[i] = BitsSetTable256[c & 0xFF] + BitsSet"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Define a lookup table for bit counts.\n    # The table is initialized with the number of bits set to 1 in each 4-bit number.\n    # The table is used to speed up the calculation of the bit count for each 64-bit number.\n    # The table is used as a constant, so it is defined as a tuple.\n    # The table is used as a constant, so it is defined as a tuple.\n    bit_count_table = (\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 1, 2, 2, 3, 2, 3, 3, 4,\n        2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,\n        2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6,\n        4, 5, 5, 6, 5, 6, 6, 7, 1, "}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Step 1: Mask and shift\n    # The purpose of this step is to count the number of bits set to 1 in each 2-bit group of the 64-bit integers.\n    # The idea is to use bitwise operations to mask out the 2-bit groups and count the number of bits set to 1 in each group.\n    # The mask is 0x5555555555555555, which is a 64-bit number with alternating 1s and 0s.\n    # The shift operation shifts the mask by 1 bit to the right for each iteration of the loop.\n    # The result of the masking and shifting operation is stored in the array itself, so the original array is modified in-place.\n    # The loop runs 5 times, because there are 5 2-bit groups in a 64-bit integer.\n    # The resulting array contains the number of bits set to 1 in each 2-bit group of the original array.\n    # The resulting array is a 64-bit integer array, where each element represents the number of bits set to 1 in the corresponding 2-bit group of the original array.\n\n    arr = arr - ((arr >> 1) & 0x5555555555555555)\n\n    # Step 2: Mask and shift\n    # The purpose of this step is to count the number of bits set to 1 in each 4-bit group of the 64-bit integers.\n    # The idea is to use bitwise operations to mask out the 4-bit groups and count the number of bits set to 1 in each group.\n    # The mask is 0x3333333333333333, which is a 64-bit number with alternating 2s and 0s.\n    # The shift operation shifts the mask by 2 bits to the right for each iteration of the loop.\n    # The result of the masking and shifting operation is stored in the array itself, so the original array is modified in-place.\n    # The loop runs 3 times, because"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Create a Solr instance\n    solr = pysolr.Solr(SOLR_URL, timeout=10)\n\n    # Define the query parameters\n    q_params = {\n        \"q\": q,\n        \"qf\": qf,\n        \"mm\": mm,\n        \"pf\": pf,\n        \"pf2\": pf2,\n        \"pf3\": pf3,\n        \"q.op\": q_op,\n    }\n\n    # Perform the edismax search\n    results = solr.search(**q_params)\n\n    # Extract the search results scores\n    scores = np.array([doc[\"score\"] for doc in results.docs])\n\n    # Generate an explanation of the scoring\n    explanation = f\"The search results scores were calculated using the following parameters: {q_params}. The similarity measure used was {similarity}.\"\n\n    return scores, explanation"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Create a Solr query string\n    query = f\"{q}\"\n\n    # Add query fields\n    for field in qf:\n        query += f\" {field}:{q}\"\n\n    # Add minimum match specification\n    if mm:\n        query += f\"~{mm}\"\n\n    # Add phrase matches\n    if pf:\n        for field in pf:\n            query += f\" {field}:\\\"{q}\\\"\"\n\n    # Add bigram matches\n    if pf2:\n        for field in pf2:\n            query += f\" {field}:\\\"{q}\\\"\"\n\n    # Add trigram matches\n    if pf3:\n        for field in pf3:\n            query += f\" {field}:\\\"{q}\\\"\"\n\n    # Add query operator\n    query += f\" {q_op}\"\n\n    # Create a Solr query object\n    solr_query = SolrQuery(query)\n\n    # Get the search results\n    results = frame.search(query, similarity=similarity)\n\n    # Get the search results scores\n    scores = results.scores\n\n    # Get the search results scores explanation\n    scores_explain = results.scores_explain\n\n    # Return the search results scores and explanation\n    return scores, scores_explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Initialize a Solr instance\n    solr = SolrInterface(frame,\n                         search_fields=qf,\n                         recency_buckets=None,\n                         q=q,\n                         mm=mm,\n                         pf=pf,\n                         pf2=pf2,\n                         pf3=pf3,\n                         q_op=q_op,\n                         similarity=similarity)\n\n    # Get the Solr query\n    solr_query = solr.query()\n\n    # Get the explanation of the scoring\n    explanation = solr.explain()\n\n    # Get the search results scores\n    scores = solr.get_scores()\n\n    # Return the scores and explanation\n    return scores, explanation"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse the query\n    query = parse_query(q, q_op)\n\n    # Get the query terms\n    query_terms = get_query_terms(query)\n\n    # Get the query fields\n    query_fields = get_query_fields(query, qf)\n\n    # Get the minimum match specification\n    minimum_match = get_minimum_match(mm)\n\n    # Get the phrase matches\n    phrase_matches = get_phrase_matches(query, pf)\n\n    # Get the bigram matches\n    bigram_matches = get_bigram_matches(query, pf2)\n\n    # Get the trigram matches\n    trigram_matches = get_trigram_matches(query, pf3)\n\n    # Calculate the term-centric scores\n    term_scores = calculate_term_scores(frame, query_terms, query_fields, similarity)\n\n    # Calculate the field-centric scores\n    field_scores = calculate_field_scores(frame, query_terms, query_fields, similarity)\n\n    # Calculate the phrase scores\n    phrase_scores = calculate_phrase_scores(frame, phrase_matches, similarity)\n\n    # Calculate the bigram scores\n    bigram_scores = calculate_bigram_scores(frame, bigram_matches, similarity)\n\n    # Calculate the trigram scores\n    trigram_scores = calculate_trigram_scores(frame, trigram_matches, similarity)\n\n    # Combine the scores\n    scores = combine_scores(term_scores, field_scores, phrase_scores, bigram_scores, trigram_scores)\n\n    # Apply the minimum match specification\n    scores = apply_minimum_match(scores, minimum_match)\n\n    # Return the scores and the explanation\n    return scores, \"Term-centric scores: {}, Field-centric scores: {}, Phrase scores: {}, Bigram scores: {}, Trigram scores: {}, Minimum match: {}\".format(\n        term_scores, field_scores, phrase"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse the query string into a list of query terms\n    query_terms = parse_query(q)\n\n    # Initialize a list to store the scores for each document\n    scores = []\n\n    # Iterate over each document in the DataFrame\n    for i in range(len(frame)):\n        # Initialize a variable to store the score for the current document\n        score = 0\n\n        # Iterate over each query term\n        for term in query_terms:\n            # Initialize a variable to store the maximum score for the current query term\n            max_score = 0\n\n            # Iterate over each field in the DataFrame\n            for field in qf:\n                # Get the value of the field for the current document\n                value = frame.iloc[i][field]\n\n                # Check if the field contains a list of values\n                if isinstance(value, list):\n                    # Iterate over each value in the list\n                    for val in value:\n                        # Calculate the similarity score between the query term and the value\n                        sim_score = similarity(term, val)\n\n                        # Update the maximum score if the current score is higher\n                        if sim_score > max_score:\n                            max_score = sim_score\n                else:\n                    # Calculate the similarity score between the query term and the value\n                    sim_score = similarity(term, value)\n\n                    # Update the maximum score if the current score is higher\n                    if sim_score > max_score:\n                        max_score = sim_score\n\n            # Add the maximum score for the current query term to the overall score for the document\n            score += max_score\n\n        # Add the overall score for the current document to the list of scores\n        scores.append(score)\n\n    # Convert the list of scores to a numpy array\n    scores = np.array(scores)\n\n    # Return the scores and a string explaining the scoring\n    return scores, \"edismax\""}
{"namespace": "searcharray.solr.edismax", "completion": "    # Initialize the explain string\n    explain = \"\"\n\n    # Check if the DataFrame is empty\n    if frame.shape[0] == 0:\n        return np.array([]), explain\n\n    # Check if the query string is empty\n    if not q:\n        return np.zeros(frame.shape[0]), explain\n\n    # Check if the query fields list is empty\n    if not qf:\n        return np.zeros(frame.shape[0]), explain\n\n    # Calculate the term-centric scores\n    term_scores = np.zeros(frame.shape[0])\n    for field in qf:\n        term_scores += similarity(frame[field], q)\n\n    # Calculate the field-centric scores\n    field_scores = np.zeros(frame.shape[0])\n    for field in qf:\n        field_scores += similarity(frame[field], q)\n\n    # Calculate the phrase scores\n    phrase_scores = np.zeros(frame.shape[0])\n    if pf:\n        for field in pf:\n            phrase_scores += similarity(frame[field], q)\n\n    # Calculate the bigram scores\n    bigram_scores = np.zeros(frame.shape[0])\n    if pf2:\n        for field in pf2:\n            bigram_scores += similarity(frame[field], q)\n\n    # Calculate the trigram scores\n    trigram_scores = np.zeros(frame.shape[0])\n    if pf3:\n        for field in pf3:\n            trigram_scores += similarity(frame[field], q)\n\n    # Combine the scores\n    scores = term_scores + field_scores + phrase_scores + bigram_scores + trigram_scores\n\n    # Apply the minimum should match specification\n    if mm:\n        mm_scores = np.zeros(frame.shape[0])\n        for field in qf:\n            mm_scores += similarity(frame[field], q)\n        scores += mm_sc"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is not None:\n        mm = mm.replace(\"<\", \"\").replace(\">\", \"\")\n\n    if q_op == \"OR\":\n        q_op = \"or\"\n    elif q_op == \"AND\":\n        q_op = \"and\"\n    else:\n        raise ValueError(\"q_op must be either 'OR' or 'AND'\")\n\n    if mm is None:\n        mm = \"100%\"\n\n    if not isinstance(pf, list):\n        pf = [pf]\n\n    if not isinstance(pf2, list):\n        pf2 = [pf2]\n\n    if not isinstance(pf3, list):\n        pf3 = [pf3]\n\n    if not isinstance(qf, list):\n        qf = [qf]\n\n    if not isinstance(q, list):\n        q = [q]\n\n    if pf is None:\n        pf = []\n\n    if pf2 is None:\n        pf2 = []\n\n    if pf3 is None:\n        pf3 = []\n\n    if qf is None:\n        qf = []\n\n    if q is None:\n        q = []\n\n    if q_op == \"and\":\n        q_op = \"AND\"\n    elif q_op == \"or\":\n        q_op = \"OR\"\n    else:\n        raise ValueError(\"q_op must be either 'and' or 'or'\")\n\n    if mm is None:\n        mm = \"100%\"\n\n    if not isinstance(pf, list):\n        pf = [pf]\n\n    if not isinstance(pf2, list):\n        pf2 = [pf2]\n\n    if not isinstance(pf3, list):\n        pf3 = [pf3]\n\n    if not isinstance(qf, list):\n        qf = [qf]\n\n    if not isinstance(q, list):\n        q = [q]\n\n    if pf is None:\n        pf = []\n\n    if pf2"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Create a SolrQuery object and set the query string, query operator, and minimum match specification\n    query = SolrQuery()\n    query.set_q(q)\n    query.set_qop(q_op)\n    query.set_mm(mm)\n\n    # Create a SolrFieldList object for the query fields and set the boost value for each field\n    qf_list = SolrFieldList()\n    for field in qf:\n        qf_list.add(field, 1.0)\n\n    # Add the query fields to the SolrQuery object\n    query.set_qf(qf_list)\n\n    # Create a SolrFieldList object for the phrase fields and set the boost value for each field\n    if pf is not None:\n        pf_list = SolrFieldList()\n        for field in pf:\n            pf_list.add(field, 1.0)\n        query.set_pf(pf_list)\n\n    # Create a SolrFieldList object for the bigram fields and set the boost value for each field\n    if pf2 is not None:\n        pf2_list = SolrFieldList()\n        for field in pf2:\n            pf2_list.add(field, 1.0)\n        query.set_pf2(pf2_list)\n\n    # Create a SolrFieldList object for the trigram fields and set the boost value for each field\n    if pf3 is not None:\n        pf3_list = SolrFieldList()\n        for field in pf3:\n            pf3_list.add(field, 1.0)\n        query.set_pf3(pf3_list)\n\n    # Create a SolrDocumentList object and add each document from the DataFrame to it\n    doc_list = SolrDocumentList()\n    for i, row in frame.iterrows():\n        doc = SolrInputDocument()\n        for field in qf:\n            doc.add_field(field, row[field])\n        doc_list.add(doc)\n\n    # Create a SolrQueryResponse"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    q_parser = ExtendedDisjunctionMaxParser(q_op=q_op, mm=mm)\n\n    q_ast = q_parser.parse(q)\n\n    q_terms = q_ast.terms\n\n    q_terms_set = set(q_terms)\n\n    q_terms_set_len = len(q_terms_set)\n\n    q_terms_set_len_str = str(q_terms_set_len)\n\n    q_terms_str = \" \".join(q_terms)\n\n    q_terms_str_len = len(q_terms_str)\n\n    q_terms_str_len_str = str(q_terms_str_len)\n\n    q_terms_str_len_str_str = \"len(\" + q_terms_str_len_str + \")\"\n\n    q_terms_set_len_str_str = \"len(\" + q_terms_set_len_str + \")\"\n\n    q_terms_str_len_str_str_str = \"len(\" + q_terms_str_len_str_str + \")\"\n\n    q_terms_str_len_str_str_str_str = \"len(\" + q_terms_str_len_str_str_str + \")\"\n\n    q_terms_str_len_str_str_str_str_str = \"len(\" + q_terms_str_len_str_str_str_str + \")\"\n\n    q_terms_str_len_str_str_str_str_str_str = \"len(\" + q_terms_str_len_str_str_str_str_str + \")\"\n\n    q_terms_str_len_str_str_str_str_str_str_str = \"len(\" + q_terms_str_len_str_str_str_str_str_str + \")\"\n\n    q_terms_str_len_str_str_str_str_str_str_str_str = \"len(\" + q_terms_str"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Define the query parser\n    parser = QueryParser(fieldnames=qf, schema=frame.schema)\n\n    # Parse the query\n    query = parser.parse(q)\n\n    # Define the query object\n    q_obj = Query(query, q_op=q_op, mm=mm)\n\n    # Define the phrase fields\n    phrase_fields = pf or []\n\n    # Define the bigram fields\n    bigram_fields = pf2 or []\n\n    # Define the trigram fields\n    trigram_fields = pf3 or []\n\n    # Initialize the scores array\n    scores = np.zeros(len(frame))\n\n    # Initialize the explanation string\n    explanation = \"\"\n\n    # Iterate over the query terms\n    for term in q_obj.all_terms():\n        # Get the term text\n        term_text = term.text\n\n        # Get the term boost\n        term_boost = term.boost\n\n        # Get the term field\n        term_field = term.fieldname\n\n        # Check if the term field is in the phrase fields\n        if term_field in phrase_fields:\n            # Calculate the phrase score\n            phrase_score = similarity.phrase_similarity(term_text, frame[term_field], term_boost)\n\n            # Update the scores array\n            scores += phrase_score\n\n            # Update the explanation string\n            explanation += f\"Phrase similarity of '{term_text}' in field '{term_field}' with boost {term_boost} \"\n\n        # Check if the term field is in the bigram fields\n        elif term_field in bigram_fields:\n            # Calculate the bigram score\n            bigram_score = similarity.bigram_similarity(term_text, frame[term_field], term_boost)\n\n            # Update the scores array\n            scores += bigram_score\n\n            # Update the explanation string\n            explanation += f\"Bigram similarity of '{term_text}' in field '{term_field}' with boost {term_boost} \"\n\n        # Check if the term field is in the trigram fields\n        elif term"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse the query string\n    query = parse_query(q)\n\n    # Initialize an empty list to store the scores\n    scores = []\n\n    # Iterate over the rows in the DataFrame\n    for i, row in frame.iterrows():\n        # Initialize a variable to store the score for the current row\n        score = 0\n        # Iterate over the query terms\n        for term in query:\n            # Initialize a variable to store the maximum score for the current term\n            max_score = 0\n            # Iterate over the specified query fields\n            for field in qf:\n                # Check if the field exists in the current row\n                if field in row:\n                    # Calculate the similarity score between the query term and the field value\n                    similarity_score = similarity(term, row[field])\n                    # Update the maximum score if the similarity score is higher\n                    max_score = max(max_score, similarity_score)\n            # Update the score for the current row by adding the maximum score for the current term\n            score += max_score\n        # Append the score for the current row to the list of scores\n        scores.append(score)\n\n    # Convert the list of scores to a numpy array\n    scores = np.array(scores)\n\n    # Return the scores and a string explaining the scoring\n    return scores, \"The score is calculated by summing the maximum similarity scores for each query term across the specified query fields.\"\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Split the query into individual terms\n    terms = q.split()\n\n    # Create a set of unique terms from the query\n    unique_terms = set(terms)\n\n    # Initialize the scores array with zeros\n    scores = np.zeros(len(frame))\n\n    # Initialize the explanation string\n    explanation = \"\"\n\n    # Iterate over the unique terms in the query\n    for term in unique_terms:\n        # Initialize the term scores array with zeros\n        term_scores = np.zeros(len(frame))\n\n        # Iterate over the query fields\n        for field in qf:\n            # Get the values of the current field in the DataFrame\n            values = frame[field].values\n\n            # Calculate the similarity scores for the current term and field\n            term_scores += similarity(values, term)\n\n        # Add the term scores to the overall scores\n        scores += term_scores\n\n        # Add the term scores to the explanation string\n        explanation += f\"{term} ({', '.join(qf)}): {term_scores}\\n\"\n\n    # If phrase matches are specified\n    if pf is not None:\n        # Initialize the phrase scores array with zeros\n        phrase_scores = np.zeros(len(frame))\n\n        # Iterate over the phrase matches\n        for phrase in pf:\n            # Get the values of the current phrase field in the DataFrame\n            values = frame[phrase].values\n\n            # Calculate the similarity scores for the current phrase\n            phrase_scores += similarity(values, q)\n\n        # Add the phrase scores to the overall scores\n        scores += phrase_scores\n\n        # Add the phrase scores to the explanation string\n        explanation += f\"{q} ({', '.join(pf)}): {phrase_scores}\\n\"\n\n    # If bigram matches are specified\n    if pf2 is not None:\n        # Initialize the bigram scores array with zeros\n        bigram_scores = np.zeros(len(frame))\n\n        # Iterate over the bigram matches\n        for bigram in pf"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Create a Solr query object\n    query = Query(q, q_op=q_op, mm=mm)\n\n    # Add phrase and bigram matches to the query\n    if pf is not None:\n        query.add_phrase_fields(*pf)\n    if pf2 is not None:\n        query.add_phrase_fields(*pf2, slop=1)\n    if pf3 is not None:\n        query.add_phrase_fields(*pf3, slop=2)\n\n    # Create a Solr query parser\n    parser = QueryParser(qf, similarity)\n\n    # Parse the query and get the search results\n    results = parser.search(query, frame)\n\n    # Return the search results and an explanation of how they were scored\n    return results, parser.explain(query)\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Initialize the scorer with the specified similarity measure\n    scorer = Scorer(similarity)\n\n    # Initialize the search engine with the specified query fields and minimum should match specification\n    search_engine = SearchEngine(qf=qf, mm=mm)\n\n    # If phrase matches are specified, add them to the search engine\n    if pf:\n        search_engine.add_phrase_matches(pf)\n\n    # If bigram matches are specified, add them to the search engine\n    if pf2:\n        search_engine.add_phrase_matches(pf2, 2)\n\n    # If trigram matches are specified, add them to the search engine\n    if pf3:\n        search_engine.add_phrase_matches(pf3, 3)\n\n    # Perform the search using the search engine and scorer\n    scores = search_engine.search(frame, q, scorer, q_op)\n\n    # Return the search results scores and a string explaining the scoring\n    return scores, search_engine.explain(q, q_op)\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Check if the input DataFrame is empty\n    if frame.empty:\n        # If the DataFrame is empty, return an empty array and an explanation string\n        return np.array([]), \"No documents to search\"\n\n    # Initialize an empty list to store the scores\n    scores = []\n\n    # Iterate over each document in the DataFrame\n    for _, doc in frame.iterrows():\n        # Initialize a variable to store the score for the current document\n        score = 0\n\n        # Iterate over each field in the query fields\n        for field in qf:\n            # Check if the current field exists in the current document\n            if field in doc:\n                # If the field exists, get the text value of the field\n                text = doc[field]\n                # Calculate the similarity score between the query and the text\n                score += similarity(q, text)\n\n        # Check if there are any phrase matches specified\n        if pf is not None:\n            # Iterate over each phrase field in the phrase fields\n            for field in pf:\n                # Check if the current field exists in the current document\n                if field in doc:\n                    # If the field exists, get the text value of the field\n                    text = doc[field]\n                    # Calculate the similarity score between the query and the text\n                    score += similarity(q, text, phrase=True)\n\n        # Check if there are any bigram matches specified\n        if pf2 is not None:\n            # Iterate over each bigram field in the bigram fields\n            for field in pf2:\n                # Check if the current field exists in the current document\n                if field in doc:\n                    # If the field exists, get the text value of the field\n                    text = doc[field]\n                    # Calculate the similarity score between the query and the text\n                    score += similarity(q, text, bigram=True)\n\n        # Check if there are any trigram matches specified\n        if pf3 is not None:\n            # Iterate over each trigram field in the trigram fields\n            for field in pf3:\n                # Check if the current field exists in the current document\n                if field in doc"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse the query string into a Query object\n    query = parse_query(q, q_op=q_op)\n\n    # Initialize a list to store the scores\n    scores = []\n\n    # Iterate over each document in the DataFrame\n    for i in range(len(frame)):\n        # Initialize the score for the current document\n        score = 0\n\n        # Iterate over each query term in the Query object\n        for term in query.terms:\n            # Calculate the term frequency for the current term in the current document\n            tf = term_frequency(term, frame.iloc[i], qf)\n\n            # Calculate the document frequency for the current term\n            df = document_frequency(term, frame, qf)\n\n            # Calculate the inverse document frequency for the current term\n            idf = inverse_document_frequency(len(frame), df)\n\n            # Calculate the term frequency/inverse document frequency (TF-IDF) score for the current term in the current document\n            tfidf = tfidf_score(tf, idf)\n\n            # Add the TF-IDF score to the current document's score\n            score += tfidf\n\n        # Append the current document's score to the list of scores\n        scores.append(score)\n\n    # Convert the list of scores to a NumPy array\n    scores = np.array(scores)\n\n    # Create a string to explain the scoring\n    explanation = f\"The score is calculated using the following formula: TF-IDF(term, document) = TF(term, document) * IDF(term)\"\n\n    # Return the scores and explanation as a tuple\n    return scores, explanation"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse the query string into a list of terms\n    q = parse_query(q)\n\n    # Create a set of all the unique query terms\n    q_terms = set(q)\n\n    # Initialize an empty list to store the search results scores\n    scores = []\n\n    # Initialize an empty list to store the search results explanations\n    explanations = []\n\n    # Loop through each document in the DataFrame\n    for i, doc in frame.iterrows():\n\n        # Initialize a score for the current document\n        score = 0\n\n        # Initialize an empty list to store the explanations for the current document\n        explanation = []\n\n        # Loop through each query term\n        for term in q_terms:\n\n            # Initialize a score for the current query term\n            term_score = 0\n\n            # Loop through each field to search against\n            for field in qf:\n\n                # If the field is not in the current document, skip it\n                if field not in doc:\n                    continue\n\n                # If the current field is in the phrase fields, calculate the phrase score\n                if pf is not None and field in pf:\n                    term_score += similarity.phrase_score(doc[field], term)\n\n                # If the current field is in the bigram fields, calculate the bigram score\n                if pf2 is not None and field in pf2:\n                    term_score += similarity.bigram_score(doc[field], term)\n\n                # If the current field is in the trigram fields, calculate the trigram score\n                if pf3 is not None and field in pf3:\n                    term_score += similarity.trigram_score(doc[field], term)\n\n                # If the current field is not in any phrase, bigram, or trigram fields, calculate the term score\n                if field not in pf and field not in pf2 and field not in pf3:\n                    term_score += similarity.term_score(doc[field], term)\n\n            # If the term score is greater than 0, add it to the overall document score\n            if term_score > 0:\n                score += term"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse the query string into a Query object\n    query = Query(q)\n\n    # Create a new DataFrame to store the search results\n    results = pd.DataFrame(index=frame.index)\n\n    # Initialize the explanation string\n    explanation = \"\"\n\n    # Iterate over each query term\n    for term in query.terms:\n        # If the term is a wildcard, perform a prefix search\n        if term.is_wildcard():\n            # Get the field to search against\n            field = term.field\n\n            # Get the prefix to search for\n            prefix = term.text\n\n            # Perform a prefix search on the specified field\n            mask = frame[field].str.startswith(prefix)\n\n            # Add the search results to the results DataFrame\n            results[term.text] = mask.astype(int)\n\n            # Add the search results to the explanation string\n            explanation += f\"{term.text} in {field}^2.0 \"\n        else:\n            # If the term is not a wildcard, perform a term search\n            # Get the field to search against\n            field = term.field\n\n            # Get the term text to search for\n            text = term.text\n\n            # Perform a term search on the specified field\n            mask = frame[field] == text\n\n            # Add the search results to the results DataFrame\n            results[term.text] = mask.astype(int)\n\n            # Add the search results to the explanation string\n            explanation += f\"{term.text} in {field}^1.0 \"\n\n    # If there are phrase matches specified, perform a phrase search\n    if pf is not None:\n        # Iterate over each phrase match field\n        for field in pf:\n            # Get the phrase to search for\n            phrase = \" \".join(query.terms)\n\n            # Perform a phrase search on the specified field\n            mask = frame[field].str.contains(phrase)\n\n            # Add the search results to the results DataFrame\n            results[phrase] = mask.astype(int)\n\n            # Add the search results to the explanation string"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Set the default operator for the query\n    q_op = q_op\n\n    # Initialize an empty list to store the search results\n    results = []\n\n    # Initialize an empty string to store the explanation of the scoring\n    explain = \"\"\n\n    # Split the query string into individual terms\n    query_terms = q.split()\n\n    # If the minimum should match specification is not provided, set it to the number of query terms\n    if mm is None:\n        mm = len(query_terms)\n\n    # If the phrase match fields are provided, search for phrase matches\n    if pf is not None:\n        # Initialize an empty list to store the phrase match scores\n        phrase_scores = []\n\n        # Iterate over each phrase match field\n        for field in pf:\n            # Initialize an empty list to store the phrase match scores for the current field\n            field_scores = []\n\n            # Iterate over each query term\n            for term in query_terms:\n                # Search for the current query term in the current phrase match field\n                field_scores.append(\n                    similarity(term, frame[field], field, mm=mm, q_op=q_op))\n\n            # Append the phrase match scores for the current field to the list of phrase match scores\n            phrase_scores.append(field_scores)\n\n        # If there are multiple phrase match fields, take the maximum score across all fields\n        if len(phrase_scores) > 1:\n            phrase_scores = np.max(phrase_scores, axis=0)\n        else:\n            phrase_scores = phrase_scores[0]\n\n        # Add the phrase match scores to the list of search results\n        results.append(phrase_scores)\n\n        # Add the phrase match explanation to the explanation string\n        explain += \"Phrase match: \" + \" \".join(pf) + \"\\n\"\n\n    # If the bigram match fields are provided, search for bigram matches\n    if pf2 is not None:\n        # Initialize an empty list to store the bigram match scores\n        bigram_scores = []\n\n       "}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse the query string\n    q = QueryParser(q, q_op=q_op).parse()\n\n    # Extract the search terms from the query\n    q_terms = [t for t, _ in q.terms.items()]\n\n    # Create a dictionary to store the search results\n    results = {}\n\n    # Initialize the explanation string\n    explanation = \"\"\n\n    # Check if the query is empty\n    if len(q_terms) == 0:\n        # If the query is empty, return all documents with a score of 0\n        results = {i: 0 for i in range(len(frame))}\n        explanation = \"No query terms provided. All documents have a score of 0.\"\n    else:\n        # If the query is not empty, perform the search\n\n        # Initialize the minimum number of matches required\n        mm_val = 0\n\n        # Check if a minimum match specification is provided\n        if mm is not None:\n            # If a minimum match specification is provided, parse it\n            mm = MinimumMatchParser(mm).parse()\n            # Extract the minimum number of matches required\n            mm_val = mm.value\n\n        # Initialize a dictionary to store the search results for each field\n        field_results = {}\n\n        # Iterate over each search field\n        for field in qf:\n            # Initialize a list to store the search results for the current field\n            field_result = []\n            # Iterate over each document in the DataFrame\n            for i, doc in enumerate(frame[field]):\n                # Initialize the score for the current document\n                score = 0\n                # Iterate over each search term in the query\n                for term in q_terms:\n                    # Check if the current term is present in the current document\n                    if term in doc:\n                        # If the term is present, calculate the term frequency (TF)\n                        tf = doc.count(term)\n                        # Calculate the inverse document frequency (IDF)\n                        idf = np.log(len(frame) / len(frame[frame[field].str.contains(term)]))\n                        # Calculate the term frequency/inverse"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.handle_close()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.close()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.handle_close(process)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.handle_close()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.handle_close()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.close()\n\n        return message\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.handle_close(process)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.handle_close(message)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.handle_close()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.close()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.handle_close(process)\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.handle_close(message.data)\n        else:\n            raise NotImplementedError(f\"Unknown message type {message}\")\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.handle_close(message)\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.handle_close()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.handle_close()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n        if self.server:\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n        if self.server:\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n        if self.server:\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n        if self.server:\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.close()\n        if self.server:\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.close()\n        if self.server:\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.close()\n\n        if self.server:\n            self.server.stop()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for conn in self.connections:\n            conn.close()\n        if self.server:\n            self.server.close()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.close()\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.close()\n        if self.server:\n            self.server.close()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all active connections\n        for connection in self.connections:\n            connection.stop()\n\n        # Stop the server if it exists\n        if self.server:\n            self.server.stop()\n\n        # Clear the connections list\n        self.connections = []\n"}
