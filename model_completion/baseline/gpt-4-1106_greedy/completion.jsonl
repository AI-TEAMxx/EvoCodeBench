{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of Dir class\")\n\n    if not str(output_dir).startswith(\"s3://\"):\n        raise ValueError(\"output_dir must start with 's3://'\")\n\n    if not append and not overwrite and not output_dir.is_empty():\n        raise FileExistsError(\"The directory is not empty. To overwrite data, set overwrite=True.\")\n\n    if append:\n        raise NotImplementedError(\"Appending data is not supported yet.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data is not supported yet.\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    from botocore.exceptions import ClientError\n    import boto3\n    if output_dir.has_index_file():\n        raise ValueError(f\"The directory '{output_dir.prefix}' already contains an index file.\")\n    else:\n        output_dir.delete_objects()"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        from typing import Optional\n        import time\n        import os\n        index_parts = [f'index_part_{i}.bin' for i in range(num_workers)]\n        merged_index_file = 'index_merged.bin'\n        cache_directory = './cache'  # Assuming the index files are stored in a 'cache' directory\n\n        # Wait for all index parts to be available\n        all_parts_available = False\n        while not all_parts_available:\n            all_parts_available = all(os.path.exists(os.path.join(cache_directory, part)) for part in index_parts)\n            if not all_parts_available:\n                print(\"Waiting for all index parts to be available...\")\n                time.sleep(5)  # Wait for 5 seconds before checking again\n\n        # Perform the merge if this is the master node (rank 0)\n        if node_rank is None or node_rank == 0:\n            print(\"Merging index parts...\")\n            with open(os.path.join(cache_directory, merged_index_file), 'wb') as merged_file:\n                for part in index_parts:\n                    part_path = os.path.join(cache_directory, part)\n                    with open(part_path, 'rb') as part_file:\n                        merged_file.write(part_file.read())\n            print(\"Merge completed.\")\n\n        # If not the master node, wait until the merged index file is available\n        elif node_rank != 0:\n            while not os.path.exists(os.path.join(cache_directory, merged_index_file)):\n                print(f\"Node {node_rank} waiting for the merged index file...\")\n                time.sleep(5)  # Wait for 5 seconds before checking again\n\n        print(\"Merge function completed.\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    import random\n    import time\n    from typing import Optional\n    if machine is None:\n        machine = get_default_machine()\n    \n    if command is None:\n        command = \"default_command\"  # Replace with actual default command logic\n\n    job_id = create_job(name, num_nodes, machine, command)\n    print(f\"Job {job_id} created. Waiting for it to start...\")\n\n    while True:\n        status = get_job_status(job_id)\n        if status == \"RUNNING\":\n            print(f\"Job is running. URL: {get_job_url(job_id)}\")\n            break\n        elif status == \"FAILED\":\n            raise Exception(f\"Job {job_id} has failed.\")\n        time.sleep(5)  # Wait for 5 seconds before checking the status again\n\n    # Additional logic to handle the job's lifecycle could be added here"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        from typing import List\n        # Add the chunk indexes to the deletion queue\n        self.deletion_queue.extend(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        from typing import Optional\n        config_filename = 'config.json'\n        config_path = os.path.join(self._cache_dir, config_filename)\n\n        # Check if the configuration file exists\n        if not os.path.exists(config_path):\n            return None\n\n        # Assuming that the serializers are keyed by file extension\n        file_extension = os.path.splitext(config_filename)[1].lstrip('.')\n        serializer = self._serializers.get(file_extension)\n\n        if serializer is None:\n            # No serializer found for the file extension\n            return None\n\n        # Attempt to load and deserialize the configuration file\n        try:\n            with open(config_path, 'r') as config_file:\n                config_data = config_file.read()\n                chunks_config = serializer.deserialize(config_data)\n                if isinstance(chunks_config, ChunksConfig):\n                    return chunks_config\n                else:\n                    # Deserialization did not return a ChunksConfig object\n                    return None\n        except Exception as e:\n            # Handle exceptions that may occur during file reading or deserialization\n            print(f\"Failed to load configuration: {e}\")\n            return None"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        from queue import Queue\n        from typing import List\n        for index in chunk_indexes:\n            self._to_download_queue.put(index)\n        # Optionally, you can add some logging or additional processing here\n        print(f\"Enqueued {len(chunk_indexes)} chunks for download.\")"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration is not set. Please define the configuration before accessing it.\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        from typing import Any\n        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"Provided index must be an instance of ChunkedIndex\")\n\n        # Check if the reader's index configuration is defined\n        if self.index_configuration is None:\n            raise Exception(\"Reader's index configuration is not defined\")\n\n        # Assert the presence of a prepare thread\n        assert self.prepare_thread is not None, \"Prepare thread must be present\"\n\n        # Determine the specific chunk based on the index\n        chunk_id = index.get_chunk_id()  # Assuming get_chunk_id() is a method of ChunkedIndex\n\n        # Check if the chunk is available locally or in memory\n        if chunk_id not in self.chunks:\n            # If not, initiate download or load the chunk\n            self.chunks[chunk_id] = self._download_chunk(chunk_id)\n\n        # Prefetch next chunk if necessary (not shown here)\n\n        # Read the item from the chunk\n        item = self._read_item_from_chunk(chunk_id, index)\n\n        # Handle the lifecycle of the chunk, delete if fully consumed\n        if self._is_chunk_fully_consumed(chunk_id):\n            del self.chunks[chunk_id]\n\n        # Return the read item\n        return item"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n    total_size = 0\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            try:\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                # If the file was not found, which can happen if it was deleted\n                # while the script was running, ignore the error.\n                pass\n    return total_size"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    import os\n    if is_distributed_environment():\n        # In a real distributed environment, you would use a proper distributed map or broadcast mechanism\n        # Here we're just simulating it by storing the object in a global dictionary\n        distributed_map[key] = obj\n        # In a real scenario, other machines would retrieve the object from the distributed map\n        return distributed_map[key]\n    else:\n        # If not in a distributed environment, return the object as is\n        return obj"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    from typing import Any, List, Dict, Tuple\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n    \n    # Sort items by weight in descending order\n    items_with_weights = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n    \n    # Distribute items into bins greedily\n    for item, weight in items_with_weights:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n        # Place the item in this bin\n        bins[min_bin].append(item)\n        # Update the bin's total weight\n        bin_weights[min_bin] += weight\n    \n    return bins, bin_weights"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    from typing import List\n    import random\n    random.seed(seed + current_epoch)\n\n    # Shuffle the chunks within each rank\n    shuffled_chunks = []\n    for rank_chunks in chunks_per_ranks:\n        # Shuffle the chunks for the current rank\n        random.shuffle(rank_chunks)\n        # Extend the shuffled_chunks list with the shuffled chunks of the current rank\n        shuffled_chunks.extend(rank_chunks)\n\n    return shuffled_chunks"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "\n    # Define the units for human-readable format\n    units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n    unit_index = 0\n\n    # Loop to find the appropriate unit\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000.0\n        unit_index += 1\n\n    # Format the number with the appropriate unit\n    return f\"{num_bytes:.2f} {units[unit_index]}\""}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    from typing import Sequence, Any, Optional\n    import os\n    input_dirs = []\n\n    # Check the first two elements for valid file paths\n    for input_item in inputs[:2]:\n        if isinstance(input_item, str) and os.path.isfile(input_item):\n            input_dir = os.path.dirname(os.path.abspath(input_item))\n            input_dirs.append(input_dir)\n\n    # If no valid paths are found, return None\n    if not input_dirs:\n        return None\n\n    # Check if the paths are consistent\n    if len(input_dirs) == 2 and input_dirs[0] != input_dirs[1]:\n        raise ValueError(\"Inconsistent file paths found in inputs.\")\n\n    # Return the input directory\n    return input_dirs[0]"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    from contextlib import contextmanager\n    try:\n        # Here you would put the logic to enable DNS optimization if 'enable' is True.\n        if enable:\n            print(\"DNS optimization enabled.\")\n        else:\n            print(\"DNS optimization disabled.\")\n        # Yield control back to the caller to perform operations within the context.\n        yield\n    finally:\n        # Here you would put the logic to always disable DNS optimization when exiting the context.\n        print(\"DNS optimization disabled.\")"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    from typing import List, Tuple, Any\n    world_size = distributed_env.world_size\n    num_chunks = len(indexes)\n    chunks_per_rank = num_chunks // world_size\n    remainder = num_chunks % world_size\n\n    if drop_last and remainder:\n        # Drop the last few chunks to make the distribution even\n        indexes = indexes[:chunks_per_rank * world_size]\n        chunk_intervals = chunk_intervals[:chunks_per_rank * world_size]\n\n    # Initialize the lists to hold chunk indexes and intervals for each rank\n    rank_chunk_indexes = [[] for _ in range(world_size)]\n    rank_chunk_intervals = [[] for _ in range(world_size)]\n\n    # Distribute chunks and intervals to ranks\n    for rank in range(world_size):\n        start_index = rank * chunks_per_rank\n        end_index = start_index + chunks_per_rank\n        if rank < remainder:\n            end_index += 1\n        rank_chunk_indexes[rank] = indexes[start_index:end_index]\n        rank_chunk_intervals[rank] = chunk_intervals[start_index:end_index]\n\n    return rank_chunk_indexes, rank_chunk_intervals"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {'is_last': is_last} if self._contains_is_last else {}\n        \n        # Call the transformation function with the required arguments and kwargs\n        self._fn(item_metadata, output_dir, **kwargs)"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    from botocore.client import BaseClient as S3Client\n    from typing import Any\n    from urllib import parse\n    import botocore\n    import time\n    while True:\n        try:\n            # Attempt to retrieve the file metadata using head_object\n            response = s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n            # If the file exists, return the response\n            return response\n        except botocore.exceptions.ClientError as e:\n            # If the error code is 404 (Not Found), then wait and try again\n            if e.response['Error']['Code'] == '404':\n                time.sleep(sleep_time)\n            else:\n                # If any other error occurs, raise the error\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n    import shutil\n    threshold_in_bytes = threshold_in_gb * 1024 * 1024 * 1024  # Convert GB to bytes\n\n    # Check if the input directory exists\n    if not os.path.exists(input_dir):\n        raise ValueError(f\"The directory {input_dir} does not exist.\")\n\n    while True:\n        # Get the disk usage statistics about the given path\n        total, used, free = shutil.disk_usage(input_dir)\n\n        # Check if the free space is lower than the threshold\n        if free < threshold_in_bytes:\n            break  # Exit the loop if the condition is met\n\n        # Sleep for the specified amount of time before checking again\n        time.sleep(sleep_time)"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    from pathlib import Path\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n    from typing import Any, Callable, List, Optional, Sequence, Union\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n\n    # If fast_dev_run is True, only process a subset of inputs\n    if fast_dev_run:\n        inputs = inputs[:10]  # Process only the first 10 inputs for example\n\n    # If reorder_files is True, reorder the inputs based on some criteria (e.g., file size)\n    if reorder_files:\n        inputs = sorted(inputs, key=lambda x: os.path.getsize(x))\n\n    # If num_workers is not specified, use the maximum number of available workers\n    if num_workers is None:\n        num_workers = os.cpu_count() or 1\n\n    # Process inputs in parallel using ThreadPoolExecutor\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        future_to_input = {executor.submit(fn, inp): inp for inp in inputs}\n\n        for future in as_completed(future_to_input):\n            input_item = future_to_input[future]\n            try:\n                result = future.result()\n                # Save the result to the output directory\n                output_path = os.path.join(output_dir, os.path.basename(input_item))\n                with open(output_path, 'wb') as f:\n                    f.write(result)\n            except Exception as exc:\n                print(f'Input {input_item} generated an exception: {exc}')\n\n    # Note: This is a simplified example and does not handle all the parameters such as chunking, compression, etc.\n    # You would need to implement additional logic to handle those features."}
{"namespace": "litdata.processing.functions.map", "completion": "    from concurrent.futures import ThreadPoolExecutor\n    import os\n    from pathlib import Path\n    from typing import Callable, Sequence, Union, Optional, List, Any\n    output_dir = Path(output_dir) if isinstance(output_dir, str) else output_dir\n\n    # Check if the output directory is not empty and raise an error if required\n    if error_when_not_empty and any(output_dir.iterdir()):\n        raise ValueError(f\"The output directory {output_dir} is not empty.\")\n\n    # If fast_dev_run is an integer, limit the number of inputs to process\n    if isinstance(fast_dev_run, int) and fast_dev_run > 0:\n        inputs = inputs[:fast_dev_run]\n    elif fast_dev_run is True:  # If fast_dev_run is True, process only one input\n        inputs = inputs[:1]\n\n    # If reorder_files is True, reorder inputs by file size (assuming inputs are file paths)\n    if reorder_files:\n        inputs = sorted(inputs, key=lambda x: os.path.getsize(x) if os.path.isfile(x) else 0)\n\n    # Define the processing function that will be applied to each input\n    def process_input(input_item):\n        # If a reader is provided, use it to read the input, otherwise use the input directly\n        input_data = reader.read(input_item) if reader else input_item\n        # Call the provided function `fn` with the output directory and the input data\n        fn(str(output_dir), input_data)\n\n    # Determine the number of workers to use\n    num_workers = num_workers or os.cpu_count() or 1\n\n    # Process the inputs using a ThreadPoolExecutor\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        # If batch_size is specified, process inputs in batches\n        if batch_size:\n            for i in range(0, len(inputs), batch_size):\n                batch = inputs[i:i + batch_size]\n                executor.map(process_input, batch)\n        else:\n            # Process each input individually\n            executor.map(process_input, inputs)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    from queue import Queue\n    import os\n    while True:\n        # Fetch the next task from the input queue\n        task = queue_in.get()\n        if task is None:\n            # A None task is used to signal the worker to shut down\n            break\n\n        task_index, file_paths = task\n\n        # Process each file in the task\n        for file_path in file_paths:\n            # Construct the full path to the cache directory\n            cache_file_path = os.path.join(cache_dir, file_path)\n\n            # Check if the file already exists in the cache\n            if not os.path.exists(cache_file_path):\n                # If the file does not exist, download it\n                source_url = input_dir.get_url(file_path)  # Hypothetical method to get the full URL of the file\n                os.makedirs(os.path.dirname(cache_file_path), exist_ok=True)  # Ensure the directory exists\n                download_file(source_url, cache_file_path)\n\n        # Signal that the task is complete by putting the task index into the output queue\n        queue_out.put(task_index)\n\n    # Optionally, signal that this worker has finished\n    queue_out.put(None)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    from queue import Queue, Empty\n    import boto3\n    import shutil\n    import os\n    is_s3 = output_dir.url.startswith(\"s3://\")\n    if is_s3:\n        # Extract bucket name and path from the URL\n        s3_bucket_name = output_dir.url[5:].split('/')[0]\n        s3_base_path = '/'.join(output_dir.url[5:].split('/')[1:])\n        s3_client = boto3.client('s3')\n\n    while True:\n        try:\n            # Get the next file path from the upload queue\n            item = upload_queue.get(timeout=5)  # Adjust timeout as needed\n            if item is None:\n                # Termination signal received\n                break\n\n            # If the item is a tuple, it contains a temporary directory and a file path\n            if isinstance(item, tuple):\n                temp_dir, file_path = item\n            else:\n                file_path = item\n\n            # Ensure the file path is absolute by prepending the cache directory if necessary\n            if not os.path.isabs(file_path):\n                file_path = os.path.join(cache_dir, file_path)\n\n            # Determine the destination path\n            if is_s3:\n                # For S3, the destination is the bucket and the object key\n                s3_key = os.path.join(s3_base_path, os.path.relpath(file_path, cache_dir))\n                # Upload the file to S3\n                s3_client.upload_file(file_path, s3_bucket_name, s3_key)\n            else:\n                # For local file system, the destination is a path within the output directory\n                dest_path = os.path.join(output_dir.url, os.path.relpath(file_path, cache_dir))\n                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n                # Move the file to the destination\n                shutil.move(file_path, dest_path)\n\n            # Put the file path into the remove queue\n            remove_queue.put(file_path)\n\n        except Empty:\n            # No item in the queue, continue to check for new items\n            continue\n        except Exception as e:\n            print(f\"An error occurred while uploading: {e}\")\n            # Handle the error as appropriate (e.g., retry, log, etc.)\n\n    # Perform any necessary cleanup before exiting the function\n    # ..."}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    from typing import List, Any, Optional\n    import random\n    if weights is None:\n        weights = [1] * len(user_items)  # Assign equal weight if no weights are provided\n\n    # Calculate the total weight and the target weight per worker\n    total_weight = sum(weights)\n    target_weight_per_worker = total_weight / num_workers\n\n    # Initialize the list of items for each worker\n    worker_items = [[] for _ in range(num_workers)]\n    worker_weights = [0] * num_workers  # Track the total weight for each worker\n\n    # Sort items by weight in descending order to distribute larger items first\n    items_with_weights = sorted(zip(user_items, weights), key=lambda x: x[1], reverse=True)\n\n    for item, weight in items_with_weights:\n        # Find the worker with the least weight and assign the item to them\n        worker_index = worker_weights.index(min(worker_weights))\n        worker_items[worker_index].append(item)\n        worker_weights[worker_index] += weight\n\n    # Shuffle the items for each worker to ensure randomness\n    for items in worker_items:\n        random.shuffle(items)\n\n    # Optionally print the distribution details\n    if file_size:\n        for i, items in enumerate(worker_items):\n            sizes = [weights[user_items.index(item)] for item in items]\n            print(f\"Worker {i}: {sum(sizes) / (1024 * 1024):.2f} MB\")\n    else:\n        for i, weight in enumerate(worker_weights):\n            print(f\"Worker {i}: {weight} weight units\")\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    from typing import List, Any\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    total_workers = num_workers * num_nodes\n\n    # Calculate the number of items per worker and the remainder\n    items_per_worker = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    # Initialize the list of lists to hold the items for each worker\n    worker_items = [[] for _ in range(num_workers)]\n\n    # Distribute the items to the workers\n    for i, item in enumerate(user_items):\n        # Determine the worker index based on the current item index\n        worker_index = (i // items_per_worker) % num_workers\n        # If we have gone through all workers, distribute the remainder starting from the last worker\n        if i >= items_per_worker * total_workers:\n            worker_index = num_workers - 1 - ((i - items_per_worker * total_workers) % num_workers)\n        worker_items[worker_index].append(item)\n\n    # Check if the distribution is correct\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        import shutil\n        import os\n        for cache_dir in self.cache_directories:\n            # Check if the cache directory exists\n            if os.path.exists(cache_dir):\n                # Remove the cache directory and its contents\n                shutil.rmtree(cache_dir)\n            # Recreate the cache directory\n            os.makedirs(cache_dir)"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    from typing import List, Any\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n    import os\n    def get_file_size(item):\n        # Assuming item is a path or can be converted to a path\n        # If item is not a string, you might need to adjust this line\n        file_path = os.path.join(base_path, str(item))\n        try:\n            return os.path.getsize(file_path)\n        except OSError as e:\n            print(f\"Error getting size for {file_path}: {e}\")\n            return 0  # Return 0 if there's an error getting the file size\n\n    # Determine the number of workers based on the CPU count\n    num_workers = os.cpu_count() or 1\n\n    # Use ThreadPoolExecutor to parallelize the file size retrieval\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        # Submit all file size retrieval tasks to the executor\n        future_to_item = {executor.submit(get_file_size, item): item for item in items}\n\n        # Retrieve the results as they are completed\n        file_sizes = []\n        for future in as_completed(future_to_item):\n            file_sizes.append(future.result())\n\n    return file_sizes"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    from typing import Optional, Any\n    import os\n    if not isinstance(element, str):\n        return False\n\n    # Convert element to an absolute path if it's not already one\n    element_path = os.path.abspath(element)\n\n    # Check if the path exists in the file system\n    if os.path.exists(element_path):\n        return True\n\n    # If input_dir is specified, check if element is a subpath of input_dir\n    if input_dir:\n        input_dir_path = os.path.abspath(input_dir)\n        # Check if the element_path starts with the input_dir_path\n        return element_path.startswith(input_dir_path)\n\n    return False"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        from typing import Literal\n        import torch.nn as nn\n        import torch\n        assert n_layers > 0, \"Number of layers must be greater than 0\"\n        assert n_neurons > 0, \"Number of neurons must be greater than 0\"\n\n        # Define the activation function for hidden layers\n        if activation == \"ReLU\":\n            hidden_activation = nn.ReLU()\n        elif activation == \"None\":\n            hidden_activation = nn.Identity()\n\n        # Define the activation function for the output layer\n        if output_activation == \"ReLU\":\n            output_activation_fn = nn.ReLU()\n        elif output_activation == \"Sigmoid\":\n            output_activation_fn = nn.Sigmoid()\n        elif output_activation == \"None\":\n            output_activation_fn = nn.Identity()\n\n        # Create the layers of the network\n        layers = []\n        for i in range(n_layers):\n            # Determine the input and output dimensions for the current layer\n            in_features = n_input_dims if i == 0 else n_neurons\n            out_features = n_output_dims if i == n_layers - 1 else n_neurons\n\n            # Add the linear layer\n            layers.append(nn.Linear(in_features, out_features))\n\n            # Add the activation function (skip for the last layer)\n            if i < n_layers - 1:\n                layers.append(hidden_activation)\n            else:\n                layers.append(output_activation_fn)\n\n        # Create the neural network model\n        model = nn.Sequential(*layers)\n        return model"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    import numpy as np\n    from shapely.geometry import Polygon\n    from typing import List\n    areas = [Polygon(polygon).area for polygon in polygons]\n    \n    # Find the largest area\n    max_area = max(areas) if areas else 0\n    \n    # Calculate the relative area threshold\n    rel_area_threshold = rel_tr * max_area\n    \n    # Determine the effective threshold (maximum of relative and absolute thresholds)\n    effective_threshold = max(rel_area_threshold, abs_tr)\n    \n    # Filter out polygons that don't meet the area criteria\n    filtered_polygons = [\n        polygon for polygon, area in zip(polygons, areas)\n        if area >= effective_threshold\n    ]\n    \n    return filtered_polygons"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        import numpy as np\n        shifted_signals = []\n        \n        # Generate shifted versions of the signal\n        for shift in range(-kernel_offset, kernel_offset + 1):\n            # Shift the signal and pad with NaN to maintain the array size\n            shifted_signal = np.roll(signal, shift)\n            if shift < 0:\n                shifted_signal[shift:] = np.nan\n            elif shift > 0:\n                shifted_signal[:shift] = np.nan\n            shifted_signals.append(shifted_signal)\n        \n        # Stack the shifted signals and compute the median along the first axis\n        stacked_signals = np.vstack(shifted_signals)\n        median_signal = np.nanmedian(stacked_signals, axis=0)\n        \n        # Trim the resulting median array to remove edge effects\n        trimmed_median_signal = median_signal[kernel_offset:-kernel_offset]\n        \n        return trimmed_median_signal"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    import numpy as np\n    from typing import Tuple, Optional, List\n    min_distance = float('inf')\n    min_shift = 0\n    code_length = template_probe.code.shape[1]\n\n    for shift in range(-rotation_shift, rotation_shift + 1):\n        # Shift the probe template\n        shifted_code = np.roll(template_probe.code, shift, axis=1)\n        shifted_mask = np.roll(template_probe.mask, shift, axis=1)\n\n        # Calculate the Hamming distance\n        mask = np.logical_or(shifted_mask, template_gallery.mask)\n        total_bits = np.sum(np.logical_not(mask))\n        if total_bits == 0:\n            continue\n\n        diff = np.logical_xor(shifted_code, template_gallery.code)\n        diff = np.logical_and(diff, np.logical_not(mask))\n\n        # Calculate the weighted Hamming distance if weights are provided\n        if weights is not None:\n            weighted_diff = diff * weights[shift + rotation_shift]\n            distance = np.sum(weighted_diff) / total_bits\n        else:\n            distance = np.sum(diff) / total_bits\n\n        # Check for normalized Hamming distance\n        if nm_dist is not None:\n            distance = distance / nm_dist\n\n        # Update minimum distance and corresponding shift\n        if distance < min_distance:\n            min_distance = distance\n            min_shift = shift\n\n    return min_distance, min_shift"}
{"namespace": "iris.utils.math.area", "completion": "Area = |(\u03a3(x_i * y_(i+1)) - \u03a3(y_i * x_(i+1)))| / 2"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        from typing import Tuple\n        import numpy as np\n        bisector_start_points = []\n        bisector_end_points = []\n        num_bisectors_calculated = 0\n        iterations = 0\n\n        while num_bisectors_calculated < self.num_bisectors and iterations < self.max_iterations:\n            # Randomly select two distinct vertices from the polygon\n            idx1, idx2 = np.random.choice(len(polygon), 2, replace=False)\n            point1, point2 = polygon[idx1], polygon[idx2]\n\n            # Check if the distance between the points is greater than the minimum required\n            if np.linalg.norm(point1 - point2) >= min_distance_between_sector_points_in_px:\n                # Calculate the midpoint\n                midpoint = (point1 + point2) / 2\n\n                # Calculate the slope of the line perpendicular to the line connecting the two points\n                dx, dy = point2 - point1\n                if dx == 0:\n                    # Line is vertical, so the perpendicular bisector is horizontal\n                    perp_slope = 0\n                elif dy == 0:\n                    # Line is horizontal, so the perpendicular bisector is vertical\n                    perp_slope = float('inf')\n                else:\n                    perp_slope = -dx / dy\n\n                # Calculate the start and end points of the perpendicular bisector\n                if perp_slope == float('inf'):\n                    # Vertical line\n                    start_point = np.array([midpoint[0], 0])\n                    end_point = np.array([midpoint[0], 1])\n                elif perp_slope == 0:\n                    # Horizontal line\n                    start_point = np.array([0, midpoint[1]])\n                    end_point = np.array([1, midpoint[1]])\n                else:\n                    # General case\n                    start_point = midpoint + np.array([1, perp_slope])\n                    end_point = midpoint - np.array([1, perp_slope])\n\n                bisector_start_points.append(start_point)\n                bisector_end_points.append(end_point)\n                num_bisectors_calculated += 1\n\n            iterations += 1\n\n        if num_bisectors_calculated < self.num_bisectors:\n            raise EyeCentersEstimationError(\"Failed to find sufficient number of point pairs within the maximum iterations.\")\n\n        return np.array(bisector_start_points), np.array(bisector_end_points)"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    import numpy as np\n    total_length = 0.0\n    for i in range(len(polygon) - 1):\n        point1 = polygon[i]\n        point2 = polygon[i + 1]\n        distance = np.linalg.norm(point1 - point2)\n        if distance <= max_point_distance:\n            total_length += distance\n    return total_length"}
{"namespace": "iris.io.validators.is_binary", "completion": "    from pydantic import fields\n    import numpy as np\n    if v.dtype == np.bool_ and np.all(np.isin(v, [True, False])):\n        return v\n    else:\n        # Raise a ValueError with a message indicating the class, field name, and incorrect data type\n        raise ValueError(f\"Non-binary array encountered in class '{cls.__name__}' for field '{field.name}'. \"\n                         f\"Array must contain only boolean values (True or False), but found {v.dtype}.\")"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    from pydantic import fields\n    import numpy as np\n    if not isinstance(v, np.ndarray):\n        raise ValueError(f\"The field '{field.name}' should be a numpy array, but got {type(v).__name__}.\")\n    if len(v.shape) != 2 or v.shape[1] != 2:\n        raise ValueError(f\"The field '{field.name}' should be a numpy array with shape (_, 2), but got shape {v.shape}.\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "from pydantic import fields\nfrom typing import Any\n\n    # Check if v is a single value by trying to iterate over it\n    try:\n        values = iter(v)\n    except TypeError:\n        values = [v]\n\n    # Check each value in the iterable for positivity\n    for value in values:\n        if value <= 0:\n            raise ValueError(f\"All values in '{field.name}' must be positive, but got {value} in class '{cls.__name__}'.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    # Check if all required keys are present in the dictionary\n    required_keys = {\"x_min\", \"x_max\", \"y_min\", \"y_max\"}\n    if not required_keys.issubset(values.keys()):\n        raise ValueError(f\"{cls.__name__}: Missing keys in bounding box values. Required keys are {required_keys}\")\n\n    # Check if the minimum values are less than the maximum values\n    if values['x_min'] >= values['x_max'] or values['y_min'] >= values['y_max']:\n        raise ValueError(f\"{cls.__name__}: Bounding box values are invalid. 'x_min' must be less than 'x_max' and 'y_min' must be less than 'y_max'.\")\n\n    return values"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    from pydantic import BaseModel, validator, ValidationError, fields\n    import numpy as np\n    from typing import Callable\n    def _validate_array_dimensions(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if not isinstance(v, np.ndarray):\n            raise TypeError(f\"The field '{field.name}' must be of type 'np.ndarray'.\")\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"The field '{field.name}' must have {nb_dimensions} dimensions, but got {v.ndim}.\")\n        return v\n    return _validate_array_dimensions"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    import numpy as np\n    from pydantic import BaseModel, validator\n    from typing import Callable, Type\n    def _validate_shapes(cls: Type[BaseModel], values: dict) -> dict:\n        list1 = values.get(field1)\n        list2 = values.get(field2)\n\n        if not isinstance(list1, list) or not isinstance(list2, list):\n            raise ValueError(f\"{field1} and {field2} must be lists of numpy arrays.\")\n\n        if len(list1) != len(list2):\n            raise ValueError(f\"The length of {field1} and {field2} must be the same.\")\n\n        for array1, array2 in zip(list1, list2):\n            if not isinstance(array1, np.ndarray) or not isinstance(array2, np.ndarray):\n                raise ValueError(f\"Both {field1} and {field2} must contain only numpy arrays.\")\n\n            if array1.shape != array2.shape:\n                raise ValueError(f\"The shapes of arrays in {field1} and {field2} at the same index must be equal.\")\n\n        return values\n\n    return validator(field1, field2, allow_reuse=True)(_validate_shapes)"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    from typing import Callable\n    from pydantic import BaseModel, validator\n    def _validate_shapes(cls, value, values, **kwargs):\n        shape1 = getattr(values, field1, None).shape\n        shape2 = getattr(values, field2, None).shape\n        if shape1 != shape2:\n            raise ValueError(f\"The shapes of {field1} and {field2} must be equal, but got {shape1} and {shape2}.\")\n        return value\n\n    return validator(field1, allow_reuse=True)(_validate_shapes)"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        from typing import Any\n        for callback in self._callbacks:\n            if hasattr(callback, 'pre_execution'):\n                callback.pre_execution(*args, **kwargs)\n\n        # Run the main algorithm\n        result = self.run(*args, **kwargs)\n\n        # Call post-execution hooks\n        for callback in self._callbacks:\n            if hasattr(callback, 'post_execution'):\n                callback.post_execution(*args, **kwargs)\n\n        return result"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        from typing import Any\n        import json\n        try:\n            # Deserialize the JSON string\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            # If JSON is invalid, return False\n            return False\n\n        # Check if the deserialized output matches the type definition\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n\n    \"\"\"\n    Encodes an integer into a single character based on a predefined character set. The character set consists of lowercase letters, digits, and an underscore.\n\n    Input-Output Arguments\n    :param n: Integer. The integer to be encoded. It is used as an index to select a character from the character set.\n    :return: String. The encoded character corresponding to the input integer.\n    \"\"\"\n\n    # Check if the input integer is within the valid range\n    if 0 <= n < len(charset):\n        # Return the character at the index of the input integer\n        return charset[n]\n    else:\n        # Raise an error if the input integer is out of range\n        raise ValueError(\"Integer out of encoding range.\")"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        from typing import Any, get_type_hints, Union\n        import inspect\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Extract the function's name and docstring\n        name = func_object.__name__\n        docstring = func_object.__doc__\n\n        # Separate input and output type hints\n        input_types = {param.name: type_hints[param.name] for param in signature.parameters.values()}\n        output_type = type_hints.get('return', None)\n\n        # Get class definitions for input types\n        input_classes = {name: get_class_definition(hint) for name, hint in input_types.items()}\n\n        # Determine the function type and get the output class definition\n        if inspect.isclass(output_type) and issubclass(output_type, Embedding):\n            function_type = 'EMBEDDABLE'\n        elif isinstance(output_type, Union):\n            function_type = 'SYMBOLIC'\n        else:\n            function_type = 'UNKNOWN'\n\n        output_class = get_class_definition(output_type)\n\n        # Create and return the FunctionDescription instance\n        return FunctionDescription(\n            name=name,\n            docstring=docstring,\n            input_types=input_types,\n            output_type=output_type,\n            function_type=function_type,\n            input_classes=input_classes,\n            output_class=output_class\n        )"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for hash_func in self.hash_functions:\n            index = hash_func(string) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        import logging\n        try:\n            # Load the bit array from persistence\n            loaded_bit_array = self.persistence.load()\n\n            # Check if the loaded bit array's length matches the expected length\n            if len(loaded_bit_array) == self.size:\n                self.bit_array = loaded_bit_array\n            else:\n                # Log a warning if there is a length mismatch\n                logging.warning(\"Loaded bit array length does not match expected size. Reinitializing.\")\n                # Reinitialize the bit array and indices\n                self.bit_array = self.init_bit_array()\n                # Save the new state\n                self.save()\n\n        except Exception as e:\n            # Log an error if an exception occurs during loading\n            logging.error(f\"An error occurred while loading the bit array: {e}\")\n            # Reinitialize the bit array and indices\n            self.bit_array = self.init_bit_array()\n            # Save the new state\n            self.save()"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for hash_func in self.hash_functions:\n            # Generate the index for the current hash function\n            index = hash_func(string) % self.size\n            # Check if the bit at the index is not set\n            if not self.bit_array[index]:\n                # If any bit is not set, the string is definitely not in the filter\n                return False\n        # If all bits are set, the string might be in the filter\n        return True"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    import numpy as np\n    if zeros >= n:\n        raise ValueError(\"The number of zeros must be less than the number of weights\")\n\n    # Generate n-zeros positive numbers\n    weights = np.random.rand(n - zeros)\n    \n    # Normalize the weights so they sum up to 1\n    weights /= weights.sum()\n    \n    # If zeros are required, extend the array with zeros and shuffle it\n    if zeros > 0:\n        weights = np.concatenate((weights, np.zeros(zeros)))\n        np.random.shuffle(weights)\n    \n    return weights"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    import numpy as np\n    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        return False\n\n    try:\n        # Attempt to perform Cholesky decomposition\n        np.linalg.cholesky(x)\n        return True  # Decomposition was successful\n    except np.linalg.LinAlgError:\n        # Decomposition failed, which means the matrix is not positive definite\n        return False"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        # Assuming that the dictionary keys match the attribute names of the FunctionConfig class\n        for key, value in json_dict.items():\n            setattr(self, key, value)\n        return self"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        import random\n        import time\n        max_retries = 5\n        backoff_factor = 2\n        retry_count = 0\n\n        while retry_count < max_retries:\n            try:\n                # Simulate API call to OpenAI\n                response = self._mock_openai_api_call(model, system_message, prompt, **kwargs)\n                \n                # Process the response to remove parsing helper tokens\n                processed_response = self._process_response(response, model.parsing_helper_tokens)\n                \n                return processed_response\n            except Exception as e:\n                print(f\"An error occurred: {e}\")\n                retry_count += 1\n                time.sleep(backoff_factor ** retry_count)  # Exponential backoff\n\n        raise Exception(\"Failed to generate response after maximum retries.\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    import numpy as np\n    if not isinstance(x, np.ndarray):\n        raise ValueError(\"Input must be a numpy array.\")\n    \n    if x.ndim != 2:\n        raise ValueError(\"Input must be a 2-dimensional array.\")\n    \n    rows, cols = x.shape\n    if rows != cols:\n        raise ValueError(\"Input array must be square (number of rows and columns must be equal).\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    import numpy as np\n    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n    # Check if the matrix is equal to its transpose\n    if not np.array_equal(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    import numpy as np\n    # Ensure the input is a 2D array\n    if cov.ndim != 2 or cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"Input must be a square 2D numpy array\")\n\n    # Calculate the standard deviations\n    std_devs = np.sqrt(np.diag(cov))\n\n    # Avoid division by zero\n    std_devs[std_devs == 0] = 1\n\n    # Create the correlation matrix\n    corr = cov / np.outer(std_devs, std_devs)\n\n    # Ensure the diagonal elements are exactly 1\n    np.fill_diagonal(corr, 1)\n\n    return corr, std_devs"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    import numpy as np\n    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n    \n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T, atol=1e-8):\n        raise ValueError(\"The matrix is not symmetric.\")\n    \n    # Check if the diagonal elements are close to zero\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0]), atol=1e-8):\n        raise ValueError(\"The diagonal elements are not close to zero.\")\n    \n    # If all checks pass, the matrix is a valid distance matrix\n    # No further action is required"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        is_initialized = func_hash in self.initialized_functions\n\n        # Determine if the model is suitable for distillation based on token count\n        token_count = len(str(args)) + len(str(kwargs)) + len(str(function_description))\n        suitable_for_distillation = token_count < llm_parameters.get('token_threshold', 1000)\n\n        # Select the model based on suitability for distillation\n        if suitable_for_distillation:\n            selected_model = 'distilled_model'\n        else:\n            selected_model = 'teacher_model'\n\n        # Construct the prompt based on function description and arguments\n        prompt = f\"Function: {function_description}\\nArguments: {args}\\nKeyword Arguments: {kwargs}\\n\"\n\n        # Update examples for fine-tuning if necessary\n        if not is_initialized and not suitable_for_distillation:\n            self.initialized_functions[func_hash] = {'examples': []}  # Initialize with empty examples\n            # Here you would add logic to update examples for fine-tuning\n\n        # Return the constructed prompt, selected model, suitability for distillation, and initialization status\n        return prompt, selected_model, suitable_for_distillation, is_initialized"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    from scipy.linalg import cholesky, eigh\n    import numpy as np\n    if higham:\n        # Higham & Nick (2002) algorithm\n        n = cov.shape[0]\n        A_k = cov\n        for _ in range(higham_max_iteration):\n            R_k = cholesky(A_k, lower=True)\n            S_k = A_k @ np.linalg.inv(R_k.T) @ np.linalg.inv(R_k)\n            A_k1 = (S_k + S_k.T) / 2\n            norm_diff = np.linalg.norm(A_k1 - A_k, 'fro')\n            A_k = A_k1\n            if norm_diff < 1e-10:\n                break\n        return A_k\n    else:\n        # Eigenvalue clipping method\n        eigvals, eigvecs = eigh(cov)\n        eigvals_clipped = np.clip(eigvals, a_min=0, a_max=None)\n        cov_nearest = eigvecs @ np.diag(eigvals_clipped) @ eigvecs.T\n        return (cov_nearest + cov_nearest.T) / 2  # Ensure symmetry"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    from typing import Iterator, List\n    import numpy as np\n    for arr in x:\n        if arr.size > 1:  # Check if the array has more than one element\n            midpoint = arr.size // 2  # Find the midpoint of the array\n            # Yield the two halves as a list of numpy arrays\n            yield [arr[:midpoint], arr[midpoint:]]"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:  # Check for NaN (NaN is the only value that is not equal to itself)\n        return \"NaN\"\n    \n    if percent:\n        x *= 100\n    \n    # Determine the number of decimal places based on the magnitude of x\n    if abs(x) < 0.01:\n        decimal_places = 4\n    elif abs(x) < 1:\n        decimal_places = 3\n    elif abs(x) < 10:\n        decimal_places = 2\n    elif abs(x) < 100:\n        decimal_places = 1\n    else:\n        decimal_places = 0\n    \n    formatted_number = f\"{x:.{decimal_places}f}\"\n    \n    if percent:\n        formatted_number += \"%\"\n    \n    return formatted_number"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    import numpy.typing as npt\n    import numpy as np\n    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"When {name} is a dictionary, assets_names must be provided.\")\n        array = np.full((n_assets,) if dim == 1 else (n_assets, n_assets), fill_value)\n        for i, asset_name in enumerate(assets_names):\n            array[i] = items.get(asset_name, fill_value)\n    else:\n        array = np.asarray(items)\n        if array.ndim != dim:\n            raise ValueError(f\"The dimension of {name} must be {dim}.\")\n        if dim == 1 and array.shape[0] != n_assets:\n            raise ValueError(f\"The shape of {name} must be ({n_assets},) for dim=1.\")\n        if dim == 2 and (array.shape[0] != n_assets or array.shape[1] != n_assets):\n            raise ValueError(f\"The shape of {name} must be ({n_assets}, {n_assets}) for dim=2.\")\n\n    return array"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "from pathlib import Path\nimport os\n\n    if data_home is None:\n        data_home = os.environ.get('SKFOLIO_DATA', '~/skfolio_data')\n    \n    data_home = Path(data_home).expanduser()\n    \n    if not data_home.exists():\n        data_home.mkdir(parents=True, exist_ok=True)\n    \n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    from pathlib import Path\n    import shutil\n    import os\n    if data_home is None:\n        data_home = Path.home() / 'skfolio_data'\n    else:\n        data_home = Path(data_home)\n\n    # Check if the directory exists\n    if data_home.exists() and data_home.is_dir():\n        # Remove all contents of the directory\n        for item in data_home.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)\n            else:\n                item.unlink()\n    else:\n        print(f\"The specified data home directory {data_home} does not exist.\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    from dataclasses import dataclass\n    from collections.abc import Mapping\n    if isinstance(obj, (str, bytes)):\n        res = (obj,)\n        schema = Reconstructor(type(obj), None)\n    elif isinstance(obj, (list, tuple)):\n        res = tuple(obj)\n        schema = Reconstructor(type(obj), None)\n    elif isinstance(obj, Mapping):\n        keys = tuple(obj.keys())\n        values = tuple(obj.values())\n        res = values\n        schema = Reconstructor(type(obj), keys)\n    else:\n        # Handle custom types like Instances, Boxes, ROIMasks here\n        # Assuming they have a to_flattened method for flattening\n        res = obj.to_flattened()\n        schema = Reconstructor(type(obj), None)\n\n    return res, schema"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    import re\n    import numpy.typing as npt\n    import numpy as np\n    group_names = np.array(groups)\n    n_groups = len(group_names)\n    n_equations = len(equations)\n    \n    # Initialize the matrices A and B\n    A = np.zeros((n_equations, n_groups))\n    B = np.zeros(n_equations)\n    \n    # Parse each equation\n    for i, eq in enumerate(equations):\n        # Split the equation into left and right parts\n        left_side, right_side = eq.split('<=')\n        \n        # Process the right side\n        B[i] = float(right_side.strip())\n        \n        # Process the left side\n        terms = re.findall(r'([+-]?[0-9]*\\.?[0-9]+)?\\s*\\*?\\s*([a-zA-Z]+)', left_side)\n        for term in terms:\n            coefficient, group_name = term\n            coefficient = float(coefficient) if coefficient else 1.0\n            \n            # Find the index of the group in the group_names\n            if group_name in group_names:\n                group_index = np.where(group_names == group_name)[0][0]\n                A[i, group_index] = coefficient\n            else:\n                if raise_if_group_missing:\n                    raise ValueError(f\"{names[0]} '{group_name}' not found in {names[1]}\")\n                else:\n                    print(f\"Warning: {names[0]} '{group_name}' not found in {names[1]}\")\n    \n    # If sum_to_one is True, add an additional constraint\n    if sum_to_one:\n        sum_row = np.ones(n_groups)\n        A = np.vstack([A, sum_row])\n        B = np.append(B, 1.0)\n    \n    return A, B"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    from detectron2.structures import Instances\n    import os\n    import importlib.util\n    import tempfile\n    import contextlib\n    try:\n        # Create the class definition string based on the fields provided\n        class_definition = \"class PatchedInstances:\\n\"\n        for field_name, field_type in fields:\n            class_definition += f\"    {field_name}: {field_type}\\n\"\n        \n        # Create a temporary file to store the new class definition\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as temp_file:\n            temp_file_name = temp_file.name\n            temp_file.write(class_definition)\n        \n        # Load the module from the temporary file\n        spec = importlib.util.spec_from_file_location(\"patched_instances_module\", temp_file_name)\n        patched_instances_module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(patched_instances_module)\n        \n        # Replace the 'Instances' class with the new class\n        original_instances = Instances\n        detectron2.structures.Instances = getattr(patched_instances_module, 'PatchedInstances')\n        \n        # Yield the new class to be used within the context\n        yield patched_instances_module.PatchedInstances\n        \n    finally:\n        # Clean up: Restore the original 'Instances' class and delete the temporary file\n        detectron2.structures.Instances = original_instances\n        os.remove(temp_file_name)"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    from contextlib import contextmanager\n    original_training_states = {}\n\n    try:\n        # Save the original training states and set training to False\n        for name, submodule in model.named_modules():\n            original_training_states[name] = submodule.training\n            submodule.training = False\n\n        # Yield control back to the caller, with the model in a frozen training state\n        yield\n    finally:\n        # Revert the training states of all submodules to their original values\n        for name, submodule in model.named_modules():\n            submodule.training = original_training_states.get(name, True)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "    if eps is None:\n        eps = xnp.finfo(xnp.float32).eps\n\n    # Ensure the input is within the valid range [0, 1]\n    srgb = xnp.clip(srgb, 0, 1)\n\n    # Apply the piecewise conversion function\n    linear_mask = srgb <= 0.04045\n    linear_low = srgb / 12.92\n    linear_high = ((srgb + 0.055) / 1.055) ** 2.4\n\n    # Combine the results based on the mask\n    linear = xnp.where(linear_mask, linear_low, linear_high)\n\n    return linear"}
{"namespace": "resample.resample_3d", "completion": "    import numpy as np\n    if half_pixel_center:\n        locations -= 0.5\n\n    # Handle coordinate order\n    if coordinate_order == 'zyx':\n        locations = locations[..., ::-1]\n\n    # Prepare output array\n    output_shape = list(locations.shape[:-1]) + [data.shape[-1]]\n    output = np.empty(output_shape, dtype=data.dtype)\n\n    # Define interpolation function\n    def interpolate(data, loc):\n        if method == 'TRILINEAR':\n            # Compute indices of the 8 surrounding voxels\n            x0 = np.floor(loc[0]).astype(int)\n            y0 = np.floor(loc[1]).astype(int)\n            z0 = np.floor(loc[2]).astype(int)\n            x1 = x0 + 1\n            y1 = y0 + 1\n            z1 = z0 + 1\n\n            # Compute weights for interpolation\n            xd = loc[0] - x0\n            yd = loc[1] - y0\n            zd = loc[2] - z0\n\n            # Interpolate along x, y, and z\n            c00 = data[x0, y0, z0] * (1 - xd) + data[x1, y0, z0] * xd\n            c01 = data[x0, y0, z1] * (1 - xd) + data[x1, y0, z1] * xd\n            c10 = data[x0, y1, z0] * (1 - xd) + data[x1, y1, z0] * xd\n            c11 = data[x0, y1, z1] * (1 - xd) + data[x1, y1, z1] * xd\n\n            c0 = c00 * (1 - yd) + c10 * yd\n            c1 = c01 * (1 - yd) + c11 * yd\n\n            c = c0 * (1 - zd) + c1 * zd\n            return c\n        elif method == 'NEAREST':\n            # Find nearest neighbor\n            x = np.round(loc[0]).astype(int)\n            y = np.round(loc[1]).astype(int)\n            z = np.round(loc[2]).astype(int)\n            return data[x, y, z]\n        else:\n            raise ValueError(\"Unknown interpolation method: {}\".format(method))\n\n    # Iterate over all locations\n    for index in np.ndindex(*locations.shape[:-1]):\n        loc = locations[index]\n        if edge_behavior == 'CONSTANT_OUTSIDE':\n            # Check if location is outside the data bounds\n            if (loc < 0).any() or (loc >= np.array(data.shape[:3])).any():\n                output[index] = constant_values\n            else:\n                output[index] = interpolate(data, loc)\n        elif edge_behavior == 'CLAMP':\n            # Clamp locations to the bounds of the data\n            loc = np.clip(loc, 0, np.array(data.shape[:3]) - 1)\n            output[index] = interpolate(data, loc)\n        else:\n            raise ValueError(\"Unknown edge behavior: {}\".format(edge_behavior))\n\n    return output"}
{"namespace": "linspline.integrate", "completion": "    # Check if the input arrays are of the same length\n    if len(t) != len(w):\n        raise ValueError(\"Input arrays t and w must have the same length.\")\n\n    # Initialize the integral value\n    integral = 0.0\n\n    # Iterate over the array, summing the areas of the trapezoids\n    for i in range(1, len(t)):\n        # Calculate the width of the trapezoid\n        delta_t = t[i] - t[i - 1]\n        # Calculate the average height of the trapezoid\n        avg_w = (w[i] + w[i - 1]) / 2.0\n        # Add the area of the trapezoid to the integral\n        integral += delta_t * avg_w\n\n    return integral"}
{"namespace": "linspline.query", "completion": "    # Ensure that the input arrays are sorted and have the same length\n    if len(t) != len(v) or any(t[i] >= t[i+1] for i in range(len(t)-1)):\n        raise ValueError(\"Time points 't' must be strictly increasing and have corresponding values 'v'.\")\n\n    # Initialize the result array with zeros\n    result = []\n\n    # Iterate over each query point\n    for tqi in tq:\n        # If the query point is outside the range of 't', set the result to 0\n        if tqi < t[0] or tqi > t[-1]:\n            result.append(0)\n        else:\n            # Find the interval in which the query point lies\n            for i in range(len(t) - 1):\n                if t[i] <= tqi <= t[i + 1]:\n                    # Perform linear interpolation\n                    slope = (v[i + 1] - v[i]) / (t[i + 1] - t[i])\n                    interpolated_value = v[i] + slope * (tqi - t[i])\n                    result.append(interpolated_value)\n                    break\n\n    return result"}
{"namespace": "geometry.are_lines_parallel", "completion": "    import numpy as np\n    norm_dir1 = dir1 / np.linalg.norm(dir1)\n    norm_dir2 = dir2 / np.linalg.norm(dir2)\n    \n    # Calculate the dot product of the normalized direction vectors\n    dot_product = np.dot(norm_dir1, norm_dir2)\n    \n    # Define a small epsilon for numerical precision issues\n    epsilon = 1e-6\n    \n    # Check if the absolute value of the dot product is close to 1 (parallel or anti-parallel)\n    return abs(abs(dot_product) - 1) < epsilon"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "x = r * sin(theta) * cos(phi)\ny = r * sin(theta) * sin(phi)\nz = r * cos(theta)"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "    import numpy as np\n    if v < 1:\n        raise ValueError(\"Tessellation factor must be greater than or equal to 1\")\n\n    # Initialize an empty list to store the barycentric weights\n    weights = []\n\n    # Loop through all the points in the tessellated triangle\n    for i in range(v + 1):\n        for j in range(v + 1 - i):\n            # The third weight is determined by the fact that they must sum up to 1\n            k = v - i - j\n            # Append the normalized weights for this point\n            weights.append((i / v, j / v, k / v))\n\n    # Convert the list of weights to a numpy array\n    return np.array(weights)"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "    import numpy as onp\n    x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n    \n    # Calculate the radius\n    r = onp.sqrt(x**2 + y**2 + z**2)\n    \n    # Calculate the inclination (theta)\n    # We add eps to the denominator to prevent division by zero\n    theta = onp.arccos(onp.clip(z / (r + eps), -1.0, 1.0))\n    \n    # Calculate the azimuth (phi)\n    phi = onp.arctan2(y, x)\n    \n    return r, theta, phi"}
{"namespace": "geopoly.compute_sq_dist", "completion": "    import numpy as np\n    if mat1 is None:\n        mat1 = mat0\n\n    # Compute the squared norms of each column in both matrices\n    norm_mat0 = np.sum(mat0**2, axis=0, keepdims=True)\n    norm_mat1 = np.sum(mat1**2, axis=0, keepdims=True)\n\n    # Compute the dot product between all pairs of columns\n    dot_product = np.dot(mat0.T, mat1)\n\n    # Compute the squared distances using the formula\n    sq_dists = norm_mat0.T + norm_mat1 - 2 * dot_product\n\n    # Ensure numerical stability by setting negative distances to zero\n    sq_dists[sq_dists < 0] = 0\n\n    return sq_dists"}
{"namespace": "math.plus_eps", "completion": "    import math\n    # Define a tiny threshold value\n    tiny_threshold = 1e-15\n    \n    # Check if x is close to zero and adjust if necessary\n    if abs(x) < tiny_threshold:\n        return tiny_threshold if x >= 0 else -tiny_threshold\n    else:\n        # Calculate the next representable floating-point value towards positive infinity\n        return math.nextafter(x, math.inf)"}
{"namespace": "math.minus_eps", "completion": "    import math\n    tiny_val = 1e-10  # Define a very small value\n    if x <= tiny_val:\n        return -tiny_val\n    else:\n        # Use math.nextafter to find the next smallest floating-point number towards negative infinity\n        return math.nextafter(x, -math.inf)"}
{"namespace": "math.safe_exp", "completion": "    import numpy as np\n    # Define a maximum value for x to prevent overflow in exp\n    MAX_EXP_INPUT = 709  # np.exp(709) is the largest number before overflow for float64\n\n    # Clip the input x to be within the range [-MAX_EXP_INPUT, MAX_EXP_INPUT]\n    safe_x = np.clip(x, -MAX_EXP_INPUT, MAX_EXP_INPUT)\n\n    # Compute the exponential of the clipped input\n    result = np.exp(safe_x)\n\n    return result"}
{"namespace": "math.safe_log", "completion": "  import jax.numpy as jnp\n  # Ensure that x is a JAX array for compatibility with JAX operations\n  x = jnp.array(x)\n\n  # Define a small epsilon value to avoid log(0) which is undefined\n  epsilon = 1e-10\n\n  # Use jnp.maximum to ensure that x is always greater than epsilon\n  # This avoids the issue of taking the log of non-positive numbers\n  safe_x = jnp.maximum(x, epsilon)\n\n  # Compute the logarithm using the safe version of x\n  return jnp.log(safe_x)"}
{"namespace": "math.safe_sqrt", "completion": "    import numpy as np\n    # Ensure x is not negative and does not exceed the maximum value\n    clamped_x = np.clip(x, 0, max_value)\n    \n    # Compute the square root of the clamped value\n    result = np.sqrt(clamped_x)\n    \n    return result"}
{"namespace": "math.power_ladder_max_output", "completion": "    if p > 1:\n        # The function grows without bound as x approaches infinity.\n        return float('inf')\n    elif p == 1:\n        # The function grows linearly, and its limit as x approaches infinity is also infinity.\n        return float('inf')\n    elif 0 < p < 1:\n        # The function grows, but it will approach a finite limit.\n        # Since we don't have the specific function, we cannot determine the exact limit.\n        # We return 'finite' to indicate that the limit is not infinity.\n        return 'finite'\n    elif p == 0:\n        # The function is constant.\n        # We return the constant value, which we'll assume to be 1 for this example.\n        return 1\n    else:  # p < 0\n        # The function approaches 0 as x approaches infinity.\n        return 0"}
{"namespace": "geopoly.generate_basis", "completion": "    from scipy.spatial import ConvexHull\n    import numpy as np\n    if base_shape == 'tetrahedron':\n        vertices = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]]) / np.sqrt(3)\n    elif base_shape == 'icosahedron':\n        phi = (1 + np.sqrt(5)) / 2\n        vertices = np.array([\n            [-1, phi, 0], [1, phi, 0], [-1, -phi, 0], [1, -phi, 0],\n            [0, -1, phi], [0, 1, phi], [0, -1, -phi], [0, 1, -phi],\n            [phi, 0, -1], [phi, 0, 1], [-phi, 0, -1], [-phi, 0, 1]\n        ]) / np.sqrt(1 + phi**2)\n    elif base_shape == 'octahedron':\n        vertices = np.array([\n            [1, 0, 0], [-1, 0, 0], [0, 1, 0],\n            [0, -1, 0], [0, 0, 1], [0, 0, -1]\n        ])\n    else:\n        raise ValueError(\"Invalid base_shape. Must be 'tetrahedron', 'icosahedron', or 'octahedron'.\")\n\n    # Tessellate the polyhedron\n    for _ in range(angular_tesselation - 1):\n        hull = ConvexHull(vertices)\n        new_vertices = []\n        for simplex in hull.simplices:\n            for i in range(len(simplex)):\n                for j in range(i + 1, len(simplex)):\n                    midpoint = (vertices[simplex[i]] + vertices[simplex[j]]) / 2\n                    midpoint /= np.linalg.norm(midpoint)\n                    new_vertices.append(midpoint)\n        vertices = np.vstack({tuple(row) for row in np.vstack((vertices, new_vertices))})\n\n    # Optionally remove symmetric basis columns\n    if remove_symmetries:\n        unique_vertices = []\n        for v in vertices:\n            if not any(np.all(np.abs(v + u) < eps) for u in unique_vertices):\n                unique_vertices.append(v)\n        vertices = np.array(unique_vertices)\n\n    # Return the basis matrix\n    return vertices.T"}
{"namespace": "math.safe_log1p", "completion": "    import numpy as np\n    # Ensure x is a numpy array for element-wise operations\n    x = np.asarray(x)\n    \n    # Define a safe lower bound (slightly greater than -1 to avoid log(0))\n    safe_lower_bound = -1 + np.finfo(float).eps\n    \n    # Define a safe upper bound (large positive value to avoid overflow)\n    safe_upper_bound = np.log(np.finfo(float).max)\n    \n    # Clip x to be within the safe bounds\n    safe_x = np.clip(x, safe_lower_bound, safe_upper_bound)\n    \n    # Compute the natural logarithm of 1 plus the safe input value\n    result = np.log1p(safe_x)\n    \n    return result"}
{"namespace": "math.power_ladder", "completion": "    import numpy as np\n    if premult is not None:\n        x = np.multiply(x, premult)\n    \n    # Apply the transformation based on the value of p\n    if p == 0:\n        # Logarithmic transformation\n        transformed_x = np.log(x)\n    elif p == 1:\n        # No transformation needed\n        transformed_x = x\n    elif p == -np.inf:\n        # Reciprocal transformation\n        transformed_x = 1 / x\n    elif p == np.inf:\n        # Exponential transformation\n        transformed_x = np.exp(x)\n    else:\n        # Power transformation\n        transformed_x = np.power(x, p)\n    \n    # Apply post-multiplication if specified\n    if postmult is not None:\n        transformed_x = np.multiply(transformed_x, postmult)\n    \n    return transformed_x"}
{"namespace": "math.inv_power_ladder", "completion": "    import numpy as np\n    if postmult is not None:\n        y = y * postmult\n\n    # Apply the inverse power transformation based on the value of p\n    if p == 0:\n        result = np.exp(y)\n    else:\n        result = np.power(y, 1/p)\n\n    # Apply pre-multiplication if provided\n    if premult is not None:\n        result = result * premult\n\n    return result"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "    import numpy as np\n    origins_homogeneous = xnp.concatenate([origins, xnp.ones((origins.shape[0], 1), dtype=origins.dtype)], axis=-1)\n    directions_homogeneous = xnp.concatenate([directions, xnp.zeros((directions.shape[0], 1), dtype=directions.dtype)], axis=-1)\n    \n    # Apply the pixtocam transformation\n    origins_cam = xnp.dot(origins_homogeneous, pixtocam.T)[:, :3]\n    directions_cam = xnp.dot(directions_homogeneous, pixtocam.T)[:, :3]\n    \n    # Step 2: Adjust ray origins to the near plane\n    t = -near / directions_cam[:, 2]\n    origins_ndc = origins_cam + directions_cam * t[:, None]\n    \n    # Step 3: Calculate the corresponding directions in NDC\n    # Since we're assuming an identity extrinsic matrix and the pixtocam already\n    # includes the intrinsic parameters, we can directly use the directions in camera space\n    directions_ndc = directions_cam\n    \n    # Step 4: Return the transformed origins and directions\n    return origins_ndc, directions_ndc"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "    import numpy as np\n    from scipy.interpolate import UnivariateSpline\n    # Ensure that the inputs are numpy arrays\n    x = np.asarray(x)\n    t_input = np.asarray(t_input)\n    t_output = np.asarray(t_output)\n\n    # Adjust the spline degree if necessary\n    n_points = len(x)\n    spline_degree = min(spline_degree, n_points - 1)\n\n    # Create the spline object\n    spline = UnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n\n    # Evaluate the spline at the output times\n    interpolated_values = spline(t_output)\n\n    return interpolated_values"}
{"namespace": "math.learning_rate_decay", "completion": "import numpy as np\n\n    if step < lr_delay_steps:\n        # Apply the delay multiplier to the initial learning rate\n        lr = lr_init * lr_delay_mult\n    else:\n        # Calculate the progress as a fraction between 0 and 1\n        progress = (step - lr_delay_steps) / float(max_steps - lr_delay_steps)\n        # Calculate the learning rate with log-linear interpolation (exponential decay)\n        lr = lr_final + (lr_init - lr_final) * np.exp(-progress)\n    \n    return lr"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "import numpy as np\n\n  # Create the intrinsic matrix using the provided focal lengths and optical center coordinates\n  intrinsic_matrix = xnp.array([\n    [fx,  0, cx],\n    [ 0, fy, cy],\n    [ 0,  0,  1]\n  ])\n\n  return intrinsic_matrix"}
{"namespace": "utils.dummy_rays", "completion": "    import random\n    # Call the generate_random_rays function with the specified parameters\n    return generate_random_rays(include_exposure_idx, include_exposure_values, include_device_idx)"}
{"namespace": "camera_utils.points_to_pixels", "completion": "    from enum import Enum\n    if camtype != ProjectionType.PERSPECTIVE:\n        raise NotImplementedError(\"Only perspective projection is currently supported.\")\n\n    # Transform points from world coordinates to camera coordinates\n    num_points = points.shape[0]\n    points_homogeneous = xnp.concatenate([points, xnp.ones((num_points, 1))], axis=-1)\n    cam_coords = xnp.dot(points_homogeneous, xnp.transpose(camtoworlds, (0, 2, 1)))\n\n    # Extract the depth before applying perspective division\n    depth = cam_coords[..., 2]\n\n    # Apply perspective division to get normalized device coordinates\n    ndc = cam_coords[..., :2] / cam_coords[..., 2:]\n\n    # Apply camera intrinsics to get pixel coordinates\n    ndc_homogeneous = xnp.concatenate([ndc, xnp.ones((num_points, 1))], axis=-1)\n    pixels_homogeneous = xnp.dot(ndc_homogeneous, xnp.transpose(pixtocams, (0, 2, 1)))\n    pixels = pixels_homogeneous[..., :2]\n\n    # Optionally apply distortion correction\n    if distortion_params is not None:\n        # Implement distortion correction here\n        # This will depend on the specific distortion model and parameters provided\n        raise NotImplementedError(\"Distortion correction is not implemented in this example.\")\n\n    return pixels, depth"}
{"namespace": "rigid_body.exp_se3", "completion": "    from scipy.linalg import expm, norm\n    import numpy as np\n    w = screw_axis[:3]\n    v = screw_axis[3:]\n    w_hat = np.array([[0, -w[2], w[1]],\n                      [w[2], 0, -w[0]],\n                      [-w[1], w[0], 0]])\n    w_norm = norm(w)\n    \n    if w_norm < eps:\n        # If the rotation part is too small, use the approximation for the rotation matrix\n        R = np.eye(3) + w_hat * theta\n        p = v * theta\n    else:\n        # Compute the rotation matrix using Rodrigues' formula\n        R = np.eye(3) + (np.sin(theta * w_norm) / w_norm) * w_hat + ((1 - np.cos(theta * w_norm)) / (w_norm ** 2)) * np.dot(w_hat, w_hat)\n        p = (np.eye(3) * theta + (1 - np.cos(theta * w_norm)) / (w_norm ** 2) * w_hat + (theta - np.sin(theta * w_norm)) / (w_norm ** 3) * np.dot(w_hat, w_hat)).dot(v)\n    \n    # Construct the homogeneous transformation matrix\n    T = np.eye(4)\n    T[:3, :3] = R\n    T[:3, 3] = p\n    \n    return T"}
{"namespace": "rigid_body.exp_so3", "completion": "    import numpy as np\n    # Calculate the angle of rotation\n    angle = np.linalg.norm(axis_angle)\n    \n    # Normalize the axis of rotation\n    if angle > eps:\n        axis = axis_angle / angle\n    else:\n        # If the angle is too small, return the identity matrix\n        return np.eye(3)\n    \n    # Compute the skew-symmetric matrix of the axis\n    axis_skew = np.array([\n        [0, -axis[2], axis[1]],\n        [axis[2], 0, -axis[0]],\n        [-axis[1], axis[0], 0]\n    ])\n    \n    # Use Rodrigues' formula to compute the rotation matrix\n    rotation_matrix = np.eye(3) + np.sin(angle) * axis_skew + (1 - np.cos(angle)) * np.dot(axis_skew, axis_skew)\n    \n    return rotation_matrix"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "    import numpy as np\n    d = d / np.linalg.norm(d)\n    \n    # Calculate the mean (center) of the frustum\n    mean = d * (t0 + t1) / 2\n    \n    # Calculate the radii at the start and end of the frustum\n    r0 = base_radius * t0\n    r1 = base_radius * t1\n    \n    # Calculate the average radius for the covariance\n    avg_radius = (r0 + r1) / 2\n    \n    # Calculate the height of the frustum\n    height = t1 - t0\n    \n    # Calculate the covariance matrix\n    if diag:\n        # For a diagonal covariance, we only consider the variance along the main axes\n        # Assuming the axis of the cone is aligned with one of the coordinate axes\n        variance_along_d = (height / 2) ** 2\n        variance_perpendicular_to_d = avg_radius ** 2\n        covariance = np.diag([variance_perpendicular_to_d, variance_perpendicular_to_d, variance_along_d])\n    else:\n        # For a full covariance, we need to consider the orientation of the frustum\n        # We construct a rotation matrix that aligns the z-axis with the direction vector d\n        z_axis = np.array([0, 0, 1])\n        axis = np.cross(z_axis, d)\n        angle = np.arccos(np.dot(z_axis, d))\n        rotation_matrix = rotation_matrix_from_axis_angle(axis, angle)\n        \n        # Construct the covariance in the aligned frame\n        aligned_covariance = np.diag([avg_radius ** 2, avg_radius ** 2, (height / 2) ** 2])\n        \n        # Rotate the covariance back to the original frame\n        covariance = rotation_matrix @ aligned_covariance @ rotation_matrix.T\n    \n    return mean, covariance"}
{"namespace": "render.cylinder_to_gaussian", "completion": "    import numpy as np\n    d = d / np.linalg.norm(d)\n    \n    # Calculate the mean of the Gaussian\n    mean = d * (t0 + t1) / 2.0\n    \n    # Calculate the variance along the axis of the cylinder\n    variance_along_axis = ((t1 - t0) ** 2) / 12.0\n    \n    # Calculate the variance perpendicular to the axis of the cylinder\n    variance_perpendicular = radius ** 2 / 2.0\n    \n    if diag:\n        # For a diagonal covariance, we only consider variances along the principal axes\n        covariance = np.diag([variance_perpendicular, variance_perpendicular, variance_along_axis])\n    else:\n        # For a full covariance, we need to construct the covariance matrix\n        # that accounts for the orientation of the cylinder\n        # First, create a basis where d is the last vector\n        U, _, _ = np.linalg.svd(np.eye(3) - np.outer(d, d))\n        basis = np.column_stack((U, d))\n        \n        # Create the covariance in the local basis\n        local_covariance = np.diag([variance_perpendicular, variance_perpendicular, variance_along_axis])\n        \n        # Rotate the covariance to the global basis\n        covariance = basis @ local_covariance @ basis.T\n    \n    # Lift the Gaussian from 2D to 3D if necessary\n    mean, covariance = lift_gaussian(mean, covariance, diag)\n    \n    return mean, covariance"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "    from enum import Enum\n    import numpy as np\n    pix_x = xnp.array(pix_x_int, dtype=xnp.float32)\n    pix_y = xnp.array(pix_y_int, dtype=xnp.float32)\n\n    # Convert pixel coordinates to camera coordinates\n    pix_coords = xnp.stack([pix_x, pix_y, xnp.ones_like(pix_x)], axis=-1)\n    cam_coords = xnp.einsum('...ij,...j->...i', pixtocams, pix_coords)\n\n    # Apply distortion correction if necessary\n    if distortion_params is not None:\n        # Implement distortion correction based on the provided distortion parameters\n        pass  # Placeholder for actual distortion correction implementation\n\n    # Compute ray directions based on camera type\n    if camtype == ProjectionType.PERSPECTIVE:\n        ray_dirs = cam_coords\n    elif camtype == ProjectionType.FISHEYE:\n        # Implement fisheye projection ray direction computation\n        pass  # Placeholder for fisheye projection implementation\n    else:\n        raise ValueError(\"Unsupported camera projection type\")\n\n    # Normalize ray directions to get view directions\n    viewdirs = xnp.linalg.norm(ray_dirs, axis=-1, keepdims=True)\n    ray_dirs = ray_dirs / viewdirs\n\n    # Compute ray origins in world coordinates\n    origins = camtoworlds[..., :3, 3]\n\n    # Transform ray directions to world coordinates\n    directions = xnp.einsum('...ij,...j->...i', camtoworlds[..., :3, :3], ray_dirs)\n\n    # Compute differential radii (if necessary, e.g., for mip-NeRF)\n    radii = xnp.ones_like(viewdirs)  # Placeholder for actual computation\n\n    # Compute image plane coordinates (if necessary)\n    if pixtocam_ndc is not None:\n        imageplane = xnp.einsum('ij,...j->...i', pixtocam_ndc, cam_coords)\n    else:\n        imageplane = cam_coords[..., :2]  # Use xy coordinates of cam_coords\n\n    return origins, directions, viewdirs, radii, imageplane"}
{"namespace": "stepfun.weight_to_pdf", "completion": "    # Check if the input vectors are valid\n    if len(t) != len(w) + 1:\n        raise ValueError(\"The length of t should be one more than the length of w.\")\n    if not 0.9999 < sum(w) < 1.0001:\n        raise ValueError(\"The weights should sum to 1.\")\n    \n    # Calculate the differences between consecutive elements in t\n    dt = [t[i+1] - t[i] for i in range(len(t) - 1)]\n    \n    # Calculate the PDF by dividing each weight by the corresponding difference in t\n    pdf = [w[i] / dt[i] for i in range(len(w))]\n    \n    return pdf"}
{"namespace": "render.compute_alpha_weights", "completion": "import numpy as np\n\n    # Calculate the norm of each direction vector\n    norms = np.linalg.norm(dirs, axis=1)\n    \n    # Adjust the distances by the norms of the direction vectors\n    adjusted_distances = tdist / norms\n    \n    # Compute the product of density and adjusted distances\n    product = density * adjusted_distances\n    \n    # Call the helper function to compute the alpha weights\n    alpha_weights = calculate_alpha(product, **kwargs)\n    \n    return alpha_weights"}
{"namespace": "stepfun.pdf_to_weight", "completion": "    import numpy as np\n    # Calculate the differences between consecutive elements in 't'\n    dt = np.diff(t)\n    \n    # Multiply the PDF values by the differences to get the weights\n    # Since 'dt' has one less element than 'p', we need to use slicing to align their sizes\n    weights = p[:-1] * dt\n    \n    # Normalize the weights so that they sum to 1\n    weights /= np.sum(weights)\n    \n    return weights"}
{"namespace": "stepfun.sample", "completion": "    from jax import random\n    import jax.numpy as jnp\n    w = jnp.exp(w_logits - jnp.max(w_logits))\n    w = w / jnp.sum(w)\n\n    # Compute the cumulative distribution function (CDF)\n    cdf = jnp.cumsum(w)\n\n    if rng is not None:\n        # Random sampling\n        if single_jitter:\n            # Single jitter for all samples\n            u = random.uniform(rng, shape=(1,), minval=eps, maxval=1.0 - eps)\n            u = jnp.tile(u, num_samples)\n        else:\n            # Independent jitter for each sample\n            u = random.uniform(rng, shape=(num_samples,), minval=eps, maxval=1.0 - eps)\n        \n        # Find the bins each sample falls into\n        bins = jnp.digitize(u, cdf)\n        \n        # Sample uniformly within each bin\n        t_min = jnp.take(t, bins - 1)\n        t_max = jnp.take(t, bins)\n        samples = t_min + (t_max - t_min) * (u - jnp.take(cdf, bins - 1, mode='clip')) / jnp.take(w, bins - 1)\n    else:\n        # Deterministic sampling\n        if deterministic_center:\n            # Center samples in each interval\n            intervals = (t[1:] + t[:-1]) / 2\n            samples = jnp.linspace(intervals[0], intervals[-1], num_samples)\n        else:\n            # Span the entire range\n            samples = jnp.linspace(t[0], t[-1], num_samples)\n\n    return samples"}
{"namespace": "stepfun.sample_intervals", "completion": "    from jax.scipy.special import logsumexp\n    from jax import random\n    import jax.numpy as jnp\n    w_probs = jnp.exp(w_logits - logsumexp(w_logits))\n\n    # Compute the cumulative distribution function (CDF) from the probabilities\n    cdf = jnp.cumsum(w_probs)\n\n    # Sample uniform random values\n    if rng is not None:\n        if single_jitter:\n            # Jitter every sample by the same amount\n            u = random.uniform(rng, shape=(1,))\n            u = jnp.tile(u, num_samples)\n        else:\n            # Jitter each sample independently\n            u = random.uniform(rng, shape=(num_samples,))\n    else:\n        # Use linspace sampling if rng is None\n        u = jnp.linspace(0, 1, num_samples + 2)[1:-1]\n\n    # Find the bins each sample falls into using the CDF\n    bins = jnp.digitize(u, cdf)\n\n    # Calculate the sampled points by interpolating within the bins\n    t_min = t[bins - 1]\n    t_max = t[bins]\n    bin_widths = cdf[bins] - cdf[bins - 1]\n    bin_positions = (u - cdf[bins - 1]) / bin_widths\n    samples = t_min + bin_positions * (t_max - t_min)\n\n    # Calculate midpoints between adjacent samples\n    midpoints = (samples[:-1] + samples[1:]) / 2\n\n    # Adjust the first and last intervals to fit within the domain\n    first_interval = jnp.maximum(domain[0], samples[0] - (midpoints[0] - samples[0]))\n    last_interval = jnp.minimum(domain[1], samples[-1] + (samples[-1] - midpoints[-1]))\n    intervals = jnp.concatenate(([first_interval], midpoints, [last_interval]))\n\n    return intervals"}
{"namespace": "stepfun.weighted_percentile", "completion": "    import numpy as np\n    w = np.array(w, dtype=float)\n    w /= np.sum(w)\n    \n    # Compute the cumulative sum of weights\n    cum_weights = np.cumsum(w)\n    \n    # The cumulative weights are the y-values, and we want to find the corresponding x-values for the given percentiles\n    # The first value in the cumulative weights should be 0 for interpolation\n    cum_weights = np.insert(cum_weights, 0, 0)\n    t = np.insert(t, 0, t[0])\n    \n    # Interpolate the percentiles\n    percentiles = np.interp(np.array(ps) / 100.0, cum_weights, t)\n    \n    return percentiles"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "    from scipy.ndimage import gaussian_filter1d\n    import numpy as np\n    total_weight = np.sum(w)\n    if total_weight > 0:\n        pdf = w / total_weight\n    else:\n        raise ValueError(\"Total weight must be positive to create a valid PDF.\")\n\n    # Step 2: Blur the PDF using a Gaussian filter\n    # Calculate the standard deviation for the Gaussian kernel\n    # Assuming that the time points are evenly spaced for simplicity\n    if len(t) > 1:\n        dt = t[1] - t[0]\n        sigma = blur_halfwidth / dt\n    else:\n        raise ValueError(\"At least two time points are required for blurring.\")\n    \n    blurred_pdf = gaussian_filter1d(pdf, sigma)\n\n    # Step 3: Resample the blurred PDF at the new time points `tq`\n    # Interpolate the blurred PDF to the query points\n    resampled_pdf = np.interp(tq, t, blurred_pdf)\n\n    # Convert the resampled PDF back to weights by multiplying by the total weight\n    resampled_weights = resampled_pdf * total_weight\n\n    return resampled_weights"}
{"namespace": "spin_math.safe_sqrt", "completion": "  import jax.numpy as jnp\n  if x < eps:\n    return jnp.sqrt(value_at_zero)\n  else:\n    return jnp.sqrt(x)"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "    import numpy as np\n    # Ensure vectors are in homogeneous coordinates\n    num_points = vectors.shape[:-1]\n    homogeneous_vectors = np.ones(num_points + (4,))\n    homogeneous_vectors[..., :3] = vectors\n    \n    # Perform the transformation\n    transformed_homogeneous_vectors = np.dot(homogeneous_vectors, transform.T)\n    \n    # Convert back to original dimensionality\n    transformed_vectors = transformed_homogeneous_vectors[..., :3] / transformed_homogeneous_vectors[..., 3:4]\n    \n    return transformed_vectors"}
{"namespace": "spin_math.safe_log", "completion": "  import jax.numpy as jnp\n  # Clamp the values at or near zero to value_at_zero\n  safe_x = jnp.where(x > eps, x, value_at_zero)\n  # Compute the logarithm of the clamped values\n  return jnp.log(safe_x)"}
{"namespace": "stepfun.resample", "completion": "    import numpy as np\n    resampled_values = np.zeros_like(t[:-1])\n    \n    # Iterate over the intervals defined by t\n    for i in range(len(t) - 1):\n        # Find the indices of the original time points that fall within the current interval\n        indices = np.where((tp >= t[i]) & (tp < t[i+1]))[0]\n        \n        # If there are no original time points in the interval, continue to the next interval\n        if len(indices) == 0:\n            continue\n        \n        # Calculate the sum of the values for the current interval\n        interval_sum = np.sum(vp[indices])\n        \n        # If averaging is requested, divide the sum by the number of points to get the average\n        if use_avg:\n            # Calculate the width of the interval in the original time points\n            interval_width = np.sum(np.diff(tp[indices], prepend=t[i], append=t[i+1]))\n            # Calculate the average value for the interval\n            resampled_values[i] = interval_sum / interval_width\n        else:\n            # If not averaging, just assign the sum to the resampled values\n            resampled_values[i] = interval_sum\n    \n    return resampled_values"}
{"namespace": "coord.contract", "completion": "    import numpy as np\n    # Calculate the magnitude squared of the points\n    magnitude_squared = np.sum(x**2, axis=1)\n    \n    # Initialize the scaling factor array\n    scaling_factor = np.ones_like(magnitude_squared)\n    \n    # Apply different scaling factors based on the magnitude\n    scaling_factor[magnitude_squared <= 1] = 0.5  # Scale by 0.5 if within unit distance\n    scaling_factor[magnitude_squared > 1] = 0.8   # Scale by 0.8 if outside unit distance\n    \n    # Reshape scaling_factor to be able to multiply with x\n    scaling_factor = scaling_factor.reshape(-1, 1)\n    \n    # Scale the points towards the origin\n    contracted_x = x * scaling_factor\n    \n    return contracted_x"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n  # Assuming the contract function scales the input by 0.5, the inverse would scale it by 2.\n  inverse_factor = 2\n  return np.array(z) * inverse_factor"}
{"namespace": "grid_utils.trilerp", "completion": "    import numpy as np\n    if datastructure == 'grid':\n        # Get the dimensions of the grid\n        D, H, W, C = values.shape\n        \n        # Floor the coordinates to get the base indices\n        base_coords = np.floor(coordinates).astype(int)\n        \n        # Calculate the fractional part for interpolation\n        fractional_coords = coordinates - base_coords\n        \n        # Ensure the base indices are within the valid range\n        base_coords = np.clip(base_coords, 0, np.array([D - 2, H - 2, W - 2]))\n        \n        # Calculate the indices of the 8 neighboring corners\n        i0, j0, k0 = base_coords.T\n        i1 = i0 + 1\n        j1 = j0 + 1\n        k1 = k0 + 1\n        \n        # Gather the values at the 8 corners\n        c000 = values[i0, j0, k0]\n        c100 = values[i1, j0, k0]\n        c010 = values[i0, j1, k0]\n        c110 = values[i1, j1, k0]\n        c001 = values[i0, j0, k1]\n        c101 = values[i1, j0, k1]\n        c011 = values[i0, j1, k1]\n        c111 = values[i1, j1, k1]\n        \n        # Interpolate along the x-axis\n        c00 = c000 * (1 - fractional_coords[:, 0])[:, None] + c100 * fractional_coords[:, 0][:, None]\n        c01 = c001 * (1 - fractional_coords[:, 0])[:, None] + c101 * fractional_coords[:, 0][:, None]\n        c10 = c010 * (1 - fractional_coords[:, 0])[:, None] + c110 * fractional_coords[:, 0][:, None]\n        c11 = c011 * (1 - fractional_coords[:, 0])[:, None] + c111 * fractional_coords[:, 0][:, None]\n        \n        # Interpolate along the y-axis\n        c0 = c00 * (1 - fractional_coords[:, 1])[:, None] + c10 * fractional_coords[:, 1][:, None]\n        c1 = c01 * (1 - fractional_coords[:, 1])[:, None] + c11 * fractional_coords[:, 1][:, None]\n        \n        # Interpolate along the z-axis and return the result\n        interpolated_values = c0 * (1 - fractional_coords[:, 2])[:, None] + c1 * fractional_coords[:, 2][:, None]\n        return interpolated_values\n\n    elif datastructure == 'hash':\n        # For a hash data structure, we assume that the values are indexed by their integer coordinates\n        # Convert coordinates to integer indices\n        indices = np.round(coordinates).astype(int)\n        \n        # Ensure the indices are within the valid range\n        indices = np.clip(indices, 0, values.shape[0] - 1)\n        \n        # Fetch the values from the hash data structure\n        interpolated_values = values[indices[:, 0], indices[:, 1], indices[:, 2]]\n        return interpolated_values\n\n    else:\n        raise ValueError(\"Invalid datastructure type. Only 'grid' or 'hash' are supported.\")"}
{"namespace": "coord.track_linearize", "completion": "    import numpy as np\n    \n    # Calculate the Jacobian matrix of fn at mean\n    # For simplicity, we assume fn is a function that can be differentiated\n    # and that we can compute the Jacobian analytically or numerically\n    # Here we use a numerical approximation for the Jacobian\n    epsilon = 1e-5\n    dim = mean.shape[-1]\n    jacobian = np.zeros((dim, dim))\n    for i in range(dim):\n        delta = np.zeros(dim)\n        delta[i] = epsilon\n        jacobian[:, i] = (fn(mean + delta) - fn(mean - delta)) / (2 * epsilon)\n    \n    # Transform the mean\n    fn_mean = fn(mean)\n    \n    # Transform the covariance\n    fn_cov = jacobian @ cov @ jacobian.T\n    \n    return fn_mean, fn_cov"}
{"namespace": "coord.contract3_isoscale", "completion": "    import numpy as np\n    # Calculate the norm of the input array\n    norm = np.linalg.norm(x)\n    \n    # Check if the norm is zero to avoid division by zero\n    if norm == 0:\n        return x\n    \n    # Scale the array to make its norm equal to 1\n    scaled_x = x / norm\n    \n    return scaled_x"}
{"namespace": "coord.pos_enc", "completion": "    import numpy as np\n    scales = 2 ** np.arange(min_deg, max_deg)\n    features = []\n    for scale in scales:\n        features.append(np.sin(scale * x))\n        features.append(np.cos(scale * x))\n    encoded = np.concatenate(features, axis=-1)\n    if append_identity:\n        encoded = np.concatenate((x, encoded), axis=-1)\n    return encoded"}
{"namespace": "coord.integrated_pos_enc", "completion": "    import jax.numpy as jnp\n    scales = jnp.array([2**i for i in range(min_deg, max_deg)])\n    scales = scales.reshape((len(scales), 1))\n\n    # Scale the mean and variance\n    scaled_mean = mean * scales\n    scaled_var = var * scales\n\n    # Apply sinusoidal encoding\n    encoded_mean = jnp.concatenate([jnp.sin(scaled_mean), jnp.cos(scaled_mean)], axis=-1)\n    encoded_var = jnp.concatenate([jnp.sin(scaled_var), jnp.cos(scaled_var)], axis=-1)\n\n    # Concatenate the encoded mean and variance\n    encoded = jnp.concatenate([encoded_mean, encoded_var], axis=-1)\n\n    return encoded"}
{"namespace": "coord.isotropize", "completion": "    import numpy as np\n    if cov.ndim == 2:\n        cov = np.atleast_3d(cov)\n    \n    isotropic_covs = np.zeros_like(cov)\n    for i, matrix in enumerate(cov):\n        det = np.linalg.det(matrix)\n        if det <= 0:\n            raise ValueError(\"Covariance matrix must have a positive determinant.\")\n        \n        n = matrix.shape[0]\n        if mode == 'fast':\n            factor = det ** (1.0 / n)\n        elif mode == 'accurate':\n            log_det = np.log(det) / n\n            factor = np.exp(log_det)\n        else:\n            raise ValueError(\"Mode must be either 'fast' or 'accurate'.\")\n        \n        isotropic_covs[i] = np.eye(n) * factor\n    \n    if isotropic_covs.shape[0] == 1:\n        return isotropic_covs[0]\n    else:\n        return isotropic_covs"}
{"namespace": "coord.construct_ray_warps", "completion": "\n    # Define the forward mapping function\n    def t_to_s(t):\n        # Clip the input distances to the valid range [t_near, t_far]\n        t_clipped = max(min(t, t_far), t_near)\n        # Apply the transformation function to the clipped distances\n        return (fn(t_clipped) - fn(t_near)) / (fn(t_far) - fn(t_near))\n\n    # If the inverse function is not provided, we need to find it or raise an error\n    if fn_inv is None:\n        raise ValueError(\"Inverse function `fn_inv` must be provided if not automatically determinable.\")\n\n    # Define the backward mapping function\n    def s_to_t(s):\n        # Clip the input normalized distances to the range [0, 1]\n        s_clipped = max(min(s, 1), 0)\n        # Apply the inverse transformation function to the clipped distances\n        return fn_inv(s_clipped * (fn(t_far) - fn(t_near)) + fn(t_near))\n\n    return t_to_s, s_to_t"}
{"namespace": "ref_utils.l2_normalize", "completion": "    import jax.numpy as jnp\n    # Calculate the L2 norm squared along the last axis and clamp it to grad_eps\n    squared_norm = jnp.maximum(jnp.sum(x**2, axis=-1, keepdims=True), grad_eps)\n    \n    # Calculate the L2 norm by taking the square root of the squared norm\n    norm = jnp.sqrt(squared_norm)\n    \n    # Normalize the input array by dividing by the norm\n    normalized_x = x / norm\n    \n    return normalized_x"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "\n    # Check if the word is already in a consistent casing (all upper or all lower)\n    if word.isupper() or word.islower():\n        return word\n\n    # If the first letter is lowercase and the second letter is uppercase, capitalize the whole word\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n\n    # If the first letter is uppercase and the second letter is lowercase, lowercase the whole word\n    if word[0].isupper() and word[1].islower():\n        return word.lower()\n\n    # If the first two letters are uppercase, uppercase the whole word\n    if word[0].isupper() and word[1].isupper():\n        return word.upper()\n\n    # If none of the above conditions are met, return the word as is\n    return word"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    import re\n    # Remove all whitespace characters (spaces, tabs, newlines, etc.)\n    no_whitespace_text = re.sub(r'\\s+', '', line_text)\n    \n    # Segment the text into tokens. This example simply treats each character as a token.\n    # If you need a different segmentation logic, you can modify this part.\n    tokens = list(no_whitespace_text)\n    \n    return tokens"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "    import scipy.special\n    import numpy as np\n    # Calculate the number of spherical harmonics components\n    num_components = (deg_view + 1) ** 2\n\n    def sph_harmonic(l, m, theta, phi):\n        \"\"\"\n        Compute the spherical harmonic of degree l and order m on theta and phi.\n        \"\"\"\n        return scipy.special.sph_harm(m, l, phi, theta)\n\n    def dir_enc_fn(points):\n        \"\"\"\n        The directional encoding function that takes 3D points and returns their encoding.\n        \"\"\"\n        # Convert points to spherical coordinates\n        points = np.array(points)\n        r = np.linalg.norm(points, axis=-1, keepdims=True)\n        theta = np.arccos(points[..., 2] / r[..., 0])\n        phi = np.arctan2(points[..., 1], points[..., 0])\n\n        # Calculate the spherical harmonics for each degree and order\n        encoding = np.zeros((len(points), num_components), dtype=np.complex)\n        idx = 0\n        for l in range(deg_view + 1):\n            for m in range(-l, l + 1):\n                encoding[:, idx] = sph_harmonic(l, m, theta, phi).flatten()\n                idx += 1\n\n        # Return the real part of the encoding\n        return np.real(encoding)\n\n    return dir_enc_fn"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    import re\n    result = []\n    block_index = 0\n    header_block_index = None\n    current_block = {'text': '', 'type': 'paragraph', 'index': block_index, 'header_index': None}\n\n    for line in lines:\n        # Remove duplicate lines (ignoring numbers)\n        line = re.sub(r'\\d+', '', line)\n        if line in [block['text'] for block in result]:\n            continue\n\n        # Fix spaced characters\n        line = line.replace(' ', '')\n\n        # Connect incomplete lines\n        if not line.endswith('.'):\n            current_block['text'] += line + ' '\n            continue\n        else:\n            current_block['text'] += line\n\n        # Categorize lines into paragraphs, headers, or list items\n        if line.startswith('#'):\n            current_block['type'] = 'header'\n            header_block_index = block_index\n        elif line.startswith('-'):\n            current_block['type'] = 'list_item'\n            current_block['header_index'] = header_block_index\n        else:\n            current_block['type'] = 'paragraph'\n            current_block['header_index'] = header_block_index\n\n        # Append the current block to the result list\n        result.append(current_block)\n\n        # Prepare for the next block\n        block_index += 1\n        current_block = {'text': '', 'type': 'paragraph', 'index': block_index, 'header_index': None}\n\n    return result"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    from nltk.tokenize import PunktSentenceTokenizer\n    import re\n    if not org_texts:\n        return org_texts\n\n    # Normalize spaces\n    text = space_rule.sub(' ', org_texts).strip()\n\n    # Handle brackets\n    text = bracket_rule.sub(lambda m: m.group(0).replace('.', ''), text)\n\n    # Normalize quotation marks\n    text = quotation_pattern.sub('\"', text)\n\n    # Tokenize sentences using nltk_tokenizer\n    sentences = nltk_tokenizer.tokenize(text)\n\n    # Apply additional rules if necessary\n    # Assuming 'rules' is a list of (pattern, replacement) tuples\n    for pattern, replacement in rules:\n        sentences = [re.sub(pattern, replacement, sent) for sent in sentences]\n\n    return sentences"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        from typing import List\n        import numpy as np\n        positions_list = []\n\n        # If a specific document key is provided\n        if key is not None:\n            # Ensure the key exists in the documents\n            if key in self.documents:\n                # Find the positions of the token in the specific document\n                positions = np.where(np.array(self.documents[key]) == token)[0]\n                positions_list.append(positions)\n        else:\n            # If no specific key is provided, search across all documents\n            for doc_key, words in self.documents.items():\n                positions = np.where(np.array(words) == token)[0]\n                positions_list.append(positions)\n\n        return positions_list"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "\n    # Check if the spec is a percentage\n    if spec.endswith('%'):\n        # Calculate the percentage of the total number of clauses\n        percentage = int(spec[:-1])\n        min_should_match = (num_clauses * percentage) // 100\n        return min_should_match\n\n    # Check if the spec is a conditional expression\n    elif '<' in spec:\n        # Split the condition and the value\n        condition, value = spec.split('<')\n        condition = int(condition)\n        value = int(value)\n\n        # If the number of clauses is less than the condition, use the value\n        if num_clauses < condition:\n            return value\n        else:\n            # Otherwise, use the condition as the minimum should match\n            return condition\n\n    # Otherwise, assume the spec is an absolute number\n    else:\n        return int(spec)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        import numpy as np\n        from typing import List\n        if slop == 1 and len(tokens) == len(set(tokens)):\n            # Calculate phrase frequencies for unique tokens with slop 1\n            return self._calculate_phrase_freq(tokens)\n        else:\n            # Delegate to another method for different slops or non-unique tokens\n            return self._calculate_phrase_freq_with_slop(tokens, slop)"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        import numpy as np\n        from collections import defaultdict\n        from typing import Iterable, Callable\n        term_dict = defaultdict(int)\n        doc_lengths = []\n        documents = []\n        total_length = 0\n        \n        # Process the array in batches\n        for i, document in enumerate(array):\n            # Tokenize the document\n            tokens = tokenizer(document)\n            documents.append(tokens)\n            doc_length = len(tokens)\n            doc_lengths.append(doc_length)\n            total_length += doc_length\n            \n            # Update term dictionary with token frequencies\n            for token in tokens:\n                term_dict[token] += 1\n            \n            # If batch processing is needed, handle it here (not implemented)\n            # ...\n\n        # Calculate average document length\n        avg_doc_length = total_length / len(doc_lengths) if doc_lengths else 0\n        \n        # Create term matrix and positions (not implemented)\n        # For simplicity, we'll just create dummy arrays\n        term_matrix = np.zeros((len(documents), len(term_dict)))\n        positions = np.zeros((len(documents), len(term_dict)))\n        \n        # Create an instance of SearchArray with the indexed data\n        return cls(term_matrix=term_matrix, positions=positions, term_dict=term_dict,\n                   avg_doc_length=avg_doc_length, doc_lengths=doc_lengths)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "import threading\nimport socket\n\nclass ProxifierMessageInterceptor:\n    def __init__(self):\n        \"\"\"\n        Initializes an instance of the ProxifierMessageInterceptor class by setting up a server with specific configurations and starting it. It also initializes a dictionary to keep track of connections and a lock for thread safety.\n\n        Input-Output Arguments\n        :param self: ProxifierMessageInterceptor. An instance of the ProxifierMessageInterceptor class. It uses its configuration and logger attributes to set up the server and for logging purposes.\n        :return: No return values.\n        \"\"\"\n        # Assuming configuration and logger are attributes of the class\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        \n        # Assuming the configuration attribute has host and port information\n        self.server_socket.bind((self.configuration['host'], self.configuration['port']))\n        self.server_socket.listen(5)  # Listen for connections, 5 is the max queued connections\n        \n        # Start the server thread\n        self.server_thread = threading.Thread(target=self.run_server)\n        self.server_thread.daemon = True  # Daemonize thread so it ends with the main program\n        self.server_thread.start()\n        \n        # Log that the server has started\n        self.logger.info(f\"Server started on {self.configuration['host']}:{self.configuration['port']}\")\n\n    def run_server(self):\n        \"\"\"\n        The method to run within the server thread to handle incoming connections.\n        \"\"\"\n        while True:\n            client_socket, client_address = self.server_socket.accept()\n            with self.lock:\n                self.connections[client_address] = client_socket\n            # Handle the connection in a separate thread or similar\n            # ...\n\n# Note: The above code assumes that the class has 'configuration' and 'logger' attributes\n# which are used to configure the server and log messages, respectively.\n# The 'run_server' method is just a placeholder to indicate where the server's main loop would go."}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    for i in range(len(arr)):\n        count = 0\n        n = arr[i]\n        while n:\n            count += n & 1\n            n >>= 1\n        arr[i] = count\n    return arr"}
{"namespace": "searcharray.solr.edismax", "completion": "    from typing import List, Optional, Tuple\n    import numpy as np\n    import pandas as pd\n    query_terms = q.split()\n    \n    # Initialize scores array\n    scores = np.zeros(len(frame))\n    \n    # Iterate over each document in the DataFrame\n    for index, row in frame.iterrows():\n        # Initialize max score for this document\n        doc_max_score = 0.0\n        \n        # Check each query field\n        for field in qf:\n            # Calculate field score using the similarity measure\n            field_score = similarity.score(row[field], q)\n            \n            # Update max score for the document\n            doc_max_score = max(doc_max_score, field_score)\n        \n        # Update the scores array with the max score for this document\n        scores[index] = doc_max_score\n    \n    # Explanation string (placeholder)\n    explanation = \"Scores calculated using edismax with provided similarity measure.\"\n    \n    # Return the scores and the explanation\n    return scores, explanation"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            # Transform the message data using the connection's c2s method\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            # Transform the message data using the connection's s2c method\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            # Handle the connection closure without modifying the message data\n            process.connection.close()\n        else:\n            raise ValueError(\"Unknown message type\")"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.close()  # Assuming each connection has a close method\n        self.connections.clear()  # Clear the list of connections\n\n        # Stop the server if it exists\n        if self.server is not None:\n            self.server.stop()  # Assuming the server has a stop method\n            self.server = None  # Set the server attribute to None"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance['bbox']\n    bbox_mode = instance.get('bbox_mode', 'xyxy')  # Assuming 'xyxy' as default mode\n\n    # Convert bbox to xyxy if it's in a different mode (e.g., xywh)\n    if bbox_mode == 'xywh':\n        x, y, w, h = bbox\n        bbox = [x, y, x + w, y + h]\n\n    # Calculate the center of the bounding box\n    bbox_center_x = (bbox[0] + bbox[2]) / 2\n    bbox_center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the top-left corner of the crop ensuring the instance center is within the crop\n    crop_width, crop_height = crop_size\n    image_width, image_height = image_size\n\n    crop_x0 = max(0, min(image_width - crop_width, int(bbox_center_x - crop_width / 2)))\n    crop_y0 = max(0, min(image_height - crop_height, int(bbox_center_y - crop_height / 2)))\n\n    # Ensure the crop is within the image boundaries\n    crop_x1 = min(image_width, crop_x0 + crop_width)\n    crop_y1 = min(image_height, crop_y0 + crop_height)\n\n    # Adjust the top-left corner if the crop goes beyond the image boundaries\n    if crop_x1 - crop_x0 < crop_width:\n        crop_x0 = crop_x1 - crop_width\n    if crop_y1 - crop_y0 < crop_height:\n        crop_y0 = crop_y1 - crop_height\n\n    # Create and return the CropTransform object\n    return CropTransform(crop_x0, crop_y0, crop_width, crop_height)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    import numpy as np\n    from PIL import Image, ExifTags\n    with Image.open(file_name) as img:\n        # Apply image orientation from EXIF data\n        for orientation in ExifTags.TAGS.keys():\n            if ExifTags.TAGS[orientation] == 'Orientation':\n                break\n        exif = img._getexif()\n        if exif is not None:\n            exif = dict(exif.items())\n            orientation_value = exif.get(orientation, None)\n            if orientation_value == 3:\n                img = img.rotate(180, expand=True)\n            elif orientation_value == 6:\n                img = img.rotate(270, expand=True)\n            elif orientation_value == 8:\n                img = img.rotate(90, expand=True)\n\n        # Convert image to the specified format\n        if format:\n            if format == 'BGR':\n                img = img.convert('RGB')\n                img_array = np.array(img)[:, :, ::-1]  # Convert RGB to BGR\n            elif format == 'YUV-BT.601':\n                img = img.convert('YCbCr')\n                img_array = np.array(img, dtype=np.float32) / 255.0  # Normalize to 0-1 range\n            else:\n                img = img.convert(format)\n                img_array = np.array(img)\n        else:\n            img_array = np.array(img)\n\n    return img_array"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if \"bbox\" in annotation and annotation[\"bbox\"] is not None:\n        bbox = transforms.apply_box(annotation[\"bbox\"])\n        annotation[\"bbox\"] = bbox.clip_to_image(remove_empty=True)\n\n    # Apply transformation to the segmentation if present in the annotation\n    if \"segmentation\" in annotation and annotation[\"segmentation\"] is not None:\n        if isinstance(annotation[\"segmentation\"], list):\n            # Assuming segmentation is a list of polygons\n            segmentation = [\n                transforms.apply_polygons(poly) for poly in annotation[\"segmentation\"]\n            ]\n            annotation[\"segmentation\"] = segmentation\n        else:\n            # Assuming segmentation is in RLE format\n            # Apply transformation to RLE (not implemented here)\n            pass\n\n    # Apply transformation to the keypoints if present in the annotation\n    if \"keypoints\" in annotation and annotation[\"keypoints\"] is not None:\n        keypoints = transforms.apply_keypoints(annotation[\"keypoints\"])\n        if keypoint_hflip_indices is not None and transforms.is_horizontal_flip():\n            keypoints = keypoints.horizontal_flip(image_width=image_size[1], indices=keypoint_hflip_indices)\n        annotation[\"keypoints\"] = keypoints.clip_to_image(remove_empty=True)\n\n    # Set the bbox_mode to XYXY_ABS after transformation\n    annotation[\"bbox_mode\"] = \"XYXY_ABS\"\n\n    return annotation"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        import numpy as np\n        if coords == [] or self.angle % 360 == 0:\n            return coords\n        \n        # Convert input coordinates to a numpy array for matrix operations\n        coords_array = np.array(coords)\n        \n        # Perform the rotation transformation\n        transformed_coords = np.dot(coords_array, self.rm.T)\n        \n        return transformed_coords.tolist()"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    import torch\n    from detectron2.structures import Boxes, BoxMode, Instances, PolygonMasks, BitMasks\n    # Create an empty Instances object\n    ret = Instances(image_size)\n    \n    # Convert list of annotations to fields\n    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    boxes = torch.as_tensor(boxes).reshape(-1, 4)  # (x1, y1, x2, y2) format\n    ret.gt_boxes = Boxes(boxes)\n    \n    classes = [obj[\"category_id\"] for obj in annos]\n    ret.gt_classes = torch.tensor(classes)\n    \n    if mask_format == \"polygon\":\n        # Each instance contains one or more polygons\n        polygons = [obj[\"segmentation\"] for obj in annos]\n        ret.gt_masks = PolygonMasks(polygons)\n    elif mask_format == \"bitmask\":\n        # Each instance is represented by a binary mask of shape (height, width)\n        masks = [obj[\"segmentation\"] for obj in annos]\n        ret.gt_masks = BitMasks(torch.stack([torch.tensor(mask, dtype=torch.uint8) for mask in masks]))\n    \n    if any(\"keypoints\" in obj for obj in annos):\n        keypoints = [obj.get(\"keypoints\", []) for obj in annos]\n        if keypoints:\n            ret.gt_keypoints = Keypoints(keypoints)\n    \n    return ret"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    from fvcore.nn import FlopCountAnalysis\n    from torch import nn\n    from collections import defaultdict\n    import typing\n    flops_dict = defaultdict(float)\n\n    # Perform FLOPs analysis using fvcore's FlopCountAnalysis\n    with torch.no_grad():  # Ensure that we do not compute gradients\n        flop_analysis = FlopCountAnalysis(model, inputs)\n\n    # Get the FLOPs for each operator and accumulate them in the dictionary\n    for op, flops in flop_analysis.by_operator().items():\n        flops_dict[op] += flops / 1e9  # Convert to GigaFLOPs\n\n    return flops_dict"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        import numpy as np\n        import cv2\n        if img is None or img.size == 0:\n            # Return the original image if it's empty\n            return img\n\n        # Check if the angle is a multiple of 360 degrees\n        if self.angle % 360 == 0:\n            # Return the original image if the angle results in no change\n            return img\n\n        # Use the instance's default interpolation method if not provided\n        if interp is None:\n            interp = self.interp\n\n        # Calculate the rotation matrix for the given angle\n        image_center = tuple(np.array(img.shape[1::-1]) / 2)\n        rot_mat = cv2.getRotationMatrix2D(image_center, self.angle, 1.0)\n\n        # Calculate the size of the new image\n        cos = np.abs(rot_mat[0, 0])\n        sin = np.abs(rot_mat[0, 1])\n        bound_w = int((self.h * sin) + (self.w * cos))\n        bound_h = int((self.h * cos) + (self.w * sin))\n\n        # Adjust the rotation matrix to take into account translation\n        rot_mat[0, 2] += (bound_w / 2) - image_center[0]\n        rot_mat[1, 2] += (bound_h / 2) - image_center[1]\n\n        # Perform the actual rotation and return the result\n        result = cv2.warpAffine(img, rot_mat, (bound_w, bound_h), flags=interp)\n        return result"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        import numpy as np\n        from typing import List\n        for i in range(len(predictions.pred_boxes)):\n            box = predictions.pred_boxes[i]\n            _class = predictions.pred_classes[i]\n            score = predictions.scores[i]\n            mask = predictions.pred_masks[i] if predictions.pred_masks else None\n            keypoints = predictions.pred_keypoints[i] if predictions.pred_keypoints else None\n\n            # Draw the bounding box\n            self.draw_box(box, color=\"red\", thickness=2)\n\n            # Draw the class label and score\n            label = f\"Class: {_class}, Score: {score:.2f}\"\n            self.draw_text(label, position=(box[0], box[1]), font_size=12, color=\"white\")\n\n            # Draw the mask if available\n            if mask is not None:\n                self.draw_mask(mask, color=\"green\", alpha=0.5)\n\n            # Draw keypoints if available\n            if keypoints is not None:\n                self.draw_keypoints(keypoints, color=\"blue\", radius=3)\n\n        # Return the image with the visualizations\n        return VisImage()"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        import numpy as np\n        # Assuming self.canvas is a numpy ndarray with an RGBA image\n        if self.canvas.shape[2] == 4:\n            # Convert RGBA to RGB by discarding the alpha channel\n            rgb_image = self.canvas[:, :, :3]\n        else:\n            # If the image is already in RGB format, just use it as is\n            rgb_image = self.canvas\n\n        # Ensure the image is of type uint8\n        rgb_image = rgb_image.astype(np.uint8)\n\n        return rgb_image"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        import cv2\n        from detectron2.data import MetadataCatalog\n        from detectron2.utils.visualizer import Visualizer as D2Visualizer\n        # Load the image using OpenCV\n        image = cv2.imread(dic[\"file_name\"])\n        \n        # Get metadata for the dataset (e.g., class names, colors)\n        metadata = MetadataCatalog.get(dic.get(\"dataset_name\", \"__unused\"))\n        \n        # Create a Detectron2 Visualizer instance\n        visualizer = D2Visualizer(image[:, :, ::-1], metadata=metadata)\n        \n        # If the dictionary has \"annotations\" key, visualize annotations\n        if \"annotations\" in dic:\n            for annotation in dic[\"annotations\"]:\n                # Draw segmentation masks\n                if \"segmentation\" in annotation:\n                    visualizer.draw_polygon(annotation[\"segmentation\"], color=\"g\")\n                \n                # Draw keypoints\n                if \"keypoints\" in annotation:\n                    visualizer.draw_keypoints(annotation[\"keypoints\"])\n                \n                # Draw bounding boxes\n                if \"bbox\" in annotation:\n                    visualizer.draw_box(annotation[\"bbox\"])\n        \n        # If the dictionary has \"sem_seg\" key, visualize semantic segmentation\n        if \"sem_seg\" in dic:\n            visualizer.draw_sem_seg(dic[\"sem_seg\"])\n        \n        # If the dictionary has \"panoptic_seg\" key, visualize panoptic segmentation\n        if \"panoptic_seg\" in dic:\n            visualizer.draw_panoptic_seg(dic[\"panoptic_seg\"])\n        \n        # Get the visualized image\n        vis_image = visualizer.get_output()\n        \n        # Convert the visualized image back to BGR format for OpenCV compatibility\n        vis_image = vis_image.get_image()[:, :, ::-1]\n        \n        return vis_image"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    import io\n    import torch\n    # Create an in-memory bytes buffer\n    buffer = io.BytesIO()\n    \n    # Save the original module to the buffer\n    torch.jit.save(module, buffer)\n    \n    # Seek to the beginning of the buffer to read from it\n    buffer.seek(0)\n    \n    # Load the module from the buffer\n    reloaded_module = torch.jit.load(buffer)\n    \n    return reloaded_module"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        from matplotlib.collections import PatchCollection\n        from matplotlib.patches import Polygon\n        from skimage import measure\n        import matplotlib.pyplot as plt\n        import numpy as np\n        if color is None:\n            color = np.random.rand(3)\n\n        # Find contours from the binary mask\n        contours = measure.find_contours(binary_mask, 0.5)\n\n        # Filter out small areas\n        contours = [contour for contour in contours if measure.area(contour) > area_threshold]\n\n        # Create a matplotlib figure and axis\n        fig, ax = plt.subplots()\n        ax.imshow(self.output_image)\n\n        # Draw the contours\n        for contour in contours:\n            polygon = Polygon(contour, fill=True, color=color + (alpha,), edgecolor=edge_color)\n            ax.add_patch(polygon)\n\n        # If text is provided, draw it on the mask\n        if text:\n            # Compute the centroid of the first contour to place the text\n            if contours:\n                centroid = measure.centroid(contours[0])\n                ax.text(centroid[1], centroid[0], text, color='w', fontsize=12, ha='center', va='center')\n\n        # Remove axis ticks and labels\n        ax.set_axis_off()\n\n        # Show the result\n        plt.show()\n\n        # Return the image object with the mask drawn on it (for this example, we return the axis)\n        return ax"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    from torch.testing import assert_allclose\n    import torch\n    assert isinstance(input, Instances), f\"{msg}The first argument must be an instance of the 'Instances' class.\"\n    assert isinstance(other, Instances), f\"{msg}The second argument must be an instance of the 'Instances' class.\"\n\n    if size_as_tensor:\n        assert torch.all(torch.tensor(input.image_size) == torch.tensor(other.image_size)), \\\n            f\"{msg}The 'image_size' tensors are not equal.\"\n    else:\n        assert input.image_size == other.image_size, \\\n            f\"{msg}The 'image_size' tuples are not equal.\"\n\n    for field in input.fields:\n        assert field in other.fields, f\"{msg}Field '{field}' is not present in both instances.\"\n\n        input_field = input.fields[field]\n        other_field = other.fields[field]\n\n        if isinstance(input_field, torch.Tensor) and isinstance(other_field, torch.Tensor):\n            assert_allclose(input_field, other_field, rtol=rtol), \\\n                f\"{msg}The tensors for field '{field}' are not close.\"\n        elif isinstance(input_field, Boxes) and isinstance(other_field, Boxes):\n            assert_allclose(input_field.tensor, other_field.tensor, rtol=rtol), \\\n                f\"{msg}The 'Boxes' tensors for field '{field}' are not close.\"\n        elif isinstance(input_field, ROIMasks) and isinstance(other_field, ROIMasks):\n            assert_allclose(input_field.tensor, other_field.tensor, rtol=rtol), \\\n                f\"{msg}The 'ROIMasks' tensors for field '{field}' are not close.\"\n        else:\n            raise ValueError(f\"{msg}Unsupported field type for field '{field}'.\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    from typing import Any\n    import importlib\n    try:\n        # Split the name by dots to separate the module from the attribute\n        parts = name.split('.')\n        # The module name is all parts except the last one\n        module_name = '.'.join(parts[:-1])\n        # The attribute name is the last part of the name\n        attribute_name = parts[-1]\n        \n        # Import the module\n        module = importlib.import_module(module_name)\n        \n        # Get the attribute from the module\n        return getattr(module, attribute_name)\n    except (ImportError, AttributeError) as e:\n        # If the standard method fails, you can implement a fallback method here\n        # For now, we'll just re-raise the exception\n        raise e from None"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        import torch\n        # Assuming the boxes tensor is of shape [N, 5] and contains [center_x, center_y, width, height, angle]\n        widths = self.boxes_tensor[:, 2]\n        heights = self.boxes_tensor[:, 3]\n        areas = widths * heights\n        return areas"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    # Check if the proposal generator is precomputed\n    if cfg.proposal_generator_name == \"PrecomputedProposals\":\n        return None\n\n    # Retrieve the proposal generator class from the registry\n    proposal_generator_class = PROPOSAL_GENERATOR_REGISTRY.get(cfg.proposal_generator_name)\n\n    # Check if the proposal generator class exists in the registry\n    if proposal_generator_class is None:\n        raise ValueError(f\"Proposal generator '{cfg.proposal_generator_name}' not found in the registry.\")\n\n    # Initialize the proposal generator with the configuration and input shape\n    proposal_generator = proposal_generator_class(cfg, input_shape)\n\n    return proposal_generator"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        from torch.nn import functional as F\n        import torch\n        scores, proposal_deltas = predictions\n        gt_classes = [prop.gt_classes for prop in proposals]\n        gt_boxes = [prop.gt_boxes for prop in proposals]\n\n        # Flatten all the ground truth classes and box deltas\n        gt_classes_flattened = torch.cat(gt_classes, dim=0)\n        gt_boxes_flattened = torch.cat(gt_boxes, dim=0)\n\n        # Calculate classification loss\n        loss_cls = F.cross_entropy(scores, gt_classes_flattened)\n\n        # Calculate box regression loss\n        # Assuming that we have a function `self.box_reg_loss` that computes the box regression loss\n        loss_box_reg = self.box_reg_loss(proposal_deltas, gt_boxes_flattened, gt_classes_flattened)\n\n        # Scale the losses by their respective weights\n        loss_dict = {\n            'loss_cls': loss_cls * self.loss_weight['loss_cls'],\n            'loss_box_reg': loss_box_reg * self.loss_weight['loss_box_reg']\n        }\n\n        return loss_dict"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    from typing import Type\n    # Retrieve the tracker name from the configuration\n    tracker_name = cfg.TRACKER_NAME\n    \n    # Look up the corresponding tracker class from the registry\n    if tracker_name in TRACKER_REGISTRY:\n        tracker_class = TRACKER_REGISTRY[tracker_name]\n    else:\n        raise ValueError(f\"Tracker name '{tracker_name}' is not registered.\")\n    \n    # Instantiate the tracker class with the configuration\n    tracker_instance = tracker_class(cfg)\n    \n    # Return the tracker instance\n    return tracker_instance"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        import numpy as np\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        # Prevent division by zero\n        widths = np.maximum(widths, 1e-6)\n        heights = np.maximum(heights, 1e-6)\n\n        # Apply deltas\n        pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n        pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n        pred_w = np.exp(dw) * widths[:, np.newaxis]\n        pred_h = np.exp(dh) * heights[:, np.newaxis]\n\n        # Convert centers and sizes back to x1, y1, x2, y2\n        pred_boxes = np.zeros(deltas.shape)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        processed_output = self._process_image(image)\n        \n        # If no specific annotation type is requested, return the entire processed output\n        if anno_type is None:\n            return processed_output\n        \n        # If a specific annotation type is requested, filter the output\n        if isinstance(anno_type, str):\n            # Return the specific annotation if it exists\n            return processed_output.get(anno_type)\n        elif isinstance(anno_type, list):\n            # Return a dictionary of the requested annotation types found in the output\n            return {atype: processed_output.get(atype) for atype in anno_type if atype in processed_output}\n        else:\n            raise ValueError(\"anno_type must be a string or a list of strings\")"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        from collections import defaultdict\n        import math\n        query = query.lower()\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Initialize a dictionary to hold the scores\n        scores = defaultdict(float)\n\n        # Constants for BM25\n        k1 = 1.5\n        b = 0.75\n\n        # Calculate BM25 score for each keyword and URL\n        for keyword in keywords:\n            if keyword in self.term_frequencies:\n                for url, freq in self.term_frequencies[keyword].items():\n                    # Calculate term-specific variables\n                    idf = math.log((self.total_documents - self.document_frequencies[keyword] + 0.5) / (self.document_frequencies[keyword] + 0.5) + 1)\n                    term_freq = freq\n                    doc_length = self.document_lengths[url]\n                    # BM25 score calculation\n                    score = idf * ((term_freq * (k1 + 1)) / (term_freq + k1 * (1 - b + b * (doc_length / self.avg_doc_length))))\n                    # Aggregate the score\n                    scores[url] += score\n\n        return dict(scores)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)  # Index each document"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        from typing import Tuple\n        import numpy as np\n        self.tensor[:, 4] = (self.tensor[:, 4] + 180) % 360 - 180\n\n        # Identify indices of nearly horizontal boxes\n        horizontal_indices = np.where(np.abs(self.tensor[:, 4]) <= clip_angle_threshold)[0]\n\n        # Convert to corner representation\n        for idx in horizontal_indices:\n            cx, cy, w, h, angle = self.tensor[idx]\n            x1 = cx - w / 2\n            y1 = cy - h / 2\n            x2 = cx + w / 2\n            y2 = cy + h / 2\n\n            # Clamp coordinates\n            x1 = np.clip(x1, 0, box_size[1])\n            y1 = np.clip(y1, 0, box_size[0])\n            x2 = np.clip(x2, 0, box_size[1])\n            y2 = np.clip(y2, 0, box_size[0])\n\n            # Convert back to original representation\n            self.tensor[idx, 0] = (x1 + x2) / 2\n            self.tensor[idx, 1] = (y1 + y2) / 2\n            self.tensor[idx, 2] = x2 - x1\n            self.tensor[idx, 3] = y2 - y1\n            # Ensure the angle remains the same and the size does not increase\n            self.tensor[idx, 2] = min(self.tensor[idx, 2], w)\n            self.tensor[idx, 3] = min(self.tensor[idx, 3], h)"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        # Iterate over each item in the data attribute\n        for item in self.data:\n            # Check if the item has a 'type' key and the type is one of the expected types\n            if 'type' in item and item['type'] in stats:\n                # Increment the count for the type in the stats dictionary\n                stats[item['type']] += 1\n\n        # Return the statistics dictionary\n        return stats"}
{"namespace": "common.bleu4_score", "completion": "    from nltk import word_tokenize\n    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n    # Tokenize the sentences\n    continuation_tokens = word_tokenize(continuation)\n    reference_tokens = word_tokenize(reference)\n\n    # Calculate BLEU-4 score\n    weights = (0.25, 0.25, 0.25, 0.25)  # Equal weights for 1-gram, 2-gram, 3-gram, and 4-gram\n    smoothing_function = SmoothingFunction().method1  # Smoothing function to avoid zero counts\n\n    # Calculate BLEU score\n    score = sentence_bleu(\n        [reference_tokens],  # Reference should be a list of token lists\n        continuation_tokens,\n        weights=weights,\n        smoothing_function=smoothing_function\n    )\n\n    # Apply brevity penalty if required\n    if with_penalty:\n        # Calculate brevity penalty\n        ref_length = len(reference_tokens)\n        cont_length = len(continuation_tokens)\n        bp = min(1, (cont_length / ref_length) ** (1 - weights[0])) if cont_length < ref_length else 1\n        score *= bp\n\n    return score"}
{"namespace": "common.rougeL_score", "completion": "pip install jieba"}
{"namespace": "chat.utils.run_cmd", "completion": "    import subprocess\n    print(f\"Executing command: {cmd}\")\n    completed_process = subprocess.run(cmd, shell=True)\n    return completed_process.returncode"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    neck_type = cfg.get('type')\n    if neck_type is None:\n        raise ValueError(\"The configuration for the neck must include a 'type' key.\")\n\n    # Check if the neck type is in NECKS\n    if neck_type in NECKS:\n        neck_class = NECKS[neck_type]\n    # If not, check if it's in MMDET_NECKS\n    elif neck_type in MMDET_NECKS:\n        neck_class = MMDET_NECKS[neck_type]\n    else:\n        raise ValueError(f\"The specified neck type '{neck_type}' is not recognized.\")\n\n    # Instantiate the neck class with the provided configuration\n    neck_instance = neck_class(**cfg)\n    \n    return neck_instance"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    import torch.nn as nn\n    loss_type = cfg.get('type', 'cross_entropy').lower()  # Default to cross entropy if not specified\n\n    # Define a dictionary mapping loss type strings to actual loss function classes\n    loss_dict = {\n        'cross_entropy': nn.CrossEntropyLoss,\n        'mse': nn.MSELoss,\n        'l1': nn.L1Loss,\n        'nll': nn.NLLLoss,\n        # Add more loss functions as needed\n    }\n\n    # Check if the specified loss type is available\n    if loss_type not in loss_dict:\n        raise ValueError(f\"Unknown loss type '{loss_type}'. Available types are: {list(loss_dict.keys())}\")\n\n    # Instantiate the loss function with any additional arguments from the config\n    loss_args = cfg.get('args', {})  # Default to an empty dict if no args are provided\n    loss_function = loss_dict[loss_type](**loss_args)\n\n    return loss_function"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    head_type = cfg.get('type')\n    if head_type in HEADS:\n        head_class = HEADS[head_type]\n    elif head_type in MMDET_HEADS:\n        head_class = MMDET_HEADS[head_type]\n    else:\n        raise ValueError(f\"Head type '{head_type}' not found in HEADS or MMDET_HEADS modules\")\n\n    # Instantiate the head object with the configuration\n    head = head_class(**cfg)\n    \n    return head"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "\n    # Check for potential duplication in the cfg dictionary\n    if 'train_cfg' in cfg and train_cfg is not None:\n        print(\"Warning: The training configuration is specified both in the cfg dictionary and as a function argument. The argument version will be used.\")\n        cfg['train_cfg'] = train_cfg\n    elif train_cfg is not None:\n        cfg['train_cfg'] = train_cfg\n\n    if 'test_cfg' in cfg and test_cfg is not None:\n        print(\"Warning: The testing configuration is specified both in the cfg dictionary and as a function argument. The argument version will be used.\")\n        cfg['test_cfg'] = test_cfg\n    elif test_cfg is not None:\n        cfg['test_cfg'] = test_cfg\n\n    # Build the segmentor model instance based on the provided configurations\n    # This is a placeholder for the actual model building logic, which would depend on the specific framework or library being used.\n    # For example, if using a library like PyTorch, you might initialize a model class with the cfg dictionary.\n    segmentor_model = initialize_segmentor_model(cfg)\n\n    return segmentor_model"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    assert 'type' in cfg, \"The configuration must include the 'type' of the detector\"\n    \n    # Warn if train_cfg or test_cfg is passed directly to the function\n    if train_cfg is not None or test_cfg is not None:\n        print(\"Warning: train_cfg and test_cfg should be specified in the cfg dictionary, not as separate arguments.\")\n    \n    # Check for duplication of train_cfg and test_cfg in cfg and arguments\n    if 'train_cfg' in cfg and train_cfg is not None:\n        raise ValueError(\"train_cfg is specified in both cfg and as a function argument.\")\n    if 'test_cfg' in cfg and test_cfg is not None:\n        raise ValueError(\"test_cfg is specified in both cfg and as a function argument.\")\n    \n    # Use the cfg train_cfg and test_cfg if provided, otherwise use the function arguments\n    cfg_train_cfg = cfg.get('train_cfg', train_cfg)\n    cfg_test_cfg = cfg.get('test_cfg', test_cfg)\n    \n    # Ensure that the detector type is registered\n    detector_type = cfg['type']\n    if detector_type not in DETECTORS:\n        raise ValueError(f\"Detector type '{detector_type}' is not registered.\")\n    \n    # Create an instance of the detector\n    detector_cls = DETECTORS[detector_type]\n    detector = detector_cls(cfg, cfg_train_cfg, cfg_test_cfg)\n    \n    return detector"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    import torch\n    import numpy as np\n    # Ensure the value is within [0, period]\n    val = val % period\n\n    # Shift the value to be within [-offset * period, (1-offset) * period]\n    val = val - offset * period\n\n    # If the value is less than the lower bound, add the period to bring it within the range\n    val = val + (val < -offset * period) * period\n\n    # If the value is greater than the upper bound, subtract the period to bring it within the range\n    val = val - (val > (1 - offset) * period) * period\n\n    return val"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    from collections import defaultdict\n    ap_dict = defaultdict(lambda: defaultdict(list))\n    ar_dict = defaultdict(lambda: defaultdict(list))\n    \n    # Loop over each IoU threshold\n    for iou_threshold in metric:\n        # Loop over each class label\n        for label, category in label2cat.items():\n            # Calculate AP and AR for the current class and IoU threshold\n            ap, ar = calculate_ap_ar_per_class(gt_annos, dt_annos, iou_threshold, label)\n            \n            # Store the AP and AR in the dictionaries\n            ap_dict[category][iou_threshold].append(ap)\n            ar_dict[category][iou_threshold].append(ar)\n    \n    # Calculate mean AP and AR across all IoU thresholds for each class\n    map_dict = {category: np.mean(ap_list) for category, ap_list in ap_dict.items()}\n    mar_dict = {category: np.mean(ar_list) for category, ar_list in ar_dict.items()}\n    \n    # Calculate overall mAP and mAR across all classes and IoU thresholds\n    overall_map = np.mean([map_val for map_val in map_dict.values()])\n    overall_mar = np.mean([mar_val for mar_val in mar_dict.values()])\n    \n    # Combine class-wise and overall results into a single dictionary\n    eval_results = {\n        'mAP': overall_map,\n        'mAR': overall_mar,\n        'class_wise': {\n            category: {'mAP': map_val, 'mAR': mar_val}\n            for category, map_val, mar_val in zip(label2cat.values(), map_dict.values(), mar_dict.values())\n        }\n    }\n    \n    # Optionally log the results\n    if logger:\n        if isinstance(logger, str):\n            with open(logger, 'w') as log_file:\n                log_file.write(str(eval_results))\n        else:\n            logger.info(eval_results)\n    \n    return eval_results"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        return (LiDARBox, \"LiDAR\")\n    elif box_type == \"Camera\":\n        return (CameraBox, \"Camera\")\n    elif box_type == \"Depth\":\n        return (DepthBox, \"Depth\")\n    else:\n        raise ValueError(f\"Unrecognized box type: {box_type}\")"}
{"namespace": "ollama._client.Client.chat", "completion": "                    import json\n        from collections.abc import Iterable\n        from typing import Optional, Sequence, Union, Mapping, Any, Iterator, Literal\n        if not model:\n            raise RequestError(\"A model identifier must be provided.\")\n\n        # Validate messages\n        if messages is not None:\n            if not isinstance(messages, Iterable):\n                raise TypeError(\"Messages must be a list of Message or dict-like objects.\")\n            for message in messages:\n                if not isinstance(message, Message) and not isinstance(message, dict):\n                    raise TypeError(\"Each message must be a Message or dict-like object.\")\n                if 'role' not in message or 'content' not in message:\n                    raise RequestError(\"Each message must contain a 'role' and 'content'.\")\n\n        # Here we would normally send a request to the API endpoint with the provided parameters\n        # For the purpose of this example, we'll simulate a response\n        def simulate_api_response():\n            # Simulate a single response or a stream of responses\n            if stream:\n                for i in range(5):  # Simulate 5 responses\n                    yield {'response': f\"Simulated response {i+1} for model '{model}'\"}\n            else:\n                return {'response': f\"Simulated single response for model '{model}'\"}\n\n        # Call the simulated API response function\n        try:\n            api_response = simulate_api_response()\n        except Exception as e:\n            raise ResponseError(f\"An error occurred while getting the response: {e}\")\n\n        # Return the response in the requested format\n        if stream:\n            return (resp if format == '' else self._format_response(resp, format) for resp in api_response)\n        else:\n            response = next(api_response)  # Get the single response\n            return response if format == '' else self._format_response(response, format)"}
{"namespace": "ollama._client.Client.pull", "completion": "        from typing import Union, Mapping, Any, Iterator\n        import requests\n        url = f\"https://server.com/models/{model}\"\n        \n        # Set the appropriate protocol if an insecure connection is requested\n        if insecure:\n            url = url.replace(\"https://\", \"http://\")\n        \n        try:\n            # Send the POST request to the server\n            with requests.post(url, stream=stream, verify=not insecure) as response:\n                # Raise an error if the request was unsuccessful\n                response.raise_for_status()\n                \n                if stream:\n                    # Return an iterator of ProgressResponse objects for streaming\n                    return (ProgressResponse(chunk) for chunk in response.iter_content(chunk_size=8192))\n                else:\n                    # Return a single ProgressResponse object for non-streaming\n                    return ProgressResponse(response.json())\n        except requests.RequestException as e:\n            # Raise a custom ResponseError if there's an issue with the request\n            raise ResponseError(f\"Failed to pull model '{model}': {e}\")"}
{"namespace": "ollama._client.Client.generate", "completion": "        from typing import Any, AnyStr, Iterator, Literal, Mapping, Optional, Sequence, Union\n        if not model:\n            raise ValueError(\"Model is required for generating a response.\")\n\n        # Mock response data\n        response_data = {\n            \"model\": model,\n            \"prompt\": prompt,\n            \"system\": system,\n            \"template\": template,\n            \"context\": context if context is not None else [],\n            \"stream\": stream,\n            \"raw\": raw,\n            \"format\": format,\n            \"images\": images if images is not None else [],\n            \"options\": options,\n            \"keep_alive\": keep_alive,\n            \"response\": \"Generated response based on the provided parameters.\"\n        }\n\n        # If stream is True, return a generator\n        if stream:\n            def response_stream():\n                while True:\n                    # Simulate streaming by yielding the same response multiple times\n                    yield response_data\n            return response_stream()\n\n        # If format is 'json', return the response as a JSON-like dictionary\n        if format == 'json':\n            return response_data\n\n        # Otherwise, return a single response\n        return response_data"}
{"namespace": "ollama._client.Client.push", "completion": "        from typing import Union, Mapping, Any, Iterator\n        import requests\n        url = 'https://example.com/api/push' if not insecure else 'http://example.com/api/push'\n        \n        # Prepare the data to be sent in the POST request\n        data = {'model': model}\n        \n        # Send the POST request\n        try:\n            response = requests.post(url, json=data, stream=stream)\n            \n            # Check if the response is successful (status code 2xx)\n            response.raise_for_status()\n            \n            if stream:\n                # If 'stream' is True, return a generator yielding 'ProgressResponse' objects\n                return (ProgressResponse(chunk) for chunk in response.iter_content(chunk_size=None))\n            else:\n                # If 'stream' is False, return a single 'ProgressResponse' object\n                return ProgressResponse(response.json())\n        except requests.RequestException as e:\n            # Handle any exceptions that occur during the request\n            raise ResponseError(f\"Request failed: {e}\")"}
{"namespace": "ollama._client.Client.create", "completion": "        import time\n        from os import PathLike\n        from typing import Union, Optional, Mapping, Iterator, Any\n        if not path and not modelfile:\n            raise RequestError(\"Either 'path' or 'modelfile' must be provided.\")\n\n        # Mock reading the model file content\n        model_content = \"\"\n        if path:\n            try:\n                with open(path, 'r') as file:\n                    model_content = file.read()\n            except Exception as e:\n                raise ResponseError(f\"Failed to read model file from path: {e}\")\n        elif modelfile:\n            model_content = modelfile\n\n        # Mock the model creation process\n        def model_creation_process():\n            yield ProgressResponse(status='started', details='Model creation has started.')\n            time.sleep(1)  # Simulate some processing time\n            yield ProgressResponse(status='in_progress', details='Model is being created...')\n            time.sleep(1)  # Simulate more processing time\n            yield ProgressResponse(status='completed', details=f\"Model '{model}' created successfully.\")\n\n        # Handle streaming or single response\n        if stream:\n            return model_creation_process()  # Return the generator\n        else:\n            # Get the last response from the generator\n            last_response = None\n            for response in model_creation_process():\n                last_response = response\n            return last_response  # Return the final ProgressResponse"}
{"namespace": "ollama._client.Client._create_blob", "completion": "        from typing import Union\n        from pathlib import Path\n        import requests\n        import hashlib\n        if isinstance(path, str):\n            path = Path(path)\n\n        # Calculate the SHA-256 checksum of the file\n        sha256_hash = hashlib.sha256()\n        with open(path, 'rb') as f:\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n        digest = sha256_hash.hexdigest()\n        sha256_digest = f'sha256:{digest}'\n\n        # Check if the blob already exists on the server\n        head_url = f'{self.server_url}/blobs/{sha256_digest}'\n        response = requests.head(head_url)\n\n        # If the blob does not exist, upload it\n        if response.status_code == 404:\n            with open(path, 'rb') as f:\n                post_url = f'{self.server_url}/blobs/'\n                files = {'file': (path.name, f)}\n                response = requests.post(post_url, files=files)\n                response.raise_for_status()  # Raise an error for bad status codes\n\n        # Return the SHA-256 digest of the file\n        return sha256_digest"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "        from typing import Any, AsyncIterator, Mapping, Optional, Sequence, Union, Literal\n        import asyncio\n        if not model:\n            raise ValueError(\"Model identifier is required.\")\n\n        # Simulate the request and response process\n        response_data = {\n            \"model\": model,\n            \"prompt\": prompt,\n            \"system\": system,\n            \"template\": template,\n            \"context\": context,\n            \"stream\": stream,\n            \"raw\": raw,\n            \"format\": format,\n            \"images\": images,\n            \"options\": options,\n            \"keep_alive\": keep_alive,\n        }\n\n        # If stream is True, return an asynchronous generator\n        if stream:\n            async def response_stream():\n                for _ in range(5):  # Simulate 5 streaming responses\n                    await asyncio.sleep(1)  # Simulate network delay\n                    yield response_data  # Yield the same response data for simplicity\n            return response_stream()\n\n        # If stream is False, return a single response object\n        return response_data"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "pip install aiohttp"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "        from collections.abc import Iterable\n        from typing import Optional, Sequence, Union, Mapping, Any, AsyncIterator, Literal\n        if not model:\n            raise ValueError(\"Model identifier is required.\")\n\n        # Validate messages parameter\n        if messages is not None:\n            if not isinstance(messages, Iterable):\n                raise ValueError(\"Messages must be a sequence of Message objects.\")\n            for message in messages:\n                if not isinstance(message, Message):\n                    raise ValueError(\"Each message must be an instance of Message.\")\n\n        # Validate keep_alive parameter\n        if keep_alive is not None:\n            if not isinstance(keep_alive, (float, str)):\n                raise ValueError(\"Keep_alive must be a float or a string.\")\n\n        # Validate format parameter\n        if format not in ['', 'json']:\n            raise ValueError(\"Format must be '' or 'json'.\")\n\n        # Prepare the request payload\n        payload = {\n            \"model\": model,\n            \"messages\": messages,\n            \"options\": options,\n            \"keep_alive\": keep_alive\n        }\n\n        # Make the asynchronous request and return the response\n        if stream:\n            # Return an asynchronous generator if stream is True\n            async for response in self._stream_chat_responses(payload):\n                yield response\n        else:\n            # Return a single response if stream is False\n            response = await self._make_request(payload)\n            return response"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "        from typing import Union, Mapping, Any, AsyncIterator\n        import aiohttp\n        url = 'https://example.com/api/push'  # Replace with the actual URL\n        data = {'model': model}\n        connector = aiohttp.TCPConnector(ssl=False) if insecure else None\n\n        async with aiohttp.ClientSession(connector=connector) as session:\n            async with session.post(url, json=data) as response:\n                if stream:\n                    # If stream is True, return an asynchronous generator\n                    async def response_stream():\n                        async for chunk in response.content.iter_any():\n                            progress_response = {'chunk': chunk}\n                            yield progress_response\n                    return response_stream()\n                else:\n                    # If stream is False, return a single ProgressResponse object\n                    progress_response = await response.json()\n                    return progress_response"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "        from typing import Union\n        from pathlib import Path\n        import hashlib\n        import asyncio\n        import aiohttp\n        def calculate_checksum(file_path):\n            sha256_hash = hashlib.sha256()\n            with open(file_path, 'rb') as f:\n                for byte_block in iter(lambda: f.read(4096), b\"\"):\n                    sha256_hash.update(byte_block)\n            return sha256_hash.hexdigest()\n\n        # Convert path to Path object if it's a string\n        if isinstance(path, str):\n            path = Path(path)\n\n        # Calculate the file checksum\n        checksum = calculate_checksum(path)\n        digest = f'sha256:{checksum}'\n\n        # Prepare the URL for the HEAD and POST requests\n        # Assuming the server URL and endpoint are known and stored in self.server_url\n        url = f'{self.server_url}/blobs/{digest}'\n\n        # Check if the blob already exists on the server\n        async with aiohttp.ClientSession() as session:\n            async with session.head(url) as response:\n                if response.status == 404:\n                    # Blob not found, proceed to upload\n                    async with session.post(url, data=aiohttp.AsyncIterablePayload(self._file_chunk_generator(path))) as post_response:\n                        if post_response.status == 201:\n                            # Successfully uploaded the blob\n                            return digest\n                        else:\n                            # Handle unsuccessful upload\n                            post_response.raise_for_status()\n                elif response.status == 200:\n                    # Blob already exists, no need to upload\n                    return digest\n                else:\n                    # Handle other HTTP errors\n                    response.raise_for_status()"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        from typing import NamedTuple\n        import tempfile\n        import subprocess\n        combined_code = user_code + \"\\n\" + test_code\n\n        # Create a temporary file to hold the combined code\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as temp_file:\n            temp_file_path = temp_file.name\n            temp_file.write(combined_code.encode('utf-8'))\n\n        # Run Pyright on the temporary file\n        try:\n            result = subprocess.run(\n                [\"pyright\", temp_file_path],\n                capture_output=True,\n                text=True,\n                check=False\n            )\n\n            # Check if Pyright found any type errors\n            if result.returncode == 0:\n                # No type errors found\n                return TypeCheckResult(True, \"Type check passed.\")\n            else:\n                # Type errors found\n                return TypeCheckResult(False, result.stdout + \"\\n\" + result.stderr)\n        finally:\n            # Clean up the temporary file\n            os.remove(temp_file_path)"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "        from aiohttp import ClientSession\n        from os import PathLike\n        from typing import Mapping, Any, Union, Optional, AsyncIterator\n        import aiohttp\n        if not path and not modelfile:\n            raise RequestError(\"Either 'path' or 'modelfile' must be provided.\")\n\n        url = f\"{self._base_url}/{model}/create\"  # Assuming _base_url is the base URL for the API\n        data = None\n\n        if path:\n            with open(path, 'r') as file:\n                data = file.read()\n        else:\n            data = modelfile\n\n        async with ClientSession() as session:\n            async with session.post(url, data=data) as response:\n                if response.status != 200:\n                    raise ResponseError(f\"Server responded with status code: {response.status}\")\n\n                if stream:\n                    async def response_stream():\n                        async for chunk in response.content.iter_any():\n                            yield chunk\n                    return response_stream()\n                else:\n                    return await response.json()"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    import torch\n    if isinstance(fn, torch.nn.Module):\n        # Compile the module\n        return module_forward_backward_compiler(fn)\n    elif callable(fn):\n        # Compile the function\n        return function_forward_backward_compiler(fn)\n    else:\n        raise TypeError(\"Input must be a PyTorch module or a callable function.\")"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    from typing import Dict, Optional\n    import yaml\n    import csv\n    summary_file = os.path.join(trial_path, 'summary.csv')\n    best_trial_id = None\n    best_metric = float('inf')  # Assuming we want to minimize the metric\n    with open(summary_file, mode='r') as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            # Assuming the evaluation metric is named 'loss' and we want to minimize it\n            current_metric = float(row['loss'])\n            if current_metric < best_metric:\n                best_metric = current_metric\n                best_trial_id = row['trial_id']\n    \n    if best_trial_id is None:\n        raise ValueError(\"No best trial found in the summary file.\")\n    \n    # Step 2: Locate the configuration file for the best trial\n    config_filename = f'config_{best_trial_id}.yaml'\n    config_path = os.path.join(trial_path, config_filename)\n    \n    # Step 3: Load the configuration file into a dictionary\n    with open(config_path, 'r') as config_file:\n        config_dict = yaml.safe_load(config_file)\n    \n    # Step 4: Save the configuration dictionary to a YAML file if output_path is provided\n    if output_path:\n        if not output_path.lower().endswith(('.yaml', '.yml')):\n            raise ValueError(\"Output file must have a .yaml or .yml extension.\")\n        with open(output_path, 'w') as output_file:\n            yaml.dump(config_dict, output_file)\n    \n    # Step 5: Return the configuration dictionary\n    return config_dict"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    from torch.jit import trace\n    import threading\n    import functools\n    cache = {}\n    cache_lock = threading.Lock()\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Create a key based on the arguments to check for a cached version\n        key = (args, frozenset(kwargs.items()))\n\n        # Check the cache for an existing traced version\n        with cache_lock:\n            if key in cache:\n                return cache[key](*args, **kwargs)\n\n        # Trace the function or module's forward method if not in cache\n        if hasattr(func, 'forward'):\n            # It's a module, so we trace the forward method\n            traced_func = trace(func.forward, *args, **kwargs_, optimize=True)\n        else:\n            # It's a regular function\n            traced_func = trace(func, *args, **kwargs_, optimize=True)\n\n        # Optionally compile the traced function or module\n        if ts_compiler is not None:\n            traced_func = ts_compiler(traced_func, **kwargs_)\n\n        # Update the cache with the new traced version\n        with cache_lock:\n            cache[key] = traced_func\n\n        # Return the traced (and possibly compiled) function or module\n        return traced_func(*args, **kwargs)\n\n    return wrapper"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        import os\n        import json\n        config_path = os.path.join(trial_path, 'best_config.json')\n        \n        # Load the best configuration from the file\n        with open(config_path, 'r') as config_file:\n            best_config = json.load(config_file)\n        \n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n        \n        # Instantiate the Runner with the extracted configuration and project directory\n        return cls(best_config, project_dir)"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    from typing import List, Tuple\n    filtered_results = []\n    filtered_metadatas = []\n\n    # If metadatas is not provided, create a list of None values with the same length as results\n    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    # Iterate over the results and their corresponding values and metadatas\n    for result, val, metadata in zip(results, value, metadatas):\n        # Check if the value is less than or equal to the threshold\n        if val <= threshold:\n            # If so, add the result and metadata to the filtered lists\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    # Return the filtered lists as a tuple\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    from typing import List, Callable, Dict\n    import os\n    import time\n    import pandas as pd\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    results = []\n    execution_times = []\n    evaluation_metrics = []\n\n    # Run each module and collect results\n    for i, module in enumerate(modules):\n        start_time = time.time()\n        result = module(previous_result, **module_params[i])\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Evaluate the result based on the strategies\n        # For simplicity, let's assume there's a function called evaluate_result that takes the result and strategies and returns a metric score\n        metric_score = evaluate_result(result, strategies)\n\n        # Save the result and metrics\n        result_file = os.path.join(node_line_dir, f\"result_{i}.csv\")\n        result.to_csv(result_file, index=False)\n        results.append(result)\n        execution_times.append(execution_time)\n        evaluation_metrics.append(metric_score)\n\n    # Select the best result based on evaluation metrics\n    # For simplicity, let's assume we select the result with the highest metric score\n    best_index = evaluation_metrics.index(max(evaluation_metrics))\n    best_result = results[best_index]\n\n    # Combine the previous result columns with the selected retrieval node's result columns\n    combined_result = pd.concat([previous_result, best_result], axis=1)\n\n    # Save the summary of execution times and evaluation metrics\n    summary_file = os.path.join(node_line_dir, \"summary.csv\")\n    summary_df = pd.DataFrame({\n        'module': [f\"module_{i}\" for i in range(len(modules))],\n        'execution_time': execution_times,\n        'evaluation_metric': evaluation_metrics\n    })\n    summary_df.to_csv(summary_file, index=False)\n\n    return combined_result"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    from typing import List, Tuple\n    if not (len(ids) == len(scores) == len(weights)):\n        raise ValueError(\"The length of ids, scores, and weights must be the same.\")\n\n    # Check if the sum of weights is 1\n    if not sum(weights) == 1:\n        raise ValueError(\"The sum of weights must be equal to 1.\")\n\n    # Normalize scores\n    normalized_scores = []\n    for score_list in scores:\n        max_score = max(score_list)\n        min_score = min(score_list)\n        if max_score == min_score:\n            normalized_scores.append([1.0] * len(score_list))  # Avoid division by zero\n        else:\n            normalized_scores.append([(score - min_score) / (max_score - min_score) for score in score_list])\n\n    # Combine scores using weights\n    combined_scores = {}\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            id = ids[i][j]\n            score = normalized_scores[i][j] * weights[i]\n            if id in combined_scores:\n                combined_scores[id] += score\n            else:\n                combined_scores[id] = score\n\n    # Sort combined results based on scores\n    sorted_combined_results = sorted(combined_scores.items(), key=lambda item: item[1], reverse=True)\n\n    # Select top_k results\n    top_ids = [result[0] for result in sorted_combined_results[:top_k]]\n    top_scores = [result[1] for result in sorted_combined_results[:top_k]]\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    from typing import Tuple, List\n    weighted_scores = {}\n    for id_group, score_group, weight in zip(ids, scores, weights):\n        for id, score in zip(id_group, score_group):\n            if id not in weighted_scores:\n                weighted_scores[id] = 0\n            weighted_scores[id] += score * weight\n    \n    # Step 2: Normalize the scores (if normalization is needed)\n    # Assuming normalization means scaling scores to be between 0 and 1\n    max_score = max(weighted_scores.values())\n    min_score = min(weighted_scores.values())\n    for id in weighted_scores:\n        weighted_scores[id] = (weighted_scores[id] - min_score) / (max_score - min_score) if max_score != min_score else 1\n    \n    # Step 3: Sort the IDs based on the weighted sum in descending order\n    sorted_ids_scores = sorted(weighted_scores.items(), key=lambda item: item[1], reverse=True)\n    \n    # Step 4: Return the top K IDs and their corresponding scores\n    top_ids = [id_score[0] for id_score in sorted_ids_scores[:top_k]]\n    top_scores = [id_score[1] for id_score in sorted_ids_scores[:top_k]]\n    \n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    from typing import List, Callable, Dict\n    import time\n    import pandas as pd\n    best_result = None\n    best_evaluation = None\n    best_module_name = None\n    results_summary = []\n\n    for i, module in enumerate(modules):\n        start_time = time.time()\n        params = module_params[i]\n        result = module(previous_result, **params)\n        execution_time = time.time() - start_time\n\n        # Evaluate the result based on the strategies\n        evaluation = evaluate_result(result, strategies)\n\n        # Save the result and summary\n        result.to_csv(f\"{node_line_dir}/result_{i}.csv\", index=False)\n        summary = {\n            'module_name': module.__name__,\n            'execution_time': execution_time,\n            'evaluation': evaluation\n        }\n        results_summary.append(summary)\n\n        # Select the best result based on the evaluation\n        if best_evaluation is None or evaluation > best_evaluation:\n            best_evaluation = evaluation\n            best_result = result\n            best_module_name = module.__name__\n\n    # Save the summary of all results\n    summary_df = pd.DataFrame(results_summary)\n    summary_df.to_csv(f\"{node_line_dir}/summary.csv\", index=False)\n\n    # Save the best result\n    if best_result is not None:\n        best_result.to_csv(f\"{node_line_dir}/best_result.csv\", index=False)\n\n    print(f\"The best module is {best_module_name} with an evaluation score of {best_evaluation}\")\n    return best_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    from typing import List, Callable, Dict\n    import pandas as pd\n    import time\n    import os\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    results = []\n    evaluation_metrics = []\n    execution_times = []\n\n    for i, module in enumerate(modules):\n        start_time = time.time()\n        params = module_params[i]\n        result = module(**params)\n        execution_time = time.time() - start_time\n\n        # Evaluate the result using the generator module and strategies\n        # Assuming there is a function evaluate_module that takes the result and strategies and returns a metric\n        metric = evaluate_module(result, strategies)\n\n        results.append(result)\n        evaluation_metrics.append(metric)\n        execution_times.append(execution_time)\n\n    # Select the best module based on the evaluation metrics\n    # Assuming higher metric is better; modify if necessary\n    best_index = evaluation_metrics.index(max(evaluation_metrics))\n    best_result = results[best_index]\n\n    # Combine the best result with the previous result\n    combined_result = pd.concat([previous_result, best_result], ignore_index=True)\n\n    # Save the results and summary\n    combined_result.to_csv(os.path.join(node_line_dir, 'combined_result.csv'), index=False)\n    summary = {\n        'execution_times': execution_times,\n        'evaluation_metrics': evaluation_metrics,\n        'best_module_index': best_index,\n    }\n    with open(os.path.join(node_line_dir, 'summary.txt'), 'w') as f:\n        for key, value in summary.items():\n            f.write(f'{key}: {value}\\n')\n\n    return combined_result"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    from typing import List\n    # Initialize an empty set to store unique values\n    unique_values = set()\n\n    # Iterate over each node in the list\n    for node in nodes:\n        # Check if the key exists in the node's module_params\n        if key in node.module_params:\n            # Add the value associated with the key to the set\n            unique_values.add(node.module_params[key])\n\n    # Convert the set to a list to return\n    return list(unique_values)"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    import ast\n    from typing import Optional, List\n    import pandas as pd\n    # Load the file into a pandas DataFrame\n    df = pd.read_csv(summary_path)\n\n    # Default dict_columns to ['module_params'] if not provided\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Convert specified columns to dictionaries\n    for column in dict_columns:\n        if column in df.columns:\n            df[column] = df[column].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else x)\n\n    return df"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        from typing import Dict\n        # Extract the module type from the dictionary\n        module_type = module_dict.pop('module_type')\n        \n        # Use the remaining dictionary as module parameters\n        return cls(module_type, **module_dict)"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    from typing import List, Dict, Any, Tuple, Union\n    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            # If the metric is a string, add it to the metric names list\n            # and append an empty dictionary to the metric parameters list\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            # If the metric is a dictionary, extract the name and parameters\n            # The name is expected to be under a key such as 'name'\n            name = metric.get('name')\n            if name:\n                metric_names.append(name)\n                # Create a copy of the dictionary without the 'name' key\n                params = {k: v for k, v in metric.items() if k != 'name'}\n                metric_params.append(params)\n            else:\n                raise ValueError(\"Metric dictionary must have a 'name' key\")\n        else:\n            raise TypeError(\"Metrics must be either a list of strings or a list of dictionaries\")\n\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    from sklearn.metrics.pairwise import cosine_similarity\n    from sentence_transformers import SentenceTransformer\n    from typing import List, Optional\n    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n\n    # Convert the ground truth strings and the predicted string into embeddings\n    gt_embeddings = embedding_model.encode(generation_gt)\n    pred_embedding = embedding_model.encode([pred])[0]\n\n    # Calculate cosine similarity between the predicted embedding and each ground truth embedding\n    similarities = cosine_similarity([pred_embedding], gt_embeddings)[0]\n\n    # Return the maximum cosine similarity score\n    max_similarity = max(similarities)\n    return max_similarity"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    from gfpgan import GFPGANer\n    import logging\n    try:\n        # Initialize GFPGANer\n        # The model_path can be changed to the specific model weights you have downloaded\n        # upscale is set to 1 since we are only interested in face enhancement\n        restorer = GFPGANer(model_path='experiments/pretrained_models/GFPGANv1.3.pth', upscale=1, arch='clean', channel_multiplier=2, bg_upsampler=None)\n\n        # Restore faces in the image\n        _, _, restored_image = restorer.enhance(np_image, has_aligned=False, only_center_face=False, paste_back=True)\n\n        return restored_image\n\n    except Exception as e:\n        logging.warning(f\"GFPGAN face restoration could not be performed: {e}\")\n        return np_image"}
{"namespace": "codeformer_model.setup_model", "completion": "    global face_restorers\n    try:\n        # Initialize the FaceRestorerCodeFormer instance with the given directory name\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        # Add the initialized instance to the global list of face restorers\n        face_restorers.append(face_restorer)\n        print(f\"Model setup completed successfully for directory: {dirname}\")\n    except Exception as e:\n        # Report any errors that occur during setup\n        print(f\"An error occurred while setting up the model: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    from gfpgan import GFPGANer\n    import sys\n    import os\n    try:\n        # Check if the directory exists\n        if not os.path.exists(dirname):\n            raise FileNotFoundError(f\"The directory {dirname} does not exist.\")\n\n        # Patch the sys.path to include the directory for facexlib\n        sys.path.append(dirname)\n\n        # Initialize the GFPGAN face restorer\n        # Assuming the model file is named 'GFPGANv1.pth' and located in the directory\n        model_path = os.path.join(dirname, 'GFPGANv1.pth')\n        if not os.path.isfile(model_path):\n            raise FileNotFoundError(f\"The GFPGAN model file does not exist in the directory {dirname}.\")\n\n        # Create the GFPGANer instance\n        restorer = GFPGANer(\n            model_path=model_path,\n            upscale=1,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None\n        )\n\n        print(\"GFPGAN model setup is complete.\")\n\n    except Exception as e:\n        print(f\"An error occurred during GFPGAN model setup: {e}\")"}
{"namespace": "quaternion.rotate", "completion": "    import numpy as np\n    # Normalize the quaternion to avoid scaling the vector\n    q = q / np.linalg.norm(q)\n    \n    # Convert the vector into a pure quaternion (with a zero scalar part)\n    v_quat = np.array([0] + list(v))\n    \n    # Compute the conjugate of the quaternion\n    q_conj = quaternion_conjugate(q)\n    \n    # Rotate the vector by applying the quaternion multiplication\n    # q * v * q_conj (where * denotes quaternion multiplication)\n    v_rotated_quat = quaternion_multiply(quaternion_multiply(q, v_quat), q_conj)\n    \n    # Extract the vector part from the rotated quaternion\n    v_rotated = v_rotated_quat[1:]\n    \n    return v_rotated"}
{"namespace": "quaternion.from_axis_angle", "completion": "q = [cos(theta/2), x*sin(theta/2), y*sin(theta/2), z*sin(theta/2)]"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    import math\n    low = 0\n    calls = 0\n    best_log_prob = -math.inf\n\n    while low <= high:\n        mid = (high + low) // 2\n        model.adjust_bias(idx, mid)\n        calls += 1\n\n        topk_indices = model.get_topk(prefix, k)\n        if idx in topk_indices:\n            log_prob = model.get_log_probability(prefix, idx)\n            calls += 1\n            if log_prob > best_log_prob:\n                best_log_prob = log_prob\n                if topk_indices[0] == idx:\n                    high = mid - 1\n                else:\n                    low = mid + 1\n            else:\n                high = mid - 1\n        else:\n            low = mid + 1\n\n    # Reset the bias for the index after search\n    model.adjust_bias(idx, 0)\n    calls += 1\n\n    return best_log_prob, calls"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.width, camera.height)\n        \n        # Make the context current before rendering\n        eglctx.make_current()\n        \n        # Perform the rendering using the camera's settings\n        # This part of the code would depend on the specific rendering API being used (e.g., OpenGL, Vulkan, etc.)\n        # For example, in OpenGL, you might set up the view and projection matrices based on the camera and then draw the mesh.\n        # Since the specifics are not provided, this is a placeholder for the rendering code.\n        self.render_with_camera_settings(camera)\n        \n        # After rendering, you might want to do some cleanup or swap buffers if necessary\n        # eglctx.swap_buffers()  # Hypothetical method, if double buffering is used"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    from transformers import BertConfig\n    config_dict = bert_config.to_dict()\n\n    # Add or modify the dictionary with Nomic-specific configurations\n    config_dict['nomic_specific_attribute_1'] = some_value_1\n    config_dict['nomic_specific_attribute_2'] = some_value_2\n    # Add or modify any other Nomic-specific attributes here\n\n    # Create a new NomicBertConfig object with the updated dictionary\n    nomic_config = NomicBertConfig(**config_dict)\n\n    return nomic_config"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return  # Do not render if the mesh is not visible\n\n        # Select the appropriate shader program based on the render type\n        if self.render_type == 'points':\n            shader_program = self.point_shader_program\n        else:\n            shader_program = self.mesh_shader_program\n\n        # Bind the shader program\n        shader_program.bind()\n\n        # Upload necessary uniforms to the GPU (e.g., view and projection matrices)\n        self.upload_gl_uniforms(camera)\n\n        # Bind the vertex array object (VAO)\n        self.vao.bind()\n\n        # Issue the appropriate OpenGL draw call based on the render type\n        if self.render_type == 'points':\n            # Draw points\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n        elif self.render_type == 'lines':\n            # Draw lines\n            if self.ebo:\n                glDrawElements(GL_LINES, len(self.faces), GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_LINES, 0, len(self.vertices))\n        elif self.render_type == 'triangles':\n            # Draw triangles\n            if self.ebo:\n                glDrawElements(GL_TRIANGLES, len(self.faces), GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n        elif self.render_type == 'quads':\n            # Draw quads (note: quads are deprecated in modern OpenGL)\n            if self.ebo:\n                glDrawElements(GL_QUADS, len(self.faces), GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_QUADS, 0, len(self.vertices))\n        elif self.render_type == 'triangle_strip':\n            # Draw triangle strip\n            if self.ebo:\n                glDrawElements(GL_TRIANGLE_STRIP, len(self.faces), GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLE_STRIP, 0, len(self.vertices))\n        else:\n            raise ValueError(f\"Unknown render type: {self.render_type}\")\n\n        # Unbind the vertex array object (VAO) to clean up\n        self.vao.unbind()\n\n        # Unbind the shader program\n        shader_program.unbind()"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        import OpenGL.GL as gl\n        import numpy as np\n        if 'torch' in str(type(ptr)):\n            # Convert PyTorch tensor to numpy array\n            ptr = ptr.cpu().numpy()\n\n        # If w or h are 0, use the object's width and height\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        # Bind the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.texture_id)\n\n        # Set unpack alignment to 1 for tightly packed data\n        gl.glPixelStorei(gl.GL_UNPACK_ALIGNMENT, 1)\n\n        # Upload the data to the texture\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n\n        # Unbind the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    import torch\n    assert R.dim() == 3 and R.shape[1:] == (3, 3), \"R must be a batch of 3x3 rotation matrices\"\n    assert tvec.dim() == 2 and tvec.shape[1] == 3, \"tvec must be a batch of translation vectors\"\n    assert camera_matrix.dim() == 3 and camera_matrix.shape[1:] == (3, 3), \"camera_matrix must be a batch of 3x3 matrices\"\n    assert image_size.dim() == 2 and image_size.shape[1] == 2, \"image_size must be a batch of image sizes\"\n    \n    # Calculate camera position (camera center in world coordinates)\n    camera_position = -torch.matmul(R.transpose(1, 2), tvec.unsqueeze(-1)).squeeze(-1)\n    \n    # Convert rotation matrix to a suitable representation for Pulsar (e.g., quaternion, euler angles, etc.)\n    # Here we assume Pulsar uses rotation matrices directly\n    camera_rotation = R\n    \n    # Calculate intrinsic parameters\n    # Focal lengths\n    fx = camera_matrix[:, 0, 0]\n    fy = camera_matrix[:, 1, 1]\n    \n    # Principal points\n    cx = camera_matrix[:, 0, 2]\n    cy = camera_matrix[:, 1, 2]\n    \n    # Image size\n    width = image_size[:, 0]\n    height = image_size[:, 1]\n    \n    # Normalize focal lengths by the image width and height\n    fx_normalized = fx / width\n    fy_normalized = fy / height\n    \n    # Adjust principal points to be relative to the image center\n    cx_normalized = (cx - width / 2) / width\n    cy_normalized = (cy - height / 2) / height\n    \n    # Sensor width is assumed to be 1 unit in Pulsar's coordinate system\n    # Adjust focal length based on znear and sensor width\n    focal_length_normalized = fx_normalized * znear\n    \n    # Combine all parameters into a single tensor\n    camera_params = torch.stack([\n        camera_position,  # Camera position\n        camera_rotation,  # Camera rotation\n        focal_length_normalized.unsqueeze(-1),  # Normalized focal length\n        cx_normalized.unsqueeze(-1),  # Normalized principal point x\n        cy_normalized.unsqueeze(-1),  # Normalized principal point y\n    ], dim=-1)\n    \n    return camera_params"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        import OpenGL.GL as gl\n        if not self.use_quad_draw:\n            # Fallback to a simpler blit method\n            self.blit(x, y, w, h)\n            return\n\n        # Use the provided width and height, or fall back to the instance's attributes\n        width = w if w else self.W\n        height = h if h else self.H\n\n        # Save the current viewport and scissor box\n        original_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        original_scissor_box = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n\n        # Set up the viewport and scissor box for rendering\n        gl.glViewport(x, y, width, height)\n        gl.glScissor(x, y, width, height)\n\n        # Activate the shader program\n        gl.glUseProgram(self.quad_program)\n\n        # Bind the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Bind the VAO (Vertex Array Object)\n        gl.glBindVertexArray(self.vao)\n\n        # Draw the quadrilateral\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Unbind the VAO\n        gl.glBindVertexArray(0)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(*original_viewport)\n        gl.glScissor(*original_scissor_box)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    from pytorch3d.renderer import PerspectiveCameras\n    import torch\n    R = batch['R']  # Rotation matrix\n    T = batch['T']  # Translation vector\n    K = batch['K']  # Intrinsic matrix\n    H = batch['H']  # Image height\n    W = batch['W']  # Image width\n\n    # Adjust rotation matrix for PyTorch3D (transpose and potentially flip axes)\n    R_pytorch3d = R.transpose(1, 2)  # Assuming R is of shape (B, 3, 3)\n\n    # Adjust translation vector for PyTorch3D (potentially flip axes)\n    T_pytorch3d = T  # Assuming T is of shape (B, 3)\n\n    # Recalculate intrinsic matrix for NDC\n    # Assuming fx, fy are the focal lengths and cx, cy are the principal point offsets\n    fx, fy, cx, cy = K[:, 0, 0], K[:, 1, 1], K[:, 0, 2], K[:, 1, 2]\n    K_ndc = torch.zeros_like(K)\n    K_ndc[:, 0, 0] = 2.0 * fx / W\n    K_ndc[:, 1, 1] = 2.0 * fy / H\n    K_ndc[:, 0, 2] = (2.0 * cx - W) / W\n    K_ndc[:, 1, 2] = (2.0 * cy - H) / H\n    K_ndc[:, 2, 2] = 1.0\n\n    # Compute camera center in camera's coordinate system\n    C = -torch.bmm(R_pytorch3d, T_pytorch3d.unsqueeze(-1)).squeeze(-1)\n\n    # Return the adjusted camera parameters\n    return H, W, K_ndc, R_pytorch3d, T_pytorch3d, C"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        import OpenGL.GL as gl\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        # Save the current read FBO binding\n        prev_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n\n        # Bind the Quad's FBO as the read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n\n        # Perform the blit operation\n        gl.glBlitFramebuffer(x, y, x + w, y + h,  # Source rectangle\n                             x, y, x + w, y + h,  # Destination rectangle\n                             gl.GL_COLOR_BUFFER_BIT,  # Mask indicating what buffer is to be copied\n                             gl.GL_NEAREST)  # Interpolation method if the image is stretched\n\n        # Restore the previous read framebuffer binding\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, prev_fbo)"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    import numpy as np\n    sorted_indices = np.argsort(t1)\n    t1 = t1[sorted_indices]\n    y1 = y1[sorted_indices]\n\n    # Find the index of the rightmost value less than or equal to t0 in t1\n    right_indices = np.searchsorted(t1, t0, side='right') - 1\n    # Ensure indices are within the bounds of t1 and y1\n    right_indices = np.clip(right_indices, 0, len(t1) - 1)\n\n    # Find the index of the leftmost value greater than t0 in t1\n    left_indices = np.searchsorted(t1, t0, side='left')\n    # Ensure indices are within the bounds of t1 and y1\n    left_indices = np.clip(left_indices, 0, len(t1) - 1)\n\n    # Calculate the inner measure using linear interpolation\n    inner_measure = np.zeros_like(t0)\n    for i, (right_idx, left_idx) in enumerate(zip(right_indices, left_indices)):\n        if right_idx == left_idx or t1[right_idx] == t1[left_idx]:\n            # If t0 is exactly at a t1 point or there is no interval to interpolate\n            inner_measure[i] = y1[right_idx]\n        else:\n            # Linear interpolation\n            t_fraction = (t0[i] - t1[right_idx]) / (t1[left_idx] - t1[right_idx])\n            inner_measure[i] = y1[right_idx] + t_fraction * (y1[left_idx] - y1[right_idx])\n\n    # Calculate the outer measure using a step function (take the next value after t0)\n    outer_measure = y1[left_indices]\n\n    return inner_measure, outer_measure"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    import torch\n    w_env_at_t = torch.interp(t, t_env, w_env, right=0)\n\n    # Calculate the difference between the target weights and the environment weights\n    diff = w - w_env_at_t\n\n    # Apply a half-quadratic penalty to the positive differences\n    # This penalizes target weights that are above the environment weights\n    loss = torch.where(diff > 0, diff ** 2, torch.zeros_like(diff))\n\n    # Sum the loss over the last dimension to get the total loss\n    total_loss = torch.sum(loss, dim=-1)\n\n    return total_loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    import torch\n    if t.shape[-1] != w.shape[-1] + 1:\n        raise ValueError(\"The last dimension of 't' must be one more than that of 'w'.\")\n\n    # Calculate inter-interval loss\n    # This could be the weighted sum of differences between adjacent elements in 't'\n    inter_interval_loss = torch.sum(w * (t[..., 1:] - t[..., :-1])**2)\n\n    # Calculate intra-interval loss\n    # This could be the weighted sum of squared differences between elements in 't' and a fixed point\n    # For simplicity, let's assume the fixed point is 0\n    intra_interval_loss = torch.sum(w * t[..., :-1]**2)\n\n    # Combine the losses to get the total distortion loss\n    total_loss = inter_interval_loss + intra_interval_loss\n\n    return total_loss"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    from typing import List\n    import torch\n    # Sort the tensor 't' and weights 'w'\n    sorted_indices = torch.argsort(t)\n    sorted_t = t[sorted_indices]\n    sorted_w = w[sorted_indices]\n\n    # Compute the cumulative distribution function (CDF) of the weights\n    cdf = torch.cumsum(sorted_w, dim=0)\n\n    # Interpolate to find the percentile values\n    percentiles = torch.tensor(ps) / 100.0  # Convert percentiles to fractions\n    percentile_values = torch.zeros_like(percentiles)\n\n    for i, p in enumerate(percentiles):\n        idx = torch.searchsorted(cdf, p, right=True)\n        if idx == 0:\n            percentile_values[i] = sorted_t[0]\n        elif idx == len(cdf):\n            percentile_values[i] = sorted_t[-1]\n        else:\n            # Linear interpolation\n            fraction = (p - cdf[idx - 1]) / (cdf[idx] - cdf[idx - 1])\n            percentile_values[i] = sorted_t[idx - 1] + fraction * (sorted_t[idx] - sorted_t[idx - 1])\n\n    return percentile_values"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    from typing import Tuple\n    import torch\n    assert torch.all(a[1:] >= a[:-1]), \"The input tensor 'a' must be sorted.\"\n\n    # Search for the indices where elements of 'v' should be inserted\n    idx_hi = torch.searchsorted(a, v, right=True)\n    # To find the lower bound, we need to find the index where a value\n    # just smaller than 'v' would be inserted.\n    # We use torch.nextafter to get the next smallest representable number\n    # that is smaller than 'v' to ensure we get the lower bound index.\n    v_next_smaller = torch.nextafter(v, torch.full_like(v, -float('inf')))\n    idx_lo = torch.searchsorted(a, v_next_smaller, right=False)\n\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    import torch\n    w_normalized = w / w.sum()\n\n    # Compute the cumulative distribution function (CDF)\n    cdf = torch.cumsum(w_normalized, dim=0)\n\n    # Generate uniform random samples for inverse transform sampling\n    u = torch.rand(num_samples)\n\n    # Find the indices of the bins each sample falls into\n    bin_indices = torch.searchsorted(cdf, u, right=True)\n\n    # Get the bin endpoints for each sample\n    t0 = t[bin_indices]\n    t1 = t[bin_indices + 1]\n\n    # Apply perturbation if enabled\n    if perturb:\n        # Generate perturbation for each sample\n        if single_jitter:\n            # Apply the same jitter to every sample\n            jitter = torch.rand(1) * (t1 - t0)\n        else:\n            # Apply independent jitter to each sample\n            jitter = torch.rand(num_samples) * (t1 - t0)\n    else:\n        jitter = torch.zeros(num_samples)\n\n    # Compute the final samples\n    samples = t0 + jitter\n\n    return samples"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    import torch\n    dilated_t = t * dilation\n    \n    # Clip the dilated time steps to the domain\n    clipped_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n    \n    # Adjust the weights\n    # This is a simplified example where we assume that if multiple time steps\n    # are combined due to dilation, their weights should be summed.\n    # The actual implementation may vary based on the specific requirements.\n    unique_t, inverse_indices = torch.unique(clipped_t, sorted=True, return_inverse=True)\n    adjusted_w = torch.zeros_like(unique_t)\n    for i, idx in enumerate(inverse_indices):\n        adjusted_w[idx] += w[i]\n    \n    return unique_t, adjusted_w"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    import numpy as np\n    # Ensure that the input tensors are numpy arrays for easier manipulation\n    tq = np.asarray(tq)\n    t = np.asarray(t)\n    y = np.asarray(y)\n\n    # Initialize the result array with the same shape as tq\n    result = np.empty_like(tq)\n\n    # Iterate over each query time\n    for i, time in enumerate(tq):\n        # Check if the query time matches a step change time\n        if time in t:\n            result[i] = outside_value\n        else:\n            # Find the index of the last step change before the query time\n            idx = np.searchsorted(t, time, side='right') - 1\n            # Use the value at that index\n            result[i] = y[idx] if idx >= 0 else y[0]  # Assuming y[0] is the value before the first step change\n\n    return result"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    import torch\n    bias = (t - t[..., :-1]) / (t[..., 1:] - t[..., :-1] + eps)\n    bias = bias * (1.0 - train_frac) + train_frac\n\n    # Apply the annealing slope to the bias\n    bias = bias ** anneal_slope / (bias ** anneal_slope + (1 - bias) ** anneal_slope + eps)\n\n    # Adjust the weights using the bias\n    adjusted_weights = w * bias\n\n    # Normalize the adjusted weights to prevent NaN values and ensure stability\n    adjusted_weights = torch.nn.functional.softmax(adjusted_weights, dim=-1)\n\n    return adjusted_weights"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    import torch\n    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if not (ignore_list and k == \"meta\")}\n    elif isinstance(batch, list):\n        return [to_cuda(x, device, ignore_list) for x in batch]\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(x, device, ignore_list) for x in batch)\n    else:\n        raise TypeError(f\"Unsupported type for to_cuda: {type(batch)}\")"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    import torch\n    if len(f.shape) < len(v.shape):\n        # Expand the faces tensor to match the batch dimension of the vertices tensor\n        f = f.unsqueeze(0).expand(v.shape[0], *f.shape)\n\n    # Gather the vertices corresponding to each face index\n    # The gather operation is performed along the specified dimension\n    gathered_tris = torch.gather(v, dim, f.unsqueeze(dim))\n\n    # Reshape the result to maintain the structure of the original faces tensor\n    # with additional dimensions for batch processing\n    gathered_tris = gathered_tris.transpose(dim, -2)\n    return gathered_tris"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    from typing import Union\n    import numpy as np\n    import torch\n    if isinstance(batch, (torch.Tensor, np.ndarray)):\n        # Add a new dimension at the zeroth position for tensors and arrays\n        return np.expand_dims(batch, axis=0) if isinstance(batch, np.ndarray) else batch.unsqueeze(0)\n    elif isinstance(batch, (list, tuple)):\n        # Recursively call add_batch on each element in the list or tuple\n        return type(batch)(add_batch(item) for item in batch)\n    elif isinstance(batch, dict):\n        # Recursively call add_batch on each value in the dictionary\n        return {key: add_batch(value) for key, value in batch.items()}\n    else:\n        raise TypeError(\"Unsupported data type for add_batch function\")"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        import torch\n        batch = dotdict({\n            'focal_length': torch.tensor(self.focal_length),\n            'sensor_width': torch.tensor(self.sensor_width),\n            'sensor_height': torch.tensor(self.sensor_height),\n            'iso': torch.tensor(self.iso),\n            'shutter_speed': torch.tensor(self.shutter_speed),\n            'aperture': torch.tensor(self.aperture),\n            # ... convert other parameters as needed\n        })\n\n        # Create a nested 'meta' dictionary with the same content\n        batch.meta = dotdict({k: torch.tensor(v) for k, v in self.__dict__.items()})\n\n        return batch"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            # Serialize the agent's state into a dictionary format\n            serialized_data = agent.serialize()\n            # Save the serialized data using the persistence mechanism\n            self.save_to_storage(serialized_data)\n        else:\n            # If the agent is not a working agent or is a prime agent, do not save\n            print(\"Agent is not eligible for saving.\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        from sklearn.metrics.pairwise import cosine_similarity\n        from typing import Optional, Tuple\n        import numpy as np\n        closest_agent = None\n        highest_similarity = float('-inf')\n\n        for agent in self.agents:\n            similarity_score = cosine_similarity(purpose_embedding.reshape(1, -1), agent.embedding.reshape(1, -1))[0][0]\n            if similarity_score > highest_similarity:\n                highest_similarity = similarity_score\n                closest_agent = agent\n\n        return closest_agent, highest_similarity"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = {\n            'prompt': 'Prime Agent',  # Example prompt, can be customized\n            'name': 'Prime',  # Example name, can be customized\n            'weight': 1.0,  # Example weight, can be customized\n            'is_prime': True,  # Flag indicating prime status\n            'other_flag': False  # Another unspecified flag\n        }\n        # Adds the prime agent to the agent list\n        self.agent_list.append(prime_agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        import json\n        agent_data = self._retrieve_agent_data_from_db(purpose)\n        if agent_data:\n            # Deserialize the agent data\n            agent = self._deserialize_agent(agent_data, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Find the start and end indices of the agent information\n        start_index = response.find('Use Agent[')\n        end_index = response.find(']', start_index)\n        \n        # If the indices are valid, extract the agent information\n        if start_index != -1 and end_index != -1:\n            # Extract the substring containing the agent information\n            agent_info = response[start_index + len('Use Agent['):end_index]\n            \n            # Split the agent information into name and input text\n            if ':' in agent_info:\n                agent_name, input_text = agent_info.split(':', 1)\n            else:\n                agent_name = agent_info\n                input_text = ''\n            \n            return agent_name, input_text\n        else:\n            # If the format is incorrect, return an empty tuple\n            return ('', '')"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Assuming there is a method to get all agents from the database\n        all_agents_data = self.get_all_agents_from_database()\n        loaded_agents = []\n\n        for agent_data in all_agents_data:\n            try:\n                # Assuming the agent_lifecycle has a method to load an agent\n                agent = agent_lifecycle.load_agent(agent_data, openai_wrapper)\n                loaded_agents.append(agent)\n            except Exception as e:\n                # Handle exceptions that may occur during the loading process\n                print(f\"Failed to load agent: {e}\")\n\n        return loaded_agents"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            # Assuming the MicroAgent class has a save method\n            agent.save()\n        except Exception as e:\n            # Log the exception (assuming a logging mechanism is in place)\n            print(f\"An error occurred while saving the agent: {e}\")\n            # Re-raise the error\n            raise"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        from typing import List, Any\n        # Perform cleanup before returning the list of agents\n        self._cleanup_agents()\n        return self.agents"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        import numpy as np\n        # Assuming that the MicroAgent class has the attributes mentioned in the docstring\n        # Convert numpy array to list if necessary\n        purpose_embedding = agent.purpose_embedding\n        if isinstance(purpose_embedding, np.ndarray):\n            purpose_embedding = purpose_embedding.tolist()\n\n        # Serialize the MicroAgent object into a dictionary\n        agent_dict = {\n            'dynamic_prompt': agent.dynamic_prompt,\n            'purpose': agent.purpose,\n            'purpose_embedding': purpose_embedding,\n            'depth': agent.depth,\n            'max_depth': agent.max_depth,\n            'usage_count': agent.usage_count,\n            'id': agent.id,\n            'parent_id': agent.parent_id,\n            'working_agent': agent.working_agent,\n            'is_prime': agent.is_prime,\n            'evolve_count': agent.evolve_count,\n            'number_of_code_executions': agent.number_of_code_executions,\n            'last_input': agent.last_input\n        }\n        return agent_dict"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = f\"The goal is to {goal}. Given the input: '{sample_input}', how should the task be approached?\"\n        try:\n            # Assuming there is a method to get the completion from an OpenAI wrapper\n            completion = self.get_llm_completion(prompt)\n            return completion\n        except Exception as e:\n            # Log the exception (logging method not shown here)\n            print(f\"An error occurred: {e}\")\n            return \"\""}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Create a MicroAgent instance with the provided lifecycle and wrapper\n        micro_agent = MicroAgent(agent_lifecycle, openai_wrapper, **data)\n        \n        # Return the deserialized MicroAgent object\n        return micro_agent"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        import sqlite3\n        # Connect to the SQLite database\n        conn = sqlite3.connect(self.db_filename)\n        cursor = conn.cursor()\n\n        # Create table if it doesn't exist\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS agents (\n                id TEXT PRIMARY KEY,\n                purpose TEXT,\n                data TEXT\n            )\n        ''')\n\n        # Prepare the data for insertion or update\n        agent_id = agent_dict.get('id')\n        purpose = agent_dict.get('purpose')\n        data = agent_dict.get('data')  # Assuming 'data' is a serialized string\n\n        # Insert or update the agent's record\n        cursor.execute('''\n            INSERT INTO agents (id, purpose, data)\n            VALUES (?, ?, ?)\n            ON CONFLICT(id) DO UPDATE SET\n                purpose = excluded.purpose,\n                data = excluded.data\n        ''', (agent_id, purpose, data))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        import json\n        import sqlite3\n        conn = sqlite3.connect(self.db_filename)\n        cursor = conn.cursor()\n\n        try:\n            # Prepare the SQL query to fetch the agent data\n            query = \"SELECT data FROM agents WHERE purpose = ?\"\n            cursor.execute(query, (purpose,))\n\n            # Fetch the result\n            result = cursor.fetchone()\n\n            # Check if an agent was found\n            if result:\n                # Deserialize the agent data from JSON format\n                agent_data = json.loads(result[0])\n                return agent_data\n            else:\n                # No agent found with the given purpose\n                return None\n        except sqlite3.Error as e:\n            print(f\"An error occurred: {e}\")\n            return None\n        finally:\n            # Close the database connection\n            conn.close()"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        import sqlite3\n        # Connect to the SQLite database\n        connection = sqlite3.connect(self.database_filename)\n        cursor = connection.cursor()\n        \n        # Execute the query to retrieve all purposes\n        cursor.execute(\"SELECT purpose FROM agents\")\n        \n        # Fetch all results\n        results = cursor.fetchall()\n        \n        # Close the connection to the database\n        cursor.close()\n        connection.close()\n        \n        # Extract the purposes from the results and return them as a list\n        purposes = [result[0] for result in results]\n        return purposes"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        import hashlib\n        unique_string = func_name + str(args) + str(kwargs)\n        \n        # Encode the unique string to bytes\n        unique_bytes = unique_string.encode('utf-8')\n        \n        # Create a SHA-256 hash object\n        hash_obj = hashlib.sha256()\n        \n        # Update the hash object with the unique bytes\n        hash_obj.update(unique_bytes)\n        \n        # Return the hexadecimal digest of the hash\n        return hash_obj.hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        import json\n        import sqlite3\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        try:\n            # Query the database for the cached result using the provided arg_hash\n            cursor.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n            row = cursor.fetchone()\n            \n            # If a result is found, deserialize it from JSON format and return it\n            if row is not None:\n                return json.loads(row[0])\n            else:\n                return None\n        except sqlite3.Error as e:\n            print(f\"An error occurred: {e}\")\n            return None\n        finally:\n            # Close the database connection\n            conn.close()"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import functools\n    import json\n    import sqlite3\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Convert arguments to a JSON string to use as a key\n            key = json.dumps({'args': args, 'kwargs': kwargs}, sort_keys=True)\n            \n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n            \n            # Create table if it doesn't exist\n            cursor.execute(f'''\n                CREATE TABLE IF NOT EXISTS memoization (\n                    func_name TEXT,\n                    args_key TEXT,\n                    result TEXT,\n                    PRIMARY KEY (func_name, args_key)\n                )\n            ''')\n            conn.commit()\n            \n            # Check if the result is already in the database\n            cursor.execute('''\n                SELECT result FROM memoization WHERE func_name = ? AND args_key = ?\n            ''', (func_name, key))\n            row = cursor.fetchone()\n            \n            if row:\n                # If the result is in the database, return it\n                result = json.loads(row[0])\n            else:\n                # If the result is not in the database, call the function and store the result\n                result = func(*args, **kwargs)\n                cursor.execute('''\n                    INSERT INTO memoization (func_name, args_key, result) VALUES (?, ?, ?)\n                ''', (func_name, key, json.dumps(result)))\n                conn.commit()\n            \n            # Close the database connection\n            conn.close()\n            \n            return result\n        \n        return wrapper\n    return decorator"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        import json\n        import sqlite3\n        # Serialize the result to JSON format\n        json_result = json.dumps(result)\n\n        # Insert the arg_hash and json_result into the cache table\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.cursor()\n            try:\n                cursor.execute('''\n                    INSERT INTO cache (arg_hash, result) VALUES (?, ?)\n                ''', (arg_hash, json_result))\n                conn.commit()\n            except sqlite3.IntegrityError:\n                # If the arg_hash already exists, we can choose to ignore the error or update the result\n                # For this example, we'll just print a message and ignore the error\n                print(f\"Cache entry for arg_hash {arg_hash} already exists. Skipping insertion.\")"}
{"namespace": "run.execute_command_line_process", "completion": "import subprocess\nimport argparse\n\n    # Convert the argparse.Namespace to a list of command line arguments\n    command = []\n    for arg, value in vars(args).items():\n        if value is not None:\n            command.append(f'--{arg}')\n            if not isinstance(value, bool):  # Assuming that a boolean flag doesn't require a value\n                command.append(str(value))\n\n    # Execute the command line process\n    with subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True) as process:\n        if quiet_mode:\n            # Redirect output to a file\n            with open('output.txt', 'w') as file:\n                for line in process.stdout:\n                    file.write(line)\n        else:\n            # Print output to the terminal\n            for line in process.stdout:\n                print(line, end='')\n\n        # Wait for the process to complete\n        process.wait()\n\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(process.returncode, process.args)"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "    import openai\n    # Define the list of models in order of increasing capacity\n    models = ['text-davinci-002', 'text-davinci-003']  # Example models, replace with actual model names if different\n    model = kwargs.get('model', models[0])  # Default to the first model if not specified\n\n    try:\n        # Make the API request\n        response = openai.ChatCompletion.create(**kwargs)\n        return response\n    except openai.error.InvalidRequestError as e:\n        # Check if the error is due to context length\n        if 'context length' in str(e):\n            # Attempt to use a higher-capacity model if available\n            current_model_index = models.index(model)\n            if current_model_index + 1 < len(models):\n                kwargs['model'] = models[current_model_index + 1]\n                return chatcompletion_request(**kwargs)  # Recursive call with the next model\n            else:\n                raise BadRequestError(\"Context length limit exceeded and no higher-capacity models available.\")\n        else:\n            # Raise the error for any other invalid requests\n            raise BadRequestError(str(e))\n    except Exception as e:\n        # Handle other exceptions\n        raise BadRequestError(f\"An error occurred during the chat completion request: {e}\")"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        from datetime import datetime, timedelta\n        import boto3\n        if self._s3_client is None or self.credentials_expired():\n            self._s3_client = boto3.client('s3')\n            self._last_client_creation_time = datetime.now()\n        return self._s3_client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        from typing import Dict, Any\n        if self._is_worker_process():\n            raise RuntimeError(\"state_dict cannot be called from a DataLoader worker process.\")\n\n        state = {\n            'num_samples_yielded': num_samples_yielded,\n            'num_workers': num_workers,\n            'batch_size': batch_size,\n            'current_epoch': self.current_epoch,\n            'input_dir': self.input_dir,\n            'input_url': self.input_url,\n            'item_loader_state': self.item_loader_state,\n            'drop_last': self.drop_last,\n            'seed': self.seed,\n            'world_size': self.world_size,\n            'shuffle': self.shuffle,\n        }\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        from typing import Dict, Any\n        for key, value in state_dict.items():\n            # Here we are setting the attribute of the instance with the name 'key' to the 'value'\n            setattr(self, key, value)"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "\n        # Assuming the state dictionary and the attributes it should match are as follows:\n        # _state_dict = {\n        #     'shuffle': True/False,\n        #     'num_workers': int,\n        #     'input_dir': str,\n        #     'url': str,\n        #     'seed': int,\n        #     'item_loader_state': object,\n        #     'drop_last': True/False\n        # }\n        \n        # Validate shuffle\n        if self._state_dict['shuffle'] != self.shuffle:\n            raise ValueError(\"Mismatch in 'shuffle' between state dictionary and current state.\")\n        \n        # Validate num_workers\n        if self._state_dict['num_workers'] != self.num_workers:\n            raise ValueError(\"Mismatch in 'num_workers' between state dictionary and current state.\")\n        \n        # Validate input directory path\n        if self._state_dict['input_dir'] != self.input_dir:\n            raise ValueError(\"Mismatch in 'input_dir' between state dictionary and current state.\")\n        \n        # Validate URL\n        if self._state_dict['url'] != self.url:\n            raise ValueError(\"Mismatch in 'url' between state dictionary and current state.\")\n        \n        # Validate seed\n        if self._state_dict['seed'] != self.seed:\n            raise ValueError(\"Mismatch in 'seed' between state dictionary and current state.\")\n        \n        # Validate item_loader state\n        if self._state_dict['item_loader_state'] != self.item_loader.state:\n            raise ValueError(\"Mismatch in 'item_loader_state' between state dictionary and current state.\")\n        \n        # Validate drop_last\n        if self._state_dict['drop_last'] != self.drop_last:\n            raise ValueError(\"Mismatch in 'drop_last' between state dictionary and current state.\")\n        \n        # If no exceptions were raised, the state dictionary is valid for the current state"}
{"namespace": "run.parse_args", "completion": "    import os\n    import argparse\n    parser = argparse.ArgumentParser(description=\"Parses command line arguments for the task execution program.\")\n\n    # Required arguments\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n\n    # Optional arguments\n    parser.add_argument(\"--upload-files\", nargs='*', help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=['auto', 'manual'], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action='store_true', default=False, help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action='store_true', help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be retried upon failure.\")\n    parser.add_argument(\"--config-file\", type=str, default=os.getenv('CONFIG_FILE', 'assets/config.yml'), help=\"Path to the configuration file, specifying where to find the configuration settings.\")\n\n    # Parse the arguments\n    args = parser.parse_args()\n\n    return args"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    from typing import Optional\n    import hashlib\n    import os\n    if input_dir is None:\n        input_dir = ''\n    \n    # Generate a unique directory name by hashing the input directory\n    hasher = hashlib.sha256()\n    hasher.update(input_dir.encode('utf-8'))\n    hash_digest = hasher.hexdigest()\n    unique_dir_name = f\"cache_{hash_digest}\"\n    \n    # Determine the base directory for the cache\n    base_cache_dir = os.environ.get('MY_APP_CACHE_DIR', os.path.expanduser('~/.my_app_cache'))\n    \n    # Create the full path for the cache directory\n    cache_dir_path = os.path.join(base_cache_dir, unique_dir_name)\n    \n    # Attempt to create the cache directory\n    try:\n        os.makedirs(cache_dir_path, exist_ok=True)\n        return cache_dir_path\n    except OSError as e:\n        print(f\"Error creating cache directory: {e}\")\n        return None"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    from typing import Optional\n    # Define the special prefixes that indicate a path should be replaced\n    special_prefixes = ('/tmp', '/var', '/etc')\n\n    # Check if the path is None or an empty string\n    if path is None or path == '':\n        return True\n\n    # Check if the path starts with any of the special prefixes\n    for prefix in special_prefixes:\n        if path.startswith(prefix):\n            return True\n\n    # If none of the conditions are met, the path should not be replaced\n    return False"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    full_batches_per_worker = (num_samples_yielded // batch_size) // num_workers\n    # Calculate the number of samples processed in full batches by each worker\n    samples_per_worker = full_batches_per_worker * batch_size\n    \n    # Initialize the dictionary to store the number of samples per worker\n    worker_samples = {worker: samples_per_worker for worker in range(num_workers)}\n    \n    # Calculate the remaining samples after full batches have been assigned\n    remaining_samples = num_samples_yielded - (samples_per_worker * num_workers)\n    \n    # Distribute the remaining samples among the workers as evenly as possible\n    for i in range(remaining_samples):\n        worker = i % num_workers\n        worker_samples[worker] += 1\n    \n    return worker_samples"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "pip install boto3 filelock"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    from typing import List, Any, Dict, Tuple\n    worker_chunks: Dict[int, List[int]] = {i: [] for i in range(num_workers)}\n    worker_intervals: Dict[int, List[Any]] = {i: [] for i in range(num_workers)}\n\n    # Distribute chunks and intervals using a round-robin approach\n    for i, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        # Determine the worker index using the modulo operation\n        worker_index = i % worker_env.world_size\n\n        # Assign the chunk and interval to the appropriate worker\n        worker_chunks[worker_index].append(chunk)\n        worker_intervals[worker_index].append(interval)\n\n    return worker_chunks, worker_intervals"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        print(f\"Downloading from {remote_filepath} to {local_filepath}\")"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}  # Dictionary to store the updated chunk index for each worker\n    updated_indexes = {}  # Dictionary to store the updated indexes within the chunks for each worker\n\n    for worker_id, intervals in workers_intervals.items():\n        current_index = indexes.get(worker_id, 0)  # Get the current index for the worker, default to 0 if not found\n        for interval in intervals:\n            interval_size = interval[-1] - interval[0]  # Calculate the size of the interval\n            if current_index < interval_size:  # If the current index is within the interval size\n                chunk_indexes[worker_id] = interval[0] + current_index  # Update the chunk index\n                updated_indexes[worker_id] = current_index  # Update the index within the chunk\n                break  # Move to the next worker\n            else:\n                current_index -= interval_size  # Decrease the current index by the interval size and continue\n\n    return chunk_indexes, updated_indexes"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        import struct\n        from typing import Tuple, Optional\n        from PIL import Image\n        width, height = item.size\n        mode = item.mode\n        mode_length = len(mode)\n\n        # Get the raw pixel data as bytes\n        raw_data = item.tobytes()\n\n        # Pack the width, height, mode length, mode, and raw pixel data into a bytes object\n        # The format string 'IIB' means:\n        # 'I' for an unsigned int (width),\n        # 'I' for another unsigned int (height),\n        # 'B' for an unsigned char (mode length).\n        # We then add the mode string encoded in UTF-8 and the raw pixel data.\n        format_string = f'IIB{mode_length}s{len(raw_data)}s'\n        serialized_data = struct.pack(format_string, width, height, mode_length, mode.encode('utf-8'), raw_data)\n\n        # Return the serialized data and None as a tuple\n        return (serialized_data, None)"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        import os\n        from typing import Tuple, Optional\n        from PIL import Image\n        if not isinstance(item, Image.Image):\n            raise TypeError(\"Unsupported image type\")\n\n        if hasattr(item, 'filename') and item.filename.lower().endswith('.jpg') and os.path.isfile(item.filename):\n            try:\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n            except IOError as e:\n                return b'', f\"Error reading file {item.filename}: {e}\"\n        else:\n            try:\n                with BytesIO() as output:\n                    item.save(output, format=\"JPEG\")\n                    return output.getvalue(), None\n            except IOError as e:\n                return b'', f\"Error converting image to JPEG: {e}\""}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        import struct\n        import io\n        from PIL import Image\n        width, height, mode_size = struct.unpack('III', data[:12])\n        \n        # Calculate the starting position of the image data\n        image_data_start = 12 + mode_size\n        \n        # Extract the mode string\n        mode = data[12:image_data_start].decode('utf-8')\n        \n        # Extract the raw image data\n        image_data = data[image_data_start:]\n        \n        # Create an image object using the extracted information\n        image = Image.frombytes(mode, (width, height), image_data)\n        \n        return image"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        import struct\n        import torch\n        dtype_map = {\n            0: torch.float32,\n            1: torch.float64,\n            2: torch.int32,\n            3: torch.int64,\n            # Add more mappings as needed\n        }\n\n        # Read the first byte for the data type\n        dtype_code = data[0]\n        dtype = dtype_map.get(dtype_code)\n        if dtype is None:\n            raise ValueError(f\"Unknown dtype code: {dtype_code}\")\n\n        # Read the next 4 bytes for the length of the shape tuple\n        shape_len = struct.unpack('I', data[1:5])[0]\n\n        # Read the shape of the tensor\n        shape = struct.unpack(f'{shape_len}I', data[5:5 + 4 * shape_len])\n\n        # The rest of the data is the tensor's raw data\n        raw_data = data[5 + 4 * shape_len:]\n\n        # Construct the tensor from the raw data and the shape\n        tensor = torch.frombuffer(raw_data, dtype=dtype).reshape(shape)\n\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        from typing import Tuple, Optional\n        import torch\n        dtype_str = str(item.dtype)\n        dtype_bytes = dtype_str.encode('utf-8')\n\n        # Convert shape to string and encode as bytes\n        shape_str = str(item.shape)\n        shape_bytes = shape_str.encode('utf-8')\n\n        # Get raw data from the tensor as bytes\n        raw_data_bytes = item.numpy().tobytes()\n\n        # Construct the header with dtype and shape information\n        header = f\"{len(dtype_bytes):08d}{len(shape_bytes):08d}\".encode('utf-8') + dtype_bytes + shape_bytes\n\n        # Concatenate the header and raw data to form the complete serialized bytes object\n        serialized_tensor = header + raw_data_bytes\n\n        # Return the serialized tensor and None for the metadata\n        return serialized_tensor, None"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "            from torchvision.io import read_image\n            import torchvision.transforms as transforms\n        import io\n        from PIL import Image, JpegImagePlugin\n        from typing import Union\n        if TORCHVISION_AVAILABLE:\n            try:\n                # Attempt to decode the JPEG using torchvision\n                image_tensor = read_image(io.BytesIO(data), mode='RGB')\n                return image_tensor\n            except RuntimeError:\n                # If decoding fails, fall back to using PIL\n                pass\n        \n        # Use PIL to open the image\n        image = Image.open(io.BytesIO(data))\n        image = image.convert('RGB')  # Ensure image is in RGB format\n        \n        if TORCHVISION_AVAILABLE:\n            # Convert the PIL image to a PyTorch tensor\n            transform = transforms.ToTensor()\n            image_tensor = transform(image)\n            return image_tensor\n        else:\n            return image"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        from typing import Tuple, Optional\n        import numpy as np\n        import torch\n        numpy_array = item.numpy()\n        \n        # Convert the NumPy array to bytes\n        bytes_data = numpy_array.tobytes()\n        \n        # Map the tensor's data type to a string\n        dtype_str = str(item.dtype)\n        dtype_map = {\n            'torch.float32': '0',\n            'torch.float64': '1',\n            'torch.int32': '2',\n            'torch.int64': '3',\n            'torch.uint8': '4',\n            # Add other data types as needed\n        }\n        \n        # Get the data type index as a string\n        dtype_index = dtype_map.get(dtype_str, None)\n        if dtype_index is None:\n            raise ValueError(f\"Unsupported tensor data type: {dtype_str}\")\n        \n        # Return the bytes and the data type string\n        return bytes_data, f\"no_header_tensor:{dtype_index}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        import torch\n        tensor = torch.frombuffer(data, dtype=self._dtype)\n\n        # If the shape is known and provided, reshape the tensor accordingly\n        if self._shape is not None:\n            tensor = tensor.view(self._shape)\n\n        return tensor"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        import struct\n        import numpy as np\n        dtype_length = struct.unpack('I', data[:4])[0]\n        # Calculate the end position of the dtype string\n        dtype_end = 4 + dtype_length\n        # Extract the dtype string\n        dtype_str = data[4:dtype_end].decode('utf-8')\n        # Create the numpy dtype object\n        dtype = np.dtype(dtype_str)\n        \n        # Read the number of dimensions\n        num_dims = struct.unpack('I', data[dtype_end:dtype_end+4])[0]\n        # Calculate the start position of the dimension data\n        dims_start = dtype_end + 4\n        # Calculate the end position of the dimension data\n        dims_end = dims_start + num_dims * 4\n        # Extract the dimensions\n        dims = struct.unpack('I' * num_dims, data[dims_start:dims_end])\n        \n        # Extract the array data\n        array_data = data[dims_end:]\n        # Construct the numpy array using the dtype and dimensions\n        array = np.frombuffer(array_data, dtype=dtype).reshape(dims)\n        \n        return array"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        import numpy as np\n        # Calculate the number of elements in the data based on the data type size\n        num_elements = len(data) // np.dtype(self._dtype).itemsize\n        \n        # Convert the bytes data to a NumPy array using the specified data type\n        return np.frombuffer(data, dtype=self._dtype, count=num_elements)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        from typing import Tuple, Optional\n        import numpy as np\n        serialized_item = item.tobytes()\n\n        # Generate a custom dtype identifier string\n        # The format is \"no_header_numpy:{dtype_str}\", where {dtype_str} is the string representation of the array's dtype\n        dtype_identifier = f\"no_header_numpy:{item.dtype.str}\"\n\n        return serialized_item, dtype_identifier"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        from typing import Tuple, Optional\n        import numpy as np\n        dtype_str = str(item.dtype)\n        \n        # Get the shape of the array as a list of dimensions\n        shape = item.shape\n        \n        # Convert the shape to a string representation\n        shape_str = ','.join(map(str, shape))\n        \n        # Serialize the data type and shape as a header\n        header = f'{dtype_str};{shape_str}|'.encode('utf-8')\n        \n        # Get the binary data of the array\n        data = item.tobytes()\n        \n        # Combine the header and the data\n        serialized_item = header + data\n        \n        # Return the serialized bytes object and None for the placeholder\n        return serialized_item, None"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        from typing import Dict, Any\n        pass"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "                from torchvision.io import read_video\n        from typing import Any\n        import tempfile\n        import importlib.util\n        if not (importlib.util.find_spec(\"torchvision\") and importlib.util.find_spec(\"av\")):\n            raise ImportError(\"The required libraries 'torchvision' and 'av' are not installed.\")\n\n        # Import the necessary functions from torchvision\n        from torchvision.io import read_video\n\n        # Write the data to a temporary file\n        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp_file:\n            tmp_file_name = tmp_file.name\n            tmp_file.write(data)\n            tmp_file.flush()\n\n        # Use torchvision's read_video function to deserialize the video file\n        video_object, _, _ = read_video(tmp_file_name)\n\n        # Optionally, you can remove the temporary file if you don't need it after deserialization\n        # os.remove(tmp_file_name)\n\n        return video_object"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        from typing import List\n        if self.filled:\n            raise Exception(\"Writing process is already complete.\")\n\n        # Write any remaining chunks\n        while self._should_write:\n            self.write_chunk()\n\n        # Generate the index file for the chunks\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of file paths to the chunks\n        return self._file_paths"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        required_keys = {'current_epoch', 'samples_yielded', 'latest_worker_index'}\n        if not required_keys.issubset(obj.keys()):\n            raise ValueError(f\"The state dictionary is missing required keys: {required_keys - set(obj.keys())}\")\n\n        # Update the StreamingDataLoader state\n        self.current_epoch = obj['current_epoch']\n        self.samples_yielded = obj['samples_yielded']\n        self.latest_worker_index = obj['latest_worker_index']\n\n        # Check the type of dataset and update its state if applicable\n        if hasattr(self, 'dataset'):\n            if isinstance(self.dataset, StreamingDataset):\n                self.dataset.load_state_dict(obj.get('dataset_state', {}))\n            elif isinstance(self.dataset, CombinedStreamingDataset):\n                self.dataset.load_state_dict(obj.get('combined_dataset_state', {}))\n            else:\n                raise RuntimeError(\"The dataset associated with the StreamingDataLoader is neither a StreamingDataset nor a CombinedStreamingDataset.\")\n\n        # Prepare the DataLoader for resuming\n        # This is a placeholder for any additional logic required to adjust internal iterators and flags\n        # For example:\n        # self._reset_iterators()\n        # self._update_flags()\n        # Note: The actual implementation details would depend on how the StreamingDataLoader is designed to work"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        from typing import Dict, Any, List, Optional\n        state = {\n            'num_workers': num_workers,\n            'batch_size': batch_size,\n            'num_samples_yielded': num_samples_yielded if num_samples_yielded is not None else []\n        }\n\n        if self.internal_iterator is not None:\n            # Assuming the internal iterator has a method to return its state\n            state['internal_iterator_state'] = self.internal_iterator.state_dict()\n        else:\n            # If the internal iterator is None and num_samples_yielded is not provided, return an empty dict\n            if num_samples_yielded is None:\n                return {}\n\n        return state"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        from typing import Dict, Any\n        # Assuming state_dict contains keys 'datasets_state' and 'current_sample_index'\n        datasets_state = state_dict.get('datasets_state', [])\n        current_sample_index = state_dict.get('current_sample_index', 0)\n\n        # Update the state of each dataset\n        for dataset, dataset_state in zip(self.datasets, datasets_state):\n            if hasattr(dataset, 'load_state_dict'):\n                dataset.load_state_dict(dataset_state)\n\n        # Update the current sample index\n        self.current_sample_index = current_sample_index"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    from typing import Optional, Union\n    if isinstance(dir_path, Dir):\n        # If dir_path is already a Dir object, return it as is\n        return dir_path\n    elif isinstance(dir_path, str):\n        # If dir_path is a string, determine the type of path and create a Dir object\n        if dir_path.startswith(\"s3://\"):\n            # Handle S3 URL\n            return Dir(path=dir_path, url=dir_path)\n        elif dir_path.startswith(\"/\"):\n            # Handle absolute local path\n            return Dir(path=dir_path)\n        elif dir_path.startswith(\"./\") or dir_path.startswith(\"../\"):\n            # Handle relative local path\n            return Dir(path=dir_path)\n        else:\n            # Handle other specific project paths or raise an error\n            # This part depends on the specific project path formats you want to support\n            raise ValueError(f\"Unsupported directory path format: {dir_path}\")\n    else:\n        raise TypeError(\"dir_path must be a string or Dir object\")"}
