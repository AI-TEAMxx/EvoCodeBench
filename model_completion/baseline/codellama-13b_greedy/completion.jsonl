{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Import required modules\n  import numpy as np\n\n  # Compute the inverse of the contract function\n  z_inv = np.sign(z) * np.sqrt(np.abs(z))\n\n  return z_inv\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import functools\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            cur = conn.cursor()\n            cur.execute(f\"SELECT result FROM {func_name} WHERE args = ? AND kwargs = ?\", (str(args), str(kwargs)))\n            result = cur.fetchone()\n            if result:\n                result = result[0]\n            else:\n                result = func(*args, **kwargs)\n                cur.execute(f\"INSERT INTO {func_name} VALUES (?, ?, ?)\", (str(args), str(kwargs), str(result)))\n                conn.commit()\n            conn.close()\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: Invalid bounding box values. The minimum x value must be less than the maximum x value.\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: Invalid bounding box values. The minimum y value must be less than the maximum y value.\")\n\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the norms of each column of each matrix\n  norms0 = np.sum(mat0**2, axis=0)\n  norms1 = np.sum(mat1**2, axis=0)\n\n  # Compute the dot products of each column of each matrix\n  dot_prod = mat0.T.dot(mat1)\n\n  # Compute the squared distances\n  sq_dist = norms0[:, np.newaxis] + norms1 - 2*dot_prod\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"~\"):\n        return True\n    if path.startswith(\".\"):\n        return True\n    if path.startswith(\"/\"):\n        return True\n    if path.startswith(\"\\\\\"):\n        return True\n    return False\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"If 'items' is a dictionary, then 'assets_names' must be provided.\"\n            )\n        if dim == 1:\n            items_array = np.full(n_assets, fill_value)\n        else:\n            items_array = np.full((len(assets_names), n_assets), fill_value)\n        for i, asset in enumerate(assets_names):\n            if asset in items:\n                items_array[i, :] = items[asset]\n    else:\n        items_array = np.asarray(items)\n\n    if dim == 1:\n        if items_array.ndim == 1:\n            if items_array.shape[0] != n_assets:\n                raise ValueError(\n                    f\"The number of elements in '{name}' must match the number of assets.\"\n                )\n        elif items_array.ndim == 2:\n            if items_array.shape[1] != n_assets:\n                raise ValueError(\n                    f\"The number of columns in '{name}' must match the number of assets.\"\n                )\n            if items_array.shape[0] != 1:\n                raise ValueError(\n                    f\"The number of rows in '{name}' must be 1. If you want to pass multiple rows, pass '{name}' as a 1-D array.\"\n                )\n            items_array = items_array[0, :]\n        else:\n            raise ValueError(\n                f\"'{name}' must be a 1-D array or a 2-D array with a single row.\"\n            )\n    elif dim == 2:\n        if items_array.ndim == 1:\n            if items_array.shape[0] != n_assets:\n                raise ValueError(\n                    f\"The number of elements in '{name}' must match the number of assets.\"\n                )\n            items_array = items_array[np.newaxis, :]\n        elif items_array.ndim == 2:\n            if items_array"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Create a new MicroAgent object.\n        microagent = MicroAgent(agent_lifecycle, openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the dictionary's key-value pairs.\n        for key in data:\n            setattr(microagent, key, data[key])\n\n        return microagent\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = jnp.finfo(jnp.float32).eps\n\n  linear = xnp.where(\n    srgb <= 0.04045,\n    srgb / 12.92,\n    xnp.power((srgb + eps) / 1.055, 2.4)\n  )\n\n  return linear\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import splev, splrep\n\n  # Adjust spline degree to be at most one less than the number of points in x.\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Get knots and spline coefficients.\n  t, c, k = splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Interpolate.\n  return splev(t_output, (t, c, k))\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0].isupper() and word[1].isupper():\n            return word.lower()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        elif word[0].islower() and word[1].isupper():\n            return word.upper()\n        else:\n            return word\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.all(np.isin(v, [True, False])):\n        raise ValueError(f\"{cls.__name__}.{field.name}: {v.dtype} is not a binary array. Please check the data type of the array.\")\n\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # get the norm of the input\n  norm = np.linalg.norm(x)\n\n  # compute the scaling factor\n  scale = 1.0 / norm\n\n  # apply the scaling operation\n  y = x * scale\n\n  return y\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file\n    summary_df = pd.read_csv(summary_path)\n\n    # Convert specified columns to dictionary objects\n    if dict_columns is None:\n        dict_columns = ['module_params']\n    for dict_column in dict_columns:\n        summary_df[dict_column] = summary_df[dict_column].apply(ast.literal_eval)\n\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n  from scipy.linalg import det, inv\n\n  if mode == 'fast':\n    det_cov = det(cov)\n    if det_cov <= 0:\n      raise ValueError('The determinant of the covariance matrix is negative.')\n    else:\n      det_cov = np.sqrt(det_cov)\n      cov_iso = det_cov * inv(det_cov * cov)\n  elif mode == 'accurate':\n    det_cov = np.exp(np.log(det(cov)) / cov.shape[0])\n    if det_cov <= 0:\n      raise ValueError('The determinant of the covariance matrix is negative.')\n    else:\n      cov_iso = det_cov * inv(det_cov * cov)\n  else:\n    raise ValueError('Invalid mode of operation.')\n\n  return cov_iso"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Run a task\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be retried upon failure.\")\n    parser.add_argument(\"--config-file\", type=str"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[-1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name}: Expected a list of 2D points, but got shape {v.shape} instead.\")\n\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = 'abcdefghijklmnopqrstuvwxyz0123456789_'\n\n    # Check if the integer is within the range of the character set\n    if n < len(char_set):\n        return char_set[n]\n    else:\n        raise ValueError('Integer is out of range of the character set.')\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, eps) + value_at_zero - value_at_zero)\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    # initialize the chunk index and the chunk index for each worker\n    chunk_index = 0\n    chunk_indexes = {worker: 0 for worker in workers_intervals.keys()}\n\n    # iterate through each worker's intervals\n    for worker in workers_intervals.keys():\n\n        # iterate through each interval\n        for interval in workers_intervals[worker]:\n\n            # update the chunk index and the chunk index for the current worker\n            chunk_index += 1\n            chunk_indexes[worker] += 1\n\n            # update the index for the current worker\n            indexes[worker] += interval[-1] - interval[0]\n\n    return chunk_indexes, indexes\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  # Import functions\n  from ._trilerp import _trilerp_grid, _trilerp_hash\n\n  # Check if the data structure is valid\n  if datastructure not in ['grid', 'hash']:\n    raise ValueError('Invalid data structure.')\n\n  # If the data structure is a grid, adjust the coordinates\n  if datastructure == 'grid':\n    coordinates = coordinates / (np.array(values.shape[:3]) - 1)\n\n  # Perform the interpolation\n  if datastructure == 'grid':\n    return _trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return _trilerp_hash(values, coordinates)\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Initialize the weights\n  weights = np.zeros((v+1,v+1,v+1))\n\n  # Loop over the rows\n  for i in range(v+1):\n\n    # Loop over the columns\n    for j in range(v+1):\n\n      # Loop over the entries\n      for k in range(v+1):\n\n        # Compute the barycentric weights\n        weights[i,j,k] = (v-i)*(v-j)*(v-k)/(v*(v+1)*(v+2)/6)\n\n  # Return the weights\n  return weights\n\n"}
{"namespace": "linspline.query", "completion": "  # Ensure the spline is valid\n  if len(t) != len(v):\n    raise ValueError(\"The length of 't' and 'v' must be equal.\")\n\n  if len(t) < 2:\n    raise ValueError(\"The length of 't' must be at least 2.\")\n\n  if not all(t[i] < t[i+1] for i in range(len(t)-1)):\n    raise ValueError(\"The values in 't' must be in ascending order.\")\n\n  # Find the indices of the knots that bound each query point\n  i = np.searchsorted(t, tq)\n\n  # Find the fractional distance from each knot to the query point\n  frac = (tq - t[i-1]) / (t[i] - t[i-1])\n\n  # Interpolate the values at each query point\n  vq = v[i-1] + frac * (v[i] - v[i-1])\n\n  # Set the values outside the original range to 0\n  vq[i == 0] = 0\n  vq[i == len(t)] = 0\n\n  return vq\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    # Check if the value is an iterable.\n    if isinstance(v, Iterable):\n\n        # Check if all values are positive.\n        if not all(v_i > 0 for v_i in v):\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n\n    # If the value is not an iterable, check if it is positive.\n    elif v <= 0:\n        raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n\n    # Return the original value.\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert the rays to the near plane\n  origins = origins + directions * xnp.expand_dims(near, axis=1)\n\n  # Convert the rays to NDC\n  origins = xnp.matmul(pixtocam, xnp.concatenate((origins, xnp.ones_like(origins[:, :1])), axis=1).T).T[:, :3]\n  directions = xnp.matmul(pixtocam, directions.T).T\n\n  # Normalize the directions\n  directions = directions / xnp.linalg.norm(directions, axis=1, keepdims=True)\n\n  return origins, directions\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is close to -1 or 1\n  return np.isclose(dot_product, -1) or np.isclose(dot_product, 1)\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the continuation and reference text\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the BLEU score\n    score = bleu_score(continuation_tokens, reference_tokens, n_gram=4, with_penalty=with_penalty)\n\n    return score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.maximum(x, eps))\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Check if the input vector of weights sums to 1\n  if sum(w) != 1:\n    raise ValueError('The input vector of weights must sum to 1.')\n\n  # Check if the input vector of weights has the same length as the input vector of thresholds\n  if len(t) != len(w):\n    raise ValueError('The input vector of weights must have the same length as the input vector of thresholds.')\n\n  # Check if the input vector of thresholds is sorted\n  if not all(t[i] <= t[i+1] for i in range(len(t)-1)):\n    raise ValueError('The input vector of thresholds must be sorted.')\n\n  # Check if the input vector of weights is non-negative\n  if any(w[i] < 0 for i in range(len(w))):\n    raise ValueError('The input vector of weights must be non-negative.')\n\n  # Check if the input vector of weights is non-zero\n  if any(w[i] == 0 for i in range(len(w))):\n    raise ValueError('The input vector of weights must be non-zero.')\n\n  # Calculate the PDF\n  pdf = w / np.diff(t)\n\n  # Return the PDF\n  return pdf\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    # If the input is a tensor, we convert it to a numpy array\n    if torch.is_tensor(val):\n        val = val.cpu().numpy()\n\n    # We adjust the value to fit within the range of [-offset * period, (1-offset) * period]\n    val = (val + offset * period) % period - offset * period\n\n    # If the input was a tensor, we convert the output to a tensor\n    if torch.is_tensor(val):\n        val = torch.tensor(val, dtype=torch.float32)\n\n    return val\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        # If the purpose_embedding is a numpy array, convert it to a list.\n        if type(agent.purpose_embedding) == np.ndarray:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        # Return the dictionary representation of the MicroAgent instance.\n        return {\n            'dynamic_prompt': agent.dynamic_prompt,\n            'purpose': agent.purpose,\n            'purpose_embedding': agent.purpose_embedding,\n            'depth': agent.depth,\n            'max_depth': agent.max_depth,\n            'usage_count': agent.usage_count,\n            'id': agent.id,\n            'parent_id': agent.parent_id,\n            'working_agent': agent.working_agent,\n            'is_prime': agent.is_prime,\n            'evolve_count': agent.evolve_count,\n            'number_of_code_executions': agent.number_of_code_executions,\n            'last_input': agent.last_input\n        }\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n\n    # Check if the number of items and weights are equal\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be equal.\")\n\n    # Check if the weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"The weights must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins\n    bins = {i: [] for i in range(num_bins)}\n\n    # Initialize the total weights\n    total_weights = {i: 0 for i in range(num_bins)}\n\n    # Distribute the items into the bins\n    for item, weight in sorted_items:\n\n        # Find the bin with the lowest total weight\n        min_bin = min(total_weights, key=total_weights.get)\n\n        # Add the item to the bin\n        bins[min_bin].append(item)\n\n        # Update the total weight of the bin\n        total_weights[min_bin] += weight\n\n    return bins, total_weights\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # get the function name\n        func_name = func_name.encode('utf-8')\n\n        # get the positional arguments\n        args_str = str(args).encode('utf-8')\n\n        # get the keyword arguments\n        kwargs_str = str(kwargs).encode('utf-8')\n\n        # get the data to hash\n        data_to_hash = func_name + args_str + kwargs_str\n\n        # return the hexadecimal digest of the SHA-256 hash of the data\n        return hashlib.sha256(data_to_hash).hexdigest()\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points\n    point_distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Compute the total length of the polygon\n    polygon_length = np.sum(point_distances[point_distances < max_point_distance])\n\n    return polygon_length\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Check if the input arguments are valid\n    if rel_tr < 0 or abs_tr < 0:\n        raise ValueError(\"The relative and absolute thresholds must be non-negative.\")\n\n    # Compute the area of each polygon\n    areas = [Polygon(polygon).area for polygon in polygons]\n\n    # Compute the largest area\n    max_area = max(areas)\n\n    # Filter out polygons based on their area\n    filtered_polygons = [\n        polygon\n        for polygon, area in zip(polygons, areas)\n        if area > abs_tr or area > rel_tr * max_area\n    ]\n\n    return filtered_polygons\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples that each worker will process\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of remaining samples\n    num_remaining_samples = num_samples_yielded % num_workers\n\n    # Calculate the number of batches that each worker will process\n    num_batches_per_worker = num_samples_per_worker // batch_size\n\n    # Calculate the number of remaining batches\n    num_remaining_batches = num_samples_per_worker % batch_size\n\n    # Create a dictionary to store the number of samples each worker has processed\n    num_samples_per_worker_dict = {}\n\n    # Iterate over each worker\n    for worker_index in range(num_workers):\n\n        # Calculate the number of samples that the worker will process\n        num_samples_per_worker = num_batches_per_worker * batch_size\n\n        # If there are remaining samples, add one batch to the worker\n        if num_remaining_samples > 0:\n            num_samples_per_worker += batch_size\n            num_remaining_samples -= 1\n\n        # If there are remaining batches, add one batch to the worker\n        if num_remaining_batches > 0:\n            num_samples_per_worker += 1\n            num_remaining_batches -= 1\n\n        # Add the number of samples to the dictionary\n        num_samples_per_worker_dict[worker_index] = num_samples_per_worker\n\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    assert len(results) == len(value) == len(metadatas), \"The results, values, and metadatas lists must have the same length.\"\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, val, metadata in zip(results, value, metadatas):\n        if val <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    # Checking if the input array is a valid list of points\n    if not isinstance(array, np.ndarray):\n        raise ValueError(\"Input array must be a list of points.\")\n    if array.shape[1] != 2:\n        raise ValueError(\"Input array must be a list of points.\")\n\n    # Calculating the area of the polygon\n    area = 0.5 * np.abs(np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1)))\n\n    return area"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Find the upper and lower bounds of each element in v\n    idx_lo = torch.searchsorted(a, v, right=True)\n    idx_hi = torch.searchsorted(a, v, right=False)\n\n    # If the lower and upper bounds are the same, then the element is already in a\n    idx_same = idx_lo == idx_hi\n\n    # If the lower and upper bounds differ, then the element needs to be inserted\n    idx_diff = idx_lo != idx_hi\n\n    # If the lower and upper bounds are the same, then the element is already in a\n    idx_lo[idx_same] = torch.arange(0, idx_same.sum(), device=a.device)\n\n    # If the lower and upper bounds differ, then the element needs to be inserted\n    idx_hi[idx_diff] = torch.arange(1, idx_diff.sum(), device=a.device)\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n\n\n"}
{"namespace": "coord.contract", "completion": "  # Importing dependencies\n  import numpy as np\n\n  # Calculating the magnitude squared of the points\n  x_mag_sq = np.sum(np.square(x), axis = 1)\n\n  # Calculating the scaling factor for the points\n  x_scale = np.sqrt(1 - x_mag_sq) / np.sqrt(x_mag_sq)\n\n  # Scaling the points\n  x_scaled = x * x_scale[:, np.newaxis]\n\n  return x_scaled"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in ['', 'K', 'M', 'G', 'T', 'P']:\n        if abs(num_bytes) < 1000.0:\n            return f'{num_bytes:3.1f}{unit}B'\n        num_bytes /= 1000.0\n    return f'{num_bytes:.1f}YB'"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _validator(cls: Type, v: np.ndarray, field: ModelField) -> np.ndarray:\n\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"The number of dimensions of the array {field.name} must be {nb_dimensions}.\"\n            )\n\n        return v\n\n    return validator(\"*\", allow_reuse=True)(_validator)"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = onp.linalg.norm(cartesian_vector, axis = -1)\n  theta = onp.arccos(onp.maximum(onp.minimum(cartesian_vector[..., 2] / (r + eps), 1), -1))\n  phi = onp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi\n"}
{"namespace": "common.rougeL_score", "completion": "    # Import rouge_score\n    from rouge_score import rouge_scorer\n    from rouge_score import scoring\n\n    # Tokenize the continuation and reference text\n    tokenized_continuation = [token for token in jieba.cut(continuation)]\n    tokenized_reference = [token for token in jieba.cut(reference)]\n\n    # Create a scorer object\n    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n\n    # Calculate the ROUGE-L score\n    rouge_score = scorer.score(continuation, reference)\n\n    # Return the ROUGE-L score\n    return rouge_score['rougeL']\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Standard method\n    try:\n        components = name.split('.')\n        module = __import__(components[0])\n        for comp in components[1:]:\n            module = getattr(module, comp)\n        return module\n    except (AttributeError, ModuleNotFoundError):\n        pass\n\n    # Fallback method\n    try:\n        return locate_fallback(name)\n    except (AttributeError, ModuleNotFoundError):\n        pass\n\n    # Raise exception\n    raise ModuleNotFoundError(f\"Could not locate module '{name}'.\")\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Reset the buffer's current position to the beginning\n    buffer.seek(0)\n\n    # Load the module from the buffer\n    return torch.jit.load(buffer)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the ids and scores tuples are the same length\n    if len(ids) != len(scores):\n        raise ValueError(\"The ids and scores tuples must be the same length.\")\n\n    # Check that the weights tuple is the same length as the ids and scores tuples\n    if len(weights) != len(ids):\n        raise ValueError(\"The weights tuple must be the same length as the ids and scores tuples.\")\n\n    # Check that the weights tuple sums to 1\n    if round(sum(weights), 2) != 1:\n        raise ValueError(\"The weights tuple must sum to 1.\")\n\n    # Normalize the scores based on the weights\n    normalized_scores = []\n    for i in range(len(scores)):\n        normalized_scores.append([score * weights[i] for score in scores[i]])\n\n    # Combine the normalized scores into a single list\n    combined_scores = []\n    for i in range(len(normalized_scores[0])):\n        combined_scores.append(sum([score[i] for score in normalized_scores]))\n\n    # Sort the combined scores and ids\n    combined_scores, combined_ids = (list(t) for t in zip(*sorted(zip(combined_scores, ids), reverse=True)))\n\n    # Return the top k results\n    return combined_ids[:top_k], combined_scores[:top_k]"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    # Check if the input is a number\n    if not isinstance(x, (int, float)):\n        raise TypeError(\"The input must be a number.\")\n\n    # Check if the input is a percentage\n    if percent:\n        x *= 100\n\n    # Check if the input is NaN\n    if x != x:\n        return \"NaN\"\n\n    # Check if the input is zero\n    if x == 0:\n        return \"0\"\n\n    # Check if the input is an integer\n    if x % 1 == 0:\n        return str(int(x))\n\n    # Check if the input is a float\n    if x % 1 != 0:\n\n        # Check if the input is a negative number\n        if x < 0:\n            x = abs(x)\n            sign = \"-\"\n        else:\n            sign = \"\"\n\n        # Find the number of decimal places\n        if x < 0.001:\n            return f\"{sign}{x:.3f}\"\n        elif x < 0.01:\n            return f\"{sign}{x:.4f}\"\n        elif x < 0.1:\n            return f\"{sign}{x:.5f}\"\n        elif x < 1:\n            return f\"{sign}{x:.6f}\"\n        elif x < 10:\n            return f\"{sign}{x:.5f}\"\n        elif x < 100:\n            return f\"{sign}{x:.4f}\"\n        elif x < 1000:\n            return f\"{sign}{x:.3f}\"\n        else:\n            return f\"{sign}{x:.2f}\""}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        disk_usage = 100 - (os.statvfs(input_dir).f_bavail * 100 / os.statvfs(input_dir).f_blocks)\n        if disk_usage > threshold_in_gb:\n            print(f\"Disk usage is {disk_usage}%. Waiting for it to drop below {threshold_in_gb}%...\")\n            time.sleep(sleep_time)\n        else:\n            print(f\"Disk usage is {disk_usage}%. Continuing...\")\n            break\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in the time or position vector\n  dt = np.diff(t)\n\n  # Multiply the PDF values by the differences\n  w = p * dt\n\n  # Sum the weights\n  w_sum = np.sum(w)\n\n  # Normalize the weights\n  w = w / w_sum\n\n  return w\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    line_text = line_text.split()\n\n    return line_text\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check if the number of zeros is greater than the number of weights\n    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the number of weights.\")\n\n    # Generate the weights\n    weights = np.random.dirichlet(np.ones(n), size=1)\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(np.arange(n), size=zeros, replace=False)\n        weights[0, zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, **module_dict)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = instance['bbox']\n\n    # Get the center of the bounding box\n    center = (bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2)\n\n    # Get the dimensions of the bounding box\n    bbox_dims = (bbox[2], bbox[3])\n\n    # Get the dimensions of the crop\n    crop_dims = (crop_size[0], crop_size[1])\n\n    # Get the dimensions of the image\n    image_dims = (image_size[0], image_size[1])\n\n    # Get the top-left corner of the crop\n    crop_top_left = (center[0] - crop_dims[0] / 2, center[1] - crop_dims[1] / 2)\n\n    # Get the bottom-right corner of the crop\n    crop_bottom_right = (center[0] + crop_dims[0] / 2, center[1] + crop_dims[1] / 2)\n\n    # Get the top-left corner of the image\n    image_top_left = (0, 0)\n\n    # Get the bottom-right corner of the image\n    image_bottom_right = (image_dims[0], image_dims[1])\n\n    # Get the top-left corner of the crop, adjusted to fit within the image\n    crop_top_left_adjusted = (max(min(crop_top_left[0], image_bottom_right[0] - crop_dims[0]), image_top_left[0]),\n                              max(min(crop_top_left[1], image_bottom_right[1] - crop_dims[1]), image_top_left[1]))\n\n    # Get the bottom-right corner of the crop, adjusted to fit within the image\n    crop_bottom_right_adjusted = (min(max(crop"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm along the last axis\n  x_norm = jnp.sum(jnp.square(x), axis=-1, keepdims=True)\n\n  # Clamp the norm to a minimum value before taking the square root\n  x_norm_eps = jnp.maximum(x_norm, grad_eps)\n\n  # Normalize the input\n  x_norm_eps_sqrt = jnp.sqrt(x_norm_eps)\n  x_normalized = x / x_norm_eps_sqrt\n\n  return x_normalized\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # split the response string by the delimiter\n        split_response = response.split('Use Agent[')\n\n        # the second element of the split response is the agent information\n        agent_info = split_response[1]\n\n        # split the agent information by the colon\n        split_agent_info = agent_info.split(':')\n\n        # the first element of the split agent information is the agent name\n        agent_name = split_agent_info[0]\n\n        # the second element of the split agent information is the input text\n        input_text = split_agent_info[1]\n\n        # return the agent name and input text\n        return agent_name, input_text\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # import the necessary modules\n    import numpy as np\n    from detectron2.structures import BoxMode, Instances\n\n    # initialize an empty Instances object\n    instances = Instances(image_size)\n\n    # check if the annotations contain bounding boxes\n    if \"bbox\" in annos[0]:\n\n        # extract the bounding boxes from the annotations\n        boxes = [BoxMode.convert(obj[\"bbox\"], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS) for obj in annos]\n\n        # set the bounding boxes in the Instances object\n        instances.gt_boxes = boxes\n\n    # check if the annotations contain classes\n    if \"category_id\" in annos[0]:\n\n        # extract the classes from the annotations\n        classes = [obj[\"category_id\"] for obj in annos]\n\n        # set the classes in the Instances object\n        instances.gt_classes = classes\n\n    # check if the annotations contain segmentation masks\n    if \"segmentation\" in annos[0]:\n\n        # extract the segmentation masks from the annotations\n        if mask_format == \"polygon\":\n            masks = [obj[\"segmentation\"] for obj in annos]\n        elif mask_format == \"bitmask\":\n            masks = [obj[\"segmentation\"].astype(np.uint8) for obj in annos]\n\n        # set the segmentation masks in the Instances object\n        instances.gt_masks = masks\n\n    # check if the annotations contain keypoints\n    if \"keypoints\" in annos[0]:\n\n        # extract the keypoints from the annotations\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n\n        # set the keypoints in the Instances object\n        instances.gt_keypoints = keypoints\n\n    # return the Instances object\n    return instances"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    import os\n    import pathlib\n\n    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", str(pathlib.Path.home() / \"skfolio_data\"))\n\n    data_home = pathlib.Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n\n    return str(data_home)"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Check if the input is a 2D array\n    if cov.ndim != 2:\n        raise ValueError(\"The input array is not a 2D array.\")\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Set the \"training\" attribute of every submodule in the model to a constant value.\n    for module in model.modules():\n        module.training = False\n\n    # Return a context manager that reverts the \"training\" attributes of the submodules back to their original state.\n    return contextlib.nullcontext()\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def _are_shapes_equal(cls, values: BaseModel) -> BaseModel:\n\n        \"\"\"\n        This function is an internal validator function that checks if two specified fields within a model have the same shape, primarily used for validating data structures like NumPy arrays to ensure they are compatible for operations that require matching dimensions.\n\n        Input-Output Arguments\n        :param cls: BaseModel, The Pydantic model class that is being validated.\n        :param values: BaseModel, The values of the Pydantic model that is being validated.\n        :return: BaseModel, The values of the Pydantic model that is being validated.\n        \"\"\"\n\n        if values.get(field1).shape != values.get(field2).shape:\n            raise ValueError(\n                f\"The shape of {field1} and {field2} must be the same.\"\n            )\n\n        return values\n\n    return _are_shapes_equal"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # check if metrics is a list\n    if not isinstance(metrics, list):\n        raise TypeError(\"The metrics argument must be a list.\")\n\n    # check if metrics is empty\n    if len(metrics) == 0:\n        raise ValueError(\"The metrics argument cannot be empty.\")\n\n    # check if metrics is a list of strings\n    if all(isinstance(metric, str) for metric in metrics):\n        return metrics, [{} for _ in range(len(metrics))]\n\n    # check if metrics is a list of dictionaries\n    if all(isinstance(metric, dict) for metric in metrics):\n        metric_names = [metric[\"name\"] for metric in metrics]\n        metric_params = [metric[\"params\"] for metric in metrics]\n        return metric_names, metric_params\n\n    # metrics is neither a list of strings nor a list of dictionaries\n    raise TypeError(\"The metrics argument must be a list of strings or a list of dictionaries.\")\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == torch.exp:\n      fn_inv = torch.log\n    elif fn == torch.nn.functional.softplus:\n      fn_inv = lambda x: torch.log(torch.exp(x) - 1)\n    else:\n      raise NotImplementedError\n\n  def t_to_s(t):\n    t = torch.clamp(t, t_near, t_far)\n    return (t - t_near) / (t_far - t_near)\n\n  def s_to_t(s):\n    return t_near + s * (t_far - t_near)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) != len(w):\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  # Calculate the integral of the data points using the trapezoid rule\n  integral = 0.0\n  for i in range(len(t) - 1):\n    integral += (w[i] + w[i + 1]) * (t[i + 1] - t[i]) / 2.0\n\n  return integral"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check that the number of IDs, scores, and weights are equal\n    assert len(ids) == len(scores) == len(weights), \"The number of IDs, scores, and weights must be equal.\"\n\n    # Check that the weights are valid\n    assert sum(weights) == 1, \"The weights must sum to 1.\"\n\n    # Check that the top_k is valid\n    assert isinstance(top_k, int), \"The top_k must be an integer.\"\n    assert top_k > 0, \"The top_k must be a positive integer.\"\n\n    # Create a dictionary to store the weighted sum of scores for each ID\n    weighted_sums = {}\n\n    # Iterate through each set of IDs, scores, and weights\n    for i in range(len(ids)):\n\n        # Iterate through each ID and its corresponding score\n        for j in range(len(ids[i])):\n\n            # If the ID is already in the dictionary, add the weighted score to the existing value\n            if ids[i][j] in weighted_sums:\n                weighted_sums[ids[i][j]] += scores[i][j] * weights[i]\n\n            # Otherwise, add the ID to the dictionary with its corresponding weighted score\n            else:\n                weighted_sums[ids[i][j]] = scores[i][j] * weights[i]\n\n    # Sort the dictionary by the weighted sum of scores in descending order\n    weighted_sums = dict(sorted(weighted_sums.items(), key=lambda item: item[1], reverse=True))\n\n    # Extract the top K IDs and their corresponding weighted sums\n    top_ids = list(weighted_sums.keys())[:top_k]\n    top_weighted_sums = list(weighted_sums.values())[:top_k]\n\n    return top_ids, top_weighted_sums"}
{"namespace": "coord.track_linearize", "completion": "  # Find the number of dimensions of the mean\n  dim = tf.shape(mean)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the covariance\n  dim_cov = tf.shape(cov)[-1]\n\n  # Find the number of dimensions of the"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[:i.size // 2], i[i.size // 2:]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # get the number of dimensions\n  dim = x.shape[-1]\n\n  # get the number of scales\n  num_scales = max_deg - min_deg + 1\n\n  # generate the scales\n  scales = 2.0 ** torch.arange(min_deg, max_deg + 1)\n\n  # generate the indices\n  indices = torch.arange(dim)\n\n  # generate the indices for each scale\n  scales_indices = torch.stack([indices] * num_scales, dim=0)\n\n  # generate the scales for each index\n  scales_indices = scales_indices.unsqueeze(-1)\n  scales = scales.unsqueeze(1)\n\n  # compute the product of the scales and indices\n  x_enc = torch.zeros_like(scales_indices)\n  x_enc[scales_indices] = scales\n\n  # apply the sine function\n  x_enc = torch.sin(x * x_enc)\n\n  # reshape the result\n  x_enc = x_enc.view(num_scales, dim)\n\n  # append the original input\n  if append_identity:\n    x_enc = torch.cat([x, x_enc], dim=-1)\n\n  return x_enc"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def _are_all_shapes_equal(cls: Type[Any], values: Dict[str, Any]) -> Dict[str, Any]:\n\n        \"\"\"\n        This function checks if two lists of numpy arrays have the same length and if each corresponding pair of arrays has the same shape.\n\n        Input-Output Arguments\n        :param cls: Type[Any]. The class type to be validated.\n        :param values: Dict[str, Any]. A dictionary of values to be validated.\n        :return: Dict[str, Any]. The validated values.\n\n        Raises\n        :raises ValueError: If the two lists of numpy arrays do not have the same length or if any corresponding pair of arrays do not have the same shape.\n\n        \"\"\"\n\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"The lengths of the two lists of numpy arrays must be equal. The length of {field1} is {len(values[field1])} and the length of {field2} is {len(values[field2])}.\")\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"The shapes of the two lists of numpy arrays must be equal. The shape of {field1}[{i}] is {values[field1][i].shape} and the shape of {field2}[{i}] is {values[field2][i].shape}.\")\n\n        return values\n\n    return validator(_are_all_shapes_equal, allow_reuse=True)"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # resize the rendering context to match the camera's width and height\n        eglctx.set_size(camera.width, camera.height)\n\n        # bind the rendering context\n        eglctx.make_current()\n\n        # set the camera settings\n        self.set_camera(camera)\n\n        # render the mesh\n        self.render()\n\n        # unbind the rendering context\n        eglctx.unmake_current()\n\n        # get the rendered image\n        image = eglctx.read_pixels()\n\n        return image\n\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new configuration object\n    nomic_config = NomicBertConfig()\n\n    # Inherit the attributes from the original configuration\n    nomic_config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    nomic_config.hidden_act = bert_config.hidden_act\n    nomic_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    nomic_config.hidden_size = bert_config.hidden_size\n    nomic_config.initializer_range = bert_config.initializer_range\n    nomic_config.intermediate_size = bert_config.intermediate_size\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n    nomic_config.vocab_size = bert_config.vocab_size\n\n    # Add new attributes\n    nomic_config.num_labels = bert_config.num_labels\n    nomic_config.num_choices = bert_config.num_choices\n    nomic_config.num_decoder_layers = bert_config.num_decoder_layers\n    nomic_config.num_decoder_attention_heads = bert_config.num_decoder_attention_heads\n    nomic_config.decoder_intermediate_size = bert_config.decoder_intermediate_size\n    nomic_config.num_decoder_intermediate_layers = bert_config.num_decoder_intermediate_layers\n    nomic_config.num_decoder_intermediate_heads = bert_config.num_decoder_intermediate_heads\n    nomic_"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == 'point':\n            shader = self.shader_point\n        else:\n            shader = self.shader\n\n        shader.use()\n\n        self.upload_gl_uniforms(camera)\n\n        glBindVertexArray(self.vao)\n\n        if self.render_type == 'line':\n            if self.ebo is None:\n                glDrawArrays(GL_LINE_STRIP, 0, self.vertices.shape[0])\n            else:\n                glDrawElements(GL_LINE_STRIP, self.faces.shape[0], GL_UNSIGNED_INT, None)\n        elif self.render_type == 'triangle':\n            if self.ebo is None:\n                glDrawArrays(GL_TRIANGLES, 0, self.vertices.shape[0])\n            else:\n                glDrawElements(GL_TRIANGLES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n        elif self.render_type == 'quad':\n            if self.ebo is None:\n                glDrawArrays(GL_QUADS, 0, self.vertices.shape[0])\n            else:\n                glDrawElements(GL_QUADS, self.faces.shape[0], GL_UNSIGNED_INT, None)\n        elif self.render_type == 'triangle_strip':\n            if self.ebo is None:\n                glDrawArrays(GL_TRIANGLE_STRIP, 0, self.vertices.shape[0])\n            else:\n                glDrawElements(GL_TRIANGLE_STRIP, self.faces.shape[0], GL_UNSIGNED_INT, None)\n\n        glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # If the input is a PyTorch tensor, convert it to a numpy array\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # If the width and height are not provided, use the default dimensions\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        # Upload the texture data\n        glBindTexture(GL_TEXTURE_2D, self.texture)\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_UNSIGNED_BYTE, ptr)\n        glBindTexture(GL_TEXTURE_2D, 0)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    assert (\n        R.shape[-2:] == (3, 3)\n    ), f\"Input rotation matrix R must be of shape (..., 3, 3), but is of shape {R.shape}.\"\n    assert (\n        tvec.shape[-1] == 3\n    ), f\"Input translation vector tvec must be of shape (..., 3), but is of shape {tvec.shape}.\"\n    assert (\n        camera_matrix.shape[-2:] == (3, 3)\n    ), f\"Input camera matrix camera_matrix must be of shape (..., 3, 3), but is of shape {camera_matrix.shape}.\"\n    assert (\n        image_size.shape[-1] == 2\n    ), f\"Input image size image_size must be of shape (..., 2), but is of shape {image_size.shape}.\"\n\n    assert (\n        R.device == tvec.device == camera_matrix.device == image_size.device\n    ), f\"All input tensors must be on the same device, but are on devices {R.device}, {tvec.device}, {camera_matrix.device}, and {image_size.device}, respectively.\"\n\n    assert (\n        R.dtype == tvec.dtype == camera_matrix.dtype == image_size.dtype\n    ), f\"All input tensors must be of the same dtype, but are of dtypes {R.dtype}, {tvec.dtype}, {camera_matrix.dtype}, and {image_size.dtype}, respectively.\"\n\n    assert (\n        R.shape[:-2] == tvec.shape[:-1] == camera_matrix.shape[:-2] == image_size.shape[:-1]\n    ), f\"All input tensors must share the same batch dimension(s), but have shapes {R.shape}, {tvec.shape}, {camera_matrix.shape}, and {image_size.shape}, respectively.\"\n\n    # Get the camera position\n    cam_pos = -torch.bmm(R.transpose(-1, -2), tvec[..., None])[..., 0]\n\n    # Get"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.tex.blit(x, y, w, h)\n            return\n\n        # Set up a viewport and scissor box for rendering\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Set up the shader program\n        self.quad_program.use()\n        self.quad_program.set_uniform_data(\"tex\", self.tex.id)\n\n        # Bind the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex.id)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n        gl.glBindVertexArray(0)\n\n        # Restore the viewport and scissor box to their original sizes\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n\n        # Unbind the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)\n\n        # Unuse the shader program\n        self.quad_program.unuse()\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract intrinsic matrix\n    K = batch.K\n\n    # Extract image dimensions\n    H, W = batch.H, batch.W\n\n    # Extract camera rotation and translation\n    R = batch.R\n    T = batch.T\n\n    # Convert R to a 3x3 matrix\n    R = R.view(-1, 3, 3)\n\n    # Convert T to a 3x1 vector\n    T = T.view(-1, 3)\n\n    # Adjust R to match PyTorch3D's coordinate system\n    R = R.transpose(1, 2)\n\n    # Adjust T to match PyTorch3D's coordinate system\n    T = T * [-1, -1, 1]\n\n    # Compute the camera center\n    C = -torch.bmm(R.transpose(1, 2), T[:, :, None])[:, :, 0]\n\n    # Compute the intrinsic matrix for NDC\n    K = K.clone()\n    K[:, 0, 0] = 2.0 / W\n    K[:, 1, 1] = 2.0 / H\n    K[:, 0, 2] = -1.0\n    K[:, 1, 2] = -1.0\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # If the width and height are not specified, use the Quad's dimensions.\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        # Get the currently bound read framebuffer.\n        prev_read_fbo = glGetInteger(GL_READ_FRAMEBUFFER_BINDING)\n\n        # Bind the Quad's framebuffer as the read framebuffer.\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.fbo)\n\n        # Copy the pixel block.\n        glBlitFramebuffer(x, y, x + w, y + h, x, y, x + w, y + h, GL_COLOR_BUFFER_BIT, GL_LINEAR)\n\n        # Restore the previously bound read framebuffer.\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, prev_read_fbo)\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the source times (t1) that are closest to the target times (t0)\n    idx = tf.searchsorted(t1, t0, side='left')\n\n    # Find the indices of the source times (t1) that are closest to the target times (t0) and the next source time\n    idx_next = tf.minimum(idx + 1, tf.shape(t1)[0] - 1)\n\n    # Find the difference between the target times (t0) and the source times (t1)\n    t0_t1 = t0 - tf.gather(t1, idx)\n\n    # Find the difference between the next source times (t1) and the target times (t0)\n    t1_t0 = tf.gather(t1, idx_next) - t0\n\n    # Find the difference between the next source times (t1) and the source times (t1)\n    t1_t1 = tf.gather(t1, idx_next) - tf.gather(t1, idx)\n\n    # Construct the inner measure\n    inner = tf.gather(y1, idx) + t0_t1 / t1_t1 * (tf.gather(y1, idx_next) - tf.gather(y1, idx))\n\n    # Construct the outer measure\n    outer = tf.gather(y1, idx) + t0_t1 / t1_t0 * (tf.gather(y1, idx) - tf.gather(y1, idx_next))\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope\n    w_env_upper = w_env * (1 + (t_env - t) ** 2)\n\n    # calculate the loss\n    loss = (w * (1 + (t_env - t) ** 2) - w_env_upper).clamp(min=0)\n\n    # return the loss\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # calculate the inter-interval loss\n    inter_interval_loss = torch.sum(w * (t[..., 1:] - t[..., :-1]) ** 2)\n\n    # calculate the intra-interval loss\n    intra_interval_loss = torch.sum(w * (t[..., 1:] - t[..., :-1] - w[..., 1:] * (t[..., 1:] - t[..., :-1])) ** 2)\n\n    # return the total distortion loss\n    return inter_interval_loss + intra_interval_loss\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check that the tensors have the same shape\n    if t.shape != w.shape:\n        raise ValueError(\"The tensors must have the same shape.\")\n\n    # Check that the weights sum to 1\n    if not torch.allclose(w.sum(), torch.tensor(1.0)):\n        raise ValueError(\"The weights must sum to 1.\")\n\n    # Check that the percentiles are in the range [0, 1]\n    if not all(0 <= p <= 1 for p in ps):\n        raise ValueError(\"The percentiles must be in the range [0, 1].\")\n\n    # Check that the percentiles are sorted\n    if not sorted(ps) == ps:\n        raise ValueError(\"The percentiles must be sorted.\")\n\n    # Check that the percentiles are unique\n    if not len(ps) == len(set(ps)):\n        raise ValueError(\"The percentiles must be unique.\")\n\n    # Check that the percentiles are a list\n    if not isinstance(ps, list):\n        raise ValueError(\"The percentiles must be a list.\")\n\n    # Check that the percentiles are a list of floats\n    if not all(isinstance(p, float) for p in ps):\n        raise ValueError(\"The percentiles must be a list of floats.\")\n\n    # Check that the percentiles are a list of floats\n    if not all(isinstance(p, float) for p in ps):\n        raise ValueError(\"The percentiles must be a list of floats.\")\n\n    # Check that the percentiles are a list of floats\n    if not all(isinstance(p, float) for p in ps):\n        raise ValueError(\"The percentiles must be a list of floats.\")\n\n    # Check that the percentiles are a list of floats\n    if not all(isinstance(p, float) for p in ps):\n        raise ValueError(\"The percentiles must be a list of floats.\")\n\n    # Check that the percentiles are a list of floats\n    if not all(isinstance(p, float) for p in ps):\n        raise ValueError(\"The percentiles must be"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Check that the input tensors are 1D\n    if t.ndim != 1:\n        raise ValueError(\"The tensor t must be 1D.\")\n    if w.ndim != 1:\n        raise ValueError(\"The tensor w must be 1D.\")\n\n    # Check that the input tensors have the same length\n    if t.shape[0] != w.shape[0]:\n        raise ValueError(\"The tensor t and w must have the same length.\")\n\n    # Check that the input tensors are sorted\n    if not torch.all(t[:-1] <= t[1:]):\n        raise ValueError(\"The tensor t must be sorted.\")\n\n    # Check that the input tensors are non-negative\n    if torch.any(w < 0) or torch.any(t < 0):\n        raise ValueError(\"The tensor w and t must be non-negative.\")\n\n    # Check that the input tensors are finite\n    if torch.any(torch.isnan(w)) or torch.any(torch.isnan(t)):\n        raise ValueError(\"The tensor w and t must be finite.\")\n\n    # Check that the number of samples is positive\n    if num_samples < 1:\n        raise ValueError(\"The number of samples must be positive.\")\n\n    # Check that the perturbation flag is a boolean\n    if not isinstance(perturb, bool):\n        raise ValueError(\"The perturb flag must be a boolean.\")\n\n    # Check that the single jitter flag is a boolean\n    if not isinstance(single_jitter, bool):\n        raise ValueError(\"The single jitter flag must be a boolean.\")\n\n    # Check that the perturbation flag is consistent with the single jitter flag\n    if perturb and single_jitter:\n        raise ValueError(\"The perturb flag cannot be True if the single jitter flag is True.\")\n\n    # Check that the number of samples is less than the number of bins\n    if num_samples > t.shape[0]:\n        raise ValueError(\"The number of samples must be less than the number of bins.\")\n\n    # Check that the number of samples is"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Check if the dilation parameter is valid\n    if dilation <= 0:\n        raise ValueError(\"The dilation parameter must be a positive number.\")\n\n    # Check if the domain is valid\n    if domain[0] >= domain[1]:\n        raise ValueError(\"The domain must be a tuple of two numbers, where the first number is less than the second number.\")\n\n    # Check if the time steps and weights are valid\n    if t.shape[0] != w.shape[0]:\n        raise ValueError(\"The time steps and weights must have the same length.\")\n\n    # Check if the time steps and weights are non-negative\n    if torch.any(t < 0) or torch.any(w < 0):\n        raise ValueError(\"The time steps and weights must be non-negative.\")\n\n    # Check if the time steps are in ascending order\n    if torch.any(t[:-1] >= t[1:]):\n        raise ValueError(\"The time steps must be in ascending order.\")\n\n    # Check if the weights are non-negative\n    if torch.any(w < 0):\n        raise ValueError(\"The weights must be non-negative.\")\n\n    # Check if the weights are non-zero\n    if torch.all(w == 0):\n        raise ValueError(\"At least one weight must be non-zero.\")\n\n    # Check if the weights are normalized\n    if torch.abs(torch.sum(w) - 1) > 1e-12:\n        raise ValueError(\"The weights must be normalized.\")\n\n    # Dilate the time steps\n    t_dilated = torch.arange(domain[0], domain[1], dilation, device=t.device)\n\n    # Find the indices of the dilated time steps that are within the domain\n    t_dilated_indices = torch.searchsorted(t, t_dilated)\n\n    # Clip the dilated time steps to the domain\n    t_dilated = t_dilated.clamp(min=domain[0], max=domain[1])\n\n    # Ad"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Find the index of the first step change that occurs after the query time.\n    t_ind = tf.searchsorted(t, tq, side='left', out_type=tf.int32)\n\n    # If the query time exactly matches a step change time, return the outside value.\n    yq = tf.where(tf.equal(tf.gather(t, t_ind), tq), outside_value, tf.gather(y, t_ind))\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # get the time dimension\n    t_dim = t.shape[-1]\n\n    # get the weights dimension\n    w_dim = w.shape[-1]\n\n    # get the number of intervals\n    n_intervals = t_dim - 1\n\n    # get the interval length\n    interval_length = 1.0 / n_intervals\n\n    # get the interval index\n    interval_index = torch.floor(t * n_intervals).long()\n\n    # get the interval start and end\n    interval_start = interval_index * interval_length\n    interval_end = interval_start + interval_length\n\n    # get the interval length\n    interval_length = interval_end - interval_start\n\n    # get the interval center\n    interval_center = interval_start + interval_length / 2.0\n\n    # get the interval bias\n    interval_bias = interval_center - train_frac\n\n    # get the interval weight\n    interval_weight = 1.0 / (1.0 + torch.exp(-anneal_slope * interval_bias))\n\n    # get the interval weight mask\n    interval_weight_mask = (interval_weight > 0.0).float()\n\n    # get the interval weight\n    interval_weight = interval_weight * interval_weight_mask\n\n    # get the interval weight\n    interval_weight = interval_weight + (interval_weight == 0.0).float() * eps\n\n    # get the interval weight\n    interval_weight = interval_weight / interval_weight.sum(dim=-1, keepdim=True)\n\n    # get the interval weight\n    interval_weight = interval_weight.unsqueeze(dim=-1)\n\n    # get the interval weight\n    interval_weight = interval_weight.repeat_interleave(w_dim, dim=-1)\n\n    # get the adjusted weights\n    w_adj = w * interval_weight\n\n    return w_adj\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, list):\n        if ignore_list:\n            return batch\n        else:\n            return [to_cuda(el, device, ignore_list) for el in batch]\n    elif isinstance(batch, tuple):\n        return tuple([to_cuda(el, device, ignore_list) for el in batch])\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return batch\n        else:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # get the batch dimension of the vertices tensor\n    batch_dim = v.shape[0]\n\n    # get the number of vertices per batch\n    num_verts_per_batch = v.shape[1]\n\n    # get the number of faces per batch\n    num_faces_per_batch = f.shape[1]\n\n    # get the number of vertices per face\n    num_verts_per_face = f.shape[2]\n\n    # get the number of dimensions of the vertices\n    num_dims_per_vert = v.shape[3]\n\n    # get the number of dimensions of the faces\n    num_dims_per_face = f.shape[3]\n\n    # get the number of dimensions of the triangles\n    num_dims_per_tri = num_dims_per_face * num_verts_per_face\n\n    # get the number of triangles per face\n    num_tris_per_face = num_verts_per_face - 2\n\n    # get the number of triangles per batch\n    num_tris_per_batch = num_faces_per_batch * num_tris_per_face\n\n    # get the number of triangles per batch\n    num_tris_per_batch = num_faces_per_batch * num_tris_per_face\n\n    # get the number of triangles per batch\n    num_tris_per_batch = num_faces_per_batch * num_tris_per_face\n\n    # get the number of triangles per batch\n    num_tris_per_batch = num_faces_per_batch * num_tris_per_face\n\n    # get the number of triangles per batch\n    num_tris_per_batch = num_faces_per_batch * num_tris_per_face\n\n    # get the number of triangles per batch\n    num_tris_per_batch = num_faces_per_batch * num_tris_per_face\n\n    # get the number of triangles per batch\n    num_tris_per_batch = num_faces"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    # if the input is a tuple, recursively call this function on each element of the tuple\n    if isinstance(batch, tuple):\n        return tuple(add_batch(item) for item in batch)\n\n    # if the input is a list, recursively call this function on each element of the list\n    elif isinstance(batch, list):\n        return [add_batch(item) for item in batch]\n\n    # if the input is a dictionary, recursively call this function on each element of the dictionary\n    elif isinstance(batch, dict):\n        return {key: add_batch(item) for key, item in batch.items()}\n\n    # if the input is a torch.Tensor or a np.ndarray, add a new dimension at the zeroth position\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n\n    # if the input is a scalar, return it as a list\n    else:\n        return [batch]\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Convert all the camera parameters into tensors\n        batch = dotdict({\n            'image_size': torch.tensor(self.image_size, dtype=torch.float32),\n            'focal_length': torch.tensor(self.focal_length, dtype=torch.float32),\n            'focal_plane_size': torch.tensor(self.focal_plane_size, dtype=torch.float32),\n            'principal_point_offset': torch.tensor(self.principal_point_offset, dtype=torch.float32),\n            'rotation_vector': torch.tensor(self.rotation_vector, dtype=torch.float32),\n            'translation_vector': torch.tensor(self.translation_vector, dtype=torch.float32),\n            'camera_matrix': torch.tensor(self.camera_matrix, dtype=torch.float32),\n            'camera_matrix_inv': torch.tensor(self.camera_matrix_inv, dtype=torch.float32),\n            'distortion_coefficients': torch.tensor(self.distortion_coefficients, dtype=torch.float32),\n            'image_path': self.image_path,\n            'image_size_original': self.image_size_original,\n            'image_size_render': self.image_size_render,\n            'image_size_display': self.image_size_display,\n            'image_size_display_actual': self.image_size_display_actual,\n            'image_size_input': self.image_size_input,\n            'image_size_input_actual': self.image_size_input_actual,\n            'image_size_input_resized': self.image_size_input_resized,\n            'image_size_input_resized_actual': self.image_size_input_resized_actual,\n            'image_size_input_cropped': self.image_size_input_cropped,\n            'image_size_input_cropped_"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = self.serialize_agent_state(agent)\n            self.save_agent_state(agent_state)\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Find the closest agent to the given purpose embedding\n        closest_agent = None\n        max_similarity_score = -np.inf\n        for agent in self.agents:\n            similarity_score = self.cosine_similarity(agent.purpose_embedding, purpose_embedding)\n            if similarity_score > max_similarity_score:\n                max_similarity_score = similarity_score\n                closest_agent = agent\n\n        return closest_agent, max_similarity_score\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agent_list.append(\n            Agent(\n                prompt=self.prompt,\n                name=self.name,\n                weight=self.weight,\n                prime=True,\n                prime_flag=True,\n                unspecified_flag=True,\n            )\n        )\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        # Get the agent from the database\n        agent_json = self.get_agent(purpose)\n\n        # If the agent is not found, return None\n        if agent_json is None:\n            return None\n\n        # Deserialize the agent\n        agent = self.deserialize_agent(agent_json, agent_lifecycle, openai_wrapper)\n\n        return agent\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # get all agents from the database\n        agents = self.get_all_agents()\n\n        # load each agent\n        for agent in agents:\n            # get the agent's class name\n            class_name = agent['class_name']\n\n            # get the agent's purpose\n            purpose = agent['purpose']\n\n            # get the agent's parameters\n            parameters = agent['parameters']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent's state\n            state = agent['state']\n\n            # get the agent'"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save(agent)\n        except Exception as e:\n            self.logger.exception(f'Error saving agent {agent}: {e}')\n            raise\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Clean up the agents\n        self.clean_up_agents()\n\n        # Return the current list of agents\n        return self.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        # generate prompt\n        prompt = f\"The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\\n\\nHuman: {sample_input}\\nAI: \"\n\n        # attempt to get completion\n        try:\n            completion = self.openai_api.create_completion(\n                engine=\"davinci\",\n                prompt=prompt,\n                temperature=0.9,\n                max_tokens=150,\n                top_p=1,\n                frequency_penalty=0,\n                presence_penalty=0.6,\n                stop=[\"\\n\", \" Human:\", \" AI:\"]\n            )\n\n            # get text from completion response\n            text = completion.choices[0].text\n\n            # return text\n            return f\"{prompt}{text}\"\n\n        # if an error occurs, log it and return an empty string\n        except Exception as e:\n            self.logger.error(f\"Error when attempting to generate prompt for LLM: {e}\")\n            return \"\"\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Get the agent's ID\n        agent_id = agent_dict['id']\n\n        # Get the agent's purpose\n        agent_purpose = agent_dict['purpose']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_data = agent_dict['data']\n\n        # Get the agent's data\n        agent_"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Get the agent data from the database\n        agent_data = self.fetch_agent_data(purpose)\n\n        # If no agent is found, return None\n        if agent_data is None:\n            return None\n\n        # Deserialize the agent data\n        agent = self.deserialize_agent(agent_data)\n\n        # Return the agent\n        return agent\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # connect to the database\n        conn = sqlite3.connect(self.filename)\n\n        # load all purposes\n        c = conn.cursor()\n        c.execute('SELECT purpose FROM agent')\n        purposes = c.fetchall()\n\n        # close the database connection\n        conn.close()\n\n        # return the list of purposes\n        return [purpose[0] for purpose in purposes]\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Get the cursor\n        cursor = self.conn.cursor()\n\n        # Execute the query\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n\n        # Fetch the result\n        result = cursor.fetchone()\n\n        # If the result is found\n        if result is not None:\n\n            # Load the result from JSON\n            result = json.loads(result[0])\n\n        # Return the result\n        return result\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Create a cursor to execute SQL queries.\n        cursor = self.connection.cursor()\n\n        # Insert the result into the cache.\n        cursor.execute('''\n            INSERT INTO cache (arg_hash, result)\n            VALUES (?, ?)\n        ''', (arg_hash, json.dumps(result)))\n\n        # Commit the changes to the database.\n        self.connection.commit()\n\n        # Close the cursor.\n        cursor.close()\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters\n    global_config.update_global_config_from_args(args)\n\n    # Redirect standard output to a file if quiet mode is enabled\n    if quiet_mode:\n        sys.stdout = open(global_config.quiet_mode_file_path, 'w')\n\n    # Execute the command line process\n    if args.command == 'train':\n        train_model.execute_train_model_command(args)\n    elif args.command == 'test':\n        test_model.execute_test_model_command(args)\n    elif args.command == 'predict':\n        predict_model.execute_predict_model_command(args)\n    elif args.command == 'evaluate':\n        evaluate_model.execute_evaluate_model_command(args)\n    elif args.command == 'visualize':\n        visualize_model.execute_visualize_model_command(args)\n    elif args.command == 'analyze':\n        analyze_model.execute_analyze_model_command(args)\n    elif args.command == 'optimize':\n        optimize_model.execute_optimize_model_command(args)\n    elif args.command == 'optimize-hyperparameters':\n        optimize_hyperparameters.execute_optimize_hyperparameters_command(args)\n    elif args.command == 'optimize-features':\n        optimize_features.execute_optimize_features_command(args)\n    elif args.command == 'optimize-ensemble':\n        optimize_ensemble.execute_optimize_ensemble_command(args)\n    elif args.command == 'optimize-threshold':\n        optimize_threshold.execute_optimize_threshold_command(args)\n    elif args.command == 'optimize-all':\n        optimize_all.execute_optimize_all_command(args)\n    elif args.command == 'optimize-hyperparameters-all':\n        optimize_hyperparameters_all.execute_optimize_hyperparameters_all_command(args)\n    elif args.command == 'optimize-features-all':\n        optimize_features_all.execute_optimize_features_all"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Import the necessary modules\n        import requests\n        import json\n        import os\n\n        # Set the API key\n        api_key = os.getenv('OPENAI_API_KEY')\n\n        # Set the API endpoint\n        api_endpoint = 'https://api.openai.com/v1/engines/davinci-codex/completions'\n\n        # Set the request headers\n        headers = {\n            'Content-Type': 'application/json',\n            'Authorization': 'Bearer ' + api_key\n        }\n\n        # Set the request body\n        body = {\n            'prompt': kwargs['prompt'],\n            'max_tokens': kwargs['max_tokens'],\n            'temperature': kwargs['temperature'],\n            'top_p': kwargs['top_p'],\n            'n': kwargs['n'],\n            'stream': kwargs['stream'],\n            'logprobs': kwargs['logprobs'],\n            'stop': kwargs['stop']\n        }\n\n        # Set the model\n        if 'model' in kwargs:\n            model = kwargs['model']\n        else:\n            model = 'davinci-codex'\n\n        # Set the fallback models\n        fallback_models = ['davinci', 'davinci-codex', 'davinci-codex/codex-java', 'davinci-codex/codex-python', 'davinci-codex/codex-ruby', 'davinci-codex/codex-javascript', 'davinci-codex/codex-php', 'davinci-codex/codex-go', 'davinci-codex/codex-html', 'davinci-codex/codex-css', 'davinci-codex/codex-csharp', 'davinci-codex/codex-c', 'davinci-codex/codex-cpp', 'davinci-codex/codex-pytorch', 'davinci-codex/codex-tensorflow', 'davinci-codex/codex-paddle"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        # If the client does not exist or the credentials have expired, create a new client.\n        if not hasattr(self, 'client') or self.client_expired():\n            self.client = boto3.client(\n                's3',\n                aws_access_key_id=self.access_key,\n                aws_secret_access_key=self.secret_key,\n                region_name=self.region\n            )\n\n        return self.client\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if num_workers > 1:\n            raise RuntimeError(\n                \"The `state_dict` method must be called from the main process only. \"\n                \"To save the state of the dataset, call `state_dict` from the main process and \"\n                \"restore the state by passing the state dictionary to the `load_state_dict` method.\"\n            )\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_path\": self.input_path,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader is not None else None,\n            \"drop_last_batch\": self.drop_last_batch,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        # Load the state dictionary\n        self.state_dict = state_dict\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_state = self.state_dict['current_state']\n\n        # Set the current state of the dataset\n        self.current_"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if the state dictionary is empty\n        if not self._state_dict:\n            return\n\n        # Check if the state dictionary is consistent with the current state of the StreamingDataset instance\n        if self._state_dict['shuffle'] != self.shuffle:\n            raise ValueError(\n                f\"The state dictionary is inconsistent with the current state of the StreamingDataset instance. The value of the 'shuffle' key in the state dictionary is {self._state_dict['shuffle']}, but the current value of the shuffle attribute is {self.shuffle}\"\n            )\n\n        if self._state_dict['num_workers'] != self.num_workers:\n            raise ValueError(\n                f\"The state dictionary is inconsistent with the current state of the StreamingDataset instance. The value of the 'num_workers' key in the state dictionary is {self._state_dict['num_workers']}, but the current value of the num_workers attribute is {self.num_workers}\"\n            )\n\n        if self._state_dict['input_path'] != self.input_path:\n            raise ValueError(\n                f\"The state dictionary is inconsistent with the current state of the StreamingDataset instance. The value of the 'input_path' key in the state dictionary is {self._state_dict['input_path']}, but the current value of the input_path attribute is {self.input_path}\"\n            )\n\n        if self._state_dict['url'] != self.url:\n            raise ValueError(\n                f\"The state dictionary is inconsistent with the current state of the StreamingDataset instance. The value of the 'url' key in the state dictionary is {self._state_dict['url']}, but the current value of the url attribute is {self.url}\"\n            )\n\n        if self._state_dict['seed'] != self.seed:\n            raise ValueError(\n                f\"The state dictionary is inconsistent with the current state of the StreamingDataset instance. The value of the 'seed' key in the state dictionary is {self._state_dict['seed']}, but the current value of the seed attribute is {self"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # Import the necessary libraries\n    import os\n    import hashlib\n    import shutil\n\n    # Check if the input directory is None\n    if input_dir is None:\n\n        # Set the input directory to an empty string\n        input_dir = \"\"\n\n    # Hash the input directory\n    cache_dir = os.path.join(\n        os.getenv(\"CACHE_DIR\", os.path.join(os.getcwd(), \"cache\")),\n        hashlib.md5(input_dir.encode(\"utf-8\")).hexdigest(),\n    )\n\n    # Check if the cache directory exists\n    if not os.path.exists(cache_dir):\n\n        # Create the cache directory\n        os.makedirs(cache_dir)\n\n    # Return the cache directory\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL.\n        if not self.is_s3_url(remote_filepath):\n            raise ValueError(f'The remote file path must be an S3 URL. The given remote file path is: {remote_filepath}')\n\n        # Check if the local file already exists.\n        if os.path.exists(local_filepath):\n            return\n\n        # Get the S3 bucket and key from the remote file path.\n        s3_bucket, s3_key = self.parse_s3_url(remote_filepath)\n\n        # Create the local directory if it does not exist.\n        local_directory = os.path.dirname(local_filepath)\n        if not os.path.exists(local_directory):\n            os.makedirs(local_directory)\n\n        # Create a lock file to prevent multiple processes from attempting to download the same file simultaneously.\n        lock_filepath = local_filepath + '.lock'\n        lock = FileLock(lock_filepath)\n        try:\n            with lock.acquire(timeout=self.lock_timeout):\n\n                # Check if the local file already exists.\n                if os.path.exists(local_filepath):\n                    return\n\n                # Download the file using the s5cmd command-line tool if it is available.\n                if shutil.which('s5cmd') is not None:\n                    self.download_file_s5cmd(s3_bucket, s3_key, local_filepath)\n\n                # Download the file using the boto3 library if the s5cmd command-line tool is not available.\n                else:\n                    self.download_file_boto3(s3_bucket, s3_key, local_filepath)\n\n        except TimeoutError:\n            raise TimeoutError(f'The lock for the file {local_filepath} could not be acquired within {self.lock_timeout} seconds.')\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize the return value\n    chunks_dict = {}\n    intervals_dict = {}\n\n    # Distribute chunks and intervals across workers\n    for worker_idx in range(num_workers):\n        # Get the indices of chunks and intervals to be assigned to the current worker\n        chunk_indices = _get_chunk_indices(\n            worker_idx, worker_env.world_size, chunks_replica\n        )\n        interval_indices = _get_interval_indices(\n            worker_idx, worker_env.world_size, intervals_replica\n        )\n\n        # Get the chunks and intervals corresponding to the indices\n        chunks = [chunks_replica[idx] for idx in chunk_indices]\n        intervals = [intervals_replica[idx] for idx in interval_indices]\n\n        # Add the chunks and intervals to the return value\n        chunks_dict[worker_idx] = chunks\n        intervals_dict[worker_idx] = intervals\n\n    return chunks_dict, intervals_dict\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith('local:'):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)\n\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Get the image's dimensions and mode\n        width, height = item.size\n        mode = item.mode\n\n        # Get the length of the mode\n        mode_length = len(mode)\n\n        # Get the raw pixel data\n        data = item.tobytes()\n\n        # Create a bytes object containing the image's dimensions, mode length, and raw pixel data\n        serialized_data = struct.pack('<II{}s{}s'.format(mode_length, len(data)), width, height, mode.encode('utf-8'), data)\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image.Image):\n            if item.format == 'JPEG' and os.path.exists(item.filename):\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n            else:\n                return item.tobytes(), None\n        else:\n            raise TypeError('The item is not an image.')\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        # Extract the width, height, and mode from the data.\n        width, height, mode_len = struct.unpack('<III', data[:12])\n\n        # Extract the mode string.\n        mode = data[12:12 + mode_len]\n\n        # Extract the raw image data.\n        img_data = data[12 + mode_len:]\n\n        # Reconstruct the image.\n        return cls(img_data, mode, size=(width, height))\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the tensor's dtype and shape information from the first 16 bytes\n        dtype = data[:16].decode('ascii')\n        shape = data[16:].decode('ascii')\n\n        # Convert the dtype and shape strings to their corresponding python types\n        dtype = eval(dtype)\n        shape = eval(shape)\n\n        # Convert the byte array to a numpy array with the correct dtype and shape\n        np_array = np.frombuffer(data[16 + len(shape):], dtype=dtype)\n        np_array = np.reshape(np_array, shape)\n\n        # Convert the numpy array to a torch tensor\n        tensor = torch.from_numpy(np_array)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a byte array\n        byte_array = item.numpy().tobytes()\n\n        # Serialize the dtype and shape\n        dtype_shape_bytes = np.lib.format.dtype_to_descr(item.dtype) + str(item.shape).encode('utf-8')\n\n        # Combine the dtype and shape with the byte array\n        return dtype_shape_bytes + byte_array, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            if torchvision_available:\n                return JpegImageFile(torchvision.io.decode_jpeg(data))\n            else:\n                return JpegImageFile(PIL.Image.open(io.BytesIO(data)))\n        except RuntimeError:\n            return JpegImageFile(PIL.Image.open(io.BytesIO(data)))\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Get the data type index of the tensor\n        dtype_index = self.dtype_index_mapping[str(item.dtype)]\n\n        # Serialize the tensor\n        return item.numpy().tobytes(), f\"no_header_tensor:{dtype_index}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.tensor(np.frombuffer(data, dtype=self._dtype))\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte data\n        data_type, shape = self.extract_data_type_and_shape(data)\n\n        # Extract the data from the byte data\n        data = data[16:]\n\n        # Reconstruct the numpy array\n        return np.frombuffer(data, dtype=data_type).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{item.dtype.num}\"\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # serialize the data type index\n        data_type_index = self.data_type_index_map[item.dtype.name]\n        data_type_index_bytes = data_type_index.to_bytes(1, byteorder='big', signed=False)\n\n        # serialize the number of dimensions\n        num_dims = len(item.shape)\n        num_dims_bytes = num_dims.to_bytes(1, byteorder='big', signed=False)\n\n        # serialize the size of each dimension\n        dim_sizes = b''\n        for dim_size in item.shape:\n            dim_sizes += dim_size.to_bytes(4, byteorder='big', signed=False)\n\n        # serialize the array's binary content\n        array_bytes = item.tobytes()\n\n        # combine the serialized components\n        return data_type_index_bytes + num_dims_bytes + dim_sizes + array_bytes, None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        # Check if torchvision and av are installed\n        try:\n            import torchvision\n            import av\n        except ImportError:\n            raise ImportError(\n                \"Please install torchvision and av to deserialize videos.\"\n            )\n\n        # Write the data to a temporary file\n        with tempfile.NamedTemporaryFile(suffix='.mp4') as f:\n            f.write(data)\n            f.seek(0)\n\n            # Use torchvision's read_video function to deserialize the video file into a video object\n            return torchvision.io.read_video(f.name)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Check if the writing process is already complete\n        if self._is_done:\n            return self._written_chunks\n\n        # Write any remaining chunks\n        if self._should_write():\n            self.write_chunk()\n\n        # Write the index file\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        return self._written_chunks\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        # Update the current epoch, the number of samples yielded, and the latest worker index.\n        self.epoch = obj['epoch']\n        self.num_yielded = obj['num_yielded']\n        self.latest_worker_idx = obj['latest_worker_idx']\n\n        # Prepare the DataLoader for resuming by adjusting internal iterators and flags.\n        self.end_of_epoch = False\n        self.num_active_workers = 0\n        self.num_retired_workers = 0\n        self.num_yielded_this_epoch = 0\n        self.workers_done = []\n\n        # Update the dataset's state if it is a StreamingDataset or a CombinedStreamingDataset.\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            for dataset in self.dataset.datasets:\n                if isinstance(dataset, StreamingDataset):\n                    dataset.load_state_dict(obj['dataset'][dataset.name])\n\n        return\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None and num_samples_yielded is None:\n            return {}\n        elif self.iterator is None:\n            return {\n                'num_workers': num_workers,\n                'batch_size': batch_size,\n                'num_samples_yielded': num_samples_yielded,\n            }\n        else:\n            return self.iterator.state_dict()\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # load the state of each dataset\n        for dataset_name, dataset in self.datasets.items():\n            dataset.load_state_dict(state_dict[dataset_name])\n\n        # update the number of samples yielded by the streaming dataloader\n        self.num_samples = self.datasets[self.dataset_names[0]].num_samples\n\n        return None\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    # Initialize the Dir object\n    dir_obj = Dir()\n\n    # If the input is a Dir object, return it\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    # If the input is a string, set the path attribute\n    if isinstance(dir_path, str):\n        dir_obj.path = dir_path\n\n    # If the path is a local path, set the local path attribute\n    if dir_obj.path.startswith(\"/\"):\n        dir_obj.local_path = dir_obj.path\n\n    # If the path is an S3 path, set the S3 path attribute\n    if dir_obj.path.startswith(\"s3://\"):\n        dir_obj.s3_path = dir_obj.path\n\n    # If the path is a project path, set the project path attribute\n    if dir_obj.path.startswith(\"project://\"):\n        dir_obj.project_path = dir_obj.path\n\n    # Return the Dir object\n    return dir_obj\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    # Check if the output_dir is a Dir object\n    if not isinstance(output_dir, Dir):\n        raise TypeError(\"The output_dir must be a Dir object.\")\n\n    # Check if the output_dir starts with \"s3://\"\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"The output_dir must be an S3 bucket.\")\n\n    # Check if the output_dir is empty\n    if not output_dir.is_empty():\n        raise ValueError(\"The output_dir is not empty.\")\n\n    # Check if appending is allowed\n    if append:\n        raise NotImplementedError(\"Appending is not implemented yet.\")\n\n    # Check if overwriting is allowed\n    if overwrite:\n        raise NotImplementedError(\"Overwriting is not implemented yet.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the directory is an S3 bucket directory.\n    if not output_dir.is_s3_bucket_dir:\n        raise ValueError(f\"The directory {output_dir.name} is not an S3 bucket directory.\")\n\n    # Check if the directory contains an index file.\n    if output_dir.has_index_file:\n        raise ValueError(f\"The directory {output_dir.name} already contains an index file.\")\n\n    # Delete all objects in the directory.\n    output_dir.delete_all_objects()\n\n    return None"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None or node_rank == 0:\n            # wait for all workers to finish\n            while len(os.listdir(self.cache_dir)) < num_workers:\n                time.sleep(1)\n\n            # merge all parts\n            self.merger.merge(self.cache_dir)\n\n            # wait for master node to finish\n            while not os.path.exists(self.index_abspath):\n                time.sleep(1)\n\n        else:\n            # wait for master node to finish\n            while not os.path.exists(self.index_abspath):\n                time.sleep(1)\n\n        # copy to the final location\n        shutil.move(self.index_abspath, self.index_abspath_final)\n\n        # copy to the final location\n        shutil.move(self.index_abspath, self.index_abspath_final)\n\n        # remove all cached files\n        shutil.rmtree(self.cache_dir)\n\n        # finalize\n        self.finalize()\n\n        return None\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Import the required SDK\n    try:\n        from studio.client import StudioClient\n        from studio.util.util import parse_verbosity\n        from studio.credentials.credentials import Credentials\n        from studio.local_queue import LocalQueue\n        from studio.logs import Logs\n        from studio.util.logs import reset_logging\n        from studio.util.util import check_for_kb_interrupt\n    except ImportError:\n        raise ImportError(\n            \"The studio SDK is not installed. Please install it using 'pip install studio-client'.\"\n        )\n\n    # Get the default machine configuration\n    if not machine:\n        machine = Machine.get_default()\n\n    # Get the default command\n    if not command:\n        command = f\"cd {os.getcwd()} && {os.environ['PATH_TO_PYTHON']} -m studio_lib.runner\"\n\n    # Get the default credentials\n    credentials = Credentials.get_credentials()\n\n    # Create a Studio client\n    client = StudioClient(credentials)\n\n    # Create a local queue\n    local_queue = LocalQueue(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n        client=client,\n        logger=Logs(),\n        verbose=parse_verbosity(\"0\"),\n    )\n\n    # Submit the job\n    local_queue.submit()\n\n    # Wait for the job to start\n    while not local_queue.job_url:\n        time.sleep(1)\n\n    # Print the job URL\n    print(f\"Job URL: {local_queue.job_url}\")\n\n    # Wait for the job to finish\n    local_queue.wait()\n\n    # Check for keyboard interrupts\n    check_for_kb_interrupt()\n\n    # Check for errors\n    if local_queue.status != \"finished\":\n        raise Exception(f\"Job failed with status {local_queue.status}\")\n\n    # Reset the logging\n    reset_logging()\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.deletion_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # check if the config file exists\n        config_file_path = os.path.join(self._cache_dir, 'chunks.json')\n        if not os.path.exists(config_file_path):\n            return None\n\n        # load the config file\n        with open(config_file_path, 'r') as f:\n            config_dict = json.load(f)\n\n        # create the config object\n        config = ChunksConfig(\n            chunks=config_dict['chunks'],\n            chunk_size=config_dict['chunk_size'],\n            chunk_keys=config_dict['chunk_keys'],\n            chunk_key_prefix=config_dict['chunk_key_prefix'],\n            chunk_key_suffix=config_dict['chunk_key_suffix'],\n            chunk_key_separator=config_dict['chunk_key_separator'],\n            chunk_key_format=config_dict['chunk_key_format'],\n            chunk_key_regex=config_dict['chunk_key_regex'],\n            chunk_key_replacement=config_dict['chunk_key_replacement'],\n            chunk_key_replacement_regex=config_dict['chunk_key_replacement_regex'],\n            chunk_key_replacement_value=config_dict['chunk_key_replacement_value'],\n            chunk_key_replacement_format=config_dict['chunk_key_replacement_format'],\n            chunk_key_replacement_regex_flags=config_dict['chunk_key_replacement_regex_flags'],\n            chunk_key_replacement_regex_group=config_dict['chunk_key_replacement_regex_group'],\n            chunk_key_replacement_regex_group_format=config_dict['chunk_key_replacement_regex_group_format'],\n            chunk_key_replacement_regex_group_separator=config_dict['chunk_key_replacement_regex_group_separator'],\n            chunk_key_replacement_regex_group_format_separator=config_dict['chunk_key_replacement_regex_group_format_separator'],\n            chunk"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError('Configuration not set.')\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"Index must be an instance of ChunkedIndex\")\n\n        if self.index_config is None:\n            raise Exception(\"Index configuration is not defined\")\n\n        # Get the chunk index and the item index within the chunk\n        chunk_index = index.chunk_index\n        item_index = index.item_index\n\n        # Get the chunk\n        chunk = self.get_chunk(chunk_index)\n\n        # Get the item\n        item = chunk.get_item(item_index)\n\n        # Return the item\n        return item\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    # import libraries\n    import dask.distributed\n\n    # check if running in distributed environment\n    try:\n        worker = dask.distributed.get_worker()\n        has_distributed_environment = True\n    except ValueError:\n        has_distributed_environment = False\n\n    # broadcast object\n    if has_distributed_environment:\n        # get distributed map\n        shared_map = worker.shared_state\n\n        # put object in map\n        shared_map[key] = obj\n\n        # get object from map\n        obj = shared_map[key]\n\n    return obj"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # get the rank of the current node\n    rank = distributed_env.rank\n\n    # get the number of ranks\n    world_size = distributed_env.world_size\n\n    # get the chunks assigned to the current node\n    chunks_per_rank = chunks_per_ranks[rank]\n\n    # get the number of chunks assigned to the current node\n    num_chunks_per_rank = len(chunks_per_rank)\n\n    # get the number of chunks assigned to each rank\n    num_chunks_per_rank_list = [len(chunks_per_ranks[i]) for i in range(world_size)]\n\n    # get the cumulative sum of the number of chunks assigned to each rank\n    cumsum_num_chunks_per_rank_list = np.cumsum(num_chunks_per_rank_list)\n\n    # get the cumulative sum of the number of chunks assigned to each rank, shifted by one\n    cumsum_num_chunks_per_rank_list_shifted = np.insert(\n        cumsum_num_chunks_per_rank_list, 0, 0\n    )[:-1]\n\n    # get the cumulative sum of the number of chunks assigned to each rank, shifted by one\n    cumsum_num_chunks_per_rank_list_shifted = np.insert(\n        cumsum_num_chunks_per_rank_list, 0, 0\n    )[:-1]\n\n    # get the cumulative sum of the number of chunks assigned to each rank, shifted by one\n    cumsum_num_chunks_per_rank_list_shifted = np.insert(\n        cumsum_num_chunks_per_rank_list, 0, 0\n    )[:-1]\n\n    # get the cumulative sum of the number of chunks assigned to each rank, shifted by one\n    cumsum_num_chunks_per_rank_list_shifted = np.insert(\n        cumsum_num_chunks_per_rank_list, 0"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Import standard modules ...\n    import os\n\n    # Extract the first two elements of the input sequence ...\n    if len(inputs) > 1:\n        a1 = inputs[0]\n        a2 = inputs[1]\n    else:\n        a1 = inputs[0]\n        a2 = None\n\n    # Check if the first element is a valid file path ...\n    if os.path.isfile(a1):\n        # Extract the directory name ...\n        a1 = os.path.dirname(a1)\n\n        # Check if the second element is a valid file path ...\n        if a2 is not None and os.path.isfile(a2):\n            # Extract the directory name ...\n            a2 = os.path.dirname(a2)\n\n            # Check that the two directory names are the same ...\n            if a1 != a2:\n                raise Exception(\"inconsistent file paths found\") from None\n\n    # Check if the first element is a valid directory path ...\n    if os.path.isdir(a1):\n        # Return the absolute path ...\n        return os.path.abspath(a1)\n\n    # Check if the second element is a valid directory path ...\n    if a2 is not None and os.path.isdir(a2):\n        # Return the absolute path ...\n        return os.path.abspath(a2)\n\n    # Return None ...\n    return None"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Import modules and define functions\n    import os\n    import socket\n    import subprocess\n    import sys\n    import time\n\n    def get_dns_context_state() -> bool:\n        \"\"\"\n        This function is designed to return the current state of DNS optimization.\n\n        Input-Output Arguments\n        :return: Bool, True if DNS optimization is enabled, False if DNS optimization is disabled.\n        \"\"\"\n\n        # Import modules and define functions\n        import os\n        import socket\n        import subprocess\n        import sys\n        import time\n\n        # Check if the OS is Windows\n        if os.name == \"nt\":\n            # Check if the OS is Windows 10\n            if sys.getwindowsversion().major >= 10:\n                # Check if DNS optimization is enabled\n                return bool(int(subprocess.check_output(\"powershell.exe Get-DnsClientGlobalSetting | Select-Object -expand EnableMulticast | Out-String\", shell=True).decode(\"utf-8\").replace(\"\\r\", \"\").replace(\"\\n\", \"\")))\n            else:\n                # Check if DNS optimization is enabled\n                return bool(int(subprocess.check_output(\"powershell.exe Get-DnsClientGlobalSetting | Select-Object -expand EnableMulticast | Out-String\", shell=True).decode(\"utf-8\").replace(\"\\r\", \"\").replace(\"\\n\", \"\")))\n        else:\n            # Check if DNS optimization is enabled\n            return bool(int(subprocess.check_output(\"cat /etc/resolv.conf | grep options | grep rotate | wc -l\", shell=True).decode(\"utf-8\").replace(\"\\r\", \"\").replace(\"\\n\", \"\")))\n\n    # Check if the OS is Windows\n    if os.name == \"nt\":\n        # Check if the OS is Windows 10\n        if sys.getwindowsversion().major >= 10:\n            # Check if DNS optimization is enabled\n            if get_dns_context_state() != enable:\n                # Check if DNS optimization should be enabled\n                if enable:\n                    # Enable DNS optimization\n                    subprocess.call(\"powershell.exe Set-DnsClient"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # get the world size\n    world_size = distributed_env.world_size\n\n    # get the total number of chunks\n    total_chunks = len(indexes)\n\n    # get the number of chunks each rank should process\n    chunks_per_rank = _get_chunks_per_rank(total_chunks, world_size, drop_last)\n\n    # get the number of chunks each rank should process\n    chunks_per_rank = _get_chunks_per_rank(total_chunks, world_size, drop_last)\n\n    # get the number of chunks each rank should process\n    chunks_per_rank = _get_chunks_per_rank(total_chunks, world_size, drop_last)\n\n    # get the number of chunks each rank should process\n    chunks_per_rank = _get_chunks_per_rank(total_chunks, world_size, drop_last)\n\n    # get the number of chunks each rank should process\n    chunks_per_rank = _get_chunks_per_rank(total_chunks, world_size, drop_last)\n\n    # get the number of chunks each rank should process\n    chunks_per_rank = _get_chunks_per_rank(total_chunks, world_size, drop_last)\n\n    # get the number of chunks each rank should process\n    chunks_per_rank = _get_chunks_per_rank(total_chunks, world_size, drop_last)\n\n    # get the number of chunks each rank should process\n    chunks_per_rank = _get_chunks_per_rank(total_chunks, world_size, drop_last)\n\n    # get the number of chunks each rank should process\n    chunks_per_rank = _get_chunks_per_rank(total_chunks, world_size, drop_last)\n\n    # get the number of chunks each rank should process\n    chunks_per_rank = _get_chunks_per_rank(total_chunks, world_size,"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        # Add the is_last flag to the keyword arguments if it's required\n        kwargs = {}\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n\n        # Apply the transformation function\n        self._fn(item_metadata, output_dir, **kwargs)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    import time\n    import botocore\n\n    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                time.sleep(sleep_time)\n                continue\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Import the necessary libraries\n    import os\n    from pathlib import Path\n    from typing import Any, Callable, List, Optional, Sequence, Union\n\n    from tqdm import tqdm\n\n    from .chunk import Chunk\n    from .chunk_processor import ChunkProcessor\n    from .chunk_writer import ChunkWriter\n    from .compression import get_compression_wrapper\n    from .executors import get_executor\n    from .utils import get_file_size, get_file_type, get_reader, get_size_str\n\n    # Check if the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Get the compression wrapper\n    compression_wrapper = get_compression_wrapper(compression)\n\n    # Get the reader\n    if reader is None:\n        reader = get_reader(inputs)\n\n    # Get the chunk writer\n    chunk_writer = ChunkWriter(\n        output_dir=output_dir,\n        compression=compression,\n        chunk_size=chunk_size,\n        chunk_bytes=chunk_bytes,\n        compression_wrapper=compression_wrapper,\n    )\n\n    # Get the chunk processor\n    chunk_processor = ChunkProcessor(\n        fn=fn,\n        chunk_writer=chunk_writer,\n        reader=reader,\n        batch_size=batch_size,\n    )\n\n    # Get the executor\n    executor = get_executor(\n        num_workers=num_workers,\n        num_nodes=num_nodes,\n        machine=machine,\n        num_downloaders=num_downloaders,\n        num_uploaders=num_uploaders,\n    )\n\n    # Get the list of inputs\n    inputs = reader.get_inputs(inputs)\n\n    # Get the total number of inputs\n    num_inputs = len(inputs)\n\n    # Get the total size of the inputs\n    total_size = sum(get_file_size(input) for input in inputs)\n\n    # Get the total number of chunks\n   "}
{"namespace": "litdata.processing.functions.map", "completion": "    # TODO: Implement map function\n    pass"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Import the necessary libraries\n    import os\n    from pathlib import Path\n    from tqdm import tqdm\n    from multiprocessing import cpu_count\n    from multiprocessing.pool import ThreadPool\n    from urllib.request import urlretrieve\n\n    # Create the cache directory if it does not exist\n    Path(cache_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a function to download a file\n    def download_file(source_path: str, destination_path: str) -> None:\n\n        \"\"\"\n        This function downloads a file from a source path to a destination path.\n\n        Input Arguments:\n        :param source_path: str. The path to the source file to be downloaded.\n        :param destination_path: str. The path to the destination file to be saved.\n        :return: None. There are no return values as the function's purpose is to perform side effects (downloading files).\n        \"\"\"\n\n        # Download the file\n        urlretrieve(source_path, destination_path)\n\n    # Create a function to check if a file exists\n    def file_exists(file_path: str) -> bool:\n\n        \"\"\"\n        This function checks if a file exists.\n\n        Input Arguments:\n        :param file_path: str. The path to the file to check.\n        :return: bool. A boolean value indicating whether the file exists.\n        \"\"\"\n\n        # Return the result of the file existence check\n        return os.path.isfile(file_path)\n\n    # Create a function to download a task\n    def download_task(task: tuple) -> None:\n\n        \"\"\"\n        This function downloads a task, which is a tuple of an index and a list of file paths to download.\n\n        Input Arguments:\n        :param task: tuple. A tuple of an index and a list of file paths to download.\n        :return: None. There are no return values as the function's purpose is to perform side effects (downloading files).\n        \"\"\"\n\n        # Get the task index and file paths\n        task_index, file_paths"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # Import the required modules\n    import os\n    import shutil\n    from urllib.parse import urlparse\n\n    # Import the required internal modules\n    from ._validate_fn import _validate_dir\n\n    # Check if the output directory is a valid directory\n    _validate_dir(output_dir)\n\n    # Continuously process items from the upload queue\n    while True:\n\n        # Get the next item from the upload queue\n        item = upload_queue.get()\n\n        # Check if the item is a tuple\n        if isinstance(item, tuple):\n\n            # Unpack the item\n            temp_dir, file_path = item\n\n            # Get the file name\n            file_name = os.path.basename(file_path)\n\n            # Get the file path\n            file_path = os.path.join(temp_dir, file_name)\n\n        # Check if the item is a string\n        elif isinstance(item, str):\n\n            # Get the file name\n            file_name = os.path.basename(item)\n\n            # Get the file path\n            file_path = os.path.join(cache_dir, file_name)\n\n        # Check if the item is None\n        elif item is None:\n\n            # Break the loop\n            break\n\n        # Get the file name\n        file_name = os.path.basename(file_path)\n\n        # Get the file path\n        file_path = os.path.join(cache_dir, file_name)\n\n        # Check if the file path exists\n        if os.path.exists(file_path):\n\n            # Get the file name\n            file_name = os.path.basename(file_path)\n\n            # Get the file path\n            file_path = os.path.join(cache_dir, file_name)\n\n            # Check if the output directory is a local directory\n            if urlparse(output_dir).scheme == \"file\":\n\n                # Get the file path\n                file_path = os.path.join(output_dir, file_name)\n\n                # Move the file to the"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Get the number of nodes and the current node's rank.\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    # Pack items greedily.\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n\n    # Print the packing details.\n    _print_packing_details(worker_items=worker_items, worker_weights=worker_weights, num_bins=world_size, file_size=file_size)\n\n    # Shuffle the items for each worker.\n    worker_items = [random.sample(worker_items[i], len(worker_items[i])) for i in range(world_size)]\n\n    # Return the items for this node.\n    return [worker_items[i] for i in worker_ids_this_node]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of nodes in the environment\n    num_nodes = _get_num_nodes()\n\n    # Calculate the total number of workers across all nodes\n    total_num_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_num_workers\n\n    # Calculate the number of items that will be left over\n    num_remainder_items = len(user_items) % total_num_workers\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = [num_items_per_worker for _ in range(num_workers)]\n\n    # Add any remainder items to the last workers\n    for i in range(num_remainder_items):\n        num_items_per_worker[-(i + 1)] += 1\n\n    # Get the current node's rank\n    node_rank = _get_node_rank()\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.cumsum(num_items_per_worker) - num_items_per_worker\n    end_indices = np.cumsum(num_items_per_worker)\n\n    # Get the start and end indices for the current node's workers\n    start_index = start_indices[num_workers * node_rank]\n    end_index = end_indices[num_workers * node_rank]\n\n    # Get the items for the current node's workers\n    items = user_items[start_index:end_index]\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return items\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Clean up cache directories\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n        os.makedirs(self.cache_dir)\n        if os.path.exists(self.cache_dir_train):\n            shutil.rmtree(self.cache_dir_train)\n        os.makedirs(self.cache_dir_train)\n        if os.path.exists(self.cache_dir_test):\n            shutil.rmtree(self.cache_dir_test)\n        os.makedirs(self.cache_dir_test)\n\n        return\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        if input_dir is not None:\n            if element.startswith(str(Path(input_dir).absolute())):\n                return True\n        if Path(element).exists():\n            return True\n    return False\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"Number of layers must be greater than 0.\"\n        assert n_neurons > 0, \"Number of neurons must be greater than 0.\"\n\n        if self.tcnn:\n            if n_neurons <= 256:\n                return tcnn.Network(\n                    n_input_dims,\n                    n_output_dims,\n                    n_neurons,\n                    n_hidden_layers=n_layers - 1,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n            else:\n                return tcnn.Network(\n                    n_input_dims,\n                    n_output_dims,\n                    n_neurons,\n                    n_hidden_layers=n_layers - 1,\n                    activation=activation,\n                    output_activation=output_activation,\n                    arch_type=\"G\",\n                )\n        else:\n            model = nn.Sequential()\n            if n_layers == 1:\n                model.add_module(\"linear\", nn.Linear(n_input_dims, n_output_dims))\n            else:\n                model.add_module(\"linear_0\", nn.Linear(n_input_dims, n_neurons))\n                if activation == \"ReLU\":\n                    model.add_module(\"activation_0\", nn.ReLU())\n                for i in range(n_layers - 2):\n                    model.add_module(f\"linear_{i + 1}\", nn.Linear(n_neurons, n_neurons))\n                    if activation == \"ReLU\":\n                        model.add_module(f\"activation_{i + 1}\", nn.ReLU())\n                model.add_module(\n                    f\"linear_{n_layers - 1}\", nn.Linear(n_neurons, n_output_dims)\n                )\n                if output_activation == \"ReLU\":\n                    model.add_module(f"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the rolling median of the signal\n        rolling_median = np.zeros(signal.shape)\n        for i in range(signal.shape[0]):\n            rolling_median[i] = np.median(signal[np.max([0, i - kernel_offset]):np.min([signal.shape[0], i + kernel_offset])])\n\n        # Trim the rolling median to remove edge effects\n        rolling_median = rolling_median[kernel_offset:np.min([rolling_median.shape[0], rolling_median.shape[0] - kernel_offset])]\n\n        return rolling_median\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the probe and gallery templates have the same size\n    if template_probe.template.shape != template_gallery.template.shape:\n        raise ValueError(\n            f\"The probe and gallery templates must have the same size. The probe template has size {template_probe.template.shape} and the gallery template has size {template_gallery.template.shape}.\"\n        )\n\n    # Check if the probe and gallery templates have the same size\n    if (\n        template_probe.template.shape[0]\n        != template_probe.template.shape[1]\n        != template_gallery.template.shape[0]\n        != template_gallery.template.shape[1]\n    ):\n        raise ValueError(\n            f\"The probe and gallery templates must be square matrices. The probe template has size {template_probe.template.shape} and the gallery template has size {template_gallery.template.shape}.\"\n        )\n\n    # Check if the rotation shift is valid\n    if rotation_shift < 0 or rotation_shift > template_probe.template.shape[0] - 1:\n        raise ValueError(\n            f\"The rotation shift must be a non-negative integer smaller than the template size. The rotation shift is {rotation_shift} and the template size is {template_probe.template.shape[0]}.\"\n        )\n\n    # Check if the nonmatch distance is valid\n    if nm_dist is not None and (nm_dist < 0 or nm_dist > 1):\n        raise ValueError(\n            f\"The nonmatch distance must be a float number between 0 and 1. The nonmatch distance is {nm_dist}.\"\n        )\n\n    # Check if the weights are valid\n    if weights is not None:\n        for weight in weights:\n            if (\n                weight.shape != template_probe.template.shape\n                or weight.shape != template_gallery.template.shape\n            ):\n                raise ValueError(\n                    f\"The weights must have the same size as the probe and gallery templates. The weights have size {weight."}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize the number of iterations to zero.\n        num_iterations = 0\n\n        # Initialize the number of points in the polygon.\n        num_points = polygon.shape[0]\n\n        # Initialize the number of points to choose from.\n        num_points_to_choose_from = num_points - 1\n\n        # Initialize the number of points to choose from.\n        num_points_to_choose_from = num_points - 1\n\n        # Initialize the number of points to choose from.\n        num_points_to_choose_from = num_points - 1\n\n        # Initialize the number of points to choose from.\n        num_points_to_choose_from = num_points - 1\n\n        # Initialize the number of points to choose from.\n        num_points_to_choose_from = num_points - 1\n\n        # Initialize the number of points to choose from.\n        num_points_to_choose_from = num_points - 1\n\n        # Initialize the number of points to choose from.\n        num_points_to_choose_from = num_points - 1\n\n        # Initialize the number of points to choose from.\n        num_points_to_choose_from = num_points - 1\n\n        # Initialize the number of points to choose from.\n        num_points_to_choose_from = num_points - 1\n\n        # Initialize the number of points to choose from.\n        num_points_to_choose_from = num_points - 1\n\n        # Initialize the number of points to choose from.\n        num_points_to_choose_from = num_points - 1\n\n        # Initialize the number of points to choose from.\n        num_points_to_choose_from = num_points - 1\n\n        # Initialize the number of points to choose from.\n        num_points_to_choose_from = num_points - 1\n\n        # Initialize the number of points to choose from.\n        num_points_to_choose_from = num_points - 1\n\n        # Initialize the number of points to choose from.\n        num_points_to"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self._execute_pre_hooks(*args, **kwargs)\n        result = self._run(*args, **kwargs)\n        self._execute_post_hooks(*args, **kwargs)\n\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output = json.loads(output)\n        except:\n            return False\n\n        return self.check_type(output, type_definition)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get the function's signature and type hints\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Get the input and output type hints\n        input_type_hint = type_hints['x']\n        output_type_hint = type_hints['return']\n\n        # Get the input and output class definitions\n        input_class_definition = get_class_definition(input_type_hint)\n        output_class_definition = get_class_definition(output_type_hint)\n\n        # Determine the function type\n        function_type = FunctionType.SYMBOLIC\n        if issubclass(output_class_definition.class_type, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n\n        # Get the function description\n        function_description = FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_class_definition=input_class_definition,\n            output_class_definition=output_class_definition,\n            function_type=function_type\n        )\n\n        return function_description\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for seed in self.seeds:\n            h = self.hash_function(string, seed)\n            self.bit_array[h % self.size] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        if self.persistence.exists(self.name):\n            bit_array = self.persistence.load(self.name)\n            if len(bit_array) != self.size:\n                self.logger.warning(\n                    \"Loaded bit array length does not match expected length. Reinitializing bit array and indices.\"\n                )\n                self.init_bit_array()\n                self.save()\n            else:\n                self.bit_array = bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Check if the string is present in the Bloom Filter\n        for hash_function in self.hash_functions:\n            index = hash_function(string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n\n        # If all bits are set, return True\n        return True\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        # update the distilled model\n        self.distilled_model = json_dict['distilled_model']\n\n        # update the current model stats\n        self.current_model_stats = json_dict['current_model_stats']\n\n        # update the last training run\n        self.last_training_run = json_dict['last_training_run']\n\n        # update the current training run\n        self.current_training_run = json_dict['current_training_run']\n\n        # update the number of training runs\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n\n        # update the teacher models\n        self.teacher_models = json_dict['teacher_models']\n\n        return self\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify the API key\n        if not self.api_key:\n            raise ValueError('The API key is not set.')\n\n        # Verify the model name\n        if not model.name:\n            raise ValueError('The model name is not set.')\n\n        # Verify the system message\n        if not system_message:\n            raise ValueError('The system message is not set.')\n\n        # Verify the prompt\n        if not prompt:\n            raise ValueError('The prompt is not set.')\n\n        # Set the default parameters\n        parameters = {\n            'model': model.name,\n            'prompt': system_message + prompt,\n            'max_tokens': model.max_tokens,\n            'temperature': model.temperature,\n            'top_p': model.top_p,\n            'frequency_penalty': model.frequency_penalty,\n            'presence_penalty': model.presence_penalty,\n            'stop': model.stop\n        }\n\n        # Update the parameters with any provided overrides\n        for key, value in kwargs.items():\n            if key in parameters:\n                parameters[key] = value\n\n        # Generate the response\n        response = self.generate_with_retry(parameters)\n\n        # Process the response\n        response = self.process_response(response, model.start_token, model.end_token)\n\n        return response\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square.\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric.\")\n\n    return\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric.\")\n\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0])):\n        raise ValueError(\"Matrix is not a distance matrix.\")\n\n    return\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Get the function name\n        func_name = function_description.name\n\n        # Get the function arguments\n        func_args = function_description.args\n\n        # Get the function keyword arguments\n        func_kwargs = function_description.kwargs\n\n        # Get the function return type\n        func_return_type = function_description.return_type\n\n        # Get the function return description\n        func_return_description = function_description.return_description\n\n        # Get the function docstring\n        func_docstring = function_description.docstring\n\n        # Get the function source code\n        func_source_code = function_description.source_code\n\n        # Get the function source code comment\n        func_source_code_comment = function_description.source_code_comment\n\n        # Get the function source code comment\n        func_source_code_no_comment = function_description.source_code_no_comment\n\n        # Get the function source code comment\n        func_source_code_comment_no_docstring = function_description.source_code_comment_no_docstring\n\n        # Get the function source code comment\n        func_source_code_comment_no_docstring_no_comment = function_description.source_code_comment_no_docstring_no_comment\n\n        # Get the function source code comment\n        func_source_code_comment_no_docstring_no_comment_no_args = function_description.source_code_comment_no_docstring_no_comment_no_args\n\n        # Get the function source code comment\n        func_source_code_comment_no_docstring_no_comment_no_args_no_return = function_description.source_code_comment_no_docstring_no_comment_no_args_no_return\n\n        # Get the function source code comment\n        func_source_code_comment_no_docstring_no_comment_no_args_no_return_no_self = function_description.source_code_comment_no_docstring_no_comment_no_args_no_return_no_self\n\n        # Get the function source code comment"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    # Import required modules\n    import numpy as np\n    from numpy.linalg import eig\n    from numpy.linalg import cholesky\n    from numpy.linalg import inv\n    from numpy.linalg import svd\n    from scipy.stats import chi2\n\n    # Check if the input covariance matrix is positive definite\n    try:\n        cholesky(cov)\n    except np.linalg.LinAlgError:\n        # If the input covariance matrix is not positive definite, find the nearest positive definite matrix\n        if higham:\n            # Use the Higham & Nick (2002) algorithm to find the nearest positive definite matrix\n            # Initialize the iteration counter\n            iteration = 0\n            # Initialize the tolerance\n            tol = 1e-10\n            # Initialize the maximum number of iterations\n            max_iteration = higham_max_iteration\n            # Initialize the original covariance matrix\n            cov_old = cov\n            # Initialize the new covariance matrix\n            cov_new = cov\n            # Initialize the difference between the original and new covariance matrices\n            diff = 1\n            # Initialize the eigenvalue matrix\n            eig_val = np.array([])\n            # Initialize the eigenvector matrix\n            eig_vec = np.array([])\n            # Initialize the eigenvalue matrix of the new covariance matrix\n            eig_val_new = np.array([])\n            # Initialize the eigenvector matrix of the new covariance matrix\n            eig_vec_new = np.array([])\n            # Initialize the eigenvalue matrix of the original covariance matrix\n            eig_val_old = np.array([])\n            # Initialize the eigenvector matrix of the original covariance matrix\n            eig_vec_old = np.array([])\n            # Initialize the eigenvalue matrix of the original covariance matrix\n            eig_val_old_new = np.array([])\n            # Initialize the eigenvector matrix of the original covariance matrix\n            eig_vec_old_new = np.array([])\n            # Initialize the eigenvalue matrix of the original covariance matrix\n            eig_val_old_old ="}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    import shutil\n    from pathlib import Path\n\n    if data_home is None:\n        data_home = Path.home() / \"skfolio_data\"\n\n    shutil.rmtree(data_home)"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), str\n    elif isinstance(obj, bytes):\n        return (obj,), bytes\n    elif isinstance(obj, list):\n        return (obj,), list\n    elif isinstance(obj, tuple):\n        return (obj,), tuple\n    elif isinstance(obj, dict):\n        return (obj,), dict\n    elif isinstance(obj, torch.Tensor):\n        return (obj,), torch.Tensor\n    elif isinstance(obj, Instances):\n        return (obj.image_size, obj.pred_boxes, obj.scores, obj.pred_classes, obj.pred_masks, obj.pred_keypoints), Instances\n    elif isinstance(obj, Boxes):\n        return (obj.tensor,), Boxes\n    elif isinstance(obj, ROIMasks):\n        return (obj.tensor,), ROIMasks\n    else:\n        raise ValueError(\"Unsupported type {}\".format(type(obj)))\n\n"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Checks\n    if not isinstance(groups, (np.ndarray, pd.DataFrame)):\n        groups = np.array(groups)\n    if not isinstance(equations, (np.ndarray, pd.Series)):\n        equations = np.array(equations)\n    if not isinstance(names, tuple):\n        raise TypeError(f\"'names' must be a tuple of strings.\")\n    if not isinstance(groups, np.ndarray):\n        raise TypeError(f\"'{names[0]}' must be a numpy array.\")\n    if not isinstance(equations, np.ndarray):\n        raise TypeError(f\"'{names[1]}' must be a numpy array.\")\n    if not isinstance(sum_to_one, bool):\n        raise TypeError(f\"'sum_to_one' must be a boolean.\")\n    if not isinstance(raise_if_group_missing, bool):\n        raise TypeError(f\"'raise_if_group_missing' must be a boolean.\")\n    if not isinstance(names, tuple):\n        raise TypeError(f\"'names' must be a tuple of strings.\")\n    if not isinstance(names[0], str):\n        raise TypeError(f\"'names[0]' must be a string.\")\n    if not isinstance(names[1], str):\n        raise TypeError(f\"'names[1]' must be a string.\")\n\n    # Checks\n    if groups.ndim != 2:\n        raise ValueError(f\"'{names[0]}' must be a 2D array.\")\n    if equations.ndim != 1:\n        raise ValueError(f\"'{names[1]}' must be a 1D array.\")\n    if groups.shape[1] != equations.shape[0]:\n        raise ValueError(\n            f\"The number of columns in '{names[0]}' must be equal to the number of elements in '{names[1]}'.\"\n        )\n\n    # Get groups names\n    groups_names = np.array(list(map(lambda x: x.lower(), groups.columns)))\n\n    # Get equations names\n    equations_names ="}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # Import Statements\n    import contextlib\n    import importlib\n    import inspect\n    import os\n    import sys\n    import tempfile\n    import torch\n    from detectron2.structures import instances\n\n    # Define the new class\n    class Instances(torch.jit.ScriptModule):\n        def __init__(self, image_size: torch.Tensor):\n            super().__init__()\n            self.image_size = image_size\n            for field in fields:\n                self.set_attribute(field)\n\n        def set_attribute(self, field):\n            if isinstance(field, tuple):\n                name, value = field\n                self.__dict__[name] = value\n            else:\n                raise ValueError(\"Field must be a tuple of (name, value).\")\n\n        @torch.jit.script_method\n        def __getitem__(self, item: int):\n            if isinstance(item, str):\n                if item in self.__dict__:\n                    return self.__dict__[item]\n                else:\n                    raise AttributeError(item)\n            elif isinstance(item, int):\n                if item < len(self.__dict__):\n                    return list(self.__dict__.values())[item]\n                else:\n                    raise IndexError(\"Out of bounds access\")\n            else:\n                raise TypeError(\"Argument to __getitem__ must be int or str\")\n\n        @torch.jit.script_method\n        def __len__(self):\n            return len(self.__dict__)\n\n        @torch.jit.script_method\n        def __iter__(self):\n            return self.__dict__.items()\n\n        @torch.jit.script_method\n        def __str__(self):\n            s = \"Instances(image_size=\" + str(self.image_size) + \")\\n\"\n            s += \"\\n\".join(\n                [\" * \" + name + \": \" + str(value) for name, value in self.__dict__.items()]\n            )\n            return s\n\n        @torch.jit.script_method\n        def"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Import the necessary libraries\n    import os\n    import PIL.Image\n    import numpy as np\n\n    # Read the image\n    image = PIL.Image.open(file_name)\n\n    # Apply any necessary orientation corrections based on the image's EXIF data\n    image = PIL.ImageOps.exif_transpose(image)\n\n    # Convert the image to the specified format\n    if format is not None:\n\n        # Convert the image to the specified format\n        if format == \"BGR\":\n            image = image.convert(\"RGB\")\n            image = np.asarray(image, dtype=np.uint8)\n            image = image[:, :, ::-1]\n        elif format == \"YUV-BT.601\":\n            image = image.convert(\"YCbCr\")\n            image = np.asarray(image, dtype=np.float32)\n            image /= 255.0\n        else:\n            image = image.convert(format)\n            image = np.asarray(image, dtype=np.uint8)\n\n    else:\n\n        # Convert the image to the original format\n        image = np.asarray(image, dtype=np.uint8)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed boxes to image size\n    bbox = transforms.apply_box(np.array([bbox]))[0].clip(min=0)\n    annotation[\"bbox\"] = np.minimum(bbox, list(image_size + image_size)[::-1])\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        # each instance contains 1 or more polygons\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygons\n            polygons = [np.asarray(p).reshape(-1, 2) for p in segm]\n            annotation[\"segmentation\"] = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n        elif isinstance(segm, dict) and \"counts\" in segm and \"size\" in segm:\n            # RLE\n            mask = mask_util.decode(segm)\n            mask = transforms.apply_segmentation(mask)\n            assert tuple(mask.shape[:2]) == image_size\n            annotation[\"segmentation\"] = mask\n        else:\n            raise ValueError(\n                \"Cannot transform segmentation of type '{}'!\" % type(segm)\n            )\n\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n\n        return coords @ self.rm_coords\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # ===============================\n    # Setup\n    # ===============================\n\n    # Init.\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model = model.eval()\n\n    # ===============================\n    # Computation\n    # ===============================\n\n    # Prepare inputs.\n    inputs = [\n        {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()}\n        for t in inputs\n    ]\n\n    # Prepare model.\n    model_ = model\n    if isinstance(model, torch.nn.DataParallel):\n        model_ = model_.module\n\n    # Prepare flops_model.\n    flops_model = FlopCountAnalysis(model_, inputs)\n    flops_model.unsupported_ops_warnings(False)\n    flops_model.uncalled_modules_warnings(False)\n\n    # Run the flops computation.\n    flops_model.skipped_ops_warnings(False)\n    flops_model.tracer.flags[\"warn_non_clever_flops\"] = False\n    flops_model.run()\n\n    # ===============================\n    # Output\n    # ===============================\n\n    # Prepare output.\n    output = defaultdict(float)\n    for op, flop in flops_model.by_operator().items():\n        output[op] = flop / 1e9\n\n    return output"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.size == 0 or self.rm_image is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n            colors = [\n                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n            ]\n            alpha = 0.8\n        else:\n            colors = None\n            alpha = 0.5\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.img = self._create_grayscale_image(\n                (predictions.pred_masks.any(dim=0) > 0).numpy()\n            )\n            alpha = 0.3\n\n        self.overlay_instances(\n            masks=masks,\n            boxes=boxes,\n            labels=classes,\n            keypoints=keypoints,\n            assigned_colors=colors,\n            alpha=alpha,\n        )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the visualized image from RGBA to RGB format\n        visualized_image = self.canvas.copy()\n        visualized_image = visualized_image[:, :, :3]\n\n        # Convert the visualized image from RGB to BGR format\n        visualized_image = visualized_image[:, :, ::-1]\n\n        return visualized_image\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw segmentation\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"], area_threshold=0, alpha=0.5)\n\n        # Draw panoptic segmentation\n        if \"pan_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"pan_seg\"])\n\n        # Draw keypoints\n        if \"keypoints\" in dic:\n            keypoints = dic[\"keypoints\"]\n            num_instances = len(keypoints)\n            for i in range(num_instances):\n                self.draw_and_connect_keypoints(keypoints[i])\n\n        # Draw bounding boxes\n        if \"instances\" in dic:\n            boxes = dic[\"instances\"].get(\"pred_boxes\")\n            assert boxes is not None, \"Instances must contain boxes!\"\n            boxes = boxes.tensor.numpy()\n            scores = dic[\"instances\"].get(\"scores\")\n            classes = dic[\"instances\"].get(\"pred_classes\")\n            labels = [\n                f\"{i}\"\n                for i in classes\n                if classes is not None\n            ]\n            keypoints = dic[\"instances\"].get(\"pred_keypoints\")\n            if keypoints:\n                keypoints = keypoints.numpy()\n\n            if scores is not None:\n                for i, (bbox, score) in enumerate(zip(boxes, scores)):\n                    if score > 0.5:\n                        label = labels[i] if len(labels) > i else \"\"\n                        self.draw_box(bbox, edge_color=None, label=label)\n\n                        if keypoints is not None:\n                            self.draw_and_connect_keypoints(keypoints[i])\n\n            else:\n                for i, bbox in enumerate(boxes):\n                    if len(labels) > i:\n                        label = labels[i]\n                        self.draw_box(bbox, edge_color=None, label=label)\n\n                        if keypoints is not None:\n                            self.draw_and_connect_keypoints("}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # Convert the binary mask to an RGB image.\n        rgb_mask = self._create_rgb_mask(binary_mask, color)\n\n        # Convert the RGB mask to a BGR image.\n        bgr_mask = rgb_mask[:, :, ::-1]\n\n        # Blend the mask with the original image.\n        bgr_image = cv2.addWeighted(\n            self.image[:, :, ::-1], alpha, bgr_mask, 1 - alpha, 0\n        )\n\n        # Convert the BGR image to an RGB image.\n        image = bgr_image[:, :, ::-1]\n\n        # Draw the mask on the image.\n        self.output = image\n        self._draw_mask_contours(binary_mask, edge_color, area_threshold)\n\n        # Draw the text on the image.\n        if text is not None:\n            self._draw_text(text)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check that the sizes are the same\n    if size_as_tensor:\n        assert torch.equal(input.image_size, other.image_size), \\\n            f\"{msg} Instance image sizes are not the same: {input.image_size} vs {other.image_size}!\"\n    else:\n        assert input.image_size == other.image_size, \\\n            f\"{msg} Instance image sizes are not the same: {input.image_size} vs {other.image_size}!\"\n\n    # Check that the fields are the same\n    assert input.fields().keys() == other.fields().keys(), \\\n        f\"{msg} Instance fields are not the same: {input.fields().keys()} vs {other.fields().keys()}!\"\n\n    # Check that the values are the same\n    for field in input.fields():\n        assert isinstance(input.get(field), type(other.get(field))), \\\n            f\"{msg} Instance field {field} is of different types: {type(input.get(field))} vs {type(other.get(field))}!\"\n\n        if isinstance(input.get(field), Boxes):\n            assert_boxes_allclose(input.get(field), other.get(field), rtol=rtol, msg=msg)\n        elif isinstance(input.get(field), ROIMasks):\n            assert_roimasks_allclose(input.get(field), other.get(field), rtol=rtol, msg=msg)\n        elif isinstance(input.get(field), torch.Tensor):\n            assert torch.allclose(input.get(field), other.get(field), rtol=rtol), \\\n                f\"{msg} Instance field {field} is not close enough!\"\n        else:\n            raise ValueError(f\"{msg} Encountered an unsupported field type: {type(input.get(field))}!\")\n\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = (box[:, 2] * box[:, 3]).abs()\n\n        return area\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        proposal_generator = registry.PROPOSAL_GENERATOR[cfg.PROPOSAL_GENERATOR.NAME](cfg, input_shape)\n        return proposal_generator"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        losses = FastRCNNOutputs(scores, proposal_deltas, proposals, self.box2box_transform, self.smooth_l1_beta).losses()\n        return {\n            \"loss_cls\": losses.loss_cls,\n            \"loss_box_reg\": losses.loss_box_reg,\n        }\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_registry = {\n        \"BaseTracker\": BaseTracker,\n        \"BaseSiamTracker\": BaseSiamTracker,\n        \"SiamFC\": SiamFC,\n        \"SiamRPN\": SiamRPN,\n        \"SiamMask\": SiamMask,\n        \"SiamRPN++\": SiamRPNpp,\n    }\n\n    if tracker_name not in tracker_registry:\n        raise NotImplementedError(\"Tracker not available: {}\".format(tracker_name))\n\n    tracker_class = tracker_registry[tracker_name]\n    tracker = tracker_class(cfg)\n\n    return tracker"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Convert the boxes to a tensor if they are a list\n        if isinstance(boxes, list):\n            boxes = torch.stack(boxes, dim=0)\n\n        # Convert the deltas to a tensor if they are a list\n        if isinstance(deltas, list):\n            deltas = torch.stack(deltas, dim=0)\n\n        # Convert the boxes to a tensor if they are a list\n        if isinstance(boxes, list):\n            boxes = torch.stack(boxes, dim=0)\n\n        # Convert the deltas to a tensor if they are a list\n        if isinstance(deltas, list):\n            deltas = torch.stack(deltas, dim=0)\n\n        # Get the number of boxes\n        N = boxes.size(0)\n\n        # Get the number of box transformations\n        num_deltas = deltas.size(1) // 4\n\n        # Split the deltas into 4 different tensors\n        deltas = deltas.view(N, num_deltas, 4)\n        center_x_delta = deltas[:, :, 0]\n        center_y_delta = deltas[:, :, 1]\n        width_delta = deltas[:, :, 2]\n        height_delta = deltas[:, :, 3]\n\n        # Get the boxes' width and height\n        width = boxes[:, 2] - boxes[:, 0]\n        height = boxes[:, 3] - boxes[:, 1]\n\n        # Get the boxes' center x and y\n        center_x = boxes[:, 0] + 0.5 * width\n        center_y = boxes[:, 1] + 0.5 * height\n\n        # Compute the new width and height\n        width = width * torch.exp(width_delta)\n        height = height * torch.exp(height_delta)\n\n        # Compute the new center x and y\n        center_x = center_x + center_x_delta * width"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image\n        output = self.process(image)\n\n        # If no anno_type is specified, return the entire output\n        if anno_type is None:\n            return output\n\n        # If anno_type is specified, filter the output\n        else:\n\n            # If anno_type is a string, convert it to a list\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n\n            # If anno_type is a list, check if it is empty\n            if len(anno_type) == 0:\n                return output\n\n            # If anno_type is a list, check if it contains only strings\n            if not all(isinstance(t, str) for t in anno_type):\n                raise TypeError('anno_type must be a string or a list of strings.')\n\n            # If anno_type is a list, check if it contains only unique strings\n            if len(set(anno_type)) < len(anno_type):\n                raise ValueError('anno_type must not contain duplicate strings.')\n\n            # If anno_type is a list, check if it contains only valid strings\n            if not set(anno_type).issubset(set(output.keys())):\n                raise ValueError('anno_type must only contain valid strings.')\n\n            # If anno_type is a list, check if it contains only one string\n            if len(anno_type) == 1:\n                return output[anno_type[0]]\n\n            # If anno_type is a list, check if it contains more than one string\n            else:\n                return {t: output[t] for t in anno_type}\n\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query\n        query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Get the BM25 score for each keyword\n        keyword_scores = self.get_keyword_scores(keywords)\n\n        # Get the BM25 score for each URL\n        url_scores = self.get_url_scores(keyword_scores)\n\n        return url_scores\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # convert to (x1, y1, x2, y2)\n        self.tensor[:, 0], self.tensor[:, 1], self.tensor[:, 2], self.tensor[:, 3] = self.tensor[:, 0] - self.tensor[:, 2] / 2, self.tensor[:, 1] - self.tensor[:, 3] / 2, self.tensor[:, 0] + self.tensor[:, 2] / 2, self.tensor[:, 1] + self.tensor[:, 3] / 2\n\n        # clip\n        self.tensor[:, 0].clamp_(min=0, max=box_size[1])\n        self.tensor[:, 1].clamp_(min=0, max=box_size[0])\n        self.tensor[:, 2].clamp_(min=0, max=box_size[1])\n        self.tensor[:, 3].clamp_(min=0, max=box_size[0])\n\n        # convert back to original format\n        self.tensor[:, 0], self.tensor[:, 1], self.tensor[:, 2], self.tensor[:, 3] = self.tensor[:, 0] + self.tensor[:, 2] / 2, self.tensor[:, 1] + self.tensor[:, 3] / 2, self.tensor[:, 2] - self.tensor[:, 0], self.tensor[:, 3] - self.tensor[:, 1]\n\n        # convert angle to degrees\n        self.tensor[:, 4] = 180. * self.tensor[:, 4] / math.pi\n\n        # normalize angles to be within (-180, 180] degrees\n        self.tensor[:, 4][self.tensor[:, 4] < -180.] += 360.\n        self.tensor[:, 4][self.tensor[:, 4] > 180.] -= 360.\n\n        # mark boxes that are nearly horizontal for clipping\n        is_box_long"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initializing the statistics dictionary\n        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        # Iterating over the data\n        for item in self.data:\n\n            # Updating the statistics dictionary\n            statistics[item['type']] += 1\n\n        # Returning the statistics dictionary\n        return statistics\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS:\n        return NECKS[cfg['type']](**cfg)\n    elif cfg['type'] in MMDET_NECKS:\n        return MMDET_NECKS[cfg['type']](**cfg)\n    else:\n        raise NotImplementedError\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    # Import the necessary libraries\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n    # Get the type of loss function to be built\n    loss_type = cfg['type']\n\n    # Build the loss function\n    if loss_type == 'CrossEntropyLoss':\n        loss = nn.CrossEntropyLoss()\n    elif loss_type == 'BCEWithLogitsLoss':\n        loss = nn.BCEWithLogitsLoss()\n    elif loss_type == 'MSELoss':\n        loss = nn.MSELoss()\n    elif loss_type == 'L1Loss':\n        loss = nn.L1Loss()\n    elif loss_type == 'SmoothL1Loss':\n        loss = nn.SmoothL1Loss()\n    elif loss_type == 'NLLLoss':\n        loss = nn.NLLLoss()\n    elif loss_type == 'KLDivLoss':\n        loss = nn.KLDivLoss()\n    elif loss_type == 'BCELoss':\n        loss = nn.BCELoss()\n    elif loss_type == 'MarginRankingLoss':\n        loss = nn.MarginRankingLoss()\n    elif loss_type == 'HingeEmbeddingLoss':\n        loss = nn.HingeEmbeddingLoss()\n    elif loss_type == 'MultiLabelMarginLoss':\n        loss = nn.MultiLabelMarginLoss()\n    elif loss_type == 'MultiLabelSoftMarginLoss':\n        loss = nn.MultiLabelSoftMarginLoss()\n    elif loss_type == 'MultiMarginLoss':\n        loss = nn.MultiMarginLoss()\n    elif loss_type == 'SigmoidFocalLoss':\n        loss = nn.SigmoidFocalLoss()\n    elif loss_type == 'SoftMarginLoss':\n        loss = nn.SoftMarginLoss()\n    elif loss_type =="}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    # Import the head object from the HEADS module\n    if cfg['type'] in HEADS:\n        head = HEADS[cfg['type']](cfg)\n    else:\n        head = MMDET_HEADS[cfg['type']](cfg)\n\n    return head\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    # Import the segmentor model class\n    from .segmentor import Segmentor\n\n    # Check if the training configuration is specified both in the function arguments and the model configuration\n    if train_cfg is not None and 'train_cfg' in cfg:\n        # If it is, raise a warning\n        warnings.warn('The training configuration is specified both in the model configuration and the function arguments. '\n                      'The configuration in the function arguments will be used.')\n\n    # Check if the testing configuration is specified both in the function arguments and the model configuration\n    if test_cfg is not None and 'test_cfg' in cfg:\n        # If it is, raise a warning\n        warnings.warn('The testing configuration is specified both in the model configuration and the function arguments. '\n                      'The configuration in the function arguments will be used.')\n\n    # If the training configuration is not specified in the function arguments\n    if train_cfg is None:\n        # If it is specified in the model configuration\n        if 'train_cfg' in cfg:\n            # Extract the training configuration\n            train_cfg = cfg['train_cfg']\n\n    # If the testing configuration is not specified in the function arguments\n    if test_cfg is None:\n        # If it is specified in the model configuration\n        if 'test_cfg' in cfg:\n            # Extract the testing configuration\n            test_cfg = cfg['test_cfg']\n\n    # Build the segmentor model\n    segmentor = Segmentor(cfg, train_cfg, test_cfg)\n\n    # Return the segmentor model\n    return segmentor"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None:\n        assert 'train_cfg' not in cfg, 'train_cfg specified in both outer field and model field'\n        cfg['train_cfg'] = train_cfg\n    if test_cfg is not None:\n        assert 'test_cfg' not in cfg, 'test_cfg specified in both outer field and model field'\n        cfg['test_cfg'] = test_cfg\n\n    if cfg['type'].startswith('mmdet::'):\n        from mmcv.utils import import_module_error_func\n        from mmdet.models import build_detector as build_detector_mmdet\n        try:\n            from mmdet.models import DETECTORS\n            return build_detector_mmdet(cfg, DETECTORS, default_args)\n        except (ImportError, ModuleNotFoundError) as e:\n            import_module_error_func('mmdet', e.name)\n    else:\n        from mmcv.utils import import_module_error_func\n        from mmcv.cnn import build_model\n        try:\n            from mmcv.cnn import DETECTORS\n            return build_model(cfg, DETECTORS, default_args)\n        except (ImportError, ModuleNotFoundError) as e:\n            import_module_error_func('mmcv', e.name)\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from nuscenes.eval.detection.config import config_factory\n    from nuscenes.eval.detection.evaluate import NuScenesEval\n\n    # Initialize the NuScenesEval object\n    nusc_eval = NuScenesEval(\n        nusc,\n        config=config_factory(self.eval_version),\n        result_path=self.eval_dir,\n        eval_set=self.eval_split,\n        output_dir=self.eval_dir,\n        verbose=False,\n    )\n\n    # Evaluate the detection results\n    nusc_eval.main(gt_annos, dt_annos,\n                   min_recall=self.min_recall,\n                   max_boxes_per_sample=self.max_boxes_per_sample,\n                   nms_thresh=self.nms_thresh,\n                   eval_min_recall=self.eval_min_recall,\n                   eval_max_dets=self.eval_max_dets,\n                   box_type_3d=box_type_3d,\n                   box_mode_3d=box_mode_3d)\n\n    # Print the evaluation results\n    nusc_eval.print_results()\n\n    # Return the evaluation results\n    return nusc_eval.metrics"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        return LiDARBox, LiDARBoxMode.LiDAR_box\n    elif box_type == \"Camera\":\n        return CameraBox, CameraBoxMode.camera_box\n    elif box_type == \"Depth\":\n        return DepthBox, DepthBoxMode.depth_box\n    else:\n        raise ValueError(\"Input box type is not recognized\")\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    # Validate the input parameters\n    if not model:\n      raise RequestError('A model is required to create a chat response.')\n\n    if messages:\n      if not isinstance(messages, list):\n        raise TypeError('Messages must be a list of Message or dict-like objects.')\n\n      for message in messages:\n        if not isinstance(message, Message) and not isinstance(message, dict):\n          raise TypeError('Messages must be a list of Message or dict-like objects.')\n\n    # Create the request body\n    body = {\n      'model': model,\n      'messages': messages,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    # Send the request\n    response = self.session.post(\n      url='https://api.aichatbot.com/api/v1/chat',\n      json=body,\n    )\n\n    # Validate the response\n    if response.status_code != 200:\n      raise ResponseError(response.status_code, response.text)\n\n    # Return the response\n    if stream:\n      return self._stream_response(response, format)\n    else:\n      return self._get_response(response, format)\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = f'{self.url}/pull'\n    params = {\n      'model': model,\n      'insecure': insecure,\n    }\n    response = self.session.post(url, params=params, stream=stream)\n    response.raise_for_status()\n    return response.json() if stream else response.json()[0]\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('The model is required.')\n\n    if not isinstance(model, str):\n      raise TypeError('The model must be a string.')\n\n    if not isinstance(prompt, str):\n      raise TypeError('The prompt must be a string.')\n\n    if not isinstance(system, str):\n      raise TypeError('The system must be a string.')\n\n    if not isinstance(template, str):\n      raise TypeError('The template must be a string.')\n\n    if context is not None and not isinstance(context, Sequence):\n      raise TypeError('The context must be a sequence.')\n\n    if not isinstance(stream, bool):\n      raise TypeError('The stream parameter must be a boolean.')\n\n    if not isinstance(raw, bool):\n      raise TypeError('The raw parameter must be a boolean.')\n\n    if format not in ['', 'json']:\n      raise ValueError('The format parameter must be an empty string or \"json\".')\n\n    if images is not None and not isinstance(images, Sequence):\n      raise TypeError('The images must be a sequence.')\n\n    if options is not None and not isinstance(options, Options):\n      raise TypeError('The options must be an instance of the Options class.')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError('The keep_alive parameter must be a float, a string, or None.')\n\n    if not isinstance(stream, bool):\n      raise TypeError('The stream parameter must be a boolean.')\n\n    if not isinstance(raw, bool):\n      raise TypeError('The raw parameter must be a boolean.')\n\n    if format not in ['', 'json']:\n      raise ValueError('The format parameter must be an empty string or \"json\".')\n\n    if images is not None and not isinstance(images, Sequence):\n      raise TypeError('The images must be a sequence.')\n\n    if options is not None and not isinstance(options, Options):\n      raise TypeError('The options must be an instance of the Options class.')"}
{"namespace": "ollama._client.Client.push", "completion": "    url = f'{self.base_url}/api/push'\n    params = {\n      'model': model,\n      'insecure': insecure,\n    }\n    response = self.session.post(url, params=params, stream=stream)\n    if response.status_code == 200:\n      if stream:\n        return (ProgressResponse(**item) for item in response.iter_content())\n      else:\n        return ProgressResponse(**response.json())\n    else:\n      raise ResponseError(response.status_code, response.text)\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError(\n        'Either `path` or `modelfile` is required for the operation.'\n      )\n\n    if path:\n      modelfile = Path(path).read_text()\n\n    return self._create(\n      model=model,\n      modelfile=modelfile,\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Calculate the SHA-256 checksum of the file\n    with open(path, 'rb') as f:\n      digest = hashlib.sha256(f.read()).hexdigest()\n\n    # Check if the blob already exists on the server\n    response = self._session.head(f'{self._url}/v2/{self._repo}/blobs/sha256:{digest}')\n\n    # If the blob does not exist on the server, upload it\n    if response.status_code == 404:\n      with open(path, 'rb') as f:\n        response = self._session.post(f'{self._url}/v2/{self._repo}/blobs/uploads/',\n                                      headers={'Content-Type': 'application/octet-stream'},\n                                      data=f)\n\n    # Raise an exception if the response status code is not 200\n    response.raise_for_status()\n\n    # Return the digest of the file\n    return f'sha256:{digest}'\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    # Check if the model is provided\n    if not model:\n      raise ValueError('Model is required')\n\n    # Check if the model is supported\n    if model not in self.models:\n      raise ValueError(f'Model {model} is not supported')\n\n    # Check if the format is supported\n    if format and format not in ['json']:\n      raise ValueError(f'Format {format} is not supported')\n\n    # Check if the images are provided\n    if images and not isinstance(images, Sequence):\n      raise ValueError('Images must be a sequence')\n\n    # Check if the options are provided\n    if options and not isinstance(options, Options):\n      raise ValueError('Options must be an instance of Options')\n\n    # Check if the keep-alive parameter is provided\n    if keep_alive and not isinstance(keep_alive, (float, str)):\n      raise ValueError('Keep-alive must be a float or string')\n\n    # Create the request\n    request = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    # Send the request\n    async with self.session.post(\n      f'{self.url}/generate',\n      json=request,\n      headers=self.headers,\n      params=self.params,\n      timeout=self.timeout,\n    ) as response:\n\n      # Check if the response is valid\n      if response.status != 200:\n        raise ValueError(f'Invalid response: {response.status}')\n\n      # Check if the response is streaming\n      if stream:\n        async for data in response.content.iter_chunks():\n          yield json.loads(data)\n      else:\n        yield json.loads(await response.content.read())\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    url = f'{self.base_url}/{model}/pull'\n    params = {'insecure': insecure}\n    response = await self._request(\n      url=url,\n      method='get',\n      params=params,\n      stream=stream,\n    )\n    return response\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # validate the input parameters\n    if not isinstance(model, str):\n      raise TypeError(f\"'model' argument must be of type 'str' but received type {type(model)}\")\n    if not isinstance(messages, (list, tuple, type(None))):\n      raise TypeError(f\"'messages' argument must be of type 'list' or 'tuple' but received type {type(messages)}\")\n    if not isinstance(stream, bool):\n      raise TypeError(f\"'stream' argument must be of type 'bool' but received type {type(stream)}\")\n    if not isinstance(format, str):\n      raise TypeError(f\"'format' argument must be of type 'str' but received type {type(format)}\")\n    if not isinstance(options, (Options, type(None))):\n      raise TypeError(f\"'options' argument must be of type 'Options' but received type {type(options)}\")\n    if not isinstance(keep_alive, (float, int, str, type(None))):\n      raise TypeError(f\"'keep_alive' argument must be of type 'float', 'int', or 'str' but received type {type(keep_alive)}\")\n\n    # validate the input parameters\n    if model == '':\n      raise ValueError(\"'model' argument must be a non-empty string\")\n    if messages is not None:\n      for message in messages:\n        if not isinstance(message, dict):\n          raise TypeError(f\"'messages' argument must be a sequence of 'dict' but received type {type(message)}\")\n        if 'role' not in message:\n          raise ValueError(\"'messages' argument must contain 'role' key\")\n        if 'content' not in message:\n          raise ValueError(\"'messages' argument must contain 'content' key\")\n        if not isinstance(message['role'], str):\n          raise TypeError(f\"'messages' argument must contain 'role' key with value of type 'str' but received type {type(message['role'])}\")\n        if message['role'] not in ['user', 'agent']:\n          raise ValueError(f\"'messages' argument must contain"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    url = f'{self.url}/api/push'\n    params = {\n      'model': model,\n      'insecure': insecure,\n    }\n    async with self.session.post(url, params=params, stream=stream) as response:\n      if stream:\n        async for line in response.content:\n          yield ProgressResponse.from_json(line)\n      else:\n        return ProgressResponse.from_json(await response.text())\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file.\n    with open(path, 'rb') as f:\n      checksum = hashlib.sha256(f.read()).hexdigest()\n\n    # Check if the blob already exists on the server.\n    response = await self._head_blob(checksum)\n\n    # If the blob does not exist on the server, upload it.\n    if response.status_code == 404:\n      # Create a session to upload the file in chunks.\n      async with aiohttp.ClientSession() as session:\n        # Create a task to upload the file in chunks.\n        task = asyncio.create_task(self._upload_blob(session, checksum, path))\n\n        # Wait for the task to complete.\n        await task\n\n    return f'sha256:{checksum}'\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user-provided code and test code into a single file.\n        combined_code = user_code + test_code\n\n        # Create a temporary directory to store the combined code.\n        with tempfile.TemporaryDirectory() as temp_dir:\n\n            # Create a temporary file to store the combined code.\n            with tempfile.NamedTemporaryFile(\n                dir=temp_dir, suffix='.py', mode='w'\n            ) as temp_file:\n\n                # Write the combined code to the temporary file.\n                temp_file.write(combined_code)\n\n                # Close the temporary file.\n                temp_file.close()\n\n                # Create a temporary directory to store the Pyright output.\n                with tempfile.TemporaryDirectory() as pyright_output_dir:\n\n                    # Run Pyright on the temporary file.\n                    subprocess.run(\n                        [\n                            'pyright',\n                            '--outputjson',\n                            '--output',\n                            pyright_output_dir,\n                            temp_file.name,\n                        ]\n                    )\n\n                    # Read the Pyright output.\n                    with open(\n                        os.path.join(pyright_output_dir, 'report.json'), 'r'\n                    ) as pyright_output_file:\n\n                        # Load the Pyright output.\n                        pyright_output = json.load(pyright_output_file)\n\n                        # Initialize a list to store the Pyright errors.\n                        pyright_errors = []\n\n                        # Iterate over the Pyright output.\n                        for pyright_output_item in pyright_output:\n\n                            # If the Pyright output item is an error.\n                            if pyright_output_item['severity'] == 'error':\n\n                                # Append the Pyright error to the list of Pyright errors.\n                                pyright_errors.append(pyright_output_item)\n\n                        # If there are no Pyright errors.\n                        if len(pyright_errors) == 0:"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not path and not modelfile:\n      raise RequestError(\n        'Either `path` or `modelfile` must be specified for the request.'\n      )\n\n    if path:\n      with open(path, 'r') as f:\n        data = f.read()\n    else:\n      data = modelfile\n\n    return await self._post(\n      f'/models/{model}',\n      data=data,\n      stream=stream,\n    )\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    # Import the necessary libraries\n    import torch\n    import torch.nn as nn\n    import numpy as np\n    import copy\n    import time\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torch.cuda.amp as amp\n    import aot_autograd_lib as aot_lib\n    import aot_autograd_lib.functional as aot_lib_fn\n    import aot_autograd_lib.pooling as aot_lib_pooling\n    import aot_autograd_lib.normalization as aot_lib_norm\n    import aot_autograd_lib.indexing as aot_lib_indexing\n    import aot_autograd_lib.reduction as aot_lib_reduction\n    import aot_autograd_lib.linear as aot_lib_linear\n    import aot_autograd_lib.conv as aot_lib_conv\n    import aot_autograd_lib.activation as aot_lib_activation\n    import aot_autograd_lib.math as aot_lib_math\n    import aot_autograd_lib.indexing as aot_lib_indexing\n    import aot_autograd_lib.indexing as aot_lib_indexing\n    import aot_autograd_lib.indexing as aot_lib_indexing\n    import aot_autograd_lib.indexing as aot_lib_indexing\n    import aot_autograd_lib.indexing as aot_lib_indexing\n    import aot_autograd_lib.indexing as aot_lib_indexing\n    import aot_autograd_lib.indexing as aot_lib_indexing\n    import aot_autograd_lib.indexing as aot_lib_indexing\n    import aot_autograd_lib.indexing as aot_lib_indexing\n    import aot_autograd_lib.indexing as aot_lib_indexing\n    import a"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # check if the trial directory exists\n    if not os.path.exists(trial_path):\n        raise ValueError(f\"The trial directory {trial_path} does not exist.\")\n\n    # check if the summary file exists\n    summary_file = os.path.join(trial_path, \"summary.csv\")\n    if not os.path.exists(summary_file):\n        raise ValueError(f\"The summary file {summary_file} does not exist.\")\n\n    # check if the configuration file exists\n    config_file = os.path.join(trial_path, \"config.yaml\")\n    if not os.path.exists(config_file):\n        raise ValueError(f\"The configuration file {config_file} does not exist.\")\n\n    # read the summary file\n    summary = pd.read_csv(summary_file)\n\n    # read the configuration file\n    with open(config_file, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    # extract the best trial number\n    best_trial = summary.loc[summary[\"rank_test_score\"] == 1, \"trial_id\"].values[0]\n\n    # extract the best configuration\n    best_config = config[best_trial]\n\n    # save the best configuration to a file\n    if output_path is not None:\n\n        # check if the output path is a file\n        if not os.path.isfile(output_path):\n            raise ValueError(f\"The output path {output_path} is not a file.\")\n\n        # check if the output path has a valid extension\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            raise ValueError(f\"The output path {output_path} must have a .yaml or .yml extension.\")\n\n        # write the best configuration to a file\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config, f)\n\n    return best_config\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Import the tracing function\n    from torchdynamo.optimizations import normalize_ir\n\n    # Import the lock\n    from .lock import lock\n\n    # Import the tracing function\n    from .tracing_exists import tracing_exists\n\n    # Import the cache\n    from .cache import cache\n\n    # Import the compiler\n    from .compiler import compiler\n\n    # Import the traced module class\n    from .traced_module import TracedModule\n\n    # Import the call helper class\n    from .call_helper import CallHelper\n\n    # Import the utilities for the compiler\n    from .compiler_utils import (\n        get_module_name,\n        get_module_name_from_source,\n        get_module_name_from_path,\n        get_module_name_from_path_and_source,\n    )\n\n    # Import the utilities for the tracing\n    from .tracing_utils import (\n        get_module_name_from_func,\n        get_module_name_from_module,\n        get_module_name_from_module_class,\n        get_module_name_from_module_class_and_func,\n        get_module_name_from_module_class_and_module,\n        get_module_name_from_module_class_and_module_class,\n        get_module_name_from_module_class_and_module_class_and_func,\n        get_module_name_from_module_class_and_module_class_and_module,\n        get_module_name_from_module_class_and_module_class_and_module_and_func,\n        get_module_name_from_module_class_and_module_class_and_module_and_module,\n        get_module_name_from_module_class_and_module_class_and_module_and_module_and_func,\n        get_module_name_from_module_class_and_module_class_and_module_and_module_and_module,\n        get_module_name_from_module"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Import Kratos\n        import KratosMultiphysics as KM\n\n        # Import Kratos \"wrapper\" for python\n        import KratosMultiphysics.ShapeOptimizationApplication as KSO\n\n        # Import json module\n        import json\n\n        # Import pathlib\n        import pathlib\n\n        # Get trial path\n        trial_path = pathlib.Path(trial_path)\n\n        # Get project directory\n        project_dir = str(trial_path.parent)\n\n        # Get optimization log file\n        optimization_log_file = str(trial_path / \"optimization_log.json\")\n\n        # Load optimization log\n        with open(optimization_log_file, 'r') as fh:\n            optimization_log = json.load(fh)\n\n        # Get best iteration index\n        best_iteration_index = optimization_log[\"best_objective_value\"][\"iteration_number\"]\n\n        # Get best configuration\n        best_configuration = optimization_log[\"best_objective_value\"][\"configuration\"]\n\n        # Get optimization settings\n        optimization_settings = KM.Parameters(optimization_log[\"optimization_settings\"])\n\n        # Get response functions\n        response_functions = KSO.OptimizationTestUtilities(optimization_settings).GetResponseFunctions()\n\n        # Get response function settings\n        response_settings = optimization_settings[\"objectives\"][0][\"responses\"][0][\"response_settings\"]\n\n        # Get response function instance\n        response_function = response_functions[response_settings[\"identifier\"].GetString()](response_settings)\n\n        # Get response function gradient identifier\n        response_gradient_identifiers = response_function.GetGradientIdentifiers()\n\n        # Get response function gradient settings\n        response_gradient_settings = KM.Parameters()\n        for gradient_identifier in response_gradient_identifiers:\n            response_gradient_settings.AddValue(\n                gradient_identifier.ToString(),\n                response_settings[\"gradient\"][gradient_identifier.ToString()])\n\n        # Get response function gradient instance\n        response_gradient = response_function.GetGradient(response"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Import modules\n    import pandas as pd\n    import time\n    import os\n    import numpy as np\n    from evaluation.evaluation_utils import evaluate_results\n\n    # Initialize variables\n    start_time = time.time()\n    node_line_dir = os.path.join(node_line_dir, 'retrieval_node')\n    os.makedirs(node_line_dir, exist_ok=True)\n    results = []\n    result_summary = []\n    best_result = None\n    best_result_summary = None\n\n    # Loop through modules\n    for i, module in enumerate(modules):\n\n        # Initialize variables\n        module_start_time = time.time()\n        module_dir = os.path.join(node_line_dir, module.__name__)\n        os.makedirs(module_dir, exist_ok=True)\n        module_params_str = str(module_params[i])\n        module_result_path = os.path.join(module_dir, module_params_str + '.csv')\n        module_result_summary_path = os.path.join(module_dir, module_params_str + '_summary.csv')\n        module_result = None\n        module_result_summary = None\n\n        # If the result already exists, load it\n        if os.path.exists(module_result_path):\n            module_result = pd.read_csv(module_result_path)\n            module_result_summary = pd.read_csv(module_result_summary_path)\n\n        # Otherwise, run the module\n        else:\n            # Run the module\n            module_result = module(**module_params[i])\n\n            # Evaluate the module result\n            module_result_summary = evaluate_results(module_result, previous_result, strategies)\n\n            # Save the result\n            module_result.to_csv(module_result_path, index=False)\n            module_result_summary.to_csv(module_result_summary_path, index=False)\n\n        # Append the result to the list\n        results.append(module_"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Import the necessary modules\n    import pandas as pd\n    import time\n    import os\n    import numpy as np\n    from pathlib import Path\n    from .evaluation import evaluate_results\n    from .utils import save_results\n\n    # Initialize the list of results\n    results = []\n\n    # Initialize the list of execution times\n    execution_times = []\n\n    # Initialize the list of best results\n    best_results = []\n\n    # Initialize the list of best results' metrics\n    best_results_metrics = []\n\n    # Initialize the list of best results' metrics\n    best_results_times = []\n\n    # Initialize the list of best results' metrics\n    best_results_strategies = []\n\n    # Initialize the list of best results' metrics\n    best_results_names = []\n\n    # Initialize the list of best results' metrics\n    best_results_params = []\n\n    # Initialize the list of best results' metrics\n    best_results_params_names = []\n\n    # Initialize the list of best results' metrics\n    best_results_params_values = []\n\n    # Initialize the list of best results' metrics\n    best_results_params_values_names = []\n\n    # Initialize the list of best results' metrics\n    best_results_params_values_values = []\n\n    # Initialize the list of best results' metrics\n    best_results_params_values_values_names = []\n\n    # Initialize the list of best results' metrics\n    best_results_params_values_values_values = []\n\n    # Initialize the list of best results' metrics\n    best_results_params_values_values_values_names = []\n\n    # Initialize the list of best results' metrics\n    best_results_params_values_values_values_values = []\n\n    # Initialize the list of best results' metrics\n    best_results_params_values_values_values_values_names = []\n\n    # Initialize the list of best results' metrics\n    best_results_params_values_values_values_values_values = []\n\n    # Initialize the list of best results' metrics\n    best_results_params_"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Import the necessary modules\n    import pandas as pd\n    import time\n    import os\n    from pathlib import Path\n    from .prompt_maker_modules import *\n    from .prompt_evaluation_modules import *\n    from .prompt_generation_modules import *\n\n    # Create the necessary directories\n    Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n    Path(os.path.join(node_line_dir, 'results')).mkdir(parents=True, exist_ok=True)\n    Path(os.path.join(node_line_dir, 'results', 'prompt_maker')).mkdir(parents=True, exist_ok=True)\n    Path(os.path.join(node_line_dir, 'results', 'prompt_evaluation')).mkdir(parents=True, exist_ok=True)\n    Path(os.path.join(node_line_dir, 'results', 'prompt_generation')).mkdir(parents=True, exist_ok=True)\n\n    # Create a dataframe to store the results\n    results = pd.DataFrame()\n\n    # Create a dataframe to store the summary\n    summary = pd.DataFrame()\n\n    # Create a list to store the execution times\n    execution_times = []\n\n    # Create a list to store the evaluation metrics\n    evaluation_metrics = []\n\n    # Create a list to store the best prompt maker module\n    best_prompt_maker = []\n\n    # Create a list to store the best prompt maker module's parameters\n    best_prompt_maker_params = []\n\n    # Create a list to store the best prompt maker module's execution time\n    best_prompt_maker_execution_time = []\n\n    # Create a list to store the best prompt maker module's evaluation metrics\n    best_prompt_maker_evaluation_metrics = []\n\n    # Create a list to store the best prompt maker module's prompt generation results\n    best_prompt_maker_prompt_generation_results = []\n\n    # Create a list"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    # Extract values from each node\n    values = [node.module_params[key] for node in nodes]\n\n    # Remove duplicates\n    values = list(set(values))\n\n    return values\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n\n    gt_embeddings = embedding_model.encode(generation_gt)\n    pred_embedding = embedding_model.encode([pred])\n\n    cos_sim = cosine_similarity(pred_embedding, gt_embeddings)\n\n    return cos_sim.max()"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    # Import the necessary libraries\n    import os\n    import numpy as np\n    import cv2\n    import torch\n    import torch.nn as nn\n    import kornia as K\n    import kornia.augmentation as KA\n    import kornia.geometry.transform as KGT\n    import models\n    import utils\n    import glob\n    import time\n    import argparse\n    import torchvision.transforms as transforms\n    import torchvision.datasets as datasets\n    import torch.nn.functional as F\n    import os\n    import glob\n    import time\n    import argparse\n    import torchvision.transforms as transforms\n    import torchvision.datasets as datasets\n    import torch.nn.functional as F\n    import os\n    import glob\n    import time\n    import argparse\n    import torchvision.transforms as transforms\n    import torchvision.datasets as datasets\n    import torch.nn.functional as F\n    import os\n    import glob\n    import time\n    import argparse\n    import torchvision.transforms as transforms\n    import torchvision.datasets as datasets\n    import torch.nn.functional as F\n    import os\n    import glob\n    import time\n    import argparse\n    import torchvision.transforms as transforms\n    import torchvision.datasets as datasets\n    import torch.nn.functional as F\n    import os\n    import glob\n    import time\n    import argparse\n    import torchvision.transforms as transforms\n    import torchvision.datasets as datasets\n    import torch.nn.functional as F\n    import os\n    import glob\n    import time\n    import argparse\n    import torchvision.transforms as transforms\n    import torchvision.datasets as datasets\n    import torch.nn.functional as F\n    import os\n    import glob\n    import time\n    import argparse\n    import torchvision.transforms as transforms\n    import torchvision.datasets as datasets\n    import torch.nn.functional as F\n    import os\n    import"}
{"namespace": "codeformer_model.setup_model", "completion": "    # Import the necessary modules\n    import os\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torchvision.transforms as transforms\n    import numpy as np\n    import cv2\n    import glob\n    import dill\n    import logging\n    import warnings\n    import face_recognition\n    import face_recognition_models\n    import PIL.Image\n    import PIL.ImageFile\n    import PIL.ImageFilter\n    import PIL.ImageOps\n    import PIL.ImageStat\n    import PIL.ImageEnhance\n    import PIL.ImageChops\n    import PIL.ImageDraw\n    import PIL.ImageColor\n    import PIL.ImagePath\n    import PIL.ImageMode\n    import PIL.ImageSequence\n    import PIL.ImagePalette\n    import PIL.ImageWin\n    import PIL.ImageContainer\n    import PIL.GifImagePlugin\n    import PIL.BmpImagePlugin\n    import PIL.ImImagePlugin\n    import PIL.ImtImagePlugin\n    import PIL.McIdasImagePlugin\n    import PIL.MspImagePlugin\n    import PIL.PalmImagePlugin\n    import PIL.PcdImagePlugin\n    import PIL.PcxImagePlugin\n    import PIL.PdfImagePlugin\n    import PIL.PixarImagePlugin\n    import PIL.PngImagePlugin\n    import PIL.PpmImagePlugin\n    import PIL.SgiImagePlugin\n    import PIL.SpiderImagePlugin\n    import PIL.SunImagePlugin\n    import PIL.TgaImagePlugin\n    import PIL.TiffImagePlugin\n    import PIL.WebPImagePlugin\n    import PIL.WmfImagePlugin\n    import PIL.XVThumbImagePlugin\n    import PIL.JpegImagePlugin\n    import PIL.GimpGradientFile\n    import PIL.GimpPaletteFile\n    import PIL.MpegImagePlugin\n    import PIL.MpoImagePlugin\n    import PIL.GribStubImagePlugin\n   "}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        import facexlib\n        facexlib.init(dirname)\n        from gfpgan.archs.gfpganv1_clean_arch import GFPGANv1Clean\n        from gfpgan.archs.gfpganv1_arch import GFPGANv1\n        from gfpgan.restorers.gfpgan_degradation_restorer import GFPGANDegradationRestorer\n        from gfpgan.archs.gfpganv1_clean_arch import GFPGANv1Clean\n        from gfpgan.archs.gfpganv1_arch import GFPGANv1\n        from gfpgan.restorers.gfpgan_degradation_restorer import GFPGANDegradationRestorer\n        from gfpgan.restorers.gfpgan_restorer import GFPGANRestorer\n        from gfpgan.utils.load_model_util import load_default_gfpgan_v1_face\n        from gfpgan.utils.load_model_util import load_default_gfpgan_v1_clean_face\n        from gfpgan.utils.load_model_util import load_default_gfpgan_v1_config\n        from gfpgan.utils.load_model_util import load_default_gfpgan_v1_restorer_config\n        from gfpgan.utils.load_model_util import load_default_gfpgan_v1_clean_restorer_config\n        from gfpgan.utils.load_model_util import load_default_gfpgan_config\n        from gfpgan.utils.load_model_util import load_default_gfpgan_restorer_config\n        from gfpgan.utils.load_model_util import load_default_gfpgan_clean_restorer_config\n        from gfpgan.utils.load_model_util import load_default_gfpgan_clean_config\n        from gfpg"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion\n  v = np.append(0, v)\n\n  # Apply the rotation\n  q = quaternion.as_quat_array(q)\n  v = quaternion.as_quat_array(v)\n  v = quaternion.quaternion_multiply(quaternion.quaternion_multiply(q, v), quaternion.quaternion_conjugate(q))\n\n  # Convert the quaternion back to a vector\n  v = quaternion.as_float_array(v)[1:]\n\n  return v\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Convert the axis-angle to a quaternion\n  axis = jnp.array(axis_angle[:3])\n  angle = axis_angle[3]\n\n  # Normalize the axis\n  axis = axis / (jnp.linalg.norm(axis) + eps)\n\n  # Compute the quaternion\n  quaternion = jnp.append(\n    jnp.cos(angle / 2),\n    jnp.sin(angle / 2) * axis\n  )\n\n  return quaternion\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk, could be done outside and passed in\n    topk = model.get_topk(prefix, k=k)\n    topk_indices = topk.indices\n    topk_log_probs = topk.log_probs\n\n    # adjust bias until target idx is in topk\n    while idx not in topk_indices:\n        # adjust bias\n        model.adjust_bias(prefix, idx, high)\n\n        # get new topk\n        topk = model.get_topk(prefix, k=k)\n        topk_indices = topk.indices\n        topk_log_probs = topk.log_probs\n\n        # increment high bias\n        high += 1\n\n    # get log prob of target idx\n    idx_log_probs = topk_log_probs[topk_indices == idx]\n\n    # return log prob and number of calls\n    return idx_log_probs, model.get_num_calls()\n\n"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order == 'xyz':\n    locations = tf.reverse(locations, axis=[-1])\n\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = tf.pad(data, [[0, 0], [1, 1], [1, 1], [1, 1], [0, 0]], constant_values=constant_values)\n    locations = locations + 1\n\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  if method == 'TRILINEAR':\n    data = tf.pad(data, [[0, 0], [1, 1], [1, 1], [1, 1], [0, 0]], mode='SYMMETRIC')\n    locations = locations + 1\n\n  locations = tf.reverse(locations, axis=[-1])\n\n  if method == 'TRILINEAR':\n    data = tf.nn.extrapolation_pad(data, 1)\n    data = tf.expand_dims(data, axis=-2)\n    data = tf.compat.v1.batch_to_spatial_nd_interpolate(data, locations, 3, 1.0)\n    data = tf.squeeze(data, axis=-2)\n  elif method == 'NEAREST':\n    data = tf.compat.v1.batch_to_space_nd(data, [1, 1, 1], [[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    data = tf.compat.v1.batch_to_space_nd(data, [1, 1, 1], [[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    data = tf.compat.v1.batch_to_space_nd(data, [1, 1, 1], [[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    data = tf.compat.v1"}
{"namespace": "math.plus_eps", "completion": "  import numpy as np\n\n  return np.fmax(x, np.finfo(x.dtype).eps)\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n\n"}
{"namespace": "math.safe_exp", "completion": "  # define the custom gradient function\n  def grad_exp(upstream):\n    return upstream * safe_exp(x)\n\n  # define the helper function\n  def safe_exp_helper(x):\n    return tf.exp(tf.clip_by_value(x, -10, 10))\n\n  # return the result of applying the safe exponential function to the input x\n  return tf.custom_gradient(safe_exp_helper, grad_exp)(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return safe_fn(x, jnp.log)\n"}
{"namespace": "math.safe_sqrt", "completion": "  # Define the maximum value for the safe square root function\n  max_val = 1e9\n\n  # Define the safe square root function\n  def sqrt(x):\n    return tf.sqrt(tf.clip_by_value(x, 0, max_val))\n\n  # Define the gradient of the safe square root function\n  @tf.custom_gradient\n  def sqrt_grad(x):\n    # Define the custom gradient function\n    def grad(dy):\n      # Define the gradient of the safe square root function\n      return dy * 0.5 * tf.clip_by_value(x, 0, max_val) ** -0.5\n    # Return the custom gradient function\n    return tf.sqrt(tf.clip_by_value(x, 0, max_val)), grad\n\n  # Return the result of applying the safe square root function to the input x\n  return sqrt_grad(x)\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 1\n  elif p == 2:\n    return 2\n  elif p == 3:\n    return 3\n  elif p == 4:\n    return 4\n  elif p == 5:\n    return 5\n  elif p == 6:\n    return 6\n  elif p == 7:\n    return 7\n  elif p == 8:\n    return 8\n  elif p == 9:\n    return 9\n  elif p == 10:\n    return 10\n  elif p == 11:\n    return 11\n  elif p == 12:\n    return 12\n  elif p == 13:\n    return 13\n  elif p == 14:\n    return 14\n  elif p == 15:\n    return 15\n  elif p == 16:\n    return 16\n  elif p == 17:\n    return 17\n  elif p == 18:\n    return 18\n  elif p == 19:\n    return 19\n  elif p == 20:\n    return 20\n  elif p == 21:\n    return 21\n  elif p == 22:\n    return 22\n  elif p == 23:\n    return 23\n  elif p == 24:\n    return 24\n  elif p == 25:\n    return 25\n  elif p == 26:\n    return 26\n  elif p == 27:\n    return 27\n  elif p == 28:\n    return 28\n  elif p == 29:\n    return 29\n  elif p == 30:\n    return 30\n  elif p == 31:\n    return 31\n  elif p == 32:\n    return 32\n  elif p == 33:\n    return 33\n  elif p == 34:\n    return 34\n  elif p == 3"}
{"namespace": "geopoly.generate_basis", "completion": "  # Check input arguments\n  if base_shape not in ['tetrahedron', 'icosahedron', 'octahedron']:\n    raise ValueError('Invalid base shape.')\n  if angular_tesselation < 1:\n    raise ValueError('Invalid number of tesselations.')\n\n  # Define the vertices of the base shape\n  if base_shape == 'tetrahedron':\n    vertices = np.array([[1, 1, 1], [-1, -1, 1], [1, -1, -1], [-1, 1, -1]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array([\n        [-1, phi, 0],\n        [1, phi, 0],\n        [-1, -phi, 0],\n        [1, -phi, 0],\n        [0, -1, phi],\n        [0, 1, phi],\n        [0, -1, -phi],\n        [0, 1, -phi],\n        [phi, 0, -1],\n        [phi, 0, 1],\n        [-phi, 0, -1],\n        [-phi, 0, 1],\n    ])\n  elif base_shape == 'octahedron':\n    vertices = np.array([[1, 0, 0], [-1, 0, 0], [0, 1, 0], [0, -1, 0], [0, 0, 1],\n                         [0, 0, -1]])\n\n  # Tessellate the base shape\n  for i in range(angular_tesselation):\n    new_vertices = []\n    for v in vertices:\n      for e in vertices:\n        new_vertices.append(v + e)\n    vertices = np.array(new_vertices)\n\n  # Remove symmetries\n  if remove_symmetries:\n    # Normalize the vertices\n    vertices = vertices / np.linalg.norm(vert"}
{"namespace": "math.safe_log1p", "completion": "  # If the input is greater than 1e-6, the safe log1p function is applied.\n  if x > 1e-6:\n    return tf.math.log1p(x)\n  else:\n    return tf.math.log(1. + x)"}
{"namespace": "math.power_ladder", "completion": "  # Handle special cases for p\n  if p == 1:\n    return x\n  elif p == 0:\n    return np.log(x)\n  elif p == -np.inf:\n    return np.log(x) / np.log(2)\n  elif p == np.inf:\n    return 2 ** x\n\n  # Handle the general case for p\n  else:\n    return (x ** p - 1) / (x ** p + 1)"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 0:\n    return y\n  elif p == 1:\n    return y\n  elif p == 2:\n    return y\n  elif p == 3:\n    return y\n  elif p == 4:\n    return y\n  elif p == 5:\n    return y\n  elif p == 6:\n    return y\n  elif p == 7:\n    return y\n  elif p == 8:\n    return y\n  elif p == 9:\n    return y\n  elif p == 10:\n    return y\n  elif p == 11:\n    return y\n  elif p == 12:\n    return y\n  elif p == 13:\n    return y\n  elif p == 14:\n    return y\n  elif p == 15:\n    return y\n  elif p == 16:\n    return y\n  elif p == 17:\n    return y\n  elif p == 18:\n    return y\n  elif p == 19:\n    return y\n  elif p == 20:\n    return y\n  elif p == 21:\n    return y\n  elif p == 22:\n    return y\n  elif p == 23:\n    return y\n  elif p == 24:\n    return y\n  elif p == 25:\n    return y\n  elif p == 26:\n    return y\n  elif p == 27:\n    return y\n  elif p == 28:\n    return y\n  elif p == 29:\n    return y\n  elif p == 30:\n    return y\n  elif p == 31:\n    return y\n  elif p == 32:\n    return y\n  elif p == 33:\n    return y\n  elif p == 34:\n    return y\n  elif p == 35:\n    return y\n  elif p == 36:\n    return y\n  elif p == 37:\n    return y\n  elif p == 38:\n    return y\n "}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the rate of change of the learning rate\n  lr_rate = (lr_final - lr_init) / max_steps\n\n  # If a delay is specified, then scale down the initial learning rate\n  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n\n  # Calculate the learning rate for the current step\n  lr = lr_init + step * lr_rate\n\n  # If a delay is specified, then gradually increase the learning rate\n  if lr_delay_steps > 0:\n    lr = lr * min(1.0, step / lr_delay_steps)\n\n  # If the step is past the maximum number of steps, then use the final learning rate\n  if step >= max_steps:\n    lr = lr_final\n\n  return lr"}
{"namespace": "utils.dummy_rays", "completion": "  from .generate_random_rays import generate_random_rays\n\n  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # check that the input arrays have the correct dimensions\n  assert points.ndim == 2 and points.shape[1] == 3, \"points must be of shape (N, 3)\"\n  assert pixtocams.ndim == 3 and pixtocams.shape[1:] == (3, 3), \"pixtocams must be of shape (N, 3, 3)\"\n  assert camtoworlds.ndim == 3 and camtoworlds.shape[1:] == (4, 4), \"camtoworlds must be of shape (N, 4, 4)\"\n\n  # check that the input arrays have the same number of elements\n  assert points.shape[0] == pixtocams.shape[0] == camtoworlds.shape[0], \"pixtocams, camtoworlds, and points must have the same batch dimension\"\n\n  # check that the distortion parameters are provided if the camera type is not PERSPECTIVE\n  if camtype != ProjectionType.PERSPECTIVE:\n    assert distortion_params is not None, \"distortion_params must be provided for non-perspective projections\"\n\n  # check that the distortion parameters are of the correct type\n  if distortion_params is not None:\n    assert isinstance(distortion_params, dict), \"distortion_params must be a dictionary\"\n\n  # check that the distortion parameters are of the correct type\n  if distortion_params is not None:\n    assert isinstance(distortion_params, dict), \"distortion_params must be a dictionary\"\n\n  # check that the distortion parameters are of the correct type\n  if distortion_params is not None:\n    assert isinstance(distortion_params, dict), \"distortion_params must be a dictionary\"\n\n  # check that the distortion parameters are of the correct type\n  if distortion_params is not None:\n    assert isinstance(distortion_params, dict), \"distortion_params must be a dictionary\"\n\n  # check that the dist"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the rotation angle and axis from the screw axis\n  theta = jnp.linalg.norm(screw_axis[:3])\n  w_axis = screw_axis[:3] / (theta + eps)\n  v_axis = screw_axis[3:]\n\n  # Compute the exponential map\n  R = jnp.eye(3) + (jnp.sin(theta) / (theta + eps)) * jnp.cross(w_axis, v_axis) + (\n      (1 - jnp.cos(theta)) / (theta + eps)**2) * jnp.matmul(jnp.cross(w_axis, v_axis),\n                                                           jnp.cross(w_axis, v_axis))\n  t = (theta + eps) * v_axis\n\n  # Construct the homogeneous transformation matrix\n  T = jnp.block([[R, t], [jnp.array([0, 0, 0, 1])]])\n\n  return T\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute theta (rotation angle) and the rotation axis (normalized).\n  theta = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (theta + eps)\n\n  # Compute the skew-symmetric cross-product matrix of the rotation axis.\n  skew_symmetric_cross_product_matrix = jnp.array([[0, -axis[2], axis[1]],\n                                                  [axis[2], 0, -axis[0]],\n                                                  [-axis[1], axis[0], 0]])\n\n  # Compute the rotation matrix.\n  rotation_matrix = jnp.eye(3) + jnp.sin(theta) * skew_symmetric_cross_product_matrix + (1 - jnp.cos(theta)) * jnp.matmul(\n      skew_symmetric_cross_product_matrix, skew_symmetric_cross_product_matrix)\n\n  return rotation_matrix\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Import sub-functions\n  from jax import numpy as jnp\n  from .conical_frustum_to_gaussian_mean import conical_frustum_to_gaussian_mean\n  from .conical_frustum_to_gaussian_covariance import conical_frustum_to_gaussian_covariance\n\n  # Calculate the mean of the Gaussian distribution\n  mean = conical_frustum_to_gaussian_mean(d, t0, t1, base_radius)\n\n  # Calculate the covariance of the Gaussian distribution\n  covariance = conical_frustum_to_gaussian_covariance(d, t0, t1, base_radius, diag)\n\n  # Return the mean and covariance of the Gaussian distribution\n  return mean, covariance"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  import jax.numpy as jnp\n\n  # lift the cylinder to a Gaussian distribution\n  gaussian = lift_gaussian(d, t0, t1, radius, diag)\n\n  return gaussian\n\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Compute the ray origins and directions in camera coordinates.\n  if camtype == ProjectionType.PERSPECTIVE:\n    origins, directions = perspective_rays(\n      pix_x_int,\n      pix_y_int,\n      pixtocams,\n      camtoworlds,\n      distortion_params = distortion_params,\n      xnp = xnp,\n    )\n  elif camtype == ProjectionType.FISHEYE:\n    origins, directions = fisheye_rays(\n      pix_x_int,\n      pix_y_int,\n      pixtocams,\n      camtoworlds,\n      distortion_params = distortion_params,\n      xnp = xnp,\n    )\n  elif camtype == ProjectionType.PANORAMIC:\n    origins, directions = panoramic_rays(\n      pix_x_int,\n      pix_y_int,\n      pixtocams,\n      camtoworlds,\n      distortion_params = distortion_params,\n      xnp = xnp,\n    )\n  else:\n    raise ValueError(f'Unknown camera type {camtype}.')\n\n  # Compute the normalized view directions.\n  viewdirs = xnp.broadcast_to(\n    xnp.expand_dims(directions / xnp.linalg.norm(directions, axis = -1, keepdims = True), axis = -1),\n    directions.shape,\n  )\n\n  # Compute the ray differential radii.\n  radii = xnp.ones(origins.shape[:-1] + (1,))\n\n  # Compute the image plane coordinates.\n  imageplane = xnp.stack(\n    [\n      xnp.broadcast_to(pix_x_int, origins.shape[:-1] + (1,)),\n      xnp.broadcast_to(pix_y_int, origins.shape[:-1] + (1,)),\n    ],\n    axis = -1"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the adjusted distance between points\n  tdist_norm = tdist / np.linalg.norm(dirs, axis=1)\n\n  # Compute the alpha weights\n  return compute_alpha_weights_helper(density * tdist_norm, **kwargs)\n\n\n"}
{"namespace": "stepfun.sample", "completion": "  # Get the number of bins\n  num_bins = t.shape[0]\n\n  # Get the bin widths\n  t_diff = jnp.diff(t)\n\n  # Get the bin centers\n  t_center = t[:-1] + t_diff / 2\n\n  # Get the bin weights\n  w = jnp.exp(w_logits)\n\n  # Get the bin weights normalized to sum to 1\n  w_norm = w / (w.sum() + eps)\n\n  # Get the cumulative sum of the normalized bin weights\n  w_cumsum = jnp.cumsum(w_norm)\n\n  # Get the bin endpoints\n  t_left = t[:-1]\n  t_right = t[1:]\n\n  # Get the bin midpoints\n  t_mid = t[:-1] + t_diff / 2\n\n  # Get the bin midpoint coordinates\n  t_mid_coord = jnp.arange(num_bins)\n\n  # Get the bin midpoint coordinates for the left endpoints\n  t_left_coord = jnp.arange(num_bins) - 1\n\n  # Get the bin midpoint coordinates for the right endpoints\n  t_right_coord = jnp.arange(num_bins) + 1\n\n  # Get the bin midpoint coordinates for the left endpoints\n  t_left_coord = jnp.where(t_left_coord < 0, 0, t_left_coord)\n\n  # Get the bin midpoint coordinates for the right endpoints\n  t_right_coord = jnp.where(t_right_coord >= num_bins, num_bins - 1, t_right_coord)\n\n  # Get the bin midpoint coordinates for the left endpoints\n  t_left_coord = jnp.where(t_left_coord < 0, 0, t_left_coord)\n\n  # Get the bin midpoint coordinates for the right endpoints\n  t_right_coord"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function\n  if rng is None:\n    # Use linspace\n    t_samples = jnp.linspace(t[0], t[-1], num_samples)\n  else:\n    # Use inverse CDF\n    t_samples = jax.random.choice(\n        rng,\n        t,\n        shape=(num_samples,),\n        p=jnp.exp(w_logits - logsumexp(w_logits)),\n        replace=True,\n    )\n\n  # Calculate midpoints between adjacent samples\n  t_samples = (t_samples[1:] + t_samples[:-1]) / 2\n\n  # Adjust first and last intervals to ensure they are within the specified domain\n  t_samples = jnp.concatenate([\n      jnp.array([domain[0]]),\n      t_samples,\n      jnp.array([domain[1]]),\n  ])\n\n  # Jitter the first and last intervals by the same amount\n  if single_jitter:\n    t_samples = jnp.concatenate([\n        jnp.array([domain[0]]),\n        t_samples[1:-1] + jax.random.uniform(rng, shape=(num_samples - 1,)) *\n        (t_samples[1:] - t_samples[:-1]),\n        jnp.array([domain[1]]),\n    ])\n\n  return t_samples\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Check that the weights sum to 1\n  if not np.isclose(np.sum(w), 1):\n    raise ValueError(\"The weights must sum to 1.\")\n\n  # Sort the data\n  sort_ind = np.argsort(t)\n  t = t[sort_ind]\n  w = w[sort_ind]\n\n  # Compute the cumulative weights\n  cw = np.cumsum(w)\n\n  # Interpolate the percentiles\n  return np.interp(ps/100, cw, t)\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  w = w / np.trapz(w, t)\n\n  # Blur the PDF\n  w = np.convolve(w, np.exp(-(t - np.linspace(t[0], t[-1], 2 * blur_halfwidth + 1)) ** 2 / 2 / blur_halfwidth ** 2), 'same')\n\n  # Normalize the blurred PDF\n  w = w / np.trapz(w, t)\n\n  # Resample the PDF\n  return np.interp(tq, t, w)\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Get the shape of the input array\n  shape = vectors.shape\n\n  # Get the dimensionality of the vectors\n  C = shape[-1]\n\n  # Make sure the transform is a homogeneous transform\n  assert transform.shape == (C+1,C+1)\n\n  # Make sure the last dimension of the input array is the same as the number of columns of the transform\n  assert C == transform.shape[1] - 1\n\n  # Make sure the input array is a float\n  assert vectors.dtype == np.float32\n\n  # Reshape the input array to a 2D array\n  vectors = np.reshape(vectors,[-1,C])\n\n  # Add a homogeneous coordinate to each point\n  vectors = np.concatenate((vectors,np.ones_like(vectors[:,:1])),axis=-1)\n\n  # Transform the points\n  vectors = np.matmul(vectors,transform.T)\n\n  # Reshape the output array to the shape of the input array\n  vectors = np.reshape(vectors,shape)\n\n  # Return the transformed points\n  return vectors\n\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Find the number of intervals in t\n  nt = t.size()[0]\n\n  # Find the number of intervals in tp\n  ntp = tp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in vp\n  nvp = vp.size()[0]\n\n  # Find the number of values in"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # import required packages\n  import jax.numpy as jnp\n  from jax import jit\n\n  # encode the mean and variance\n  @jit\n  def encode(mean, var, min_deg, max_deg):\n\n    # encode the mean\n    mean_enc = encode_mean(mean, min_deg, max_deg)\n\n    # encode the variance\n    var_enc = encode_var(var, min_deg, max_deg)\n\n    # concatenate the mean and variance encodings\n    enc = jnp.concatenate((mean_enc, var_enc), axis = -1)\n\n    return enc\n\n  # encode the mean\n  @jit\n  def encode_mean(mean, min_deg, max_deg):\n\n    # scale the mean\n    mean_scaled = mean * 2 ** jnp.arange(min_deg, max_deg)\n\n    # encode the scaled mean\n    mean_enc = encode_sinusoidal(mean_scaled)\n\n    return mean_enc\n\n  # encode the variance\n  @jit\n  def encode_var(var, min_deg, max_deg):\n\n    # scale the variance\n    var_scaled = var * 2 ** (2 * jnp.arange(min_deg, max_deg))\n\n    # encode the scaled variance\n    var_enc = encode_sinusoidal(var_scaled)\n\n    return var_enc\n\n  # encode the sinusoidal function\n  @jit\n  def encode_sinusoidal(x):\n\n    # encode the sinusoidal function\n    enc = jnp.sin(x)\n\n    return enc\n\n  # return the encoded variables\n  return encode(mean, var, min_deg, max_deg)\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # import dependencies\n  import tensorflow as tf\n  from tensorflow.math import sin, cos\n\n  # define the function\n  def dir_enc_fn(x):\n\n    \"\"\"\n    Evaluates the directional encoding for given inputs.\n\n    Input-Output Arguments\n    :param x: 3D point (or points).\n    :return: Directional encoding of the input.\n\n    \"\"\"\n\n    # define spherical harmonics\n    def sh(x, deg):\n\n      \"\"\"\n      Evaluates the spherical harmonics for given inputs.\n\n      Input-Output Arguments\n      :param x: 3D point (or points).\n      :param deg: Int. The number of spherical harmonics degrees to use for generating the directional encoding function. It determines the complexity and accuracy of the encoding.\n      :return: Spherical harmonics of the input.\n\n      \"\"\"\n\n      # define the function\n      def sh_deg(x, deg):\n\n        \"\"\"\n        Evaluates the spherical harmonics for given inputs.\n\n        Input-Output Arguments\n        :param x: 3D point (or points).\n        :param deg: Int. The number of spherical harmonics degrees to use for generating the directional encoding function. It determines the complexity and accuracy of the encoding.\n        :return: Spherical harmonics of the input.\n\n        \"\"\"\n\n        # define the function\n        def sh_ord(x, ord):\n\n          \"\"\"\n          Evaluates the spherical harmonics for given inputs.\n\n          Input-Output Arguments\n          :param x: 3D point (or points).\n          :param ord: Int. The number of spherical harmonics degrees to use for generating the directional encoding function. It determines the complexity and accuracy of the encoding.\n          :return: Spherical harmonics of the input.\n\n          \"\"\"\n\n          # define the function\n          def sh_ang(x, ang):\n\n            \"\"\"\n            Evaluates the spherical harmonics for given inputs.\n\n            Input-Output Arguments\n            :param x: 3D point"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Importing required modules\n    import re\n    import numpy as np\n\n    # Initializing the result list\n    result = []\n\n    # Initializing the index of the current block\n    block_index = 0\n\n    # Initializing the index of the current header block\n    header_index = 0\n\n    # Initializing the index of the current list block\n    list_index = 0\n\n    # Initializing the index of the current paragraph block\n    paragraph_index = 0\n\n    # Initializing the index of the current list item\n    list_item_index = 0\n\n    # Initializing the index of the current paragraph\n    paragraph_index = 0\n\n    # Initializing the index of the current line\n    line_index = 0\n\n    # Initializing the index of the current character\n    char_index = 0\n\n    # Initializing the index of the current word\n    word_index = 0\n\n    # Initializing the index of the current word in the current line\n    line_word_index = 0\n\n    # Initializing the index of the current word in the current paragraph\n    paragraph_word_index = 0\n\n    # Initializing the index of the current word in the current list item\n    list_item_word_index = 0\n\n    # Initializing the index of the current word in the current paragraph\n    paragraph_word_index = 0\n\n    # Initializing the index of the current word in the current list\n    list_word_index = 0\n\n    # Initializing the index of the current word in the current block\n    block_word_index = 0\n\n    # Initializing the index of the current word in the current header\n    header_word_index = 0\n\n    # Initializing the index of the current word in the current list item\n    list_item_word_index = 0\n\n    # Initializing the index of the current word in the current paragraph\n    paragraph_word_index = 0\n\n    # Initializing the index of the current word in the current block\n    block_word_index = 0\n\n    # Initializing the index of the current word in the current header"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # If the input is empty or None, return the input as is\n    if not org_texts:\n        return org_texts\n\n    # Replace all new line characters with a space\n    space_rule = re.compile(r'\\n')\n    texts = space_rule.sub(' ', org_texts)\n\n    # Replace all multi spaces with a single space\n    space_rule = re.compile(r'\\s+')\n    texts = space_rule.sub(' ', texts)\n\n    # Replace all commas and semi-colons with a single space\n    comma_rule = re.compile(r',')\n    texts = comma_rule.sub(' ', texts)\n    semi_rule = re.compile(r';')\n    texts = semi_rule.sub(' ', texts)\n\n    # Replace all quotation marks with a single space\n    quotation_pattern = re.compile(r'[\\u2018\\u2019\\u201c\\u201d\\u0022\\u201a\\u201b\\u201e\\u201f\\u2039\\u203a\\u00ab\\u00bb\\u2019\\u201d\\u2018\\u201c\\u00ab\\u00bb\\u201b\\u201e\\u201f\\u2039\\u203a\\u00ab\\u00bb\\u2019\\u201d\\u2018\\u201c\\u00ab\\u00bb\\u201b\\u201e\\u201f\\u2039\\u203a\\u00ab\\u00bb\\u2019\\u201d\\u2018\\u201c\\u00ab\\u00bb\\u201b\\u201e\\u201f\\u2039\\u203a\\u"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        # If a key is provided, return the positions for that document only\n        if key is not None:\n            return self.documents[key].positions(token)\n\n        # Otherwise, return the positions for all documents\n        return [document.positions(token) for document in self.documents.values()]\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # If the spec is a number, return it as an integer\n    if spec.isnumeric():\n        return int(spec)\n\n    # If the spec is a percentage, calculate the minimum number of clauses that must match\n    if spec.endswith('%'):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n\n    # If the spec is a conditional expression, calculate the minimum number of clauses that must match\n    if spec.startswith('<'):\n        return int(spec[1:]) - 1\n\n    # If the spec is a conditional expression, calculate the minimum number of clauses that must match\n    if spec.startswith('>'):\n        return num_clauses - int(spec[1:])\n\n    # If the spec is a conditional expression, calculate the minimum number of clauses that must match\n    if spec.startswith('<='):\n        return int(spec[2:])\n\n    # If the spec is a conditional expression, calculate the minimum number of clauses that must match\n    if spec.startswith('>='):\n        return num_clauses - int(spec[2:])\n\n    # If the spec is a conditional expression, calculate the minimum number of clauses that must match\n    if spec.startswith('='):\n        return int(spec[1:])\n\n    # If the spec is a conditional expression, calculate the minimum number of clauses that must match\n    if spec.startswith('!='):\n        return num_clauses - int(spec[2:])\n\n    # If the spec is a conditional expression, calculate the minimum number of clauses that must match\n    if spec.startswith('<='):\n        return int(spec[2:])\n\n    # If the spec is a conditional expression, calculate the minimum number of clauses that must match\n    if spec.startswith('>='):\n        return num_clauses - int(spec[2:])\n\n    # If the spec is a conditional expression, calculate the minimum number of clauses that must match\n    if spec.startswith('='):\n        return int(spec[1:])\n\n    # If the spec is a"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If the slop is 1 and all tokens are unique, we can directly calculate the phrase frequency using the positions of terms.\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_indirect(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Initialize the return object\n        search_array = cls()\n\n        # Initialize the term dictionary\n        search_array.term_dict = {}\n\n        # Initialize the term matrix\n        search_array.term_matrix = []\n\n        # Initialize the positions\n        search_array.positions = []\n\n        # Initialize the document lengths\n        search_array.doc_lengths = []\n\n        # Initialize the average document length\n        search_array.avg_doc_length = 0\n\n        # Initialize the document count\n        search_array.doc_count = 0\n\n        # Initialize the total terms\n        search_array.total_terms = 0\n\n        # Initialize the total tokens\n        search_array.total_tokens = 0\n\n        # Initialize the total documents\n        search_array.total_docs = 0\n\n        # Initialize the total positions\n        search_array.total_positions = 0\n\n        # Initialize the total unique terms\n        search_array.total_unique_terms = 0\n\n        # Initialize the total unique positions\n        search_array.total_unique_positions = 0\n\n        # Initialize the total unique documents\n        search_array.total_unique_docs = 0\n\n        # Initialize the total unique tokens\n        search_array.total_unique_tokens = 0\n\n        # Initialize the total unique positions\n        search_array.total_unique_positions = 0\n\n        # Initialize the total unique documents\n        search_array.total_unique_docs = 0\n\n        # Initialize the total unique tokens\n        search_array.total_unique_tokens = 0\n\n        # Initialize the total unique positions\n        search_array.total_unique_positions = 0\n\n        # Initialize the total unique documents\n        search_array.total_unique_docs = 0\n\n        # Initialize the total unique tokens\n        search_array.total_unique_tokens = 0\n\n        # Initialize the total unique positions\n        search_array.total_unique_positions = 0\n\n        # Initialize the total unique documents\n        search_array.total_unique_docs"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Initializing ProxifierMessageInterceptor...\")\n\n        self.logger.info(\"Setting up the server...\")\n        self.server = ProxifierMessageInterceptorServer(self.configuration['server_ip'], self.configuration['server_port'], self.configuration['server_buffer_size'], self.configuration['server_queue_size'], self.logger)\n\n        self.logger.info(\"Starting the server...\")\n        self.server.start()\n\n        self.connections = {}\n        self.connections_lock = threading.Lock()\n\n        self.logger.info(\"ProxifierMessageInterceptor initialized.\")\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Iterate over the array\n    for i in range(len(arr)):\n\n        # Use bitwise operations to count the number of bits set to 1 in the current element\n        arr[i] = arr[i] - ((arr[i] >> 1) & 0x5555555555555555)\n        arr[i] = (arr[i] & 0x3333333333333333) + ((arr[i] >> 2) & 0x3333333333333333)\n        arr[i] = (arr[i] + (arr[i] >> 4)) & 0x0F0F0F0F0F0F0F0F\n        arr[i] = arr[i] + (arr[i] >> 8)\n        arr[i] = arr[i] + (arr[i] >> 16)\n        arr[i] = arr[i] + (arr[i] >> 32)\n\n    return arr\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Check that the query is a string\n    if not isinstance(q, str):\n        raise TypeError(\"The query must be a string.\")\n\n    # Check that the query fields are a list of strings\n    if not isinstance(qf, list) or not all(isinstance(field, str) for field in qf):\n        raise TypeError(\"The query fields must be a list of strings.\")\n\n    # Check that the minimum should match specification is a string or None\n    if not isinstance(mm, (str, type(None))):\n        raise TypeError(\"The minimum should match specification must be a string or None.\")\n\n    # Check that the phrase fields are a list of strings or None\n    if not isinstance(pf, (list, type(None))) or not all(isinstance(field, str) for field in pf):\n        raise TypeError(\"The phrase fields must be a list of strings or None.\")\n\n    # Check that the bigram fields are a list of strings or None\n    if not isinstance(pf2, (list, type(None))) or not all(isinstance(field, str) for field in pf2):\n        raise TypeError(\"The bigram fields must be a list of strings or None.\")\n\n    # Check that the trigram fields are a list of strings or None\n    if not isinstance(pf3, (list, type(None))) or not all(isinstance(field, str) for field in pf3):\n        raise TypeError(\"The trigram fields must be a list of strings or None.\")\n\n    # Check that the query operator is a string\n    if not isinstance(q_op, str):\n        raise TypeError(\"The query operator must be a string.\")\n\n    # Check that the query operator is either \"OR\" or \"AND\"\n    if q_op not in [\"OR\", \"AND\"]:\n        raise ValueError(\"The query operator must be either 'OR' or 'AND'.\")\n\n    # Check that the similarity measure is a Similarity object\n    if not isinstance(similarity, Similarity):\n        raise TypeError(\"The similarity measure must be a Similarity object.\")\n\n    # Check that the DataFrame is not empty\n    if frame.empty:"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = process.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.close()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections:\n            connection.stop()\n\n        # Stop the server\n        if self.server:\n            self.server.stop()\n\n        # Clear the connections list\n        self.connections = []\n\n        # Clear the server\n        self.server = None\n\n        return\n\n"}
