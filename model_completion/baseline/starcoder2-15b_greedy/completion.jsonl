{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except:\n        return False\n"}
{"namespace": "coord.inv_contract", "completion": "  return np.array([z[0], z[1], z[2], z[3], z[4], z[5], z[6], z[7], z[8], z[9], z[10], z[11], z[12], z[13], z[14], z[15], z[16], z[17], z[18], z[19], z[20], z[21], z[22], z[23], z[24], z[25], z[26], z[27], z[28], z[29], z[30], z[31], z[32], z[33], z[34], z[35], z[36], z[37], z[38], z[39], z[40], z[41], z[42], z[43], z[44], z[45], z[46], z[47], z[48], z[49], z[50], z[51], z[52], z[53], z[54], z[55], z[56], z[57], z[58], z[59], z[60], z[61], z[62], z[63], z[64], z[65], z[66], z[67], z[68], z[69], z[70], z[71], z[72], z[73], z[74], z[75], z[76], z[77], z[78], z[79], z[80], z[81], z[82], z[83], z[84], z[85], z[86], z[87], z[88], z[89], z[90], z[91], z[92], z[93], z[94], z[95], z[96], z[97], z[98], z[99], z[10"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import hashlib\n    import pickle\n\n    def memoize(func):\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result BLOB)\")\n            c.execute(\"SELECT * FROM cache WHERE func_name = ? AND args = ? AND kwargs = ?\", (func_name, str(args), str(kwargs)))\n            row = c.fetchone()\n            if row is None:\n                result = func(*args, **kwargs)\n                c.execute(\"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\", (func_name, str(args), str(kwargs), pickle.dumps(result)))\n                conn.commit()\n                return result\n            else:\n                return pickle.loads(row[3])\n        return wrapper\n    return memoize"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__} bounding box x_min must be less than x_max\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__} bounding box y_min must be less than y_max\")\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of the columns of each matrix\n  norm0 = np.sum(np.square(mat0), axis=0)\n  norm1 = np.sum(np.square(mat1), axis=0)\n\n  # Compute the dot product of the columns of each matrix\n  dot = np.dot(mat0.T, mat1)\n\n  # Compute the squared distances between the columns of each matrix\n  dist = -2 * dot + norm0[:, None] + norm1[None, :]\n\n  # Set negative distances to zero\n  dist[dist < 0] = 0\n\n  return dist"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return False\n    if path == \"\":\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return False\n    if path.startswith(\"://\"):\n        return"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"The assets_names must be provided when {name} is a dictionary.\"\n            )\n        items = np.array(\n            [items.get(asset, fill_value) for asset in assets_names]\n        ).reshape(-1)\n    elif isinstance(items, npt.ArrayLike):\n        items = np.array(items).reshape(-1)\n    else:\n        raise ValueError(\n            f\"The {name} must be a dictionary, numpy array, or any array-like structure.\"\n        )\n\n    if items.shape[0] != n_assets:\n        raise ValueError(\n            f\"The {name} must have shape ({n_assets},) for dim=1 or ({n_assets},) for dim=2.\"\n        )\n\n    if dim == 1:\n        return items.reshape(-1)\n    elif dim == 2:\n        return items.reshape(1, -1)\n    else:\n        raise ValueError(f\"The dim must be 1 or 2.\")\n\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        if isinstance(data, dict):\n            agent = MicroAgent(agent_lifecycle, openai_wrapper)\n            agent.id = data.get(\"id\")\n            agent.name = data.get(\"name\")\n            agent.description = data.get(\"description\")\n            agent.image = data.get(\"image\")\n            agent.prompt = data.get(\"prompt\")\n            agent.response = data.get(\"response\")\n            agent.is_active = data.get(\"is_active\")\n            agent.is_trained = data.get(\"is_trained\")\n            agent.is_ready = data.get(\"is_ready\")\n            agent.is_error = data.get(\"is_error\")\n            agent.error_message = data.get(\"error_message\")\n            agent.is_deleted = data.get(\"is_deleted\")\n            agent.is_deleted_permanently = data.get(\"is_deleted_permanently\")\n            agent.is_deleted_permanently_at = data.get(\"is_deleted_permanently_at\")\n            agent.created_at = data.get(\"created_at\")\n            agent.updated_at = data.get(\"updated_at\")\n            agent.deleted_at = data.get(\"deleted_at\")\n            agent.deleted_permanently_at = data.get(\"deleted_permanently_at\")\n            agent.deleted_by = data.get(\"deleted_by\")\n            agent.deleted_permanently_by = data.get(\"deleted_permanently_by\")\n            agent.deleted_by_name = data.get(\"deleted_by_name\")\n            agent.deleted_permanently_by_name = data.get(\"deleted_permanently_by_name\")\n            agent.deleted_by_email = data.get(\"deleted_by_email\")\n            agent.deleted_permanently_by_email = data.get(\"deleted_permanently_by_email\")\n            agent.deleted_by_role = data.get(\"deleted_by_role\")\n            agent.deleted_permanently_by_role = data.get(\"deleted_permanently_by_role\")\n            agent.deleted_by_organization = data.get(\"deleted_by_organization\")\n            agent.deleted_permanently_by_organization = data.get(\"deleted_permanently_by"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  return xnp.where(srgb <= 0.04045, srgb / 12.92, ((srgb + 0.055) / 1.055) ** 2.4)\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import UnivariateSpline\n\n  # Check if the input and output times are arrays\n  if not isinstance(t_input, np.ndarray):\n    raise TypeError(\"t_input must be a numpy array.\")\n  if not isinstance(t_output, np.ndarray):\n    raise TypeError(\"t_output must be a numpy array.\")\n\n  # Check if the input and output times are 1-dimensional\n  if t_input.ndim != 1:\n    raise ValueError(\"t_input must be a 1-dimensional array.\")\n  if t_output.ndim != 1:\n    raise ValueError(\"t_output must be a 1-dimensional array.\")\n\n  # Check if the input and output times are sorted\n  if not np.all(np.diff(t_input) > 0):\n    raise ValueError(\"t_input must be sorted in ascending order.\")\n  if not np.all(np.diff(t_output) > 0):\n    raise ValueError(\"t_output must be sorted in ascending order.\")\n\n  # Check if the input and output times are of the same length\n  if len(t_input) != len(x):\n    raise ValueError(\"t_input and x must have the same length.\")\n\n  # Check if the input and output times are of the same length\n  if len(t_output) != len(x):\n    raise ValueError(\"t_output and x must have the same length.\")\n\n  # Check if the input and output times are of the same length\n  if len(t_input) != len(t_output):\n    raise ValueError(\"t_input and t_output must have the same length.\")\n\n  # Check if the input and output times are of the same length\n  if len(x) != len(t_output):\n    raise ValueError(\"x and t_output must have the same length.\")\n\n  # Check if the input and output times are of the same length\n  if len(x) != len(t_input):\n    raise ValueError(\"x and t_input must have the same length.\")\n\n  # Check if the input and output times are of the same length\n  if len(t_input) != len(t_output):\n    raise ValueError(\"t_input and t_output must have the same length.\")\n\n  # Check if the input and output times are of the same length\n  if len("}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    elif word.isupper():\n        return word\n    elif word.istitle():\n        return word\n    elif word[0].isupper():\n        return word.capitalize()\n    else:\n        return word.lower()\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}.{field.name} must be boolean, not {v.dtype}\"\n        )\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  import numpy as np\n\n  # Check if the input is 3D\n  if len(x.shape) != 3:\n    raise ValueError(\"Input must be 3D\")\n\n  # Calculate the norm of the input\n  norm = np.linalg.norm(x)\n\n  # Check if the norm is zero\n  if norm == 0:\n    raise ValueError(\"Norm cannot be zero\")\n\n  # Calculate the scaling factor\n  scale = 1 / norm\n\n  # Apply the scaling operation\n  y = scale * x\n\n  return y"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    df = pd.read_csv(summary_path)\n    if dict_columns is None:\n        dict_columns = ['module_params']\n    for col in dict_columns:\n        df[col] = df[col].apply(lambda x: ast.literal_eval(x))\n    return df\n\n"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  if mode == 'fast':\n    det = np.linalg.det(cov)\n    if det == 0:\n      return np.zeros(cov.shape)\n    else:\n      return np.eye(cov.shape[0]) * det\n  elif mode == 'accurate':\n    logdet = np.linalg.slogdet(cov)[1]\n    if logdet == 0:\n      return np.zeros(cov.shape)\n    else:\n      return np.eye(cov.shape[0]) * np.exp(logdet)\n  else:\n    raise ValueError(\"Invalid mode: %s\" % mode)"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"A tool for executing tasks.\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be retried upon failure.\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file, specifying where to find the configuration settings.\")\n    args = parser.parse_args()\n    return args"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points\")\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    char_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n    return char_set[n]\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.clip(x, a_min = eps, a_max = None))\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_id, intervals in workers_intervals.items():\n        chunk_indexes[worker_id] = 0\n        for interval in intervals:\n            indexes[worker_id] += len(interval) - 1\n            chunk_indexes[worker_id] += 1\n    return chunk_indexes, indexes\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  import numpy as np\n\n  if datastructure == \"grid\":\n    # Adjust coordinates to be within the bounds of the voxel grid\n    coordinates = np.clip(coordinates, 0, values.shape[:3] - 1)\n\n    # Convert coordinates to integer indices\n    indices = coordinates.astype(int)\n\n    # Extract the values at the indices\n    values_at_indices = values[indices[:, 0], indices[:, 1], indices[:, 2]]\n\n    # Calculate the fractional parts of the coordinates\n    fractions = coordinates - indices\n\n    # Perform trilinear interpolation\n    interpolated_values = (\n        values_at_indices * (1 - fractions[:, 0]) * (1 - fractions[:, 1]) * (1 - fractions[:, 2])\n        + values[indices[:, 0] + 1, indices[:, 1], indices[:, 2]] * fractions[:, 0] * (1 - fractions[:, 1]) * (1 - fractions[:, 2])\n        + values[indices[:, 0], indices[:, 1] + 1, indices[:, 2]] * (1 - fractions[:, 0]) * fractions[:, 1] * (1 - fractions[:, 2])\n        + values[indices[:, 0], indices[:, 1], indices[:, 2] + 1] * (1 - fractions[:, 0]) * (1 - fractions[:, 1]) * fractions[:, 2]\n        + values[indices[:, 0] + 1, indices[:, 1] + 1, indices[:, 2]] * fractions[:, 0] * fractions[:, 1] * (1 - fractions[:, 2])\n        + values[indices[:, 0] + 1, indices[:, 1], indices[:, 2] + 1] * fractions[:, 0] * (1 - fractions[:, 1]) * fractions[:, 2]\n        + values[indices[:, 0], indices[:, 1] + 1, indices[:, 2] + 1] * (1 - fractions[:, 0]) * fractions[:, 1] * fractions[:, 2]\n        + values[indices[:, 0] + 1, indices[:, 1] + 1, indices[:, 2] + 1] * fractions[:, "}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the tessellation factor is valid\n  if v < 1:\n    raise ValueError(\"The tessellation factor must be greater than or equal to 1.\")\n\n  # Initialize the barycentric weights array\n  weights = np.zeros((v + 1) ** 2, dtype=np.float64)\n\n  # Compute the barycentric weights for each point in the tessellated triangle\n  for i in range(v + 1):\n    for j in range(v + 1):\n      weights[i * (v + 1) + j] = i * j * (v - i - j)\n\n  # Normalize the barycentric weights\n  weights /= (v + 1) ** 3\n\n  # Return the barycentric weights\n  return weights"}
{"namespace": "linspline.query", "completion": "  import numpy as np\n\n  # Check that the time points are in ascending order\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"Time points must be in ascending order.\")\n\n  # Check that the time points and values have the same length\n  if len(t) != len(v):\n    raise ValueError(\"Time points and values must have the same length.\")\n\n  # Check that the query points are in ascending order\n  if not np.all(np.diff(tq) > 0):\n    raise ValueError(\"Query points must be in ascending order.\")\n\n  # Check that the query points are within the range of the time points\n  if not np.all((tq >= np.min(t)) & (tq <= np.max(t))):\n    raise ValueError(\"Query points must be within the range of the time points.\")\n\n  # Find the index of the first time point greater than or equal to each query point\n  idx = np.searchsorted(t, tq)\n\n  # Calculate the slope of the line between each pair of time points and values\n  slope = (v[1:] - v[:-1]) / (t[1:] - t[:-1])\n\n  # Calculate the interpolated values at each query point\n  vq = v[idx - 1] + slope[idx - 1] * (tq - t[idx - 1])\n\n  # Set the interpolated values to 0 if the query point is outside the range of the time points\n  vq[tq < np.min(t)] = 0\n  vq[tq > np.max(t)] = 0\n\n  return vq"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable) and not all(isinstance(x, int) for x in v):\n        raise ValueError(\n            f\"All values in {cls.__name__}.{field.name} must be positive.\"\n        )\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert the origins to NDC\n  origins_ndc = xnp.matmul(pixtocam, xnp.vstack((origins, xnp.ones(origins.shape[1]))))\n  origins_ndc = origins_ndc / origins_ndc[2, :]\n  origins_ndc = origins_ndc[:2, :]\n\n  # Convert the directions to NDC\n  directions_ndc = xnp.matmul(pixtocam, xnp.vstack((directions, xnp.zeros(directions.shape[1]))))\n  directions_ndc = directions_ndc / directions_ndc[2, :]\n  directions_ndc = directions_ndc[:2, :]\n\n  # Adjust the origins to the near plane\n  origins_ndc = origins_ndc * near / origins_ndc[2, :]\n\n  return origins_ndc, directions_ndc"}
{"namespace": "geometry.are_lines_parallel", "completion": "  import numpy as np\n\n  dir1 = np.array(dir1)\n  dir2 = np.array(dir2)\n\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  return np.isclose(np.dot(dir1, dir2), 1)\n\n"}
{"namespace": "common.bleu4_score", "completion": "    from nltk.translate import bleu_score\n    from nltk.tokenize import word_tokenize\n\n    def tokenize(text: str) -> list:\n        \"\"\"\n        Tokenizes a given text string using the word_tokenize function from the nltk library.\n\n        Input-Output Arguments\n        :param text: str, the text to be tokenized.\n        :return: list, the list of tokens generated by the tokenizer.\n        \"\"\"\n        return word_tokenize(text)\n\n    def bleu_score_with_penalty(\n        continuation: str,\n        reference: str,\n        with_penalty = False\n    ) -> float:\n        \"\"\"\n        Calculates the BLEU-4 score for a given continuation and reference text, optionally including the brevity penalty in the score calculation. It tokenizes the input texts using a custom tokenizer function before computing the BLEU score.\n\n        Input-Output Arguments\n        :param continuation: str, the generated text whose quality is being evaluated.\n        :param reference: str, the reference text against which the continuation is compared.\n        :param with_penalty: Bool, indicates whether to include the brevity penalty in the final score calculation. Defaults to False.\n        :return: float, the calculated BLEU-4 score, optionally adjusted for brevity penalty.\n        \"\"\"\n        return bleu_score.sentence_bleu(\n            references=[tokenize(reference)],\n            hypothesis=tokenize(continuation),\n            weights=(0.25, 0.25, 0.25, 0.25),\n            smoothing_function=bleu_score.SmoothingFunction().method1,\n            auto_reweigh=True,\n        )\n\n    if with_penalty:\n        return bleu_score_with_penalty(continuation, reference)\n    else:\n        return bleu_score.sentence_bleu(\n            references=[tokenize(reference)],\n            hypothesis=tokenize(continuation),\n            weights=(0.25, 0.25, 0.25, 0.25),\n            smoothing_function=bleu_score.SmoothingFunction().method1,\n            auto_reweigh=True,\n        )"}
{"namespace": "spin_math.safe_sqrt", "completion": "  if x < eps:\n    return jnp.sqrt(value_at_zero)\n  else:\n    return jnp.sqrt(x)\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  import numpy as np\n\n  # Check if the input vectors t and w have the same length\n  if len(t) != len(w):\n    raise ValueError(\"The input vectors t and w must have the same length.\")\n\n  # Check if the input vector t is sorted in ascending order\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"The input vector t must be sorted in ascending order.\")\n\n  # Check if the input vector w sums to 1\n  if not np.isclose(np.sum(w), 1):\n    raise ValueError(\"The input vector w must sum to 1.\")\n\n  # Check if the input vector w is non-negative\n  if not np.all(w >= 0):\n    raise ValueError(\"The input vector w must be non-negative.\")\n\n  # Check if the input vector w is non-zero\n  if not np.any(w > 0):\n    raise ValueError(\"The input vector w must be non-zero.\")\n\n  # Check if the input vector w is not all zeros\n  if not np.any(w != 0):\n    raise ValueError(\"The input vector w must not be all zeros.\")\n\n  # Check if the input vector w is not all ones\n  if not np.any(w != 1):\n    raise ValueError(\"The input vector w must not be all ones.\")\n\n  # Check if the input vector w is not all equal\n  if not np.any(w[1:] != w[:-1]):\n    raise ValueError(\"The input vector w must not be all equal.\")\n\n  # Check if the input vector w is not all equal to 1/len(w)\n  if not np.any(w != 1 / len(w)):\n    raise ValueError(\"The input vector w must not be all equal to 1/len(w).\")\n\n  # Check if the input vector w is not all equal to 1/len(w)\n  if not np.any(w != 1 / len(w)):\n    raise ValueError(\"The input vector w must not be all equal to 1/len(w).\")\n\n  # Check if the input vector w is not all equal to 1/len(w)\n  if not np.any(w != 1 / len(w)):\n    raise ValueError(\"The input vector w must not be all equal to 1/len(w"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n    import pathlib\n    import sys\n\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return (val - offset * period) % period + offset * period\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = agent.__dict__\n        agent_dict['purpose_embedding'] = agent.purpose_embedding.tolist()\n        return agent_dict"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of items is less than the number of bins\n    if len(items) < num_bins:\n        raise ValueError(\"The number of items is less than the number of bins.\")\n\n    # Check if the number of weights is less than the number of bins\n    if len(weights) < num_bins:\n        raise ValueError(\"The number of weights is less than the number of bins.\")\n\n    # Check if the number of items is not equal to the number of weights\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items is not equal to the number of weights.\")\n\n    # Check if the number of items is not equal to the number of weights\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items is not equal to the number of weights.\")\n\n    # Check if the number of items is not equal to the number of weights\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items is not equal to the number of weights.\")\n\n    # Check if the number of items is not equal to the number of weights\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items is not equal to the number of weights.\")\n\n    # Check if the number of items is not equal to the number of weights\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items is not equal to the number of weights.\")\n\n    # Check if the number of items is not equal to the number of weights\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items is not equal to the number of weights.\")\n\n    # Check if the number of items is not equal to the number of weights\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items is not equal to the number of weights.\")\n\n    # Check if the number of items is not equal to the number of weights\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items is not equal to the number of weights.\")\n\n    # Check if the number of items is not equal to the number of weights\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items is not equal to the number of weights.\")\n\n    # Check if the number of items is not equal to the number of weights\n    if len("}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Create a new SHA-256 hash object\n        hash_obj = hashlib.sha256()\n\n        # Update the hash object with the function name\n        hash_obj.update(func_name.encode())\n\n        # Update the hash object with the positional arguments\n        for arg in args:\n            hash_obj.update(str(arg).encode())\n\n        # Update the hash object with the keyword arguments\n        for key, value in kwargs.items():\n            hash_obj.update(str(key).encode())\n            hash_obj.update(str(value).encode())\n\n        # Return the hexadecimal digest of the hash object\n        return hash_obj.hexdigest()"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Calculate the total length of the polygon by summing the distances between consecutive points\n    total_length = 0\n    for i in range(len(polygon) - 1):\n        # Calculate the distance between the current point and the next point\n        distance = np.linalg.norm(polygon[i + 1] - polygon[i])\n        # Check if the distance is below the specified maximum\n        if distance < max_point_distance:\n            # Add the distance to the total length\n            total_length += distance\n\n    return total_length"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Find the largest polygon's area\n    largest_area = max(poly.area for poly in polygons)\n\n    # Filter out polygons whose area is below either an absolute threshold or a relative threshold compared to the largest polygon's area\n    filtered_polygons = [\n        poly for poly in polygons\n        if poly.area >= abs_tr or poly.area >= rel_tr * largest_area\n    ]\n\n    return filtered_polygons"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_batch = num_samples_per_worker // batch_size\n    num_samples_per_worker_batch_remainder = num_samples_per_worker % batch_size\n    num_samples_per_worker_remainder = num_samples_yielded % num_workers\n\n    num_samples_per_worker_dict = {}\n    for i in range(num_workers):\n        num_samples_per_worker_dict[i] = num_samples_per_worker_batch * batch_size\n        if i < num_samples_per_worker_batch_remainder:\n            num_samples_per_worker_dict[i] += batch_size\n        if i < num_samples_per_worker_remainder:\n            num_samples_per_worker_dict[i] += 1\n\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            if metadatas is not None:\n                filtered_metadatas.append(metadatas[i])\n    return filtered_results, filtered_metadatas"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2).\")\n\n    return 0.5 * np.abs(np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1)))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the input tensors are valid\n    if not isinstance(a, torch.Tensor) or not isinstance(v, torch.Tensor):\n        raise ValueError(\"Both input tensors must be PyTorch tensors.\")\n\n    # Check if the input tensors have the correct dimensions\n    if a.dim() < 1 or v.dim() < 1:\n        raise ValueError(\"Both input tensors must have at least one dimension.\")\n\n    # Check if the input tensors have the same number of dimensions\n    if a.dim() != v.dim():\n        raise ValueError(\"Both input tensors must have the same number of dimensions.\")\n\n    # Check if the input tensors have the same number of elements in the first dimension\n    if a.size(0) != v.size(0):\n        raise ValueError(\"Both input tensors must have the same number of elements in the first dimension.\")\n\n    # Check if the input tensors have the same number of elements in the last dimension\n    if a.size(-1) != v.size(-1):\n        raise ValueError(\"Both input tensors must have the same number of elements in the last dimension.\")\n\n    # Check if the input tensors have the same number of elements in the last dimension\n    if a.size(-1) != v.size(-1):\n        raise ValueError(\"Both input tensors must have the same number of elements in the last dimension.\")\n\n    # Check if the input tensors have the same number of elements in the last dimension\n    if a.size(-1) != v.size(-1):\n        raise ValueError(\"Both input tensors must have the same number of elements in the last dimension.\")\n\n    # Check if the input tensors have the same number of elements in the last dimension\n    if a.size(-1) != v.size(-1):\n        raise ValueError(\"Both input tensors must have the same number of elements in the last dimension.\")\n\n    # Check if the input tensors have the same number of elements in the last dimension\n    if a.size(-1) != v.size(-1):\n        raise ValueError(\"Both input tensors must have the same number of elements in the last dimension.\")\n\n    # Check if the input tensors have the same number of elements in the last dimension\n    if a.size(-1) != v.size(-1):\n        raise ValueError(\"Both input tensors must have the same number of elements in the last dimension.\")\n\n    # Check if the input tensors have the same number of elements in the last dimension\n    if"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])"}
{"namespace": "coord.contract", "completion": "  return np.sqrt(np.sum(np.square(x), axis=1)) * x / np.sqrt(np.sum(np.square(x), axis=1))[:, None]\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 1000:\n        return f\"{num_bytes:.2f} B\"\n    elif num_bytes < 1000 ** 2:\n        return f\"{num_bytes / 1000:.2f} KB\"\n    elif num_bytes < 1000 ** 3:\n        return f\"{num_bytes / 1000 ** 2:.2f} MB\"\n    elif num_bytes < 1000 ** 4:\n        return f\"{num_bytes / 1000 ** 3:.2f} GB\"\n    elif num_bytes < 1000 ** 5:\n        return f\"{num_bytes / 1000 ** 4:.2f} TB\"\n    else:\n        return f\"{num_bytes / 1000 ** 5:.2f} PB\"\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_n_dimensions(cls, v, field):\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"Array must have {nb_dimensions} dimensions\")\n        return v\n\n    return validate_array_n_dimensions\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = onp.sqrt(x**2 + y**2 + z**2)\n  theta = onp.arccos(z / (r + eps))\n  phi = onp.arctan2(y, x)\n  return r, theta, phi\n"}
{"namespace": "common.rougeL_score", "completion": "    from rouge import rougeL\n    from jieba import tokenize\n\n    def tokenize_text(text: str) -> list:\n        \"\"\"\n        Tokenizes a text using jieba.tokenize.\n\n        Input-Output Arguments\n        :param text: str, The text to be tokenized.\n        :return: list. The list of tokens.\n        \"\"\"\n        return [token for token in tokenize(text)]\n\n    return rougeL(\n        hypothesis=tokenize_text(continuation),\n        reference=tokenize_text(reference)\n    )"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return import_module(name)\n    except ImportError:\n        return getattr(sys.modules[__name__], name)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    import io\n    import torch\n\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the lengths of the ids and scores tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\"The lengths of the ids and scores tuples must be equal.\")\n\n    # Check if the lengths of the ids, scores, and weights tuples match\n    if len(ids) != len(scores) != len(weights):\n        raise ValueError(\"The lengths of the ids, scores, and weights tuples must be equal.\")\n\n    # Check if the sum of the weights is equal to 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must be equal to 1.\")\n\n    # Normalize the scores of each retrieval result based on the weights\n    normalized_scores = []\n    for i, score in enumerate(scores):\n        normalized_scores.append([s * weights[i] for s in score])\n\n    # Combine the normalized scores of each retrieval result\n    combined_scores = []\n    for i in range(len(normalized_scores[0])):\n        combined_scores.append(sum([s[i] for s in normalized_scores]))\n\n    # Sort the combined scores and their corresponding ids in descending order\n    sorted_scores = sorted(combined_scores, reverse=True)\n    sorted_ids = [ids[i] for i in sorted(range(len(combined_scores)), key=lambda i: combined_scores[i], reverse=True)]\n\n    # Select the top_k results based on the sorted scores and ids\n    top_scores = sorted_scores[:top_k]\n    top_ids = sorted_ids[:top_k]\n\n    return top_ids, top_scores"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n\n    if percent:\n        x *= 100\n        return f\"{x:.1f}%\"\n\n    if x < 1:\n        return f\"{x:.3f}\"\n    if x < 10:\n        return f\"{x:.2f}\"\n    if x < 100:\n        return f\"{x:.1f}\"\n    return f\"{x:.0f}\"\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        disk_usage = os.statvfs(input_dir).f_bavail * os.statvfs(input_dir).f_frsize / (1024 * 1024 * 1024)\n        if disk_usage < threshold_in_gb:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  import numpy as np\n\n  # Calculate the differences between consecutive elements in 't'\n  dt = np.diff(t)\n\n  # Multiply the PDF values by the differences to obtain the weights\n  w = p * dt\n\n  # Normalize the weights to sum to 1\n  w = w / np.sum(w)\n\n  return w"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"\\t\", \"\")\n    line_text = line_text.replace(\"\\n\", \"\")\n    line_text = line_text.replace(\"\\r\", \"\")\n    line_text = line_text.replace(\"\\f\", \"\")\n    line_text = line_text.replace(\"\\v\", \"\")\n    line_text = line_text.replace(\"\\b\", \"\")\n    line_text = line_text.replace(\"\\a\", \"\")\n    line_text = line_text.replace(\"\\0\", \"\")\n    line_text = line_text.replace(\"\\x0b\", \"\")\n    line_text = line_text.replace(\"\\x0c\", \"\")\n    line_text = line_text.replace(\"\\x1c\", \"\")\n    line_text = line_text.replace(\"\\x1d\", \"\")\n    line_text = line_text.replace(\"\\x1e\", \"\")\n    line_text = line_text.replace(\"\\x85\", \"\")\n    line_text = line_text.replace(\"\\u2028\", \"\")\n    line_text = line_text.replace(\"\\u2029\", \"\")\n    line_text = line_text.replace(\"\\ufeff\", \"\")\n    line_text = line_text.replace(\"\\ufffe\", \"\")\n    line_text = line_text.replace(\"\\uffff\", \"\")\n    line_text = line_text.replace(\"\\u200b\", \"\")\n    line_text = line_text.replace(\"\\u200c\", \"\")\n    line_text = line_text.replace(\"\\u200d\", \"\")\n    line_text = line_text.replace(\"\\u200e\", \"\")\n    line_text = line_text.replace(\"\\u200f\", \"\")\n    line_text = line_text.replace(\"\\u202a\", \"\")\n    line_text = line_text.replace(\"\\u202b\", \"\")\n    line_text = line_text.replace(\"\\u202c\", \"\")\n    line_text = line_text.replace(\"\\u202d\", \"\")\n    line_text = line_text.replace(\"\\u202e\", \"\")\n    line_text = line_text.replace(\"\\u2060\", \"\")"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check if the number of zeros is valid\n    if zeros < 0 or zeros > n:\n        raise ValueError(\"The number of zeros must be between 0 and n.\")\n\n    # Generate the weights\n    weights = np.random.rand(n - zeros)\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    # Normalize the weights\n    weights /= weights.sum()\n\n    return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, **module_dict)"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = instance['bbox']\n\n    # Get the mode of the bounding box\n    bbox_mode = instance['bbox_mode']\n\n    # Compute the center of the bounding box\n    center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n\n    # Compute the top-left corner of the crop\n    top_left = (center[0] - crop_size[0] / 2, center[1] - crop_size[1] / 2)\n\n    # Adjust the top-left corner if it is outside the image boundaries\n    top_left = (max(top_left[0], 0), max(top_left[1], 0))\n\n    # Compute the bottom-right corner of the crop\n    bottom_right = (top_left[0] + crop_size[0], top_left[1] + crop_size[1])\n\n    # Adjust the bottom-right corner if it is outside the image boundaries\n    bottom_right = (min(bottom_right[0], image_size[0]), min(bottom_right[1], image_size[1]))\n\n    # Compute the dimensions of the crop\n    crop_size = (bottom_right[0] - top_left[0], bottom_right[1] - top_left[1])\n\n    # Create a CropTransform object with the specified parameters\n    transform = T.CropTransform(top_left, crop_size)\n\n    return transform\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm along the last axis\n  sqnorm = jnp.sum(x * x, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value during the forward pass\n  sqnorm = jnp.maximum(sqnorm, jnp.finfo(x.dtype).eps)\n\n  # Clamp the squared norm to a minimum value during the backward pass\n  sqnorm = jnp.maximum(sqnorm, grad_eps)\n\n  # Normalize the array along the last axis\n  return x / jnp.sqrt(sqnorm)"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        if response.startswith(\"Use Agent[\"):\n            parts = response[len(\"Use Agent[\"):].split(\"]\")\n            if len(parts) > 0:\n                agent_name = parts[0]\n                if len(parts) > 1:\n                    input_text = parts[1]\n                    return agent_name, input_text\n        return None, None"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    obj_ids = np.arange(len(annos))\n    cat_ids = [ann['category_id'] for ann in annos]\n    areas = [ann['area'] for ann in annos]\n    iscrowd = [ann['iscrowd'] for ann in annos]\n    gt_boxes = [BoxMode.convert(ann['bbox'], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS) for ann in annos]\n    gt_classes = [ann['category_id'] for ann in annos]\n    gt_masks = [ann['segmentation'] for ann in annos]\n    gt_keypoints = [ann['keypoints'] for ann in annos]\n    gt_boxes = torch.tensor(gt_boxes, dtype=torch.float32)\n    gt_classes = torch.tensor(gt_classes, dtype=torch.int64)\n    gt_masks = SegmentationMask(gt_masks, image_size, mask_format=mask_format)\n    gt_keypoints = Keypoint(gt_keypoints, image_size)\n    return Instances(image_size, obj_ids=obj_ids, gt_boxes=gt_boxes, gt_classes=gt_classes, gt_masks=gt_masks, gt_keypoints=gt_keypoints, iscrowd=iscrowd, areas=areas)"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if isinstance(data_home, Path):\n        data_home = str(data_home)\n\n    if data_home is None:\n        data_home = os.environ.get('SKFOLIO_DATA', os.path.join('~', 'skfolio_data'))\n\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n\n    return data_home\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if not isinstance(cov, np.ndarray):\n        raise TypeError(\"cov must be a numpy array\")\n    if not cov.ndim == 2:\n        raise ValueError(\"cov must be a 2D array\")\n\n    corr = np.zeros_like(cov)\n    std = np.zeros(cov.shape[0])\n\n    for i in range(cov.shape[0]):\n        for j in range(cov.shape[1]):\n            corr[i, j] = cov[i, j] / np.sqrt(cov[i, i] * cov[j, j])\n        std[i] = np.sqrt(cov[i, i])\n\n    return corr, std\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def freeze_submodule(submodule):\n        \"\"\"\n        This function temporarily sets the \"training\" attribute of a given submodule to a constant value, allowing for optimization by meta-compilation. It uses a context manager to ensure that these changes are revert back to their original state after the context manager exits.\n        Input-Output Arguments\n        :param submodule: The submodule whose \"training\" attribute is to be temporarily annotated as a constant. It is used to modify its class definition.\n        :return: No return values. This function operates by side effects, modifying the class definition of the submodule within the context.\n        \"\"\"\n        class FreezeTrainingMode(submodule.__class__):\n            def __init__(self, *args, **kwargs):\n                super(FreezeTrainingMode, self).__init__(*args, **kwargs)\n                self.training = False\n        return FreezeTrainingMode\n\n    with torch.no_grad():\n        for name, module in model.named_modules():\n            new_class = freeze_submodule(module)\n            model.add_module(name, new_class(*list(module.parameters())))\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validator(cls, field1, field2):\n        if field1.shape != field2.shape:\n            raise ValueError(f\"Shapes of {field1} and {field2} do not match.\")\n\n    return validator"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            metrics = [{\"name\": m} for m in metrics]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], dict):\n            metrics = [m for m in metrics if \"name\" in m]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], dict):\n            metrics = [m for m in metrics if \"name\" in m]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], dict):\n            metrics = [m for m in metrics if \"name\" in m]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], dict):\n            metrics = [m for m in metrics if \"name\" in m]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], dict):\n            metrics = [m for m in metrics if \"name\" in m]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], dict):\n            metrics = [m for m in metrics if \"name\" in m]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], dict):\n            metrics = [m for m in metrics if \"name\" in m]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], dict):\n            metrics = [m for m in metrics if \"name\" in m]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], dict):\n            metrics = [m for m in metrics if \"name\" in m]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], dict):\n            metrics = [m for m in metrics if \"name\" in m]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], dict):\n            metrics = [m for m in metrics if \"name\" in m]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], dict):\n            metrics = [m for m in metrics if \"name\" in m]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], dict):\n            metrics = [m for m in metrics if \"name\" in m]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    :param t: Tensor. Represents the metric distances to be mapped.\n    :return: Tensor. The normalized distances in the range [0, 1].\n    \"\"\"\n    t = torch.clamp(t, t_near, t_far)\n    s = fn(t)\n    s = torch.clamp(s, 0, 1)\n    return s\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    :param s: Tensor. Represents the normalized distances to be mapped.\n    :return: Tensor. The metric distances.\n    \"\"\"\n    s = torch.clamp(s, 0, 1)\n    t = fn_inv(s)\n    t = torch.clamp(t, t_near, t_far)\n    return t\n\n  return t_to_s, s_to_t\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) != len(w):\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  # Calculate the integral of the data points using the trapezoid rule\n  integral = 0.0\n  for i in range(1, len(t)):\n    integral += (t[i] - t[i - 1]) * (w[i] + w[i - 1]) / 2.0\n\n  return integral\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Calculate the weighted sum of scores for each ID\n    scores_dict = {}\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            scores_dict[ids[i][j]] = scores[i][j] * weights[i]\n\n    # Normalize the scores\n    scores_sum = sum(scores_dict.values())\n    scores_dict = {k: v / scores_sum for k, v in scores_dict.items()}\n\n    # Sort the IDs by their weighted sum in descending order\n    sorted_ids = sorted(scores_dict.items(), key=lambda x: x[1], reverse=True)\n\n    # Return the top K IDs and their corresponding scores based on the weighted sum\n    return [x[0] for x in sorted_ids[:top_k]], [x[1] for x in sorted_ids[:top_k]]\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Get the dimension of the means\n  dim = mean.shape[-1]\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Calculate the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[:len(i)//2], i[len(i)//2:]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if not (x.shape[0] == x.shape[1]):\n        raise ValueError(\"The matrix is not square.\")\n"}
{"namespace": "coord.pos_enc", "completion": "  if append_identity:\n    x = np.concatenate([x, np.zeros_like(x)], -1)\n\n  for i in range(min_deg, max_deg + 1):\n    for func in [np.sin, np.cos]:\n      x = np.concatenate([x, func(1 / 10000 ** (i / max_deg) * x)], -1)\n\n  return x"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validator(cls, values):\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"The lengths of the fields {field1} and {field2} are not equal.\")\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"The shapes of the arrays in the fields {field1} and {field2} are not equal.\")\n        return values\n\n    return validator"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(camera)"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINT:\n            shader = self.shader_point\n        else:\n            shader = self.shader\n\n        shader.bind()\n        self.upload_gl_uniforms(shader, camera)\n        self.vao.bind()\n\n        if self.render_type == RenderType.POINT:\n            glDrawArrays(GL_POINTS, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.LINE:\n            if self.faces is None:\n                glDrawArrays(GL_LINES, 0, self.vertices.shape[0])\n            else:\n                glDrawElements(GL_LINES, self.faces.shape[0] * 2, GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.TRIANGLE:\n            if self.faces is None:\n                glDrawArrays(GL_TRIANGLES, 0, self.vertices.shape[0])\n            else:\n                glDrawElements(GL_TRIANGLES, self.faces.shape[0] * 3, GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.QUAD:\n            if self.faces is None:\n                glDrawArrays(GL_QUADS, 0, self.vertices.shape[0])\n            else:\n                glDrawElements(GL_QUADS, self.faces.shape[0] * 4, GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            if self.faces is None:\n                glDrawArrays(GL_TRIANGLE_STRIP, 0, self.vertices.shape[0])\n            else:\n                glDrawElements(GL_TRIANGLE_STRIP, self.faces.shape[0] * 3, GL_UNSIGNED_INT, None)\n\n        self.vao.unbind()\n        shader.unbind()"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure that all inputs are batched\n    assert R.ndim == 3 and tvec.ndim == 3 and camera_matrix.ndim == 3 and image_size.ndim == 2\n    # Validate the shapes and values of the inputs\n    assert R.shape[0] == tvec.shape[0] == camera_matrix.shape[0] == image_size.shape[0]\n    assert R.shape[1] == 3 and R.shape[2] == 3\n    assert tvec.shape[1] == 3\n    assert camera_matrix.shape[1] == 3 and camera_matrix.shape[2] == 3\n    assert image_size.shape[1] == 2\n    assert torch.all(R.abs() <= 1)\n    assert torch.all(tvec.abs() <= 1)\n    assert torch.all(camera_matrix.abs() <= 1)\n    assert torch.all(image_size.abs() <= 1)\n\n    # Calculate the camera position and rotation\n    camera_position = -torch.bmm(R, tvec)\n    camera_rotation = rotation_matrix_to_euler(R)\n\n    # Calculate the intrinsic parameters\n    focal_length = camera_matrix[:, 0, 0] / image_size[:, 0]\n    focal_length = focal_length / (2 * torch.tan(torch.atan(znear / focal_length)))\n    principal_point_offset = (\n        camera_matrix[:, 0, 2] / image_size[:, 0] - 0.5\n    ) * image_size[:, 0]\n    sensor_width = camera_matrix[:, 0, 0] / focal_length\n\n    # Return the camera parameters\n    return torch.stack(\n        [\n            camera_position[:, 0],\n            camera_position[:, 1],\n            camera_position[:, 2],\n            camera_rotation[:, 0],\n            camera_rotation[:, 1],\n            camera_rotation[:, 2],\n            focal_length,\n            principal_point_offset,\n            sensor_width,\n        ],\n        dim=1,\n    )\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        glViewport(x, y, w or self.W, h or self.H)\n        glScissor(x, y, w or self.W, h or self.H)\n\n        self.quad_program.use()\n        self.tex.bind()\n\n        glEnableVertexAttribArray(0)\n        glEnableVertexAttribArray(1)\n\n        glBindBuffer(GL_ARRAY_BUFFER, self.vbo)\n        glVertexAttribPointer(0, 2, GL_FLOAT, GL_FALSE, 4 * 4, None)\n        glVertexAttribPointer(1, 2, GL_FLOAT, GL_FALSE, 4 * 4, ctypes.c_void_p(2 * 4))\n\n        glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, self.ibo)\n        glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_INT, None)\n\n        glDisableVertexAttribArray(0)\n        glDisableVertexAttribArray(1)\n\n        glBindBuffer(GL_ARRAY_BUFFER, 0)\n        glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, 0)\n\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract camera parameters from batch input\n    R = batch.R\n    T = batch.T\n    K = batch.K\n    H = batch.H\n    W = batch.W\n\n    # Adjust rotation matrix to match PyTorch3D's requirements\n    R = R.transpose(1, 2)\n\n    # Adjust translation vector to match PyTorch3D's requirements\n    T = T.transpose(0, 1)\n\n    # Compute camera intrinsic matrix for normalized device coordinates (NDC)\n    K = K.clone()\n    K[0, 0] = K[0, 0] * 2 / W\n    K[1, 1] = K[1, 1] * 2 / H\n    K[0, 2] = K[0, 2] * 2 / W - 1\n    K[1, 2] = K[1, 2] * 2 / H - 1\n\n    # Compute camera center in the camera's coordinate system\n    C = -torch.inverse(R) @ T\n\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n        glBlitFramebuffer(0, 0, self.W, self.H, x, y, x + w, y + h, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the cumulative sums for the source times (t1) and the corresponding values (y1).\n    cumsum_t1 = torch.cumsum(t1)\n    cumsum_y1 = torch.cumsum(y1)\n\n    # Compute the cumulative sums for the target time (t0) and the corresponding values (y1).\n    cumsum_t0 = torch.cumsum(t0)\n    cumsum_y0 = torch.cumsum(y0)\n\n    # Compute the inner measure based on the cumulative sums for the source times (t1) and the corresponding values (y1).\n    inner = cumsum_y1[t1 <= t0] - cumsum_y0[t0 <= t1]\n\n    # Compute the outer measure based on the cumulative sums for the target time (t0) and the corresponding values (y0).\n    outer = cumsum_y0[t0 <= t1] - cumsum_y1[t1 <= t0]\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_env_upper = torch.nn.functional.interpolate(w_env, t.shape[-1], mode='linear', align_corners=False)\n\n    # calculate the loss based on the difference between target weights and the upper envelope\n    loss = torch.nn.functional.relu(w - w_env_upper)\n\n    # scale the loss by a half-quadratic loss function\n    loss = loss * torch.sqrt(torch.nn.functional.relu(t - t_env) + eps)\n\n    # return the calculated loss\n    return loss\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # compute the inter-interval loss\n    inter_interval_loss = torch.sum(w * torch.abs(t[:, 1:] - t[:, :-1]))\n\n    # compute the intra-interval loss\n    intra_interval_loss = torch.sum(w * torch.abs(t[:, 1:] - 2 * t[:, :-1] + t[:, :-2]))\n\n    # combine the inter-interval and intra-interval losses\n    total_distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return total_distortion_loss\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the weights sum to 1\n    if torch.allclose(torch.sum(w, dim=0), torch.ones_like(torch.sum(w, dim=0))):\n        # If the weights sum to 1, continue with the computation\n        pass\n    else:\n        # If the weights do not sum to 1, raise an error\n        raise ValueError(\"The weights must sum to 1.\")\n\n    # Compute the cumulative sum of the weights\n    cum_sum = torch.cumsum(w, dim=0)\n\n    # Compute the weighted percentiles\n    weighted_percentiles = torch.zeros_like(t)\n    for i, p in enumerate(ps):\n        # Find the index of the first value in 'cum_sum' that is greater than or equal to 'p'\n        idx = torch.argmax(cum_sum >= p)\n        # Compute the weighted percentile for the current percentile value\n        weighted_percentiles[i] = t[idx]\n\n    return weighted_percentiles"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Check if the input tensors are valid\n    if not isinstance(t, torch.Tensor):\n        raise ValueError('Input tensor t must be a torch.Tensor.')\n    if not isinstance(w, torch.Tensor):\n        raise ValueError('Input tensor w must be a torch.Tensor.')\n    if not isinstance(num_samples, int):\n        raise ValueError('Input num_samples must be an integer.')\n    if not isinstance(perturb, bool):\n        raise ValueError('Input perturb must be a boolean.')\n    if not isinstance(single_jitter, bool):\n        raise ValueError('Input single_jitter must be a boolean.')\n\n    # Check if the input tensors have the correct shape\n    if t.ndim != 1:\n        raise ValueError('Input tensor t must be a 1-dimensional tensor.')\n    if w.ndim != 1:\n        raise ValueError('Input tensor w must be a 1-dimensional tensor.')\n    if t.shape != w.shape:\n        raise ValueError('Input tensors t and w must have the same shape.')\n\n    # Check if the input tensors are sorted\n    if not torch.all(torch.diff(t) >= 0):\n        raise ValueError('Input tensor t must be sorted in ascending order.')\n\n    # Check if the input tensors are non-negative\n    if torch.any(t < 0) or torch.any(w < 0):\n        raise ValueError('Input tensors t and w must be non-negative.')\n\n    # Check if the input tensors are valid probability distributions\n    if not torch.allclose(torch.sum(w), 1):\n        raise ValueError('Input tensor w must be a valid probability distribution.')\n\n    # Check if the input tensors are valid probability distributions\n    if not torch.allclose(torch.sum(w), 1):\n        raise ValueError('Input tensor w must be a valid probability distribution.')\n\n    # Check if the input tensors are valid probability distributions\n    if not torch.allclose(torch.sum(w), 1):\n        raise ValueError('Input tensor w must be a valid probability distribution.')\n\n    # Check if the input tensors are valid probability distributions\n    if not torch.allclose(torch.sum(w), 1):\n        raise ValueError('Input tensor w must be a valid probability distribution.')\n\n    # Check if the input tensors are valid probability distributions\n    if not torch.allclose(torch.sum(w), 1):\n        raise ValueError('Input"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Perform dilation (via max-pooling) on the time steps.\n    t_dilated = torch.max_pool1d(t.unsqueeze(0), kernel_size=w.size()[0], stride=dilation)\n    t_dilated = t_dilated.squeeze(0)\n\n    # Clip the dilated time steps within the specified domain.\n    t_dilated = torch.clamp(t_dilated, min=domain[0], max=domain[1])\n\n    # Adjust the weights accordingly to match the dilated time steps.\n    w_dilated = w.repeat(dilation)\n\n    return t_dilated, w_dilated\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Find the indices of the query times that are less than or equal to the step change times\n    idx = tf.searchsorted(t, tq, side='right')\n\n    # Find the indices of the query times that are greater than the last step change time\n    idx_out = tf.where(tq > t[-1])\n\n    # Find the indices of the query times that are equal to a step change time\n    idx_eq = tf.where(tq == t)\n\n    # Find the indices of the query times that are between two step change times\n    idx_between = tf.where((tq > t[0]) & (tq < t[-1]))\n\n    # Find the indices of the query times that are less than the first step change time\n    idx_before = tf.where(tq < t[0])\n\n    # Find the indices of the query times that are greater than or equal to the last step change time\n    idx_after = tf.where(tq >= t[-1])\n\n    # Find the indices of the query times that are between two step change times\n    idx_between = tf.where((tq > t[0]) & (tq < t[-1]))\n\n    # Find the indices of the query times that are less than the first step change time\n    idx_before = tf.where(tq < t[0])\n\n    # Find the indices of the query times that are greater than or equal to the last step change time\n    idx_after = tf.where(tq >= t[-1])\n\n    # Find the indices of the query times that are between two step change times\n    idx_between = tf.where((tq > t[0]) & (tq < t[-1]))\n\n    # Find the indices of the query times that are less than the first step change time\n    idx_before = tf.where(tq < t[0])\n\n    # Find the indices of the query times that are greater than or equal to the last step change time\n    idx_after = tf.where(tq >= t[-1])\n\n    # Find the indices of the query times that are between two step change times\n    idx_between = tf.where((tq > t[0]) & (tq < t[-1]))\n\n    # Find the indices of the query times that are less than the first step change time\n    idx_before = tf.where(tq < t[0])\n\n    # Find the"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # get the number of intervals\n    n_intervals = w.shape[-1] - 1\n\n    # get the interval widths\n    interval_widths = t[..., 1:] - t[..., :-1]\n\n    # get the interval midpoints\n    interval_midpoints = (t[..., 1:] + t[..., :-1]) / 2\n\n    # get the interval midpoints relative to the training fraction\n    interval_midpoints_rel = interval_midpoints / train_frac\n\n    # get the interval midpoints relative to the training fraction\n    interval_midpoints_rel = interval_midpoints / train_frac\n\n    # get the interval midpoints relative to the training fraction\n    interval_midpoints_rel = interval_midpoints / train_frac\n\n    # get the interval midpoints relative to the training fraction\n    interval_midpoints_rel = interval_midpoints / train_frac\n\n    # get the interval midpoints relative to the training fraction\n    interval_midpoints_rel = interval_midpoints / train_frac\n\n    # get the interval midpoints relative to the training fraction\n    interval_midpoints_rel = interval_midpoints / train_frac\n\n    # get the interval midpoints relative to the training fraction\n    interval_midpoints_rel = interval_midpoints / train_frac\n\n    # get the interval midpoints relative to the training fraction\n    interval_midpoints_rel = interval_midpoints / train_frac\n\n    # get the interval midpoints relative to the training fraction\n    interval_midpoints_rel = interval_midpoints / train_frac\n\n    # get the interval midpoints relative to the training fraction\n    interval_midpoints_rel = interval_midpoints / train_frac\n\n    # get the interval midpoints relative to the training fraction\n    interval_midpoints_rel = interval_midpoints / train_frac\n\n    # get the interval midpoints relative to the training fraction\n    interval_midpoints_rel = interval_midpoints / train_frac\n\n    # get the interval midpoints relative to the training fraction\n    interval_midpoints_rel = interval_midpoints / train_frac\n\n    # get the interval midpoints relative to the training fraction\n    interval_midpoints_rel = interval_midpoints / train_frac\n\n    # get the interval midpoints relative to the training fraction\n    interval_midpoints_rel = interval_midpoints / train_frac\n\n    # get the interval midpoints relative to"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, (list, tuple)):\n        return [to_cuda(x, device, ignore_list) for x in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # compute faces normals w.r.t the vertices (consider"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, tuple):\n        return tuple(add_batch(x) for x in batch)\n    elif isinstance(batch, list):\n        return [add_batch(x) for x in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, 0)\n    else:\n        raise ValueError(f\"Unsupported data type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.meta = dotdict()\n        batch.meta.meta = dotdict()\n        batch.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta.meta.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta.meta.meta.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta = dotdict()\n        batch.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta.meta = dotdict()"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            state = agent.get_state()\n            state_dict = state.to_dict()\n            self.save(state_dict)"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        closest_agent = None\n        max_similarity = -math.inf\n        for agent in self.agents:\n            similarity = cosine_similarity(purpose_embedding, agent.purpose_embedding)\n            if similarity > max_similarity:\n                closest_agent = agent\n                max_similarity = similarity\n        return closest_agent, max_similarity"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"What's your name? \",\n            name=\"Prime\",\n            weight=100,\n            flags=[FLAG_PRIME, FLAG_UNSPECIFIED],\n        )\n        self.agents.append(prime_agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent = None\n        if self.db.exists(purpose):\n            agent = self.db.load(purpose)\n            agent = self.deserializer.deserialize(agent, agent_lifecycle, openai_wrapper)\n        return agent\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for purpose in PURPOSES:\n            agents_from_db = self.get_agents_from_db(purpose)\n            for agent in agents_from_db:\n                agent_from_db = self.get_agent_from_db(agent.id)\n                agent_from_db.load(agent_lifecycle, openai_wrapper)\n                agents.append(agent_from_db)\n        return agents"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.persistence.save(agent)\n        except Exception as e:\n            self.logger.error(f\"Error saving agent {agent.id}\")\n            raise e"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        agents = self.agents\n        agents = [agent for agent in agents if agent.is_alive()]\n        self.agents = agents\n        return agents"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"\"\"\n            Your role is to generate a prompt for the Language Learning Model (LLM) based on a specified goal and sample input.\n            Goal: {goal}\n            Sample Input: {sample_input}\n            Your prompt:\n            \"\"\"\n            response = self.openai_wrapper.get_chat_completion(prompt)\n            return response.content\n        except Exception as e:\n            self.logger.error(f\"Error generating prompt: {e}\")\n            return \"\""}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        conn = sqlite3.connect(self.db_file)\n        c = conn.cursor()\n        c.execute(\"\"\"INSERT OR REPLACE INTO agents VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n        c.execute(\"SELECT * FROM agents WHERE purpose=?\", (purpose,))\n        row = c.fetchone()\n        conn.close()\n        if row is None:\n            return None\n        return json.loads(row[1])\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        purposes = []\n        for purpose in self.db.execute(\"SELECT DISTINCT PURPOSE FROM AGENTS\"):\n            purposes.append(purpose[0])\n        return purposes"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n        c.execute(\"SELECT result FROM cache WHERE arg_hash = ?\", (arg_hash,))\n        result = c.fetchone()\n        conn.close()\n        if result:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        self._conn.execute(\n            \"INSERT INTO cache (key, value) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self._conn.commit()\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    update_global_config(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        sys.stdout = open(os.path.join(args.output_dir, \"output.txt\"), \"w\")\n\n    # Execute the command line process based on the provided arguments\n    if args.command == \"train\":\n        train_model(args)\n    elif args.command == \"test\":\n        test_model(args)\n    elif args.command == \"predict\":\n        predict_model(args)\n    elif args.command == \"evaluate\":\n        evaluate_model(args)\n    elif args.command == \"visualize\":\n        visualize_model(args)\n    elif args.command == \"convert\":\n        convert_model(args)\n    elif args.command == \"version\":\n        print_version()\n    else:\n        raise ValueError(f\"Invalid command: {args.command}\")\n\n    # If quiet mode is enabled, close the output file\n    if quiet_mode:\n        sys.stdout.close()\n\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Check if the context length limit is exceeded\n        if len(kwargs['messages']) > self.configuration.max_context_length:\n            # If a higher-capacity model is available, use it\n            if self.configuration.high_capacity_model:\n                kwargs['model'] = self.configuration.high_capacity_model\n            else:\n                # If no fallback model is available, raise an error\n                raise BadRequestError(\n                    \"Context length limit exceeded. No fallback model available.\")\n\n        # Make the request to the OpenAI API\n        response = self.openai.chat.completions(**kwargs)\n\n        # Return the response\n        return response"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self.client or self.creation_time + self.interval < time.time():\n            self.client = boto3.client(\"s3\")\n            self.creation_time = time.time()\n        return self.client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if num_workers > 0 and not self.rank:\n            raise RuntimeError(\"state_dict cannot be retrieved in a DataLoader worker process\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir\": self.input_dir,\n            \"url\": self.url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n        self._state_dict_loaded = True\n        self._state_dict_loaded_at = time.time()\n        self._state_dict_loaded_at_str = datetime.datetime.fromtimestamp(self._state_dict_loaded_at).strftime(\n            '%Y-%m-%d %H:%M:%S')\n        self._state_dict_loaded_at_str_utc = datetime.datetime.utcfromtimestamp(self._state_dict_loaded_at).strftime(\n            '%Y-%m-%d %H:%M:%S')\n        self._state_dict_loaded_at_str_utc_tz = datetime.datetime.utcfromtimestamp(self._state_dict_loaded_at).strftime(\n            '%Y-%m-%d %H:%M:%S %Z%z')\n        self._state_dict_loaded_at_str_local_tz = datetime.datetime.fromtimestamp(self._state_dict_loaded_at).strftime(\n            '%Y-%m-%d %H:%M:%S %Z%z')\n        self._state_dict_loaded_at_str_utc_tz_aware = datetime.datetime.utcfromtimestamp(self._state_dict_loaded_at).replace(\n            tzinfo=datetime.timezone.utc).strftime('%Y-%m-%d %H:%M:%S %Z%z')\n        self._state_dict_loaded_at_str_local_tz_aware = datetime.datetime.fromtimestamp(self._state_dict_loaded_at).replace(\n            tzinfo=datetime.timezone.utc).strftime('%Y-%m-%d %H:%M:%S %Z%z')\n        self._state_dict_loaded_at_str_utc_tz_aware_iso = datetime.datetime.utcfromtimestamp(self._state_dict_loaded_at).replace(\n            tzinfo=datetime.timezone.utc).isoformat()\n        self._state_dict_loaded_at_str_local_tz_aware_iso = datetime.datetime.fromtimestamp(self._state_dict_loaded_at).replace(\n            tzinfo=datetime.timezone.utc).isoformat()\n        self._state_dict_loaded_at_str_utc_tz_aware_iso_"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n\n        if self._state_dict.get(\"shuffle\", None) != self.shuffle:\n            raise ValueError(\n                \"State dictionary's shuffle doesn't match the current shuffle\"\n            )\n\n        if self._state_dict.get(\"num_workers\", None) != self.num_workers:\n            raise ValueError(\n                \"State dictionary's num_workers doesn't match the current num_workers\"\n            )\n\n        if self._state_dict.get(\"input_path\", None) != self.input_path:\n            raise ValueError(\n                \"State dictionary's input_path doesn't match the current input_path\"\n            )\n\n        if self._state_dict.get(\"url\", None) != self.url:\n            raise ValueError(\n                \"State dictionary's url doesn't match the current url\"\n            )\n\n        if self._state_dict.get(\"seed\", None) != self.seed:\n            raise ValueError(\n                \"State dictionary's seed doesn't match the current seed\"\n            )\n\n        if self._state_dict.get(\"item_loader_state\", None) != self.item_loader.state_dict():\n            raise ValueError(\n                \"State dictionary's item_loader_state doesn't match the current item_loader_state\"\n            )\n\n        if self._state_dict.get(\"drop_last\", None) != self.drop_last:\n            raise ValueError(\n                \"State dictionary's drop_last doesn't match the current drop_last\"\n            )\n\n        if self._state_dict.get(\"cache\", None) != self.cache:\n            raise ValueError(\n                \"State dictionary's cache doesn't match the current cache\"\n            )\n\n        if self._state_dict.get(\"worker_env\", None) != self.worker_env:\n            raise ValueError(\n                \"State dictionary's worker_env doesn't match the current worker_env\"\n            )\n\n        if self._state_dict.get(\"worker_init_fn\", None) != self.worker_init_fn:\n            raise ValueError(\n                \"State dictionary's worker_init_fn doesn't match the current worker_init_fn\"\n            )\n\n        if self._state_dict.get(\"worker_init_args\", None) != self.worker_init_args:\n            raise ValueError(\n                \"State dictionary's worker_init_args doesn"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    cache_dir = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    if \"XDG_CACHE_HOME\" in os.environ:\n        cache_dir = os.path.join(os.environ[\"XDG_CACHE_HOME\"], cache_dir)\n    elif \"HOME\" in os.environ:\n        cache_dir = os.path.join(os.environ[\"HOME\"], \".cache\", cache_dir)\n    else:\n        cache_dir = os.path.join(tempfile.gettempdir(), cache_dir)\n\n    try:\n        os.makedirs(cache_dir)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path uses the \"s3\" scheme\n        if urlparse(remote_filepath).scheme != \"s3\":\n            raise ValueError(f\"The remote file path {remote_filepath} does not use the 's3' scheme.\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            raise ValueError(f\"The local file {local_filepath} already exists.\")\n\n        # Acquire a file lock to prevent multiple processes from downloading the same file simultaneously\n        with FileLock(local_filepath):\n            # Check if the s5cmd command-line tool is available\n            if shutil.which(\"s5cmd\"):\n                # Use the s5cmd command-line tool to download the file\n                subprocess.run([\"s5cmd\", \"cp\", remote_filepath, local_filepath], check=True)\n            else:\n                # Use the boto3 library to download the file\n                s3 = boto3.client(\"s3\")\n                bucket, key = urlparse(remote_filepath).netloc, urlparse(remote_filepath).path.lstrip(\"/\")\n                s3.download_file(bucket, key, local_filepath)"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Calculate the number of chunks per worker\n    num_chunks_per_worker = len(chunks_replica) // num_workers\n\n    # Create a dictionary to map workers to their assigned chunks\n    workers_to_chunks = {}\n\n    # Create a dictionary to map workers to their assigned intervals\n    workers_to_intervals = {}\n\n    # Iterate over the workers\n    for worker_id in range(num_workers):\n\n        # Calculate the start and end indices of the chunks for the current worker\n        start_index = worker_id * num_chunks_per_worker\n        end_index = (worker_id + 1) * num_chunks_per_worker\n\n        # Assign the chunks to the current worker\n        workers_to_chunks[worker_id] = chunks_replica[start_index:end_index]\n\n        # Assign the intervals to the current worker\n        workers_to_intervals[worker_id] = intervals_replica[start_index:end_index]\n\n    # Return the dictionaries\n    return workers_to_chunks, workers_to_intervals\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Get the image's dimensions and mode\n        width, height = item.size\n        mode = item.mode\n\n        # Serialize the image's dimensions and mode\n        width_bytes = width.to_bytes(4, \"big\")\n        height_bytes = height.to_bytes(4, \"big\")\n        mode_bytes = len(mode).to_bytes(4, \"big\")\n        mode_bytes += mode.encode(\"utf-8\")\n\n        # Serialize the image's raw pixel data\n        data_bytes = item.tobytes()\n\n        # Return the serialized data and None\n        return width_bytes + height_bytes + mode_bytes + data_bytes, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if isinstance(item, JPEGImage):\n                if item.filename is not None and os.path.isfile(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    return item.to_bytes(), None\n            elif isinstance(item, PNGImage):\n                return item.to_bytes(), None\n            elif isinstance(item, GIFImage):\n                return item.to_bytes(), None\n            elif isinstance(item, BMPImage):\n                return item.to_bytes(), None\n            else:\n                raise TypeError(f\"Unsupported image type: {type(item)}\")\n        else:\n            raise TypeError(f\"Expected Image, got {type(item)}\")"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width = struct.unpack('I', data[:4])[0]\n        height = struct.unpack('I', data[4:8])[0]\n        mode_size = struct.unpack('I', data[8:12])[0]\n        mode = data[12:12 + mode_size].decode('utf-8')\n        image = Image.frombytes(mode, (width, height), data[12 + mode_size:])\n        return image"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype = torch.dtype(dtype=dtype)\n        shape = tuple(shape)\n        tensor = torch.empty(size=shape, dtype=dtype)\n        tensor.copy_from(src=src)\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        header = struct.pack(\">dH\", item.dtype.itemsize, len(item.shape))\n        header += struct.pack(\">*dH\", *item.shape)\n        data = item.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1).contiguous().view(-1).contiguous().view(-1).contiguous()\n        data = data.contiguous().view(-1)."}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if torchvision is not None:\n            try:\n                img = torchvision.io.read_image(data)\n                return img\n            except RuntimeError:\n                pass\n\n        img = Image.open(io.BytesIO(data))\n        if torchvision is not None:\n            img = torchvision.transforms.ToTensor()(img)\n        return img"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if isinstance(item, torch.Tensor):\n            item = item.numpy()\n        if isinstance(item, np.ndarray):\n            item = item.tobytes()\n        if isinstance(item, bytes):\n            dtype = self.get_dtype(item)\n            return item, dtype\n        raise ValueError(f\"Unsupported type {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        tensor = torch.empty(dtype=_dtype)\n        tensor.deserialize_from_buffer(data)\n        return tensor"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype = np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype=np.dtype(dtype"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return self.serialize_without_header(item)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_index = np.dtype(item.dtype).kind\n        shape = item.shape\n        data = item.tobytes()\n        return (dtype_index, shape, data), None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict() if isinstance(self.dataset, StreamingDataset) else self.dataset,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_loaded\": self.num_samples_loaded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not hasattr(self, 'torchvision') or not hasattr(self, 'av'):\n            raise Exception('Required libraries not installed')\n\n        with open('temp.mp4', 'wb') as f:\n            f.write(data)\n\n        return self.torchvision.read_video('temp.mp4')[0]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        self.write_chunks_index()\n        self.write_chunks()\n        self._is_done = True\n        return self.files\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj)\n        else:\n            raise RuntimeError(\n                \"Dataset type not supported. Only StreamingDataset and CombinedStreamingDataset are supported.\"\n            )\n\n        self.current_epoch = obj[\"epoch\"]\n        self.num_yielded = obj[\"num_yielded\"]\n        self.worker_index = obj[\"worker_index\"]\n\n        self._prepare_data_loader_for_resume()\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\"num_samples_yielded\": num_samples_yielded}\n        else:\n            return self.iterator.state_dict(num_workers, batch_size, num_samples_yielded)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset in self.datasets:\n            dataset.load_state_dict(state_dict[dataset.name])\n\n        if self.streaming_dataloader is not None:\n            self.streaming_dataloader.num_samples = state_dict[\"num_samples\"]"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"gs://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"az://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"file://\"):\n            return Dir(path=dir_path[7:])\n        elif dir_path.startswith(\"~\"):\n            return Dir(path=os.path.expanduser(dir_path))\n        elif dir_path.startswith(\"/\"):\n            return Dir(path=dir_path)\n        elif dir_path.startswith(\"..\"):\n            return Dir(path=os.path.abspath(dir_path))\n        elif dir_path.startswith(\".\"):\n            return Dir(path=os.path.abspath(dir_path))\n        else:\n            return Dir(path=os.path.abspath(dir_path))\n\n    raise ValueError(f\"Invalid directory path: {dir_path}\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"The output directory must be a Dir object, not {type(output_dir)}\")\n\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(f\"The output directory must be an S3 bucket, not {output_dir}\")\n\n    if not output_dir.is_empty():\n        raise ValueError(f\"The output directory must be empty, not {output_dir}\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.is_dir():\n        if output_dir.contains(\"index.json\"):\n            raise ValueError(\n                \"The directory already contains an index file. Please remove it before running the script.\"\n            )\n        else:\n            output_dir.delete_all_objects()\n    else:\n        raise ValueError(\n            \"The directory is not an S3 bucket. Please provide a valid S3 bucket directory.\"\n        )\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n\n        if node_rank == 0:\n            # Wait for all index parts to be available\n            while True:\n                if len(os.listdir(self.cache_dir)) == num_workers:\n                    break\n                time.sleep(1)\n\n            # Merge all index parts\n            for i in range(num_workers):\n                with open(os.path.join(self.cache_dir, f\"index_{i}.bin\"), \"rb\") as f:\n                    self.index.extend(pickle.load(f))\n\n            # Sort the index\n            self.index.sort(key=lambda x: x[0])\n\n            # Write the merged index to the cache directory\n            with open(os.path.join(self.cache_dir, \"index.bin\"), \"wb\") as f:\n                pickle.dump(self.index, f)\n\n            # Wait for all nodes to finish merging\n            while True:\n                if len(os.listdir(self.cache_dir)) == num_workers + 1:\n                    break\n                time.sleep(1)\n\n            # Move the merged index to the final destination\n            shutil.move(os.path.join(self.cache_dir, \"index.bin\"), self.index_path)\n        else:\n            # Wait for the merged index to be available\n            while True:\n                if os.path.exists(self.index_path):\n                    break\n                time.sleep(1)"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available():\n        raise ImportError(\n            \"The package 'requests' is required for this function to work.\"\n        )\n\n    if not _is_available"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        self.deletion_queue.extend(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._cache_dir is None:\n            return None\n\n        if self._remote_input_dir is None:\n            return None\n\n        if self._item_loader is None:\n            return None\n\n        if self._serializers is None:\n            return None\n\n        if self._cache_dir is None:\n            return None\n\n        if self._remote_input_dir is None:\n            return None\n\n        if self._item_loader is None:\n            return None\n\n        if self._serializers is None:\n            return None\n\n        if self._cache_dir is None:\n            return None\n\n        if self._remote_input_dir is None:\n            return None\n\n        if self._item_loader is None:\n            return None\n\n        if self._serializers is None:\n            return None\n\n        if self._cache_dir is None:\n            return None\n\n        if self._remote_input_dir is None:\n            return None\n\n        if self._item_loader is None:\n            return None\n\n        if self._serializers is None:\n            return None\n\n        if self._cache_dir is None:\n            return None\n\n        if self._remote_input_dir is None:\n            return None\n\n        if self._item_loader is None:\n            return None\n\n        if self._serializers is None:\n            return None\n\n        if self._cache_dir is None:\n            return None\n\n        if self._remote_input_dir is None:\n            return None\n\n        if self._item_loader is None:\n            return None\n\n        if self._serializers is None:\n            return None\n\n        if self._cache_dir is None:\n            return None\n\n        if self._remote_input_dir is None:\n            return None\n\n        if self._item_loader is None:\n            return None\n\n        if self._serializers is None:\n            return None\n\n        if self._cache_dir is None:\n            return None\n\n        if self._remote_input_dir is None:\n            return None\n\n        if self._item_loader is None:\n            return None\n\n        if self._serializers is None:\n            return None\n\n        if self._cache_dir is None:\n            return None\n\n        if self._remote_input_dir is None:\n            return None\n\n        if self._item_loader is None:\n            return None\n\n        if self._serializers is None:\n            return None\n\n        if self._cache_dir is None:\n            return None\n\n        if self._remote_input_dir is None:\n            return None\n\n        if self._"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"Expected ChunkedIndex, got {type(index)}\")\n\n        if self.index_config is None:\n            raise Exception(\"No index config\")\n\n        if self.prepare_thread is None:\n            assert self.prepare_thread is not None\n\n        if index.chunk_id not in self.chunks:\n            self.chunks[index.chunk_id] = self.prepare_thread.get_chunk(index.chunk_id)\n\n        chunk = self.chunks[index.chunk_id]\n        return chunk.read(index.item_id)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if is_running_in_cluster():\n        return get_or_create(key, obj)\n    return obj"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the number of nodes in the distributed environment\n    num_nodes = distributed_env.num_nodes\n\n    # Get the number of chunks per node\n    num_chunks_per_node = len(chunks_per_ranks[0])\n\n    # Get the number of chunks per rank\n    num_chunks_per_rank = len(chunks_per_ranks[0][0])\n\n    # Get the number of chunks per node per rank\n    num_chunks_per_node_per_rank = num_chunks_per_node * num_chunks_per_rank\n\n    # Get the number of chunks per rank per node\n    num_chunks_per_rank_per_node = num_chunks_per_rank * num_nodes\n\n    # Get the number of chunks per node per rank per node\n    num_chunks_per_node_per_rank_per_node = num_chunks_per_node_per_rank * num_nodes\n\n    # Get the number of chunks per rank per node per node\n    num_chunks_per_rank_per_node_per_node = num_chunks_per_rank_per_node * num_nodes\n\n    # Get the number of chunks per node per rank per node per rank\n    num_chunks_per_node_per_rank_per_node_per_rank = num_chunks_per_node_per_rank_per_node * num_chunks_per_rank\n\n    # Get the number of chunks per rank per node per node per rank\n    num_chunks_per_rank_per_node_per_node_per_rank = num_chunks_per_rank_per_node_per_node * num_chunks_per_rank\n\n    # Get the number of chunks per node per rank per node per rank per node\n    num_chunks_per_node_per_rank_per_node_per_rank_per_node = num_chunks_per_node_per_rank_per_node_per_rank * num_nodes\n\n    # Get the number of chunks per rank per node per node per rank per node\n    num_chunks_per_rank_per_node_per_node_per_rank_per_node = num_chunks_per_rank_per_node_per_node_per_rank * num_nodes\n\n    # Get the number of chunks"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) < 2:\n        return None\n\n    input_dir = None\n    for input_ in inputs:\n        if isinstance(input_, str):\n            input_dir = input_\n            break\n\n    if input_dir is None:\n        return None\n\n    input_dir = os.path.abspath(input_dir)\n    if not os.path.isdir(input_dir):\n        return None\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if enable:\n        dns_optimize_enable()\n    else:\n        dns_optimize_disable()\n\n    try:\n        yield\n    finally:\n        dns_optimize_disable()"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not.\n    num_items = len(indexes)\n    num_items_per_rank = int(num_items / distributed_env.world_size)\n    if drop_last:\n        num_items_per_rank += int(num_items % distributed_env.world_size)\n\n    # Assign chunks and their intervals to each rank accordingly.\n    chunks_and_intervals = []\n    for rank in range(distributed_env.world_size):\n        start = rank * num_items_per_rank\n        end = (rank + 1) * num_items_per_rank\n        if rank == distributed_env.world_size - 1 and drop_last:\n            end = num_items\n        chunks_and_intervals.append((indexes[start:end], chunk_intervals[start:end]))\n\n    return chunks_and_intervals\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            kwargs = {\"is_last\": is_last}\n        else:\n            kwargs = {}\n\n        self._fn(item_metadata, output_dir, **kwargs)"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.bucket, Key=obj.key)\n        except ClientError as e:\n            if e.response['code'] == 'NotFound':\n                time.sleep(sleep_time)\n            else:\n                raise\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the inputs are a list of dictionaries\n    if isinstance(inputs, list) and all(isinstance(item, dict) for item in inputs):\n        # Check if the inputs have a 'filepath' key\n        if all(\"filepath\" in item for item in inputs):\n            # Check if the inputs have a 'weight' key\n            if all(\"weight\" in item for item in inputs):\n                # Extract the filepaths and weights from the inputs\n                filepaths = [item[\"filepath\"] for item in inputs]\n                weights = [item[\"weight\"] for item in inputs]\n            else:\n                # Extract the filepaths from the inputs\n                filepaths = [item[\"filepath\"] for item in inputs]\n                weights = None\n        else:\n            raise ValueError(\"Inputs must be a list of dictionaries with a 'filepath' key.\")\n    else:\n        raise ValueError(\"Inputs must be a list of dictionaries.\")\n\n    # Check if the output_dir is a valid directory\n    if not os.path.isdir(output_dir):\n        raise ValueError(\"Output directory does not exist.\")\n\n    # Check if the weights are valid\n    if weights is not None and len(weights) != len(filepaths):\n        raise ValueError(\"Weights must be the same length as the filepaths.\")\n\n    # Check if the chunk_size is valid\n    if chunk_size is not None and chunk_size <= 0:\n        raise ValueError(\"Chunk size must be greater than 0.\")\n\n    # Check if the chunk_bytes is valid\n    if chunk_bytes is not None and chunk_bytes <= 0:\n        raise ValueError(\"Chunk bytes must be greater than 0.\")\n\n    # Check if the compression is valid\n    if compression is not None and compression not in [\"gzip\", \"zstd\"]:\n        raise ValueError(\"Compression must be either 'gzip' or 'zstd'.\")\n\n    # Check if the num_workers is valid\n    if num_workers is not None and num_workers <= 0:\n        raise ValueError(\"Number of workers must be greater than 0.\")\n\n    # Check if the num_nodes is valid\n    if num_nodes is not None and num_nodes <= 0:\n        raise ValueError(\"Number of nodes must be greater than 0.\")\n\n    # Check if the num_downloaders is valid\n    if num_downloaders is not None and num_downloaders <= 0:\n        raise ValueError(\"Number of downloaders must"}
{"namespace": "litdata.processing.functions.map", "completion": "    pass\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Create a directory object for the cache directory\n    cache_dir = Dir(cache_dir)\n\n    # Create a directory object for the input directory\n    input_dir = Dir(input_dir)\n\n    # Create a directory object for the cache directory\n    cache_dir = Dir(cache_dir)\n\n    # Create a directory object for the input directory\n    input_dir = Dir(input_dir)\n\n    # Create a directory object for the cache directory\n    cache_dir = Dir(cache_dir)\n\n    # Create a directory object for the input directory\n    input_dir = Dir(input_dir)\n\n    # Create a directory object for the cache directory\n    cache_dir = Dir(cache_dir)\n\n    # Create a directory object for the input directory\n    input_dir = Dir(input_dir)\n\n    # Create a directory object for the cache directory\n    cache_dir = Dir(cache_dir)\n\n    # Create a directory object for the input directory\n    input_dir = Dir(input_dir)\n\n    # Create a directory object for the cache directory\n    cache_dir = Dir(cache_dir)\n\n    # Create a directory object for the input directory\n    input_dir = Dir(input_dir)\n\n    # Create a directory object for the cache directory\n    cache_dir = Dir(cache_dir)\n\n    # Create a directory object for the input directory\n    input_dir = Dir(input_dir)\n\n    # Create a directory object for the cache directory\n    cache_dir = Dir(cache_dir)\n\n    # Create a directory object for the input directory\n    input_dir = Dir(input_dir)\n\n    # Create a directory object for the cache directory\n    cache_dir = Dir(cache_dir)\n\n    # Create a directory object for the input directory\n    input_dir = Dir(input_dir)\n\n    # Create a directory object for the cache directory\n    cache_dir = Dir(cache_dir)\n\n    # Create a directory object for the input directory\n    input_dir = Dir(input_dir)\n\n    # Create a directory object for the cache directory\n    cache_dir = Dir(cache_dir)\n\n    # Create a directory object for the input directory\n    input_dir = Dir(input_dir)\n\n    # Create a directory object for the cache directory\n    cache_dir = Dir(cache_dir)\n\n    # Create a directory object for the input directory\n    input_dir = Dir("}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n        else:\n            temp_dir = None\n            file_path = item\n\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n\n        if temp_dir is not None:\n            file_path = os.path.join(temp_dir, file_path)\n\n        if output_dir.scheme == \"s3\":\n            output_dir.put_object(file_path)\n        else:\n            shutil.copy(file_path, output_dir.path)\n\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    if file_size:\n        worker_sizes = [sum(item_sizes) for item_sizes in worker_items]\n        worker_sizes_mb = [size / 1024 / 1024 for size in worker_sizes]\n        print(f\"Worker {worker_ids_this_node} has {worker_sizes_mb} MB of data\")\n    else:\n        print(f\"Worker {worker_ids_this_node} has {worker_weights} of data\")\n\n    worker_items = [shuffle(items) for items in worker_items]\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    num_workers_per_node = num_workers\n    num_workers_total = num_nodes * num_workers_per_node\n    num_items = len(user_items)\n    items_per_worker = num_items // num_workers_total\n    remainder = num_items % num_workers_total\n    items_per_worker_list = [items_per_worker] * num_workers_total\n    if remainder > 0:\n        items_per_worker_list[-remainder:] = [items_per_worker + 1] * remainder\n    items_per_worker_list_cumsum = np.cumsum(items_per_worker_list)\n    items_per_worker_list_cumsum = np.insert(items_per_worker_list_cumsum, 0, 0)\n    items_per_worker_list_cumsum = items_per_worker_list_cumsum.tolist()\n    items_per_worker_list_cumsum = [\n        items_per_worker_list_cumsum[i:i + 2]\n        for i in range(0, len(items_per_worker_list_cumsum), 2)\n    ]\n    items_per_worker_list_cumsum = [\n        [items_per_worker_list_cumsum[i][0], items_per_worker_list_cumsum[i][1]]\n        for i in range(num_workers_per_node * node_rank, num_workers_per_node * (node_rank + 1))\n    ]\n    items_per_worker_list_cumsum = [\n        user_items[items_per_worker_list_cumsum[i][0]:items_per_worker_list_cumsum[i][1]]\n        for i in range(len(items_per_worker_list_cumsum))\n    ]\n    if len(items_per_worker_list_cumsum) != num_workers_per_node:\n        raise RuntimeError(\n            \"The number of workers does not match the number of items assigned to each worker.\"\n        )\n    return items_per_worker_list_cumsum\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Clean up cache directories\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n        os.makedirs(self.cache_dir)\n\n        if os.path.exists(self.cache_dir_train):\n            shutil.rmtree(self.cache_dir_train)\n        os.makedirs(self.cache_dir_train)\n\n        if os.path.exists(self.cache_dir_test):\n            shutil.rmtree(self.cache_dir_test)\n        os.makedirs(self.cache_dir_test)\n\n        if os.path.exists(self.cache_dir_val):\n            shutil.rmtree(self.cache_dir_val)\n        os.makedirs(self.cache_dir_val)\n\n        if os.path.exists(self.cache_dir_train_labels):\n            shutil.rmtree(self.cache_dir_train_labels)\n        os.makedirs(self.cache_dir_train_labels)\n\n        if os.path.exists(self.cache_dir_test_labels):\n            shutil.rmtree(self.cache_dir_test_labels)\n        os.makedirs(self.cache_dir_test_labels)\n\n        if os.path.exists(self.cache_dir_val_labels):\n            shutil.rmtree(self.cache_dir_val_labels)\n        os.makedirs(self.cache_dir_val_labels)\n\n        if os.path.exists(self.cache_dir_train_labels_onehot):\n            shutil.rmtree(self.cache_dir_train_labels_onehot)\n        os.makedirs(self.cache_dir_train_labels_onehot)\n\n        if os.path.exists(self.cache_dir_test_labels_onehot):\n            shutil.rmtree(self.cache_dir_test_labels_onehot)\n        os.makedirs(self.cache_dir_test_labels_onehot)\n\n        if os.path.exists(self.cache_dir_val_labels_onehot):\n            shutil.rmtree(self.cache_dir_val_labels_onehot)\n        os.makedirs(self.cache_dir_val_labels_onehot)\n\n        if os.path.exists"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n    return [future.result() for future in futures]"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        if input_dir:\n            if element.startswith(input_dir):\n                return True\n            elif os.path.exists(element):\n                return True\n        else:\n            if os.path.exists(element):\n                return True\n    return False\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"Number of layers must be greater than 0\"\n        assert n_neurons > 0, \"Number of neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons == 1:\n                model = tinycudann.Network(\n                    n_input_dims,\n                    n_output_dims,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n            else:\n                model = tinycudann.Network(\n                    n_input_dims,\n                    n_output_dims,\n                    n_layers,\n                    n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n        else:\n            model = nn.Sequential()\n            model.add_module(\"input\", nn.Linear(n_input_dims, n_neurons))\n            model.add_module(\"hidden\", nn.Linear(n_neurons, n_neurons))\n            model.add_module(\"output\", nn.Linear(n_neurons, n_output_dims))\n\n            if activation == \"ReLU\":\n                model.add_module(\"activation\", nn.ReLU())\n            elif activation == \"Sigmoid\":\n                model.add_module(\"activation\", nn.Sigmoid())\n\n            if output_activation == \"ReLU\":\n                model.add_module(\"output_activation\", nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                model.add_module(\"output_activation\", nn.Sigmoid())\n\n        return model"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the rolling median of the signal by shifting it by a range defined by the kernel offset.\n        # This generates shifted versions of the signal, which are then used to calculate the rolling median.\n        rolling_median = np.array([np.median(signal[i - kernel_offset:i + kernel_offset]) for i in range(kernel_offset, len(signal) - kernel_offset)])\n\n        # Trim the rolling median array to remove edge effects introduced by the shifting process.\n        # This is done by removing the first and last kernel_offset elements from the rolling median array.\n        return rolling_median[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Convert the rotation shift to columns\n    rotation_shift = int(rotation_shift / 4)\n\n    # Initialize the minimum Hamming distance and corresponding rotation shift\n    min_hamming_distance = np.inf\n    min_rotation_shift = 0\n\n    # Iterate over the possible rotation shifts\n    for i in range(-rotation_shift, rotation_shift + 1):\n        # Calculate the Hamming distance for the current rotation shift\n        hamming_distance = np.sum(\n            np.bitwise_xor(template_probe.code, np.roll(template_gallery.code, i))\n        )\n\n        # Update the minimum Hamming distance and corresponding rotation shift if necessary\n        if hamming_distance < min_hamming_distance:\n            min_hamming_distance = hamming_distance\n            min_rotation_shift = i\n\n    # Calculate the normalized Hamming distance if the nonmatch distance is provided\n    if nm_dist is not None:\n        min_hamming_distance = min_hamming_distance / nm_dist\n\n    # Calculate the weighted Hamming distance if weights are provided\n    if weights is not None:\n        min_hamming_distance = np.sum(\n            min_hamming_distance * np.array(weights)\n        )  # TODO: Check if this is correct\n\n    # Return the minimum Hamming distance and corresponding rotation shift\n    return min_hamming_distance, min_rotation_shift\n\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize the number of bisectors to calculate and the maximum number of iterations for the random selection process.\n        num_bisectors = self.num_bisectors\n        max_iterations = self.max_iterations\n\n        # Initialize the starting and ending points of the calculated perpendicular bisectors.\n        start_points = np.zeros((num_bisectors, 2))\n        end_points = np.zeros((num_bisectors, 2))\n\n        # Initialize the number of iterations and the number of pairs of points that meet the distance criterion.\n        num_iterations = 0\n        num_pairs = 0\n\n        # Loop until the number of pairs of points that meet the distance criterion is equal to the number of bisectors to calculate or the maximum number of iterations is reached.\n        while num_pairs < num_bisectors and num_iterations < max_iterations:\n            # Choose two random points from the polygon's vertices.\n            point1 = polygon[np.random.randint(0, len(polygon))]\n            point2 = polygon[np.random.randint(0, len(polygon))]\n\n            # Calculate the distance between the two points.\n            distance = np.linalg.norm(point1 - point2)\n\n            # If the distance is greater than the minimum distance, add the points to the list of pairs and increment the number of pairs.\n            if distance > min_distance_between_sector_points_in_px:\n                start_points[num_pairs] = point1\n                end_points[num_pairs] = point2\n                num_pairs += 1\n\n            # Increment the number of iterations.\n            num_iterations += 1\n\n        # If the number of pairs of points that meet the distance criterion is less than the number of bisectors to calculate, raise an exception.\n        if num_pairs < num_bisectors:\n            raise EyeCentersEstimationError(\n                f\"Failed to find {num_bisectors} pairs of points that meet the distance criterion within {max_iterations} iterations.\"\n            )\n\n        # Return the starting and ending points of the calculated perpendicular bisectors.\n        return start_points, end_points"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self._pre_execute(*args, **kwargs)\n        result = self._run(*args, **kwargs)\n        self._post_execute(*args, **kwargs)\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output_dict = json.loads(output)\n        except ValueError:\n            return False\n\n        return self.check_type(output_dict, type_definition)"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        def get_class_definition(type_hint):\n            \"\"\"\n            This function returns the class definition for a given type hint. It handles generic types and ensures that class definitions are fetched for non-built-in types.\n\n            Input-Output Arguments\n            :param type_hint: The type hint to get the class definition for.\n            :return: The class definition for the type hint.\n\n            Additional details:\n            - The function checks if the type hint is a generic type and handles it accordingly.\n            - If the type hint is not a generic type, it checks if it is a built-in type.\n            - If it is not a built-in type, it fetches the class definition for the type hint.\n            - If the type hint is a built-in type, it returns None.\n            \"\"\"\n            if inspect.isgeneric(type_hint):\n                return get_class_definition(type_hint.__origin__)\n            if type_hint is not None and not isinstance(type_hint, type):\n                return None\n            return type_hint\n\n        def get_function_type(output_type_hint):\n            \"\"\"\n            This function determines the function type based on the output type hint. It checks if the output type hint is a class or a subclass of a Union and processes it accordingly to determine the function type.\n\n            Input-Output Arguments\n            :param output_type_hint: The output type hint to determine the function type for.\n            :return: The function type, either SYMBOLIC or EMBEDDABLE.\n\n            Additional details:\n            - The function checks if the output type hint is a class or a subclass of a Union.\n            - If it is a class, it checks if it is a subclass of an Embedding class.\n            - If it is a subclass of an Embedding class, the function type is EMBEDDABLE.\n            - If it is not a subclass of an Embedding class, the function type is SYMBOLIC.\n            - If the output type hint is a subclass of a Union, the function type is determined based on the first non-None type in the Union.\n            - If the first non-None type is a subclass of an Embedding class, the function type is EMBEDDABLE.\n            - If the first non-None type is not a subclass of an Embedding class, the function type is SYMBOLIC.\n            \"\"\"\n            if inspect.isclass("}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for i in range(self.num_hashes):\n            index = mmh3.hash(string, seed=i) % self.size\n            self.bit_array[index] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            self.bit_array = self.persistence.load()\n            expected_length = self.size * self.hash_count\n            if len(self.bit_array) != expected_length:\n                self.logger.warning(\n                    \"Loaded bit array length does not match expected length. Reinitializing the bit array and indices.\")\n                self.init_bit_array()\n                self.save()\n        except Exception as e:\n            self.logger.warning(\n                \"Error loading the bit array. Reinitializing the bit array and indices. Error: %s\" % e)\n            self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for func in self.hash_functions:\n            index = func(string) % self.size\n            if not self.bit_array[index]:\n                return False\n        return True"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict.get('distilled_model', None)\n        self.current_model_stats = json_dict.get('current_model_stats', None)\n        self.last_training_run = json_dict.get('last_training_run', None)\n        self.current_training_run = json_dict.get('current_training_run', None)\n        self.nr_of_training_runs = json_dict.get('nr_of_training_runs', None)\n        self.teacher_models = json_dict.get('teacher_models', None)\n        return self"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        if not self.api_key:\n            raise ValueError(\"No API key found\")\n\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration\")\n\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message\")\n\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt\")\n\n        if not isinstance(kwargs, dict):\n            raise ValueError(\"Invalid kwargs\")\n\n        if not isinstance(kwargs.get(\"temperature\"), float):\n            raise ValueError(\"Invalid temperature\")\n\n        if not isinstance(kwargs.get(\"top_p\"), float):\n            raise ValueError(\"Invalid top_p\")\n\n        if not isinstance(kwargs.get(\"frequency_penalty\"), float):\n            raise ValueError(\"Invalid frequency_penalty\")\n\n        if not isinstance(kwargs.get(\"presence_penalty\"), float):\n            raise ValueError(\"Invalid presence_penalty\")\n\n        if not isinstance(kwargs.get(\"max_new_tokens\"), int):\n            raise ValueError(\"Invalid max_new_tokens\")\n\n        if not isinstance(kwargs.get(\"stop\"), list):\n            raise ValueError(\"Invalid stop\")\n\n        if not isinstance(kwargs.get(\"echo\"), bool):\n            raise ValueError(\"Invalid echo\")\n\n        if not isinstance(kwargs.get(\"stream\"), bool):\n            raise ValueError(\"Invalid stream\")\n\n        if not isinstance(kwargs.get(\"logprobs\"), int):\n            raise ValueError(\"Invalid logprobs\")\n\n        if not isinstance(kwargs.get(\"n\"), int):\n            raise ValueError(\"Invalid n\")\n\n        if not isinstance(kwargs.get(\"logit_bias\"), dict):\n            raise ValueError(\"Invalid logit_bias\")\n\n        if not isinstance(kwargs.get(\"user\"), str):\n            raise ValueError(\"Invalid user\")\n\n        if not isinstance(kwargs.get(\"user_message\"), str):\n            raise ValueError(\"Invalid user_message\")\n\n        if not isinstance(kwargs.get(\"model_kwargs\"), dict):\n            raise ValueError(\"Invalid model_kwargs\")\n\n        if not isinstance(kwargs.get(\"model_kwargs\"), dict):\n            raise ValueError(\"Invalid model_kwargs\")\n\n        if not isinstance(kwargs.get(\"model_kwargs\"), dict):\n            raise ValueError(\"Invalid model_kwargs\")\n\n        if not isinstance(kwargs.get(\"model_kwargs\"), dict):\n            raise ValueError(\"Invalid model_kwargs\")\n\n        if not isinstance(kwargs.get(\"model_kwargs\"), dict):\n            raise ValueError(\"Invalid model_kwargs\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert isinstance(x, np.ndarray)\n    assert x.shape[0] == x.shape[1]\n    assert np.allclose(np.diag(x), 0)\n    assert np.allclose(x, x.T)\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        if self.llm_cache.get(func_hash):\n            return self.llm_cache.get(func_hash)\n\n        if not self.initialized_functions.get(func_hash):\n            self.initialized_functions[func_hash] = {\n                \"examples\": [],\n                \"function_description\": function_description,\n                \"func_hash\": func_hash,\n            }\n\n        if self.initialized_functions[func_hash][\"examples\"]:\n            model = self.get_model_for_fine_tuning(\n                self.initialized_functions[func_hash][\"examples\"],\n                self.initialized_functions[func_hash][\"function_description\"],\n            )\n            is_distillable = False\n        else:\n            model = self.get_model_for_zero_shot(\n                self.initialized_functions[func_hash][\"function_description\"]\n            )\n            is_distillable = True\n\n        prompt = self.get_prompt(\n            args,\n            kwargs,\n            self.initialized_functions[func_hash][\"function_description\"],\n            llm_parameters,\n        )\n\n        self.llm_cache[func_hash] = (prompt, model, is_distillable, False)\n        return self.llm_cache[func_hash]"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Compute the nearest positive definite covariance matrix using the Higham & Nick (2002) algorithm\n        cov = nearest_cov_matrix(cov, max_iteration=higham_max_iteration)\n    else:\n        # Compute the nearest positive definite covariance matrix by clipping eigenvalues\n        cov = nearest_cov_matrix_clipping(cov)\n\n    return cov\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    if data_home is None:\n        data_home = Path(get_data_home())\n\n    if not data_home.exists():\n        return\n\n    shutil.rmtree(data_home)"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return obj, None\n    elif isinstance(obj, bytes):\n        return obj, None\n    elif isinstance(obj, list):\n        res = []\n        schema = []\n        for i in obj:\n            r, s = flatten_to_tuple(i)\n            res.append(r)\n            schema.append(s)\n        return tuple(res), schema\n    elif isinstance(obj, tuple):\n        res = []\n        schema = []\n        for i in obj:\n            r, s = flatten_to_tuple(i)\n            res.append(r)\n            schema.append(s)\n        return tuple(res), schema\n    elif isinstance(obj, dict):\n        res = {}\n        schema = {}\n        for k, v in obj.items():\n            r, s = flatten_to_tuple(v)\n            res[k] = r\n            schema[k] = s\n        return res, schema\n    elif isinstance(obj, np.ndarray):\n        return obj, None\n    elif isinstance(obj, torch.Tensor):\n        return obj, None\n    elif isinstance(obj, Instances):\n        res = {}\n        schema = {}\n        for k, v in obj.items():\n            r, s = flatten_to_tuple(v)\n            res[k] = r\n            schema[k] = s\n        return res, schema\n    elif isinstance(obj, Boxes):\n        res = {}\n        schema = {}\n        for k, v in obj.items():\n            r, s = flatten_to_tuple(v)\n            res[k] = r\n            schema[k] = s\n        return res, schema\n    elif isinstance(obj, ROIMasks):\n        res = {}\n        schema = {}\n        for k, v in obj.items():\n            r, s = flatten_to_tuple(v)\n            res[k] = r\n            schema[k] = s\n        return res, schema\n    else:\n        raise ValueError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if the groups and equations are valid\n    if not isinstance(groups, np.ndarray):\n        raise ValueError(f\"The {names[0]} parameter must be a numpy array.\")\n    if not isinstance(equations, np.ndarray):\n        raise ValueError(f\"The {names[1]} parameter must be a numpy array.\")\n    if groups.ndim != 2:\n        raise ValueError(f\"The {names[0]} parameter must be a 2D array.\")\n    if equations.ndim != 1:\n        raise ValueError(f\"The {names[1]} parameter must be a 1D array.\")\n\n    # Check if the groups and equations have the same number of assets\n    if groups.shape[1] != equations.shape[0]:\n        raise ValueError(\n            f\"The number of assets in the {names[0]} parameter must be the same as the number of assets in the {names[1]} parameter.\"\n        )\n\n    # Check if the groups and equations have the same number of assets\n    if groups.shape[1] != equations.shape[0]:\n        raise ValueError(\n            f\"The number of assets in the {names[0]} parameter must be the same as the number of assets in the {names[1]} parameter.\"\n        )\n\n    # Check if the groups and equations have the same number of assets\n    if groups.shape[1] != equations.shape[0]:\n        raise ValueError(\n            f\"The number of assets in the {names[0]} parameter must be the same as the number of assets in the {names[1]} parameter.\"\n        )\n\n    # Check if the groups and equations have the same number of assets\n    if groups.shape[1] != equations.shape[0]:\n        raise ValueError(\n            f\"The number of assets in the {names[0]} parameter must be the same as the number of assets in the {names[1]} parameter.\"\n        )\n\n    # Check if the groups and equations have the same number of assets\n    if groups.shape[1] != equations.shape[0]:\n        raise ValueError(\n            f\"The number of assets in the {names[0]} parameter must be the same as the number of assets in the {names[1]} parameter.\"\n        )\n\n    # Check if the groups and equations have the same number of assets\n    if groups.shape[1] != equations.shape[0]:"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import inspect\n    import importlib\n    import tempfile\n    import os\n    import sys\n    import torch\n    import torch.utils.detectron2\n    import torch.utils.detectron2.data.datasets\n    import torch.utils.detectron2.data.detection_utils\n    import torch.utils.detectron2.data.transforms\n    import torch.utils.detectron2.modeling.backbones\n    import torch.utils.detectron2.modeling.detectors\n    import torch.utils.detectron2.modeling.meta_arch\n    import torch.utils.detectron2.modeling.postprocessing\n    import torch.utils.detectron2.modeling.roi_heads\n    import torch.utils.detectron2.structures.boxes\n    import torch.utils.detectron2.structures.instances\n    import torch.utils.detectron2.structures.keypoints\n    import torch.utils.detectron2.structures.masks\n    import torch.utils.detectron2.utils.comm\n    import torch.utils.detectron2.utils.env\n    import torch.utils.detectron2.utils.logger\n    import torch.utils.detectron2.utils.registry\n    import torch.utils.detectron2.utils.visualizer\n    import torch.utils.detectron2.utils.visualizer_utils\n    import torch.utils.detectron2.data.transforms\n    import torch.utils.detectron2.data.detection_utils\n    import torch.utils.detectron2.data.datasets\n    import torch.utils.detectron2.data.samplers\n    import torch.utils.detectron2.data.transforms\n    import torch.utils.detectron2.data.common\n    import torch.utils.detectron2.data.catalog\n    import torch.utils.detectron2.data.detection_utils\n    import torch.utils.detectron2.data.transforms\n    import torch.utils.detectron2.data.samplers\n    import torch.utils.detectron2.data.common\n    import torch.utils.detectron2.data.catalog\n    import torch.utils.detectron2.data.transforms\n    import torch.utils.detectron2.data.samplers\n    import torch.utils.detectron2.data.common\n    import torch.utils.detectron2.data.catalog\n    import torch.utils.detectron2."}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    import numpy as np\n    import PIL.Image as Image\n    import PIL.ImageOps as ImageOps\n    import PIL.ImageFile as ImageFile\n    import PIL.ImageFilter as ImageFilter\n    import PIL.ImageEnhance as ImageEnhance\n    import PIL.ImageDraw as ImageDraw\n    import PIL.ImageFont as ImageFont\n    import PIL.ImageMath as ImageMath\n    import PIL.ImageColor as ImageColor\n    import PIL.ImageSequence as ImageSequence\n    import PIL.ImageChops as ImageChops\n    import PIL.ImageStat as ImageStat\n    import PIL.ImagePath as ImagePath\n    import PIL.ImagePalette as ImagePalette\n    import PIL.ImageTransform as ImageTransform\n    import PIL.ImageGrab as ImageGrab\n    import PIL.ImageColor as ImageColor\n    import PIL.ImageMath as ImageMath\n    import PIL.ImageQt as ImageQt\n    import PIL.ImageTk as ImageTk\n    import PIL.ImageShow as ImageShow\n    import PIL.ImageFilter as ImageFilter\n    import PIL.ImageEnhance as ImageEnhance\n    import PIL.ImageDraw as ImageDraw\n    import PIL.ImageFont as ImageFont\n    import PIL.ImageMath as ImageMath\n    import PIL.ImageColor as ImageColor\n    import PIL.ImageSequence as ImageSequence\n    import PIL.ImageChops as ImageChops\n    import PIL.ImageStat as ImageStat\n    import PIL.ImagePath as ImagePath\n    import PIL.ImagePalette as ImagePalette\n    import PIL.ImageTransform as ImageTransform\n    import PIL.ImageGrab as ImageGrab\n    import PIL.ImageColor as ImageColor\n    import PIL.ImageMath as ImageMath\n    import PIL.ImageQt as ImageQt\n    import PIL.ImageTk as ImageTk\n    import PIL.ImageShow as ImageShow\n    import PIL.ImageFilter as ImageFilter\n    import PIL.ImageEnhance as ImageEnhance\n    import PIL.ImageDraw as ImageDraw\n    import PIL.ImageFont as ImageFont\n    import PIL.ImageMath as ImageMath\n    import PIL.ImageColor as ImageColor\n    import PIL.ImageSequence as ImageSequence\n    import PIL.ImageChops as ImageChops\n    import PIL.ImageStat as ImageStat\n    import PIL.ImagePath as ImagePath\n    import PIL.ImagePalette as ImagePalette\n    import PIL.ImageTransform as ImageTransform\n    import PIL.ImageGrab as ImageGrab\n    import PIL.ImageColor as ImageColor\n    import PIL.ImageMath as Image"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Apply transforms to bounding box\n    if \"bbox\" in annotation:\n        bbox = annotation[\"bbox\"]\n        bbox_mode = annotation[\"bbox_mode\"]\n        bbox_transformed = transforms.apply_bbox(bbox, bbox_mode)\n        annotation[\"bbox\"] = bbox_transformed\n        annotation[\"bbox_mode\"] = bbox_mode\n\n    # Apply transforms to segmentation\n    if \"segmentation\" in annotation:\n        segmentation = annotation[\"segmentation\"]\n        if isinstance(segmentation, list):\n            # If segmentation is a list of polygons, apply transforms to each polygon\n            segmentation_transformed = []\n            for poly in segmentation:\n                poly_transformed = transforms.apply_poly(poly)\n                segmentation_transformed.append(poly_transformed)\n        elif isinstance(segmentation, dict):\n            # If segmentation is a dict with keys 'counts' and 'size', it's a RLE\n            # Apply transforms to the RLE\n            segmentation_transformed = transforms.apply_rle(segmentation)\n        else:\n            raise ValueError(\n                \"Invalid segmentation format: \"\n                f\"{segmentation.__class__.__name__}\"\n            )\n        annotation[\"segmentation\"] = segmentation_transformed\n\n    # Apply transforms to keypoints\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints_transformed = transforms.apply_keypoints(\n            keypoints, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints_transformed\n\n    return annotation"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n\n        return np.dot(coords, self.rm_coords)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    def get_flops(model: nn.Module, inputs: list) -> typing.DefaultDict[str, float]:\n        \"\"\"\n        This function calculates the floating-point operations (flops) count for each operator in a given model using just-in-time (jit) compilation. It is specifically designed to support standard detection models in detectron2 by running the model with a given input and computing the flops. It emphasizes that the computed flops might vary with different inputs, suggesting averaging across multiple inputs for more accurate estimation.\n\n        Input-Output Arguments\n        :param model: nn.Module. A detectron2 model that is expected to take a list of dictionaries as input. The model is used to compute the flops by running it with the provided inputs.\n        :param inputs: list of dictionaries. These are the inputs to the model, adhering to detectron2's standard input format. Only the \"image\" key within these dictionaries is utilized for the flops computation.\n        :return: DefaultDict[str, float]. A dictionary where keys are operator names and values are the corresponding Gflop counts. This provides an overview of the computational cost associated with each operator in the model.\n        \"\"\"\n        def add_flops_count(m: nn.Module):\n            \"\"\"\n            This function is a hook function that is used to calculate the number of floating-point operations (flops) for each operator in a given model. It is designed to be used with the register_forward_hook method of PyTorch modules.\n\n            Input-Output Arguments\n            :param m: nn.Module. The module for which the flops count is to be calculated.\n            :return: None. The function modifies the global variable named flops_dict to store the flops count for each operator.\n            \"\"\"\n            list_1 = list(m.children())\n            if len(list_1) == 0:\n                global flops_dict\n                if m.__class__.__name__ not in flops_dict:\n                    flops_dict[m.__class__.__name__] = 0\n                flops_dict[m.__class__.__name__] += m.flops\n            else:\n                for i in range(len(list_1)):\n                    add_flops_count(list_1[i])\n\n        def clean_flops_count():\n            \"\"\"\n            This function is"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.rm_image is None:\n            return img\n\n        if img is None or img.shape[0] == 0 or img.shape[1] == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        if self.angle % 360 == 0:\n            return img\n\n        height, width = img.shape[:2]\n        img_center = (width / 2, height / 2)\n        rot_mat = cv2.getRotationMatrix2D(img_center, self.angle, 1)\n        abs_cos = abs(rot_mat[0, 0])\n        abs_sin = abs(rot_mat[0, 1])\n        bound_w = int(height * abs_sin + width * abs_cos)\n        bound_h = int(height * abs_cos + width * abs_sin)\n        rot_mat[0, 2] += (bound_w / 2) - img_center[0]\n        rot_mat[1, 2] += (bound_h / 2) - img_center[1]\n        return cv2.warpAffine(img, rot_mat, (bound_w, bound_h), flags=interp)"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n            colors = [\n                [x / 255 for x in self.metadata.get(\"thing_colors\")[c]] for c in classes\n            ]\n            alpha = 0.8\n        else:\n            colors = None\n            alpha = 0.5\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.img = self._create_grayscale_image(\n                (predictions.pred_masks.any(dim=0) > 0).numpy()\n                if predictions.has(\"pred_masks\")\n                else None\n            )\n            alpha = 0.3\n\n        self.overlay_instances(\n            masks=masks,\n            boxes=boxes,\n            labels=labels,\n            keypoints=keypoints,\n            assigned_colors=colors,\n            alpha=alpha,\n        )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        image = self.get_image()\n        image = image[:, :, :3]\n        return image"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        anns = dic.get(\"annotations\", None)\n        sem_seg = dic.get(\"sem_seg\", None)\n        pan_seg = dic.get(\"pan_seg\", None)\n        if anns:\n            self.dataset_catalog.metadata[\"thing_colors\"] = [\n                [220, 0, 0],\n                [0, 219, 219],\n                [0, 0, 142],\n                [0, 0, 230],\n                [119, 11, 32],\n                [0, 0, 0],\n            ]\n            self.dataset_catalog.metadata[\"stuff_colors\"] = [\n                [128, 64, 128],\n                [244, 35, 232],\n                [70, 70, 70],\n                [102, 102, 156],\n                [190, 153, 153],\n                [128, 128, 0],\n            ]\n            self.dataset_catalog.metadata[\"keypoint_colors\"] = [\n                [220, 220, 0],\n                [170, 0, 220],\n                [0, 220, 220],\n                [48, 230, 230],\n                [220, 50, 170],\n                [230, 220, 220],\n                [150, 50, 220],\n                [170, 170, 220],\n                [0, 117, 220],\n                [220, 117, 0],\n                [117, 0, 220],\n                [204, 204, 0],\n                [78, 204, 0],\n                [0, 204, 204],\n                [204, 0, 204],\n                [102, 51, 0],\n                [255, 0, 0"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        import numpy as np\n        import cv2\n        import matplotlib.pyplot as plt\n        import matplotlib.colors as colors\n        import matplotlib.patches as patches\n        from PIL import Image, ImageDraw, ImageFont\n\n        # If no color is specified, choose a random color\n        if color is None:\n            color = np.random.rand(3)\n\n        # Convert the color to RGB format\n        color = colors.to_rgb(color)\n\n        # Convert the binary mask to an RGB image\n        mask_rgb = np.zeros((binary_mask.shape[0], binary_mask.shape[1], 3), dtype=np.uint8)\n        mask_rgb[binary_mask == 1] = color * 255\n\n        # Convert the RGB image to a PIL image\n        mask_pil = Image.fromarray(mask_rgb)\n\n        # Convert the PIL image to a numpy array\n        mask_np = np.array(mask_pil)\n\n        # Convert the numpy array to a cv2 image\n        mask_cv2 = cv2.cvtColor(mask_np, cv2.COLOR_RGB2BGR)\n\n        # Convert the cv2 image to a PIL image\n        mask_pil = Image.fromarray(mask_cv2)\n\n        # Convert the PIL image to a numpy array\n        mask_np = np.array(mask_pil)\n\n        # Convert the numpy array to a cv2 image\n        mask_cv2 = cv2.cvtColor(mask_np, cv2.COLOR_RGB2BGR)\n\n        # Convert the cv2 image to a PIL image\n        mask_pil = Image.fromarray(mask_cv2)\n\n        # Convert the PIL image to a numpy array\n        mask_np = np.array(mask_pil)\n\n        # Convert the numpy array to a cv2 image\n        mask_cv2 = cv2.cvtColor(mask_np, cv2.COLOR_RGB2BGR)\n\n        # Convert the cv2 image to a PIL image\n        mask_pil = Image.fromarray(mask_cv2)\n\n        # Convert the PIL image to a numpy array\n        mask_np = np.array(mask_pil)\n\n        # Convert the numpy array to a cv2 image\n        mask_cv2 = cv2.cvtColor(mask_np, cv2.COLOR_RGB2BGR)\n\n        # Convert the cv2 image to a"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if size_as_tensor:\n        assert input.image_size.equal(other.image_size), \\\n            f\"{msg}image_size mismatch: {input.image_size} vs. {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, \\\n            f\"{msg}image_size mismatch: {input.image_size} vs. {other.image_size}\"\n\n    for k, v in input.get_fields().items():\n        if k not in other.get_fields():\n            raise ValueError(f\"{msg}field '{k}' is not present in the other Instances\")\n        other_v = other.get_fields()[k]\n        if isinstance(v, torch.Tensor):\n            assert torch.allclose(v, other_v, rtol=rtol), \\\n                f\"{msg}field '{k}' mismatch: {v} vs. {other_v}\"\n        elif isinstance(v, Boxes):\n            assert v.tensor.equal(other_v.tensor), \\\n                f\"{msg}field '{k}' mismatch: {v.tensor} vs. {other_v.tensor}\"\n        elif isinstance(v, ROIMasks):\n            assert v.tensor.equal(other_v.tensor), \\\n                f\"{msg}field '{k}' mismatch: {v.tensor} vs. {other_v.tensor}\"\n        else:\n            raise ValueError(f\"{msg}field '{k}' has unsupported type '{type(v)}'\")"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR == \"PrecomputedProposals\":\n        return None\n    proposal_generator = ProposalGeneratorRegistry.get(cfg.MODEL.PROPOSAL_GENERATOR)(cfg)\n    proposal_generator.to(input_shape.device)\n    proposal_generator.eval()\n    with torch.no_grad():\n        proposal_generator(input_shape)\n    return proposal_generator"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposals_deltas = predictions\n        proposals = self.prepare_inputs(proposals)\n        scores = self.classify(scores)\n        losses = self.losses_classification(scores, proposals)\n        losses.update(self.losses_box_regression(proposals_deltas, proposals))\n        return losses\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER_NAME\n    tracker_head_class = TRACKER_REGISTRY[tracker_name]\n    return tracker_head_class(cfg)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.process(image)\n        elif isinstance(anno_type, str):\n            return self.process(image)[anno_type]\n        elif isinstance(anno_type, list):\n            return {k: v for k, v in self.process(image).items() if k in anno_type}\n        else:\n            raise ValueError(\"Invalid annotation type.\")"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {url: 0 for url in self.urls}\n        for keyword in keywords:\n            for url in self.urls:\n                scores[url] += self.bm25(keyword, url)\n\n        # Aggregate the scores for URLs\n        aggregated_scores = {}\n        for url in self.urls:\n            aggregated_scores[url] = sum(scores[url] for keyword in keywords)\n\n        # Return the dictionary of URLs and their aggregated scores\n        return aggregated_scores"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles of the boxes to be within the range (-180, 180] degrees.\n        self.tensor[:, 4] = torch.remainder(self.tensor[:, 4], 360.0)\n        self.tensor[:, 4] = torch.where(self.tensor[:, 4] > 180.0, self.tensor[:, 4] - 360.0, self.tensor[:, 4])\n\n        # Identify the indices of the boxes that are nearly horizontal based on the clip_angle_threshold.\n        horizontal_boxes_mask = (self.tensor[:, 4] <= clip_angle_threshold) & (self.tensor[:, 4] >= -clip_angle_threshold)\n\n        # For these identified boxes, convert their representation from (center x, center y, width, height, angle) to (x1, y1, x2, y2), where (x1, y1) and (x2, y2) are the coordinates of the top-left and bottom-right corners, respectively.\n        horizontal_boxes = self.tensor[horizontal_boxes_mask]\n        horizontal_boxes[:, 0] -= horizontal_boxes[:, 2] / 2.0\n        horizontal_boxes[:, 1] -= horizontal_boxes[:, 3] / 2.0\n        horizontal_boxes[:, 2] = horizontal_boxes[:, 0] + horizontal_boxes[:, 2]\n        horizontal_boxes[:, 3] = horizontal_boxes[:, 1] + horizontal_boxes[:, 3]\n\n        # Clamp the x and y coordinates to ensure they do not exceed the specified box_size limits.\n        horizontal_boxes[:, 0] = torch.clamp(horizontal_boxes[:, 0], min=0.0, max=box_size[1])\n        horizontal_boxes[:, 1] = torch.clamp(horizontal_boxes[:, 1], min=0.0, max=box_size[0])\n        horizontal_boxes[:, 2] = torch.clamp(horizontal_boxes[:, 2], min=0.0, max=box_size[1])\n        horizontal_boxes[:, 3] = torch.clamp(horizontal_boxes[:, 3], min=0.0, max=box_size[0])\n\n        # Convert the"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary with keys corresponding to the types ('doc', 'gen', 'kno', 'num') and values set to 0.\n        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        # Iterate over the 'data' attribute of the instance, which is expected to be a list of dictionaries, each with a 'type' key.\n        for item in self.data:\n            # Get the 'type' value of the current item.\n            item_type = item['type']\n            # Check if the 'type' value is one of the types ('doc', 'gen', 'kno', 'num').\n            if item_type in statistics:\n                # If it is, increment the corresponding value in the statistics dictionary.\n                statistics[item_type] += 1\n\n        # Return the statistics dictionary.\n        return statistics"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if hasattr(cfg, 'type'):\n        neck_type = cfg.type\n    else:\n        raise ValueError(f\"The configuration dictionary for the neck model must include a 'type' key that specifies the type of neck to be built.\")\n\n    if hasattr(cfg, 'in_channels'):\n        in_channels = cfg.in_channels\n    else:\n        raise ValueError(f\"The configuration dictionary for the neck model must include an 'in_channels' key that specifies the number of input channels for the neck.\")\n\n    if hasattr(cfg, 'out_channels'):\n        out_channels = cfg.out_channels\n    else:\n        raise ValueError(f\"The configuration dictionary for the neck model must include an 'out_channels' key that specifies the number of output channels for the neck.\")\n\n    if hasattr(cfg, 'num_stages'):\n        num_stages = cfg.num_stages\n    else:\n        raise ValueError(f\"The configuration dictionary for the neck model must include a 'num_stages' key that specifies the number of stages for the neck.\")\n\n    if hasattr(cfg, 'norm_cfg'):\n        norm_cfg = cfg.norm_cfg\n    else:\n        raise ValueError(f\"The configuration dictionary for the neck model must include a 'norm_cfg' key that specifies the normalization configuration for the neck.\")\n\n    if hasattr(cfg, 'order'):\n        order = cfg.order\n    else:\n        raise ValueError(f\"The configuration dictionary for the neck model must include an 'order' key that specifies the order of the neck.\")\n\n    if hasattr(cfg, 'act_cfg'):\n        act_cfg = cfg.act_cfg\n    else:\n        raise ValueError(f\"The configuration dictionary for the neck model must include an 'act_cfg' key that specifies the activation configuration for the neck.\")\n\n    if hasattr(cfg, 'init_cfg'):\n        init_cfg = cfg.init_cfg\n    else:\n        raise ValueError(f\"The configuration dictionary for the neck model must include an 'init_cfg' key that specifies the initialization configuration for the neck.\")\n\n    if hasattr(cfg, 'in_indices'):\n        in_indices = cfg.in_indices\n    else:\n        raise ValueError(f\"The configuration dictionary for the neck model must include an 'in_indices' key that specifies the indices of"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg.get('type') == 'mse':\n        return nn.MSELoss()\n    elif cfg.get('type') == 'ce':\n        return nn.CrossEntropyLoss()\n    elif cfg.get('type') == 'nll':\n        return nn.NLLLoss()\n    elif cfg.get('type') == 'bce':\n        return nn.BCELoss()\n    elif cfg.get('type') == 'bcelogit':\n        return nn.BCEWithLogitsLoss()\n    elif cfg.get('type') == 'smoothl1':\n        return nn.SmoothL1Loss()\n    elif cfg.get('type') == 'l1':\n        return nn.L1Loss()\n    elif cfg.get('type') == 'cosine':\n        return nn.CosineEmbeddingLoss()\n    elif cfg.get('type') == 'hinge':\n        return nn.HingeEmbeddingLoss()\n    elif cfg.get('type') == 'multilabelmargin':\n        return nn.MultiLabelMarginLoss()\n    elif cfg.get('type') == 'multilabelsoftmargin':\n        return nn.MultiLabelSoftMarginLoss()\n    elif cfg.get('type') == 'multimargin':\n        return nn.MultiMarginLoss()\n    elif cfg.get('type') == 'triplet':\n        return nn.TripletMarginLoss()\n    elif cfg.get('type') == 'poisson':\n        return nn.PoissonNLLLoss()\n    elif cfg.get('type') == 'kldiv':\n        return nn.KLDivLoss()\n    elif cfg.get('type') == 'marginranking':\n        return nn.MarginRankingLoss()\n    elif cfg.get('type') == 'huber':\n        return nn.HuberLoss()\n    elif cfg.get('type') == 'softmargin':\n        return nn.SoftMarginLoss()\n    elif cfg.get('type') == 'multisoftmargin':\n        return nn.MultiLabelSoftMarginLoss()\n    elif cfg.get('type') == 'cosinesimilarity':\n        return nn.CosineSimilarity()\n    elif cfg.get('type') == 'cosineembedding':\n        return nn.CosineEmbeddingLoss()\n    elif cfg.get('type') == 'ctc':\n        return nn.CTCLoss()\n    elif cfg.get('type') == '"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if hasattr(mmdet.HEADS, cfg.type):\n        head = build_head(cfg)\n    elif hasattr(mmdet.HEADS, cfg.type):\n        head = build_head(cfg)\n    else:\n        raise NotImplementedError(f\"head type {cfg.type} is not implemented\")\n    return head"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from mmseg.models import build_segmentor\n    return build_segmentor(cfg, train_cfg, test_cfg)"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None:\n        warnings.warn(\n            'train_cfg is deprecated in detector.py, please set '\n            'it in the model field.', DeprecationWarning)\n        cfg.train_cfg = train_cfg\n    if test_cfg is not None:\n        warnings.warn(\n            'test_cfg is deprecated in detector.py, please set '\n            'it in the model field.', DeprecationWarning)\n        cfg.test_cfg = test_cfg\n    if train_cfg is not None and cfg.get('train_cfg') is not None:\n        raise ValueError('train_cfg is specified in both outer field '\n                         'and model field')\n    if test_cfg is not None and cfg.get('test_cfg') is not None:\n        raise ValueError('test_cfg is specified in both outer field '\n                         'and model field')\n\n    if 'type' not in cfg:\n        raise KeyError('the detector must have a type')\n    if 'type' in cfg and cfg['type'] in DETECTORS:\n        detector_class = DETECTORS.get(cfg['type'], None)\n    elif 'type' in cfg and cfg['type'] in MMDET_DETECTORS:\n        detector_class = MMDET_DETECTORS.get(cfg['type'], None)\n    else:\n        raise KeyError(f'unknown detector type {cfg[\"type\"]}')\n\n    if detector_class is None:\n        raise KeyError(f'unknown detector type {cfg[\"type\"]}')\n\n    return detector_class(cfg)"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # Initialize the evaluation results dictionary\n    eval_results = {}\n\n    # Iterate over the labels\n    for label in label2cat:\n\n        # Initialize the evaluation results for this label\n        eval_results[label] = {}\n\n        # Iterate over the IoU thresholds\n        for iou in metric:\n\n            # Initialize the true positive count for this label and IoU threshold\n            tp = 0\n\n            # Iterate over the detection annotations\n            for dt_anno in dt_annos:\n\n                # Check if the detection annotation has the correct label\n                if dt_anno['label'] == label:\n\n                    # Iterate over the ground truth annotations\n                    for gt_anno in gt_annos:\n\n                        # Check if the ground truth annotation has the correct label\n                        if gt_anno['label'] == label:\n\n                            # Calculate the IoU between the detection annotation and ground truth annotation\n                            iou_score = box_iou(dt_anno['box'], gt_anno['box'], box_type_3d, box_mode_3d)\n\n                            # Check if the IoU is greater than the current IoU threshold\n                            if iou_score > iou:\n\n                                # Increment the true positive count\n                                tp += 1\n\n                                # Break out of the loop\n                                break\n\n            # Calculate the precision for this label and IoU threshold\n            precision = tp / len(dt_annos)\n\n            # Calculate the recall for this label and IoU threshold\n            recall = tp / len(gt_annos)\n\n            # Store the precision and recall in the evaluation results dictionary\n            eval_results[label][iou] = {'precision': precision, 'recall': recall}\n\n    # Calculate the mean Average Precision (mAP) and mean Average Recall (mAR)\n    mAP = 0\n    mAR = 0\n\n    # Iterate over the labels\n    for label in label2cat:\n\n        # Initialize the average precision and recall for this label\n        ap = 0\n        ar = 0\n\n        # Iterate over the IoU thresholds\n        for iou in metric:\n\n            # Calculate the average precision for this label and IoU threshold\n            ap += eval_results[label][iou]['precision']\n\n            # Calculate the average recall for this label and IoU threshold\n            ar += eval_results[label][iou]['recall']\n\n        # Calculate the mean average precision and recall for this label\n        ap /= len(metric)\n        ar /= len(metric)\n\n        # Update"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_class = Box3DL\n        mode = 'LiDAR'\n    elif box_type == 'Camera':\n        box_class = Box3DC\n        mode = 'Camera'\n    elif box_type == 'Depth':\n        box_class = Box3DD\n        mode = 'Depth'\n    else:\n        raise ValueError(\n            'box_type must be LiDAR, Camera, or Depth, but received %s' % box_type)\n    return box_class, mode\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('A model must be provided.')\n\n    if messages is not None:\n      if not isinstance(messages, list):\n        raise TypeError('Messages must be a list.')\n      for message in messages:\n        if not isinstance(message, (Message, dict)):\n          raise TypeError('Messages must be a list of Message or dict-like objects.')\n\n    if options is not None:\n      if not isinstance(options, Options):\n        raise TypeError('Options must be an instance of Options.')\n\n    if keep_alive is not None:\n      if not isinstance(keep_alive, (float, str)):\n        raise TypeError('Keep-alive must be a float or string.')\n\n    if format not in ('', 'json'):\n      raise RequestError(f'Invalid format: {format}')\n\n    if stream:\n      return self._stream_chat(model, messages, options, keep_alive)\n\n    return self._chat(model, messages, options, keep_alive)\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    if not isinstance(model, str):\n      raise TypeError(\"model must be a string\")\n\n    if not isinstance(insecure, bool):\n      raise TypeError(\"insecure must be a boolean\")\n\n    if not isinstance(stream, bool):\n      raise TypeError(\"stream must be a boolean\")\n\n    if not isinstance(self.url, str):\n      raise TypeError(\"url must be a string\")\n\n    if not isinstance(self.token, str):\n      raise TypeError(\"token must be a string\")\n\n    if not isinstance(self.timeout, int):\n      raise TypeError(\"timeout must be an integer\")\n\n    if not isinstance(self.retries, int):\n      raise TypeError(\"retries must be an integer\")\n\n    if not isinstance(self.backoff, float):\n      raise TypeError(\"backoff must be a float\")\n\n    if not isinstance(self.jitter, float):\n      raise TypeError(\"jitter must be a float\")\n\n    if not isinstance(self.retry_statuses, list):\n      raise TypeError(\"retry_statuses must be a list\")\n\n    if not isinstance(self.retry_methods, list):\n      raise TypeError(\"retry_methods must be a list\")\n\n    if not isinstance(self.retry_after, float):\n      raise TypeError(\"retry_after must be a float\")\n\n    if not isinstance(self.retry_backoff, float):\n      raise TypeError(\"retry_backoff must be a float\")\n\n    if not isinstance(self.retry_jitter, float):\n      raise TypeError(\"retry_jitter must be a float\")\n\n    if not isinstance(self.retry_statuses, list):\n      raise TypeError(\"retry_statuses must be a list\")\n\n    if not isinstance(self.retry_methods, list):\n      raise TypeError(\"retry_methods must be a list\")\n\n    if not isinstance(self.retry_after, float):\n      raise TypeError(\"retry_after must be a float\")\n\n    if not isinstance(self.retry_backoff, float):\n      raise TypeError(\"retry_backoff must be a float\")\n\n    if not isinstance(self.retry_jitter, float):\n      raise TypeError(\"retry_jitter must be a float\")\n\n    if not isinstance(self.retry_statuses, list):\n      raise TypeError(\"retry_statuses must be a list\")\n\n    if not isinstance(self.retry_methods, list):\n      raise TypeError(\"retry_methods must be a list\")\n\n    if not isinstance(self.retry_"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not prompt:\n      raise ValueError('prompt is required')\n\n    if not isinstance(prompt, str):\n      raise ValueError('prompt must be a string')\n\n    if not isinstance(system, str):\n      raise ValueError('system must be a string')\n\n    if not isinstance(template, str):\n      raise ValueError('template must be a string')\n\n    if not isinstance(context, (list, tuple, type(None))):\n      raise ValueError('context must be a list, tuple, or None')\n\n    if not isinstance(stream, bool):\n      raise ValueError('stream must be a boolean')\n\n    if not isinstance(raw, bool):\n      raise ValueError('raw must be a boolean')\n\n    if not isinstance(format, str):\n      raise ValueError('format must be a string')\n\n    if not isinstance(images, (list, tuple, type(None))):\n      raise ValueError('images must be a list, tuple, or None')\n\n    if not isinstance(options, (Options, type(None))):\n      raise ValueError('options must be an Options object or None')\n\n    if not isinstance(keep_alive, (float, str, type(None))):\n      raise ValueError('keep_alive must be a float, string, or None')\n\n    if not isinstance(model, str):\n      raise ValueError('model must be a string')\n\n    if not isinstance(prompt, str):\n      raise ValueError('prompt must be a string')\n\n    if not isinstance(system, str):\n      raise ValueError('system must be a string')\n\n    if not isinstance(template, str):\n      raise ValueError('template must be a string')\n\n    if not isinstance(context, (list, tuple, type(None))):\n      raise ValueError('context must be a list, tuple, or None')\n\n    if not isinstance(stream, bool):\n      raise ValueError('stream must be a boolean')\n\n    if not isinstance(raw, bool):\n      raise ValueError('raw must be a boolean')\n\n    if not isinstance(format, str):\n      raise ValueError('format must be a string')\n\n    if not isinstance(images, (list, tuple, type(None))):\n      raise ValueError('images must be a list, tuple, or None')\n\n    if not isinstance(options, (Options, type(None))):\n      raise ValueError('options must be an Options object or None')\n\n    if not isinstance(keep_"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self._stream_request(\n        \"POST\",\n        \"/api/push\",\n        data={\"model\": model, \"insecure\": insecure},\n        stream=True,\n      )\n\n    return self._request(\n      \"POST\",\n      \"/api/push\",\n      data={\"model\": model, \"insecure\": insecure},\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError(\n        \"Either path or modelfile must be provided for model creation\"\n      )\n\n    if path:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      return self._create_stream(model, modelfile)\n\n    return self._create(model, modelfile)"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n        checksum = hashlib.sha256(f.read()).hexdigest()\n\n    # Check if the blob already exists on the server\n    response = self.session.head(f'{self.url}/blobs/{checksum}')\n\n    if response.status_code == 404:\n        # If the blob does not exist, upload it as a new blob\n        with open(path, 'rb') as f:\n            response = self.session.post(f'{self.url}/blobs', files={'file': f})\n\n        if response.status_code != 201:\n            raise RuntimeError(f'Failed to upload blob: {response.text}')\n\n    return f'sha256:{checksum}'"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('Model is required')\n\n    if not prompt and not system and not template and not context and not images:\n      raise ValueError('At least one of prompt, system, template, context, or images is required')\n\n    if not isinstance(prompt, str):\n      raise TypeError('Prompt must be a string')\n\n    if not isinstance(system, str):\n      raise TypeError('System must be a string')\n\n    if not isinstance(template, str):\n      raise TypeError('Template must be a string')\n\n    if not isinstance(context, Sequence):\n      raise TypeError('Context must be a sequence')\n\n    if not isinstance(images, Sequence):\n      raise TypeError('Images must be a sequence')\n\n    if not isinstance(options, Options):\n      raise TypeError('Options must be an instance of the Options class')\n\n    if not isinstance(keep_alive, (float, str)):\n      raise TypeError('Keep-alive must be a float or string')\n\n    if not isinstance(raw, bool):\n      raise TypeError('Raw must be a boolean')\n\n    if not isinstance(stream, bool):\n      raise TypeError('Stream must be a boolean')\n\n    if not isinstance(format, Literal['', 'json']):\n      raise TypeError('Format must be an empty string or \"json\"')\n\n    if not isinstance(model, str):\n      raise TypeError('Model must be a string')\n\n    if not isinstance(prompt, str):\n      raise TypeError('Prompt must be a string')\n\n    if not isinstance(system, str):\n      raise TypeError('System must be a string')\n\n    if not isinstance(template, str):\n      raise TypeError('Template must be a string')\n\n    if not isinstance(context, Sequence):\n      raise TypeError('Context must be a sequence')\n\n    if not isinstance(images, Sequence):\n      raise TypeError('Images must be a sequence')\n\n    if not isinstance(options, Options):\n      raise TypeError('Options must be an instance of the Options class')\n\n    if not isinstance(keep_alive, (float, str)):\n      raise TypeError('Keep-alive must be a float or string')\n\n    if not isinstance(raw, bool):\n      raise TypeError('Raw must be a boolean')\n\n    if not isinstance(stream, bool):\n      raise TypeError('Stream must be a boolean')\n\n    if not isinstance(format, Literal['', 'json']):\n      raise TypeError('Format must be an empty string or \"json\"')\n\n    if not isinstance(model"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    url = self._url(f\"/pull/{model}\")\n    headers = self._headers(insecure)\n    async with self._session.get(url, headers=headers, stream=stream) as response:\n      if response.status != 200:\n        raise ResponseError(response)\n      if stream:\n        async for line in response.content:\n          yield json.loads(line)\n      else:\n        return await response.json()"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not isinstance(model, str):\n      raise TypeError('model must be a string')\n\n    if not isinstance(messages, (list, tuple, type(None))):\n      raise TypeError('messages must be a list, tuple, or None')\n\n    if messages is not None:\n      for message in messages:\n        if not isinstance(message, Message):\n          raise TypeError('messages must be a list of Message objects')\n\n    if not isinstance(stream, bool):\n      raise TypeError('stream must be a boolean')\n\n    if not isinstance(format, str):\n      raise TypeError('format must be a string')\n\n    if not isinstance(options, (Options, type(None))):\n      raise TypeError('options must be an Options object or None')\n\n    if not isinstance(keep_alive, (float, str, type(None))):\n      raise TypeError('keep_alive must be a float, string, or None')\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be \"\" or \"json\"')\n\n    if options is not None:\n      if not isinstance(options.temperature, (float, type(None))):\n        raise TypeError('temperature must be a float or None')\n\n      if not isinstance(options.top_p, (float, type(None))):\n        raise TypeError('top_p must be a float or None')\n\n      if not isinstance(options.max_tokens, (int, type(None))):\n        raise TypeError('max_tokens must be an int or None')\n\n      if not isinstance(options.frequency_penalty, (float, type(None))):\n        raise TypeError('frequency_penalty must be a float or None')\n\n      if not isinstance(options.presence_penalty, (float, type(None))):\n        raise TypeError('presence_penalty must be a float or None')\n\n      if not isinstance(options.best_of, (int, type(None))):\n        raise TypeError('best_of must be an int or None')\n\n      if not isinstance(options.stop, (list, type(None))):\n        raise TypeError('stop must be a list or None')\n\n      if not isinstance(options.logit_bias, (dict, type(None))):\n        raise TypeError('logit_bias must be a dict or None')\n\n      if not isinstance(options.user, (str, type(None))):\n        raise TypeError('user must be"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    url = self.url + \"/api/push\"\n    headers = {\"Content-Type\": \"application/json\"}\n    if insecure:\n      headers[\"x-mlrun-insecure\"] = \"true\"\n    data = {\"model\": model}\n    async with self.session.post(url, headers=headers, json=data) as resp:\n      if resp.status != 200:\n        raise MLRunHTTPErr(resp.status, await resp.text())\n      if stream:\n        async for line in resp.content:\n          yield json.loads(line)\n      else:\n        return await resp.json()"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    with open(path, 'rb') as f:\n        checksum = hashlib.sha256(f.read()).hexdigest()\n\n    # Check if a blob with that checksum already exists on the server\n    response = await self.head(f'/api/blobs/{checksum}')\n\n    # If not found (404 status code), upload the file in chunks to the server\n    if response.status_code == 404:\n        with open(path, 'rb') as f:\n            async with aiohttp.ClientSession() as session:\n                async with session.post(f'{self.url}/api/blobs/{checksum}', data=f) as response:\n                    if response.status_code != 201:\n                        raise RuntimeError(f'Failed to upload file: {response.status_code}')\n\n    # Return the digest of the file\n    return f'sha256:{checksum}'"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Initialize the type check result with a message indicating that the type check is in progress.\n        type_check_result = TypeCheckResult(\n            message=\"Type check in progress...\", passed=False\n        )\n\n        # Check if the user code is empty. If it is, return the type check result with a message indicating that the user code is empty.\n        if not user_code:\n            type_check_result.message = \"User code is empty.\"\n            return type_check_result\n\n        # Check if the test code is empty. If it is, return the type check result with a message indicating that the test code is empty.\n        if not test_code:\n            type_check_result.message = \"Test code is empty.\"\n            return type_check_result\n\n        # Check if the user code contains a return statement. If it does not, append a return statement to the end of the user code.\n        if \"return\" not in user_code:\n            user_code += \"\\nreturn None\"\n\n        # Check if the user code contains a print statement. If it does not, append a print statement to the end of the user code.\n        if \"print\" not in user_code:\n            user_code += \"\\nprint(None)\"\n\n        # Check if the user code contains a pass statement. If it does not, append a pass statement to the end of the user code.\n        if \"pass\" not in user_code:\n            user_code += \"\\npass\"\n\n        # Check if the user code contains a yield statement. If it does not, append a yield statement to the end of the user code.\n        if \"yield\" not in user_code:\n            user_code += \"\\nyield None\"\n\n        # Check if the user code contains a raise statement. If it does not, append a raise statement to the end of the user code.\n        if \"raise\" not in user_code:\n            user_code += \"\\nraise Exception()\"\n\n        # Check if the user code contains a break statement. If it does not, append a break statement to the end of the user code.\n        if \"break\" not in user_code:\n            user_code += \"\\nbreak\"\n\n        # Check if the user code contains a continue statement. If it does not, append a continue statement to the end of the user code.\n        if \"continue\" not in user_code:"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not path and not modelfile:\n      raise RequestError(\"Either path or modelfile must be provided\")\n\n    if path:\n      with open(path, \"rb\") as f:\n        data = f.read()\n    else:\n      data = modelfile.encode()\n\n    headers = {\"Content-Type\": \"application/json\"}\n    url = f\"{self.url}/models/{model}\"\n    async with self.session.post(url, data=data, headers=headers) as resp:\n      if resp.status >= 400:\n        raise ResponseError(resp.status, await resp.text())\n      if stream:\n        async for chunk in resp.content.iter_chunked(1024):\n          yield json.loads(chunk)\n      else:\n        return await resp.json()"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn)\n    else:\n        return aot_function(fn)\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary file from the trial directory\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n\n    # Get the best configuration from the summary file\n    best_config = summary_df.iloc[0][\"config\"]\n\n    # Load the configuration file\n    config_path = os.path.join(trial_path, \"config.yaml\")\n    with open(config_path, \"r\") as f:\n        config_dict = yaml.safe_load(f)\n\n    # Extract the best configuration from the configuration file\n    best_config_dict = config_dict[best_config]\n\n    # Save the best configuration to a YAML file if specified\n    if output_path is not None:\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config_dict, f)\n\n    return best_config_dict"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    from torch.utils.hooks import RemovableHandle\n    from torch.utils.hooks import _remove_hooks\n    from torch.utils.hooks import _remove_hooks_by_name\n    from torch.utils.hooks import _remove_hooks_by_regex\n    from torch.utils.hooks import _remove_hooks_by_types\n    from torch.utils.hooks import _remove_hooks_by_types_and_regex\n    from torch.utils.hooks import _remove_hooks_by_types_and_regex_and_name\n    from torch.utils.hooks import _remove_hooks_by_types_and_regex_and_name_and_scope\n    from torch.utils.hooks import _remove_hooks_by_types_and_regex_and_scope\n    from torch.utils.hooks import _remove_hooks_by_types_and_scope\n    from torch.utils.hooks import _remove_hooks_by_types_and_scope_and_name\n    from torch.utils.hooks import _remove_hooks_by_types_and_scope_and_name_and_regex\n    from torch.utils.hooks import _remove_hooks_by_types_and_scope_and_regex\n    from torch.utils.hooks import _remove_hooks_by_types_and_scope_and_regex_and_name\n    from torch.utils.hooks import _remove_hooks_by_types_and_scope_and_regex_and_name_and_scope\n    from torch.utils.hooks import _remove_hooks_by_types_and_scope_and_regex_and_scope\n    from torch.utils.hooks import _remove_hooks_by_types_and_scope_and_regex_and_scope_and_name\n    from torch.utils.hooks import _remove_hooks_by_types_and_scope_and_scope\n    from torch.utils.hooks import _remove_hooks_by_types_and_scope_and_scope_and_name\n    from torch.utils.hooks import _remove_hooks_by_types_and_scope_and_scope_and_name_and_regex\n    from torch.utils.hooks import _remove_hooks_by_types_and_scope_and_scope_and_regex\n    from"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Load the best configuration from the trial folder\n        best_config = load_json(os.path.join(trial_path, \"best_config.json\"))\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        runner = cls(best_config, project_dir)\n\n        return runner"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the directory if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Run each module with its parameters and save the results\n    results = []\n    for module, params in zip(modules, module_params):\n        result = module(previous_result, **params)\n        results.append(result)\n\n    # Save the results to disk\n    for i, result in enumerate(results):\n        result.to_csv(os.path.join(node_line_dir, f\"result_{i}.csv\"), index=False)\n\n    # Evaluate the results and select the best one\n    best_result = None\n    best_score = -np.inf\n    for i, result in enumerate(results):\n        score = evaluate_result(result, previous_result, strategies)\n        if score > best_score:\n            best_result = result\n            best_score = score\n\n    # Save the best result to disk\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    # Save the evaluation metrics to disk\n    with open(os.path.join(node_line_dir, \"evaluation_metrics.txt\"), \"w\") as f:\n        f.write(str(strategies))\n\n    # Combine the previous result columns with the selected result columns\n    best_result = pd.concat([previous_result, best_result], axis=1)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize variables\n    best_result = None\n    best_module = None\n    best_module_params = None\n    best_module_time = None\n    best_module_score = None\n\n    # Iterate over each module and its parameters\n    for module, module_params in zip(modules, module_params):\n\n        # Initialize variables\n        module_time = None\n        module_score = None\n\n        # Iterate over each parameter set\n        for params in module_params:\n\n            # Initialize variables\n            module_result = None\n            module_time = None\n            module_score = None\n\n            # Run the module with the given parameters\n            try:\n                module_result = module(previous_result, **params)\n            except Exception as e:\n                print(f\"Error running module {module.__name__} with parameters {params}: {e}\")\n                continue\n\n            # Measure the execution time\n            start_time = time.time()\n            module_result = module(previous_result, **params)\n            end_time = time.time()\n            module_time = end_time - start_time\n\n            # Evaluate the module's performance\n            module_score = evaluate_module(module_result, strategies)\n\n            # Save the module's results and evaluation metrics\n            save_module_results(module_result, module_time, module_score, node_line_dir, module.__name__, params)\n\n            # Check if the module's performance is better than the current best\n            if best_result is None or module_score > best_module_score:\n                best_result = module_result\n                best_module = module\n                best_module_params = params\n                best_module_time = module_time\n                best_module_score = module_score\n\n    # Save the best module's results and evaluation metrics\n    save_best_module_results(best_result, best_module_time, best_module_score, node_line_dir, best_module.__name__,\n                             best_module_params)\n\n    # Return the best result\n    return best_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create the output directory if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Create the output directory for the best prompt maker module if it doesn't exist\n    best_prompt_maker_dir = os.path.join(node_line_dir, \"best_prompt_maker\")\n    if not os.path.exists(best_prompt_maker_dir):\n        os.makedirs(best_prompt_maker_dir)\n\n    # Create the output directory for the prompt maker modules if it doesn't exist\n    prompt_maker_modules_dir = os.path.join(node_line_dir, \"prompt_maker_modules\")\n    if not os.path.exists(prompt_maker_modules_dir):\n        os.makedirs(prompt_maker_modules_dir)\n\n    # Create the output directory for the prompt maker modules' results if it doesn't exist\n    prompt_maker_modules_results_dir = os.path.join(prompt_maker_modules_dir, \"results\")\n    if not os.path.exists(prompt_maker_modules_results_dir):\n        os.makedirs(prompt_maker_modules_results_dir)\n\n    # Create the output directory for the prompt maker modules' summaries if it doesn't exist\n    prompt_maker_modules_summaries_dir = os.path.join(prompt_maker_modules_dir, \"summaries\")\n    if not os.path.exists(prompt_maker_modules_summaries_dir):\n        os.makedirs(prompt_maker_modules_summaries_dir)\n\n    # Create the output directory for the prompt maker modules' summaries if it doesn't exist\n    prompt_maker_modules_summaries_dir = os.path.join(prompt_maker_modules_dir, \"summaries\")\n    if not os.path.exists(prompt_maker_modules_summaries_dir):\n        os.makedirs(prompt_maker_modules_summaries_dir)\n\n    # Create the output directory for the prompt maker modules' summaries if it doesn't exist\n    prompt_maker_modules_summaries_dir = os.path.join(prompt_maker_modules_dir, \"summaries\")\n    if not os.path.exists("}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        if node.module_params:\n            for module_param in node.module_params:\n                if module_param.key == key:\n                    values.append(module_param.value)\n\n    return list(set(values))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(model_name=\"all-mpnet-base-v2\")\n\n    gt_embeddings = embedding_model.encode(generation_gt)\n    pred_embedding = embedding_model.encode([pred])[0]\n\n    max_similarity = 0\n    for gt_embedding in gt_embeddings:\n        similarity = cosine_similarity(gt_embedding, pred_embedding)\n        if similarity > max_similarity:\n            max_similarity = similarity\n\n    return max_similarity"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    import os\n    import cv2\n    import numpy as np\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras import layers\n    from tensorflow.keras.layers import Input, Conv2D, DepthwiseConv2D, BatchNormalization, ReLU, Add, UpSampling2D, Concatenate\n    from tensorflow.keras.models import Model\n    from tensorflow.keras.applications import EfficientNetB4\n    from tensorflow.keras.layers.experimental import preprocessing\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.losses import MeanSquaredError\n    from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n    from tensorflow.keras.utils import plot_model\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    from tensorflow.keras.preprocessing.image import load_img\n    from tensorflow.keras.preprocessing.image import img_to_array\n    from tensorflow.keras.preprocessing.image import array_to_img\n    from tensorflow.keras.preprocessing.image import save_img\n    from tensorflow.keras.preprocessing.image import random_rotation\n    from tensorflow.keras.preprocessing.image import random_shift\n    from tensorflow.keras.preprocessing.image import random_zoom\n    from tensorflow.keras.preprocessing.image import random_shear\n    from tensorflow.keras.preprocessing.image import random_channel_shift\n    from tensorflow.keras.preprocessing.image import apply_affine_transform\n    from tensorflow.keras.preprocessing.image import apply_channel_shift\n    from tensorflow.keras.preprocessing.image import flip_axis\n    from tensorflow.keras.preprocessing.image import random_brightness\n    from tensorflow.keras.preprocessing.image import random_contrast\n    from tensorflow.keras.preprocessing.image import random_crop\n    from tensorflow.keras.preprocessing.image import random_zoom\n    from tensorflow.keras.preprocessing.image import random_shear\n    from tensorflow.keras.preprocessing.image import random_rotation\n    from tensorflow.keras.preprocessing.image import random_shift\n    from tensorflow.keras.preprocessing.image import random_channel_shift\n    from tensorflow.keras.preprocessing.image import apply_affine_transform\n    from tensorflow.keras.preprocessing.image import apply_channel_shift\n    from tensorflow.keras.preprocessing.image import flip_axis\n    from tensorflow.keras.preprocessing.image import random_brightness\n    from tensorflow.keras.preprocessing.image import random_contrast\n   "}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global face_restorers\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        print(e)\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import os\n    import sys\n    import torch\n    import numpy as np\n    import facexlib.utils.download as download\n    import facexlib.utils.face_align as face_align\n    import facexlib.utils.face_detect as face_detect\n    import facexlib.utils.video_tool as video_tool\n    import facexlib.utils.image_tool as image_tool\n    import facexlib.utils.config as config\n    import facexlib.utils.progress as progress\n    import facexlib.utils.image_pool as image_pool\n    import facexlib.models.networks as networks\n    import facexlib.models.gfpgan as gfpgan\n    import facexlib.models.gfpgan_model as gfpgan_model\n    import facexlib.models.gfpgan_model_config as gfpgan_model_config\n    import facexlib.models.gfpgan_model_config_v1 as gfpgan_model_config_v1\n    import facexlib.models.gfpgan_model_config_v2 as gfpgan_model_config_v2\n    import facexlib.models.gfpgan_model_config_v3 as gfpgan_model_config_v3\n    import facexlib.models.gfpgan_model_config_v4 as gfpgan_model_config_v4\n    import facexlib.models.gfpgan_model_config_v5 as gfpgan_model_config_v5\n    import facexlib.models.gfpgan_model_config_v6 as gfpgan_model_config_v6\n    import facexlib.models.gfpgan_model_config_v7 as gfpgan_model_config_v7\n    import facexlib.models.gfpgan_model_config_v8 as gfpgan_model_config_v8\n    import facexlib.models.gfpgan_model_config_v9 as gfpgan_model_config_v9\n    import facexlib.models.gfpgan_model_config_v10 as gfpgan_model_config_v10\n    import facexlib.models.gfpgan_model_config_v1"}
{"namespace": "quaternion.rotate", "completion": "  import numpy as np\n\n  q = np.array(q)\n  v = np.array(v)\n\n  qv = np.array([0, v[0], v[1], v[2]])\n  qv_conj = np.array([0, -v[0], -v[1], -v[2]])\n\n  qv_rot = np.dot(q, np.dot(qv, qv_conj))\n\n  return qv_rot[1:]\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis = axis_angle[:3]\n  angle = axis_angle[3]\n\n  if angle < eps:\n    return jnp.array([1, 0, 0, 0])\n\n  axis = axis / jnp.linalg.norm(axis)\n  half_angle = angle / 2\n  return jnp.array(\n      [jnp.cos(half_angle), axis[0] * jnp.sin(half_angle),\n       axis[1] * jnp.sin(half_angle), axis[2] * jnp.sin(half_angle)]\n  )\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk, could be done outside and passed in\n    topk = model.get_topk(prefix, k)\n    topk_idxs = [x[0] for x in topk]\n    topk_probs = [x[1] for x in topk]\n\n    # get the index of the target index in the topk\n    idx_in_topk = topk_idxs.index(idx)\n\n    # get the probability of the target index being the top result\n    top_prob = topk_probs[idx_in_topk]\n\n    # get the probability of the target index being the top result\n    top_prob = topk_probs[idx_in_topk]\n\n    # get the probability of the target index being the top result\n    top_prob = topk_probs[idx_in_topk]\n\n    # get the probability of the target index being the top result\n    top_prob = topk_probs[idx_in_topk]\n\n    # get the probability of the target index being the top result\n    top_prob = topk_probs[idx_in_topk]\n\n    # get the probability of the target index being the top result\n    top_prob = topk_probs[idx_in_topk]\n\n    # get the probability of the target index being the top result\n    top_prob = topk_probs[idx_in_topk]\n\n    # get the probability of the target index being the top result\n    top_prob = topk_probs[idx_in_topk]\n\n    # get the probability of the target index being the top result\n    top_prob = topk_probs[idx_in_topk]\n\n    # get the probability of the target index being the top result\n    top_prob = topk_probs[idx_in_topk]\n\n    # get the probability of the target index being the top result\n    top_prob = topk_probs[idx_in_topk]\n\n    # get the probability of the target index being the top result\n    top_prob = topk_probs[idx_in_topk]\n\n    # get the probability of the target index being the top result\n    top_prob = topk_probs[idx_in_topk]\n\n    # get the probability of the target index being the top result\n    top_prob"}
{"namespace": "resample.resample_3d", "completion": "  import tensorflow as tf\n  import numpy as np\n\n  # Check if the input data is a tensor\n  if not isinstance(data, tf.Tensor):\n    raise ValueError('The input data must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise ValueError('The input locations must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise ValueError('The input locations must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise ValueError('The input locations must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise ValueError('The input locations must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise ValueError('The input locations must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise ValueError('The input locations must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise ValueError('The input locations must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise ValueError('The input locations must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise ValueError('The input locations must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise ValueError('The input locations must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise ValueError('The input locations must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise ValueError('The input locations must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise ValueError('The input locations must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise ValueError('The input locations"}
{"namespace": "math.plus_eps", "completion": "  if abs(x) < 1e-10:\n    return 1e-10\n  else:\n    return x + 1e-10\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x > tiny_val:\n    return x - 1e-10\n  else:\n    return -tiny_val\n"}
{"namespace": "math.safe_exp", "completion": "  def _safe_exp(x):\n\n    \"\"\"\n    The function creates a safe exponential function that avoids overflow issues by limiting the input range. It uses a helper function to generate this safe version of the exponential function, applying a custom gradient function for backpropagation in automatic differentious contexts.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function and its custom gradient.\n    :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n\n    def _grad(x_):\n\n      \"\"\"\n      The function creates a custom gradient function for the safe exponential function. It applies a custom gradient to the input x_ to ensure that the output is within a specified range to prevent overflow errors.\n\n      Input-Output Arguments\n      :param x_: The input value for which the custom gradient function is computed. It is used as the argument for the exponential function and its custom gradient.\n      :return: The result of applying the custom gradient function to the input x_. This ensures that the output is within a specified range to prevent overflow errors.\n      \"\"\"\n\n      return tf.where(x_ < 100, tf.exp(x_), tf.ones_like(x_))\n\n    return tf.custom_gradient(_grad)(x)\n\n  return _safe_exp(x)\n"}
{"namespace": "math.safe_log", "completion": "  return jnp.log(x)\n"}
{"namespace": "math.safe_sqrt", "completion": "  def sqrt(x):\n    return tf.sqrt(x)\n\n  def sqrt_grad(x):\n    return 0.5 / tf.sqrt(x)\n\n  def safe_sqrt(x):\n    x = tf.clip_by_value(x, 0, 1e6)\n    return tf.sqrt(x)\n\n  def safe_sqrt_grad(x):\n    x = tf.clip_by_value(x, 0, 1e6)\n    return 0.5 / tf.sqrt(x)\n\n  return tfp.custom_gradient(safe_sqrt)(x)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 1\n  elif p == 1:\n    return 1\n  elif p == 2:\n    return 2\n  elif p == 3:\n    return 6\n  elif p == 4:\n    return 24\n  elif p == 5:\n    return 120\n  elif p == 6:\n    return 720\n  elif p == 7:\n    return 5040\n  elif p == 8:\n    return 40320\n  elif p == 9:\n    return 362880\n  elif p == 10:\n    return 3628800\n  elif p == 11:\n    return 39916800\n  elif p == 12:\n    return 479001600\n  elif p == 13:\n    return 6227020800\n  elif p == 14:\n    return 87178291200\n  elif p == 15:\n    return 1307674368000\n  elif p == 16:\n    return 20922789888000\n  elif p == 17:\n    return 355687428096000\n  elif p == 18:\n    return 6402373705728000\n  elif p == 19:\n    return 121645100408832000\n  elif p == 20:\n    return 2432902008176640000\n  elif p == 21:\n    return 51090942171709440000\n  elif p == 22:\n    return 1124000727777607680000\n  elif p == 23:\n    return 25852016738884976640000\n  elif p == 24:\n    return 620"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n\n  def get_vertices(shape):\n    if shape == \"tetrahedron\":\n      return np.array(\n          [\n              [0, 0, 1],\n              [0, 0, -1],\n              [1, 0, 0],\n              [-1, 0, 0],\n          ]\n      )\n    elif shape == \"icosahedron\":\n      phi = (1 + np.sqrt(5)) / 2\n      return np.array(\n          [\n              [0, 1, phi],\n              [0, 1, -phi],\n              [0, -1, phi],\n              [0, -1, -phi],\n              [1, phi, 0],\n              [1, -phi, 0],\n              [-1, phi, 0],\n              [-1, -phi, 0],\n              [phi, 0, 1],\n              [phi, 0, -1],\n              [-phi, 0, 1],\n              [-phi, 0, -1],\n          ]\n      )\n    elif shape == \"octahedron\":\n      return np.array(\n          [\n              [0, 0, 1],\n              [0, 0, -1],\n              [0, 1, 0],\n              [0, -1, 0],\n              [1, 0, 0],\n              [-1, 0, 0],\n          ]\n      )\n    else:\n      raise ValueError(\"Invalid shape.\")\n\n  def get_faces(shape):\n    if shape == \"tetrahedron\":\n      return np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n    elif shape == \"icosahedron\":\n      return np.array(\n          [\n              [0, 1, 2],\n              [0, 1, 3],\n              [0, 1, 4],\n              [0, 1, 5],\n              [0, 2, 3],\n              [0, 2, 4],\n              [0, 2, 6],\n              [0, 3, 5],\n              [0, 3, 6],\n              [0, 4, 5],\n              [0, 4, "}
{"namespace": "math.safe_log1p", "completion": "  if x < -1:\n    raise ValueError(\"x must be greater than -1\")\n  elif x == -1:\n    return float(\"-inf\")\n  elif x == 0:\n    return 0\n  elif x == float(\"inf\"):\n    return float(\"inf\")\n  else:\n    return np.log1p(x)\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = np.log(x)\n  elif p == -np.inf:\n    y = -np.log(-x)\n  elif p == np.inf:\n    y = np.log(x)\n  else:\n    y = x ** p\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 0:\n    return y\n  elif p == 1:\n    return y\n  elif p == 2:\n    return y * y\n  elif p == 3:\n    return y * y * y\n  elif p == 4:\n    return y * y * y * y\n  elif p == 5:\n    return y * y * y * y * y\n  elif p == 6:\n    return y * y * y * y * y * y\n  elif p == 7:\n    return y * y * y * y * y * y * y\n  elif p == 8:\n    return y * y * y * y * y * y * y * y\n  elif p == 9:\n    return y * y * y * y * y * y * y * y * y\n  elif p == 10:\n    return y * y * y * y * y * y * y * y * y * y\n  elif p == 11:\n    return y * y * y * y * y * y * y * y * y * y * y\n  elif p == 12:\n    return y * y * y * y * y * y * y * y * y * y * y * y\n  elif p == 13:\n    return y * y * y * y * y * y * y * y * y * y * y * y * y\n  elif p == 14:\n    return y * y * y * y * y * y * y * y * y * y * y * y * y * y\n  elif p == 15:\n    return y * y * y * y * y * y * y * y * y * y * y * y * y * y * y\n  elif p == 16:\n    return y * y * y * y * y * y * y * y * y * y * y * y * y * y * y * y\n  elif p == 17:\n    return y * y * y * y * y * y * y * y * y * y * y * y * y * y * y * y * y\n  elif p == 18:\n    return y * y * y * y * y * y * y * y * y * y * y * y * y * y * y *"}
{"namespace": "math.learning_rate_decay", "completion": "  if step < lr_delay_steps:\n    lr = lr_init * lr_delay_mult\n  else:\n    lr = lr_init * (lr_final / lr_init) ** (step / max_steps)\n\n  return lr"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  if camtype != ProjectionType.PERSPECTIVE:\n    raise ValueError(\"Only perspective projection is supported.\")\n\n  if distortion_params is not not None:\n    raise ValueError(\"Only perspective projection is supported.\")\n\n  # Calculate the camera coordinates of the points\n  camcoords = xnp.einsum(\"...ij,...j->...i\", camtoworlds, points)\n\n  # Calculate the pixel coordinates of the points\n  pixcoords = xnp.einsum(\"...ij,...j->...i\", pixtocams, camcoords)\n\n  # Calculate the depth values of the points\n  depth = camcoords[..., 2]\n\n  return pixcoords, depth"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = screw_axis[0]\n  w = screw_axis[1:4]\n  v = screw_axis[4:]\n\n  if theta == 0:\n    return jnp.eye(4)\n\n  w_norm = jnp.linalg.norm(w)\n  w_hat = w / w_norm\n  v_hat = v / w_norm\n\n  R = jnp.eye(3) + jnp.sin(theta) * w_hat[..., None] @ w_hat[..., None].T + (\n      1 - jnp.cos(theta)\n  ) * w_hat[..., None] @ w_hat[..., None].T\n  t = (jnp.eye(3) - R) @ v_hat + theta * w_hat\n\n  return jnp.block([[R, t], [jnp.zeros(3), 1]])\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = np.linalg.norm(axis_angle)\n  if theta < eps:\n    return np.eye(3)\n  else:\n    axis = axis_angle / theta\n    a = np.cos(theta)\n    b = 1 - np.cos(theta)\n    c = np.sin(theta)\n    R = np.array(\n        [\n            [a + axis[0] ** 2 * b, axis[0] * axis[1] * b - axis[2] * c,\n             axis[0] * axis[2] * b + axis[1] * c],\n            [axis[1] * axis[0] * b + axis[2] * c, a + axis[1] ** 2 * b,\n             axis[1] * axis[2] * b - axis[0] * c],\n            [axis[2] * axis[0] * b - axis[1] * c, axis[2] * axis[1] * b + axis[0] * c,\n             a + axis[2] ** 2 * b]\n        ]\n    )\n    return R\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean of the Gaussian distribution\n  mean = (t1 + t0) / 2 * d\n\n  # Calculate the covariance of the Gaussian distribution\n  if diag:\n    # Calculate the diagonal covariance\n    cov = jnp.diag(jnp.array([t1**2 - t0**2, t1**2 - t0**2, t1**2 - t0**2]) / 12)\n  else:\n    # Calculate the full covariance\n    cov = jnp.diag(jnp.array([t1**2 - t0**2, t1**2 - t0**2, t1**2 - t0**2]))\n\n  # Calculate the covariance of the Gaussian distribution\n  if diag:\n    # Calculate the diagonal covariance\n    cov = jnp.diag(jnp.array([t1**2 - t0**2, t1**2 - t0**2, t1**2 - t0**2]) / 12)\n  else:\n    # Calculate the full covariance\n    cov = jnp.diag(jnp.array([t1**2 - t0**2, t1**2 - t0**2, t1**2 - t0**2]))\n\n  # Calculate the covariance of the Gaussian distribution\n  if diag:\n    # Calculate the diagonal covariance\n    cov = jnp.diag(jnp.array([t1**2 - t0**2, t1**2 - t0**2, t1**2 - t0**2]) / 12)\n  else:\n    # Calculate the full covariance\n    cov = jnp.diag(jnp.array([t1**2 - t0**2, t1**2 - t0**2, t1**2 - t0**2]))\n\n  # Calculate the covariance of the Gaussian distribution\n  if diag:\n    # Calculate the diagonal covariance\n    cov = jnp.diag(jnp.array([t1**2 - t0**2, t1**2 - t0**2, t1**2 - t0**2]) / 12)\n  else:\n    # Calculate the full covariance\n    cov = jnp.diag(jnp.array([t1**2 - t0**2, t1**2 - t0**2, t1**2 - t0**2]))"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # Calculate the mean and covariance of the Gaussian distribution\n  mean = jnp.array([0, 0, 0])\n  cov = jnp.array([\n      [radius**2, 0, 0],\n      [0, radius**2, 0],\n      [0, 0, radius**2],\n  ])\n\n  # Calculate the mean and covariance of the Gaussian distribution\n  mean = jnp.array([0, 0, 0])\n  cov = jnp.array([\n      [radius**2, 0, 0],\n      [0, radius**2, 0],\n      [0, 0, radius**2],\n  ])\n\n  # Calculate the mean and covariance of the Gaussian distribution\n  mean = jnp.array([0, 0, 0])\n  cov = jnp.array([\n      [radius**2, 0, 0],\n      [0, radius**2, 0],\n      [0, 0, radius**2],\n  ])\n\n  # Calculate the mean and covariance of the Gaussian distribution\n  mean = jnp.array([0, 0, 0])\n  cov = jnp.array([\n      [radius**2, 0, 0],\n      [0, radius**2, 0],\n      [0, 0, radius**2],\n  ])\n\n  # Calculate the mean and covariance of the Gaussian distribution\n  mean = jnp.array([0, 0, 0])\n  cov = jnp.array([\n      [radius**2, 0, 0],\n      [0, radius**2, 0],\n      [0, 0, radius**2],\n  ])\n\n  # Calculate the mean and covariance of the Gaussian distribution\n  mean = jnp.array([0, 0, 0])\n  cov = jnp.array([\n      [radius**2, 0, 0],\n      [0, radius**2, 0],\n      [0, 0, radius**2],\n  ])\n\n  # Calculate the mean and covariance of the Gaussian distribution\n  mean = jnp.array([0, 0, 0])\n  cov = jnp.array([\n      [radius**2, 0, 0],\n      [0, radius**2, 0],\n      [0, 0, radius**2],\n  ])\n\n  # Calculate"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Check if the input pixel coordinates are valid\n  if pix_x_int is None or pix_y_int is None:\n    raise ValueError(\"Invalid pixel coordinates\")\n\n  # Check if the input pixel coordinates are valid\n  if pix_x_int is None or pix_y_int is None:\n    raise ValueError(\"Invalid pixel coordinates\")\n\n  # Check if the input pixel coordinates are valid\n  if pix_x_int is None or pix_y_int is None:\n    raise ValueError(\"Invalid pixel coordinates\")\n\n  # Check if the input pixel coordinates are valid\n  if pix_x_int is None or pix_y_int is None:\n    raise ValueError(\"Invalid pixel coordinates\")\n\n  # Check if the input pixel coordinates are valid\n  if pix_x_int is None or pix_y_int is None:\n    raise ValueError(\"Invalid pixel coordinates\")\n\n  # Check if the input pixel coordinates are valid\n  if pix_x_int is None or pix_y_int is None:\n    raise ValueError(\"Invalid pixel coordinates\")\n\n  # Check if the input pixel coordinates are valid\n  if pix_x_int is None or pix_y_int is None:\n    raise ValueError(\"Invalid pixel coordinates\")\n\n  # Check if the input pixel coordinates are valid\n  if pix_x_int is None or pix_y_int is None:\n    raise ValueError(\"Invalid pixel coordinates\")\n\n  # Check if the input pixel coordinates are valid\n  if pix_x_int is None or pix_y_int is None:\n    raise ValueError(\"Invalid pixel coordinates\")\n\n  # Check if the input pixel coordinates are valid\n  if pix_x_int is None or pix_y_int is None:\n    raise ValueError(\"Invalid pixel coordinates\")\n\n  # Check if the input pixel coordinates are valid\n  if pix_x_int is None or pix_y_int is None:\n    raise ValueError(\"Invalid pixel coordinates\")\n\n  # Check if the input pixel coordinates are valid\n  if pix_x_int is None or pix_y_int is None:\n    raise ValueError(\"Invalid pixel coordinates\")\n\n  # Check if the input pixel coordinates are valid\n  if pix_x_int is None or pix_y_int is None:\n    raise ValueError(\"Invalid pixel coordinates\")\n\n  # Check if the input pixel coordinates are valid\n  if pix_x_int is None or pix_y_int is None:\n    raise ValueError(\""}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the product of density and the adjusted distance between points\n  prod = density * compute_adjusted_distance(tdist, dirs)\n\n  # Call the helper function to compute the alpha weights\n  return compute_alpha_weights_helper(prod, **kwargs)\n\n"}
{"namespace": "stepfun.sample", "completion": "  # Get the number of bins\n  num_bins = len(t) - 1\n\n  # Get the weights from the logits\n  w = jax.nn.softmax(w_logits)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum(w)\n\n  # Get the cumulative sum of the weights\n  cumsum = jnp.cumsum("}
{"namespace": "stepfun.sample_intervals", "completion": "  if rng is None:\n    return jnp.linspace(domain[0], domain[1], num_samples + 1)\n\n  if single_jitter:\n    jitter = jax.random.uniform(rng, shape=(), minval=0, maxval=1)\n  else:\n    jitter = jax.random.uniform(rng, shape=(num_samples,), minval=0, maxval=1)\n\n  # Compute the cumulative distribution function (CDF) of the step function.\n  cdf = jax.nn.softmax(w_logits)\n  cdf = jnp.cumsum(cdf)\n\n  # Compute the inverse CDF of the step function.\n  inv_cdf = jax.scipy.interpolate.interp1d(cdf, t, kind=\"linear\", fill_value=\"extrapolate\")\n\n  # Sample points from the step function using the inverse CDF.\n  samples = inv_cdf(jitter)\n\n  # Compute the midpoints between adjacent samples.\n  midpoints = (samples[1:] + samples[:-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  intervals = jnp.concatenate([[domain[0]], midpoints, [domain[1]]])\n\n  return intervals"}
{"namespace": "stepfun.weighted_percentile", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Check that the weights sum to 1\n  if np.abs(np.sum(w) - 1) > 1e-5:\n    raise ValueError(\"The weights must sum to 1.\")\n\n  # Compute the cumulative distribution function (CDF)\n  cdf = np.cumsum(w)\n\n  # Interpolate the CDF to find the values corresponding to the percentiles\n  f = interp1d(cdf, t)\n  return f(ps / 100.0)\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Check if the input arrays are valid\n  if not isinstance(tq, np.ndarray):\n    raise ValueError(\"`tq` must be a numpy array.\")\n  if not isinstance(t, np.ndarray):\n    raise ValueError(\"`t` must be a numpy array.\")\n  if not isinstance(w, np.ndarray):\n    raise ValueError(\"`w` must be a numpy array.\")\n\n  # Check if the input arrays have the same length\n  if len(t) != len(w):\n    raise ValueError(\"`t` and `w` must have the same length.\")\n\n  # Check if the input arrays are sorted\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"`t` must be sorted in ascending order.\")\n\n  # Check if the input arrays are sorted\n  if not np.all(np.diff(tq) > 0):\n    raise ValueError(\"`tq` must be sorted in ascending order.\")\n\n  # Check if the input arrays are sorted\n  if not np.all(np.diff(w) > 0):\n    raise ValueError(\"`w` must be sorted in ascending order.\")\n\n  # Check if the input arrays are sorted\n  if not np.all(np.diff(tq) > 0):\n    raise ValueError(\"`tq` must be sorted in ascending order.\")\n\n  # Check if the input arrays are sorted\n  if not np.all(np.diff(w) > 0):\n    raise ValueError(\"`w` must be sorted in ascending order.\")\n\n  # Check if the input arrays are sorted\n  if not np.all(np.diff(tq) > 0):\n    raise ValueError(\"`tq` must be sorted in ascending order.\")\n\n  # Check if the input arrays are sorted\n  if not np.all(np.diff(w) > 0):\n    raise ValueError(\"`w` must be sorted in ascending order.\")\n\n  # Check if the input arrays are sorted\n  if not np.all(np.diff(tq) > 0):\n    raise ValueError(\"`tq` must be sorted in ascending order.\")\n\n  # Check if the input arrays are sorted\n  if not np.all(np.diff(w) > 0):\n    raise ValueError(\"`w` must be sorted in ascending order.\")\n\n  # Check if the input arrays are sorted\n  if not np.all(np.diff("}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Apply the homogeneous transformation to the vectors\n  transformed_vectors = np.dot(transform, np.concatenate((vectors, np.ones((vectors.shape[0], 1))), axis=1).T).T[:, :-1]\n\n  return transformed_vectors"}
{"namespace": "stepfun.resample", "completion": "  import torch\n\n  if use_avg:\n    return torch.sum(vp[1:] * (t[1:] - tp[1:]), dim=0) / (t[1:] - tp[1:])\n  else:\n    return torch.sum(vp[1:] * (t[1:] - tp[1:]), dim=0)"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  scaled_mean = mean * 2 ** (min_deg - 1)\n  scaled_var = var * 2 ** (min_deg - 1)\n\n  # Concatenate the scaled mean and variance\n  concat = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Initialize the encoding\n  encoding = jnp.zeros_like(concat)\n\n  # Loop over the degrees\n  for deg in range(min_deg, max_deg):\n    # Compute the scaling factor\n    scale = 2 ** (deg - 1)\n\n    # Apply the sinusoidal encoding\n    encoding = jnp.concatenate([encoding, jnp.sin(concat * scale), jnp.cos(concat * scale)], axis=-1)\n\n  return encoding"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  import numpy as np\n  from scipy.special import sph_harm\n  from scipy.special import factorial\n  from scipy.special import comb\n\n  def generate_dir_enc_fn_internal(deg_view):\n\n    \"\"\"\n    Generates a directional encoding function based on the specified number of spherical harmonics degrees. This function internally creates another function that evaluates the directional encoding for given inputs.\n\n    Input-Output Arguments\n    :param deg_view: Int. The number of spherical harmonics degrees to use for generating the directional encoding function. It determines the complexity and accuracy of the encoding.\n    :return: Function. A function that takes a 3D point (or points) as input and returns its directional encoding. This returned function internally uses a generated integrated directional encoding function with the specified degree of spherical harmonics.\n\n    \"\"\"\n\n    # Generate the coefficients for the spherical harmonics\n    coeffs = np.zeros((deg_view, deg_view))\n    for l in range(deg_view):\n      for m in range(deg_view):\n        coeffs[l, m] = np.sqrt((2 * l + 1) * factorial(l - m) / (4 * np.pi * factorial(l + m)))\n\n    # Generate the directional encoding function\n    def dir_enc_fn(points):\n\n      \"\"\"\n      Evaluates the directional encoding for given 3D points.\n\n      Input-Output Arguments\n      :param points: Array. A 3D array of shape (N, 3) where N is the number of points. Each row represents a 3D point.\n      :return: Array. A 2D array of shape (N, deg_view * deg_view) where N is the number of points. Each row represents the directional encoding of the corresponding 3D point.\n\n      \"\"\"\n\n      # Convert the points to spherical coordinates\n      theta = np.arctan2(points[:, 1], points[:, 0])\n      phi = np.arccos(points[:, 2])\n\n      # Evaluate the spherical harmonics for each point\n      Y = np.zeros((points.shape[0], deg_view, deg_view))\n      for l in range(deg_view):\n        for m in range(deg_view):\n          Y[:, l, m]"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Remove duplicate lines (ignoring numbers)\n    lines = list(dict.fromkeys(lines))\n\n    # Fix spaced characters\n    lines = [line.replace(' ', '') for line in lines]\n\n    # Connect incomplete lines\n    lines = [line.replace(' ', '') for line in lines]\n\n    # Categorize lines into paragraphs, headers, or list items\n    blocks = []\n    for i, line in enumerate(lines):\n        if line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks[-1]['text'] += ' ' + line.replace(' ', '')\n        elif line.startswith(' '):\n            blocks"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    texts = org_texts.replace('\\n', ' ')\n    texts = texts.replace('\\r', ' ')\n    texts = texts.replace('\\t', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts = texts.replace('  ', ' ')\n    texts"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return [self.positions(token, key) for key in self.keys()]\n        return self.positions(token, key)\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        return int(num_clauses * int(spec[:-1]) / 100)\n    elif spec.endswith(\"<\"):\n        return int(num_clauses * int(spec[:-1]) / 100)\n    else:\n        return int(spec)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_unique_tokens(tokens)\n        else:\n            return self.phrase_freq_general(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not isinstance(array, Iterable):\n            raise TypeError(\"array must be an iterable\")\n\n        if not isinstance(tokenizer, Callable):\n            raise TypeError(\"tokenizer must be a callable\")\n\n        if not isinstance(truncate, bool):\n            raise TypeError(\"truncate must be a boolean\")\n\n        if not isinstance(batch_size, int):\n            raise TypeError(\"batch_size must be an integer\")\n\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"avoid_copies must be a boolean\")\n\n        if batch_size <= 0:\n            raise ValueError(\"batch_size must be greater than 0\")\n\n        if not array:\n            return cls(\n                term_matrix=np.empty((0, 0), dtype=np.int64),\n                positions=np.empty((0, 0), dtype=np.int64),\n                term_dictionary=np.empty((0, 0), dtype=np.int64),\n                average_document_length=0,\n                document_lengths=np.empty(0, dtype=np.int64),\n            )\n\n        if truncate:\n            array = array[:MAX_ARRAY_SIZE]\n\n        term_matrix = []\n        positions = []\n        term_dictionary = {}\n        document_lengths = []\n        total_document_length = 0\n\n        for i, document in enumerate(array):\n            if not isinstance(document, str):\n                raise TypeError(f\"document {i} must be a string\")\n\n            tokens = tokenizer(document)\n            if not tokens:\n                continue\n\n            document_length = len(tokens)\n            document_lengths.append(document_length)\n            total_document_length += document_length\n\n            for j, token in enumerate(tokens):\n                if token not in term_dictionary:\n                    term_dictionary[token] = len(term_dictionary)\n\n                term_matrix.append(term_dictionary[token])\n                positions.append(j)\n\n        term_matrix = np.array(term_matrix, dtype=np.int64)\n        positions = np.array(positions, dtype=np.int64)\n        document_lengths = np.array(document_lengths, dtype=np.int64)\n\n        if avoid_copies:\n            term_matrix = term_matrix.reshape(-1, 1)\n            positions = positions.reshape(-1, 1)\n\n       "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.server.bind((self.host, self.port))\n        self.server.listen(5)\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.logger.info(\"ProxifierMessageInterceptor initialized\")\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = (arr & 0x5555555555555555) + ((arr >> 1) & 0x5555555555555555)\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n    arr = (arr & 0x0F0F0F0F0F0F0F0F) + ((arr >> 4) & 0x0F0F0F0F0F0F0F0F)\n    arr = (arr & 0x00FF00FF00FF00FF) + ((arr >> 8) & 0x00FF00FF00FF00FF)\n    arr = (arr & 0x0000FFFF0000FFFF) + ((arr >> 16) & 0x0000FFFF0000FFFF)\n    arr = (arr & 0x00000000FFFFFFFF) + ((arr >> 32) & 0x00000000FFFFFFFF)\n    return arr\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Create a Lucene index from the DataFrame\n    index = create_index(frame)\n\n    # Create a query parser for the given query fields\n    parser = QueryParser(\"id\", schema=index.schema)\n\n    # Create a query object from the query string\n    query = parser.parse(q)\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n    query_filter = QueryFilter(parser.parse(q))\n\n    # Create a query filter for the given query fields\n   "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.close()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n        if self.server:\n            self.server.stop()"}
