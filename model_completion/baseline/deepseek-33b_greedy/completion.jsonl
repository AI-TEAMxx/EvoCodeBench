{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  z = np.array(z)\n  z_norm = np.linalg.norm(z)\n  z_norm_inv = 1 / z_norm\n  z_inv = z * z_norm_inv\n\n  return z_inv\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import functools\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\")\n            c.execute(\"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\", (func_name, str(args), str(kwargs)))\n            result = c.fetchone()\n            if result:\n                return result[0]\n            else:\n                result = func(*args, **kwargs)\n                c.execute(\"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\", (func_name, str(args), str(kwargs), result))\n                conn.commit()\n                conn.close()\n                return result\n        return wrapper\n    return decorator\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"Invalid bounding box values for {cls.__name__}: x_min must be less than x_max\"\n        )\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"Invalid bounding box values for {cls.__name__}: y_min must be less than y_max\"\n        )\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of each column vector in mat0 and mat1\n  norms0 = np.sum(mat0 ** 2, axis=0)\n  norms1 = np.sum(mat1 ** 2, axis=0)\n\n  # Compute the dot product between each pair of columns in mat0 and mat1\n  dots = mat0.T @ mat1\n\n  # Compute the squared Euclidean distance between each pair of columns\n  sq_dist = norms0[:, np.newaxis] + norms1[np.newaxis, :] - 2 * dots\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"hdfs://\"):\n        return True\n    if path.startswith(\"file://\"):\n        return True\n    return False\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} is a dictionary, so you need to pass the assets_names argument\"\n            )\n        items = np.array([items.get(asset, fill_value) for asset in assets_names])\n    else:\n        items = np.array(items)\n\n    if dim == 1:\n        if items.ndim != 1:\n            raise ValueError(f\"{name} must be a 1-dimensional array\")\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have shape (n_assets,), where n_assets = {n_assets}\"\n            )\n    elif dim == 2:\n        if items.ndim != 2:\n            raise ValueError(f\"{name} must be a 2-dimensional array\")\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"{name} must have shape (n_groups, n_assets), where n_assets = {n_assets}\"\n            )\n    else:\n        raise ValueError(\"dim must be either 1 or 2\")\n\n    return items\n\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent with the required parameters\n        agent = js['class_name'](agent_lifecycle, openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the input dictionary\n        agent.id = data.get('id', None)\n        agent.name = data.get('name', None)\n        agent.description = data.get('description', None)\n        agent.prompt = data.get('prompt', None)\n        agent.temperature = data.get('temperature', None)\n        agent.max_tokens = data.get('max_tokens', None)\n        agent.top_p = data.get('top_p', None)\n        agent.frequency_penalty = data.get('frequency_penalty', None)\n        agent.presence_penalty = data.get('presence_penalty', None)\n        agent.stop = data.get('stop', None)\n        agent.best_of = data.get('best_of', None)\n        agent.n = data.get('n', None)\n        agent.logprobs = data.get('logprobs', None)\n        agent.echo = data.get('echo', None)\n        agent.suffix = data.get('suffix', None)\n        agent.agent_type = data.get('agent_type', None)\n        agent.agent_type_id = data.get('agent_type_id', None)\n        agent.agent_type_name = data.get('agent_type_name', None)\n        agent.agent_type_description = data.get('agent_type_description', None)\n        agent.agent_type_prompt = data.get('agent_type_prompt', None)\n        agent.agent_type_temperature = data.get('agent_type_temperature', None)\n        agent.agent_type_max_tokens = data.get('agent_type_max_tokens', None)\n        agent.agent_type_top_p = data.get('agent_type_top_p', None)\n        agent.agent_type_frequency_penalty ="}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  srgb = xnp.clip(srgb, 0, 1)\n\n  linear = xnp.where(\n    srgb <= 0.0404482362771082,\n    srgb / 12.92,\n    xnp.power((srgb + 0.055) / 1.055, 2.4)\n  )\n\n  return linear\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import UnivariateSpline\n\n  # Adjust the spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Create a spline object using the input signal and input times\n  spline = UnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times\n  x_output = spline(t_output)\n\n  return x_output\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0].isupper() and word[1].isupper():\n            return word.upper()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        else:\n            return word.lower()\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__} {field.name} must be binary (bool), not {v.dtype}\"\n        )\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Apply the isotropic scaling operation\n  scaled_x = x / norm\n\n  return scaled_x\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file into a pandas DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # If dict_columns is not provided, default to ['module_params']\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Convert specified columns that contain dictionary-like strings into actual dictionary objects\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the covariance matrix or matrices\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is valid\n  if det <= 0:\n    raise ValueError(\"Invalid determinant\")\n\n  # Compute the logarithm of the determinant\n  logdet = np.log(det)\n\n  # Compute the isotropic covariance matrix or matrices\n  if mode == 'fast':\n    # Use the determinant directly\n    cov_iso = cov / det\n  elif mode == 'accurate':\n    # Use the logarithm of the determinant for stability\n    cov_iso = cov / np.exp(logdet / cov.shape[0])\n  else:\n    raise ValueError(\"Invalid mode\")\n\n  # Return the isotropic covariance matrix or matrices\n  return cov_iso\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Command line arguments for the program.\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts.\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file.\")\n\n    return parser.parse_args()\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{field.name} must be a list of 2D points\")\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    char_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n    if n < 0 or n >= len(char_set):\n        raise ValueError(\"Input integer out of range\")\n    return char_set[n]\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, eps)) + jnp.log(value_at_zero) - jnp.log(eps)\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_id, intervals in workers_intervals.items():\n        chunk_indexes[worker_id] = indexes[worker_id]\n        for interval in intervals:\n            if len(interval) > 1:\n                size = interval[1] - interval[0]\n                if size > 0:\n                    indexes[worker_id] += size\n                    if indexes[worker_id] >= len(interval):\n                        indexes[worker_id] = 0\n                        chunk_indexes[worker_id] += 1\n    return chunk_indexes, indexes\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f\"Invalid datastructure: {datastructure}. Only 'grid' and 'hash' are supported.\")\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the tessellation factor is valid\n  if v < 1:\n    raise ValueError(\"Tessellation factor must be greater than or equal to 1.\")\n\n  # Initialize an empty list to store the weights\n  weights = []\n\n  # Iterate over the rows of the triangle\n  for i in range(v + 1):\n    # Iterate over the columns of the triangle\n    for j in range(v + 1 - i):\n      # Compute the weight for the current point\n      weight = (1 - i / v) * (1 - j / v)\n      # Append the weight to the list of weights\n      weights.append(weight)\n\n  # Convert the list of weights to a numpy array\n  weights = np.array(weights)\n\n  # Normalize the weights to get the barycentric coordinates\n  weights /= np.sum(weights)\n\n  return weights"}
{"namespace": "linspline.query", "completion": "  import numpy as np\n\n  # Check that the input arrays have the same length\n  if len(t) != len(v):\n    raise ValueError(\"The input arrays 't' and 'v' must have the same length.\")\n\n  # Check that the time points are in ascending order\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"The time points in 't' must be in ascending order.\")\n\n  # Check that the query points are within the range of the time points\n  if np.min(tq) < np.min(t) or np.max(tq) > np.max(t):\n    raise ValueError(\"The query points must be within the range of the time points in 't'.\")\n\n  # Interpolate the spline at the query points\n  vq = np.interp(tq, t, v, left=0, right=0)\n\n  return vq\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(x, (int, float)) and x > 0 for x in v):\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    elif not isinstance(v, (int, float)) or v <= 0:\n        raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = origins + directions * near\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[..., :1])], axis = -1)\n  origins = xnp.matmul(origins, pixtocam.T)\n  origins = origins[..., :2] / origins[..., 3:]\n  directions = xnp.matmul(directions, pixtocam.T)\n  directions = directions[..., :2]\n  return origins, directions\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 or -1, within a small epsilon\n  return np.isclose(dot_product, 1, atol=1e-8) or np.isclose(dot_product, -1, atol=1e-8)\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts using the custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the BLEU score using the nltk library\n    bleu_score = nltk.translate.bleu_score.sentence_bleu([reference_tokens], continuation_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n\n    # If the brevity penalty is included, adjust the BLEU score accordingly\n    if with_penalty:\n        reference_length = len(reference_tokens)\n        continuation_length = len(continuation_tokens)\n        if continuation_length < reference_length:\n            penalty = 1\n        else:\n            penalty = np.exp(1 - reference_length / continuation_length)\n        bleu_score = bleu_score * penalty\n\n    return bleu_score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, eps, x)\n  return jnp.sqrt(safe_x)\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  import numpy as np\n\n  # Calculate the difference between consecutive elements in the input vector t\n  diff = np.diff(t)\n\n  # Divide the weights by the difference between consecutive elements in t\n  pdf = w[:-1] / diff\n\n  # Append the last element of the input vector w to the resulting PDF\n  pdf = np.append(pdf, w[-1])\n\n  return pdf\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items and weights are equal\n    assert len(items) == len(weights)\n\n    # Check that all weights are positive\n    assert all(w >= 0 for w in weights)\n\n    # Check that the number of bins is positive\n    assert num_bins > 0\n\n    # Create a list of tuples containing the items and their weights, sorted in descending order by weight\n    items_and_weights = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize a dictionary to store the items in each bin\n    bins = {i: [] for i in range(num_bins)}\n\n    # Initialize a dictionary to store the total weight of each bin\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Iterate over the items and weights\n    for item, weight in items_and_weights:\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item and its weight to the bin\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = func_name + str(args) + str(kwargs)\n        return hashlib.sha256(data.encode()).hexdigest()\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the Euclidean distance between consecutive points in the polygon\n    distances = np.sqrt(np.sum(np.diff(polygon, axis=0) ** 2, axis=1))\n\n    # Filter out distances that exceed the maximum point distance\n    filtered_distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon\n    total_length = np.sum(filtered_distances)\n\n    return total_length\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return []\n\n    # Calculate the area of each polygon\n    areas = [cv2.contourArea(p) for p in polygons]\n\n    # Find the largest polygon's area\n    max_area = max(areas)\n\n    # Filter out polygons based on the area criteria\n    filtered_polygons = [\n        p for p, area in zip(polygons, areas) if area >= max_area * rel_tr and area >= abs_tr\n    ]\n\n    return filtered_polygons\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of remaining samples\n    num_remaining_samples = num_samples_yielded % num_workers\n\n    # Initialize a dictionary to store the number of samples each worker has processed\n    num_samples_per_worker_dict = {}\n\n    # Distribute the remaining samples among the workers\n    for i in range(num_workers):\n        num_samples_per_worker_dict[i] = num_samples_per_worker\n        if i < num_remaining_samples:\n            num_samples_per_worker_dict[i] += 1\n\n    # Calculate the number of batches each worker has processed\n    num_batches_per_worker_dict = {worker: num_samples // batch_size for worker, num_samples in num_samples_per_worker_dict.items()}\n\n    # Calculate the number of samples each worker has processed after accounting for the batch size\n    num_samples_per_worker_dict = {worker: num_batches * batch_size for worker, num_batches in num_batches_per_worker_dict.items()}\n\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    # Implementation of Shoelace formula\n    S1 = np.dot(x, np.roll(y, -1))\n    S2 = np.dot(y, np.roll(x, -1))\n    area = 0.5 * np.abs(S1 - S2)\n\n    return area\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if a and v are tensors\n    if not isinstance(a, torch.Tensor) or not isinstance(v, torch.Tensor):\n        raise TypeError(\"Both a and v must be torch.Tensor objects.\")\n\n    # Check if a and v are on the same device\n    if a.device != v.device:\n        raise ValueError(\"Both a and v must be on the same device.\")\n\n    # Check if a and v have the same dtype\n    if a.dtype != v.dtype:\n        raise ValueError(\"Both a and v must have the same dtype.\")\n\n    # Check if a and v have the same shape except for the last dimension\n    if a.shape[:-1] != v.shape[:-1]:\n        raise ValueError(\"Both a and v must have the same shape except for the last dimension.\")\n\n    # Check if a is sorted in ascending order\n    if not torch.all(a[..., :-1] <= a[..., 1:]):\n        raise ValueError(\"a must be sorted in ascending order.\")\n\n    # Check if v is within the range of a\n    if not torch.all(a[..., 0] <= v) or not torch.all(v <= a[..., -1]):\n        raise ValueError(\"All elements in v must be within the range of a.\")\n\n    # Check if a and v have the same device\n    if a.device != v.device:\n        raise ValueError(\"Both a and v must be on the same device.\")\n\n    # Check if a and v have the same dtype\n    if a.dtype != v.dtype:\n        raise ValueError(\"Both a and v must have the same dtype.\")\n\n    # Check if a and v have the same shape except for the last dimension\n    if a.shape[:-1] != v.shape[:-1]:\n        raise ValueError(\"Both a and v must have the same shape except for the last dimension.\")\n\n    # Check if a is sorted in ascending order\n    if not torch.all(a[..., :-1"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n"}
{"namespace": "coord.contract", "completion": "  x_sq = x**2\n  x_sq_sum = np.sum(x_sq, axis=1, keepdims=True)\n  x_sq_sum_sqrt = np.sqrt(x_sq_sum)\n  x_sq_sum_sqrt_inv = 1 / (1 + x_sq_sum_sqrt)\n  x_sq_sum_sqrt_inv_tiled = np.tile(x_sq_sum_sqrt_inv, (1, x.shape[1]))\n  x_contracted = x * x_sq_sum_sqrt_inv_tiled\n\n  return x_contracted\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    unit_index = 0\n    while num_bytes >= 1000 and unit_index < len(units) - 1:\n        num_bytes /= 1000\n        unit_index += 1\n    return f\"{num_bytes:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_ndim(cls, v, field):\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be {nb_dimensions}D array. \"\n                f\"Got {v.ndim}D array instead.\"\n            )\n        return v\n\n    return validate_array_ndim\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = onp.sqrt(x**2 + y**2 + z**2)\n  theta = onp.arccos(z / (r + eps))\n  phi = onp.arctan2(y, x)\n  return r, theta, phi\n"}
{"namespace": "common.rougeL_score", "completion": "    # Importing the necessary libraries\n    import jieba\n    import numpy as np\n\n    # Defining the tokenizer function\n    def tokenize(text):\n        return list(jieba.cut(text))\n\n    # Tokenizing the continuation and reference texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Computing the length of the continuation and reference texts\n    continuation_length = len(continuation_tokens)\n    reference_length = len(reference_tokens)\n\n    # Computing the length of the longest common subsequence between the continuation and reference texts\n    lcs_length = len(set(continuation_tokens).intersection(set(reference_tokens)))\n\n    # Computing the ROUGE-L score\n    rouge_l_score = lcs_length / reference_length\n\n    # Returning the ROUGE-L score\n    return rouge_l_score\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_standard(name)\n    except Exception as e:\n        return locate_fallback(name)\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the lengths of ids and scores match\n    if len(ids) != len(scores):\n        raise ValueError(\"The lengths of ids and scores must match.\")\n\n    # Check if the sum of weights equals 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of weights must equal 1.\")\n\n    # Check if the lengths of ids, scores, and weights match\n    if len(ids) != len(weights):\n        raise ValueError(\"The lengths of ids, scores, and weights must match.\")\n\n    # Normalize the scores\n    normalized_scores = []\n    for i in range(len(scores)):\n        max_score = max(scores[i])\n        min_score = min(scores[i])\n        normalized_scores.append([(score - min_score) / (max_score - min_score) for score in scores[i]])\n\n    # Combine the scores using the convex combination method\n    combined_scores = [sum([normalized_scores[j][i] * weights[j] for j in range(len(scores))]) for i in range(len(scores[0]))]\n\n    # Select the top k results\n    top_indices = sorted(range(len(combined_scores)), key=lambda i: combined_scores[i], reverse=True)[:top_k]\n    top_ids = [ids[i][j] for i in range(len(ids)) for j in top_indices]\n    top_scores = [combined_scores[i] for i in top_indices]\n\n    return top_ids, top_scores\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n    elif percent:\n        return f\"{x*100:.2f}%\"\n    elif x < 0.01:\n        return f\"{x:.2e}\"\n    elif x < 1:\n        return f\"{x:.2f}\"\n    elif x < 100:\n        return f\"{x:.1f}\"\n    else:\n        return f\"{x:.0f}\"\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import shutil\n    import time\n\n    while True:\n        total, used, free = shutil.disk_usage(input_dir)\n        free_in_gb = free // (2**30)\n        if free_in_gb < threshold_in_gb:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in 't'\n  dt = np.diff(t)\n\n  # Multiply the PDF values by the differences to obtain the weights\n  w = p[:-1] * dt\n\n  # Normalize the weights so they sum to 1\n  w /= np.sum(w)\n\n  return w\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    line_text = list(line_text)\n\n    return line_text\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check that the number of zero-weight elements does not exceed the total number of weights\n    if zeros > n:\n        raise ValueError(\"The number of zero-weight elements must not exceed the total number of weights.\")\n\n    # Generate an array of random weights that sum to one\n    weights = np.random.dirichlet(np.ones(n), size=1)[0]\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(np.arange(n), size=zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, **module_dict)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Extract the bounding box coordinates from the instance annotation\n    bbox = instance[\"bbox\"]\n\n    # Calculate the center of the bounding box\n    center_x = (bbox[0] + bbox[2]) / 2\n    center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the top-left corner of the crop region\n    crop_x = max(0, center_x - crop_size[1] / 2)\n    crop_y = max(0, center_y - crop_size[0] / 2)\n\n    # Adjust the crop region if it exceeds the image boundaries\n    if crop_x + crop_size[1] > image_size[1]:\n        crop_x = image_size[1] - crop_size[1]\n    if crop_y + crop_size[0] > image_size[0]:\n        crop_y = image_size[0] - crop_size[0]\n\n    # Create a CropTransform object with the calculated parameters\n    crop_transform = CropTransform(crop_x, crop_y, crop_size[1], crop_size[0])\n\n    return crop_transform\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of x\n  sq_norm = jnp.sum(x ** 2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value to prevent exploding gradients in the backward pass\n  sq_norm = jnp.maximum(sq_norm, grad_eps)\n\n  # Compute the normalization factor and normalize x\n  x = x / jnp.sqrt(sq_norm)\n\n  return x\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name, agent_input = agent_info.split(':')\n        return agent_name, agent_input\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 3, f\"Expects a 3-dimensional mask, got {segm.ndim}.\"\n                    # mask array\n                    masks.append(segm)\n                else:\n                    raise ValueError(\n                        \"Cannot process segmentation of type '{}'\".format(type(segm))\n                    )\n            # take the first actually\n            masks = BitMasks(\n                torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n            )\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", join(\"~\", \"skfolio_data\"))\n    data_home = str(Path(data_home).expanduser().resolve())\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    cov = np.atleast_2d(cov)\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    # Return the correlation matrix and standard deviations\n    return corr, std\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    class FreezeTrainingMode(object):\n        def __init__(self, model):\n            self.model = model\n\n        def __enter__(self):\n            for module in self.model.modules():\n                module.__class__.training = False\n\n        def __exit__(self, exc_type, exc_val, exc_tb):\n            for module in self.model.modules():\n                module.__class__.training = True\n\n    return FreezeTrainingMode(model)\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validator(cls, v, values, **kwargs):\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(f\"{field1} and {field2} must have the same shape\")\n        return v\n\n    return validator\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(metric[\"name\"])\n            metric_params.append(metric.get(\"params\", {}))\n        else:\n            raise ValueError(f\"Invalid metric format: {metric}\")\n\n    return metric_names, metric_params\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    t_clip = torch.clamp(t, t_near, t_far)\n    s = (t_clip - t_near) / (t_far - t_near)\n    s = fn(s)\n    return s\n\n  def s_to_t(s):\n    t = fn_inv(s)\n    t = t * (t_far - t_near) + t_near\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) != len(w):\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  # Initialize the integral to zero\n  integral = 0\n\n  # Loop through the data points and compute the integral using the trapezoid rule\n  for i in range(1, len(t)):\n    integral += (w[i] + w[i-1]) * (t[i] - t[i-1]) / 2\n\n  return integral\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check if the lengths of the input tuples are equal\n    if len(ids) != len(scores) or len(ids) != len(weights):\n        raise ValueError(\"The lengths of 'ids', 'scores', and 'weights' must be equal.\")\n\n    # Check if the lengths of the ID lists within the 'ids' tuple are equal\n    if len(set(len(id_list) for id_list in ids)) != 1:\n        raise ValueError(\"The lengths of the ID lists within 'ids' must be equal.\")\n\n    # Check if the lengths of the score lists within the 'scores' tuple are equal\n    if len(set(len(score_list) for score_list in scores)) != 1:\n        raise ValueError(\"The lengths of the score lists within 'scores' must be equal.\")\n\n    # Check if the lengths of the ID lists within the 'ids' tuple and the score lists within the 'scores' tuple are equal\n    if len(ids[0]) != len(scores[0]):\n        raise ValueError(\"The lengths of the ID lists within 'ids' and the score lists within 'scores' must be equal.\")\n\n    # Check if the top_k value is a positive integer\n    if not isinstance(top_k, int) or top_k <= 0:\n        raise ValueError(\"The 'top_k' value must be a positive integer.\")\n\n    # Check if the weights are non-negative\n    if any(weight < 0 for weight in weights):\n        raise ValueError(\"The weights must be non-negative.\")\n\n    # Check if the weights sum up to 1\n    if sum(weights) != 1:\n        raise ValueError(\"The weights must sum up to 1.\")\n\n    # Initialize a dictionary to store the weighted sum of scores for each ID\n    weighted_sums = {}\n\n    # Iterate over the IDs in the first ID list\n    for i, id_ in enumerate(ids[0]):\n        # Initialize the weighted sum for the current ID\n        weighted_sum = 0\n\n        # Iterate over the categories or groups\n        for"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of the function at the mean\n  jacobian = tf.vectorized_map(\n      fn=lambda x: tf.vectorized_map(\n          fn=lambda y: tf.gradients(ys=fn(x), xs=y)[0],\n          elems=mean),\n      elems=mean)\n\n  # Compute the transformed means\n  fn_mean = fn(mean)\n\n  # Compute the transformed covariances\n  fn_cov = tf.matmul(jacobian, cov)\n  fn_cov = tf.matmul(fn_cov, tf.linalg.matrix_transpose(jacobian))\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[:i.size//2], i[i.size//2:]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Generate the scales as powers of 2 from min_deg to max_deg\n  scales = 2 ** np.linspace(min_deg, max_deg, max_deg - min_deg + 1)\n\n  # Scale the input array\n  xb = x[..., np.newaxis] * scales[:, np.newaxis]\n\n  # Apply sine function to the scaled input\n  four_feat = np.sin(np.concatenate([xb, xb + 0.5 * np.pi], axis=-1))\n\n  # Concatenate the original input with the encoded features if append_identity is True\n  if append_identity:\n    return np.concatenate([x, four_feat], axis=-1)\n  else:\n    return four_feat\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate(cls, values):\n        list1 = values.get(field1)\n        list2 = values.get(field2)\n\n        if len(list1) != len(list2):\n            raise ValueError(f\"{field1} and {field2} must have the same length\")\n\n        for arr1, arr2 in zip(list1, list2):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{field1} and {field2} must have the same shape\")\n\n        return values\n\n    return validate"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(camera)\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        num_labels=bert_config.num_labels,\n        output_attentions=bert_config.output_attentions,\n        output_hidden_states=bert_config.output_hidden_states,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        is_decoder=bert_config.is_decoder,\n        add_cross_attention=bert_config.add_cross_attention,\n        pruned_heads=bert_config.pruned_heads,\n        use_auth_token=bert_config.use_auth_token,\n        use_return_dict=bert_config.use_return_dict,\n        use_mems_eval=bert_config.use_mems_eval,\n        use_mems_train=bert_config.use_mems_train,"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.programs['points'].use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n            glBindVertexArray(0)\n        else:\n            self.programs['mesh'].use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            if self.render_type == RenderType.LINES:\n                if self.ebo is not None:\n                    glDrawElements(GL_LINES, len(self.faces) * 2, GL_UNSIGNED_INT, None)\n                else:\n                    glDrawArrays(GL_LINES, 0, len(self.vertices))\n            elif self.render_type == RenderType.TRIANGLES:\n                if self.ebo is not None:\n                    glDrawElements(GL_TRIANGLES, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n                else:\n                    glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n            elif self.render_type == RenderType.QUADS:\n                if self.ebo is not None:\n                    glDrawElements(GL_QUADS, len(self.faces) * 4, GL_UNSIGNED_INT, None)\n                else:\n                    glDrawArrays(GL_QUADS, 0, len(self.vertices))\n            elif self.render_type == RenderType.TRIANGLE_STRIP:\n                if self.ebo is not None:\n                    glDrawElements(GL_TRIANGLE_STRIP, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n                else:\n                    glDrawArrays(GL_TRIANGLE_STRIP, 0, len("}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_FLOAT, ptr)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Check if the input tensors are batched\n    assert R.ndim == 3 and tvec.ndim == 2 and camera_matrix.ndim == 3 and image_size.ndim == 2\n\n    # Check if the input tensors have the correct shapes\n    assert R.shape[1:] == (3, 3) and tvec.shape[1] == 3 and camera_matrix.shape[1:] == (3, 3) and image_size.shape[0] == 2\n\n    # Check if the input tensors have valid values\n    assert torch.all(torch.isfinite(R)) and torch.all(torch.isfinite(tvec)) and torch.all(torch.isfinite(camera_matrix)) and torch.all(torch.isfinite(image_size))\n\n    # Compute the camera position\n    camera_position = -torch.bmm(R.transpose(1, 2), tvec.unsqueeze(2)).squeeze(2)\n\n    # Compute the camera rotation\n    camera_rotation = R.transpose(1, 2)\n\n    # Compute the focal length and principal point offsets\n    fx = camera_matrix[:, 0, 0]\n    fy = camera_matrix[:, 1, 1]\n    cx = camera_matrix[:, 0, 2]\n    cy = camera_matrix[:, 1, 2]\n    principal_point_offset = torch.stack([cx - image_size[0] / 2, cy - image_size[1] / 2], dim=1)\n\n    # Compute the sensor width\n    sensor_width = 2 * torch.sqrt(torch.abs(fx * fy))\n\n    # Adjust the focal length and principal point offsets based on the image size\n    focal_length = torch.sqrt(fx * fy) / (image_size[0] / 2)\n    principal_point_offset = principal_point_offset / (image_size[0] / 2)\n\n    # Compute the"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n\n        self.quad_program.use()\n        self.tex.bind()\n\n        self.vao.bind()\n        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n        self.vao.unbind()\n\n        self.quad_program.unuse()\n\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n    K = batch.K\n\n    # Adjust R\n    R = R.transpose(1, 2)  # Transpose R to match PyTorch3D's convention\n    R = R @ torch.tensor([[1, 0, 0], [0, -1, 0], [0, 0, -1]], device=R.device)  # Apply correction to R\n\n    # Adjust T\n    T = T @ torch.tensor([[1, 0, 0], [0, -1, 0], [0, 0, -1]], device=T.device)  # Apply correction to T\n\n    # Compute K for NDC\n    K = K.clone()\n    K[:, 0, 0] = K[:, 0, 0] / W\n    K[:, 1, 1] = K[:, 1, 1] / H\n    K[:, 0, 2] = 2 * K[:, 0, 2] / W - 1\n    K[:, 1, 2] = 2 * K[:, 1, 2] / H - 1\n\n    # Compute camera center in camera's coordinate system\n    C = -R.transpose(1, 2) @ T\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        with self.FBO.bind_to_read():\n            gl.glBlitFramebuffer(x, y, w, h, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Calculate the cumulative sum of y1\n    y1_cumsum = tf.math.cumsum(y1, axis=-1)\n\n    # Find the indices of t1 that are less than or equal to t0\n    indices = tf.searchsorted(t1, t0, side='right')\n\n    # Calculate the inner measure\n    y0_inner = tf.gather(y1_cumsum, indices - 1, axis=-1)\n\n    # Calculate the outer measure\n    y0_outer = tf.gather(y1_cumsum, indices, axis=-1)\n\n    return y0_inner, y0_outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the upper envelope weights\n    w_upper = torch.cat([w_env, torch.zeros_like(w_env[..., :1])], -1)\n    w_upper = torch.max(w_upper, -1)[0]\n\n    # Calculate the difference between target weights and the upper envelope\n    w_diff = w - w_upper\n\n    # Calculate the half-quadratic loss\n    loss = w_diff * w_diff / 4.0\n\n    # Scale the loss by the difference between target positions and the upper envelope\n    loss = loss * torch.exp(-1e-3 * t_env)\n\n    # Return the loss\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check that the input tensors have the same shape\n    assert t.shape == w.shape, \"t and w must have the same shape\"\n\n    # Check that the weights sum to 1\n    assert torch.allclose(w.sum(dim=-1), torch.tensor(1.)), \"weights must sum to 1\"\n\n    # Sort the values in t and the corresponding weights in w\n    t, w = t.sort(dim=-1)\n\n    # Compute the cumulative sum of the weights\n    w_cumsum = w.cumsum(dim=-1)\n\n    # Compute the percentiles of the cumulative sum of the weights\n    ps_w_cumsum = torch.tensor(ps)\n    ps_w_cumsum = ps_w_cumsum.view(-1, 1, 1)\n    ps_w_cumsum = ps_w_cumsum.repeat(1, *w_cumsum.shape)\n    ps_w_cumsum = torch.where(ps_w_cumsum <= w_cumsum, ps_w_cumsum, torch.tensor(np.nan))\n    ps_w_cumsum = torch.nanmax(ps_w_cumsum, dim=-1)\n\n    # Interpolate the values in t to find the weighted percentiles\n    t_interp = torch.zeros_like(ps_w_cumsum)\n    for i in range(t.shape[-1]):\n        t_interp += (t[..., i].unsqueeze(-1) * (w_cumsum[..., i].unsqueeze(-1) - ps_w_cumsum)) / (w_cumsum[..., i].unsqueeze(-1) - ps_w_cumsum + torch.finfo(w_cumsum.dtype).eps)\n\n    return t_interp\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Get the PDF\n    pdf = weight_to_pdf(w, t)\n\n    # Get the CDF\n    c = torch.cumsum(w, -1)\n    c = torch.cat([torch.zeros_like(c[..., :1]), c], -1)\n    c = c / c[..., -1:]\n\n    # Take uniform samples\n    u = torch.rand(list(cdf.shape[:-1]) + [num_samples])\n\n    # Invert CDF. This is a piecewise constant function on the domain of the CDF.\n    # PyTorch, and by extension JAX, don't have an inverse function.\n    # However, we can make a linear approximation by simply solving for the nearest bins.\n    # Note that this is only accurate in the limit of a large number of samples.\n    below = torch.max(torch.where(u <= c, u, torch.zeros_like(u)), -1)[0]\n    above = torch.min(torch.where(u > c, u, 1e9 * torch.ones_like(u)), -1)[0]\n    ix_below = torch.searchsorted(c, below, side='right') - 1\n    ix_above = torch.searchsorted(c, above, side='right')\n\n    # cdf lo hi     u\n    # |     |       |\n    # |-----|-------|\n    #       |--|\n    #      lo hi\n    #\n    # Handle the case where u is outside of the bounds of the CDF.\n    lo = torch.max(torch.zeros_like(ix_below), ix_below)\n    hi = torch.min(torch.full_like(ix_above, c.shape[-1] - 1), ix_above)\n\n    valid = (u > c[..., lo]) & (u < c[..., hi])\n\n    # In the case where the sample is outside the bounds of the CDF, we can simply clamp it to the bounds of the nearest"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps\n    t_dilated = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    t_dilated = torch.clamp(t_dilated, min=domain[0], max=domain[1])\n\n    # Adjust the weights to match the dilated time steps\n    w_dilated = w / dilation\n\n    return t_dilated, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check that the input tensors have the same number of elements\n    assert t.shape == y.shape, \"t and y must have the same shape\"\n\n    # Check that the query times are within the range of the step function\n    assert tq.min() >= t.min(), \"tq must be greater than or equal to the minimum value in t\"\n    assert tq.max() <= t.max(), \"tq must be less than or equal to the maximum value in t\"\n\n    # Find the index of the first time that is greater than or equal to each query time\n    idx = torch.searchsorted(t, tq)\n\n    # If the query time exactly matches a step change time, return the outside value\n    mask = (tq == t[idx])\n    yq = torch.where(mask, outside_value, y[idx])\n\n    # If the query time is between two step change times, interpolate the value\n    mask = (tq > t[idx - 1]) & (tq < t[idx])\n    yq = torch.where(mask, (y[idx] - y[idx - 1]) / (t[idx] - t[idx - 1]) * (tq - t[idx - 1]) + y[idx - 1], yq)\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t."}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n    elif isinstance(batch, (tuple, list)):\n        return [to_cuda(v, device, ignore_list) for v in batch]\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor\n    if f.ndim > v.ndim:\n        f = f[..., 0]\n    elif f.ndim < v.ndim:\n        f = f.unsqueeze(-2)\n\n    # gather the vertices using the faces tensor\n    tri = v.gather(dim, f.expand(f.shape[0], *v.shape))\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    return tri.reshape(*f.shape, *v.shape[1:])\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise ValueError(f\"Unsupported data type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        for k, v in self.__dict__.items():\n            if isinstance(v, GUI):\n                batch[k] = v.to_batch()\n            elif isinstance(v, torch.Tensor):\n                batch[k] = v\n            elif isinstance(v, np.ndarray):\n                batch[k] = torch.from_numpy(v).float()\n            elif isinstance(v, list):\n                batch[k] = []\n                for v_ in v:\n                    if isinstance(v_, GUI):\n                        batch[k].append(v_.to_batch())\n                    elif isinstance(v_, torch.Tensor):\n                        batch[k].append(v_)\n                    elif isinstance(v_, np.ndarray):\n                        batch[k].append(torch.from_numpy(v_).float())\n                    elif isinstance(v_, float):\n                        batch[k].append(torch.tensor(v_).float())\n                    else:\n                        batch[k].append(v_)\n            elif isinstance(v, float):\n                batch[k] = torch.tensor(v).float()\n            else:\n                batch[k] = v\n        batch.meta = dotdict()\n        for k, v in self.__dict__.items():\n            if isinstance(v, GUI):\n                batch.meta[k] = v.to_batch()\n            elif isinstance(v, torch.Tensor):\n                batch.meta[k] = v\n            elif isinstance(v, np.ndarray):\n                batch.meta[k] = torch.from_numpy(v).float()\n            elif isinstance(v, list):\n                batch.meta[k] = []\n                for v_ in v:\n                    if isinstance(v_, GUI):\n                        batch.meta[k].append(v_.to_batch())\n                    elif isinstance(v_, torch.Tensor):\n                        batch.meta[k].append(v_)\n                    elif isinstance(v_, np.ndarray):\n                        batch.meta[k].append(tor"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            agent_state_dict = agent_state.to_dict()\n            self.persistence_manager.save(agent_state_dict)\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Initialize variables to keep track of the closest agent and its similarity score\n        closest_agent = None\n        highest_similarity_score = -float('inf')\n\n        # Iterate over all agents in the agents dictionary\n        for agent in self.agents.values():\n            # Calculate the cosine similarity between the purpose embedding of the agent and the given purpose embedding\n            similarity_score = cosine_similarity(agent.purpose_embedding, purpose_embedding)\n\n            # If the similarity score is higher than the current highest similarity score, update the closest agent and the highest similarity score\n            if similarity_score > highest_similarity_score:\n                closest_agent = agent\n                highest_similarity_score = similarity_score\n\n        # Return the closest agent and its similarity score\n        return closest_agent, highest_similarity_score\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a prime agent. You are a helpful and knowledgeable agent. You are a good listener and a good speaker. You are a good communicator. You are a good problem solver. You are a good teacher. You are a good friend. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader. You are a good mentor. You are a good leader."}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        def load_agent(self, purpose: str, agent_lifecycle: AgentLifecycle, openai_wrapper: OpenAIWrapper) -> Optional[js['class_name']]:\n            agent_data = self.agent_repository.find_by_purpose(purpose)\n            if agent_data:\n                return self.agent_deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n            return None\n\n\n        \"\"\"\n        Saves an agent with a specified purpose to the database. If an agent with the given purpose already exists, it is updated; otherwise, a new agent is created.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent: An instance of the agent to be saved.\n        :param purpose: str, The purpose of the agent to be saved. It is used to identify the agent in the database.\n        :return: None\n        \"\"\"\n        def save_agent(self, agent: js['class_name'], purpose: str) -> None:\n            agent_data = self.agent_serializer.serialize(agent)\n            existing_agent_data = self.agent_repository.find_by_purpose(purpose)\n            if existing_agent_data:\n                self.agent_repository.update(existing_agent_data.id, agent_data)\n            else:\n                self.agent_repository.create(agent_data)\n\n\n        \"\"\"\n        Deletes an agent with a specified purpose from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be deleted. It is used to identify the agent in the database.\n        :return: None\n        \"\"\"\n        def delete_agent(self, purpose: str) -> None:\n            agent_data = self.agent_repository.find_by_purpose(purpose)\n            if agent_data:\n                self.agent_repository.delete(agent_data.id)\n\n\n        \"\"\"\n        Deletes all"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        agents = []\n        for agent_data in self.db.get_all_agents():\n            agent = self.load_agent(agent_data, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    \"\"\"\n    Loads an agent from the database based on its ID and returns the loaded agent if it is successfully loaded. The agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_id: str. The ID of the agent to be loaded.\n    :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n    :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n    :return: Agent. The loaded agent if it is successfully loaded, or None if the agent cannot be loaded.\n    \"\"\"\n\n    def load_agent_by_id(self, agent_id, agent_lifecycle, openai_wrapper):\n        agent_data = self.db.get_agent_by_id(agent_id)\n        if not agent_data:\n            return None\n        return self.load_agent(agent_data, agent_lifecycle, openai_wrapper)\n\n    \"\"\"\n    Loads an agent from the database based on its name and returns the loaded agent if it is successfully loaded. The agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_name: str. The name of the agent to be loaded.\n    :param agent_lifecycle: The lifecycle manager for agents,"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.error(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Clean up the agents\n        self.cleanup_agents()\n\n        # Return the current list of agents\n        return self.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"{self.goal_prompt_template.format(goal=goal)}\\n{self.sample_input_prompt_template.format(sample_input=sample_input)}\\n{self.prompt_template}\"\n            return self.llm_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            self.logger.exception(e)\n            return \"\"\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Get the agent's ID from the dictionary\n        agent_id = agent_dict['id']\n\n        # Check if the agent already exists in the database\n        agent_exists = self.get_agent(agent_id)\n\n        # If the agent exists, update its record\n        if agent_exists:\n            # Update the agent's record in the database\n            self.update_agent(agent_id, agent_dict)\n        else:\n            # Insert a new record for the agent in the database\n            self.insert_agent(agent_dict)\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Fetch the agent data from the database\n        c.execute(\"SELECT agent_data FROM agents WHERE purpose=?\", (purpose,))\n        agent_data = c.fetchone()\n\n        # Close the database connection\n        conn.close()\n\n        # If no agent with the given purpose is found, return None\n        if agent_data is None:\n            return None\n\n        # Deserialize the agent data and return it\n        return json.loads(agent_data[0])\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor object to execute SQL queries\n        cursor = conn.cursor()\n\n        # Execute a SQL query to retrieve all purposes from the database\n        cursor.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all rows returned by the query\n        rows = cursor.fetchall()\n\n        # Extract the purposes from the rows and store them in a list\n        purposes = [row[0] for row in rows]\n\n        # Close the cursor and connection\n        cursor.close()\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Fetch the cached result from the database\n        c.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n        result = c.fetchone()\n\n        # Close the database connection\n        conn.close()\n\n        # If the result is found, load it from JSON format and return it\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Insert a new row into the 'cache' table with the hash of the arguments as the key and the JSON-serialized result as the value\n        self.cursor.execute(\n            \"INSERT INTO cache (key, value) VALUES (?, ?)\", (arg_hash, json.dumps(result))\n        )\n        self.conn.commit()\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    global_config.update_config(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        sys.stdout = open(os.devnull, 'w')\n\n    # Execute the command line process based on the provided arguments\n    if args.command == 'train':\n        train_model(args)\n    elif args.command == 'test':\n        test_model(args)\n    elif args.command == 'predict':\n        predict_model(args)\n    elif args.command == 'evaluate':\n        evaluate_model(args)\n    elif args.command == 'visualize':\n        visualize_model(args)\n    elif args.command == 'analyze':\n        analyze_model(args)\n    elif args.command == 'optimize':\n        optimize_model(args)\n    elif args.command == 'deploy':\n        deploy_model(args)\n    elif args.command == 'serve':\n        serve_model(args)\n    elif args.command == 'monitor':\n        monitor_model(args)\n    elif args.command == 'debug':\n        debug_model(args)\n    elif args.command == 'profile':\n        profile_model(args)\n    elif args.command == 'tune':\n        tune_model(args)\n    elif args.command == 'distribute':\n        distribute_model(args)\n    elif args.command == 'parallelize':\n        parallelize_model(args)\n    elif args.command == 'quantize':\n        quantize_model(args)\n    elif args.command == 'prune':\n        prune_model(args)\n    elif args.command == 'fine-tune':\n        fine_tune_model(args)\n    elif args.command == 'distill':\n        distill_model(args)\n    elif args.command == 'interpret':\n        interpret_model(args)\n    elif args.command == 'export':\n        export_model(args)\n    elif args.command == 'convert':"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Get the model to use for chat completion\n        model = kwargs.get('model', None)\n\n        # If no model is provided, use the default model from the configuration\n        if model is None:\n            model = self.config.get('model', None)\n\n        # If no model is provided or found in the configuration, use the default model\n        if model is None:\n            model = self.default_model\n\n        # Get the context length limit from the configuration\n        context_length_limit = self.config.get('context_length_limit', None)\n\n        # If the context length limit is not provided in the configuration, use the default value\n        if context_length_limit is None:\n            context_length_limit = self.default_context_length_limit\n\n        # Get the list of available models from the configuration\n        available_models = self.config.get('available_models', None)\n\n        # If the list of available models is not provided in the configuration, use the default list\n        if available_models is None:\n            available_models = self.default_available_models\n\n        # Get the current context length from the configuration\n        current_context_length = self.config.get('current_context_length', None)\n\n        # If the current context length is not provided in the configuration, use the default value\n        if current_context_length is None:\n            current_context_length = self.default_current_context_length\n\n        # If the current context length exceeds the context length limit, try to use a higher-capacity model\n        if current_context_length > context_length_limit:\n            # Get the list of higher-capacity models from the configuration\n            higher_capacity_models = self.config.get('higher_capacity_models', None)\n\n            # If the list of higher-capacity models is not provided in the configuration, use the default list\n            if higher_capacity_models is None:\n                higher_capacity_models = self.default_higher_capacity_models\n\n            # Iterate over the available models\n            for model in available_models:\n                # If the model is in the"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not hasattr(self, 'client_instance'):\n            self.client_instance = boto3.client('s3')\n            self.client_instance_creation_time = time.time()\n        elif time.time() - self.client_instance_creation_time > self.client_instance_expiration_interval:\n            self.client_instance = boto3.client('s3')\n            self.client_instance_creation_time = time.time()\n        return self.client_instance\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if dist.is_initialized() and dist.get_rank() != 0:\n            raise RuntimeError(\n                \"state_dict should not be called from a DataLoader worker process\"\n            )\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader_state,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict['shuffle'] != self.shuffle:\n            raise ValueError(f'shuffle mismatch: {self._state_dict[\"shuffle\"]} != {self.shuffle}')\n\n        if self._state_dict['num_workers'] != self.num_workers:\n            raise ValueError(f'num_workers mismatch: {self._state_dict[\"num_workers\"]} != {self.num_workers}')\n\n        if self._state_dict['input_dir_path'] != self.input_dir_path:\n            raise ValueError(f'input_dir_path mismatch: {self._state_dict[\"input_dir_path\"]} != {self.input_dir_path}')\n\n        if self._state_dict['url'] != self.url:\n            raise ValueError(f'url mismatch: {self._state_dict[\"url\"]} != {self.url}')\n\n        if self._state_dict['seed'] != self.seed:\n            raise ValueError(f'seed mismatch: {self._state_dict[\"seed\"]} != {self.seed}')\n\n        if self._state_dict['item_loader_state'] != self.item_loader.state_dict():\n            raise ValueError(f'item_loader_state mismatch: {self._state_dict[\"item_loader_state\"]} != {self.item_loader.state_dict()}')\n\n        if self._state_dict['drop_last'] != self.drop_last:\n            raise ValueError(f'drop_last mismatch: {self._state_dict[\"drop_last\"]} != {self.drop_last}')\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # If input_dir is None, use an empty string for hashing\n    input_dir = input_dir or \"\"\n\n    # Generate a unique directory name based on the input directory\n    cache_dir_name = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Check if the environment variable is set\n    if \"CACHE_DIR\" in os.environ:\n        # If set, use the specified location for the cache directory\n        cache_dir = os.path.join(os.environ[\"CACHE_DIR\"], cache_dir_name)\n    else:\n        # If not set, use the default location for the cache directory\n        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"my_app\", cache_dir_name)\n\n    # Try to create the cache directory\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except Exception as e:\n        # If an error occurs during creation, print the error message and return None\n        print(f\"Error creating cache directory: {e}\")\n        return None\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Parse the remote file path to get the scheme, netloc, and path\n        parsed_url = urlparse(remote_filepath)\n        scheme = parsed_url.scheme\n        netloc = parsed_url.netloc\n        path = parsed_url.path\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Use the s5cmd command-line tool if available\n        if shutil.which(\"s5cmd\"):\n            cmd = f\"s5cmd --no-sign-request cp {remote_filepath} {local_filepath}\"\n            subprocess.run(cmd, shell=True, check=True)\n            return\n\n        # Use the boto3 library if s5cmd is not available\n        s3 = boto3.client(\"s3\")\n        s3.download_file(netloc, path, local_filepath)\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize dictionaries to store the assigned chunks and intervals for each worker\n    chunks_assigned = {i: [] for i in range(num_workers)}\n    intervals_assigned = {i: [] for i in range(num_workers)}\n\n    # Determine the distribution strategy based on the worker's index and the total world size\n    if worker_env.rank == 0:\n        # If the worker is the master (rank 0), assign all chunks and intervals to the master\n        chunks_assigned[0] = chunks_replica\n        intervals_assigned[0] = intervals_replica\n    else:\n        # If the worker is not the master, distribute chunks and intervals among the workers\n        for i, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            # Determine the worker index based on the chunk index and the total world size\n            worker_index = i % worker_env.world_size\n            # Assign the chunk and interval to the corresponding worker\n            chunks_assigned[worker_index].append(chunk)\n            intervals_assigned[worker_index].append(interval)\n\n    return chunks_assigned, intervals_assigned\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Get the image's mode, dimensions, and raw pixel data\n        mode = item.mode\n        size = item.size\n        data = item.tobytes()\n\n        # Serialize the image data into a bytes object\n        return (\n            len(mode).to_bytes(2, byteorder=\"big\")\n            + len(mode).to_bytes(2, byteorder=\"big\")\n            + len(mode).to_bytes(2, byteorder=\"big\")\n            + data,\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(f\"Unsupported item type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack('III', data[:12])\n        mode = data[12:12+mode_size].decode('utf-8')\n        image_data = data[12+mode_size:]\n        image = Image.frombytes(mode, (width, height), image_data)\n        return image\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = data[0]\n        shape = data[1:5]\n        shape = struct.unpack('>I', shape)[0]\n        shape = (shape,)\n\n        # Extract the tensor data from the byte array\n        tensor_data = data[5:]\n\n        # Reconstruct the tensor from the extracted data\n        tensor = torch.tensor(tensor_data, dtype=dtype)\n        tensor = tensor.view(shape)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            item.numpy().tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            return Image.open(io.BytesIO(data))\n        except RuntimeError as e:\n            if \"torchvision\" in sys.modules:\n                return torchvision.transforms.functional.to_tensor(\n                    Image.open(io.BytesIO(data))\n                )\n            else:\n                raise e\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the PyTorch tensor to a NumPy array\n        item_np = item.numpy()\n\n        # Serialize the NumPy array to bytes\n        item_bytes = item_np.tobytes()\n\n        # Get the data type of the tensor\n        dtype = item.dtype\n\n        # Map the data type to an index\n        dtype_index = {\n            torch.float32: 0,\n            torch.float64: 1,\n            torch.int32: 2,\n            torch.int64: 3,\n            torch.uint8: 4,\n            torch.bool: 5,\n        }[dtype]\n\n        # Create a string representation of the data type index\n        dtype_str = f\"no_header_tensor:{dtype_index}\"\n\n        # Return the serialized tensor data and the data type string\n        return item_bytes, dtype_str\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype))\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str, data_str = data.split(b':', 2)\n        dtype = np.dtype(dtype_str.decode('utf-8'))\n        shape = tuple(map(int, shape_str.decode('utf-8').strip('()').split(',')))\n\n        # Reconstruct the numpy array from the extracted information and the actual data\n        array = np.frombuffer(data_str, dtype=dtype).reshape(shape)\n\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{item.dtype.str}\"\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type index and number of dimensions of the input array\n        dtype_index = np.dtype(item.dtype).num\n        ndim = item.ndim\n\n        # Create a bytes object to store the serialized data\n        serialized_data = bytearray()\n\n        # Append the data type index and number of dimensions to the serialized data\n        serialized_data.extend(dtype_index.to_bytes(1, byteorder='big'))\n        serialized_data.extend(ndim.to_bytes(1, byteorder='big'))\n\n        # Append the shape of the array to the serialized data\n        for dim in item.shape:\n            serialized_data.extend(dim.to_bytes(4, byteorder='big'))\n\n        # Append the array's binary data to the serialized data\n        serialized_data.extend(item.tobytes())\n\n        # Return the serialized data and None as metadata\n        return bytes(serialized_data), None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _HAS_AV:\n            raise RuntimeError(\"av is not installed, please install av to use video_deserialize.\")\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name)\n        return video\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            self.write_chunk()\n\n        self.write_chunks_index()\n\n        self._is_done = True\n\n        return self.chunks_paths\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj['epoch']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.latest_worker = obj['latest_worker']\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset'])\n        else:\n            raise RuntimeError(\n                f'{self.__class__.__name__} only supports StreamingDataset and CombinedStreamingDataset, but got {type(self.dataset)}'\n            )\n\n        # Prepare for resuming\n        self.iterator = iter(self.dataset)\n        self.finished_worker = False\n        self.finished_iteration = False\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._internal_iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"num_samples_yielded\": num_samples_yielded,\n                }\n        else:\n            if num_samples_yielded is None:\n                return self._internal_iterator.state_dict()\n            else:\n                return {\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"num_samples_yielded\": num_samples_yielded,\n                    \"internal_iterator\": self._internal_iterator.state_dict(),\n                }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset_name, dataset_state_dict in state_dict.items():\n            if dataset_name == \"streaming_dataloader\":\n                self.streaming_dataloader.load_state_dict(dataset_state_dict)\n            else:\n                dataset = getattr(self, dataset_name)\n                dataset.load_state_dict(dataset_state_dict)\n\n        if self.streaming_dataloader.num_samples_yielded > 0:\n            self.streaming_dataloader.num_samples_yielded = 0\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n    elif isinstance(dir_path, Dir):\n        return dir_path\n    elif isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"gs://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"hdfs://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"file://\"):\n            return Dir(path=dir_path[7:])\n        elif dir_path.startswith(\"/\"):\n            return Dir(path=dir_path)\n        elif dir_path.startswith(\"~\"):\n            return Dir(path=os.path.expanduser(dir_path))\n        else:\n            return Dir(path=dir_path)\n    else:\n        raise ValueError(f\"Invalid dir_path type: {type(dir_path)}\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of the Dir class.\")\n\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must be an S3 directory.\")\n\n    if append or overwrite:\n        raise NotImplementedError(\"Append and overwrite options are not yet implemented.\")\n\n    if output_dir.exists():\n        if len(output_dir.list_contents()) > 0:\n            raise ValueError(\"output_dir must be empty.\")\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not output_dir.is_s3():\n        raise ValueError(\n            \"The output directory is not an S3 bucket. Please provide an S3 bucket directory.\"\n        )\n\n    if output_dir.exists(\"index.json\"):\n        raise ValueError(\n            \"The output directory already contains an index file. Please provide an empty directory.\"\n        )\n\n    if not output_dir.exists(\"index.json\"):\n        output_dir.delete_all_objects()\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.node_rank\n\n        if node_rank == 0 and self.index_id is not None:\n            logger.info(f\"{self.index_id}: merging {num_workers} parts\")\n            index_folder = self.index_folder\n            index_files = [\n                os.path.join(index_folder, f\"{self.index_id}.{i}.index\")\n                for i in range(num_workers)\n            ]\n\n            while not all([os.path.exists(index_file) for index_file in index_files]):\n                time.sleep(1)\n\n            index_files = [\n                os.path.join(index_folder, f\"{self.index_id}.{i}.index\")\n                for i in range(num_workers)\n            ]\n            index_files = [index_file for index_file in index_files if os.path.exists(index_file)]\n            logger.info(f\"{self.index_id}: merging {len(index_files)} parts\")\n            self.merge_files(index_files)\n            logger.info(f\"{self.index_id}: merged index to {self.index_file}\")\n\n            for index_file in index_files:\n                os.remove(index_file)\n\n        else:\n            while not os.path.exists(self.index_file):\n                time.sleep(1)\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if machine is None:\n        machine = Machine(\n            cpu=1,\n            memory=1,\n            gpu=0,\n            gpu_type=\"\",\n            min_gpu_memory=0,\n            min_gpu_cores=0,\n            min_gpu_memory_scale=0,\n            min_gpu_cores_scale=0,\n        )\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.environ}\"\n\n    job = Job(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n        num_gpus=machine.gpu,\n        num_cpus=machine.cpu,\n        num_memory=machine.memory,\n    )\n\n    job_id = job.create()\n\n    if job_id is None:\n        raise Exception(\"Job ID is None\")\n\n    job.wait_for_job_to_start()\n\n    job_url = job.get_job_url()\n\n    if job_url is None:\n        raise Exception(\"Job URL is None\")\n\n    print(f\"Job URL: {job_url}\")\n\n    job.wait_for_job_to_finish()\n\n    if job.get_job_status() == \"failed\":\n        raise Exception(\"Job failed\")\n\n    return\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.deletion_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the cache directory exists\n        if not self._cache_dir.exists():\n            return None\n\n        # Check if the index files are available\n        if not (self._cache_dir / \"index.json\").exists():\n            return None\n\n        # Load the index file\n        index = json.loads((self._cache_dir / \"index.json\").read_text())\n\n        # Check if the index file is valid\n        if \"chunks\" not in index:\n            return None\n\n        # Create a ChunksConfig object with the loaded index\n        config = ChunksConfig(\n            chunks=index[\"chunks\"],\n            serializers=self._serializers,\n            remote_input_dir=self._remote_input_dir,\n            item_loader=self._item_loader,\n        )\n\n        # Update the BinaryReader instance's configuration\n        self._config = config\n\n        return config\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Config should be defined before accessing it\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"Expected ChunkedIndex, got {type(index)}\")\n\n        if self.index_config is None:\n            raise Exception(\"Index config is not defined\")\n\n        if self.reader_thread is not None:\n            self.reader_thread.join()\n\n        if self.prefetch_thread is not None:\n            self.prefetch_thread.join()\n\n        if self.prepare_thread is not None:\n            self.prepare_thread.join()\n\n        if self.reader_thread is not None:\n            self.reader_thread.join()\n\n        if self.prefetch_thread is not None:\n            self.prefetch_thread.join()\n\n        if self.prepare_thread is not None:\n            self.prepare_thread.join()\n\n        if self.reader_thread is not None:\n            self.reader_thread.join()\n\n        if self.prefetch_thread is not None:\n            self.prefetch_thread.join()\n\n        if self.prepare_thread is not None:\n            self.prepare_thread.join()\n\n        if self.reader_thread is not None:\n            self.reader_thread.join()\n\n        if self.prefetch_thread is not None:\n            self.prefetch_thread.join()\n\n        if self.prepare_thread is not None:\n            self.prepare_thread.join()\n\n        if self.reader_thread is not None:\n            self.reader_thread.join()\n\n        if self.prefetch_thread is not None:\n            self.prefetch_thread.join()\n\n        if self.prepare_thread is not None:\n            self.prepare_thread.join()\n\n        if self.reader_thread is not None:\n            self.reader_thread.join()\n\n        if self.prefetch_thread is not None:\n            self.prefetch_thread.join()\n\n        if self.prepare_thread is not None:\n            self.prepare_thread.join()\n\n        if self.reader_thread is not None:\n           "}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if st.secrets[\"external_url\"]:\n        return st.experimental_connection.get_message(key)\n    else:\n        return obj\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Set the seed for the current epoch\n    torch.manual_seed(seed + current_epoch)\n\n    # Flatten the list of chunk indexes assigned to each rank\n    chunks_per_ranks = [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    # Shuffle the chunk indexes\n    torch.manual_seed(seed + current_epoch)\n    torch.randperm(len(chunks_per_ranks), out=torch.LongTensor(chunks_per_ranks))\n\n    # Return the shuffled chunk indexes\n    return chunks_per_ranks\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Check if the first two elements of the input sequence are valid file paths\n    if not isinstance(inputs[0], str) or not isinstance(inputs[1], str):\n        return None\n\n    # Resolve the indexed paths\n    input_dir = os.path.dirname(os.path.abspath(inputs[0]))\n    output_dir = os.path.dirname(os.path.abspath(inputs[1]))\n\n    # Check if the input and output directories are the same\n    if input_dir != output_dir:\n        raise ValueError(\n            f\"Input and output directories must be the same. Found {input_dir} and {output_dir}.\"\n        )\n\n    # Format the path to include the project root or a specified depth in the file system\n    input_dir = os.path.abspath(input_dir)\n    input_dir = os.path.join(input_dir, \"..\" * 3)\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Import the necessary modules\n    import socket\n    import functools\n\n    # Define a decorator function to disable DNS optimization\n    def decorate_dns(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Disable DNS optimization\n            socket.setdefaulttimeout(0.0)\n            # Call the original function\n            return func(*args, **kwargs)\n        return wrapper\n\n    # Define a context manager to enable or disable DNS optimization\n    class optimize_dns_context:\n        def __init__(self, enable):\n            self.enable = enable\n\n        def __enter__(self):\n            # If DNS optimization is enabled, disable it\n            if self.enable:\n                socket.setdefaulttimeout(0.0)\n\n        def __exit__(self, exc_type, exc_val, exc_tb):\n            # Always disable DNS optimization after the context\n            socket.setdefaulttimeout(None)\n\n    # If DNS optimization is enabled, return a context manager that disables it\n    if enable:\n        return decorate_dns\n    # Otherwise, return a context manager that does nothing\n    else:\n        return optimize_dns_context(enable)\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank * distributed_env.world_size\n    else:\n        num_items_per_rank = num_items_per_rank + len(indexes) % distributed_env.world_size\n\n    # Assign chunks and their intervals to each rank\n    chunks_per_rank = []\n    intervals_per_rank = []\n    for rank in range(distributed_env.world_size):\n        start = rank * num_items_per_rank\n        end = start + num_items_per_rank\n        chunks_per_rank.append(indexes[start:end])\n        intervals_per_rank.append(chunk_intervals[start:end])\n\n    return chunks_per_rank, intervals_per_rank\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the inputs are a list of strings or a list of dictionaries\n    if isinstance(inputs[0], str):\n        # If the inputs are a list of strings, create a list of dictionaries with the filepath as the value\n        inputs = [{\"filepath\": input} for input in inputs]\n\n    # Check if the inputs are a list of dictionaries\n    if isinstance(inputs[0], dict):\n        # If the inputs are a list of dictionaries, create a list of dictionaries with the filepath as the value\n        inputs = [{\"filepath\": input[\"filepath\"]} for input in inputs]\n\n    # Check if the inputs are a list of dictionaries\n    if isinstance(inputs[0], dict):\n        # If the inputs are a list of dictionaries, create a list of dictionaries with the filepath as the value\n        inputs = [{\"filepath\": input[\"filepath\"]} for input in inputs]\n\n    # Check if the inputs are a list of dictionaries\n    if isinstance(inputs[0], dict):\n        # If the inputs are a list of dictionaries, create a list of dictionaries with the filepath as the value\n        inputs = [{\"filepath\": input[\"filepath\"]} for input in inputs]\n\n    # Check if the inputs are a list of dictionaries\n    if isinstance(inputs[0], dict):\n        # If the inputs are a list of dictionaries, create a list of dictionaries with the filepath as the value\n        inputs = [{\"filepath\": input[\"filepath\"]} for input in inputs]\n\n    # Check if the inputs are a list of dictionaries\n    if isinstance(inputs[0], dict):\n        # If the inputs are a list of dictionaries, create a list of dictionaries with the filepath as the value\n        inputs = [{\"filepath\": input[\"filepath\"]} for input in inputs]\n\n    # Check if the inputs are a list of dictionaries\n    if isinstance(inputs[0], dict):\n        # If the inputs are a list of dictionaries, create a list of dictionaries with the filepath as the value\n        inputs = [{\"filepath\":"}
{"namespace": "litdata.processing.functions.map", "completion": "    # Check if the output directory is empty\n    if error_when_not_empty and len(output_dir) > 0:\n        raise ValueError(f\"Output directory {output_dir} is not empty\")\n\n    # Check if the output directory is a Dir object\n    if not isinstance(output_dir, Dir):\n        output_dir = Dir(output_dir)\n\n    # Check if the reader is provided\n    if reader is None:\n        reader = BaseReader()\n\n    # Check if the batch size is provided\n    if batch_size is None:\n        batch_size = len(inputs)\n\n    # Check if the fast_dev_run is provided\n    if isinstance(fast_dev_run, bool):\n        fast_dev_run = int(fast_dev_run)\n\n    # Check if the num_workers is provided\n    if num_workers is None:\n        num_workers = len(inputs)\n\n    # Check if the num_nodes is provided\n    if num_nodes is None:\n        num_nodes = 1\n\n    # Check if the machine is provided\n    if machine is None:\n        machine = \"default\"\n\n    # Check if the num_downloaders is provided\n    if num_downloaders is None:\n        num_downloaders = 1\n\n    # Check if the num_uploaders is provided\n    if num_uploaders is None:\n        num_uploaders = 1\n\n    # Check if the weights are provided\n    if weights is None:\n        weights = [1] * len(inputs)\n\n    # Check if the weights and inputs have the same length\n    if len(weights) != len(inputs):\n        raise ValueError(\n            f\"weights and inputs must have the same length, but got {len(weights)} and {len(inputs)}\"\n        )\n\n    # Check if the fast_dev_run is a positive integer\n    if not isinstance(fast_dev_run, int) or fast_dev_run < 0:\n        raise ValueError(\n            f\"fast_dev_run must be a positive integer, but got {fast_"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        task = queue_in.get()\n        if task is None:\n            break\n        index, files = task\n        for file in files:\n            if not os.path.exists(os.path.join(cache_dir, file)):\n                input_dir.download(file, cache_dir)\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n        if isinstance(item, tuple):\n            tmp_dir, file_path = item\n            file_path = os.path.join(tmp_dir, file_path)\n        else:\n            file_path = item\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n        if output_dir.scheme == \"s3\":\n            output_dir.upload(file_path, file_path)\n        else:\n            output_dir.move(file_path, file_path)\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Print distribution details for workers on the current node\n    for worker_id, worker_items in zip(worker_ids_this_node, worker_items):\n        if file_size:\n            worker_size = sum(item.stat().st_size for item in worker_items) / 1024 / 1024\n            print(f\"Worker {worker_id} has {len(worker_items)} files, with {worker_size:.2f} MB total\")\n        else:\n            print(f\"Worker {worker_id} has {len(worker_items)} items, with {sum(worker_weights[i] for i in worker_items)} total weight\")\n\n    # Return a list of items for each worker, with the items shuffled\n    return [list(np.random.permutation(items)) for items in worker_items]"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    num_workers_total = num_nodes * num_workers\n    num_items = len(user_items)\n    num_items_per_worker = (num_items + num_workers_total - 1) // num_workers_total\n    num_items_last_worker = num_items - (num_items_per_worker * num_workers_total - num_items)\n    num_items_per_worker_last_worker = (num_items_last_worker + num_workers - 1) // num_workers\n\n    if num_items_per_worker_last_worker == 0:\n        num_items_per_worker_last_worker = 1\n\n    num_items_per_worker = [num_items_per_worker] * (num_workers - 1) + [num_items_per_worker_last_worker]\n    num_items_per_worker_cumsum = [0] + list(np.cumsum(num_items_per_worker))\n    total_num_items = sum(num_items_per_worker)\n    assert total_num_items == num_items, f\"Error: total_num_items {total_num_items} != num_items {num_items}\"\n\n    worker_id_start = node_rank * num_workers\n    worker_id_end = worker_id_start + num_workers\n    items_assigned_to_workers = []\n\n    for i in range(worker_id_start, worker_id_end):\n        items_assigned_to_worker = user_items[num_items_per_worker_cumsum[i]:num_items_per_worker_cumsum[i + 1]]\n        items_assigned_to_workers.append(items_assigned_to_worker)\n\n    assert len(items_assigned_to_workers) == num_workers, f\"Error: len(items_assigned_to_workers)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def clean_cache(self):\n            if os.path.exists(self.cache_dir):\n                shutil.rmtree(self.cache_dir)\n            os.makedirs(self.cache_dir)\n\n\n        \"\"\"\n        The function creates a new directory for the cache if it does not already exist.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        def make_cache_dir(self):\n            if not os.path.exists(self.cache_dir):\n                os.makedirs(self.cache_dir)\n\n\n        \"\"\"\n        The function checks if the cache is empty by checking if the cache directory exists and if it is empty.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: bool. Returns True if the cache is empty, False otherwise.\n        \"\"\"\n\n        def is_cache_empty(self):\n            return not os.path.exists(self.cache_dir) or not os.listdir(self.cache_dir)\n\n\n        \"\"\"\n        The function checks if the cache is full by checking if the number of files in the cache directory is equal to the maximum number of files allowed in the cache.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: bool. Returns True if the cache is full, False otherwise.\n        \"\"\"\n\n        def is_cache_full(self):\n            return len(os.listdir(self.cache_dir)) >= self.max_cache_size\n\n\n        \"\"\"\n        The function checks if a file exists in the cache by checking if the file path is in the list of files in the cache directory.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method."}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        if input_dir is not None:\n            if element.startswith(input_dir):\n                return True\n        if os.path.exists(element):\n            return True\n    return False\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons < 16:\n                network_type = \"TinyFullyFusedMLP\"\n            else:\n                network_type = \"FullyFusedMLP\"\n\n            network = getattr(tcnn, network_type)(\n                n_input_dims,\n                n_output_dims,\n                n_layers,\n                n_neurons,\n                getattr(tcnn, activation)(n_neurons),\n                getattr(tcnn, output_activation)(n_neurons),\n            )\n        else:\n            network = nn.Sequential(\n                nn.Linear(n_input_dims, n_neurons),\n                getattr(nn, activation)(),\n                *[\n                    nn.Linear(n_neurons, n_neurons)\n                    for _ in range(n_layers - 2)\n                ],\n                nn.Linear(n_neurons, n_output_dims),\n                getattr(nn, output_activation)(),\n            )\n\n        return network\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the number of elements to shift the signal by\n        shift = kernel_offset\n\n        # Create a list of shifted versions of the signal\n        shifted_signals = [signal[shift:], signal[:-shift]]\n\n        # Compute the median of the shifted signals\n        median = np.median(shifted_signals, axis=0)\n\n        # Trim the median array to remove edge effects\n        trimmed_median = median[shift:-shift]\n\n        return trimmed_median\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance for each rotation shift\n    hamm_dist = [\n        hamming_distance_with_rotation(\n            template_probe, template_gallery, i, nm_dist, weights\n        )\n        for i in range(rotation_shift)\n    ]\n\n    # Find the minimum Hamming distance and its corresponding rotation shift\n    min_hamm_dist = min(hamm_dist)\n    min_rotation_shift = hamm_dist.index(min_hamm_dist)\n\n    return min_hamm_dist, min_rotation_shift\n\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize an empty list to store the perpendicular bisectors\n        perpendicular_bisectors = []\n\n        # Initialize a counter to keep track of the number of iterations\n        iterations = 0\n\n        # Loop until the number of perpendicular bisectors is equal to the desired number\n        while len(perpendicular_bisectors) < self.number_of_bisectors:\n\n            # Increment the iteration counter\n            iterations += 1\n\n            # If the maximum number of iterations has been reached, raise an error\n            if iterations > self.max_iterations:\n                raise EyeCentersEstimationError(\n                    \"Failed to find a sufficient number of point pairs that meet the distance criterion within the maximum number of iterations allowed.\"\n                )\n\n            # Randomly select two points from the polygon's vertices\n            point_1, point_2 = np.random.choice(\n                polygon, size=2, replace=False\n            )\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point_1 - point_2)\n\n            # If the distance is greater than the minimum distance, add the points to the list of perpendicular bisectors\n            if distance > min_distance_between_sector_points_in_px:\n                perpendicular_bisectors.append((point_1, point_2))\n\n        # Convert the list of perpendicular bisectors to numpy arrays\n        perpendicular_bisectors = np.array(perpendicular_bisectors)\n\n        # Calculate the midpoints of the perpendicular bisectors\n        midpoints = (perpendicular_bisectors[:, 0] + perpendicular_bisectors[:, 1]) / 2\n\n        # Calculate the vectors representing the perpendicular bisectors\n        vectors = perpendicular_bisectors[:, 1] - perpendicular_bisectors[:, 0]\n\n        # Calculate the normals to the perpendicular bisectors\n        normals = np.array([-vectors[:, 1], vectors[:, 0]]).T\n\n        # Calculate the starting and ending points of the perpendicular bisectors\n        start_points = midpoints - normals\n       "}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(*args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(*args, **kwargs)\n\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hints = {\n            param.name: type_hint\n            for param, type_hint in type_hints.items()\n            if param != \"return\"\n        }\n\n        output_type_hint = type_hints.get(\"return\")\n\n        input_class_definitions = {\n            param: get_class_definition(type_hint)\n            for param, type_hint in input_type_hints.items()\n        }\n\n        if isinstance(output_type_hint, type) and issubclass(\n            output_type_hint, Embedding\n        ):\n            output_class_definition = get_class_definition(output_type_hint)\n            function_type = FunctionType.EMBEDDABLE\n        elif isinstance(output_type_hint, UnionType):\n            output_class_definition = get_class_definition(output_type_hint)\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            output_class_definition = None\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hint=output_type_hint,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for seed in range(self.hash_count):\n            result = mmh3.hash(string, seed) % self.bit_size\n            self.bit_array[result] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        # Load the bit array from persistence\n        bit_array = self.persistence.load()\n\n        # Check if the loaded bit array's length matches the expected length\n        expected_length = self.size\n        if len(bit_array) != expected_length:\n            # Log a warning if there is a mismatch\n            logging.warning(\n                f\"Loaded bit array length ({len(bit_array)}) does not match expected length ({expected_length}). Reinitializing and saving.\"\n            )\n            # Reinitialize the bit array and indices\n            self.init_bit_array()\n            self.save()\n\n        # Set the loaded bit array to the BloomFilter instance\n        self.bit_array = bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if the API key is valid\n        if not self.is_api_key_valid():\n            raise ValueError(\"Invalid API key\")\n\n        # Validate the parameters\n        if not self.validate_parameters(model, system_message, prompt, **kwargs):\n            raise ValueError(\"Invalid parameters\")\n\n        # Set the default values for the parameters\n        temperature = kwargs.get(\"temperature\", 0.7)\n        top_p = kwargs.get(\"top_p\", 1)\n        frequency_penalty = kwargs.get(\"frequency_penalty\", 0)\n        presence_penalty = kwargs.get(\"presence_penalty\", 0)\n        max_new_tokens = kwargs.get(\"max_new_tokens\", 2048)\n\n        # Set the retry parameters\n        max_retries = 5\n        retry_delay = 1\n\n        # Generate the response\n        for i in range(max_retries):\n            try:\n                response = openai.Completion.create(\n                    model=model,\n                    prompt=prompt,\n                    temperature=temperature,\n                    top_p=top_p,\n                    frequency_penalty=frequency_penalty,\n                    presence_penalty=presence_penalty,\n                    max_tokens=max_new_tokens,\n                )\n                break\n            except openai.error.RateLimitError:\n                # Handle rate limit error\n                if i == max_retries - 1:\n                    raise\n                time.sleep(retry_delay)\n                retry_delay *= 2\n            except openai.error.APIError as e:\n                # Handle other API errors\n                raise ValueError(f\"OpenAI API error: {e}\")\n\n        # Process the response\n        text = response.choices[0].text\n        text = self.process_response(text, model)\n\n        return text\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2:\n        raise ValueError(\"The input matrix should be 2-dimensional.\")\n\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The input matrix should be square.\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The input matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input must be a numpy array.\")\n\n    if x.ndim != 2:\n        raise ValueError(\"Input must be a 2D array.\")\n\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Input must be a square matrix.\")\n\n    if not np.allclose(x, x.T, atol=1e-08):\n        raise ValueError(\"Input must be a symmetric matrix.\")\n\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0]), atol=1e-08):\n        raise ValueError(\"Input must have zero diagonal elements.\")\n\n    return None\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            return None, None, None, True\n\n        # Get the model and prompt for the function description\n        model, prompt = self.get_model_and_prompt(function_description)\n\n        # Check if the model is suitable for distillation\n        is_suitable_for_distillation = self.is_suitable_for_distillation(model, prompt)\n\n        # Check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            return None, None, None, True\n\n        # Check if the model is suitable for distillation\n        is_suitable_for_distillation = self.is_suitable_for_distillation(model, prompt)\n\n        # Check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            return None, None, None, True\n\n        # Check if the model is suitable for distillation\n        is_suitable_for_distillation = self.is_suitable_for_distillation(model, prompt)\n\n        # Check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            return None, None, None, True\n\n        # Check if the model is suitable for distillation\n        is_suitable_for_distillation = self.is_suitable_for_distillation(model, prompt)\n\n        # Check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            return None, None, None, True\n\n        # Check if the model is suitable for distillation\n        is_suitable_for_distillation = self.is_suitable_for_distillation(model, prompt)\n\n        # Check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            return None, None, None, True\n\n        # Check if the model is suitable for distillation\n        is_suitable_for_distill"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov = cov_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        cov = cov_clip(cov)\n\n    return cov\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj, str)\n    elif isinstance(obj, bytes):\n        return (obj, bytes)\n    elif isinstance(obj, list):\n        res = []\n        for item in obj:\n            item_res, item_schema = flatten_to_tuple(item)\n            res.append(item_res)\n        return (res, list)\n    elif isinstance(obj, tuple):\n        res = []\n        for item in obj:\n            item_res, item_schema = flatten_to_tuple(item)\n            res.append(item_res)\n        return (res, tuple)\n    elif isinstance(obj, dict):\n        res = {}\n        for key, value in obj.items():\n            value_res, value_schema = flatten_to_tuple(value)\n            res[key] = value_res\n        return (res, dict)\n    elif isinstance(obj, Instances):\n        res = {}\n        for key, value in obj.__dict__.items():\n            value_res, value_schema = flatten_to_tuple(value)\n            res[key] = value_res\n        return (res, Instances)\n    elif isinstance(obj, Boxes):\n        res = {}\n        for key, value in obj.__dict__.items():\n            value_res, value_schema = flatten_to_tuple(value)\n            res[key] = value_res\n        return (res, Boxes)\n    elif isinstance(obj, ROIMasks):\n        res = {}\n        for key, value in obj.__dict__.items():\n            value_res, value_schema = flatten_to_tuple(value)\n            res[key] = value_res\n        return (res, ROIMasks)\n    else:\n        raise ValueError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if groups is a 2D array\n    if len(groups.shape) != 2:\n        raise ValueError(f\"{names[0]} must be a 2D array.\")\n\n    # Check if equations is a 1D array\n    if len(equations.shape) != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array.\")\n\n    # Check if all elements in groups are strings\n    if not np.issubdtype(groups.dtype, np.str_):\n        raise ValueError(f\"All elements in {names[0]} must be strings.\")\n\n    # Check if all elements in equations are strings\n    if not np.issubdtype(equations.dtype, np.str_):\n        raise ValueError(f\"All elements in {names[1]} must be strings.\")\n\n    # Check if all elements in equations are valid equations\n    for equation in equations:\n        if not is_valid_equation(equation):\n            raise ValueError(f\"{equation} is not a valid equation.\")\n\n    # Check if all groups in equations are present in groups\n    for equation in equations:\n        for group in get_groups_from_equation(equation):\n            if group not in groups:\n                if raise_if_group_missing:\n                    raise ValueError(f\"{group} is not present in {names[0]}.\")\n                else:\n                    warnings.warn(f\"{group} is not present in {names[0]}.\")\n\n    # Initialize left and right matrices\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n\n    # Iterate over equations\n    for i, equation in enumerate(equations):\n        # Get groups and coefficients from equation\n        groups_in_equation, coefficients = get_groups_and_coefficients_from_equation(equation)\n\n        # Get indices of groups in groups array\n        indices = [np.where(groups == group)[0] for group in groups_in_equation]\n\n        # Set coefficients in left matrix\n        for j, index in en"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import torch\n    import detectron2\n    import os\n    import sys\n    import tempfile\n    import importlib\n    import shutil\n\n    # Get the path to the detectron2 module\n    detectron2_path = os.path.dirname(detectron2.__file__)\n\n    # Create a temporary directory to store the new module\n    temp_dir = tempfile.mkdtemp()\n\n    # Define the name of the new module\n    module_name = 'instances'\n\n    # Define the path to the new module\n    module_path = os.path.join(temp_dir, module_name + '.py')\n\n    # Define the code for the new module\n    module_code = f'''"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Importing the necessary libraries\n    import numpy as np\n    from PIL import Image\n    from PIL.ExifTags import TAGS\n\n    # Opening the image file\n    image = Image.open(file_name)\n\n    # Extracting the EXIF data from the image\n    exif_data = image.getexif()\n\n    # Creating a dictionary to store the orientation value\n    exif = {}\n\n    # Looping through the EXIF data and storing the orientation value\n    for tag_id, value in exif_data.items():\n        tag = TAGS.get(tag_id, tag_id)\n        if tag == 'Orientation':\n            exif[tag] = value\n            break\n\n    # Checking if the orientation value is present in the EXIF data\n    if exif:\n        # Checking the orientation value and rotating the image accordingly\n        if exif['Orientation'] == 3:\n            image = image.rotate(180, expand=True)\n        elif exif['Orientation'] == 6:\n            image = image.rotate(270, expand=True)\n        elif exif['Orientation'] == 8:\n            image = image.rotate(90, expand=True)\n\n    # Converting the image to the specified format\n    if format == 'BGR':\n        image = image.convert('RGB')\n        image = np.array(image)\n        image = image[:, :, ::-1].copy()\n    elif format == 'YUV-BT.601':\n        image = image.convert('YCbCr')\n        image = np.array(image)\n        image = image[:, :, (0, 2, 1)].copy()\n        image = image.astype(np.float)\n        image[:, :, 0] = (image[:, :, 0] - 16.0) / 219.0\n        image[:, :, 1:] = (image[:, :, 1:] - 128.0) / 224.0\n    else:\n        image"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(min=0)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation = polygon\n    segmentation = annotation[\"segmentation\"]\n    if isinstance(segmentation, list):\n        # polygons\n        polygons = [np.asarray(p).reshape(-1, 2) for p in segmentation]\n        # transform polygons\n        polygons = transforms.apply_polygons(polygons)\n        # clip polygons to image size\n        polygons = [p.clip(min=0) for p in polygons]\n        # round to integer (pixel) coordinates\n        polygons = [np.round(p).flatten().astype(np.int32) for p in polygons]\n        annotation[\"segmentation\"] = polygons\n    else:\n        # RLE\n        mask = mask_util.decode(segmentation)\n        # transform mask RLE\n        mask = transforms.apply_segmentation(mask)\n        # transform mask back to RLE\n        mask = mask_util.encode(np.asarray(mask, order=\"F\", dtype=\"uint8\"))\n        # area needs to be recomputed, because after transforms,\n        # the RLEs can potentially have different size.\n        annotation[\"segmentation\"] = mask\n        annotation[\"area\"] = mask_util.area(mask)\n\n    # keypoints\n    keypoints = annotation[\"keypoints\"]\n    keypoints = transforms.apply_coords(keypoints)\n    annotation[\"keypoints\"] = keypoints"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0:\n            return coords\n        if self.angle % 360 == 0:\n            return coords\n        return np.dot(coords, self.rm_coords)\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Define a dictionary to store the flops count for each operator\n    flops_dict = defaultdict(float)\n\n    # Define a hook function to extract the flops count from the model\n    def count_hook(module, input, output):\n        # Get the operator name\n        op_name = str(module).split('(')[0]\n        # Get the input shape\n        input_shape = input[0].shape\n        # Get the output shape\n        output_shape = output.shape\n        # Calculate the flops count\n        flops = calculate_flops(op_name, input_shape, output_shape)\n        # Update the flops count in the dictionary\n        flops_dict[op_name] += flops\n\n    # Register the hook function for all modules in the model\n    for module in model.modules():\n        module.register_forward_hook(count_hook)\n\n    # Run the model with the given inputs\n    with torch.no_grad():\n        model(inputs)\n\n    # Convert the flops count to Gflops\n    for op_name in flops_dict:\n        flops_dict[op_name] /= 1e9\n\n    # Return the flops dictionary\n    return flops_dict\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        if self.rm_image is None:\n            return img\n\n        if self.rm_image.shape[0] == 2:\n            img = cv2.warpAffine(img, self.rm_image, (self.w, self.h), flags=interp)\n        else:\n            img = cv2.warpAffine(img, self.rm_image[:2], (self.bound_w, self.bound_h), flags=interp)\n\n        return img\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = image[:, :, ::-1]\n        visualizer = Visualizer(image, self.metadata, instance_mode=self.instance_mode)\n        if \"pred_boxes\" in predictions:\n            boxes = predictions[\"pred_boxes\"]\n            boxes = boxes.tensor.cpu().numpy()\n            scores = predictions[\"scores\"]\n            scores = scores.cpu().numpy()\n            classes = predictions[\"pred_classes\"].cpu().numpy()\n            labels = [self.metadata.thing_classes[c] for c in classes]\n            if self.instance_mode == ColorMode.IMAGE_BW:\n                # Black background.\n                vis_image = image.astype(\"uint8\").copy()\n                vis_image[:, :, :] = 0\n                alpha = 0.5\n                for i in range(len(boxes)):\n                    if scores is None or scores[i] > self.instance_threshold:\n                        vis_image = vis_image.astype(\"uint8\")\n                        vis_image = cv2.rectangle(\n                            vis_image,\n                            (boxes[i][0], boxes[i][1]),\n                            (boxes[i][2], boxes[i][3]),\n                            (0, 0, 255),\n                            thickness=2,\n                        )\n                        vis_image = cv2.addWeighted(\n                            src1=vis_image,\n                            alpha=alpha,\n                            src2=image.astype(\"uint8\"),\n                            beta=1 - alpha,\n                            gamma=0,\n                        )\n            else:\n                if self.instance_mode == ColorMode.SEGMENTATION and self.process_prediction:\n                    masks = predictions[\"pred_masks\"].cpu().numpy()\n                    # Masks are in uint8 format, so they should be converted to bool type.\n                    masks = masks > 0.5\n                    alpha = 0.5\n                    for i in range(len(boxes)):\n                        if scores is None or scores[i]"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the current state of the canvas\n        canvas = self.canvas\n\n        # Get the image dimensions from the canvas\n        w, h = canvas.get_width_height()[:2]\n\n        # Get the image data from the canvas\n        buf = canvas.get_image_data()\n\n        # Convert the image data from RGBA to RGB format\n        image = np.frombuffer(buf, dtype='uint8')\n        image = image.reshape((h, w, 4))\n        image = image[:, :, :3]\n\n        # Return the image\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                edges = [x[\"edge_color\"] if \"edge_color\" in x else (0, 255, 0) for x in annos]\n                alpha = [x[\"alpha\"] if \"alpha\" in x else 0.5 for x in annos]\n                img = self.draw_and_connect_masks(\n                    img, masks, category_ids, edges, alpha=alpha\n                )\n                polygons = None\n                if \"polygons\" in annos[0]:\n                    polygons = [x[\"polygons\"] for x in annos]\n                img = self.draw_and_connect_polygons(\n                    img, polygons, category_ids, edges, alpha=alpha\n                )\n            else:\n                boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n                category_ids = [x[\"category_id\"] for x in annos]\n                img = self.overlay_instances(\n                    labels=category_ids, boxes=boxes, masks=None, keypoints=None, assigned_colors=None\n                )\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            sem_seg = cv2.imread(dic[\"sem_seg_file_name\"], cv2.IMREAD_GRAYSCALE)\n        if sem_seg is not None:\n            img = self.draw_sem_seg(img, sem_seg, area_threshold=0, alpha=0.5)\n\n        keypoints = dic.get(\"keypoints\", None)\n        if keypoints:\n            img = self.draw_keyp"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = self.compute_colors(binary_mask, alpha=alpha)\n        elif isinstance(color, str):\n            color = color_val(color)\n        elif isinstance(color, tuple):\n            color = color_val(color)\n        else:\n            raise ValueError(\"Unsupported color type {}\".format(type(color)))\n\n        if edge_color is None:\n            edge_color = color\n        elif isinstance(edge_color, str):\n            edge_color = color_val(edge_color)\n        elif isinstance(edge_color, tuple):\n            edge_color = color_val(edge_color)\n        else:\n            raise ValueError(\"Unsupported color type {}\".format(type(edge_color)))\n\n        if text is None:\n            text = \"\"\n\n        if binary_mask.ndim == 2:\n            binary_mask = np.ascontiguousarray(binary_mask)\n            contours = measure.find_contours(binary_mask, 0.5)\n            for contour in contours:\n                contour = np.flip(contour, axis=1)\n                self.output.ax.add_patch(\n                    Polygon(\n                        contour,\n                        facecolor=\"none\",\n                        edgecolor=edge_color,\n                        linewidth=1,\n                        alpha=alpha,\n                    )\n                )\n                self.output.ax.text(\n                    contour[0][0],\n                    contour[0][1],\n                    text,\n                    size=15,\n                    verticalalignment=\"top\",\n                    horizontalalignment=\"left\",\n                    bbox={\"facecolor\": edge_color, \"alpha\": 0.8, \"pad\": 0, \"edgecolor\": \"none\"},\n                    zorder=10,\n                )\n        else:\n            binary_mask = np.ascontiguousarray(binary_mask)\n            for part in binary_mask:\n                part = np.ascontiguousarray(part)\n                if part.sum() > area_threshold:\n                    contours = measure.find_"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size, rtol=rtol), msg + \"image_size mismatch\"\n    else:\n        assert input.image_size == other.image_size, msg + \"image_size mismatch\"\n\n    for field in input.get_fields():\n        if isinstance(input.get(field), Boxes):\n            assert torch.allclose(input.get(field).tensor, other.get(field).tensor, rtol=rtol), msg + field + \" mismatch\"\n        elif isinstance(input.get(field), ROIMasks):\n            assert torch.allclose(input.get(field).tensor, other.get(field).tensor, rtol=rtol), msg + field + \" mismatch\"\n        elif isinstance(input.get(field), torch.Tensor):\n            assert torch.allclose(input.get(field), other.get(field), rtol=rtol), msg + field + \" mismatch\"\n        else:\n            raise ValueError(\"Unknown type: {}\".format(type(input.get(field))))\n\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    return registry.PROPOSAL_GENERATORS[\n        cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    ](cfg, input_shape)\n\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        losses = {\n            \"loss_cls\": self.softmax_cross_entropy_loss(scores, proposals),\n            \"loss_box_reg\": self.smooth_l1_loss(\n                proposal_deltas, proposals, self.box2box_transform, self.smooth_l1_beta\n            ),\n        }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker = TRACKER_REGISTRY.get(tracker_name)(cfg)\n    return tracker\n\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image with the general annotation instruction\n        output = self.process(image)\n\n        # If no specific annotation type is requested, return the entire processed output\n        if anno_type is None:\n            return output\n\n        # If a specific annotation type is requested, filter the output and return the requested annotation(s)\n        if isinstance(anno_type, str):\n            return output[anno_type]\n        elif isinstance(anno_type, list):\n            return {k: v for k, v in output.items() if k in anno_type}\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url, score in self.inverted_index[keyword].items():\n                    if url not in scores:\n                        scores[url] = 0\n                    scores[url] += score\n\n        # Aggregate the scores for URLs\n        aggregated_scores = {}\n        for url, score in scores.items():\n            aggregated_scores[url] = score\n\n        return aggregated_scores\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles to be within the range (-180, 180] degrees\n        angles_normalized = self.tensor[:, 4].clone()\n        angles_normalized[angles_normalized > 180] -= 360\n\n        # Identify the indices of the boxes that are nearly horizontal\n        nearly_horizontal_indices = torch.abs(angles_normalized) < clip_angle_threshold\n\n        # Convert the nearly horizontal boxes to (x1, y1, x2, y2) representation\n        boxes_nearly_horizontal = self.tensor[nearly_horizontal_indices].clone()\n        boxes_nearly_horizontal[:, 2:] *= 0.5\n        boxes_nearly_horizontal = Boxes(boxes_nearly_horizontal).convert(\"xyxy\")\n\n        # Clamp the x and y coordinates of the nearly horizontal boxes\n        boxes_nearly_horizontal.tensor[:, 0::2].clamp_(min=0, max=box_size[1])\n        boxes_nearly_horizontal.tensor[:, 1::2].clamp_(min=0, max=box_size[0])\n\n        # Convert the nearly horizontal boxes back to (center x, center y, width, height, angle) representation\n        boxes_nearly_horizontal = Boxes(boxes_nearly_horizontal.convert(\"xywh\")).convert(\"xy_wh_a\")\n\n        # Update the original boxes with the clipped nearly horizontal boxes\n        self.tensor[nearly_horizontal_indices] = boxes_nearly_horizontal.tensor\n\n        # Ensure numerical errors do not increase the box sizes\n        self.clip_to_image(remove_empty=False)\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n\n        return statistics\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from .necks import NECKS\n    from mmdet.models.necks import NECKS as MMDET_NECKS\n\n    if cfg['type'] in NECKS:\n        return NECKS.build(cfg)\n    elif cfg['type'] in MMDET_NECKS:\n        return MMDET_NECKS.build(cfg)\n    else:\n        raise ValueError(f'neck type {cfg[\"type\"]} not supported')"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg.type == 'mse':\n        from torch.nn import MSELoss\n        return MSELoss()\n    elif cfg.type == 'bce':\n        from torch.nn import BCELoss\n        return BCELoss()\n    elif cfg.type == 'ce':\n        from torch.nn import CrossEntropyLoss\n        return CrossEntropyLoss()\n    elif cfg.type == 'bce_logits':\n        from torch.nn import BCEWithLogitsLoss\n        return BCEWithLogitsLoss()\n    elif cfg.type == 'ce_logits':\n        from torch.nn import CrossEntropyLoss\n        return CrossEntropyLoss()\n    elif cfg.type == 'focal':\n        from .focal_loss import FocalLoss\n        return FocalLoss(gamma=cfg.gamma, alpha=cfg.alpha)\n    elif cfg.type == 'dice':\n        from .dice_loss import DiceLoss\n        return DiceLoss()\n    elif cfg.type == 'tversky':\n        from .tversky_loss import TverskyLoss\n        return TverskyLoss(alpha=cfg.alpha, beta=cfg.beta)\n    elif cfg.type == 'focal_tversky':\n        from .focal_tversky_loss import FocalTverskyLoss\n        return FocalTverskyLoss(alpha=cfg.alpha, beta=cfg.beta, gamma=cfg.gamma)\n    elif cfg.type == 'focal_dice':\n        from .focal_dice_loss import FocalDiceLoss\n        return FocalDiceLoss(alpha=cfg.alpha, gamma=cfg.gamma)\n    elif cfg.type == 'focal_dice_ce':\n        from .focal_dice_ce_loss import FocalDiceCELoss\n        return FocalDiceCELoss(alpha=cfg.alpha, beta=cfg.beta, gamma=cfg.gamma)\n    elif cfg.type == 'focal"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS\n    from mmdet.models import HEADS as MMDET_HEADS\n\n    if cfg['type'] in HEADS:\n        return HEADS[cfg['type']](**cfg)\n    else:\n        return MMDET_HEADS.build_head(cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is deprecated, '\n                      'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn('train_cfg and test_cfg is deprecated, '\n                      'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    import numpy as np\n    from . import kitti_common as kitti\n    from .kitti_object_eval_python.evaluate import kitti_object_eval\n\n    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d == 'Depth' or box_type_3d == 'BEV':\n        gt_annos = kitti.transform_annotations_to_kitti_format(\n            gt_annos, box_mode_3d=box_mode_3d)\n        dt_annos = kitti.transform_annotations_to_kitti_format(\n            dt_annos, box_mode_3d=box_mode_3d)\n    else:\n        gt_annos = kitti.transform_annotations_to_kitti_format(gt_annos)\n        dt_annos = kitti.transform_annotations_to_kitti_format(dt_annos)\n\n    for gt_anno, dt_anno in zip(gt_annos, dt_annos):\n        gt_anno['name'] = np.array([label2cat[n] for n in gt_anno['name']])\n        dt_anno['name'] = np.array([label2cat[n] for n in dt_anno['name']])\n\n    eval_types = ['bbox']\n    ap_result_str, ap_dict = kitti_object_eval(gt_annos, dt_annos, metric,\n                                               np.zeros(len(metric)),\n                                               np.arange(10, 10, 10),\n                                               box_type_3d=box_type_3d,\n                                               eval_types=eval_types)\n\n    ret_dict = {}\n    for i, cls in enumerate(ap_dict[0]):\n        for j, iou in enumerate(ap_dict[3][i]):\n            if cls == 'carella':\n                ret_"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        from .LiDAR_box import LiDARBox\n        return LiDARBox, \"LiDAR\"\n    elif box_type == \"Camera\":\n        from .camera_box import CameraBox\n        return CameraBox, \"Camera\"\n    elif box_type == \"Depth\":\n        from .depth_box import DepthBox\n        return DepthBox, \"Depth\"\n    else:\n        raise ValueError(\"box_type must be one of LiDAR, Camera, or Depth\")"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('You must provide a model')\n\n    if not messages:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n\n    if not all(isinstance(m, (Message, dict)) for m in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not isinstance(stream, bool):\n      raise TypeError('stream must be a bool')\n\n    if format not in ('', 'json'):\n      raise TypeError('format must be either \"\" or \"json\"')\n\n    if options is not None and not isinstance(options, dict):\n      raise TypeError('options must be a dict')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError('keep_alive must be a float or str')\n\n    if not isinstance(model, str):\n      raise TypeError('model must be a str')\n\n    if not model:\n      raise RequestError('You must provide a model')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n\n    if not all(isinstance(m, (Message, dict)) for m in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not isinstance(stream, bool):\n      raise TypeError('stream must be a bool')\n\n    if format not in ('', 'json'):\n      raise TypeError('format must be either \"\" or \"json\"')\n\n    if options is not None and not isinstance(options, dict):\n      raise TypeError('options must be a dict')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError('keep_alive must be a float or str')\n\n    if not isinstance(model, str):\n      raise TypeError('model must be a str')\n\n    if not model:\n      raise RequestError('You must provide a model')"}
{"namespace": "ollama._client.Client.pull", "completion": "    if stream:\n      return self._pull_stream(model, insecure)\n    else:\n      return self._pull(model, insecure)\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not isinstance(context, (list, tuple)):\n      context = []\n\n    if not isinstance(images, (list, tuple)):\n      images = []\n\n    if not isinstance(options, dict):\n      options = {}\n\n    if keep_alive is not None:\n      keep_alive = float(keep_alive)\n\n    if not isinstance(keep_alive, (float, int)):\n      keep_alive = None\n\n    if keep_alive is not None and keep_alive < 0:\n      keep_alive = None\n\n    if keep_alive is not None and keep_alive > 10:\n      keep_alive = 10\n\n    if keep_alive is not None:\n      keep_alive = str(keep_alive)\n\n    if not isinstance(format, str):\n      format = ''\n\n    if format not in ('', 'json'):\n      raise ValueError('format must be either an empty string or \"json\"')\n\n    if not isinstance(stream, bool):\n      stream = False\n\n    if not isinstance(raw, bool):\n      raw = False\n\n    if not isinstance(prompt, str):\n      prompt = ''\n\n    if not isinstance(system, str):\n      system = ''\n\n    if not isinstance(template, str):\n      template = ''\n\n    if not isinstance(images, list):\n      images = []\n\n    if not isinstance(options, dict):\n      options = {}\n\n    if not isinstance(context, list):\n      context = []\n\n    if not isinstance(keep_alive, (str, type(None))):\n      keep_alive = str(keep_alive)\n\n    if not isinstance(format, str):\n      format = ''\n\n    if not isinstance(stream, bool):\n      stream = False\n\n    if not isinstance(raw, bool):\n      raw = False\n\n    if not isinstance(prompt, str):\n      prompt = ''"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self._stream(\n        self.post,\n        \"/api/push\",\n        model=model,\n        insecure=insecure,\n      )\n    return self._execute(\n      self.post,\n      \"/api/push\",\n      model=model,\n      insecure=insecure,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\n        \"Either `path` or `modelfile` is required for the operation.\"\n      )\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      return self._stream_request(\n        method=\"POST\",\n        path=f\"/models/{model}\",\n        data=modelfile,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n        response_class=ProgressResponse,\n      )\n\n    return self._request(\n      method=\"POST\",\n      path=f\"/models/{model}\",\n      data=modelfile,\n      headers={\"Content-Type\": \"application/octet-stream\"},\n      response_class=ProgressResponse,\n    )\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      data = f.read()\n    digest = 'sha256:' + hashlib.sha256(data).hexdigest()\n    url = self.url + '/v2/' + self.name + '/blobs/' + digest\n    r = self.session.head(url)\n    if r.status_code == 404:\n      r = self.session.post(url, data=data, headers={'Content-Type': 'application/octet-stream'})\n      r.raise_for_status()\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not isinstance(model, str):\n      raise TypeError('model must be a string')\n\n    if not isinstance(prompt, str):\n      raise TypeError('prompt must be a string')\n\n    if not isinstance(system, str):\n      raise TypeError('system must be a string')\n\n    if not isinstance(template, str):\n      raise TypeError('template must be a string')\n\n    if context is not None and not isinstance(context, Sequence):\n      raise TypeError('context must be a sequence')\n\n    if not isinstance(stream, bool):\n      raise TypeError('stream must be a boolean')\n\n    if not isinstance(raw, bool):\n      raise TypeError('raw must be a boolean')\n\n    if format not in ('', 'json'):\n      raise ValueError('format must be either empty or \"json\"')\n\n    if images is not None and not isinstance(images, Sequence):\n      raise TypeError('images must be a sequence')\n\n    if options is not None and not isinstance(options, Options):\n      raise TypeError('options must be an instance of Options')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError('keep_alive must be a float or a string')\n\n    if stream:\n      async def stream_generator():\n        async for chunk in self.stream(\n          model=model,\n          prompt=prompt,\n          system=system,\n          template=template,\n          context=context,\n          raw=raw,\n          format=format,\n          images=images,\n          options=options,\n          keep_alive=keep_alive,\n        ):\n          yield chunk\n\n      return stream_generator()\n\n    else:\n      return await self.request(\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        raw=raw,\n        format=format,\n        images=images,\n        options"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if insecure:\n      url = f\"{self.base_url}/{model}/pull\"\n    else:\n      url = f\"{self.base_url}/{model}/pull\"\n\n    async with self.session.get(url, stream=stream) as response:\n      if response.status != 200:\n        raise ResponseError(response.status, await response.text())\n\n      if stream:\n        async for line in response.content:\n          yield ProgressResponse.parse_raw(line)\n      else:\n        return ProgressResponse.parse_raw(await response.text()).dict()\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if messages is None:\n      messages = []\n\n    for message in messages:\n      if not isinstance(message, dict):\n        raise ValueError('messages must be a sequence of dictionaries')\n      if 'role' not in message:\n        raise ValueError('messages must have a role field')\n      if 'content' not in message:\n        raise ValueError('messages must have a content field')\n      if 'images' in message and not isinstance(message['images'], list):\n        raise ValueError('messages.images must be a list')\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be either \"\" or \"json\"')\n\n    if options is None:\n      options = {}\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, str):\n        try:\n          keep_alive = float(keep_alive)\n        except ValueError:\n          raise ValueError('keep_alive must be a float or a string representing a float')\n      elif not isinstance(keep_alive, float):\n        raise ValueError('keep_alive must be a float or a string representing a float')\n\n    if not stream:\n      return await self.request(\n        'chat',\n        model=model,\n        messages=messages,\n        format=format,\n        options=options,\n        keep_alive=keep_alive,\n      )\n\n    async def stream_chat():\n      async for chunk in self.stream_request(\n        'chat',\n        model=model,\n        messages=messages,\n        format=format,\n        options=options,\n        keep_alive=keep_alive,\n      ):\n        yield chunk\n\n    return stream_chat()\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    # Send the request\n    response = await self.post(\n      '/api/push',\n      json={\n        'model': model,\n        'insecure': insecure,\n      },\n      stream=stream,\n    )\n\n    # Handle the response\n    if stream:\n      async for line in response.iter_lines():\n        yield ProgressResponse.parse_raw(line)\n    else:\n      return ProgressResponse.parse_raw(await response.text())\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    with open(path, 'rb') as f:\n      file_hash = hashlib.sha256()\n      while True:\n        chunk = f.read(4096)\n        if not chunk:\n          break\n        file_hash.update(chunk)\n    digest = f'sha256:{file_hash.hexdigest()}'\n\n    # Check if the blob already exists on the server\n    response = await self.head(f'{self.base_url}/v2/{self.name}/blobs/{digest}')\n    if response.status_code == 200:\n      return digest\n\n    # Upload the file to the server in chunks\n    with open(path, 'rb') as f:\n      while True:\n        chunk = f.read(4096)\n        if not chunk:\n          break\n        response = await self.post(f'{self.base_url}/v2/{self.name}/blobs/uploads/', data=chunk, headers={'Content-Type': 'application/octet-stream'})\n        if response.status_code != 200:\n          raise Exception(f'Failed to upload chunk: {response.text}')\n\n    return digest\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        combined_code = user_code + \"\\n\" + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as temp_file:\n            temp_file.write(combined_code.encode())\n            temp_file.flush()\n\n            # Run Pyright on the temporary file\n            result = subprocess.run(\n                [\"pyright\", temp_file.name], capture_output=True, text=True\n            )\n\n            # Delete the temporary file\n            os.unlink(temp_file.name)\n\n        # Check if the type check passed\n        passed = result.returncode == 0\n\n        # Extract the expected type errors from the Pyright output\n        expected_errors = cls._extract_expected_errors(result.stdout)\n\n        # Create a message detailing the outcome of the type check\n        message = (\n            \"Type check passed.\"\n            if passed\n            else f\"Type check failed. Expected errors: {expected_errors}\"\n        )\n\n        # Return the result of the type check\n        return TypeCheckResult(passed, message)\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError(\"Either path or modelfile must be specified\")\n\n    if stream:\n      return self._stream_request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=modelfile,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n      )\n\n    return await self._request(\n      \"POST\",\n      f\"/models/{model}\",\n      data=modelfile,\n      headers={\"Content-Type\": \"application/octet-stream\"},\n    )\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return compile_module(fn, forward_compiler, backward_compiler)\n    else:\n        return compile_function(fn, forward_compiler, backward_compiler)\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary.csv file\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n\n    # Find the row with the highest score\n    best_row = summary_df.loc[summary_df[\"score\"].idxmax()]\n\n    # Extract the pipeline configuration from the best row\n    best_config = best_row.to_dict()\n\n    # Remove the score and trial_id keys\n    best_config.pop(\"score\", None)\n    best_config.pop(\"trial_id\", None)\n\n    # Save the best configuration to a YAML file if output_path is provided\n    if output_path is not None:\n        # Check if the output path has a valid file extension\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            raise ValueError(\"Invalid output file extension. Please use .yaml or .yml.\")\n\n        # Save the best configuration to the output path\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config, f)\n\n    return best_config\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    def wrapper(*args, **kwargs):\n        nonlocal ts_compiler\n        if ts_compiler is None:\n            ts_compiler = torch.jit.trace\n        if isinstance(func, torch.nn.Module):\n            func = func.forward\n        with lock:\n            if func not in cache:\n                cache[func] = ts_compiler(func, *args, **kwargs_)\n            return cache[func](*args, **kwargs)\n\n    return wrapper\n\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Load the best configuration from the trial folder\n        config = js['class_name'].load_best_config(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the extracted configuration and project directory\n        return cls(config, project_dir)\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize variables\n    best_result = None\n    best_result_index = None\n    best_result_time = None\n    best_result_metrics = None\n    best_result_module = None\n    best_result_params = None\n    best_result_summary = None\n\n    # Iterate over each module and its parameters\n    for i, (module, params) in enumerate(zip(modules, module_params)):\n        # Run the module with the given parameters\n        result = module(**params)\n\n        # Measure the execution time\n        start_time = time.time()\n        result = module(**params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Evaluate the result using the specified metrics\n        metrics = evaluate_result(result, previous_result, strategies['metrics'])\n\n        # Check if the result is better than the current best result\n        if best_result is None or metrics > best_result_metrics:\n            best_result = result\n            best_result_index = i\n            best_result_time = execution_time\n            best_result_metrics = metrics\n            best_result_module = module\n            best_result_params = params\n\n    # Save the best result to disk\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n\n    # Save the best result summary to disk\n    best_result_summary = {\n        'index': best_result_index,\n        'time': best_result_time,\n        'metrics': best_result_metrics,\n        'module': best_result_module.__name__,\n        'params': best_result_params\n    }\n    with open(os.path.join(node_line_dir, 'best_result_summary.json'), 'w') as f:\n        json.dump(best_result_summary, f)\n\n    # Combine the previous result columns with the best result columns\n    best_result = pd.concat([previous_result, best_result], axis"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize variables\n    best_result = None\n    best_result_name = None\n    best_result_time = None\n    best_result_metrics = None\n    best_result_summary = None\n\n    # Iterate over each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Get the name of the module\n        module_name = module.__name__\n\n        # Run the module with the given parameters\n        result = module(previous_result, **params)\n\n        # Measure the execution time of the module\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Evaluate the performance of the module based on the specified strategies\n        metrics = evaluate_performance(result, strategies)\n\n        # Save the results and a summary of the module's execution\n        save_results(result, node_line_dir, module_name)\n        save_summary(execution_time, metrics, node_line_dir, module_name)\n\n        # Select the best result based on the evaluation criteria\n        if best_result is None or (execution_time <= strategies['speed_threshold'] and metrics['score'] > best_result_metrics['score']):\n            best_result = result\n            best_result_name = module_name\n            best_result_time = execution_time\n            best_result_metrics = metrics\n            best_result_summary = f\"Execution time: {execution_time:.2f} seconds\\nMetrics: {metrics}\"\n\n    # Save the best result and its summary\n    save_results(best_result, node_line_dir, best_result_name)\n    save_summary(best_result_time, best_result_metrics, node_line_dir, best_result_name)\n\n    return best_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker_results'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker_results', 'summary'), exist_ok=True)\n\n    # Initialize variables\n    best_prompt_maker = None\n    best_result = None\n    best_time = float('inf')\n    best_metrics = None\n\n    # Iterate over prompt maker modules\n    for i, (module, params) in enumerate(zip(modules, module_params)):\n        # Run prompt maker module\n        start_time = time.time()\n        result = module(**params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Save result\n        result.to_csv(os.path.join(node_line_dir, 'prompt_maker_results', f'result_{i}.csv'), index=False)\n\n        # Evaluate result\n        if 'generator' in strategies:\n            generator_module = strategies['generator']['module']\n            generator_params = strategies['generator']['params']\n            result_evaluation = generator_module(result, **generator_params)\n        else:\n            result_evaluation = evaluate_result(result)\n\n        # Save evaluation\n        with open(os.path.join(node_line_dir, 'prompt_maker_results', 'summary', f'result_{i}_evaluation.txt'), 'w') as f:\n            f.write(str(result_evaluation))\n\n        # Check if result is better than previous best\n        if execution_time < best_time and result_evaluation > best_metrics:\n            best_prompt_maker = module\n            best_result = result\n            best_time = execution_time\n            best_metrics = result_evaluation\n\n    # Save best result\n    best_result.to_csv(os.path.join"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            if key in module.module_params:\n                value = module.module_params[key]\n                if value not in values:\n                    values.append(value)\n    return values\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding(model_name=\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.get_embedding(pred)\n    gt_embeddings = embedding_model.get_embeddings(generation_gt)\n\n    return max(cosine_similarity(pred_embedding, gt_embedding).item() for gt_embedding in gt_embeddings)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    # Check if the GFPGAN face restorer is set up\n    if not hasattr(gfpgan_model, 'gfpgan'):\n        # If not, log a warning and return the original image\n        logger.warning('GFPGAN face restorer is not set up. Returning original image.')\n        return np_image\n\n    # Attempt to restore faces in the image using the GFPGAN face restorer\n    try:\n        # Restore the faces in the image using the GFPGAN face restorer\n        _, _, restored_image = gfpgan_model.gfpgan.enhance(np_image, has_aligned=False, only_center_face=False, paste_back=True)\n    except Exception as e:\n        # If an error occurs, log a warning and return the original image\n        logger.warning(f'Error while restoring faces in image: {e}. Returning original image.')\n        return np_image\n\n    # Return the restored image\n    return restored_image\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from modules.face_restoration import FaceRestorerCodeFormer\n\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        print(f\"Error in setup_model: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        import sys\n        import os\n        import torch\n        import numpy as np\n        import cv2\n        import torch.nn as nn\n        import torch.nn.functional as F\n        import torchvision.transforms.functional as TF\n        from basicsr.utils.download_util import load_file_from_url\n        from facexlib.utils.face_restore_helper import FaceRestoreHelper\n        from gfpgan import GFPGANer\n        from realesrgan import RealESRGANer\n        from basicsr.archs.rrdbnet_arch import RRDBNet\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact_attention\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact_blind\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact_blind_attention\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact_blind_attention_restoreFormer\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact_blind_attention_restoreFormer_encoderEnhance\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact_blind_attention_restoreFormer_encoderEnhance_weightNorm\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact_blind_attention_restoreFormer_encoderEnhance_weightNorm_freq\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact_blind_attention_restoreFormer_encoderEnhance_weightNorm_freq_act\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact_blind_attention_restoreFormer_encoderEnhance_weight"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  v_quat = np.array([0, v[0], v[1], v[2]])\n\n  # Apply the rotation using quaternion multiplication\n  rotated_quat = quaternion_multiply(quaternion_multiply(q, v_quat), quaternion_conjugate(q))\n\n  # Convert the rotated quaternion back to a vector format\n  rotated_v = rotated_quat[1:]\n\n  return rotated_v\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Extract the axis and angle from the axis-angle vector\n  axis, angle = axis_angle[:3], axis_angle[3]\n\n  # Normalize the axis\n  axis = axis / jnp.linalg.norm(axis)\n\n  # Calculate the half angle\n  half_angle = angle / 2.0\n\n  # Calculate the real and imaginary parts of the quaternion\n  real = jnp.cos(half_angle)\n  imag = jnp.sin(half_angle) * axis\n\n  # Ensure numerical stability for small angles\n  if jnp.abs(angle) < eps:\n    real = 1.0 - angle ** 2 / 8.0\n    imag = axis * angle / 2.0\n\n  # Combine the real and imaginary parts to form the quaternion\n  quat = jnp.concatenate([real[None], imag])\n\n  return quat\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk = model.get_topk(prefix, k)\n    # get topk words\n    topk_words = [word for word, _ in topk]\n    # get topk indices\n    topk_indices = [index for _, index in topk]\n    # get topk log probabilities\n    topk_log_probs = [log_prob for _, _, log_prob in topk]\n\n    # initialize low and high bias values\n    low = -40\n    high = high\n\n    # initialize call counter\n    calls = 0\n\n    # loop until low and high bias values converge\n    while low < high:\n        # calculate mid bias value\n        mid = (low + high) // 2\n        # adjust search bias for the target index\n        model.adjust_search_bias([idx], [mid])\n        # get topk results with adjusted bias\n        topk_words_mid = model.get_topk(prefix, k)\n        # get topk words with adjusted bias\n        topk_words_mid = [word for word, _ in topk_words_mid]\n        # get topk indices with adjusted bias\n        topk_indices_mid = [index for _, index in topk_words_mid]\n        # get topk log probabilities with adjusted bias\n        topk_log_probs_mid = [log_prob for _, _, log_prob in topk_words_mid]\n\n        # check if the target index is in the topk results with adjusted bias\n        if topk_indices_mid[0] == idx:\n            # if the target index is in the topk results, update the high bias value\n            high = mid\n        else:\n            # if the target index is not in the topk results, update the low bias value\n            low = mid + 1\n\n        # increment call counter\n        calls += 1\n\n    # calculate the log probability of the target index being the top result\n    log_prob = topk_log_probs[0]\n\n    # return the log probability and the number of calls made to the model\n    return log_prob, calls"}
{"namespace": "resample.resample_3d", "completion": "  # Get the shape of the input data\n  data_shape = tf.shape(data)\n\n  # Get the shape of the locations tensor\n  location_shape = tf.shape(locations)\n\n  # Get the number of dimensions in the locations tensor\n  num_dims = location_shape.shape[0]\n\n  # Get the batch size from the first dimension of the locations tensor\n  batch_size = location_shape[0]\n\n  # Get the number of samples from the second dimension of the locations tensor\n  num_samples = location_shape[1]\n\n  # Get the number of channels from the last dimension of the data tensor\n  num_channels = data_shape[-1]\n\n  # Get the height, width, and depth of the input data\n  height, width, depth = data_shape[1], data_shape[2], data_shape[3]\n\n  # Get the maximum height, width, and depth of the input data\n  max_height, max_width, max_depth = tf.cast(height - 1, tf.float32), tf.cast(width - 1, tf.float32), tf.cast(depth - 1, tf.float32)\n\n  # Get the minimum height, width, and depth of the input data\n  min_height, min_width, min_depth = tf.cast(0.0, tf.float32), tf.cast(0.0, tf.float32), tf.cast(0.0, tf.float32)\n\n  # Get the maximum height, width, and depth of the input data plus 1\n  max_height_plus_1, max_width_plus_1, max_depth_plus_1 = max_height + 1.0, max_width + 1.0, max_depth + 1.0\n\n  # Get the minimum height, width, and depth of the input data minus 1\n  min_height_minus_1, min_width_minus_1, min_depth_minus_1 = min_height - 1.0, min_width - 1.0"}
{"namespace": "math.plus_eps", "completion": "  import numpy as np\n\n  tiny = np.finfo(x.dtype).tiny\n  x[x < tiny] = tiny\n  x[x < np.finfo(x.dtype).tiny] = np.finfo(x.dtype).tiny\n  return x\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-15\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n\n"}
{"namespace": "math.safe_exp", "completion": "  # Define the safe range for the exponential function\n  safe_range = (-50, 50)\n\n  # Define the custom gradient function for backpropagation\n  def custom_grad(dy):\n    # Check if the input is within the safe range\n    if x.numpy() > safe_range[0] and x.numpy() < safe_range[1]:\n      # If the input is within the safe range, use the gradient of the exponential function\n      return dy * tf.exp(x)\n    else:\n      # If the input is outside the safe range, use a constant gradient of 0\n      return dy * 0.0\n\n  # Define the safe exponential function using the custom gradient function\n  safe_exp_fn = lambda x: tf.exp(x)\n  safe_exp_fn.gradient = custom_grad\n\n  # Apply the safe exponential function to the input x\n  return safe_exp_fn(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  safe_log = generate_safe_fn(jnp.log, jnp.zeros_like(x), jnp.log(jnp.exp(1) - 1))\n  return safe_log(x)\n"}
{"namespace": "math.safe_sqrt", "completion": "  import jax\n  import jax.numpy as jnp\n\n  # Define the maximum value for the input\n  max_value = 1e12\n\n  # Define the safe square root function\n  def safe_sqrt_fn(x):\n    # Clamp the input between 0 and the maximum value\n    x = jnp.clip(x, 0, max_value)\n    # Compute the square root of the clamped input\n    return jnp.sqrt(x)\n\n  # Define the custom gradient function for the safe square root function\n  def safe_sqrt_grad(dy, variables, parameters):\n    # Extract the input from the variables\n    x, = variables\n    # Compute the gradient of the square root function\n    return 0.5 * dy / jnp.sqrt(jnp.clip(x, 0, max_value))\n\n  # Create a version of the safe square root function with the custom gradient\n  safe_sqrt_with_grad = jax.custom_vjp(safe_sqrt_fn)\n  safe_sqrt_with_grad.defvjp(safe_sqrt_grad)\n\n  # Return the safe square root function with the custom gradient\n  return safe_sqrt_with_grad(x)\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p < 0:\n    return -1\n  else:\n    return 1\n\n"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  from scipy.spatial import Delaunay\n\n  # Define the vertices of the starting polyhedron\n  if base_shape == \"tetrahedron\":\n    vertices = np.array(\n        [\n            [1, 1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n            [-1, -1, 1],\n        ]\n    )\n  elif base_shape == \"icosahedron\":\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, -1],\n        ]\n    )\n  elif base_shape == \"octahedron\":\n    vertices = np.array(\n        [\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1],\n        ]\n    )\n  else:\n    raise ValueError(\"Invalid base shape. Must be 'tetrahedron', 'icosahedron', or 'octahedron'.\")\n\n  # Tessellate the polyhedron\n  for _ in range(angular_tesselation):\n    tri = Delaunay(vertices)\n    vertices = np.concatenate([vertices, np.mean(vertices[tri.simplices], axis=1)])\n\n  # Normalize the vertices\n "}
{"namespace": "math.safe_log1p", "completion": "  return jnp.where(x > -1.0, jnp.log1p(x), jnp.log1p(1.0e-8))\n"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n\n  if premult is None:\n    premult = 1\n\n  if postmult is None:\n    postmult = 1\n\n  if p == 1:\n    return x\n  elif p == 0:\n    return np.log(x * premult + 1) * postmult\n  elif p == -np.inf:\n    return -np.log(-x * premult + 1) * postmult\n  elif p == np.inf:\n    return np.exp(x * premult) * postmult - 1\n  else:\n    return (np.sign(x) * np.abs(x * premult) ** p) * postmult\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 1:\n    return y\n  elif p == 2:\n    return y.sqrt()\n  elif p == 3:\n    return y.cbrt()\n  elif p == 4:\n    return y.sqrt().sqrt()\n  elif p == 5:\n    return y.sqrt().cbrt()\n  elif p == 6:\n    return y.cbrt().sqrt()\n  elif p == 7:\n    return y.cbrt().cbrt()\n  elif p == 8:\n    return y.sqrt().sqrt().sqrt()\n  elif p == 9:\n    return y.sqrt().sqrt().cbrt()\n  elif p == 10:\n    return y.sqrt().cbrt().sqrt()\n  elif p == 11:\n    return y.sqrt().cbrt().cbrt()\n  elif p == 12:\n    return y.cbrt().sqrt().sqrt()\n  elif p == 13:\n    return y.cbrt().sqrt().cbrt()\n  elif p == 14:\n    return y.cbrt().cbrt().sqrt()\n  elif p == 15:\n    return y.cbrt().cbrt().cbrt()\n  elif p == 16:\n    return y.sqrt().sqrt().sqrt().sqrt()\n  elif p == 17:\n    return y.sqrt().sqrt().sqrt().cbrt()\n  elif p == 18:\n    return y.sqrt().sqrt().cbrt().sqrt()\n  elif p == 19:\n    return y.sqrt().sqrt().cbrt().cbrt()\n  elif p == 20:\n    return y.sqrt().cbrt().sqrt().sqrt()\n  elif p == 21:\n    return y.sqrt().cbrt().sqrt().cbrt()\n  elif p == 22:\n    return y.sqrt().cbrt().cbrt().sqrt()\n  elif p == 23:\n    return y.sqrt().cbrt().cbrt().cbrt()\n  elif p == 24:\n    return y.cbrt().sqrt().sqrt"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * np.sin(\n        0.5 * np.pi * np.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  t = np.clip(step / max_steps, 0, 1)\n  log_lerp = np.exp(np.log(lr_init) * (1 - t) + np.log(lr_final) * t)\n  return delay_rate * log_lerp\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  if camtype != ProjectionType.PERSPECTIVE:\n    raise NotImplementedError(\n        f\"{camtype} projection is not implemented.\"\n    )\n\n  if distortion_params is not None:\n    raise NotImplementedError(\n        \"Distortion is not implemented.\"\n    )\n\n  # points: [..., 3]\n  # pixtocams: [..., 3, 3]\n  # camtoworlds: [..., 3, 4]\n  # coordinates: [..., 2]\n  # depth: [...]\n\n  # Convert to float32\n  points = xnp.asarray(points, dtype=xnp.float32)\n  pixtocams = xnp.asarray(pixtocams, dtype=xnp.float32)\n  camtoworlds = xnp.asarray(camtoworlds, dtype=xnp.float32)\n\n  # Convert from world to camera coordinates\n  points = xnp.matmul(\n      xnp.linalg.inv(camtoworlds[..., :3, :3]),\n      points[..., None],\n  )[..., 0]\n\n  # Project to pixel coordinates\n  coordinates = xnp.matmul(\n      pixtocams,\n      points[..., None],\n  )[..., 0]\n  coordinates = coordinates[..., :2] / coordinates[..., 2:3]\n\n  # Calculate depth\n  depth = points[..., 2]\n\n  return coordinates, depth\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    theta = eps\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:]\n  w_hat = skew(w)\n  R = jnp.eye(3) + jnp.sin(theta) * w_hat + (1 - jnp.cos(theta)) * w_hat @ w_hat\n  p = (jnp.eye(3) - R) @ (w_hat @ v) + w * w.T @ v * theta\n  return jnp.vstack((jnp.hstack((R, p.reshape((3, 1))), jnp.array([[0, 0, 0, 1]]))))\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the norm of the axis-angle vector\n  theta = jnp.linalg.norm(axis_angle)\n\n  # Check if the norm is less than eps\n  if theta < eps:\n    # If the norm is less than eps, return the identity matrix\n    return jnp.eye(3)\n\n  # Compute the skew-symmetric matrix of the axis-angle vector\n  axis_angle_skew = skew_symmetric(axis_angle)\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(theta) * axis_angle_skew + (1 - jnp.cos(theta)) * jnp.dot(axis_angle_skew, axis_angle_skew)\n\n  # Return the rotation matrix\n  return R\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean of the Gaussian distribution\n  mu = ((t0 + 0.5) + (t1 - 0.5)) * d\n\n  # Calculate the covariance matrix of the Gaussian distribution\n  h = t1 - t0\n  t_mean = (t0 + t1) / 2.0\n  r2 = base_radius * t_mean\n  r2_mean = jnp.mean(jnp.square(r2))\n\n  # Calculate the covariance matrix based on the diagonal or full-covariance assumption\n  if diag:\n    cov = jnp.diag(jnp.square((0.5 * h * r2) / 3.0))\n  else:\n    cov = jnp.diag(jnp.square(jnp.array([0.5 * h * r2_mean, 0.5 * h * r2_mean, 0.5 * h * r2_mean])))\n\n  return mu, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  import jax.numpy as jnp\n  from .utils import lift_gaussian\n\n  mean, cov = lift_gaussian(d, t0, t1, radius)\n\n  if diag:\n    return mean, jnp.diag(cov)\n  else:\n    return mean, cov\n\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Get the shape of the input pixel coordinates\n  sh = pix_x_int.shape\n\n  # Compute the xy coordinates of the pixel centers\n  pix_x = pix_x_int + 0.5\n  pix_y = pix_y_int + 0.5\n\n  # Compute the xy coordinates of the pixel centers in NDC space\n  if pixtocam_ndc is not None:\n    pix_x_ndc = (pix_x - pixtocam_ndc[0, 2]) / pixtocam_ndc[0, 0]\n    pix_y_ndc = (pix_y - pixtocam_ndc[1, 2]) / pixtocam_ndc[1, 1]\n\n  # Compute the xy coordinates of the pixel centers in camera space\n  if distortion_params is not None:\n    # Apply lens distortion correction\n    r2 = pix_x_ndc ** 2 + pix_y_ndc ** 2\n    pix_x_corrected = (\n        pix_x_ndc * (1 + distortion_params[\"k1\"] * r2 + distortion_params[\"k2\"] * r2 ** 2 + distortion_params[\"k3\"] * r2 ** 3)\n        + 2 * distortion_params[\"p1\"] * pix_x_ndc * pix_y_ndc\n        + distortion_params[\"p2\"] * (r2 + 2 * pix_x_ndc ** 2)\n    )\n    pix_y_corrected = (\n        pix_y_ndc * (1 + distortion_params[\"k1\"] * r2 + distortion_params[\"k2\"] * r2 ** 2 + distortion_params[\"k3\"] * r2 ** 3)\n        + distortion_params[\"p1\"] * (r2 + 2 * pix_y_ndc ** 2)\n        + 2 * distortion_params[\"p2\"] * pix_x_ndc * pix_y_ndc\n    )\n    pix_x_cam = pix_x_correct"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the product of density and the adjusted distance between points\n  adjusted_density = density * tdist\n\n  # Call the helper function to compute the alpha weights\n  return compute_alpha_weights_helper(adjusted_density, dirs, **kwargs)\n\n"}
{"namespace": "stepfun.sample", "completion": "  # Compute the weights of each bin\n  w = jax.nn.softmax(w_logits)\n\n  # Compute the bin widths\n  bin_widths = t[1:] - t[:-1]\n\n  # Compute the cumulative sum of the weights\n  cdf = jnp.cumsum(w, axis=-1)\n\n  # Compute the total weight\n  total_weight = cdf[..., -1:]\n\n  # Compute the bin centers\n  bin_centers = t[:-1] + 0.5 * bin_widths\n\n  # Compute the bin edges\n  bin_edges = jnp.concatenate([t[..., :1], t[..., 1:]], axis=-1)\n\n  # Compute the bin indices\n  bin_indices = jnp.arange(w.shape[-1])\n\n  # Compute the bin centers and edges for each sample\n  if rng is None:\n    # Deterministic sampling\n    if deterministic_center:\n      # Sample from the center of each bin\n      u = bin_centers\n    else:\n      # Sample uniformly from the bin edges\n      u = jnp.linspace(0.0, 1.0, num_samples)\n      u = u[None, ...]\n      u = u + jnp.zeros(w.shape[:-1] + (num_samples,))\n      u = u * bin_widths[..., None] + bin_edges[..., :-1, None]\n  else:\n    # Random sampling\n    if single_jitter:\n      # Sample uniformly from the bin edges and add the same jitter to all samples\n      u = jax.random.uniform(rng, w.shape[:-1] + (num_samples,))\n      u = u * (bin_edges[..., 1:] - bin_edges[..., :-1]) + bin_edges[..., :-1]\n      u = u + jax.random.uniform(rng, w.shape[:-1] + (num"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Calculate the midpoints between adjacent bin endpoints\n  midpoints = (t[..., 1:] + t[..., :-1]) / 2\n\n  # Calculate the weights of the midpoints\n  w_logits_mid = (w_logits[..., 1:] + w_logits[..., :-1]) / 2\n\n  # Calculate the CDF of the midpoints\n  cdf_mid = jax.nn.softmax(w_logits_mid, axis=-1)\n\n  # Calculate the CDF of the bin endpoints\n  cdf = jax.nn.softmax(w_logits, axis=-1)\n\n  # Calculate the CDF of the midpoints\n  cdf_mid = jax.nn.softmax(w_logits_mid, axis=-1)\n\n  # Calculate the CDF of the bin endpoints\n  cdf = jax.nn.softmax(w_logits, axis=-1)\n\n  # Calculate the CDF of the midpoints\n  cdf_mid = jax.nn.softmax(w_logits_mid, axis=-1)\n\n  # Calculate the CDF of the bin endpoints\n  cdf = jax.nn.softmax(w_logits, axis=-1)\n\n  # Calculate the CDF of the midpoints\n  cdf_mid = jax.nn.softmax(w_logits_mid, axis=-1)\n\n  # Calculate the CDF of the bin endpoints\n  cdf = jax.nn.softmax(w_logits, axis=-1)\n\n  # Calculate the CDF of the midpoints\n  cdf_mid = jax.nn.softmax(w_logits_mid, axis=-1)\n\n  # Calculate the CDF of the bin endpoints\n  cdf = jax.nn.softmax(w_logits, axis=-1)\n\n  # Calculate the CDF of the midpoints\n  cdf_mid = jax.nn.softmax(w"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  assert abs(sum(w) - 1.0) < 1e-6, \"Weights do not sum to 1\"\n\n  # Sort the values and weights based on the values\n  ind_sort = np.argsort(t)\n  t_sorted = t[ind_sort]\n  w_sorted = w[ind_sort]\n\n  # Calculate the cumulative sum of the weights\n  w_cumsum = np.cumsum(w_sorted)\n\n  # Interpolate the cumulative sum of the weights to find the percentiles\n  ps_interp = np.interp(ps, w_cumsum * 100, t_sorted)\n\n  return ps_interp\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Convert the histogram to a probability density function (PDF)\n  pdf = w / np.trapz(w, t)\n\n  # Blur the PDF using a Gaussian kernel\n  blur_sigma = blur_halfwidth / 2.355  # Convert half-width to standard deviation\n  blur_kernel = np.exp(-0.5 * (np.arange(-blur_halfwidth, blur_halfwidth + 1) / blur_sigma) ** 2)\n  blur_kernel /= np.sum(blur_kernel)\n  pdf_blurred = np.convolve(pdf, blur_kernel, mode='same')\n\n  # Resample the blurred PDF to match the new time points\n  pdf_blurred_interp = interp1d(t, pdf_blurred, kind='linear', fill_value=0, bounds_error=False)\n  pdf_blurred_resampled = pdf_blurred_interp(tq)\n\n  # Normalize the resampled PDF to obtain the resampled weights\n  wq = pdf_blurred_resampled / np.trapz(pdf_blurred_resampled, tq)\n\n  return wq\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Check that the transform is a homogeneous transformation matrix\n  if transform.shape[0] != transform.shape[1]:\n    raise ValueError(\"The transformation matrix must be a square matrix.\")\n  if transform.shape[0] != vectors.shape[-1] + 1:\n    raise ValueError(\"The transformation matrix must have one more dimension than the vectors.\")\n\n  # Add a column of ones to the vectors\n  ones = np.ones((vectors.shape[0], 1))\n  vectors_with_ones = np.concatenate((vectors, ones), axis=1)\n\n  # Apply the transformation\n  transformed_vectors_with_ones = np.dot(vectors_with_ones, transform)\n\n  # Remove the column of ones\n  transformed_vectors = transformed_vectors_with_ones[:, :-1]\n\n  return transformed_vectors\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Check if tp and vp have the same shape\n  if tp.shape != vp.shape:\n    raise ValueError(\"tp and vp must have the same shape\")\n\n  # Check if tp is sorted\n  if not torch.all(tp[:-1] <= tp[1:]):\n    raise ValueError(\"tp must be sorted\")\n\n  # Check if t is sorted\n  if not torch.all(t[:-1] <= t[1:]):\n    raise ValueError(\"t must be sorted\")\n\n  # Check if tp and t are compatible\n  if tp[0] > t[0] or tp[-1] < t[-1]:\n    raise ValueError(\"t and tp must have compatible ranges\")\n\n  # Find the indices of the intervals in tp that contain the endpoints of the intervals in t\n  tp_indices = torch.searchsorted(tp, t, right=True) - 1\n\n  # Compute the width of each interval in t\n  t_widths = t[1:] - t[:-1]\n\n  # Compute the width of each interval in tp\n  tp_widths = tp[1:] - tp[:-1]\n\n  # Compute the width of each interval in tp that overlaps with each interval in t\n  tp_overlap_widths = torch.clamp(torch.min(t_widths, tp_widths[tp_indices]), min=0)\n\n  # Compute the values of the resampled step function at the endpoints of each interval in t\n  v = vp[tp_indices]\n\n  # Compute the values of the resampled step function at the new intervals in t\n  if use_avg:\n    v = v * tp_overlap_widths / t_widths\n  else:\n    v = v * tp_overlap_widths\n\n  return v\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  scaled_mean = mean[Ellipsis, None, :] / (2 ** (jnp.arange(min_deg, max_deg) / 2))\n  scaled_var = var[Ellipsis, None, :] / (2 ** (jnp.arange(min_deg, max_deg) / 2))\n\n  # Concatenate the scaled mean and variance\n  scaled_vars = jnp.concatenate([jnp.sin(scaled_mean), jnp.cos(scaled_var)], axis=-1)\n\n  # Flatten the scaled variables\n  flattened_vars = jnp.reshape(scaled_vars, list(scaled_vars.shape[:-2]) + [-1])\n\n  return flattened_vars"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Importing the required libraries\n  import numpy as np\n  import torch\n  import torch.nn as nn\n  import torch.nn.functional as F\n\n  # Defining the function that generates the integrated directional encoding function\n  def integrated_dir_enc_fn(deg_view):\n\n    \"\"\"\n    Generates an integrated directional encoding function based on the specified number of spherical harmonics degrees.\n\n    Input-Output Arguments\n    :param deg_view: Int. The number of spherical harmonics degrees to use for generating the integrated directional encoding function. It determines the complexity and accuracy of the encoding.\n    :return: Function. A function that takes a 3D point (or points) as input and returns its integrated directional encoding.\n\n    \"\"\"\n\n    # Defining the function that evaluates the integrated directional encoding for a given input\n    def integrated_dir_enc(input):\n\n      \"\"\"\n      Evaluates the integrated directional encoding for a given input.\n\n      Input-Output Arguments\n      :param input: Tensor. A 3D point (or points) for which the integrated directional encoding needs to be computed.\n      :return: Tensor. The integrated directional encoding for the given input.\n\n      \"\"\"\n\n      # Computing the integrated directional encoding using the specified number of spherical harmonics degrees\n      x, y, z = torch.split(input, 1, dim=-1)\n      direction = torch.atan2(y, x)\n      horizon = torch.sqrt(x ** 2 + y ** 2)\n      elevation = torch.atan2(z, horizon)\n      azimuth = torch.arange(deg_view, dtype=input.dtype, device=input.device)\n      azimuth = torch.reshape(azimuth, [1, ] + [1, ] * (elevation.dim() - 1) + [-1, ])\n      azimuth = azimuth.expand([input.shape[0], ] + list(elevation.shape[1:-1]) + [-1, ])\n      y_ = torch."}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    block_index = 0\n    block_type = None\n    block_text = \"\"\n    block_start_index = None\n    block_list = []\n    block_header_index = None\n    block_level = 0\n\n    # Loop through each line in the input list\n    for i, line in enumerate(lines):\n\n        # Check if the line is a header\n        if line.startswith(\"#\"):\n            # If the current block is not empty, append it to the result list\n            if block_text:\n                result.append({\n                    \"index\": block_index,\n                    \"text\": block_text,\n                    \"type\": block_type,\n                    \"start_index\": block_start_index,\n                    \"list\": block_list,\n                    \"header_index\": block_header_index,\n                    \"level\": block_level\n                })\n\n            # Reset variables for a new block\n            block_index += 1\n            block_type = \"header\"\n            block_text = line.strip()\n            block_start_index = i\n            block_list = []\n            block_header_index = block_index\n            block_level = line.count(\"#\")\n\n        # Check if the line is a list item\n        elif line.startswith(\"-\") or line.startswith(\"*\") or line.startswith(\"+\"):\n            # If the current block is not empty, append it to the result list\n            if block_text:\n                result.append({\n                    \"index\": block_index,\n                    \"text\": block_text,\n                    \"type\": block_type,\n                    \"start_index\": block_start_index,\n                    \"list\": block_list,\n                    \"header_index\": block_header_index,\n                    \"level\": block_level\n                })\n\n            # Reset variables for a new block\n            block_index += 1\n            block_type = \"list\"\n            block_text = line.strip()\n            block_start_index = i\n            block_list = []\n            block_header_index = None\n            block"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # Apply space rule\n    space_rule = re.compile(r'([.?!]+) +(?=[A-Z])')\n    org_texts = space_rule.sub(r'\\1\\n\\n', org_texts)\n\n    # Apply bracket rule\n    bracket_rule = re.compile(r'([.?!]+) +(?=[A-Z])')\n    org_texts = bracket_rule.sub(r'\\1\\n\\n', org_texts)\n\n    # Apply rules\n    rules = [\n        (r'([.?!]+) +(?=[A-Z])', r'\\1\\n\\n'),\n        (r'([.?!]+) +(?=[A-Z])', r'\\1\\n\\n'),\n        (r'([.?!]+) +(?=[A-Z])', r'\\1\\n\\n'),\n        (r'([.?!]+) +(?=[A-Z])', r'\\1\\n\\n'),\n        (r'([.?!]+) +(?=[A-Z])', r'\\1\\n\\n'),\n        (r'([.?!]+) +(?=[A-Z])', r'\\1\\n\\n'),\n        (r'([.?!]+) +(?=[A-Z])', r'\\1\\n\\n'),\n        (r'([.?!]+) +(?=[A-Z])', r'\\1\\n\\n'),\n        (r'([.?!]+) +(?=[A-Z])', r'\\1\\n\\n'),\n        (r'([.?!]+) +(?=[A-Z])', r'\\1\\n\\n'),\n        (r'([.?!]+) +(?=[A-Z])', r'\\1\\n\\n'),\n        (r'([.?!]+) +(?=[A-Z])', r'\\1\\n\\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return [self.positions(token, key=k) for k in self.keys()]\n        else:\n            return self.get(key).positions(token)\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith('%'):\n        percentage = float(spec[:-1])\n        return int(num_clauses * percentage / 100)\n    elif spec.startswith('<'):\n        threshold = int(spec[1:])\n        return num_clauses - threshold\n    else:\n        return int(spec)\n\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_general(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = array[:truncate]\n\n        if batch_size:\n            array = cls.batch(array, batch_size)\n\n        if avoid_copies:\n            array = cls.avoid_copies(array)\n\n        term_matrix, positions, term_dict, avg_doc_len, doc_lens = cls.build_index(array, tokenizer)\n\n        return cls(term_matrix, positions, term_dict, avg_doc_len, doc_lens)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = None\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.server = socketserver.TCPServer((self.config['host'], self.config['port']), ProxifierMessageInterceptorHandler)\n        self.server.config = self.config\n        self.server.logger = self.logger\n        self.server.connections = self.connections\n        self.server.lock = self.lock\n\n        self.server.serve_forever()\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & 0x5555555555555555\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n    arr = (arr + (arr >> 4)) & 0x0f0f0f0f0f0f0f0f\n    arr += arr >> 8\n    arr += arr >> 16\n    arr += arr >> 32\n    return arr & 0x7f\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Initialize the query parser\n    parser = QueryParser(q, q_op=q_op, schema=Schema(qf))\n\n    # Parse the query\n    query = parser.parse()\n\n    # Initialize the query object\n    q_obj = Query(query, schema=Schema(qf))\n\n    # Initialize the term-centric query object\n    q_obj_term = Query(query, schema=Schema(qf))\n\n    # Initialize the field-centric query object\n    q_obj_field = Query(query, schema=Schema(qf))\n\n    # Initialize the term-centric query object with phrase matches\n    if pf is not None:\n        q_obj_term.add_phrase_fields(pf)\n\n    # Initialize the term-centric query object with bigram matches\n    if pf2 is not None:\n        q_obj_term.add_phrase_fields(pf2, 2)\n\n    # Initialize the term-centric query object with trigram matches\n    if pf3 is not None:\n        q_obj_term.add_phrase_fields(pf3, 3)\n\n    # Initialize the field-centric query object with phrase matches\n    if pf is not None:\n        q_obj_field.add_phrase_fields(pf)\n\n    # Initialize the field-centric query object with bigram matches\n    if pf2 is not None:\n        q_obj_field.add_phrase_fields(pf2, 2)\n\n    # Initialize the field-centric query object with trigram matches\n    if pf3 is not None:\n        q_obj_field.add_phrase_fields(pf3, 3)\n\n    # Initialize the term-centric query object with minimum match specification\n    if mm is not None:\n        q_obj_term.set_min_should_match(mm)\n\n    # Initialize the field-centric query object with minimum match specification\n    if mm is not None:\n        q_obj_field.set_min_should_match(mm)\n\n    # Init"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.handle_close()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n        if self.server:\n            self.server.stop()\n"}
