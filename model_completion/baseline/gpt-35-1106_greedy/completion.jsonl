{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    import numpy as np\n    from typing import Callable\n    def validator(cls, value):\n        shape1 = getattr(value, field1).shape\n        shape2 = getattr(value, field2).shape\n        if shape1 != shape2:\n            raise ValueError(f\"The shapes of {field1} and {field2} do not match: {shape1} != {shape2}\")\n        return value\n    return validator"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(*args, **kwargs)\n\n        # Main algorithm run method\n        result = self.run(*args, **kwargs)\n\n        # Post-execution hooks\n        for callback in self._callbacks:\n            callback.post_execute(result, *args, **kwargs)\n\n        return result"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        from typing import Any\n        import json\n        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except (json.JSONDecodeError, TypeError):\n            return False"}
{"namespace": "tanuki.utils.encode_int", "completion": "    char_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n\n    # Encode the integer into a single character\n    encoded_char = char_set[n % len(char_set)]\n\n    return encoded_char"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        from typing import get_type_hints, Union\n        import inspect\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = [type_hints[param] for param in signature.parameters]\n        output_type_hint = type_hints.get('return')\n\n        input_class_definitions = [get_class_definition(param) for param in input_type_hints]\n        output_class_definition = get_class_definition(output_type_hint)\n\n        function_type = determine_function_type(output_type_hint)\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=func_object.__doc__,\n            input_type_hints=input_type_hints,\n            output_type_hint=output_type_hint,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output_class_definition,\n            function_type=function_type\n        )"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for i in range(self.num_hash_functions):\n            index = self.hash_functions[i](string) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        expected_length = self.size.calculate_expected_length()\n        \n        if len(loaded_bit_array) != expected_length:\n            logging.warning(\"Potential corruption in loaded bit array. Reinitializing and saving new state.\")\n            self.init_bit_array()\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        indices = [hash_function(string) % self.size for hash_function in self.hash_functions]\n        for index in indices:\n            if not self.bit_array[index]:\n                return False\n        return True"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    import numpy as np\n    if zeros > n:\n        raise ValueError(\"Number of zeros cannot exceed total number of weights\")\n\n    weights = np.random.rand(n)\n    weights /= weights.sum()\n\n    if zeros > 0:\n        zero_indices = np.random.choice(range(n), zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    if x.shape[0] != x.shape[1]:\n        return False\n    if not np.all(np.linalg.eigvals(x) > 0):\n        return False\n    return True"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if 'distilled_model' in json_dict:\n            self.distilled_model = json_dict['distilled_model']\n        if 'current_model_stats' in json_dict:\n            self.current_model_stats = json_dict['current_model_stats']\n        if 'last_training_run' in json_dict:\n            self.last_training_run = json_dict['last_training_run']\n        if 'current_training_run' in json_dict:\n            self.current_training_run = json_dict['current_training_run']\n        if 'nr_of_training_runs' in json_dict:\n            self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Add code to call the OpenAI API with the provided model, system_message, prompt, and additional parameters\n        # Process the response to remove any parsing helper tokens\n        # Return the final text response"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    import numpy as np\n    \n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The input matrix is not square\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square, hence not symmetric\")\n    \n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")"}
{"namespace": "ref_utils.l2_normalize", "completion": "    norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n    norm = jnp.maximum(norm, grad_eps)\n    return x / norm"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    elif word.isupper():\n        return word\n    elif word[0].isupper() and word[1:].islower():\n        return word.capitalize()\n    else:\n        return word.lower()"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    segmented_text = [line_text[i:i+1] for i in range(0, len(line_text), 1)]\n\n    return segmented_text"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "    from scipy.special import sph_harm\n    import numpy as np\n    def dir_enc_fn(point):\n        theta = np.arccos(point[2])\n        phi = np.arctan2(point[1], point[0])\n        encoding = np.zeros((deg_view + 1) ** 2, dtype=np.complex128)\n        idx = 0\n        for n in range(deg_view + 1):\n            for m in range(-n, n + 1):\n                encoding[idx] = sph_harm(m, n, phi, theta)\n                idx += 1\n        return encoding\n    return dir_enc_fn"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    current_block = {\"text\": \"\", \"type\": \"\", \"start_index\": 0, \"blocks\": [], \"header_index\": None, \"indentation_level\": 0}\n\n    for i, line in enumerate(lines):\n        # Remove leading and trailing whitespaces\n        line = line.strip()\n\n        # Remove duplicate lines (ignoring numbers)\n        if line and not any(char.isdigit() for char in line):\n            if line not in current_block[\"text\"]:\n                current_block[\"text\"] += line + \" \"\n\n        # Connect incomplete lines\n        if line and line[-1] == \"-\":\n            current_block[\"text\"] += line[:-1]\n        else:\n            # Categorize lines into paragraphs, headers, or list items\n            if line:\n                if line[0] == \"<\" and line[-1] == \">\":\n                    current_block[\"type\"] = \"xml_tag\"\n                elif line[0] == \"#\" and line[1] == \" \":\n                    current_block[\"type\"] = \"header\"\n                elif line[0] == \"*\":\n                    current_block[\"type\"] = \"list_item\"\n                else:\n                    current_block[\"type\"] = \"paragraph\"\n\n                current_block[\"start_index\"] = i\n\n                # Append the current block to the result list\n                result.append(current_block.copy())\n\n                # Reset the current block\n                current_block = {\"text\": \"\", \"type\": \"\", \"start_index\": 0, \"blocks\": [], \"header_index\": None, \"indentation_level\": 0}\n\n    return result"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    from nltk.tokenize import sent_tokenize as nltk_sent_tokenize\n    import nltk\n    import re\n    if org_texts is None or org_texts == \"\":\n        return [org_texts]\n\n    # Define rules and patterns\n    space_rule = r'\\s+'\n    bracket_rule = r'\\([^)]*\\)'\n    rules = [space_rule, bracket_rule]\n    quotation_pattern = r'[\"\u201c\u201d]'\n\n    # Apply rules and patterns\n    for rule in rules:\n        org_texts = re.sub(rule, ' ', org_texts)\n\n    # Normalize quotation marks\n    org_texts = re.sub(quotation_pattern, '\"', org_texts)\n\n    # Tokenize sentences using NLTK tokenizer\n    tokenized_sentences = nltk_sent_tokenize(org_texts)\n\n    return tokenized_sentences"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        import numpy as np\n        from typing import List\n        positions_list = []\n        if key is not None:\n            # Search for positions within a specific document\n            # Assuming document_positions is a dictionary where keys are document keys and values are numpy arrays of positions\n            if key in document_positions:\n                positions_list.append(document_positions[key])\n        else:\n            # Search for positions across all documents\n            # Assuming document_positions is a dictionary where keys are document keys and values are numpy arrays of positions\n            for positions in document_positions.values():\n                positions_list.append(positions)\n        return positions_list"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.isdigit():  # absolute number\n        return int(spec)\n    elif spec.endswith('%'):  # percentage\n        percentage = int(spec[:-1])\n        return max(int(num_clauses * percentage / 100), 1)\n    elif spec.startswith('<'):  # conditional expression\n        threshold = int(spec[1:])\n        return min(num_clauses, threshold)\n    else:\n        raise ValueError(\"Invalid 'min should match' specification\")"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        import numpy as np\n        from typing import List\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            # Directly calculate phrase frequencies using positions of terms\n            phrase_freq_array = np.zeros(len(self))  # Assuming len(self) gives the length of SearchArray\n            for i in range(len(self)):\n                count = 0\n                for j in range(len(tokens)):\n                    if self[i + j] == tokens[j]:\n                        count += 1\n                if count == len(tokens):\n                    phrase_freq_array[i] = 1\n            return phrase_freq_array\n        else:\n            # Delegate the calculation to another method\n            # Handle different slops or non-unique tokens\n            # Implement the logic for handling different slops or non-unique tokens\n            pass"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Initialize an empty list to hold the indexed data\n        indexed_data = []\n\n        # Process the array in batches\n        for i in range(0, len(array), batch_size):\n            batch = array[i:i + batch_size]  # Get a batch of data\n\n            # Tokenize the batch using the specified tokenizer\n            tokenized_batch = [tokenizer(item) for item in batch]\n\n            # Add the tokenized batch to the indexed data\n            indexed_data.extend(tokenized_batch)\n\n        # Create a new instance of SearchArray with the indexed data\n        search_array_instance = cls(indexed_data)\n\n        return search_array_instance"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "class ProxifierMessageInterceptor:\n    def __init__(self, configuration, logger):\n        self.configuration = configuration\n        self.logger = logger\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        \"\"\"\n        Initializes an instance of the ProxifierMessageInterceptor class by setting up a server with specific configurations and starting it. It also initializes a dictionary to keep track of connections and a lock for thread safety.\n\n        Input-Output Arguments\n        :param self: ProxifierMessageInterceptor. An instance of the ProxifierMessageInterceptor class. It uses its configuration and logger attributes to set up the server and for logging purposes.\n        :param configuration: dict. Specific configurations for setting up the server.\n        :param logger: Logger. An instance of the logger for logging purposes.\n        :return: No return values.\n        \"\"\""}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    for i in range(len(arr)):\n        count = 0\n        num = arr[i]\n        while num:\n            count += num & 1\n            num >>= 1\n        arr[i] = count\n\n    return arr"}
{"namespace": "searcharray.solr.edismax", "completion": "\n    query_fields = ' '.join([f\"{field}^{weight}\" for field, weight in qf.items()])\n    params = {\n        'q': q,\n        'qf': query_fields,\n        'mm': mm,\n        'pf': pf,\n        'pf2': pf2,\n        'pf3': pf3,\n        'q.op': q_op,\n        'defType': 'edismax',\n        'wt': 'json'\n    }\n\n    response = requests.get('http://localhost:8983/solr/collection/select', params=params)\n    response_json = response.json()\n    scores = [doc['score'] for doc in response_json['response']['docs']]\n    explanation = response_json['debug']['explain']\n\n    return np.array(scores), explanation"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.close()\n        \n        # Stop the server if it exists\n        if self.server:\n            self.server.stop()"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "import numpy as np\n\n    # Calculate the standard deviations for each variable\n    std_dev = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std_dev, std_dev)\n\n    return corr, std_dev"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Input matrix is not a square matrix\")\n\n    if not np.allclose(x, x.T, atol=1e-8):\n        raise ValueError(\"Input matrix is not symmetric\")\n\n    if not np.allclose(np.diagonal(x), 0, atol=1e-8):\n        raise ValueError(\"Diagonal elements are not close to zero\")"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Determine the appropriate language model and prompt based on function description and arguments\n        # ...\n        # Construct the prompt to be used for generation\n        prompt = construct_prompt(function_description, args, kwargs)\n        \n        # Decide whether to use a distilled model for zero-shot prompting or a teacher model for fine-tuning\n        model, suitable_for_distillation = select_model(function_description, llm_parameters)\n        \n        # Initialize function-specific data if not already done\n        initialized = initialize_function(func_hash)\n        \n        return prompt, model, suitable_for_distillation, initialized"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "import numpy as np\n\n    if higham:\n        # Use Higham & Nick (2002) algorithm to compute the nearest positive definite matrix\n        A = cov\n        n = A.shape[0]\n        k = 0\n        I = np.eye(n)\n        while k < higham_max_iteration:\n            A = (A + A.T) / 2\n            eigval, eigvec = np.linalg.eigh(A)\n            min_eigval = np.min(eigval)\n            if min_eigval > 0:\n                return A\n            A += I * (-min_eigval + 1e-6)\n            k += 1\n        return A\n    else:\n        # Clip eigenvalues to ensure positive definiteness\n        eigval, eigvec = np.linalg.eigh(cov)\n        clipped_eigval = np.maximum(eigval, 0)\n        nearest_cov = eigvec @ np.diag(clipped_eigval) @ eigvec.T\n        return nearest_cov"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    import numpy as np\n    from typing import Iterator, List\n    for arr in x:\n        if len(arr) > 1:\n            mid = len(arr) // 2\n            yield [arr[:mid], arr[mid:]]"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if math.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if x.is_integer():\n        return str(int(x))\n    else:\n        decimal_places = max(0, 2 - int(math.floor(math.log10(abs(x))))\n        return f\"{x:.{decimal_places}f}\""}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    import numpy as np\n    if isinstance(items, dict):\n        # Convert dictionary to array\n        array = np.full(n_assets, fill_value)\n        for i, asset_name in enumerate(assets_names):\n            if asset_name in items:\n                array[i] = items[asset_name]\n    else:\n        array = np.array(items)\n\n    if dim == 2:\n        if array.shape[0] != n_assets:\n            raise ValueError(f\"The number of assets in {name} does not match the expected value.\")\n    elif dim == 1:\n        if array.shape != (n_assets,):\n            raise ValueError(f\"The shape of {name} does not match the expected shape.\")\n\n    return array"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    import os\n    from pathlib import Path\n    if data_home is not None:\n        data_home = Path(data_home)\n    elif \"SKFOLIO_DATA\" in os.environ:\n        data_home = Path(os.environ[\"SKFOLIO_DATA\"])\n    else:\n        data_home = Path.home() / \"skfolio_data\"\n\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    from pathlib import Path\n    import shutil\n    if data_home is None:\n        data_home = Path.home() / \"skfolio_data\"\n    else:\n        data_home = Path(data_home)\n\n    if data_home.exists() and data_home.is_dir():\n        shutil.rmtree(data_home)\n    else:\n        print(f\"Directory {data_home} does not exist or is not a directory.\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "        import collections\n        import torch\n    import torch\n    import collections\n\n    if isinstance(obj, (str, bytes, int, float, bool)):\n        return (obj, None)\n\n    elif isinstance(obj, (list, tuple)):\n        res = tuple(flatten_to_tuple(o)[0] for o in obj)\n        schema = {\"type\": \"list\", \"length\": len(obj)}\n        return (res, schema)\n\n    elif isinstance(obj, collections.abc.Mapping):\n        res = tuple(flatten_to_tuple(obj[k])[0] for k in sorted(obj.keys()))\n        schema = {\"type\": \"mapping\", \"keys\": sorted(obj.keys())}\n        return (res, schema)\n\n    elif isinstance(obj, torch.Tensor):\n        return (obj, {\"type\": \"torch.Tensor\", \"shape\": obj.shape, \"dtype\": obj.dtype})\n\n    else:\n        raise ValueError(\"Unsupported type for flattening: {}\".format(type(obj)))"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    import re\n    import numpy as np\n    n_groups, n_assets = groups.shape\n    n_equations = len(equations)\n    \n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        equation = equation.replace(' ', '')  # Remove spaces\n        equation = re.sub(r'([A-Za-z0-9_]+)', r'groups[\"\\1\"]', equation)  # Replace group names with groups[\"group_name\"]\n        left[i, :] = eval(equation.split(\"<=\")[0])  # Evaluate the left side of the equation\n        right[i] = eval(equation.split(\"<=\")[1])  # Evaluate the right side of the equation\n\n    if sum_to_one:\n        left = np.vstack((left, np.ones(n_assets)))  # Add a row of ones to enforce sum to one constraint\n        right = np.append(right, 1.0)  # Add 1.0 to the right side\n\n    return left, right"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    from detectron2.structures import Instances\n    import tempfile\n    import torch\n    import os\n    class_name = \"PatchedInstances\"\n    class_def = f\"class {class_name}:\\n\"\n    for field in fields:\n        class_def += f\"    {field[0]}: {field[1]}\\n\"\n\n    module_code = f\"from detectron2.structures import Instances\\n{class_def}\\n\"\n    module_code += f\"def to_instances(self):\\n    return Instances({', '.join([f'{field[0]}=self.{field[0]}' for field in fields])})\\n\"\n    module_code += f\"{class_name}.to_instances = to_instances\"\n\n    temp_file = tempfile.NamedTemporaryFile(mode=\"w\", delete=False)\n    temp_file.write(module_code)\n    temp_file.close()\n\n    module_name = os.path.basename(temp_file.name)[:-3]\n    spec = importlib.util.spec_from_file_location(module_name, temp_file.name)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n\n    yield getattr(module, class_name)\n\n    os.remove(temp_file.name)"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "\n    # Set the \"training\" attribute of every submodule to False\n    for module in model.modules():\n        if hasattr(module, 'training'):\n            module.training = False"}
{"namespace": "spin_math.safe_sqrt", "completion": "    import jax.numpy as jnp\n  import jax.numpy as jnp\n\n  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "    import numpy as np\n    homogeneous_vectors = np.hstack((vectors, np.ones((vectors.shape[0], 1))))\n\n    # Apply the transformation to the homogeneous vectors\n    transformed_homogeneous_vectors = np.dot(transform, homogeneous_vectors.T).T\n\n    # Convert the transformed homogeneous vectors back to Cartesian coordinates\n    transformed_vectors = transformed_homogeneous_vectors[:, :-1] / transformed_homogeneous_vectors[:, -1][:, np.newaxis]\n\n    return transformed_vectors"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, eps))"}
{"namespace": "stepfun.resample", "completion": "    import numpy as np\n    resampled_values = []\n    for i in range(len(t) - 1):\n        start = t[i]\n        end = t[i + 1]\n        values_in_interval = []\n        for j in range(len(tp) - 1):\n            if tp[j] >= start and tp[j + 1] <= end:\n                values_in_interval.append(vp[j])\n            elif tp[j] < start and tp[j + 1] > start:\n                values_in_interval.append(vp[j] * (tp[j + 1] - start) / (tp[j + 1] - tp[j]))\n            elif tp[j] < end and tp[j + 1] > end:\n                values_in_interval.append(vp[j + 1] * (end - tp[j]) / (tp[j + 1] - tp[j]))\n        if use_avg:\n            resampled_values.append(np.mean(values_in_interval))\n        else:\n            resampled_values.append(np.sum(values_in_interval))\n    return resampled_values"}
{"namespace": "coord.contract", "completion": "    import numpy as np\n    magnitude_squared = np.sum(x**2, axis=-1, keepdims=True)\n    scale = np.minimum(1.0, magnitude_squared) / (1.0 + magnitude_squared)\n    return x * scale"}
{"namespace": "coord.inv_contract", "completion": "    # Define the inverse of the contract function\n    inv_z = z * 2  # Example inverse operation, replace with the actual inverse operation of the contract function\n    return inv_z"}
{"namespace": "grid_utils.trilerp", "completion": "    import numpy as np\n    if datastructure == 'grid':\n        return trilerp_grid(values, coordinates)\n    elif datastructure == 'hash':\n        return trilerp_hash(values, coordinates)\n    else:\n        raise ValueError(\"Invalid datastructure. Supported data structures are 'grid' and 'hash'.\")"}
{"namespace": "coord.track_linearize", "completion": "        import jax.numpy as jnp\n        import jax\n      fn_mean = fn(mean)\n      \n      # Compute the Jacobian of the function at the mean\n      jacobian = jax.jacobian(fn)(mean)\n      \n      # Transform the covariances using the linearized function and the Jacobian\n      fn_cov = jnp.einsum('...ij,...jk->...ik', jnp.einsum('...ij,...jk->...ik', jacobian, cov), jacobian.T)\n      \n      return fn_mean, fn_cov"}
{"namespace": "coord.contract3_isoscale", "completion": "    import numpy as np\n    norm = np.linalg.norm(x)\n    scaled_x = x / norm\n    return scaled_x"}
{"namespace": "coord.pos_enc", "completion": "    import numpy as np\n    scales = 2 ** np.linspace(min_deg, max_deg, max_deg - min_deg + 1)\n    encodings = []\n    for scale in scales:\n        encodings.append(np.sin(x / scale))\n        encodings.append(np.cos(x / scale))\n    result = np.concatenate(encodings, axis=-1)\n    if append_identity:\n        result = np.concatenate([x, result], axis=-1)\n    return result"}
{"namespace": "coord.integrated_pos_enc", "completion": "        import jax.numpy as jnp\n      scaled_mean = mean * (2 ** jnp.linspace(min_deg, max_deg, mean.shape[-1]))\n      scaled_var = var * (2 ** jnp.linspace(min_deg, max_deg, var.shape[-1]))\n\n      # Concatenate the scaled mean and variance\n      concatenated = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n      # Apply sinusoidal encoding\n      encoding = jnp.sin(concatenated)\n\n      return encoding"}
{"namespace": "coord.isotropize", "completion": "    import numpy as np\n    if mode == 'fast':\n        det = np.linalg.det(cov)\n        isotropic_cov = det**(1/cov.shape[0]) * np.identity(cov.shape[0])\n    elif mode == 'accurate':\n        sign, logdet = np.linalg.slogdet(cov)\n        if sign <= 0 or np.isinf(logdet):\n            raise ValueError(\"Invalid determinant or logarithm of determinant\")\n        isotropic_cov = np.exp(logdet/cov.shape[0]) * np.identity(cov.shape[0])\n    else:\n        raise ValueError(\"Invalid mode. Please choose either 'fast' or 'accurate'\")\n    \n    return isotropic_cov"}
{"namespace": "coord.construct_ray_warps", "completion": "    if fn_inv is None:\n        if fn == some_function:\n            fn_inv = some_inverse_function\n        elif fn == another_function:\n            fn_inv = another_inverse_function\n        else:\n            raise ValueError(\"Inverse function not provided and could not be determined automatically.\")\n\n    def t_to_s(metric_distance):\n        normalized_distance = (fn(metric_distance) - fn(t_near)) / (fn(t_far) - fn(t_near))\n        return normalized_distance\n\n    def s_to_t(normalized_distance):\n        metric_distance = fn_inv(fn(t_near) + normalized_distance * (fn(t_far) - fn(t_near)))\n        return metric_distance\n\n    return t_to_s, s_to_t"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        import tinycudann\n        import torch\n        from typing import Union, Any\n        assert n_layers > 0, \"Number of layers must be greater than 0\"\n        assert n_neurons > 0, \"Number of neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons <= 128:\n                return tinycudann.create_network(n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation)\n            else:\n                raise ValueError(\"Number of neurons exceeds the limit supported by tinycudann\")\n        else:\n            model = torch.nn.Sequential()\n            for i in range(n_layers - 1):\n                if i == 0:\n                    model.add_module(\"input_layer\", torch.nn.Linear(n_input_dims, n_neurons))\n                else:\n                    model.add_module(f\"hidden_layer_{i}\", torch.nn.Linear(n_neurons, n_neurons))\n                if activation == \"ReLU\":\n                    model.add_module(f\"activation_{i}\", torch.nn.ReLU())\n                if activation == \"None\":\n                    pass\n            model.add_module(\"output_layer\", torch.nn.Linear(n_neurons, n_output_dims))\n            if output_activation == \"ReLU\":\n                model.add_module(\"output_activation\", torch.nn.ReLU())\n            if output_activation == \"Sigmoid\":\n                model.add_module(\"output_activation\", torch.nn.Sigmoid())\n            if output_activation == \"None\":\n                pass\n            return model"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    \n    areas = [np.abs(np.cross(poly[:-1, 0], poly[1:, 1]) + np.cross(poly[1:, 0], poly[:-1, 1])) / 2 for poly in polygons]\n    max_area = max(areas)\n    filtered_polygons = [poly for poly, area in zip(polygons, areas) if area >= max_area * rel_tr and area >= abs_tr]\n    \n    return filtered_polygons"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        import numpy as np\n        rolling_medians = []\n        for i in range(kernel_offset, len(signal) - kernel_offset):\n            window = signal[i - kernel_offset: i + kernel_offset + 1]\n            median = np.median(window)\n            rolling_medians.append(median)\n        return np.array(rolling_medians)"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "\n    min_distance = float('inf')\n    best_shift = 0\n\n    for shift in range(-rotation_shift, rotation_shift + 1):\n        distance = 0\n        for i in range(len(template_probe)):\n            distance += template_probe[i] != template_gallery[(i + shift) % len(template_gallery)]\n        if distance < min_distance:\n            min_distance = distance\n            best_shift = shift\n\n    return min_distance, best_shift"}
{"namespace": "iris.utils.math.area", "completion": "    import numpy as np\n    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have the shape (_, 2)\")\n\n    n = len(array)\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i, 0] * array[j, 1]\n        area -= array[j, 0] * array[i, 1]\n\n    area = abs(area) / 2.0\n    return area"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        from typing import Tuple\n        import numpy as np\n        num_points = polygon.shape[0]\n        start_points = []\n        end_points = []\n\n        for _ in range(num_points):\n            point1 = polygon[np.random.randint(num_points)]\n            point2 = polygon[np.random.randint(num_points)]\n\n            distance = np.linalg.norm(point2 - point1)\n            if distance > min_distance_between_sector_points_in_px:\n                midpoint = (point1 + point2) / 2\n                direction = np.array([point2[1] - point1[1], point1[0] - point2[0]])\n                direction /= np.linalg.norm(direction)\n                start_points.append(midpoint)\n                end_points.append(midpoint + direction)\n\n        if len(start_points) < 2:\n            raise EyeCentersEstimationError(\"Insufficient point pairs found\")\n\n        return np.array(start_points), np.array(end_points)"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    import numpy as np\n    total_length = 0\n    for i in range(len(polygon) - 1):\n        distance = np.linalg.norm(polygon[i+1] - polygon[i])\n        if distance <= max_point_distance:\n            total_length += distance\n    return total_length"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: Field '{field.name}' must contain only boolean values, but found {v.dtype} instead.\")\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points, but received shape {v.shape}\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    from pydantic import fields\n    from typing import Any, Iterable\n    if isinstance(v, Iterable):\n        for val in v:\n            if val <= 0:\n                raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"The value in {cls.__name__}.{field.name} must be positive\")\n    return v"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"Invalid bounding box values provided for {cls.__name__}. Minimum x and y values must be less than maximum x and y values.\")\n    return values"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    from pydantic import ValidationError, fields\n    import numpy as np\n    from typing import Callable\n    def validator(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValidationError(f\"Array must have {nb_dimensions} dimensions\")\n        return v\n    return validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    from pydantic import BaseModel, ValidationError\n    import numpy as np\n    from typing import Callable\n    def validator(cls, values):\n        arrays1 = values.get(field1, [])\n        arrays2 = values.get(field2, [])\n\n        if len(arrays1) != len(arrays2):\n            raise ValueError(f\"The length of {field1} and {field2} must be equal\")\n\n        for arr1, arr2 in zip(arrays1, arrays2):\n            if not np.array_equal(arr1.shape, arr2.shape):\n                raise ValueError(f\"The shapes of arrays in {field1} and {field2} must be equal\")\n\n        return values\n\n    return validator"}
{"namespace": "math.safe_log", "completion": "  import jax.numpy as jnp\n  return jnp.where(x > 0, jnp.log(x), 0.0)"}
{"namespace": "math.safe_sqrt", "completion": "  import tensorflow as tf\n  x = tf.clip_by_value(x, 0, tf.reduce_max(x))  # Clamp x between 0 and its maximum value\n  return tf.sqrt(x)"}
{"namespace": "math.power_ladder_max_output", "completion": "    if p > 1:\n        return float('inf')\n    elif p == 1:\n        return 1\n    elif p > 0:\n        return 0\n    elif p == 0:\n        return 1\n    else:\n        return float('-inf')"}
{"namespace": "geopoly.generate_basis", "completion": "    from scipy.spatial import ConvexHull\n    import numpy as np\n    if base_shape == 'tetrahedron':\n        vertices = np.array([\n            [1, 1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n            [-1, -1, 1]\n        ])\n        faces = np.array([\n            [0, 1, 2],\n            [0, 1, 3],\n            [0, 2, 3],\n            [1, 2, 3]\n        ])\n    elif base_shape == 'icosahedron':\n        phi = (1 + np.sqrt(5)) / 2\n        vertices = np.array([\n            [-1, phi, 0],\n            [1, phi, 0],\n            [-1, -phi, 0],\n            [1, -phi, 0],\n            [0, -1, phi],\n            [0, 1, phi],\n            [0, -1, -phi],\n            [0, 1, -phi],\n            [phi, 0, -1],\n            [phi, 0, 1],\n            [-phi, 0, -1],\n            [-phi, 0, 1]\n        ])\n        faces = np.array([\n            [0, 11, 5],\n            [0, 5, 1],\n            [0, 1, 7],\n            [0, 7, 10],\n            [0, 10, 11],\n            [1, 5, 9],\n            [5, 11, 4],\n            [11, 10, 2],\n            [10, 7, 6],\n            [7, 1, 8],\n            [3, 9, 4],\n            [3, 4, 2],\n            [3, 2, 6],\n            [3, 6, 8],\n            [3, 8, 9],\n            [4, 9, 5],\n            [2, 4, 11],\n            [6, 2, 10],\n            [8, 6, 7],\n            [9, 8, 1]\n        ])\n    elif base_shape == 'octahedron':\n        vertices = np.array([\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1]\n        ])\n        faces = np.array([\n            [0, 2, 4],\n            [0, 4, 3],\n            [0, 3, 5],\n            [0, 5, 2],\n            [1, 2, 4],\n            [1, 4, 3],\n            [1, 3, 5],\n            [1, 5, 2]\n        ])\n    else:\n        raise ValueError(\"Invalid base_shape. Must be 'tetrahedron', 'icosahedron', or 'octahedron'.\")\n\n    for _ in range(angular_tesselation):\n        new_vertices = []\n        new_faces = []\n        for face in faces:\n            v0, v1, v2 = vertices[face]\n            v01 = (v0 + v1) / 2\n            v12 = (v1 + v2) / 2\n            v20 = (v2 + v0) / 2\n            new_vertices.extend([v0, v01, v20, v1, v12, v2, (v01 + v12 + v20) / 3])\n            n = len(new_vertices)\n            new_faces.extend([\n                [n - 7, n - 6, n - 1],\n                [n - 6, n - 5, n - 4],\n                [n - 5, n - 7, n - 3],\n                [n - 7, n - 1, n - 4],\n                [n - 6, n - 4, n - 1],\n                [n - 5, n - 3, n - 4],\n                [n - 7, n - 4, n - 3],\n                [n - 2, n - 1, n - 3],\n                [n - 2, n - 3, n - 5],\n                [n - 2, n - 5, n - 6],\n                [n - 2, n - 6, n - 1]\n            ])\n        vertices = np.array(new_vertices)\n        faces = np.array(new_faces)\n\n    hull = ConvexHull(vertices)\n    basis = vertices[hull.vertices].T\n\n    if remove_symmetries:\n        unique_basis = []\n        for col in basis.T:\n            is_unique = True\n            for unique_col in unique_basis:\n                if np.allclose(col, unique_col, atol=eps):\n                    is_unique = False\n                    break\n            if is_unique:\n                unique_basis.append(col)\n        basis = np.array(unique_basis).T\n\n    return basis"}
{"namespace": "math.safe_log1p", "completion": "    import numpy as np\n    if x < -1:\n        return np.log1p(np.abs(x)) - np.log(2)\n    else:\n        return np.log1p(x)"}
{"namespace": "math.power_ladder", "completion": "    import numpy as np\n    if premult is not None:\n        x = x * premult\n    \n    if p == 1:\n        x_transformed = np.log(x)\n    elif p == 0:\n        x_transformed = np.log(x)\n    elif p == float('inf'):\n        x_transformed = np.sign(x) * np.log(np.abs(x))\n    elif p == float('-inf'):\n        x_transformed = np.sign(x) * np.exp(x)\n    else:\n        x_transformed = np.sign(x) * np.power(np.abs(x), p)\n    \n    if postmult is not None:\n        x_transformed = x_transformed * postmult\n    \n    return x_transformed"}
{"namespace": "math.inv_power_ladder", "completion": "    if postmult:\n        y = y * postmult\n    if p == 0:\n        result = y\n    elif p == 1:\n        result = y ** (1/p)\n    else:\n        result = (y ** (1/p) - 1) / (1/p)\n    if premult:\n        result = result * premult\n    return result"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert origins and directions to homogeneous coordinates\n  origins_homogeneous = xnp.concatenate([origins, xnp.ones((origins.shape[0], 1), dtype=xnp.float32)], axis=1)\n  directions_homogeneous = xnp.concatenate([directions, xnp.zeros((directions.shape[0], 1), dtype=xnp.float32)], axis=1)\n\n  # Apply perspective projection using the inverse intrinsic matrix\n  origins_ndc = xnp.dot(origins_homogeneous, pixtocam.T)\n  directions_ndc = xnp.dot(directions_homogeneous, pixtocam.T)\n\n  # Normalize the coordinates by dividing by the z component\n  origins_ndc /= origins_ndc[:, 2][:, None]\n  directions_ndc /= directions_ndc[:, 2][:, None]\n\n  # Adjust the origins to the near plane\n  origins_ndc *= near\n\n  return origins_ndc, directions_ndc"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "    import scipy.interpolate\n    spline_degree = min(spline_degree, len(x) - 1)\n\n    # Perform spline interpolation\n    spline = scipy.interpolate.UnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n    interpolated_values = spline(t_output)\n\n    return interpolated_values"}
{"namespace": "math.learning_rate_decay", "completion": "    if step < lr_delay_steps:\n        return lr_init * lr_delay_mult\n    else:\n        decay_rate = (lr_final / lr_init) ** (1 / (max_steps - lr_delay_steps))\n        return lr_init * (decay_rate ** (step - lr_delay_steps))"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  K = xnp.array([[fx, 0, cx],\n                 [0, fy, cy],\n                 [0, 0, 1]])\n  return K"}
{"namespace": "utils.dummy_rays", "completion": "  # Call the generate_random_rays function with the specified parameters\n  result = generate_random_rays(\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx\n  )\n  return result"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert points to homogeneous coordinates\n  points = xnp.concatenate([points, xnp.ones((points.shape[0], 1), dtype=points.dtype)], axis=-1)\n\n  # Project points to camera coordinates\n  cam_coords = xnp.einsum('ij,bj->bi', pixtocams, points)\n\n  # Apply camera extrinsics\n  world_coords = xnp.einsum('ij,bj->bi', camtoworlds[:, :3, :3], cam_coords) + camtoworlds[:, :3, 3]\n\n  # Apply perspective projection\n  if camtype == ProjectionType.PERSPECTIVE:\n      projected_coords = world_coords[:, :2] / world_coords[:, 2:3]\n  else:\n      raise ValueError(\"Unsupported camera projection type\")\n\n  # Apply distortion correction if distortion parameters are provided\n  if distortion_params is not None:\n      # Apply radial distortion\n      r = xnp.linalg.norm(projected_coords, axis=-1, keepdims=True)\n      radial_distortion = 1 + xnp.sum(distortion_params['radial'] * xnp.hstack((r ** i for i in range(1, 4))), axis=-1, keepdims=True)\n      corrected_coords = projected_coords * radial_distortion\n\n      # Apply tangential distortion\n      x = corrected_coords[:, 0:1]\n      y = corrected_coords[:, 1:2]\n      xy = x * y\n      x_distortion = 2 * distortion_params['tangential'][0] * xy + distortion_params['tangential'][1] * (r ** 2 + 2 * x ** 2)\n      y_distortion = distortion_params['tangential'][1] * xy + 2 * distortion_params['tangential'][0] * (r ** 2 + 2 * y ** 2)\n      corrected_coords = xnp.concatenate([x + x_distortion, y + y_distortion], axis=-1)\n  else:\n      corrected_coords = projected_coords\n\n  # Return the corrected 2D pixel coordinates and depth values\n  return corrected_coords, world_coords[:, 2]"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize variables to store the best result and its evaluation metrics\n    best_result = None\n    best_evaluation = None\n\n    # Iterate through each module and its parameters\n    for i, module in enumerate(modules):\n        params = module_params[i]\n\n        # Execute the module and measure its execution time\n        start_time = time.time()\n        result = module(previous_result, **params)\n        execution_time = time.time() - start_time\n\n        # Evaluate the result based on the specified strategies\n        evaluation = evaluate_result(result, strategies)\n\n        # Save the result and its evaluation metrics to the specified directory\n        save_result_and_summary(result, evaluation, execution_time, node_line_dir, i)\n\n        # Update the best result and its evaluation metrics if the current result is better\n        if best_result is None or evaluation > best_evaluation:\n            best_result = result\n            best_evaluation = evaluation\n\n    # Return the best result after evaluating all query expansion modules\n    return best_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    best_module = None\n    best_evaluation_metrics = None\n\n    # Iterate through the prompt maker modules and their parameters\n    for module, params in zip(modules, module_params):\n        # Execute the prompt maker module with the specified parameters\n        result = module(**params)\n\n        # Evaluate the performance of the prompt maker module based on the specified strategies\n        evaluation_metrics = evaluate_performance(result, strategies)\n\n        # Check if the current module is the best based on the evaluation metrics and strategies\n        if is_better_module(evaluation_metrics, best_evaluation_metrics, strategies):\n            best_module = module\n            best_evaluation_metrics = evaluation_metrics\n\n    # Save the results and a summary to the specified directory\n    save_results_and_summary(best_module, best_evaluation_metrics, node_line_dir)\n\n    # Combine the results from the previous operation with the best prompt maker's output\n    combined_results = combine_results(previous_result, best_module(**module_params))\n\n    return combined_results"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    from typing import List\n    values = []\n    for node in nodes:\n        if key in node.module_params:\n            value = node.module_params[key]\n            if value not in values:\n                values.append(value)\n    return values"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "import json\nimport pandas as pd\n\n    # Load the summary file into a DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Convert specified columns to dictionary objects\n    if dict_columns is not None:\n        for col in dict_columns:\n            summary_df[col] = summary_df[col].apply(lambda x: json.loads(x))\n\n    return summary_df"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        from typing import Dict\n        module_type = module_dict.pop('module_type')\n        return cls(module_type, **module_dict)"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    from typing import List, Dict, Union, Any, Tuple\n    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(metric.get('name', ''))\n            metric_params.append(metric)\n        else:\n            raise ValueError(\"Invalid metric format. Metric must be either a string or a dictionary.\")\n\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    from sentence_transformers.models import BaseEmbedding\n    from sentence_transformers import SentenceTransformer, util\n    from typing import List, Optional\n    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n\n    pred_embedding = embedding_model.encode(pred)\n    gt_embeddings = embedding_model.encode(generation_gt)\n\n    max_score = 0\n    for gt_embedding in gt_embeddings:\n        score = util.pytorch_cos_sim(pred_embedding, gt_embedding).max().item()\n        if score > max_score:\n            max_score = score\n\n    return max_score"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        # Check if GFPGAN face restorer is set up\n        if gfpgan_face_restorer_is_set_up:\n            # Use GFPGAN face restorer to restore faces in the image\n            restored_image = gfpgan_face_restorer(np_image)\n            return restored_image\n        else:\n            # Log a warning and return the original image\n            print(\"GFPGAN face restorer is not set up. Returning original image.\")\n            return np_image\n    except Exception as e:\n        # Log the exception and return the original image\n        print(f\"An error occurred while attempting to restore faces: {e}\")\n        return np_image"}
{"namespace": "codeformer_model.setup_model", "completion": "\n    try:\n        # Initialize the FaceRestorerCodeFormer instance with the given directory name\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        \n        # Add the initialized instance to a global list of face restorers\n        global face_restorers\n        face_restorers.append(face_restorer)\n        \n    except Exception as e:\n        # Report the error if an exception occurs during setup\n        print(f\"Error occurred during model setup: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        # Patch the facexlib with the given directory\n        facexlib.patch(dirname)\n\n        # Initialize the GFPGAN face restorer with the model located in the specified directory\n        model = GFPGANModel.load_model(dirname)\n\n        # Set up the model for face restoration\n        model.setup()\n\n    except Exception as e:\n        # Handle any exceptions that occur during the setup process and report them\n        print(f\"An error occurred during model setup: {e}\")"}
{"namespace": "quaternion.rotate", "completion": "    import numpy as np\n    v_quat = np.array([0, v[0], v[1], v[2]])\n\n    # Perform the rotation\n    rotated_v_quat = q * v_quat * np.conj(q)\n\n    # Convert the rotated vector back to array format\n    rotated_v = rotated_v_quat[1:]\n\n    return rotated_v"}
{"namespace": "quaternion.from_axis_angle", "completion": "    angle = jnp.linalg.norm(axis_angle)\n    if angle < eps:\n        return jnp.array([1.0, 0.0, 0.0, 0.0])  # Identity quaternion for small angles\n    else:\n        axis = axis_angle / angle\n        half_angle = angle / 2.0\n        sin_half_angle = jnp.sin(half_angle)\n        return jnp.array([jnp.cos(half_angle), axis[0] * sin_half_angle, axis[1] * sin_half_angle, axis[2] * sin_half_angle])"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    topk, calls = model.get_topk(prefix, k)\n    log_probs = []\n    for i in range(len(topk)):\n        if topk[i] == idx:\n            log_probs.append(high - i)\n    return max(log_probs), calls"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "\n    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of the Dir class\")\n\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must start with 's3://'\")\n\n    if not append and not overwrite:\n        if len(output_dir) > 0:\n            raise ValueError(\"output_dir is not empty\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    import os\n    if not output_dir.is_s3_bucket():\n        raise ValueError(\"The directory is not an S3 bucket directory\")\n\n    index_file_path = os.path.join(output_dir.path, \"index.json\")\n    if output_dir.contains_file(index_file_path):\n        raise ValueError(\"The directory already contains an index file named 'index.json'\")\n\n    if not output_dir.contains_file(index_file_path):\n        output_dir.delete_all_objects()"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        from typing import Optional\n        if node_rank is None or node_rank == 0:\n            # Perform the merge operation\n            print(\"Performing merge operation\")\n        else:\n            # Wait for the merged index file to be available\n            print(\"Waiting for the merged index file to be available\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "from typing import Optional\nfrom studio import Job, Machine\n\n    job = Job(name=name, num_nodes=num_nodes, machine=machine, command=command)\n    job.submit()\n    print(f\"Job URL: {job.url}\")\n\n    while not job.is_running():\n        pass\n\n    if job.is_failed():\n        raise Exception(\"Job failed\")"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        from typing import List\n        self.deletion_queue.extend(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        config_file_path = os.path.join(self._cache_dir, \"config.json\")\n        if os.path.exists(config_file_path):\n            with open(config_file_path, 'r') as config_file:\n                config_data = json.load(config_file)\n                chunks_config = ChunksConfig.from_dict(config_data)\n                return chunks_config\n        else:\n            return None"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        from typing import List\n        self._to_download_queue.extend(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The provided index is not an instance of ChunkedIndex\")\n\n        if not self.index_config:\n            raise Exception(\"Reader's index configuration is not defined\")\n\n        if not self.prepare_thread.is_alive():\n            raise Exception(\"Prepare thread is not running\")\n\n        chunk = self.index_config.get_chunk(index)\n        if not chunk.is_available_locally() and not chunk.is_in_memory():\n            self.prepare_thread.prefetch(chunk)\n\n        item = self.item_loader.load_item(chunk, index)\n        chunk.consume_item()\n\n        return item"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for file in filenames:\n            file_path = os.path.join(dirpath, file)\n            try:\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    from typing import Any\n    if is_distributed_environment():\n        distributed_map = get_distributed_map()\n        distributed_map[key] = obj\n        return distributed_map[key]\n    else:\n        return obj"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Sort items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize bins and bin weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    import random\n    random.seed(seed + current_epoch)\n    \n    # Shuffle the chunks for each rank\n    shuffled_chunks = [random.sample(chunks, len(chunks)) for chunks in chunks_per_ranks]\n    \n    # Flatten the list of shuffled chunk indexes across all nodes\n    flattened_chunks = [chunk for chunks in shuffled_chunks for chunk in chunks]\n    \n    return flattened_chunks"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in ['B', 'KB', 'MB', 'GB', 'TB', 'PB']:\n        if abs(num_bytes) < 1000.0:\n            return f\"{num_bytes:.1f} {unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f} PB\""}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    from typing import Sequence, Any, Optional\n    import os\n    if len(inputs) < 2:\n        return None\n\n    input_dir = os.path.dirname(os.path.abspath(inputs[0]))\n    input_dir2 = os.path.dirname(os.path.abspath(inputs[1]))\n\n    if input_dir != input_dir2:\n        raise ValueError(\"Inconsistent input directories\")\n\n    return input_dir"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    dilated_t = t * dilation\n    # Clip dilated time steps within the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n    # Adjust weights based on the dilation\n    adjusted_w = w / dilation\n    return dilated_t, adjusted_w"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    import numpy as np\n    interpolated_values = np.interp(tq, t, y, left=outside_value, right=outside_value)\n    return interpolated_values"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    bias = 1.0 - train_frac\n    bias = torch.clamp(bias, eps, 1.0 - eps)\n    bias = (bias ** anneal_slope) / (bias ** anneal_slope + (1.0 - bias) ** anneal_slope)\n\n    w = w * bias\n    w = torch.nn.functional.softmax(w, dim=-1)\n\n    return w"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    import torch\n    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, (list, tuple)):\n        return [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) if k != \"meta\" else v for k, v in batch.items()}\n    else:\n        raise TypeError(\"Unsupported data type for conversion to CUDA tensor\")"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # expand faces tensor to match the batch dimension of vertices tensor\n    if v.shape[:-1] != f.shape[:-1]:\n        expand_shape = list(v.shape)\n        expand_shape[-1] = 1\n        f = f.expand(*expand_shape)\n\n    # gather vertices for each face\n    tris = torch.gather(v, dim, f)\n\n    return tris"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    from typing import Union\n    import numpy as np\n    import torch\n    if isinstance(batch, tuple):\n        return tuple(add_batch(item) for item in batch)\n    elif isinstance(batch, list):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise ValueError(\"Unsupported data type for batch\")"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        import torch\n        camera_params_tensor = torch.tensor(self.camera_params)\n        gui_elements_tensor = torch.tensor(self.gui_elements)\n\n        # Create a structured dictionary format for easy access and manipulation\n        batch_dict = dotdict()\n        batch_dict.camera_params = camera_params_tensor\n        batch_dict.gui_elements = gui_elements_tensor\n        batch_dict.meta = dotdict()\n        batch_dict.meta.camera_params = camera_params_tensor\n        batch_dict.meta.gui_elements = gui_elements_tensor\n\n        return batch_dict"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working() and not agent.is_prime():\n            agent_state = agent.get_state()  # Assuming get_state() returns the state of the agent in dictionary format\n            # Save the agent state using the persistence mechanism provided by the AgentPersistenceManager\n            # Example: self.persistence_mechanism.save(agent_state)"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        import numpy as np\n        from typing import Optional, Tuple\n        closest_agent = None\n        highest_similarity_score = -float('inf')\n\n        for agent in self.agents:\n            similarity_score = np.dot(agent.purpose_embedding, purpose_embedding) / (np.linalg.norm(agent.purpose_embedding) * np.linalg.norm(purpose_embedding))\n            if similarity_score > highest_similarity_score:\n                closest_agent = agent\n                highest_similarity_score = similarity_score\n\n        return closest_agent, highest_similarity_score"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(prompt=\"Hello\", name=\"Prime\", weight=1.0, is_prime=True, unspecified_flag=True)\n        self.agent_list.append(prime_agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        deserialized_agent = self.deserialize_agent_from_database(purpose, agent_lifecycle, openai_wrapper)\n        return deserialized_agent"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        start_index = response.find('Use Agent[')\n        end_index = response.find(']', start_index)\n        agent_info = response[start_index + len('Use Agent['):end_index]\n        agent_name, input_text = agent_info.split(':') if ':' in agent_info else (agent_info, '')\n        return agent_name, input_text"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Assuming there is a method in the agent_lifecycle for loading agents\n        # and a method in the openai_wrapper for interacting with OpenAI services\n        loaded_agents = []\n        agents_from_database = agent_lifecycle.load_agents_from_database()\n        for agent in agents_from_database:\n            loaded_agent = openai_wrapper.load_agent(agent)\n            loaded_agents.append(loaded_agent)\n        return loaded_agents"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        import logging\n        try:\n            # Save the agent using the agent persistence mechanism\n            # Example: agent.save()\n            pass\n        except Exception as e:\n            # Log the exception\n            logging.error(f\"An error occurred while saving the agent: {e}\")\n            # Re-raise the error\n            raise"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Extract instance's bounding box coordinates\n    x, y, w, h = instance['bbox']\n\n    # Calculate the center of the instance\n    center_x = x + (w / 2)\n    center_y = y + (h / 2)\n\n    # Calculate the top-left corner of the crop region\n    crop_x = max(0, center_x - (crop_size[1] / 2))\n    crop_y = max(0, center_y - (crop_size[0] / 2))\n\n    # Adjust the crop region to fit within the image boundaries\n    crop_x = min(crop_x, image_size[1] - crop_size[1])\n    crop_y = min(crop_y, image_size[0] - crop_size[0])\n\n    # Create and return the CropTransform object\n    return CropTransform(crop_x, crop_y, crop_size[1], crop_size[0])"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    import numpy as np\n    import cv2\n    image = cv2.imread(file_name)\n\n    # Apply any necessary orientation corrections based on the image's EXIF data\n    # (code for orientation correction is not provided)\n\n    # Convert the image to the specified format\n    if format is not None:\n        if format == \"BGR\":\n            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        elif format == \"YUV-BT.601\":\n            image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n\n    return image"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    for transform in transforms:\n        if isinstance(transform, T.Resize):\n            # Update bounding box coordinates\n            if \"bbox\" in annotation:\n                bbox = annotation[\"bbox\"]\n                bbox = transform.resize_box(bbox, image_size)\n                annotation[\"bbox\"] = bbox\n\n            # Update segmentation coordinates\n            if \"segmentation\" in annotation:\n                segmentation = annotation[\"segmentation\"]\n                if isinstance(segmentation, list):\n                    for i in range(len(segmentation)):\n                        segmentation[i] = transform.resize(segmentation[i], image_size)\n                elif isinstance(segmentation, dict):\n                    for key in segmentation:\n                        segmentation[key] = transform.resize(segmentation[key], image_size)\n                annotation[\"segmentation\"] = segmentation\n\n            # Update keypoints\n            if \"keypoints\" in annotation:\n                keypoints = annotation[\"keypoints\"]\n                keypoints = transform.resize_keypoints(keypoints, image_size)\n                annotation[\"keypoints\"] = keypoints\n\n        elif isinstance(transform, T.RandomHorizontalFlip):\n            # Update bounding box coordinates\n            if \"bbox\" in annotation:\n                bbox = annotation[\"bbox\"]\n                bbox = transform.apply_flipped_bbox(bbox, image_size)\n                annotation[\"bbox\"] = bbox\n\n            # Update segmentation coordinates\n            if \"segmentation\" in annotation:\n                segmentation = annotation[\"segmentation\"]\n                if isinstance(segmentation, list):\n                    for i in range(len(segmentation)):\n                        segmentation[i] = transform.apply_flipped_segmentation(segmentation[i], image_size)\n                elif isinstance(segmentation, dict):\n                    for key in segmentation:\n                        segmentation[key] = transform.apply_flipped_segmentation(segmentation[key], image_size)\n                annotation[\"segmentation\"] = segmentation\n\n            # Update keypoints\n            if \"keypoints\" in annotation and keypoint_hflip_indices is not None:\n                keypoints = annotation[\"keypoints\"]\n                keypoints = transform.apply_flipped_keypoints(keypoints, image_size, keypoint_hflip_indices)\n                annotation[\"keypoints\"] = keypoints\n\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    return annotation"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        import numpy as np\n        if not coords or self.angle % 360 == 0:\n            return coords\n        else:\n            rotation_matrix = np.array([[np.cos(np.radians(self.angle)), -np.sin(np.radians(self.angle))],\n                                        [np.sin(np.radians(self.angle)), np.cos(np.radians(self.angle))]])\n            transformed_coords = np.dot(coords, rotation_matrix)\n            return transformed_coords"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    instances = Instances(image_size)\n    \n    # Process bounding boxes\n    if \"bbox\" in annos[0]:\n        boxes = [obj[\"bbox\"] for obj in annos]\n        instances.gt_boxes = Boxes(boxes)\n    \n    # Process classes\n    if \"class\" in annos[0]:\n        classes = [obj[\"class\"] for obj in annos]\n        instances.gt_classes = torch.tensor(classes)\n    \n    # Process segmentation masks\n    if \"mask\" in annos[0]:\n        if mask_format == \"polygon\":\n            masks = [polygon_to_mask(obj[\"mask\"], image_size) for obj in annos]\n        elif mask_format == \"bitmask\":\n            masks = [bitmask_to_mask(obj[\"mask\"], image_size) for obj in annos]\n        instances.gt_masks = SegmentationMask(masks, image_size)\n    \n    # Process keypoints\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        instances.gt_keypoints = Keypoints(keypoints)\n    \n    return instances"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    from collections import defaultdict\n    import typing\n    import torch.nn as nn\n    import torch\n    model.eval()\n    model = torch.jit.trace(model, [input[\"image\"] for input in inputs])\n\n    flops_dict = defaultdict(float)\n\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Conv2d):\n            flops = (2 * module.in_channels * module.out_channels * module.kernel_size[0] * module.kernel_size[1] * module.output_size[2] * module.output_size[3]) / (10**9)\n            flops_dict[name] += flops\n        elif isinstance(module, nn.Linear):\n            flops = (2 * module.in_features * module.out_features) / (10**9)\n            flops_dict[name] += flops\n\n    return flops_dict"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        import numpy as np\n        import cv2\n        if interp is None:\n            interp = self.interp\n\n        if self.angle % 360 == 0:\n            return img\n\n        h, w = img.shape[:2]\n        center = (w / 2, h / 2)\n        M = cv2.getRotationMatrix2D(center, self.angle, 1.0)\n        rotated_img = cv2.warpAffine(img, M, (w, h), flags=interp)\n\n        return rotated_img"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        from detectron2.utils.visualizer import Visualizer as VisImage\n        v = VisImage(image, metadata)\n        v = v.draw_instance_predictions(predictions)\n        return v.get_image()"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        import numpy as np\n        rgba_image = self.canvas.get_visualized_image()\n        \n        # Convert RGBA to RGB format\n        rgb_image = rgba_image[:, :, :3]\n        \n        # Convert to numpy ndarray of uint8 type\n        rgb_image = np.array(rgb_image, dtype=np.uint8)\n        \n        return rgb_image"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "                from detectron2.utils.visualizer import Visualizer as VisImage\n        from detectron2.utils.visualizer import Visualizer as VisImage\n        # Create a Visualizer object\n        visualizer = VisImage(img, metadata)\n        \n        # Draw segmentation masks\n        if \"annotations\" in dic:\n            v = visualizer.draw_instance_predictions(dic[\"annotations\"])\n        # Draw keypoints\n        if \"keypoints\" in dic:\n            v = visualizer.draw_keypoints(dic[\"keypoints\"])\n        # Draw bounding boxes\n        if \"annotations\" in dic:\n            v = visualizer.draw_instance_predictions(dic[\"annotations\"])\n        # Draw semantic segmentation\n        if \"sem_seg\" in dic:\n            v = visualizer.draw_sem_seg(dic[\"sem_seg\"])\n        # Draw panoptic segmentation\n        if \"pan_seg\" in dic:\n            v = visualizer.draw_panoptic_seg_predictions(dic[\"pan_seg\"])\n        \n        return v.get_image()"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n    return reloaded_module"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        from matplotlib.collections import PatchCollection\n        from matplotlib.patches import Polygon\n        import matplotlib.pyplot as plt\n        import numpy as np\n        fig, ax = plt.subplots()\n        patches = []\n\n        # Find contours in the binary mask\n        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        for contour in contours:\n            area = cv2.contourArea(contour)\n            if area < area_threshold:\n                continue\n\n            # Create a polygon from the contour\n            polygon = Polygon(contour.reshape(-1, 2), closed=True)\n            patches.append(polygon)\n\n        # Create a PatchCollection from the polygons\n        p = PatchCollection(patches, alpha=alpha, edgecolor=edge_color, facecolor=color)\n        ax.add_collection(p)\n\n        # Add text to the mask if specified\n        if text:\n            ax.text(0, 0, text, color='white', fontsize=12, ha='center')\n\n        ax.autoscale_view()\n        ax.axis('off')\n\n        plt.show()"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert input.__class__ == other.__class__, \"Instances are not of the same class\"\n    assert input.image_size == other.image_size, \"Image sizes are not the same\"\n\n    for field in input.get_fields():\n        if field in [\"gt_boxes\", \"proposal_boxes\", \"pred_boxes\"]:\n            assert_allclose(input[field].tensor, other[field].tensor, rtol=rtol, err_msg=msg)\n        elif field in [\"gt_masks\", \"pred_masks\"]:\n            assert_allclose(input[field].polygons, other[field].polygons, rtol=rtol, err_msg=msg)\n        elif isinstance(input[field], torch.Tensor):\n            assert_allclose(input[field], other[field], rtol=rtol, err_msg=msg)\n        else:\n            assert input[field] == other[field], f\"Field {field} does not match\"\n\n    if size_as_tensor:\n        assert_allclose(input.image_size, other.image_size, rtol=rtol, err_msg=msg)"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    from typing import Any\n    from importlib import import_module\n    try:\n        module_name, class_name = name.rsplit('.', 1)\n        module = import_module(module_name)\n        return getattr(module, class_name)\n    except (ImportError, AttributeError):\n        raise Exception(f\"Unable to locate object with name: {name}\")"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "\n    neck_type = cfg['type']\n    if neck_type in NECKS:\n        neck = NECKS[neck_type](**cfg)\n    else:\n        neck = MMDET_NECKS.build_neck(neck_type, **cfg)\n\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg.get('loss_type', 'mse')\n\n    if loss_type == 'mse':\n        loss_function = MeanSquaredError()\n    elif loss_type == 'cross_entropy':\n        loss_function = CrossEntropyLoss()\n    elif loss_type == 'huber':\n        loss_function = HuberLoss()\n    else:\n        raise ValueError(f\"Unsupported loss type: {loss_type}\")\n\n    return loss_function"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    head_type = cfg.get('type', 'default')\n    if head_type in HEADS:\n        head = HEADS[head_type](cfg)\n    else:\n        head = MMDET_HEADS[head_type](cfg)\n    \n    return head"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None and 'train_cfg' in cfg:\n        raise ValueError(\"train_cfg is duplicated in the cfg dictionary and the function argument.\")\n    if test_cfg is not None and 'test_cfg' in cfg:\n        raise ValueError(\"test_cfg is duplicated in the cfg dictionary and the function argument.\")\n\n    # Build the segmentor model based on the provided configurations\n    # ...\n\n    # Return the segmentor model instance\n    # ..."}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None:\n        assert 'train_cfg' not in cfg, 'train_cfg is specified in both the outer field and model field'\n        cfg['train_cfg'] = train_cfg\n    if test_cfg is not None:\n        assert 'test_cfg' not in cfg, 'test_cfg is specified in both the outer field and model field'\n        cfg['test_cfg'] = test_cfg\n\n    if 'type' in cfg:\n        detector_type = cfg['type']\n        if detector_type in DETECTORS:\n            detector = DETECTORS.get(detector_type)(cfg)\n        elif detector_type in MMDET_DETECTORS:\n            detector = MMDET_DETECTORS.get(detector_type)(cfg)\n        else:\n            raise ValueError(f\"Unrecognized detector type: {detector_type}\")\n        return detector\n    else:\n        raise ValueError(\"Detector type not specified in the configuration dictionary\")"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    import numpy as np\n    val = np.mod(val - offset * period, period) + offset * period\n    return val"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "\n    # Perform evaluation and calculate mAP and mAR\n    evaluation_results = {\n        'class-wise AP': {},\n        'class-wise AR': {},\n        'mAP': 0.0,\n        'mAR': 0.0\n    }\n\n    # Add code here to perform evaluation and calculate mAP and mAR\n\n    return evaluation_results"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        return (\"LiDARBox\", \"LiDAR\")\n    elif box_type == \"Camera\":\n        return (\"CameraBox\", \"Camera\")\n    elif box_type == \"Depth\":\n        return (\"DepthBox\", \"Depth\")\n    else:\n        raise ValueError(\"Unrecognized box type: {}\".format(box_type))"}
{"namespace": "ollama._client.Client.chat", "completion": "    from typing_extensions import Literal\n    from typing import Optional, Sequence, Union, Mapping, Any, Iterator\n    # Your implementation for the chat function goes here\n    if not model:\n        raise RequestError(\"Model identifier is required for generating chat responses\")\n    if messages:\n        for message in messages:\n            if not isinstance(message, Message) and not isinstance(message, dict):\n                raise TypeError(\"Messages must be a list of Message or dict-like objects\")\n            if 'role' not in message or 'content' not in message:\n                raise RequestError(\"Each message must contain a 'role' and 'content'\")\n    # Send request to API endpoint and handle response based on 'stream' parameter\n    if stream:\n        # Return a generator of ChatResponses\n        # Your implementation for streaming chat responses goes here\n        pass\n    else:\n        # Return a single ChatResponse\n        # Your implementation for non-streaming chat response goes here\n        pass"}
{"namespace": "ollama._client.Client.pull", "completion": "    from typing import Union, Mapping, Any, Iterator\n    import requests\n    url = \"http://example.com/pull\"  # Replace with actual URL\n    data = {\"model\": model, \"insecure\": insecure, \"stream\": stream}\n    response = requests.post(url, data=data)\n\n    if response.status_code != 200:\n        raise ResponseError(\"Failed to pull the model\")\n\n    if stream:\n        return (ProgressResponse(data) for data in response.iter_lines())\n    else:\n        return ProgressResponse(response.json())"}
{"namespace": "ollama._client.Client.generate", "completion": "    # Add implementation for generating a response based on the specified model and parameters\n    pass"}
{"namespace": "ollama._client.Client.push", "completion": "    from typing import Union, Mapping, Any, Iterator\n    import requests\n    url = '/api/push'\n    data = {'model': model}\n    if insecure:\n        response = requests.post(url, data=data, verify=False)\n    else:\n        response = requests.post(url, data=data)\n\n    if response.status_code != 200:\n        raise ResponseError(\"Request cannot be fulfilled\")\n\n    if stream:\n        for item in response.json():\n            yield item\n    else:\n        return response.json()"}
{"namespace": "ollama._client.Client.create", "completion": "    from os import PathLike\n    from typing import Optional, Union, Mapping, Any, Iterator\n    if path is None and modelfile is None:\n        raise RequestError(\"Either 'path' or 'modelfile' is required for the operation\")\n\n    if stream:\n        # Handle the request as a stream of responses\n        # Implement the logic to create the model and yield ProgressResponse objects\n        # Return a generator of ProgressResponse objects\n        pass\n    else:\n        # Handle the request as a single response\n        # Implement the logic to create the model and return a single ProgressResponse\n        pass"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    from typing import Union\n    from pathlib import Path\n    import requests\n    import hashlib\n    with open(path, 'rb') as file:\n      file_content = file.read()\n      sha256_checksum = hashlib.sha256(file_content).hexdigest()\n\n    # Check if the blob with the checksum already exists on the server\n    response = requests.head(f'https://server-url/blobs/{sha256_checksum}')\n\n    # If blob not found (404 status code), upload the file as a new blob\n    if response.status_code == 404:\n      upload_response = requests.post('https://server-url/blobs', data=file_content)\n      if upload_response.status_code == 201:\n        return f'sha256:{sha256_checksum}'\n      else:\n        raise Exception(f'Failed to upload blob: {upload_response.status_code}')\n    else:\n      return f'sha256:{sha256_checksum}'"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        for key, value in state_dict.items():\n            setattr(self, key, value)"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict['shuffle'] != self.shuffle:\n            raise ValueError(\"Shuffle parameter mismatch\")\n        if self._state_dict['num_workers'] != self.worker_env.num_workers:\n            raise ValueError(\"Num workers parameter mismatch\")\n        if self._state_dict['input_directory_path'] != self.worker_env.input_directory_path:\n            raise ValueError(\"Input directory path parameter mismatch\")\n        if self._state_dict['url'] != self.worker_env.url:\n            raise ValueError(\"URL parameter mismatch\")\n        if self._state_dict['seed'] != self.worker_env.seed:\n            raise ValueError(\"Seed parameter mismatch\")\n        if self._state_dict['item_loader_state'] != self.worker_env.item_loader.state:\n            raise ValueError(\"Item loader state parameter mismatch\")\n        if self._state_dict['drop_last'] != self.worker_env.drop_last:\n            raise ValueError(\"Drop last parameter mismatch\")"}
{"namespace": "run.parse_args", "completion": "    import argparse\n    parser = argparse.ArgumentParser(description='Parse command line arguments')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode, which can be \"auto\" or \"manual\", specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, specifying how many times a task can be retried upon failure.')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file, specifying where to find the configuration settings.')\n\n    return parser.parse_args()"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    import tempfile\n    import hashlib\n    import os\n    if input_dir is None:\n        input_dir = ''\n\n    cache_dir = os.path.join(tempfile.gettempdir(), hashlib.md5(input_dir.encode('utf-8')).hexdigest())\n\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except Exception as e:\n        return None"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith((\"/dev\", \"/proc\", \"/sys\")):\n        return True\n    else:\n        return False"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    samples_per_worker = num_samples_yielded // batch_size // num_workers\n    remaining_samples = num_samples_yielded % (batch_size * num_workers)\n\n    samples_distribution = {i: samples_per_worker for i in range(num_workers)}\n    for i in range(remaining_samples):\n        samples_distribution[i] += 1\n\n    return samples_distribution"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        import filelock\n        import boto3\n        import os\n        lock = filelock.FileLock(local_filepath + '.lock')\n        with lock:\n            if not remote_filepath.startswith('s3://'):\n                raise ValueError(\"Remote file path must use the 's3' scheme\")\n\n            if os.path.exists(local_filepath):\n                print(f\"File already exists at {local_filepath}\")\n                return\n\n            s3 = boto3.client('s3')\n            bucket, key = remote_filepath.replace('s3://', '').split('/', 1)\n            s3.download_file(bucket, key, local_filepath)"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    chunk_assignment = {}\n    interval_assignment = {}\n\n    for i in range(num_workers):\n        chunk_assignment[i] = []\n        interval_assignment[i] = []\n\n    for i, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % worker_env.world_size\n        chunk_assignment[worker_index].append(chunk)\n        interval_assignment[worker_index].append(interval)\n\n    return chunk_assignment, interval_assignment"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            modified_remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        else:\n            modified_remote_filepath = remote_filepath\n\n        # Call the superclass's download_file method to download the file\n        # from the modified remote file path to the specified local file path\n        super().download_file(modified_remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    updated_indexes = {}\n\n    for worker, intervals in workers_intervals.items():\n        current_index = indexes[worker]\n        interval_index = 0\n        total_intervals = len(intervals)\n\n        # Calculate the total size of intervals for the worker\n        total_interval_size = sum(interval[-1] - interval[0] for interval in intervals)\n\n        # Calculate the chunk index based on the current index and interval sizes\n        while current_index >= intervals[interval_index][-1]:\n            current_index -= intervals[interval_index][-1] - intervals[interval_index][0]\n            interval_index += 1\n\n        chunk_index = interval_index\n\n        # Update the chunk index and the index within the chunk\n        chunk_indexes[worker] = chunk_index\n        updated_indexes[worker] = current_index\n\n    return chunk_indexes, updated_indexes"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        from typing import Tuple, Optional\n        from PIL import Image\n        width, height = item.size\n        # Get image mode\n        mode = item.mode\n        # Get raw pixel data\n        raw_data = item.tobytes()\n\n        # Serialize image data\n        serialized_data = width.to_bytes(4, byteorder='big') + height.to_bytes(4, byteorder='big') + len(mode).to_bytes(1, byteorder='big') + mode.encode('utf-8') + raw_data\n\n        return serialized_data, None"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        from PIL import Image\n        from typing import Tuple, Optional\n        if not isinstance(item, Image):\n            raise TypeError(\"Item should be an instance of Image class or its subclasses\")\n\n        if item.format == \"JPEG\" and item.filename:\n            with open(item.filename, \"rb\") as file:\n                data = file.read()\n            return data, None\n        else:\n            with BytesIO() as output:\n                item.save(output, format=\"JPEG\")\n                data = output.getvalue()\n            return data, None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        import struct\n        import io\n        from PIL import Image\n        width, height, mode_length = struct.unpack('III', data[:12])\n        mode = data[12:12+mode_length].decode('utf-8')\n        image_data = data[12+mode_length:]\n        image = Image.frombytes(mode, (width, height), image_data)\n        return image"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        import struct\n        import torch\n        dtype, shape_length = struct.unpack('!BQ', data[:9])\n        shape = struct.unpack('!' + 'Q' * shape_length, data[9:9 + 8 * shape_length])\n\n        # Reconstruct the tensor from the remaining bytes\n        tensor_data = data[9 + 8 * shape_length:]\n        tensor = torch.tensor(tensor_data, dtype=torch.int64)  # Assuming the dtype is int64, change as per the extracted dtype\n\n        # Reshape the tensor\n        tensor = tensor.view(*shape)\n\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        import torch\n        from typing import Tuple, Optional\n        dtype = item.dtype\n        shape = item.shape\n        data = item.tobytes()\n        serialized_data = dtype.tobytes() + shape.tobytes() + data\n        return serialized_data, None"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  linear = xnp.where(srgb <= 0.04045, srgb / 12.92, ((srgb + 0.055) / 1.055) ** 2.4)\n  \n  return linear"}
{"namespace": "resample.resample_3d", "completion": "    import tensorflow as tf\n    if method == 'TRILINEAR':\n        interpolation = tf.image.ResizeMethod.BILINEAR\n    elif method == 'NEAREST':\n        interpolation = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    else:\n        raise ValueError(\"Invalid method. Must be either 'TRILINEAR' or 'NEAREST'.\")\n\n    if edge_behavior == 'CONSTANT_OUTSIDE':\n        data = tf.pad(data, [[1, 1], [1, 1], [1, 1], [0, 0]], constant_values=constant_values)\n        locations = tf.clip_by_value(locations, 0, tf.cast(tf.shape(data)[0:3], tf.float32) - 1)\n    elif edge_behavior == 'CLAMP':\n        locations = tf.clip_by_value(locations, 0, tf.cast(tf.shape(data)[0:3] - 1, tf.float32))\n    else:\n        raise ValueError(\"Invalid edge_behavior. Must be either 'CONSTANT_OUTSIDE' or 'CLAMP'.\")\n\n    if coordinate_order == 'zyx':\n        locations = tf.reverse(locations, axis=[-1])\n\n    if half_pixel_center:\n        locations -= 0.5\n\n    resampled_data = tf.map_fn(lambda loc: tf.contrib.resampler.resampler(tf.expand_dims(data, 0), tf.expand_dims(loc, 0), method=interpolation), locations)\n\n    return resampled_data"}
{"namespace": "linspline.integrate", "completion": "    integral = 0\n    for i in range(1, len(t)):\n        integral += (w[i-1] + w[i]) * (t[i] - t[i-1]) / 2\n    return integral"}
{"namespace": "linspline.query", "completion": "    from scipy import interpolate\n    import numpy as np\n    if len(t) < 2 or len(v) < 2 or len(t) != len(v):\n        raise ValueError(\"Invalid input: 't' and 'v' must have at least 2 elements and be of the same length\")\n\n    if np.any(np.diff(t) < 0):\n        raise ValueError(\"Invalid input: 't' must be in non-decreasing order\")\n\n    if tq[0] < t[0] or tq[-1] > t[-1]:\n        raise ValueError(\"Invalid input: 'tq' must be within the range of 't'\")\n\n    f = interpolate.interp1d(t, v, kind='linear', fill_value=(0, 0), bounds_error=False)\n    return f(tq)"}
{"namespace": "geometry.are_lines_parallel", "completion": "    import numpy as np\n    epsilon = 1e-10  # small epsilon to account for numerical precision issues\n    dir1_normalized = dir1 / np.linalg.norm(dir1)  # normalize direction vector of the first line\n    dir2_normalized = dir2 / np.linalg.norm(dir2)  # normalize direction vector of the second line\n    dot_product = np.dot(dir1_normalized, dir2_normalized)  # calculate dot product of normalized direction vectors\n    return abs(1 - dot_product) < epsilon  # return True if the lines are parallel within numerical precision, otherwise False"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "import math\n\n  x = r * math.sin(theta) * math.cos(phi)\n  y = r * math.sin(theta) * math.sin(phi)\n  z = r * math.cos(theta)\n\n  return [x, y, z]"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "    import numpy as np\n    if v < 1:\n        raise ValueError(\"Tessellation factor must be greater than or equal to 1\")\n\n    weights = np.zeros((v, v))\n    for i in range(v):\n        for j in range(v):\n            if i + j < v:\n                weights[i, j] = 1 - (i + j) / v\n            else:\n                weights[i, j] = 0\n\n    return weights.flatten()"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = onp.hsplit(cartesian_vector, 3)\n  r = onp.sqrt(x**2 + y**2 + z**2)\n  theta = onp.arccos(z / (r + eps))\n  phi = onp.arctan2(y, x)\n  return r, theta, phi"}
{"namespace": "geopoly.compute_sq_dist", "completion": "    import numpy as np\n    if mat1 is None:\n        mat1 = mat0\n    \n    norm_mat0 = np.sum(mat0**2, axis=0, keepdims=True)\n    norm_mat1 = np.sum(mat1**2, axis=0, keepdims=True)\n    dot_product = np.dot(mat0.T, mat1)\n    \n    sq_dist = norm_mat0.T + norm_mat1 - 2 * dot_product\n    sq_dist[sq_dist < 0] = 0\n    \n    return sq_dist"}
{"namespace": "math.plus_eps", "completion": "    import sys\n    tiny_threshold = sys.float_info.epsilon\n    if x < tiny_threshold:\n        return tiny_threshold\n    else:\n        return x + sys.float_info.epsilon"}
{"namespace": "math.minus_eps", "completion": "    if x >= tiny_val:\n        return x - tiny_val\n    else:\n        return -tiny_val"}
{"namespace": "math.safe_exp", "completion": "import numpy as np\n\n  # Define the range for safe exponential function\n  min_exp_input = -700\n  max_exp_input = 700\n\n  # Apply the safe exponential function\n  safe_x = np.clip(x, min_exp_input, max_exp_input)\n  result = np.exp(safe_x)\n\n  return result"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        import torch\n        from PIL import Image, JpegImagePlugin\n        from typing import Union\n        try:\n            if torch.__version__ >= \"0.8.0\" and JpegImagePlugin is not None:\n                # Try to decode the data using torchvision\n                tensor = torchvision.io.image.decode_image(data)\n                return tensor\n        except RuntimeError:\n            pass\n        \n        # Fallback to using PIL to deserialize the data\n        pil_image = Image.open(io.BytesIO(data))\n        \n        if torch.__version__ >= \"0.8.0\":\n            # Convert the PIL image to a PyTorch tensor\n            tensor = torchvision.transforms.functional.to_tensor(pil_image)\n            return tensor\n        else:\n            return pil_image"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        import numpy as np\n        from typing import Tuple, Optional\n        import torch\n        tensor_data = item.numpy().tobytes()\n        data_type_index = str(item.dtype)\n        serialized_data = \"no_header_tensor:\" + data_type_index\n        return tensor_data, serialized_data"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        import torch\n        return torch.from_buffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        import numpy as np\n        dtype, shape, data = np.lib.format.read_array(data)\n        \n        # Reconstruct the numpy array based on the extracted information\n        array = np.frombuffer(data, dtype=dtype).reshape(shape)\n        \n        return array"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        import numpy as np\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        from typing import Tuple, Optional\n        import numpy as np\n        serialized_bytes = item.tobytes()\n        dtype_identifier = f\"no_header_numpy:{item.dtype}\"\n        return serialized_bytes, dtype_identifier"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        from typing import Tuple, Optional\n        import numpy as np\n        dtype_index = np.dtype(item.dtype).num\n        shape = item.shape\n        data = item.tobytes()\n        metadata = f\"{dtype_index},{len(shape)},{','.join(map(str, shape))}\"\n        serialized_data = metadata.encode() + data\n        return serialized_data, None"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        from typing import Dict, Any\n        state = {\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx\n        }\n        if isinstance(self.dataset, StreamingDataset):\n            state[\"dataset\"] = self.dataset.state_dict()\n        else:\n            state[\"dataset\"] = self.dataset\n        return state"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        import os\n        import tempfile\n        import av\n        import torchvision\n        import torch\n        if not torch.cuda.is_available():\n            raise Exception(\"torchvision library is not installed. Please install torchvision to deserialize video data.\")\n        if not av.open:\n            raise Exception(\"av library is not installed. Please install av to deserialize video data.\")\n\n        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n            temp_file.write(data)\n            temp_file_path = temp_file.name\n\n        video, audio, info = torchvision.io.read_video(temp_file_path, pts_unit='sec')\n\n        os.remove(temp_file_path)\n\n        return video"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if not self._is_done:\n            if self._should_write():\n                self.write_chunk()\n            file_paths = self.write_chunks_index()\n            self._is_done = True\n            return file_paths\n        else:\n            return []"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj.get('current_epoch', self.current_epoch)\n        self.num_samples_yielded = obj.get('num_samples_yielded', self.num_samples_yielded)\n        self.latest_worker_index = obj.get('latest_worker_index', self.latest_worker_index)\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj)\n        else:\n            raise RuntimeError(\"Dataset is neither StreamingDataset nor CombinedStreamingDataset\")"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        from typing import Optional, List, Dict, Any\n        if self.internal_iterator is None and num_samples_yielded is None:\n            return {}\n        else:\n            if self.internal_iterator is not None:\n                return self.internal_iterator.state_dict()\n            else:\n                return {\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                    \"num_samples_yielded\": num_samples_yielded\n                }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for key, value in state_dict.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n            else:\n                for dataset in self.datasets:\n                    if hasattr(dataset, key):\n                        setattr(dataset, key, value)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n    elif isinstance(dir_path, str):\n        if dir_path.startswith('s3://'):\n            return S3Dir(url=dir_path)\n        elif dir_path.startswith('/'):\n            return LocalDir(path=dir_path)\n        else:\n            return ProjectDir(name=dir_path)\n    else:\n        raise ValueError(\"Invalid directory path format\")"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    from contextlib import contextmanager\n    try:\n        # Enable or disable DNS optimization based on the input parameter\n        # Perform operations within the context where DNS optimization is enabled or disabled\n        yield\n    finally:\n        # Ensure that DNS optimization is always disabled after these operations\n        # Reset DNS optimization to disabled"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    total_items = len(indexes)\n    world_size = distributed_env.world_size\n\n    # Calculate the number of items each rank should process\n    items_per_rank = total_items // world_size\n    if not drop_last:\n        items_per_rank += total_items % world_size\n\n    # Assign chunks and their intervals to each rank\n    chunk_indexes_per_rank = []\n    chunk_intervals_per_rank = []\n\n    start_idx = 0\n    for rank in range(world_size):\n        end_idx = start_idx + items_per_rank\n        if not drop_last and rank < total_items % world_size:\n            end_idx += 1\n\n        chunk_indexes_per_rank.append(indexes[start_idx:end_idx])\n        chunk_intervals_per_rank.append(chunk_intervals[start_idx:end_idx])\n\n        start_idx = end_idx\n\n    return chunk_indexes_per_rank, chunk_intervals_per_rank"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        from typing import Any, Callable\n        if hasattr(self, '_fn') and isinstance(self._fn, Callable):\n            kwargs = {'item_metadata': item_metadata, 'output_dir': output_dir}\n            if getattr(self, '_contains_is_last', False):\n                kwargs['is_last'] = is_last\n            self._fn(**kwargs)"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                time.sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import time\n    import psutil\n    while True:\n        disk_usage = psutil.disk_usage(input_dir)\n        free_space_gb = disk_usage.free / (2**30)  # Convert bytes to gigabytes\n        if free_space_gb <= threshold_in_gb:\n            break\n        time.sleep(sleep_time)"}
{"namespace": "litdata.processing.functions.optimize", "completion": "from pytorch_lightning import Trainer\nfrom typing import Callable, Any, Sequence, List, Optional, Union\n\n    # Your optimization code here\n    # Example: Using PyTorch Lightning Trainer for distributed training\n    trainer = Trainer(num_nodes=num_nodes, machine=machine, num_processes=num_workers)\n    trainer.fit(fn, inputs)"}
{"namespace": "litdata.processing.functions.map", "completion": "    from pytorch_lightning import Dir, BaseReader\n    from typing import Callable, Any, Sequence, Union, List, Optional\n    # Implementation of the map function goes here\n    pass"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        task = queue_in.get()\n        if task is None:\n            break\n        index, file_paths = task\n        downloaded_files = []\n        for file_path in file_paths:\n            local_file_path = os.path.join(cache_dir, os.path.basename(file_path))\n            if not os.path.exists(local_file_path):\n                input_dir.download_file(file_path, local_file_path)\n            downloaded_files.append(local_file_path)\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "            import boto3\n        from urllib.parse import urlparse\n        import os\n        import shutil\n    import shutil\n    import os\n    from urllib.parse import urlparse\n\n    def upload_to_local(file_path, target_dir):\n        if not os.path.exists(target_dir):\n            os.makedirs(target_dir)\n        shutil.move(file_path, target_dir)\n        remove_queue.put(file_path)\n\n    def upload_to_s3(file_path, bucket_name, key):\n        import boto3\n        s3 = boto3.client('s3')\n        s3.upload_file(file_path, bucket_name, key)\n        remove_queue.put(file_path)\n\n    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n            file_path = os.path.join(temp_dir, file_path)\n        else:\n            file_path = os.path.join(cache_dir, file_path)\n\n        if output_dir.scheme == 's3':\n            url_parts = urlparse(output_dir.path)\n            bucket_name = url_parts.netloc\n            key = url_parts.path.lstrip('/')\n            upload_to_s3(file_path, bucket_name, key)\n        else:\n            upload_to_local(file_path, output_dir.path)\n\n    upload_queue.task_done()"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    from typing import List, Any, Optional\n    import random\n    if weights is None:\n        weights = [1] * len(user_items)\n\n    total_workers = num_workers * len(weights)\n    items_with_weights = list(zip(user_items, weights))\n    items_with_weights.sort(key=lambda x: x[1], reverse=True)\n\n    worker_items = [[] for _ in range(total_workers)]\n    worker_index = 0\n\n    for item, weight in items_with_weights:\n        worker_items[worker_index].append(item)\n        worker_index = (worker_index + 1) % total_workers\n\n    if file_size:\n        for i in range(len(worker_items)):\n            total_size = sum(weights[user_items.index(item)] for item in worker_items[i])\n            print(f\"Worker {i+1}: Total Size = {total_size} MB\")\n    else:\n        for i in range(len(worker_items)):\n            print(f\"Worker {i+1}: Total Weight = {sum(weights[user_items.index(item)] for item in worker_items[i])}\")\n\n    for i in range(len(worker_items)):\n        random.shuffle(worker_items[i])\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    from typing import List, Any\n    total_workers = _get_num_nodes() * num_workers\n    items_per_worker = len(user_items) // total_workers\n    extra_items = len(user_items) % total_workers\n\n    start = 0\n    result = []\n\n    for i in range(total_workers):\n        if i < extra_items:\n            end = start + items_per_worker + 1\n        else:\n            end = start + items_per_worker\n\n        result.append(user_items[start:end])\n        start = end\n\n    if len(result) != total_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return result"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        import os\n        cache_directories = ['cache_dir1', 'cache_dir2', 'cache_dir3']\n\n        # Remove cache directories if they exist\n        for directory in cache_directories:\n            if os.path.exists(directory):\n                os.rmdir(directory)\n\n        # Recreate cache directories\n        for directory in cache_directories:\n            os.mkdir(directory)"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    from typing import List, Any\n    import os\n    from concurrent.futures import ThreadPoolExecutor\n    def get_file_size(item):\n        item_path = os.path.join(base_path, str(item))\n        return os.path.getsize(item_path)\n\n    with ThreadPoolExecutor() as executor:\n        file_sizes = list(executor.map(get_file_size, items))\n\n    return file_sizes"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    import os\n    if input_dir:\n        element = os.path.join(input_dir, str(element))\n    return os.path.exists(element)"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:] / theta\n  w_hat = jnp.array([\n      [0, -w[2], w[1]],\n      [w[2], 0, -w[0]],\n      [-w[1], w[0], 0]\n  ])\n  R = jnp.eye(3) + jnp.sin(theta) * w_hat + (1 - jnp.cos(theta)) * (w_hat @ w_hat)\n  p = (jnp.eye(3) * theta + (1 - jnp.cos(theta)) * w_hat + (theta - jnp.sin(theta)) * (w_hat @ w_hat)) @ v\n  T = jnp.block([\n      [R, jnp.expand_dims(p, axis=-1)],\n      [jnp.array([0, 0, 0, 1])]\n  ])\n  return T"}
{"namespace": "rigid_body.exp_so3", "completion": "  angle = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (angle + eps)\n  skew_symmetric = jnp.array([\n      [0, -axis[2], axis[1]],\n      [axis[2], 0, -axis[0]],\n      [-axis[1], axis[0], 0]\n  ])\n  R = jnp.eye(3) + jnp.sin(angle) * skew_symmetric + (1 - jnp.cos(angle)) * jnp.dot(skew_symmetric, skew_symmetric)\n  return R"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "        import jax.numpy as jnp\n      mean = t0 * d\n\n      # Calculate the covariance of the Gaussian distribution\n      if diag:\n          # If diagonal covariance is requested, use a diagonal matrix with the variance along each axis\n          variance = (base_radius * (t1 - t0))**2 / 3\n          covariance = jnp.diag(jnp.array([variance, variance, variance]))\n      else:\n          # If full-covariance is requested, use a matrix with the variance along each axis and zero covariance\n          variance = (base_radius * (t1 - t0))**2 / 3\n          covariance = jnp.array([[variance, 0, 0], [0, variance, 0], [0, 0, variance]])\n\n      return mean, covariance"}
{"namespace": "render.cylinder_to_gaussian", "completion": "    mean = (t0 + t1) / 2 * d\n\n    # Calculate the variance of the Gaussian\n    variance = radius ** 2\n\n    # Create the covariance matrix based on the diagonal flag\n    if diag:\n        covariance = jnp.diag(jnp.array([variance, variance, variance], dtype=jnp.float32))\n    else:\n        covariance = jnp.array([[variance, 0, 0], [0, variance, 0], [0, 0, variance]], dtype=jnp.float32)\n\n    return mean, covariance"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates\n  pix_coords = xnp.stack([pix_x_int, pix_y_int], axis=-1)\n  pix_coords = xnp.concatenate([pix_coords, xnp.ones_like(pix_coords[..., :1])], axis=-1)\n  cam_coords = xnp.einsum('...ij,...j->...i', pixtocams, pix_coords)\n\n  # Convert camera coordinates to world coordinates\n  cam_coords = xnp.concatenate([cam_coords, xnp.ones_like(cam_coords[..., :1])], axis=-1)\n  world_coords = xnp.einsum('...ij,...j->...i', camtoworlds, cam_coords)\n\n  # Compute ray origins and directions\n  origins = world_coords[..., :3]\n  directions = world_coords[..., 3:] - origins\n  viewdirs = xnp.linalg.norm(directions, axis=-1, keepdims=True)\n  viewdirs = directions / viewdirs\n\n  # Compute ray differential radii\n  radii = None  # To be implemented based on specific requirements\n\n  # Compute image plane coordinates\n  imageplane = world_coords[..., :2]\n\n  return origins, directions, viewdirs, radii, imageplane"}
{"namespace": "stepfun.weight_to_pdf", "completion": "    pdf = []\n    for i in range(len(t) - 1):\n        pdf.append(w[i] / (t[i + 1] - t[i]))\n    return pdf"}
{"namespace": "render.compute_alpha_weights", "completion": "  \n  adjusted_density = density * tdist  # Compute the product of density and adjusted distance\n  alpha_weights = helper_function(adjusted_density, dirs, **kwargs)  # Call the helper function to compute alpha weights\n  return alpha_weights  # Return the computed alpha weights"}
{"namespace": "stepfun.pdf_to_weight", "completion": "    differences = [t[i+1] - t[i] for i in range(len(t)-1)]\n    \n    # Multiply the PDF values by the differences to obtain the weights\n    weights = [p[i] * differences[i] for i in range(len(p))]\n    \n    # Normalize the weights so that they sum to 1\n    total_weight = sum(weights)\n    normalized_weights = [w / total_weight for w in weights]\n    \n    return normalized_weights"}
{"namespace": "stepfun.sample", "completion": "  if rng is not None:\n      u = jax.random.uniform(rng, shape=(num_samples,)) * (1 - 2 * eps) + eps\n  else:\n      u = jnp.linspace(eps, 1 - eps, num_samples)\n\n  if deterministic_center:\n      u = (u * (t.size - 1)).astype(jnp.int32)\n  else:\n      u = jnp.digitize(u, t[1:-1])\n\n  if single_jitter:\n      jitter = jax.random.uniform(rng, shape=(num_samples,)) * (t[u + 1] - t[u])\n  else:\n      jitter = jax.random.uniform(rng, shape=(num_samples,)) * (t[u + 1] - t[u - 1])\n\n  samples = t[u] + jitter\n  return samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  if rng is None:\n      samples = jnp.linspace(t[0], t[-1], num_samples + 1)\n  else:\n      u = jax.random.uniform(rng, (num_samples,), minval=0, maxval=1)\n      cdf = jnp.cumsum(jax.nn.softmax(w_logits))\n      samples = jnp.interp(u, cdf, t)\n\n  if single_jitter:\n      jitter = (t[1] - t[0]) * jax.random.uniform(rng, (num_samples,), minval=-0.5, maxval=0.5)\n      samples += jitter\n  else:\n      jitter = (t[1] - t[0]) * jax.random.uniform(rng, (num_samples,), minval=-0.5, maxval=0.5)\n      samples = samples + jitter\n\n  midpoints = (samples[1:] + samples[:-1]) / 2\n  intervals = jnp.stack([samples[:-1], samples[1:]], axis=-1)\n\n  intervals = jnp.clip(intervals, domain[0], domain[1])\n\n  return intervals"}
{"namespace": "stepfun.weighted_percentile", "completion": "    import numpy as np\n    if np.sum(w) != 1:\n        raise ValueError(\"Weights must sum to 1\")\n\n    cumulative_weights = np.cumsum(w)\n    result = np.interp(ps, (cumulative_weights - 0.5 * w), t)\n    return result"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  from scipy.ndimage import gaussian_filter1d\n  import numpy as np\n  dt = np.diff(t)\n  pdf = w / np.sum(w * dt)\n\n  # Blur the PDF\n  blurred_pdf = gaussian_filter1d(pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points\n  resampled_weights = np.interp(tq, t, blurred_pdf * np.sum(w * dt))\n\n  return resampled_weights"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    # Add your implementation here\n    pass"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    from typing import Union, Mapping, Any, AsyncIterator\n    import requests\n    if insecure:\n        response = requests.get(f'https://api.example.com/{model}', verify=False)\n    else:\n        response = requests.get(f'https://api.example.com/{model}')\n\n    if response.status_code != 200:\n        raise ResponseError(f\"Failed to pull data for model {model}\")\n\n    if stream:\n        # Assuming ProgressResponse is a custom class representing the response\n        for chunk in response.iter_content(chunk_size=128):\n            yield ProgressResponse(chunk)\n    else:\n        return ProgressResponse(response.json())"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if stream:\n        # If stream is True, return an asynchronous generator of ChatResponse mappings\n        async for response in generate_chat_responses(model, messages, options, keep_alive):\n            yield response\n    else:\n        # If stream is False, return a single ChatResponse as a mapping\n        return generate_single_chat_response(model, messages, options, keep_alive)"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    from typing import Union, Mapping, Any, AsyncIterator\n    import aiohttp\n    url = '/api/push'\n    data = {'model': model, 'insecure': insecure, 'stream': stream}\n\n    async with aiohttp.ClientSession() as session:\n        async with session.post(url, json=data) as response:\n            if stream:\n                async for chunk in response.content.iter_any():\n                    yield chunk\n            else:\n                return await response.json()"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    import aiohttp\n    import hashlib\n    async with aiohttp.ClientSession() as session:\n        # Calculate SHA256 checksum of the file\n        with open(path, 'rb') as file:\n            file_data = file.read()\n            checksum = hashlib.sha256(file_data).hexdigest()\n\n        # Check if blob with checksum exists on the server\n        head_response = await session.head(f'/blobs/{checksum}')\n        if head_response.status == 404:\n            # Upload file in chunks to the server\n            with open(path, 'rb') as file:\n                while True:\n                    chunk = file.read(1024)\n                    if not chunk:\n                        break\n                    post_response = await session.post('/blobs', data=chunk)\n            return f'sha256:{checksum}'\n        else:\n            return f'sha256:{checksum}'"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        from typing import TypeCheckResult\n        # Perform type check using Pyright\n        # Identify lines with expected type errors\n        # Return TypeCheckResult object with the result of the type check\n        pass  # Placeholder for actual implementation"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    from os import PathLike\n    from typing import Optional, Union, Mapping, Any, AsyncIterator\n    from aiohttp import ClientSession\n    import aiohttp\n    if path is None and modelfile is None:\n        raise RequestError(\"Either 'path' or 'modelfile' must be provided for the request\")\n\n    url = \"http://example.com/create_model\"  # Replace with actual API endpoint\n    data = {\"model\": model}\n\n    if path:\n        with open(path, 'rb') as file:\n            data['file'] = file\n    elif modelfile:\n        data['modelfile'] = modelfile\n\n    async with ClientSession() as session:\n        if stream:\n            async with session.post(url, data=data) as response:\n                async for chunk in response.content.iter_any():\n                    # Process streaming response\n                    pass\n        else:\n            async with session.post(url, data=data) as response:\n                # Process single response\n                return await response.json()"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    import torch\n    if isinstance(fn, torch.nn.Module):\n        # Compile the module using a forward and backward compiler specific for modules\n        compiled_module = compile_module(fn)\n        return compiled_module\n    else:\n        # Assume it's a function and compile it using a forward and backward compiler specific for functions\n        compiled_function = compile_function(fn)\n        return compiled_function"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    from typing import Optional, Dict\n    import yaml\n    import pandas as pd\n    import os\n    summary_file = os.path.join(trial_path, 'summary.csv')\n    config_file = os.path.join(trial_path, 'config.yaml')\n\n    if not os.path.exists(summary_file) or not os.path.exists(config_file):\n        raise FileNotFoundError(\"Summary.csv or config.yaml not found in the trial directory\")\n\n    summary_df = pd.read_csv(summary_file)\n    best_config = summary_df.loc[summary_df['metric'].idxmax()]['config_id']\n\n    with open(config_file, 'r') as file:\n        config_data = yaml.safe_load(file)\n\n    best_pipeline_config = config_data[best_config]\n\n    if output_path:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"Output file path must have .yaml or .yml extension\")\n        with open(output_path, 'w') as file:\n            yaml.dump(best_pipeline_config, file)\n\n    return best_pipeline_config"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import threading\n    from functools import lru_cache\n    import torch\n    cache = {}\n    lock = threading.Lock()\n\n    @lru_cache(maxsize=None)\n    def _trace(*args, **kwargs):\n        nonlocal cache\n        nonlocal lock\n\n        key = (func, args, frozenset(kwargs.items()))\n        with lock:\n            if key not in cache:\n                if isinstance(func, torch.nn.Module):\n                    traced_func = torch.jit.trace(func.forward, example_inputs=args, **kwargs_)\n                else:\n                    traced_func = torch.jit.trace(func, example_inputs=args, **kwargs_)\n\n                if ts_compiler:\n                    traced_func = ts_compiler(traced_func, **kwargs_)\n\n                cache[key] = traced_func\n\n        return cache[key](*args, **kwargs)\n\n    return _trace"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        import json\n        import os\n        config_path = os.path.join(trial_path, 'best_config.json')\n        with open(config_path, 'r') as f:\n            best_config = json.load(f)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_dir)"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    from typing import List, Tuple\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            if metadatas:\n                filtered_metadatas.append(metadatas[i])\n            else:\n                filtered_metadatas.append(None)\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    import time\n    import pandas as pd\n    results = []\n    execution_times = []\n\n    for module, params in zip(modules, module_params):\n        start_time = time.time()\n        result = module(**params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n        results.append(result)\n        execution_times.append(execution_time)\n\n    # Apply strategies to select the best result\n    # For example, you can use evaluation metrics and speed thresholds to select the best result\n\n    best_result = results[0]  # Placeholder, replace with actual selection based on strategies\n\n    # Save results and summary of execution times and evaluation metrics to disk\n    # For example, you can save the best result dataframe to a file and save the execution times and evaluation metrics to a summary file\n\n    return best_result"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    from typing import List, Tuple\n    normalized_scores = []\n    for i in range(len(scores)):\n        total = sum(scores[i])\n        normalized = [s / total for s in scores[i]]\n        normalized_scores.append(normalized)\n\n    # Combine scores using convex combination\n    combined_scores = []\n    for i in range(len(ids[0])):\n        combined = sum([normalized_scores[j][i] * weights[j] for j in range(len(weights))])\n        combined_scores.append(combined)\n\n    # Select top_k results\n    sorted_results = sorted(zip(ids[0], combined_scores), key=lambda x: x[1], reverse=True)[:top_k]\n    fused_ids = [result[0] for result in sorted_results]\n    fused_scores = [result[1] for result in sorted_results]\n\n    return [fused_ids], [fused_scores]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    from typing import List, Tuple\n    weighted_sums = []\n    for i in range(len(ids)):\n        weighted_sum = sum([score * weights[i] for score in scores[i]])\n        weighted_sums.append(weighted_sum)\n\n    sorted_ids = [id for sublist in ids for id in sublist]\n    sorted_scores = [score for sublist in scores for score in sublist]\n    sorted_ids, sorted_scores = zip(*sorted(zip(sorted_ids, sorted_scores, weighted_sums), key=lambda x: x[2], reverse=True))\n\n    return sorted_ids[:top_k], sorted_scores[:top_k]"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        from typing import List, Any\n        # Add code here to clean up the agents\n        cleaned_agents = self.clean_up_agents()\n        return cleaned_agents"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if isinstance(agent.purpose_embedding, np.ndarray) else agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n        return agent_dict"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            # Generate prompt using goal and sample input\n            prompt = f\"Goal: {goal}\\nSample Input: {sample_input}\"\n            \n            # Call OpenAI wrapper to get chat completion\n            # chat_completion = openai_wrapper(prompt)\n            \n            # Return the chat completion if successful\n            # return chat_completion\n            return prompt  # For demonstration purposes, returning the prompt itself\n        except Exception as e:\n            # Log the exception\n            print(f\"Error occurred: {e}\")\n            # Return an empty string\n            return \"\""}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        for key, value in data.items():\n            if hasattr(agent, key):\n                setattr(agent, key, value)\n        return agent"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        import sqlite3\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        c.execute(\"CREATE TABLE IF NOT EXISTS agents (id INTEGER PRIMARY KEY, purpose TEXT, other_data TEXT)\")\n\n        c.execute(\"SELECT * FROM agents WHERE id=?\", (agent_dict['id'],))\n        existing_record = c.fetchone()\n\n        if existing_record:\n            c.execute(\"UPDATE agents SET purpose=?, other_data=? WHERE id=?\", (agent_dict['purpose'], agent_dict['other_data'], agent_dict['id']))\n        else:\n            c.execute(\"INSERT INTO agents (id, purpose, other_data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], agent_dict['other_data']))\n\n        conn.commit()\n        conn.close()"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        import pickle\n        import sqlite3\n        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT agent_data FROM agents WHERE purpose=?\", (purpose,))\n        agent_data = cursor.fetchone()\n        conn.close()\n\n        if agent_data:\n            return pickle.loads(agent_data[0])\n        else:\n            return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        import sqlite3\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n        c.execute(\"SELECT purpose FROM agents\")\n        purposes = [row[0] for row in c.fetchall()]\n        conn.close()\n        return purposes"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        import hashlib\n        data = func_name.encode('utf-8')\n        for arg in args:\n            data += str(arg).encode('utf-8')\n        for key, value in kwargs.items():\n            data += str(key).encode('utf-8') + str(value).encode('utf-8')\n        return hashlib.sha256(data).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        import json\n        import sqlite3\n        conn = sqlite3.connect('cache.db')\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n        result = cursor.fetchone()\n        conn.close()\n        \n        if result:\n            return json.loads(result[0])\n        else:\n            return None"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import pickle\n    import sqlite3\n    def decorator(func):\n        conn = sqlite3.connect(filename)\n        c = conn.cursor()\n        c.execute('''CREATE TABLE IF NOT EXISTS memoization_cache\n                     (func_name text, input text, output text)''')\n        conn.commit()\n\n        def wrapper(*args, **kwargs):\n            input_str = pickle.dumps((args, kwargs))\n            c.execute(\"SELECT output FROM memoization_cache WHERE func_name=? AND input=?\", (func_name, input_str))\n            result = c.fetchone()\n            if result:\n                return pickle.loads(result[0])\n            else:\n                output = func(*args, **kwargs)\n                output_str = pickle.dumps(output)\n                c.execute(\"INSERT INTO memoization_cache VALUES (?, ?, ?)\", (func_name, input_str, output_str))\n                conn.commit()\n                return output\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        import json\n        import sqlite3\n        conn = sqlite3.connect('cache.db')\n        c = conn.cursor()\n        c.execute('CREATE TABLE IF NOT EXISTS cache (arg_hash TEXT PRIMARY KEY, result TEXT)')\n        c.execute('INSERT OR REPLACE INTO cache (arg_hash, result) VALUES (?, ?)', (arg_hash, json.dumps(result)))\n        conn.commit()\n        conn.close()"}
{"namespace": "run.execute_command_line_process", "completion": "    import sys\n    import subprocess\n    command = args.command\n    if quiet_mode:\n        with open('output.txt', 'w') as f:\n            subprocess.run(command, shell=True, stdout=f, stderr=subprocess.STDOUT)\n    else:\n        subprocess.run(command, shell=True, check=True)"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Extracting model from kwargs\n        model = kwargs.get('model')\n\n        # Making a request to the OpenAI API\n        response = openai.ChatCompletion.create(\n            model=model,\n            **kwargs\n        )\n\n        return response"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        import boto3\n        if self.s3_client is None or (time.time() - self.creation_time) > self.expiry_interval:\n            self.s3_client = boto3.client('s3')\n            self.creation_time = time.time()\n        return self.s3_client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        from typing import Dict, Any\n        state = {\n            'num_samples_yielded': num_samples_yielded,\n            'num_workers': num_workers,\n            'batch_size': batch_size,\n            'current_epoch': self.current_epoch,  # Assuming current_epoch is an attribute of the StreamingDataset instance\n            'input_directory_path': self.input_directory_path,  # Assuming input_directory_path is an attribute of the StreamingDataset instance\n            'url': self.url,  # Assuming url is an attribute of the StreamingDataset instance\n            'item_loader_state': self.item_loader_state,  # Assuming item_loader_state is an attribute of the StreamingDataset instance\n            'last_batch_dropped': self.last_batch_dropped,  # Assuming last_batch_dropped is an attribute of the StreamingDataset instance\n            'seed': self.seed,  # Assuming seed is an attribute of the StreamingDataset instance\n            'world_size': self.world_size,  # Assuming world_size is an attribute of the StreamingDataset instance\n            'shuffle_status': self.shuffle_status  # Assuming shuffle_status is an attribute of the StreamingDataset instance\n        }\n        return state"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)  # Resize rendering context\n        eglctx.render(self, camera)  # Render the Mesh instance using the camera's settings"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        nomic_specific_attribute_1=0.0,  # Add new attribute specific to Nomic model\n        nomic_specific_attribute_2=\"default_value\"  # Add another new attribute specific to Nomic model\n    )\n\n    return nomic_config"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        program.bind()\n        camera.upload_gl_uniforms(program)\n\n        self.vao.bind()\n\n        if self.render_type == RenderType.POINTS:\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n        elif self.render_type == RenderType.LINES:\n            if self.ebo:\n                glDrawElements(GL_LINES, len(self.indices), GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_LINES, 0, len(self.vertices))\n        elif self.render_type == RenderType.TRIANGLES:\n            if self.ebo:\n                glDrawElements(GL_TRIANGLES, len(self.indices), GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n        elif self.render_type == RenderType.QUADS:\n            if self.ebo:\n                glDrawElements(GL_QUADS, len(self.indices), GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_QUADS, 0, len(self.vertices))\n        elif self.render_type == RenderType.TRIANGLE_STRIPS:\n            if self.ebo:\n                glDrawElements(GL_TRIANGLE_STRIP, len(self.indices), GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLE_STRIP, 0, len(self.vertices))\n\n        self.vao.unbind()"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        \n        # Convert PyTorch tensor to numpy array if necessary\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().detach().numpy()\n        \n        # Upload the data to the texture in OpenGL\n        glBindTexture(GL_TEXTURE_2D, self.texture_id)\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_UNSIGNED_BYTE, ptr)\n        glBindTexture(GL_TEXTURE_2D, 0)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate input shapes\n    assert R.shape[0] == tvec.shape[0] == camera_matrix.shape[0] == image_size.shape[0], \"Input batch sizes do not match\"\n    assert R.shape[1:] == (3, 3), \"Rotation matrix shape should be (batch_size, 3, 3)\"\n    assert tvec.shape[1:] == (3,), \"Translation vector shape should be (batch_size, 3)\"\n    assert camera_matrix.shape[1:] == (3, 3), \"Camera matrix shape should be (batch_size, 3, 3)\"\n    assert image_size.shape[1:] == (2,), \"Image size shape should be (batch_size, 2)\"\n\n    # Compute camera position\n    tvec = tvec.unsqueeze(-1)  # Add a dimension for matrix multiplication\n    camera_position = -torch.bmm(R.transpose(1, 2), tvec).squeeze(-1)\n\n    # Compute camera rotation\n    rotation = R\n\n    # Compute intrinsic parameters\n    fx = camera_matrix[:, 0, 0] / image_size[:, 1]\n    fy = camera_matrix[:, 1, 1] / image_size[:, 0]\n    cx = (camera_matrix[:, 0, 2] / image_size[:, 1]) - 0.5\n    cy = (camera_matrix[:, 1, 2] / image_size[:, 0]) - 0.5\n    sensor_width = 2 * fx * znear\n\n    # Concatenate camera parameters\n    camera_params = torch.stack([camera_position, rotation, fx, fy, cx, cy, sensor_width], dim=1)\n\n    return camera_params"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            # Fallback to simpler blit method for drawing\n            # Implement blit method here\n            pass\n        else:\n            # Set up specific viewport and scissor box for rendering\n            # Activate shader program\n            # Bind texture\n            # Draw quadrilateral using vertex data\n            # Restore viewport and scissor box to their original sizes\n            pass"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract camera parameters from batch\n    R = batch.R  # Rotation matrix\n    T = batch.T  # Translation vector\n    K = batch.K  # Intrinsic matrix\n    H = batch.H  # Height of the images\n    W = batch.W  # Width of the images\n\n    # Adjust rotation matrix to match PyTorch3D's requirements\n    R = R.T  # Transpose the rotation matrix\n\n    # Adjust translation vector to match PyTorch3D's requirements\n    T = -R @ T  # Apply the transposed rotation to the translation vector\n\n    # Compute the camera center in the camera's coordinate system\n    C = -R.T @ T  # Compute the camera center\n\n    # Recalculate the intrinsic matrix for NDC\n    K[0, 0] = K[0, 0] / W\n    K[0, 2] = K[0, 2] / W\n    K[1, 1] = K[1, 1] / H\n    K[1, 2] = K[1, 2] / H\n\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        pass"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    import numpy as np\n\n    # Calculate the inner measure\n    inner_measure = np.interp(t0, t1, np.cumsum(y1))\n\n    # Calculate the outer measure\n    outer_measure = np.interp(t0, t1, np.cumsum(y1), right=np.nan)\n\n    return inner_measure, outer_measure"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the upper envelope weights\n    upper_env_weights = torch.nn.functional.grid_sample(w_env.unsqueeze(0).unsqueeze(0), ((t - t_env.min()) / (t_env.max() - t_env.min())).unsqueeze(1).unsqueeze(1)).squeeze()\n\n    # Calculate the difference between target weights and upper envelope weights\n    diff = w - upper_env_weights\n\n    # Calculate the scaled half-quadratic loss\n    loss = torch.sum(diff**2 / (diff**2 + eps))\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    inter_interval_loss = torch.sum(w * torch.abs(t[..., :-1] - t[..., 1:]))\n\n    # Calculate intra-interval loss\n    intra_interval_loss = torch.sum((1 - w) * torch.abs(t[..., 1:-1] - (t[..., :-2] + t[..., 2:]) / 2))\n\n    # Combine inter-interval and intra-interval losses to produce total distortion loss\n    total_distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return total_distortion_loss"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    \n    # Sort the values and weights based on the values in 't'\n    sorted_indices = torch.argsort(t)\n    sorted_t = t[sorted_indices]\n    sorted_w = w[sorted_indices]\n\n    # Compute the cumulative sum of the weights\n    cumsum_w = torch.cumsum(sorted_w, dim=0)\n\n    # Interpolate the cumulative sum of weights to find the percentiles\n    percentiles = []\n    for p in ps:\n        idx = torch.searchsorted(cumsum_w, p, right=True)\n        if idx == 0:\n            percentile = sorted_t[0]\n        elif idx == len(cumsum_w):\n            percentile = sorted_t[-1]\n        else:\n            lower_w = cumsum_w[idx - 1]\n            upper_w = cumsum_w[idx]\n            lower_t = sorted_t[idx - 1]\n            upper_t = sorted_t[idx]\n            percentile = lower_t + (p - lower_w) * (upper_t - lower_t) / (upper_w - lower_w)\n        percentiles.append(percentile)\n\n    return torch.tensor(percentiles)"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Calculate the cumulative sum of the weights\n    cdf = torch.cumsum(w, dim=0)\n\n    # Normalize the cumulative sum to get the probability distribution\n    cdf /= cdf[-1]\n\n    # Generate random numbers from a uniform distribution\n    u = torch.rand(num_samples)\n\n    # Use the inverse transform sampling method to generate samples\n    indices = torch.searchsorted(cdf, u, right=True)\n\n    # Get the corresponding bin endpoints for the generated samples\n    samples = t[indices]\n\n    # Apply perturbation if specified\n    if perturb:\n        if single_jitter:\n            # Apply the same jitter to every sample along each dimension\n            jitter = torch.rand(samples.shape) * (t[1] - t[0])\n            samples += jitter\n        else:\n            # Apply independent jitter to each sample\n            jitter = torch.rand(samples.shape) * (t[1] - t[0])\n            samples += jitter\n\n    return samples"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        width = self.data[:, 2] - self.data[:, 0]  # Calculate the width of each box\n        height = self.data[:, 3] - self.data[:, 1]  # Calculate the height of each box\n        area = width * height  # Calculate the area of each box\n        return area"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    proposal_generator_name = cfg.PROPOSAL_GENERATOR.NAME\n    if proposal_generator_name == \"PrecomputedProposals\":\n        return None\n    else:\n        proposal_generator = ProposalGeneratorRegistry.get(proposal_generator_name)(cfg, input_shape)\n        return proposal_generator"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        loss_cls = F.cross_entropy(scores, torch.cat([proposal.gt_classes for proposal in proposals]))\n        \n        gt_boxes = torch.cat([proposal.gt_boxes.tensor for proposal in proposals])\n        proposal_boxes = torch.cat([proposal.proposal_boxes.tensor for proposal in proposals])\n        loss_box_reg = smooth_l1_loss(proposal_deltas, gt_boxes - proposal_boxes)\n        \n        return {\n            \"loss_cls\": loss_cls,\n            \"loss_box_reg\": loss_box_reg\n        }"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "\n    # Retrieve the tracker name from the configuration\n    tracker_name = cfg.MODEL.TRACKER_NAME\n\n    # Look up the corresponding tracker class from a registry\n    if tracker_name not in TRACKER_REGISTRY:\n        raise ValueError(f\"Tracker '{tracker_name}' not found in the registry\")\n\n    tracker_class = TRACKER_REGISTRY[tracker_name]\n\n    # Instantiate and return an object of the tracker class using the configuration\n    return tracker_class(cfg)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        import torch\n        N, _ = deltas.shape\n        k = deltas.shape[1] // 4\n        deltas = deltas.view(N, k, 4)\n\n        # Unpack the boxes\n        x_ctr, y_ctr, width, height = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n\n        # Compute the new positions and sizes of the boxes\n        dx = deltas[:, :, 0]\n        dy = deltas[:, :, 1]\n        dw = deltas[:, :, 2]\n        dh = deltas[:, :, 3]\n\n        new_x_ctr = dx * width[:, None] + x_ctr[:, None]\n        new_y_ctr = dy * height[:, None] + y_ctr[:, None]\n        new_width = torch.exp(dw) * width[:, None]\n        new_height = torch.exp(dh) * height[:, None]\n\n        # Pack the new positions and sizes into transformed boxes\n        transformed_boxes = torch.stack([new_x_ctr, new_y_ctr, new_width, new_height], dim=2)\n\n        return transformed_boxes.view(N, -1)"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        processed_output = self.process_image(image)\n        \n        # If no specific annotation type is requested, return the entire processed output\n        if anno_type is None:\n            return processed_output\n        \n        # If a specific annotation type is requested and found in the output, only that annotation is returned\n        if isinstance(anno_type, str):\n            if anno_type in processed_output:\n                return processed_output[anno_type]\n            else:\n                return None\n        \n        # If multiple annotation types are requested, return a dictionary of those types found in the output\n        if isinstance(anno_type, list):\n            filtered_output = {}\n            for anno in anno_type:\n                if anno in processed_output:\n                    filtered_output[anno] = processed_output[anno]\n            return filtered_output"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        from typing import Dict\n        # Perform search and calculate BM25 scores\n        results: Dict[str, float] = {}  # Replace with actual search results and scores\n        return results"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for document in documents:\n            url, content = document\n            self.index(url, content)"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        from typing import Tuple\n        # Add your implementation here\n        pass"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        types_count = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        for item in self.data:\n            if 'type' in item:\n                types_count[item['type']] += 1\n\n        return types_count"}
{"namespace": "common.bleu4_score", "completion": "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport nltk\n\n    # Tokenize the input texts\n    continuation_tokens = nltk.word_tokenize(continuation)\n    reference_tokens = nltk.word_tokenize(reference)\n\n    # Calculate BLEU-4 score\n    if with_penalty:\n        smoothing_function = SmoothingFunction().method4\n        score = sentence_bleu([reference_tokens], continuation_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing_function)\n    else:\n        score = sentence_bleu([reference_tokens], continuation_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n\n    return score"}
{"namespace": "common.rougeL_score", "completion": "            from nltk.translate.bleu_score import SmoothingFunction\n            from nltk.translate.bleu_score import sentence_bleu\n        def rouge_l_fscore(hypothesis, reference):\n            \"\"\"\n            Compute ROUGE-L score given a hypothesis and a reference.\n            \"\"\"\n            if len(reference) == 0:\n                return 0.0\n            if len(hypothesis) == 0:\n                return 0.0\n            reference_words = reference.split()\n            hypothesis_words = hypothesis.split()\n            lcs = [[0] * (len(hypothesis_words) + 1) for _ in range(len(reference_words) + 1)]\n            for i in range(1, len(reference_words) + 1):\n                for j in range(1, len(hypothesis_words) + 1):\n                    if reference_words[i - 1] == hypothesis_words[j - 1]:\n                        lcs[i][j] = lcs[i - 1][j - 1] + 1\n                    else:\n                        lcs[i][j] = max(lcs[i - 1][j], lcs[i][j - 1])\n            return lcs[len(reference_words)][len(hypothesis_words)] / len(reference_words)\n\n        return rouge_l_fscore(continuation, reference)"}
{"namespace": "chat.utils.run_cmd", "completion": "    import subprocess\n    print(f\"Executing command: {cmd}\")\n    try:\n        result = subprocess.run(cmd, shell=True, check=True)\n        return result.returncode\n    except subprocess.CalledProcessError as e:\n        return e.returncode"}
