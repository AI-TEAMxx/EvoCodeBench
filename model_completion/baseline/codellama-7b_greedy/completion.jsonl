{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return system(cmd)\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    # Checking if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        return False\n\n    # Checking if the matrix is positive definite\n    if not np.all(np.linalg.eigvals(x) > 0):\n        return False\n\n    # Checking if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        return False\n\n    # Checking if the matrix is positive definite\n    if not np.all(np.linalg.eigvals(x) > 0):\n        return False\n\n    # Checking if the matrix is positive definite\n    if not np.all(np.linalg.eigvals(x) > 0):\n        return False\n\n    # Checking if the matrix is positive definite\n    if not np.all(np.linalg.eigvals(x) > 0):\n        return False\n\n    # Checking if the matrix is positive definite\n    if not np.all(np.linalg.eigvals(x) > 0):\n        return False\n\n    # Checking if the matrix is positive definite\n    if not np.all(np.linalg.eigvals(x) > 0):\n        return False\n\n    # Checking if the matrix is positive definite\n    if not np.all(np.linalg.eigvals(x) > 0):\n        return False\n\n    # Checking if the matrix is positive definite\n    if not np.all(np.linalg.eigvals(x) > 0):\n        return False\n\n    # Checking if the matrix is positive definite\n    if not np.all(np.linalg.eigvals(x) > 0):\n        return False\n\n    # Checking if the matrix is positive definite\n    if not np.all(np.linalg.eigvals(x) > 0):\n        return False\n\n    # Checking if the matrix is positive definite\n    if not np.all(np.linalg.eigval"}
{"namespace": "coord.inv_contract", "completion": "  return z / 100"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import pickle\n\n    def decorator(func):\n\n        def wrapper(*args, **kwargs):\n\n            # Connect to the database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Check if the function name is already in the database\n            c.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='{}'\".format(func_name))\n            if c.fetchone() is None:\n\n                # Create a table for the function name\n                c.execute(\"CREATE TABLE {} (args text, kwargs text, result text)\".format(func_name))\n\n            # Check if the function call is already in the database\n            c.execute(\"SELECT result FROM {} WHERE args=? AND kwargs=?\".format(func_name), (pickle.dumps(args), pickle.dumps(kwargs)))\n            result = c.fetchone()\n\n            # If the function call is not in the database, compute the result and store it in the database\n            if result is None:\n\n                result = func(*args, **kwargs)\n                c.execute(\"INSERT INTO {} VALUES (?, ?, ?)\".format(func_name), (pickle.dumps(args), pickle.dumps(kwargs), pickle.dumps(result)))\n                conn.commit()\n\n            # Close the connection to the database\n            conn.close()\n\n            return pickle.loads(result[0])\n\n        return wrapper\n\n    return decorator"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    # Check if the bounding box is valid\n    if values[\"x_min\"] > values[\"x_max\"] or values[\"y_min\"] > values[\"y_max\"]:\n        raise ValueError(f\"Invalid bounding box for {cls.__name__}. The minimum x and y values must be less than the maximum x and y values, respectively.\")\n\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # If mat1 is not provided, use mat0 for both sets of vectors\n  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the norms of the vectors in mat0 and mat1\n  norm0 = np.linalg.norm(mat0, axis=0)\n  norm1 = np.linalg.norm(mat1, axis=0)\n\n  # Compute the dot product between the vectors in mat0 and mat1\n  dot = np.dot(mat0.T, mat1)\n\n  # Compute the squared distances\n  sq_dist = norm0 ** 2 - 2 * dot + norm1 ** 2\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return False\n\n    if path == \"\":\n        return False\n\n    if path.startswith(\"~\"):\n        return True\n\n    if path.startswith(\"/\"):\n        return True\n\n    if path.startswith(\"./\"):\n        return True\n\n    if path.startswith(\"../\"):\n        return True\n\n    return False\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Convert the input items into a numpy array\n    items_array = np.array(items)\n\n    # Verify the input items array's shape\n    if dim == 1:\n        if items_array.ndim != 1:\n            raise ValueError(\n                f\"The input {name} array must be 1-dimensional, but it is {items_array.ndim}-dimensional.\"\n            )\n        if items_array.shape[0] != n_assets:\n            raise ValueError(\n                f\"The input {name} array must have {n_assets} elements, but it has {items_array.shape[0]} elements.\"\n            )\n    elif dim == 2:\n        if items_array.ndim != 2:\n            raise ValueError(\n                f\"The input {name} array must be 2-dimensional, but it is {items_array.ndim}-dimensional.\"\n            )\n        if items_array.shape[1] != n_assets:\n            raise ValueError(\n                f\"The input {name} array must have {n_assets} elements, but it has {items_array.shape[1]} elements.\"\n            )\n    else:\n        raise ValueError(\n            f\"The input {name} dimension must be either 1 or 2, but it is {dim}.\"\n        )\n\n    # Fill in missing values in the input items array\n    if isinstance(items, dict):\n        if dim == 1:\n            if assets_names is None:\n                raise ValueError(\n                    f\"The input {name} dictionary must have keys, but it does not have keys.\"\n                )\n            if len(assets_names) != n_assets:\n                raise ValueError(\n                    f\"The input {name} dictionary must have {n_assets} keys, but it has {len(assets_names)} keys.\"\n                )\n            items_array = np.array(\n                [\n                    items[asset_name] if asset_name in items else fill_value\n                    for asset_name in assets_names\n                ]\n            )\n        elif dim == 2:\n            if assets"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Create a new MicroAgent object\n        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n\n        # Set the agent's attributes\n        for key, value in data.items():\n            setattr(agent, key, value)\n\n        return agent"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = jnp.finfo(xnp.float32).eps\n\n  return jnp.where(\n    srgb <= 0.04045,\n    srgb / 12.92,\n    jnp.power((srgb + eps) / (1 + eps), 2.4)\n  )\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import splrep, splev\n\n  # Check that the input is valid\n  if len(x) != len(t_input):\n    raise ValueError('The input signal and the input times must have the same length.')\n\n  # Check that the output is valid\n  if len(t_output) < 1:\n    raise ValueError('The output times must have at least one element.')\n\n  # Check that the spline degree is valid\n  if spline_degree < 1:\n    raise ValueError('The spline degree must be at least 1.')\n\n  # Check that the smoothness is valid\n  if smoothness < 0:\n    raise ValueError('The smoothness parameter must be non-negative.')\n\n  # Check that the input times are sorted\n  if not np.all(np.diff(t_input) > 0):\n    raise ValueError('The input times must be sorted.')\n\n  # Check that the output times are sorted\n  if not np.all(np.diff(t_output) > 0):\n    raise ValueError('The output times must be sorted.')\n\n  # Check that the output times are within the input time range\n  if np.min(t_output) < np.min(t_input) or np.max(t_output) > np.max(t_input):\n    raise ValueError('The output times must be within the input time range.')\n\n  # Check that the input and output times have the same dimension\n  if len(t_input) != len(t_output):\n    raise ValueError('The input and output times must have the same length.')\n\n  # Check that the input and output times have the same dimension\n  if len(t_input) < spline_degree:\n    raise ValueError('The spline degree must be at most one less than the number of input times.')\n\n  # Check that the input and output times have the same dimension\n  if len(t_input) < spline_degree:\n    raise ValueError('The spline degree must be at most one less than the"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # if lower no uppers after\n    if word.islower():\n        return word\n\n    # if upper no\n    if word.isupper():\n        return word.lower()\n\n    # if mixed case\n    if word[0].isupper() and word[1].islower():\n        return word.lower()\n    elif word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not isinstance(v, np.ndarray):\n        raise ValueError(f\"{cls.__name__} {field.name} must be a numpy array.\")\n\n    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__} {field.name} must be a binary array (containing only True or False values).\")\n\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  import numpy as np\n\n  # Calculate the norm of the input array\n  x_norm = np.linalg.norm(x)\n\n  # Calculate the scaled version of the input array\n  x_scaled = x / x_norm\n\n  return x_scaled\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # If no columns are specified, default to ['module_params']\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Load the summary file into a DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Convert the specified columns into dictionaries\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  if mode == 'fast':\n    det = np.linalg.det(cov)\n    iso = np.ones(cov.shape) * det\n  elif mode == 'accurate':\n    det = np.linalg.slogdet(cov)\n    iso = np.ones(cov.shape) * det[1]\n  else:\n    raise ValueError('Invalid mode.')\n\n  return iso"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Run a task.\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be retried upon failure.\")\n    parser.add_argument(\"--config-file\", type=str, default=os.getenv(\""}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if not isinstance(v, np.ndarray):\n        raise ValueError(f\"{cls.__name__} expected a numpy array for field `{field.name}`, but got `{v}` instead.\")\n\n    if v.shape[-1] != 2:\n        raise ValueError(f\"{cls.__name__} expected a numpy array of shape (_, 2) for field `{field.name}`, but got `{v.shape}` instead.\")\n\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = 'abcdefghijklmnopqrstuvwxyz0123456789_'\n\n    # Return the encoded character\n    return char_set[n]\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(eps, x))\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    # iterate through each worker's intervals\n    for worker_index, intervals in workers_intervals.items():\n\n        # iterate through each interval\n        for interval in intervals:\n\n            # update the current index and the chunk index\n            indexes[worker_index] += interval[1] - interval[0]\n            indexes[worker_index] %= interval[1] - interval[0]\n\n    return indexes\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  import numpy as np\n  from . import trilinear_grid\n  from . import trilinear_hash\n\n  if datastructure == 'grid':\n    return trilinear_grid.trilinear_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilinear_hash.trilinear_hash(values, coordinates)\n  else:\n    raise ValueError('Invalid datastructure.')"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Import the required modules\n  import numpy as np\n\n  # Check if the tessellation factor is valid\n  if v < 1:\n    raise ValueError(\"The tessellation factor must be greater than or equal to 1.\")\n\n  # Generate the integer weights for each vertex of the triangle\n  w = np.arange(v, v * 3 + 1, v)\n\n  # Normalize the weights to get the barycentric coordinates\n  w = w / np.sum(w)\n\n  return w"}
{"namespace": "linspline.query", "completion": "  # Import 'numpy' package\n  import numpy as np\n\n  # Ensure that the input arguments are arrays\n  tq = np.array(tq)\n  t = np.array(t)\n  v = np.array(v)\n\n  # Ensure that the input arguments are valid\n  if tq.ndim != 1:\n    raise Exception('The query points must be a vector.')\n  if t.ndim != 1:\n    raise Exception('The time points must be a vector.')\n  if v.ndim != 1:\n    raise Exception('The values must be a vector.')\n  if tq.size == 0:\n    raise Exception('The query points must be non-empty.')\n  if t.size == 0:\n    raise Exception('The time points must be non-empty.')\n  if v.size == 0:\n    raise Exception('The values must be non-empty.')\n  if t.size != v.size:\n    raise Exception('The time points and values must be the same length.')\n  if t.size != tq.size:\n    raise Exception('The time points and query points must be the same length.')\n  if t.size < 2:\n    raise Exception('The time points must be at least two elements long.')\n  if t[0] != tq[0]:\n    raise Exception('The first query point must be equal to the first time point.')\n  if t[-1] != tq[-1]:\n    raise Exception('The last query point must be equal to the last time point.')\n  if t[0] < 0:\n    raise Exception('The first time point must be non-negative.')\n  if t[-1] < 0:\n    raise Exception('The last time point must be non-negative.')\n  if t[0] > t[-1]:\n    raise Exception('The first time point must be less than the last time point.')\n  if t[1] <= t[0]:\n    raise Exception('The second time point must be greater than the first time point.')\n  if t[-2] <= t[-1]:\n   "}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        for i in v:\n            if i < 0:\n                raise ValueError(f\"{cls.__name__}.{field.name} must contain only positive values.\")\n    else:\n        if v < 0:\n            raise ValueError(f\"{cls.__name__}.{field.name} must contain only positive values.\")\n\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert rays to camera space\n  origins_cam = xnp.dot(pixtocam, xnp.concatenate((origins, xnp.ones((origins.shape[0], 1))), axis = 1).T).T[:, :3]\n  directions_cam = xnp.dot(pixtocam, xnp.concatenate((directions, xnp.zeros((directions.shape[0], 1))), axis = 1).T).T[:, :3]\n\n  # Adjust ray origins to the near plane\n  origins_cam = origins_cam + directions_cam * near\n\n  # Calculate the ray directions in NDC\n  directions_cam_norm = directions_cam / xnp.maximum(xnp.linalg.norm(directions_cam, axis = 1, keepdims = True), 1e-10)\n  directions_ndc = directions_cam_norm * (xnp.linalg.norm(origins_cam, axis = 1, keepdims = True) / near)\n\n  return origins_cam, directions_ndc\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Determine if the lines are parallel\n  if np.abs(dot_product) > 1 - 1e-10:\n    return True\n  else:\n    return False\n"}
{"namespace": "common.bleu4_score", "completion": "    # Import the BLEU score function from the sacrebleu package\n    from sacrebleu import corpus_bleu\n\n    # Tokenize the continuation and reference text\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the BLEU score\n    bleu_score = corpus_bleu(\n        [continuation_tokens],\n        [[reference_tokens]]\n    )\n\n    # If the brevity penalty is to be included in the score calculation, calculate it and add it to the BLEU score\n    if with_penalty:\n        bleu_score = bleu_score.score + brevity_penalty(\n            continuation_tokens,\n            reference_tokens\n        )\n\n    return bleu_score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.where(x < eps, value_at_zero, x))\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  import numpy as np\n\n  # Convert to numpy arrays\n  t = np.array(t)\n  w = np.array(w)\n\n  # Check that the input vectors are of the same length\n  if len(t) != len(w):\n    raise ValueError('The input vectors must be of the same length.')\n\n  # Check that the input vector w sums to 1\n  if np.sum(w) != 1:\n    raise ValueError('The input vector w must sum to 1.')\n\n  # Check that the input vector t is strictly increasing\n  if not np.all(np.diff(t) > 0):\n    raise ValueError('The input vector t must be strictly increasing.')\n\n  # Compute the difference between consecutive elements in the input vector t\n  dt = np.diff(t)\n\n  # Divide the input vector w by the difference between consecutive elements in the input vector t\n  pdf = w / dt\n\n  return pdf"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        return {\n            'dynamic_prompt': agent.dynamic_prompt,\n            'purpose': agent.purpose,\n            'purpose_embedding': agent.purpose_embedding.tolist() if isinstance(agent.purpose_embedding, np.ndarray) else agent.purpose_embedding,\n            'depth': agent.depth,\n            'max_depth': agent.max_depth,\n            'usage_count': agent.usage_count,\n            'id': agent.id,\n            'parent_id': agent.parent_id,\n            'working_agent': agent.working_agent,\n            'is_prime': agent.is_prime,\n            'evolve_count': agent.evolve_count,\n            'number_of_code_executions': agent.number_of_code_executions,\n            'last_input': agent.last_input\n        }\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # TODO: Write your own implementation of the greedy packing algorithm.\n\n    # Raise an error if the number of bins is not positive.\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n\n    # Raise an error if the number of items is not positive.\n    if len(items) <= 0:\n        raise ValueError(\"The number of items must be positive.\")\n\n    # Raise an error if the number of items is not equal to the number of weights.\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items must be equal to the number of weights.\")\n\n    # Raise an error if the number of weights is not positive.\n    if len(weights) <= 0:\n        raise ValueError(\"The number of weights must be positive.\")\n\n    # Raise an error if the number of weights is not equal to the number of items.\n    if len(weights) != len(items):\n        raise ValueError(\"The number of weights must be equal to the number of items.\")\n\n    # Raise an error if any of the weights are not positive.\n    for weight in weights:\n        if weight <= 0:\n            raise ValueError(\"All weights must be positive.\")\n\n    # Raise an error if the number of bins is greater than the number of items.\n    if num_bins > len(items):\n        raise ValueError(\"The number of bins cannot be greater than the number of items.\")\n\n    # Raise an error if the number of bins is greater than the number of weights.\n    if num_bins > len(weights):\n        raise ValueError(\"The number of bins cannot be greater than the number of weights.\")\n\n    # Raise an error if the number of bins is greater than the number of items.\n    if num_bins > len(items):\n        raise ValueError(\"The number of bins cannot be greater than the number of items.\")\n\n    # Raise an error if the number of bins is greater than the number of weights.\n    if num_bins > len(weights):\n        raise"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # create a string of the function name and its arguments\n        data = func_name + str(args) + str(kwargs)\n\n        # compute the hash\n        hash = hashlib.sha256(data.encode()).hexdigest()\n\n        return hash"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the total length of the polygon\n    total_length = 0\n    for i in range(len(polygon) - 1):\n        total_length += np.linalg.norm(polygon[i + 1] - polygon[i])\n\n    # Compute the total length of the polygon, excluding large gaps between consecutive points\n    total_length_excluding_gaps = 0\n    for i in range(len(polygon) - 1):\n        if np.linalg.norm(polygon[i + 1] - polygon[i]) < max_point_distance:\n            total_length_excluding_gaps += np.linalg.norm(polygon[i + 1] - polygon[i])\n\n    return total_length_excluding_gaps"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # get the areas of the polygons\n    areas = [polygon_area(polygon) for polygon in polygons]\n\n    # get the largest area\n    max_area = max(areas)\n\n    # get the indices of the polygons that meet the area criteria\n    indices = [\n        index\n        for index, area in enumerate(areas)\n        if area >= max_area * rel_tr or area >= abs_tr\n    ]\n\n    # return the filtered polygons\n    return [polygons[index] for index in indices]"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed.\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that are left to be distributed.\n    remaining_samples = num_samples_yielded % num_workers\n\n    # Initialize the number of samples that each worker has processed.\n    samples_per_worker_dict = {}\n\n    # Iterate over the number of workers.\n    for worker in range(num_workers):\n\n        # If the number of samples that are left to be distributed is greater than the number of workers, then assign the number of samples that each worker has processed to the number of samples that are left to be distributed.\n        if remaining_samples > num_workers:\n\n            # Add the number of samples that each worker has processed to the dictionary.\n            samples_per_worker_dict[worker] = samples_per_worker + 1\n\n            # Decrease the number of samples that are left to be distributed by the number of samples that each worker has processed.\n            remaining_samples -= 1\n\n        # If the number of samples that are left to be distributed is less than the number of workers, then assign the number of samples that each worker has processed to the number of samples that are left to be distributed.\n        elif remaining_samples < num_workers:\n\n            # Add the number of samples that each worker has processed to the dictionary.\n            samples_per_worker_dict[worker] = samples_per_worker\n\n            # Decrease the number of samples that are left to be distributed by the number of samples that each worker has processed.\n            remaining_samples -= 1\n\n        # If the number of samples that are left to be distributed is equal to the number of workers, then assign the number of samples that each worker has processed to the number of samples that are left to be distributed.\n        elif remaining_samples == num_workers:\n\n            # Add the number of samples that each worker has processed to the dictionary.\n            samples_per_worker_dict[worker] = samples_per_worker + 1\n\n            # Decrease the number of samples that are left to be distributed by the number"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    # If no metadata is provided, create a list of None values\n    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    # If the length of the metadata list is not the same as the length of the results list, raise an error\n    if len(metadatas) != len(results):\n        raise ValueError(\"The length of the metadata list must be the same as the length of the results list.\")\n\n    # If the length of the value list is not the same as the length of the results list, raise an error\n    if len(value) != len(results):\n        raise ValueError(\"The length of the value list must be the same as the length of the results list.\")\n\n    # Create a list to store the filtered results\n    filtered_results = []\n\n    # Create a list to store the filtered metadata\n    filtered_metadatas = []\n\n    # Iterate through the results and values\n    for result, value, metadata in zip(results, value, metadatas):\n\n        # If the value is less than or equal to the threshold, add the result to the filtered results list\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    # Return the filtered results and metadata\n    return filtered_results, filtered_metadatas"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have the shape (_, 2), where _ can be any number of points.\")\n\n    # Calculate the area of the polygon\n    area = 0.5 * np.abs(np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1)))\n\n    return area"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Get the shape of the input tensors\n    shape_a = a.shape\n    shape_v = v.shape\n\n    # Get the number of dimensions of the input tensors\n    dim_a = len(shape_a)\n    dim_v = len(shape_v)\n\n    # If the number of dimensions of the input tensors is not the same, raise an error\n    if dim_a != dim_v:\n        raise ValueError(\"The number of dimensions of the input tensors must be the same.\")\n\n    # If the last dimension of the input tensors is not the same, raise an error\n    if shape_a[-1] != shape_v[-1]:\n        raise ValueError(\"The last dimension of the input tensors must be the same.\")\n\n    # If the number of dimensions of the input tensors is 1, raise an error\n    if dim_a == 1:\n        raise ValueError(\"The number of dimensions of the input tensors must be greater than 1.\")\n\n    # If the number of dimensions of the input tensors is 2, raise an error\n    if dim_a == 2:\n        raise ValueError(\"The number of dimensions of the input tensors must be greater than 2.\")\n\n    # If the number of dimensions of the input tensors is greater than 3, raise an error\n    if dim_a > 3:\n        raise ValueError(\"The number of dimensions of the input tensors must be less than or equal to 3.\")\n\n    # If the number of dimensions of the input tensors is 3, reshape the input tensors to be 2-dimensional\n    if dim_a == 3:\n        a = a.reshape(-1, shape_a[-1])\n        v = v.reshape(-1, shape_v[-1])\n\n    # Get the number of elements in the input tensors\n    num_a = a.numel()\n    num_v = v.numel()\n\n    # Get the number of elements in the last dimension of the input tensors\n    num_d = shape_a[-1]\n\n    # Get the number of elements in the first dimension of the input tensors"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input points\n  mag_sq = np.sum(x**2, axis=1)\n\n  # Scale the points based on the magnitude squared\n  return x * np.sqrt(mag_sq / np.max(mag_sq))\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in ['B', 'KB', 'MB', 'GB', 'TB', 'PB']:\n        if abs(num_bytes) < 1024.0:\n            return f'{num_bytes:3.1f}{unit}'\n        num_bytes /= 1024.0\n    return f'{num_bytes:3.1f}{unit}'"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _is_array_n_dimensions(cls, v, field):\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be a {nb_dimensions}-dimensional array.\"\n            )\n        return v\n\n    return _is_array_n_dimensions\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = onp.sqrt(x**2 + y**2 + z**2)\n  theta = onp.arccos(z / (r + eps))\n  phi = onp.arctan2(y, x)\n  return r, theta, phi\n"}
{"namespace": "common.rougeL_score", "completion": "    # Import the required libraries\n    import jieba\n    from rouge_score import rouge_l\n\n    # Tokenize the continuation and reference texts\n    continuation_tokens = jieba.lcut(continuation)\n    reference_tokens = jieba.lcut(reference)\n\n    # Calculate the ROUGE-L score\n    rouge_l_score = rouge_l(\n        continuation_tokens,\n        reference_tokens\n    )\n\n    return rouge_l_score"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Import the module.\n    module = __import__(name)\n\n    # Split the name into its constituent parts.\n    parts = name.split('.')\n\n    # Iterate over the parts.\n    for part in parts[1:]:\n\n        # Locate the object.\n        module = getattr(module, part)\n\n    # Return the located object.\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the length of the ids and scores tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\"The length of the ids and scores tuples must match.\")\n\n    # Check if the length of the ids, scores, and weights tuples match\n    if len(ids) != len(weights):\n        raise ValueError(\"The length of the ids, scores, and weights tuples must match.\")\n\n    # Check if the sum of the weights is equal to 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    # Check if the weights are positive\n    if any(weight < 0 for weight in weights):\n        raise ValueError(\"The weights must be positive.\")\n\n    # Check if the top_k parameter is positive\n    if top_k <= 0:\n        raise ValueError(\"The top_k parameter must be positive.\")\n\n    # Normalize the scores\n    normalized_scores = []\n    for score_list in scores:\n        normalized_scores.append([score / max(score_list) for score in score_list])\n\n    # Combine the scores\n    combined_scores = []\n    for i in range(len(ids[0])):\n        combined_scores.append(sum(score_list[i] * weight for score_list, weight in zip(normalized_scores, weights)))\n\n    # Select the top_k results\n    top_ids = []\n    top_scores = []\n    for i in range(top_k):\n        top_scores.append(max(combined_scores))\n        top_ids.append(ids[combined_scores.index(max(combined_scores))][i])\n        combined_scores.pop(combined_scores.index(max(combined_scores)))\n\n    return top_ids, top_scores"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x == x:\n        if percent:\n            return f\"{round(x * 100, 2)}%\"\n        else:\n            return f\"{round(x, 2)}\"\n    else:\n        return str(x)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    # Check if the input directory exists\n    if not os.path.exists(input_dir):\n        raise NotADirectoryError(\"The input directory does not exist.\")\n\n    # Check if the input directory is a directory\n    if not os.path.isdir(input_dir):\n        raise NotADirectoryError(\"The input path is not a directory.\")\n\n    # Check if the input threshold is an integer\n    if not isinstance(threshold_in_gb, int):\n        raise TypeError(\"The input threshold is not an integer.\")\n\n    # Check if the input sleep time is an integer\n    if not isinstance(sleep_time, int):\n        raise TypeError(\"The input sleep time is not an integer.\")\n\n    # Check if the input sleep time is a positive integer\n    if sleep_time < 0:\n        raise ValueError(\"The input sleep time is not a positive integer.\")\n\n    # Check if the input threshold is a positive integer\n    if threshold_in_gb < 0:\n        raise ValueError(\"The input threshold is not a positive integer.\")\n\n    # Check if the input threshold is a positive integer\n    if threshold_in_gb < 0:\n        raise ValueError(\"The input threshold is not a positive integer.\")\n\n    # Check if the input threshold is a positive integer\n    if threshold_in_gb < 0:\n        raise ValueError(\"The input threshold is not a positive integer.\")\n\n    # Check if the input threshold is a positive integer\n    if threshold_in_gb < 0:\n        raise ValueError(\"The input threshold is not a positive integer.\")\n\n    # Check if the input threshold is a positive integer\n    if threshold_in_gb < 0:\n        raise ValueError(\"The input threshold is not a positive integer.\")\n\n    # Check if the input threshold is a positive integer\n    if threshold_in_gb < 0:\n        raise ValueError(\"The input threshold is not a positive integer.\")\n\n    # Check if the input threshold is a positive integer\n    if threshold_in_gb < 0:\n        raise ValueError(\"The input threshold is not a positive integer.\")\n\n    # Check if the input"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the difference between consecutive elements in the time or position vector\n  dt = t[1:] - t[:-1]\n\n  # Multiply the PDF values by the difference between consecutive elements in the time or position vector\n  w = p * dt\n\n  # Normalize the weights to sum to 1\n  w = w / w.sum()\n\n  return w"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Removing all spaces from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segmenting the modified text into smaller parts or tokens\n    line_text = line_text.split()\n\n    return line_text"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check if the number of zeros is valid\n    if zeros > n:\n        raise ValueError(\"The number of zeros must not exceed the total number of weights.\")\n\n    # Generate the weights\n    weights = np.random.rand(n)\n\n    # Set the specified number of weights to zero\n    weights[np.random.choice(n, zeros, replace=False)] = 0\n\n    # Normalize the weights\n    weights = weights / np.sum(weights)\n\n    return weights"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, **module_dict)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = instance[\"bbox\"]\n\n    # Get the center of the bounding box\n    center = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n\n    # Get the top-left corner of the crop\n    x = max(min(center[0] - crop_size[0] / 2, image_size[0] - crop_size[0]), 0)\n    y = max(min(center[1] - crop_size[1] / 2, image_size[1] - crop_size[1]), 0)\n\n    # Get the height and width of the crop\n    h = min(crop_size[0], image_size[0] - x)\n    w = min(crop_size[1], image_size[1] - y)\n\n    # Return a CropTransform object\n    return CropTransform(x, y, h, w)\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of the input vector\n  x_norm = jnp.sum(jnp.square(x), axis=-1, keepdims=True)\n\n  # Compute the norm of the input vector\n  x_norm = jnp.sqrt(x_norm)\n\n  # Compute the reciprocal of the norm of the input vector\n  x_norm_rec = jnp.reciprocal(x_norm)\n\n  # Clamp the reciprocal of the norm of the input vector to a minimum value to prevent exploding gradients\n  x_norm_rec = jnp.maximum(x_norm_rec, grad_eps)\n\n  # Normalize the input vector\n  x_norm = x * x_norm_rec\n\n  return x_norm\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Extract the agent name and input text from the response string\n        agent_info = response.split('Use Agent[')[-1].split(']')[0]\n        agent_name = agent_info.split(':')[0]\n        input_text = agent_info.split(':')[1] if ':' in agent_info else ''\n\n        return agent_name, input_text\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # Import libraries\n    import numpy as np\n    from detectron2.structures import Boxes, Instances\n\n    # Initialize variables\n    boxes = []\n    classes = []\n    masks = []\n    keypoints = []\n\n    # Loop over all annotations\n    for anno in annos:\n\n        # Get bounding box\n        bbox = anno[\"bbox\"]\n        x1, y1, w, h = bbox\n        x2 = x1 + w\n        y2 = y1 + h\n        boxes.append([x1, y1, x2, y2])\n\n        # Get class\n        classes.append(anno[\"category_id\"])\n\n        # Get segmentation mask\n        if mask_format == \"polygon\":\n            masks.append(anno[\"segmentation\"])\n        elif mask_format == \"bitmask\":\n            masks.append(anno[\"segmentation\"])\n\n        # Get keypoints\n        if \"keypoints\" in anno:\n            keypoints.append(anno[\"keypoints\"])\n\n    # Convert to numpy arrays\n    boxes = np.array(boxes)\n    classes = np.array(classes)\n    masks = np.array(masks)\n    keypoints = np.array(keypoints)\n\n    # Convert bounding boxes to Boxes\n    boxes = Boxes(boxes)\n\n    # Initialize Instances object\n    instances = Instances(image_size)\n\n    # Set fields\n    instances.gt_boxes = boxes\n    instances.gt_classes = classes\n    instances.gt_masks = masks\n    instances.gt_keypoints = keypoints\n\n    # Return Instances object\n    return instances"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    import os\n    import pathlib\n\n    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", pathlib.Path(\"~/skfolio_data\"))\n    data_home = pathlib.Path(data_home).expanduser()\n\n    if not data_home.exists():\n        data_home.mkdir(parents=True, exist_ok=True)\n\n    return str(data_home)\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Checking if the input is a 2D array\n    if len(cov.shape) != 2:\n        raise ValueError(\"The input must be a 2D array.\")\n\n    # Calculating the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculating the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules in the model\n    for module in model.modules():\n\n        # If the module has a \"training\" attribute, set it to a constant value\n        if hasattr(module, \"training\"):\n            module.training = False\n\n    # Return the model\n    return model"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def _are_shapes_equal(cls, value):\n\n        \"\"\"\n        This function is used to check if the shapes of two data structures match.\n\n        Input-Output Arguments\n        :param cls: Pydantic model class, The Pydantic model class that is being validated.\n        :param value: Any, The value of the field being validated.\n        :return: Any, The value of the field being validated.\n        \"\"\"\n\n        if value is None:\n            return value\n\n        if not isinstance(value, dict):\n            raise TypeError(f\"Expected a dictionary, but got {type(value)}\")\n\n        if field1 not in value:\n            raise ValueError(f\"Missing field {field1}\")\n\n        if field2 not in value:\n            raise ValueError(f\"Missing field {field2}\")\n\n        if not isinstance(value[field1], type(value[field2])):\n            raise TypeError(\n                f\"Expected {type(value[field1])}, but got {type(value[field2])}\"\n            )\n\n        if isinstance(value[field1], dict):\n            return value\n\n        if isinstance(value[field1], np.ndarray):\n            if value[field1].shape != value[field2].shape:\n                raise ValueError(\n                    f\"Expected {value[field1].shape}, but got {value[field2].shape}\"\n                )\n            return value\n\n        if isinstance(value[field1], pd.DataFrame):\n            if value[field1].shape != value[field2].shape:\n                raise ValueError(\n                    f\"Expected {value[field1].shape}, but got {value[field2].shape}\"\n                )\n            return value\n\n        if isinstance(value[field1], pd.Series):\n            if value[field1].shape != value[field2].shape:\n                raise ValueError(\n                    f\"Expected {value[field1].shape}, but got {value[field2].shape}\"\n                )\n            return value\n\n        if isinstance(value[field1], list"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Initialize variables\n    metric_names: List[str] = []\n    metric_params: List[Dict[str, Any]] = []\n\n    # Loop over metrics\n    for metric in metrics:\n\n        # If metric is a string, add it to the list of metric names\n        if isinstance(metric, str):\n            metric_names.append(metric)\n\n        # If metric is a dictionary, add it to the list of metric parameters\n        elif isinstance(metric, dict):\n            metric_params.append(metric)\n\n        # Raise an error if metric is neither a string nor a dictionary\n        else:\n            raise TypeError(f'Metric {metric} is neither a string nor a dictionary.')\n\n    # Return tuple of metric names and metric parameters\n    return metric_names, metric_params\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == tf.math.exp:\n      fn_inv = tf.math.log\n    elif fn == tf.math.log:\n      fn_inv = tf.math.exp\n    elif fn == tf.math.sqrt:\n      fn_inv = tf.math.pow(2.0)\n    elif fn == tf.math.pow:\n      fn_inv = tf.math.pow(1.0 / fn.args[1])\n    else:\n      raise NotImplementedError('No inverse function for {} is available.'.format(fn))\n\n  def t_to_s(t):\n    return tf.clip_by_value(fn(t), t_near, t_far) / t_far\n\n  def s_to_t(s):\n    return fn_inv(s * t_far)\n\n  return t_to_s, s_to_t\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if not is_valid_spline(t, w):\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  # Compute the integral of the data points using the trapezoid rule\n  integral = 0.0\n  for i in range(len(t) - 1):\n    integral += (t[i + 1] - t[i]) * (w[i] + w[i + 1]) / 2.0\n\n  return integral\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Calculate the weighted sum of scores for each ID\n    weighted_scores = []\n    for i in range(len(ids)):\n        weighted_scores.append([])\n        for j in range(len(ids[i])):\n            weighted_scores[i].append(scores[i][j] * weights[i])\n\n    # Normalize the weighted scores for each ID\n    normalized_weighted_scores = []\n    for i in range(len(ids)):\n        normalized_weighted_scores.append([])\n        for j in range(len(ids[i])):\n            normalized_weighted_scores[i].append(\n                weighted_scores[i][j] / sum(weighted_scores[i]))\n\n    # Create a list of tuples containing the IDs and their corresponding weighted scores\n    id_score_tuples = []\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            id_score_tuples.append((ids[i][j], normalized_weighted_scores[i][j]))\n\n    # Sort the list of tuples by the weighted scores in descending order\n    id_score_tuples.sort(key=lambda x: x[1], reverse=True)\n\n    # Return the top K IDs and their corresponding weighted scores\n    return [id_score_tuples[i][0] for i in range(top_k)], [id_score_tuples[i][1] for i in range(top_k)]"}
{"namespace": "coord.track_linearize", "completion": "  # Get the dimensions of the input mean and covariance\n  dim_mean = mean.shape[-1]\n  dim_cov = cov.shape[-1]\n  dim_cov_2 = cov.shape[-2]\n\n  # Check if the dimensions of the mean and covariance are the same\n  if dim_mean != dim_cov_2:\n    raise ValueError(\"The dimensions of the mean and covariance must be the same.\")\n\n  # Check if the mean and covariance have the same number of dimensions\n  if dim_mean != dim_cov:\n    raise ValueError(\"The mean and covariance must have the same number of dimensions.\")\n\n  # Check if the mean and covariance have at least one dimension\n  if dim_mean < 1:\n    raise ValueError(\"The mean and covariance must have at least one dimension.\")\n\n  # Check if the mean and covariance have the same number of dimensions\n  if dim_mean != dim_cov:\n    raise ValueError(\"The mean and covariance must have the same number of dimensions.\")\n\n  # Check if the mean and covariance have at least one dimension\n  if dim_mean < 1:\n    raise ValueError(\"The mean and covariance must have at least one dimension.\")\n\n  # Check if the mean and covariance have the same number of dimensions\n  if dim_mean != dim_cov:\n    raise ValueError(\"The mean and covariance must have the same number of dimensions.\")\n\n  # Check if the mean and covariance have at least one dimension\n  if dim_mean < 1:\n    raise ValueError(\"The mean and covariance must have at least one dimension.\")\n\n  # Check if the mean and covariance have the same number of dimensions\n  if dim_mean != dim_cov:\n    raise ValueError(\"The mean and covariance must have the same number of dimensions.\")\n\n  # Check if the mean and covariance have at least one dimension\n  if dim_mean < 1:\n    raise ValueError(\"The mean and covariance must have at least one dimension.\")\n\n  # Check if the mean and covariance have the same number"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if len(x[i]) > 1:\n            yield [x[i][:len(x[i]) // 2], x[i][len(x[i]) // 2:]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The given matrix is not square.\")\n"}
{"namespace": "coord.pos_enc", "completion": "  import numpy as np\n\n  # Generate the scales\n  scales = 2**np.linspace(min_deg, max_deg, max_deg-min_deg+1)\n\n  # Generate the positional encoding\n  pe = np.concatenate([np.sin(x/scale) for scale in scales] + ([x] if append_identity else []), axis=-1)\n\n  return pe\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def check_shapes(cls, values):\n\n        \"\"\"\n        This function checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: class type. The class type of the Pydantic model to be validated.\n        :param values: dict. The dictionary of values to be validated.\n        :return: dict. The validated dictionary of values.\n\n        \"\"\"\n\n        # Retrieve the specified fields from the dictionary of values.\n        field1_values = values.get(field1)\n        field2_values = values.get(field2)\n\n        # Check if the specified fields are lists of numpy arrays.\n        if not isinstance(field1_values, list) or not isinstance(field2_values, list):\n            raise ValueError(f\"The specified fields must be lists of numpy arrays.\")\n\n        # Check if the specified fields have the same length.\n        if len(field1_values) != len(field2_values):\n            raise ValueError(f\"The specified fields must have the same length.\")\n\n        # Check if each corresponding pair of arrays within the specified fields have the same shape.\n        for field1_value, field2_value in zip(field1_values, field2_values):\n            if field1_value.shape != field2_value.shape:\n                raise ValueError(f\"The specified fields must have the same shape.\")\n\n        return values\n\n    return check_shapes"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.width, camera.height)\n\n        # Set the camera's settings\n        camera.set_view_projection_matrix()\n\n        # Render the Mesh instance\n        self.render(eglctx, camera)\n\n        # Read the rendered image\n        image = eglctx.read_image()\n\n        # Return the rendered image\n        return image"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new configuration object for a Nomic model\n    nomic_config = NomicBertConfig()\n\n    # Copy over the settings from the BERT configuration object\n    nomic_config.vocab_size = bert_config.vocab_size\n    nomic_config.hidden_size = bert_config.hidden_size\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n    nomic_config.intermediate_size = bert_config.intermediate_size\n    nomic_config.hidden_act = bert_config.hidden_act\n    nomic_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    nomic_config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n    nomic_config.initializer_range = bert_config.initializer_range\n    nomic_config.layer_norm_eps = bert_config.layer_norm_eps\n\n    # Additional settings for the Nomic model\n    nomic_config.num_labels = bert_config.num_labels\n    nomic_config.num_choices = bert_config.num_labels\n    nomic_config.num_choice_labels = bert_config.num_labels\n    nomic_config.num_choice_selections = bert_config.num_labels\n    nomic_config.num_choice_selection_labels = bert_config.num_labels\n    nomic_config.num_choice_selection_selections = bert_config.num_labels\n    nomic_config.num_choice_selection_selection_labels = bert_config.num_labels\n    nomic_"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.shader_program.use()\n            self.shader_program.set_uniform(\"projection\", camera.projection_matrix)\n            self.shader_program.set_uniform(\"view\", camera.view_matrix)\n            self.shader_program.set_uniform(\"model\", self.model_matrix)\n            self.shader_program.set_uniform(\"color\", self.color)\n            self.shader_program.set_uniform(\"point_size\", self.point_size)\n            self.shader_program.set_uniform(\"use_model\", self.use_model)\n            self.shader_program.set_uniform(\"use_view\", self.use_view)\n            self.shader_program.set_uniform(\"use_projection\", self.use_projection)\n            self.shader_program.set_uniform(\"use_color\", self.use_color)\n            self.shader_program.set_uniform(\"use_point_size\", self.use_point_size)\n            self.shader_program.set_uniform(\"use_model_matrix\", self.use_model_matrix)\n            self.shader_program.set_uniform(\"use_view_matrix\", self.use_view_matrix)\n            self.shader_program.set_uniform(\"use_projection_matrix\", self.use_projection_matrix)\n            self.shader_program.set_uniform(\"use_color_matrix\", self.use_color_matrix)\n            self.shader_program.set_uniform(\"use_point_size_matrix\", self.use_point_size_matrix)\n            self.shader_program.set_uniform(\"use_model_view_matrix\", self.use_model_view_matrix)\n            self.shader_program.set_uniform(\"use_model_view_projection_matrix\", self.use_model_view_projection_matrix)\n            self.shader_program.set_uniform(\"use_model_view_projection_matrix"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # Convert the data source to a numpy array\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # If width and height are not provided, set them to the object's width and height\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        # Upload the data to the texture\n        glBindTexture(GL_TEXTURE_2D, self.texture)\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_FLOAT, ptr)\n        glBindTexture(GL_TEXTURE_2D, 0)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate input arguments\n    assert isinstance(R, torch.Tensor)\n    assert isinstance(tvec, torch.Tensor)\n    assert isinstance(camera_matrix, torch.Tensor)\n    assert isinstance(image_size, torch.Tensor)\n    assert R.shape[-2:] == (3, 3)\n    assert tvec.shape[-2:] == (3, 1)\n    assert camera_matrix.shape[-2:] == (3, 3)\n    assert image_size.shape[-2:] == (2, 1)\n    assert R.device == tvec.device == camera_matrix.device == image_size.device\n    assert R.dtype == tvec.dtype == camera_matrix.dtype == image_size.dtype\n\n    # Ensure that all inputs are batched\n    if len(R.shape) == 2:\n        R = R.unsqueeze(0)\n    if len(tvec.shape) == 2:\n        tvec = tvec.unsqueeze(0)\n    if len(camera_matrix.shape) == 2:\n        camera_matrix = camera_matrix.unsqueeze(0)\n    if len(image_size.shape) == 2:\n        image_size = image_size.unsqueeze(0)\n\n    # Compute the camera position\n    camera_position = -torch.bmm(R.transpose(1, 2), tvec)\n\n    # Compute the camera rotation\n    camera_rotation = R.transpose(1, 2)\n\n    # Compute the focal length\n    focal_length = (\n        camera_matrix[:, 0, 0]\n        * image_size[:, 0, 0]\n        / (image_size[:, 0, 0] - 1)\n        * (image_size[:, 1, 0] - 1)\n        / image_size[:, 1, 0]\n    )\n\n    # Compute the principal point\n    principal_point = (\n        camera_"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set up viewport and scissor box\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n\n        # Activate shader program\n        glUseProgram(self.quad_program)\n\n        # Bind texture\n        glBindTexture(GL_TEXTURE_2D, self.tex)\n\n        # Draw quadrilateral\n        glBindVertexArray(self.vao)\n        glDrawArrays(GL_TRIANGLES, 0, 6)\n        glBindVertexArray(0)\n\n        # Restore viewport and scissor box\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract camera parameters\n    R = batch.R\n    T = batch.T\n    K = batch.K\n    H = batch.H\n    W = batch.W\n\n    # Adjust rotation matrix\n    R = R.transpose(-1, -2)\n\n    # Adjust translation vector\n    T = -R @ T\n\n    # Compute camera center\n    C = -torch.linalg.inv(R) @ T\n\n    # Compute intrinsic matrix\n    K_new = torch.zeros((batch.R.shape[0], 4, 4), device=R.device)\n    K_new[:, 0, 0] = K[:, 0, 0] * 2 / W\n    K_new[:, 1, 1] = K[:, 1, 1] * 2 / H\n    K_new[:, 0, 2] = 1 - 2 / W\n    K_new[:, 1, 2] = 1 - 2 / H\n    K_new[:, 2, 2] = -1\n    K_new[:, 3, 2] = -1\n    K_new[:, 2, 3] = -1\n    K_new[:, 3, 3] = 0\n\n    return H, W, K_new, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # If the width or height of the pixel block to be copied is not specified, use the width and height of the Quad instance.\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        # Bind the Quad instance's framebuffer object (FBO) as the read framebuffer.\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n\n        # Bind the default framebuffer as the draw framebuffer.\n        glBindFramebuffer(GL_DRAW_FRAMEBUFFER, 0)\n\n        # Copy the pixel block from the read framebuffer to the draw framebuffer.\n        glBlitFramebuffer(x, y, x + w, y + h, x, y, x + w, y + h, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n\n        # Restore the previously bound read framebuffer.\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Construct the inner measure\n    inner = tf.reduce_sum(tf.where(tf.logical_and(tf.greater_equal(t0, t1), tf.greater_equal(t1, t0)), y1, tf.zeros_like(y1)), axis=0)\n\n    # Construct the outer measure\n    outer = tf.reduce_sum(tf.where(tf.greater_equal(t0, t1), y1, tf.zeros_like(y1)), axis=0)\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_env_upper = torch.cat((w_env, torch.zeros(1, device=w_env.device)), dim=-1)\n\n    # calculate the loss\n    loss = (w - w_env_upper) ** 2\n\n    # scale the loss\n    loss = 0.5 * (loss + (w_env_upper[..., 0] - w_env_upper[..., 1]) ** 2)\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # calculate the inter-interval loss\n    inter_interval_loss = torch.sum(torch.abs(t[:, 1:] - t[:, :-1]) * w[:, 1:])\n\n    # calculate the intra-interval loss\n    intra_interval_loss = torch.sum(torch.abs(t[:, 1:] - t[:, :-1]) * w[:, :-1])\n\n    # combine the inter-interval and intra-interval losses\n    distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return distortion_loss\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Checking if the tensors are of the same shape\n    if t.shape != w.shape:\n        raise ValueError(\"The tensors 't' and 'w' must be of the same shape.\")\n\n    # Checking if the weights sum to 1\n    if not torch.allclose(w.sum(dim=1), torch.ones(w.shape[0])):\n        raise ValueError(\"The weights must sum to 1.\")\n\n    # Checking if the percentile values are in the correct range\n    if any(p < 0 or p > 1 for p in ps):\n        raise ValueError(\"The percentile values must be in the range [0, 1].\")\n\n    # Checking if the percentile values are in ascending order\n    if any(ps[i] >= ps[i + 1] for i in range(len(ps) - 1)):\n        raise ValueError(\"The percentile values must be in ascending order.\")\n\n    # Checking if the percentile values are unique\n    if len(ps) != len(set(ps)):\n        raise ValueError(\"The percentile values must be unique.\")\n\n    # Checking if the percentile values are in the correct range\n    if any(p < 0 or p > 1 for p in ps):\n        raise ValueError(\"The percentile values must be in the range [0, 1].\")\n\n    # Checking if the percentile values are in ascending order\n    if any(ps[i] >= ps[i + 1] for i in range(len(ps) - 1)):\n        raise ValueError(\"The percentile values must be in ascending order.\")\n\n    # Checking if the percentile values are unique\n    if len(ps) != len(set(ps)):\n        raise ValueError(\"The percentile values must be unique.\")\n\n    # Converting the tensors to float\n    t = t.float()\n    w = w.float()\n\n    # Integrating the weights\n    w = w.cumsum(dim=1)\n\n    # Initializing the output tensor\n    out = torch.zeros(t.shape"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Import libraries\n    import torch\n\n    # Check if inputs are valid\n    if not isinstance(t, torch.Tensor):\n        raise TypeError('Input tensor t must be a torch.Tensor.')\n    if not isinstance(w, torch.Tensor):\n        raise TypeError('Input tensor w must be a torch.Tensor.')\n    if not isinstance(num_samples, int):\n        raise TypeError('Input integer num_samples must be an int.')\n    if not isinstance(perturb, bool):\n        raise TypeError('Input boolean perturb must be a bool.')\n    if not isinstance(single_jitter, bool):\n        raise TypeError('Input boolean single_jitter must be a bool.')\n\n    # Check if inputs are valid\n    if t.dim() != 1:\n        raise ValueError('Input tensor t must be a 1-dimensional tensor.')\n    if w.dim() != 1:\n        raise ValueError('Input tensor w must be a 1-dimensional tensor.')\n    if t.numel() != w.numel():\n        raise ValueError('Input tensors t and w must have the same number of elements.')\n    if t.numel() < 2:\n        raise ValueError('Input tensor t must have at least two elements.')\n    if num_samples < 1:\n        raise ValueError('Input integer num_samples must be greater than or equal to one.')\n\n    # Check if inputs are valid\n    if not torch.all(t[:-1] <= t[1:]):\n        raise ValueError('Input tensor t must be monotonically increasing.')\n    if not torch.all(w >= 0.):\n        raise ValueError('Input tensor w must contain only non-negative values.')\n    if not torch.all(w > 0.):\n        w = w / torch.sum(w)\n\n    # Compute the cumulative sum of the weights\n    w_cumsum = torch.cumsum(w, dim=0)\n\n    # Generate samples\n    samples = torch.zeros(num_samples, dtype=t."}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Dilate the time steps.\n    t_dilated = t.unsqueeze(1) + torch.arange(w.shape[1], device=t.device).unsqueeze(0) * dilation\n\n    # Clip the dilated time steps.\n    t_dilated = torch.clamp(t_dilated, domain[0], domain[1])\n\n    # Adjust the weights to match the dilated time steps.\n    w_dilated = w.unsqueeze(1) * dilation\n\n    return t_dilated, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if the query times are in the range of the step function times.\n    if tq.min() < t.min() or tq.max() > t.max():\n        raise ValueError('Query times must be in the range of the step function times.')\n\n    # Check if the query times are in the range of the step function times.\n    if tq.min() < t.min() or tq.max() > t.max():\n        raise ValueError('Query times must be in the range of the step function times.')\n\n    # Check if the step function times are in ascending order.\n    if not (t[1:] > t[:-1]).all():\n        raise ValueError('Step function times must be in ascending order.')\n\n    # Check if the step function values are in ascending order.\n    if not (y[1:] > y[:-1]).all():\n        raise ValueError('Step function values must be in ascending order.')\n\n    # Check if the query times are in ascending order.\n    if not (tq[1:] > tq[:-1]).all():\n        raise ValueError('Query times must be in ascending order.')\n\n    # Check if the query times are in the range of the step function times.\n    if tq.min() < t.min() or tq.max() > t.max():\n        raise ValueError('Query times must be in the range of the step function times.')\n\n    # Check if the step function times are in ascending order.\n    if not (t[1:] > t[:-1]).all():\n        raise ValueError('Step function times must be in ascending order.')\n\n    # Check if the step function values are in ascending order.\n    if not (y[1:] > y[:-1]).all():\n        raise ValueError('Step function values must be in ascending order.')\n\n    # Check if the query times are in ascending order.\n    if not (tq[1:] > tq[:-1]).all():\n        raise ValueError('Query times must be in ascending order.')\n\n    # Check if the step function times are"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # calculate the annealing function\n    anneal_func = torch.exp(-anneal_slope * (t - train_frac))\n\n    # calculate the adjusted weights\n    adj_w = w * anneal_func\n\n    # calculate the softmax of the adjusted weights\n    softmax_adj_w = F.softmax(adj_w, dim=-1)\n\n    # calculate the adjusted weights with the softmax\n    adj_w = adj_w * softmax_adj_w\n\n    return adj_w\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict) and not batch.get(\"meta\"):\n        return {k: to_cuda(sample, device=device, ignore_list=ignore_list) for k, sample in batch.items()}\n    elif isinstance(batch, (tuple, list)):\n        return [to_cuda(sample, device=device, ignore_list=ignore_list) for sample in batch]\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # get the number of batches\n    batches = v.shape[0]\n\n    # get the number of vertices\n    vertices = v.shape[1]\n\n    # get the number of faces\n    faces = f.shape[1]\n\n    # get the number of vertices per face\n    vpf = f.shape[2]\n\n    # get the number of dimensions of the vertices\n    dims = v.dim()\n\n    # get the number of dimensions of the faces\n    fdims = f.dim()\n\n    # get the number of dimensions of the vertices tensor\n    vdims = v.dim()\n\n    # get the number of dimensions of the faces tensor\n    fdims = f.dim()\n\n    # get the number of dimensions of the vertices tensor\n    vdims = v.dim()\n\n    # get the number of dimensions of the faces tensor\n    fdims = f.dim()\n\n    # get the number of dimensions of the vertices tensor\n    vdims = v.dim()\n\n    # get the number of dimensions of the faces tensor\n    fdims = f.dim()\n\n    # get the number of dimensions of the vertices tensor\n    vdims = v.dim()\n\n    # get the number of dimensions of the faces tensor\n    fdims = f.dim()\n\n    # get the number of dimensions of the vertices tensor\n    vdims = v.dim()\n\n    # get the number of dimensions of the faces tensor\n    fdims = f.dim()\n\n    # get the number of dimensions of the vertices tensor\n    vdims = v.dim()\n\n    # get the number of dimensions of the faces tensor\n    fdims = f.dim()\n\n    # get the number of dimensions of the vertices tensor\n    vdims = v.dim()\n\n    # get the number of dimensions of the faces tensor\n    fdims = f.dim()\n\n    # get the number of dimensions of the vertices tensor\n    vdims = v.dim()\n\n    # get the number of dimensions of the faces tensor\n    fdims = f.dim()\n\n    # get the number of"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    # Check if the input is a list or a tuple\n    if isinstance(batch, (list, tuple)):\n\n        # Iterate over the elements of the list or tuple\n        for i in range(len(batch)):\n\n            # Add a new dimension to the element\n            batch[i] = add_batch(batch[i])\n\n        # Return the modified list or tuple\n        return batch\n\n    # Check if the input is a dictionary\n    elif isinstance(batch, dict):\n\n        # Iterate over the elements of the dictionary\n        for key in batch:\n\n            # Add a new dimension to the element\n            batch[key] = add_batch(batch[key])\n\n        # Return the modified dictionary\n        return batch\n\n    # Check if the input is a torch.Tensor\n    elif isinstance(batch, torch.Tensor):\n\n        # Add a new dimension to the torch.Tensor\n        return batch.unsqueeze(0)\n\n    # Check if the input is a numpy.ndarray\n    elif isinstance(batch, np.ndarray):\n\n        # Add a new dimension to the numpy.ndarray\n        return np.expand_dims(batch, axis=0)\n\n    # If the input is not a list, tuple, dict, torch.Tensor, or numpy.ndarray, return the input as is\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Create a dotdict instance to store the parameters\n        batch = dotdict()\n\n        # Store the camera parameters\n        batch.params = dotdict()\n        batch.params.fx = torch.tensor(self.fx)\n        batch.params.fy = torch.tensor(self.fy)\n        batch.params.cx = torch.tensor(self.cx)\n        batch.params.cy = torch.tensor(self.cy)\n        batch.params.k1 = torch.tensor(self.k1)\n        batch.params.k2 = torch.tensor(self.k2)\n        batch.params.k3 = torch.tensor(self.k3)\n        batch.params.p1 = torch.tensor(self.p1)\n        batch.params.p2 = torch.tensor(self.p2)\n        batch.params.height = torch.tensor(self.height)\n        batch.params.width = torch.tensor(self.width)\n\n        # Store the GUI related elements\n        batch.gui = dotdict()\n        batch.gui.show_frame = self.show_frame\n        batch.gui.show_corners = self.show_corners\n        batch.gui.show_axes = self.show_axes\n        batch.gui.show_grid = self.show_grid\n        batch.gui.show_image = self.show_image\n        batch.gui.show_depth = self.show_depth\n        batch.gui.show_mask = self.show_mask\n        batch.gui.show_normals = self.show_normals\n        batch.gui.show_flow = self.show_flow\n        batch.gui.show_motion = self.show_motion\n        batch.gui.show_segm = self.show_segm\n        batch.gui.show_tracks = self.show_tracks\n        batch.gui.show_time = self.show_time\n        batch.gui.show_info = self.show_info\n        batch.gui.show_fps = self.show_fps\n        batch.gui"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = self.serialize_agent_state(agent)\n            self.save_agent_state(agent_state)\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Get the purpose embeddings of all agents\n        purpose_embeddings = self.get_purpose_embeddings()\n\n        # Calculate the cosine similarity between the given purpose embedding and the purpose embeddings of all agents\n        similarity_scores = cosine_similarity(purpose_embeddings, purpose_embedding)\n\n        # Get the index of the agent with the highest similarity score\n        max_index = np.argmax(similarity_scores)\n\n        # If the similarity score is 0, then no agents were found\n        if similarity_scores[max_index] == 0:\n            return None, -np.inf\n\n        # Return the agent with the highest similarity score\n        return self.agents[max_index], similarity_scores[max_index]\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        # Create the prime agent\n        prime_agent = Agent(\n            prompt=\"prime\",\n            name=\"prime\",\n            weight=1,\n            flags=[\"prime\", \"unspecified\"],\n        )\n\n        # Add the prime agent to the agent list\n        self.agent_list.append(prime_agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent = None\n        try:\n            # Load the agent from the database\n            agent_data = self.get_agent_data(purpose)\n            # Deserialize the agent\n            agent = Agent.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n        except Exception as e:\n            print(e)\n        return agent"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Get all agents from the database\n        agents = self.get_all_agents()\n\n        # Iterate over all agents\n        for agent in agents:\n\n            # Load the agent\n            agent_lifecycle.load_agent(agent)\n\n            # If the agent was successfully loaded\n            if agent_lifecycle.is_loaded():\n\n                # Add the agent to the list of loaded agents\n                loaded_agents.append(agent)\n\n        # Return the list of loaded agents\n        return loaded_agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.error(f\"Error saving agent {agent.name}: {e}\")\n            raise e"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Cleanup\n        for agent in self.agents:\n            if agent.is_alive():\n                agent.join()\n\n        return self.agents"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        # Generate the prompt\n        prompt = f\"{goal}\\n{sample_input}\\n\\n\"\n\n        # Attempt to get a chat completion from the LLM\n        try:\n            chat_completion = self.llm_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            # Log the exception\n            self.logger.error(f\"Exception occurred while attempting to get a chat completion from the LLM: {e}\")\n            # Return an empty string\n            return \"\"\n\n        # Return the chat completion\n        return chat_completion"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor to the SQLite database\n        cursor = conn.cursor()\n\n        # Create a SQL statement to insert or update the agent's record in the database\n        sql = \"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?) ON CONFLICT(id) DO UPDATE SET purpose = excluded.purpose, data = excluded.data\"\n\n        # Execute the SQL statement\n        cursor.execute(sql, (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict['data'])))\n\n        # Commit the changes to the database\n        conn.commit()\n\n        # Close the connection to the SQLite database\n        conn.close()"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Fetch the agent data from the database\n        agent_data = self.fetch_agent_data(purpose)\n\n        # If no agent with the given purpose is found, return None\n        if agent_data is None:\n            return None\n\n        # Deserialize the agent data\n        return self.deserialize_agent_data(agent_data)\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n\n        # Create a cursor to interact with the database\n        cursor = conn.cursor()\n\n        # Execute a query to retrieve all agent purposes\n        cursor.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all results from the cursor\n        results = cursor.fetchall()\n\n        # Close the connection to the database\n        conn.close()\n\n        # Create a list to store the purposes\n        purposes = []\n\n        # Iterate through the results\n        for result in results:\n\n            # Append the purpose to the list\n            purposes.append(result[0])\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Fetch the result from the database\n        result = self.db.execute(\n            \"SELECT result FROM cache WHERE arg_hash = ?\", (arg_hash,)\n        ).fetchone()\n\n        # If the result is found, load it from JSON format and return it\n        if result:\n            return json.loads(result[0])\n\n        # Otherwise, return None\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Create a new connection to the database\n        conn = sqlite3.connect(self.db_path)\n\n        # Create a cursor to the database\n        cursor = conn.cursor()\n\n        # Insert the result into the database\n        cursor.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n\n        # Commit the changes to the database\n        conn.commit()\n\n        # Close the connection to the database\n        conn.close()\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # If quiet mode is enabled, redirect the standard output to a file.\n    if quiet_mode:\n        sys.stdout = open(args.output_file, \"w\")\n\n    # Execute the command line process.\n    subprocess.run(args.command_line_process, shell=True)\n\n    # If quiet mode is enabled, close the standard output file.\n    if quiet_mode:\n        sys.stdout.close()\n\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Set the model to use for chat completion\n        if kwargs['model'] == 'ada':\n            model = 'ada'\n        elif kwargs['model'] == 'babbage':\n            model = 'babbage'\n        elif kwargs['model'] == 'curie':\n            model = 'curie'\n        elif kwargs['model'] == 'davinci':\n            model = 'davinci'\n        elif kwargs['model'] == 'gpt-j':\n            model = 'gpt-j'\n        elif kwargs['model'] == 'gpt-neo':\n            model = 'gpt-neo'\n        elif kwargs['model'] == 'curie-instruct-beta':\n            model = 'curie-instruct-beta'\n        elif kwargs['model'] == 'adagio':\n            model = 'adagio'\n        elif kwargs['model'] == 'valhalla':\n            model = 'valhalla'\n        elif kwargs['model'] == 'yuval':\n            model = 'yuval'\n        elif kwargs['model'] == 'davinci-codex':\n            model = 'davinci-codex'\n        elif kwargs['model'] == 'original':\n            model = 'original'\n        else:\n            model = 'davinci'\n\n        # Set the API endpoint to use\n        if kwargs['api_key'] == 'demo':\n            url = 'https://api.openai.com/v1/engines/davinci-codex/completions'\n        else:\n            url = 'https://api.openai.com/v1/engines/' + model + '/completions'\n\n        # Set the API request parameters\n        headers = {\n            'Authorization': 'Bearer ' + kwargs['api_key'],\n            'Content-Type': 'application/json'\n        }\n        params = {\n            'prompt': kwargs['prompt'],\n            'max_tokens': kwargs['max_tokens'],\n            'temperature': kwargs['temperature'],\n            'top_p': kwargs['top_"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        # If the client does not exist, create it.\n        if self.client is None:\n            self.client = boto3.client(\n                \"s3\",\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key,\n                aws_session_token=self.aws_session_token,\n                region_name=self.region_name,\n            )\n\n        # If the client exists but the credentials have expired, create a new client.\n        elif (\n            self.client is not None\n            and self.client_creation_time is not None\n            and (\n                datetime.now() - self.client_creation_time\n            ).total_seconds() > self.credentials_expiry_interval\n        ):\n            self.client = boto3.client(\n                \"s3\",\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key,\n                aws_session_token=self.aws_session_token,\n                region_name=self.region_name,\n            )\n\n        return self.client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if num_samples_yielded == 0:\n            raise ValueError(\"The number of samples yielded must be greater than zero.\")\n\n        if num_workers < 0:\n            raise ValueError(\"The number of workers must be greater than or equal to zero.\")\n\n        if batch_size < 1:\n            raise ValueError(\"The batch size must be greater than or equal to one.\")\n\n        if self.distributed_sampler is not None:\n            if self.distributed_sampler.num_replicas != self.world_size:\n                raise ValueError(\"The number of replicas in the distributed sampler must match the world size.\")\n\n        if self.shuffle:\n            if self.seed is None:\n                raise ValueError(\"The seed must be set if shuffling is enabled.\")\n\n        if self.item_loader is not None:\n            if self.item_loader_state is None:\n                raise ValueError(\"The item loader state must be set if an item loader is used.\")\n\n        if self.last_batch is not None:\n            if self.last_batch not in [\"keep\", \"discard\", \"rollover\"]:\n                raise ValueError(\"The last batch must be either 'keep', 'discard', or 'rollover'.\")\n\n        if self.shuffle:\n            if self.seed is None:\n                raise ValueError(\"The seed must be set if shuffling is enabled.\")\n\n        if self.shuffle:\n            if self.seed is None:\n                raise ValueError(\"The seed must be set if shuffling is enabled.\")\n\n        if self.shuffle:\n            if self.seed is None:\n                raise ValueError(\"The seed must be set if shuffling is enabled.\")\n\n        if self.shuffle:\n            if self.seed is None:\n                raise ValueError(\"The seed must be set if shuffling is enabled.\")\n\n        if self.shuffle:\n            if self.seed is None:\n                raise ValueError(\"The seed must be set if shuffling is enabled.\")\n\n        if self.shuffle:\n            if self.seed is None:\n                raise ValueError(\""}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        # Iterate over the state dictionary\n        for key, value in state_dict.items():\n\n            # If the key is the 'state' key, then load the state\n            if key == 'state':\n\n                # Load the state\n                self.load_state(value)\n\n            # If the key is the 'index' key, then load the index\n            elif key == 'index':\n\n                # Load the index\n                self.load_index(value)\n\n            # If the key is the 'data' key, then load the data\n            elif key == 'data':\n\n                # Load the data\n                self.load_data(value)\n\n            # If the key is the 'metadata' key, then load the metadata\n            elif key == 'metadata':\n\n                # Load the metadata\n                self.load_metadata(value)\n\n            # If the key is the 'transform' key, then load the transform\n            elif key == 'transform':\n\n                # Load the transform\n                self.load_transform(value)\n\n            # If the key is the 'transform_metadata' key, then load the transform metadata\n            elif key == 'transform_metadata':\n\n                # Load the transform metadata\n                self.load_transform_metadata(value)\n\n            # If the key is the 'transform_state' key, then load the transform state\n            elif key == 'transform_state':\n\n                # Load the transform state\n                self.load_transform_state(value)\n\n            # If the key is the 'transform_index' key, then load the transform index\n            elif key == 'transform_index':\n\n                # Load the transform index\n                self.load_transform_index(value)\n\n            # If the key is the 'transform_data' key, then load the transform data\n            elif key == 'transform_data':\n\n                # Load the transform data\n                self.load_transform_data(value)\n\n            # If the key is the 'transform_metadata' key, then load the transform metadata\n            elif key == 'transform_metadata':\n\n                # Load the transform metadata\n                self.load_transform_metadata(value)\n\n            # If the key"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if the state dictionary is empty\n        if self._state_dict is None:\n            raise ValueError(\"The state dictionary is empty.\")\n\n        # Check if the state dictionary is not a dictionary\n        if not isinstance(self._state_dict, dict):\n            raise ValueError(\"The state dictionary is not a dictionary.\")\n\n        # Check if the state dictionary has the correct keys\n        if not all(key in self._state_dict for key in self._state_dict_keys):\n            raise ValueError(\"The state dictionary does not have the correct keys.\")\n\n        # Check if the state dictionary has the correct values\n        if not all(self._state_dict[key] == getattr(self, key) for key in self._state_dict_keys):\n            raise ValueError(\"The state dictionary does not have the correct values.\")\n\n        # Check if the state dictionary has the correct item_loader state\n        if not self._state_dict[\"item_loader_state\"] == self.item_loader.state_dict():\n            raise ValueError(\"The state dictionary does not have the correct item_loader state.\")\n\n        # Check if the state dictionary has the correct cache state\n        if not self._state_dict[\"cache\"] == self.cache:\n            raise ValueError(\"The state dictionary does not have the correct cache state.\")\n\n        # Check if the state dictionary has the correct shuffle state\n        if not self._state_dict[\"shuffle\"] == self.shuffle:\n            raise ValueError(\"The state dictionary does not have the correct shuffle state.\")\n\n        # Check if the state dictionary has the correct num_workers state\n        if not self._state_dict[\"num_workers\"] == self.num_workers:\n            raise ValueError(\"The state dictionary does not have the correct num_workers state.\")\n\n        # Check if the state dictionary has the correct input_dir state\n        if not self._state_dict[\"input_dir\"] == self.input_dir:\n            raise ValueError(\"The state dictionary does not have the correct input_dir state.\")\n\n        # Check if the state dictionary has the correct input_urls state\n        if not self._state_dict[\"input_urls\"] =="}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    import hashlib\n    import os\n    import pathlib\n    import random\n    import string\n\n    # If the input directory is None, use an empty string for hashing.\n\n    if input_dir is None:\n\n        input_dir = ''\n\n    # Hash the input directory.\n\n    input_dir_hash = hashlib.sha256(input_dir.encode('utf-8')).hexdigest()\n\n    # If the environment variable for the cache directory is not set, create the cache directory in a default location.\n\n    if os.environ.get('CACHE_DIR') is None:\n\n        # Create a random string of 10 characters.\n\n        random_string = ''.join(random.choice(string.ascii_lowercase) for _ in range(10))\n\n        # Create the cache directory.\n\n        cache_dir = os.path.join(pathlib.Path.home(), '.cache', 'cache_' + random_string, input_dir_hash)\n\n    # Otherwise, create the cache directory in the specified location.\n\n    else:\n\n        cache_dir = os.path.join(os.environ.get('CACHE_DIR'), input_dir_hash)\n\n    # Create the cache directory.\n\n    try:\n\n        os.makedirs(cache_dir)\n\n    # If the cache directory cannot be created, return None.\n\n    except FileExistsError:\n\n        return None\n\n    # Return the path of the cache directory.\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # check if the remote file path is an S3 URL\n        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\"The remote file path must be an S3 URL.\")\n\n        # check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # acquire a file lock\n        with FileLock(local_filepath + \".lock\", timeout=self.timeout):\n\n            # check if the local file already exists\n            if os.path.exists(local_filepath):\n                return\n\n            # check if the s5cmd command-line tool is available\n            if self.use_s5cmd:\n\n                # download the file using the s5cmd command-line tool\n                self.download_file_s5cmd(remote_filepath, local_filepath)\n\n            # check if the boto3 library is available\n            elif self.use_boto3:\n\n                # download the file using the boto3 library\n                self.download_file_boto3(remote_filepath, local_filepath)\n\n            else:\n\n                # raise a ValueError if the s5cmd command-line tool and the boto3 library are not available\n                raise ValueError(\"The s5cmd command-line tool and the boto3 library are not available.\")\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Step-1: Define the distribution strategy\n\n    # Define the distribution strategy based on the world size\n    if worker_env.world_size == 1:\n\n        # If the world size is 1, then all the chunks and intervals are assigned to the same worker\n        worker_to_chunks = {0: chunks_replica}\n        worker_to_intervals = {0: intervals_replica}\n\n    elif worker_env.world_size == 2:\n\n        # If the world size is 2, then the chunks and intervals are distributed in a round-robin fashion\n        worker_to_chunks = {0: chunks_replica[0::2], 1: chunks_replica[1::2]}\n        worker_to_intervals = {0: intervals_replica[0::2], 1: intervals_replica[1::2]}\n\n    else:\n\n        # If the world size is greater than 2, then the chunks and intervals are distributed in a round-robin fashion\n        worker_to_chunks = {\n            worker_index: chunks_replica[worker_index::num_workers] for worker_index in range(num_workers)\n        }\n        worker_to_intervals = {\n            worker_index: intervals_replica[worker_index::num_workers] for worker_index in range(num_workers)\n        }\n\n    # Step-2: Return the distribution\n\n    # Return the distribution\n    return worker_to_chunks, worker_to_intervals\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        # Remove the \"local:\" prefix from the remote file path if present\n        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\"):]\n\n        # Call the superclass's download_file method to download the file from the modified remote file path to the specified local file path\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Serialize the image's dimensions\n        width_bytes = item.width.to_bytes(4, byteorder='big')\n        height_bytes = item.height.to_bytes(4, byteorder='big')\n\n        # Serialize the image's mode\n        mode_bytes = item.mode.encode('utf-8')\n\n        # Serialize the image's raw pixel data\n        pixel_data = item.tobytes()\n\n        # Serialize the image's dimensions, mode, and raw pixel data\n        serialized_data = width_bytes + height_bytes + mode_bytes + pixel_data\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.format == \"JPEG\" and item.filename is not None and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    data = f.read()\n            else:\n                data = item.convert(\"RGB\").tobytes(\"jpeg\", quality=95)\n            return data, None\n        else:\n            raise TypeError(\"The item is not an image.\")"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        # Extract the width, height, and mode size from the beginning of the data.\n        width, height, mode_size = struct.unpack(\"III\", data[:12])\n\n        # Extract the mode string from the data.\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n\n        # Extract the image data from the data.\n        image_data = data[12 + mode_size:]\n\n        # Create a new image object from the extracted data.\n        image = cls(width, height, mode)\n\n        # Set the image data of the image object.\n        image.data = image_data\n\n        # Return the image object.\n        return image"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the tensor's shape and data type information from the byte array.\n        shape = tuple(int(x) for x in data[:self.shape_length].decode().split(self.shape_delimiter))\n        dtype = data[self.shape_length:self.shape_length + self.dtype_length].decode()\n\n        # Reconstruct the tensor from the remaining bytes.\n        tensor = torch.from_numpy(np.frombuffer(data[self.shape_length + self.dtype_length:], dtype=dtype))\n        tensor = tensor.view(shape)\n\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Serialize the tensor\n        buffer = io.BytesIO()\n        torch.save(item, buffer)\n        return (buffer.getvalue(), None)\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            from torchvision.io import read_jpeg\n            return read_jpeg(data)\n        except RuntimeError:\n            from PIL.Image import open\n            return torch.as_tensor(open(data).convert(\"RGB\"))\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        item = item.numpy()\n\n        # Serialize the tensor data\n        data = pickle.dumps(item)\n\n        # Get the data type index\n        index = self.dtype_indices[str(item.dtype)]\n\n        # Prefix the data type index to the serialized data\n        data = f\"no_header_tensor:{index}\".encode() + data\n\n        # Return the serialized data\n        return data, None\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        # Deserialize the byte data into a PyTorch tensor.\n        tensor = torch.tensor(np.frombuffer(data, dtype=self._dtype))\n\n        # Return the deserialized PyTorch tensor.\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = data[0:4].decode()\n        shape = tuple(np.frombuffer(data[4:12], dtype=np.int32))\n\n        # Reconstruct the numpy array based on the data type and shape information\n        if dtype == \"float\":\n            return np.frombuffer(data[12:], dtype=np.float32).reshape(shape)\n        elif dtype == \"int\":\n            return np.frombuffer(data[12:], dtype=np.int32).reshape(shape)\n        elif dtype == \"bool\":\n            return np.frombuffer(data[12:], dtype=np.bool).reshape(shape)\n        else:\n            raise ValueError(\"Invalid data type\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        # Serialize the NumPy array into a bytes object.\n        serialized_item = item.tobytes()\n\n        # Generate a string representing the data type of the array.\n        dtype_identifier = f\"no_header_numpy:{item.dtype.num}\"\n\n        # Return the serialized bytes object and the data type identifier.\n        return serialized_item, dtype_identifier\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Serialize the NumPy array's data type\n        data_type_index = self.data_type_indices[item.dtype.name]\n\n        # Serialize the NumPy array's shape\n        shape = item.shape\n\n        # Serialize the NumPy array's binary data\n        binary_data = item.tobytes()\n\n        # Serialize the NumPy array's data type, shape, and binary data into a bytes object\n        serialized_item = (\n            data_type_index.to_bytes(1, byteorder=\"little\", signed=False)\n            + len(shape).to_bytes(1, byteorder=\"little\", signed=False)\n            + sum([int(dimension).to_bytes(4, byteorder=\"little\", signed=False) for dimension in shape])\n            + binary_data\n        )\n\n        # Return the serialized bytes object and None\n        return serialized_item, None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {}\n        state_dict[\"dataset\"] = self.dataset.state_dict()\n        state_dict[\"current_epoch\"] = self.current_epoch\n        state_dict[\"num_samples_yielded\"] = self.num_samples_yielded\n        state_dict[\"latest_worker_idx\"] = self.latest_worker_idx\n        return state_dict\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        # Check if the required libraries (torchvision and av) are installed\n        if not _check_libraries():\n            raise ImportError(\n                \"The required libraries (torchvision and av) are not installed. Please install them to use this function.\"\n            )\n\n        # Write the data to a temporary file\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            f.seek(0)\n\n            # Deserialize the video file into a video object\n            video, _, info = torchvision.io.read_video(f.name)\n\n        # Delete the temporary file\n        os.unlink(f.name)\n\n        return video"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # If the writing process is already complete, return an empty list\n        if self.filled:\n            return []\n\n        # If the writing process is not complete, write any remaining chunks\n        if self._should_write():\n            self.write_chunk()\n\n        # Write the index file\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of file paths to the written chunks\n        return self.chunks"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        # Update the state of the StreamingDataLoader instance\n        self.epoch = obj[\"epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n\n        # Update the state of the dataset if it is a StreamingDataset or a CombinedStreamingDataset\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset_state\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            for i in range(len(self.dataset.datasets)):\n                self.dataset.datasets[i].load_state_dict(obj[\"dataset_state\"][i])\n        else:\n            raise RuntimeError(\"The dataset must be a StreamingDataset or a CombinedStreamingDataset.\")\n\n        # Prepare the DataLoader for resuming by adjusting internal iterators and flags\n        self.dataset_iter = iter(self.dataset)\n        self.dataset_iter_worker = None\n        self.dataset_iter_worker_index = None\n        self.dataset_iter_worker_cycle_length = None\n        self.dataset_iter_worker_cycle_index = None\n        self.dataset_iter_worker_cycle_index_start = None\n        self.dataset_iter_worker_cycle_index_end = None\n        self.dataset_iter_worker_cycle_index_current = None\n        self.dataset_iter_worker_cycle_index_current_start = None\n        self.dataset_iter_worker_cycle_index_current_end = None\n        self.dataset_iter_worker_cycle_index_current_end_exclusive = None\n        self.dataset_iter_worker_cycle_index_current_end_exclusive_start = None\n        self.dataset_iter_worker_cycle_index_current_end_exclusive_end = None\n        self.dataset_iter_worker_cycle_index_current_end_exclusive_end_start = None\n        self.dataset_iter"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self.iterator is None:\n            return {\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n                \"num_samples_yielded\": num_samples_yielded,\n            }\n\n        return self.iterator.state_dict()\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Iterate over the datasets within the CombinedStreamingDataset\n        for i, dataset in enumerate(self.datasets):\n\n            # If the dataset is a StreamingDataset, load its state\n            if isinstance(dataset, StreamingDataset):\n\n                # Load the state of the StreamingDataset\n                dataset.load_state_dict(state_dict[i])\n\n            # If the dataset is a StreamingDataset, load its state\n            elif isinstance(dataset, CombinedStreamingDataset):\n\n                # Load the state of the CombinedStreamingDataset\n                dataset.load_state_dict(state_dict[i])\n\n            # If the dataset is a StreamingDataset, load its state\n            elif isinstance(dataset, StreamingDataset):\n\n                # Load the state of the StreamingDataset\n                dataset.load_state_dict(state_dict[i])\n\n            # If the dataset is a StreamingDataset, load its state\n            elif isinstance(dataset, StreamingDataset):\n\n                # Load the state of the StreamingDataset\n                dataset.load_state_dict(state_dict[i])\n\n            # If the dataset is a StreamingDataset, load its state\n            elif isinstance(dataset, StreamingDataset):\n\n                # Load the state of the StreamingDataset\n                dataset.load_state_dict(state_dict[i])\n\n            # If the dataset is a StreamingDataset, load its state\n            elif isinstance(dataset, StreamingDataset):\n\n                # Load the state of the StreamingDataset\n                dataset.load_state_dict(state_dict[i])\n\n            # If the dataset is a StreamingDataset, load its state\n            elif isinstance(dataset, StreamingDataset):\n\n                # Load the state of the StreamingDataset\n                dataset.load_state_dict(state_dict[i])\n\n            # If the dataset is a StreamingDataset, load its state\n            elif isinstance(dataset, StreamingDataset):\n\n                # Load the state of the StreamingDataset\n                dataset.load_"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    # Import the `Dir` class from `d6tpipe.dirs`\n    from d6tpipe.dirs import Dir\n\n    # If the input `dir_path` is a `Dir` object, return it\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    # If the input `dir_path` is a string, set the `path` attribute of the `Dir` object to the input `dir_path`\n    if isinstance(dir_path, str):\n        return Dir(path=dir_path)\n\n    # Raise a `ValueError` if the input `dir_path` is not a string or `Dir` object\n    raise ValueError('The input `dir_path` must be a string or `Dir` object.')\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    # Check if the output_dir is a Dir object\n    if not isinstance(output_dir, Dir):\n        raise TypeError(\"The output_dir must be an instance of the Dir class.\")\n\n    # Check if the output_dir is an S3 directory\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"The output_dir must be an S3 directory.\")\n\n    # Check if the output_dir already contains data\n    if output_dir.exists():\n        raise ValueError(\"The output_dir already contains data.\")\n\n    # Check if appending data to the directory is allowed\n    if append:\n        raise NotImplementedError(\"Appending data to the directory is not implemented.\")\n\n    # Check if overwriting data in the directory is allowed\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data in the directory is not implemented.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the output directory is an S3 bucket directory\n    if not output_dir.is_s3_bucket_dir():\n        raise ValueError(\"The output directory is not an S3 bucket directory.\")\n\n    # Check if the output directory already contains an index file\n    if output_dir.has_index_file():\n        raise ValueError(\"The output directory already contains an index file.\")\n\n    # Check if the output directory already contains an index file\n    if output_dir.has_index_file():\n        raise ValueError(\"The output directory already contains an index file.\")\n\n    # Delete all objects in the output directory\n    output_dir.delete_all_objects()\n\n    # Assert that the output directory does not contain an index file\n    assert not output_dir.has_index_file()\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all index parts to be available\n        while len(os.listdir(self.cache_dir)) < num_workers:\n            time.sleep(1)\n\n        # Merge index parts if the current node is the master node\n        if node_rank == 0:\n\n            # Merge index parts\n            with open(self.index_path, \"wb\") as outfile:\n                for fname in os.listdir(self.cache_dir):\n                    with open(os.path.join(self.cache_dir, fname), \"rb\") as infile:\n                        outfile.write(infile.read())\n\n            # Remove index parts\n            for fname in os.listdir(self.cache_dir):\n                os.remove(os.path.join(self.cache_dir, fname))\n\n        # Wait until the merged index file is available\n        else:\n            while not os.path.exists(self.index_path):\n                time.sleep(1)"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Import the SDK modules\n    import os\n    import sys\n    import subprocess\n    import time\n    import requests\n    from pathlib import Path\n    from datetime import datetime\n    from datetime import timedelta\n    from typing import Optional\n    from typing import Union\n    from typing import List\n    from typing import Dict\n    from typing import Any\n    from typing import Tuple\n    from typing import Callable\n    from typing import Type\n    from typing import TypeVar\n    from typing import Generic\n    from typing import NewType\n    from typing import overload\n    from typing import final\n    from typing import Literal\n    from typing import Protocol\n    from typing import Final\n    from typing import ClassVar\n    from typing import runtime_checkable\n    from typing import get_type_hints\n    from typing import get_origin\n    from typing import get_args\n    from typing import get_type_aliases\n    from typing import get_origin\n    from typing import get_args\n    from typing import get_type_aliases\n    from typing import runtime\n    from typing import runtime_checkable\n    from typing import protocol\n    from typing import cast\n    from typing import overload\n    from typing import final\n    from typing import NoReturn\n    from typing import Any\n    from typing import Union\n    from typing import Type\n    from typing import Tuple\n    from typing import TypeVar\n    from typing import Generic\n    from typing import NewType\n    from typing import overload\n    from typing import final\n    from typing import NoReturn\n    from typing import Any\n    from typing import Union\n    from typing import Type\n    from typing import Tuple\n    from typing import TypeVar\n    from typing import Generic\n    from typing import NewType\n    from typing import overload\n    from typing import final\n    from typing import NoReturn\n    from typing import Any\n    from typing import Union\n    from typing import Type\n    from typing import Tuple\n    from typing import TypeVar\n    from typing import Generic\n    from typing import NewType\n    from typing import overload\n    from typing import final\n    from typing import NoReturn\n    from typing import Any\n    from typing import Union\n    from typing import Type"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        # Add the chunk indexes to the deletion queue.\n        self.delete_queue.extend(chunk_indexes)\n\n        # If the deletion queue is full, delete the chunks.\n        if len(self.delete_queue) >= self.delete_queue_max_size:\n            self.delete_chunks()\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # If the cache directory is not available, return None.\n        if not self._cache_dir:\n            return None\n\n        # If the serializers are not available, return None.\n        if not self._serializers:\n            return None\n\n        # If the remote input directory is not available, return None.\n        if not self._remote_input_dir:\n            return None\n\n        # If the item loader is not available, return None.\n        if not self._item_loader:\n            return None\n\n        # If the cache directory is not available, return None.\n        if not self._cache_dir:\n            return None\n\n        # If the cache directory is not available, return None.\n        if not self._cache_dir:\n            return None\n\n        # If the serializers are not available, return None.\n        if not self._serializers:\n            return None\n\n        # If the remote input directory is not available, return None.\n        if not self._remote_input_dir:\n            return None\n\n        # If the item loader is not available, return None.\n        if not self._item_loader:\n            return None\n\n        # If the cache directory is not available, return None.\n        if not self._cache_dir:\n            return None\n\n        # If the serializers are not available, return None.\n        if not self._serializers:\n            return None\n\n        # If the remote input directory is not available, return None.\n        if not self._remote_input_dir:\n            return None\n\n        # If the item loader is not available, return None.\n        if not self._item_loader:\n            return None\n\n        # If the cache directory is not available, return None.\n        if not self._cache_dir:\n            return None\n\n        # If the serializers are not available, return None.\n        if not self._serializers:\n            return None\n\n        # If the remote input directory is not available, return None.\n        if not self._remote_input_dir:\n            return None\n\n        # If the item loader is not available, return None.\n        if not self._item_loader:\n            return"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        # Enqueue the chunk indices to be downloaded\n        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration is not set. Please set the configuration before using the BinaryReader.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The provided index is not an instance of ChunkedIndex.\")\n\n        if not self.index_config:\n            raise Exception(\"The reader's index configuration is not defined.\")\n\n        if not self.prepare_thread:\n            raise Exception(\"The prepare thread is not running.\")\n\n        # Get the chunk and item information from the index.\n        chunk_info = index.get_chunk_info()\n        item_info = index.get_item_info()\n\n        # Get the chunk's local path.\n        chunk_path = self.index_config.get_chunk_path(chunk_info)\n\n        # Get the chunk's local path.\n        item_path = self.index_config.get_item_path(item_info)\n\n        # Get the chunk's local path.\n        item_path = self.index_config.get_item_path(item_info)\n\n        # Get the chunk's local path.\n        item_path = self.index_config.get_item_path(item_info)\n\n        # Get the chunk's local path.\n        item_path = self.index_config.get_item_path(item_info)\n\n        # Get the chunk's local path.\n        item_path = self.index_config.get_item_path(item_info)\n\n        # Get the chunk's local path.\n        item_path = self.index_config.get_item_path(item_info)\n\n        # Get the chunk's local path.\n        item_path = self.index_config.get_item_path(item_info)\n\n        # Get the chunk's local path.\n        item_path = self.index_config.get_item_path(item_info)\n\n        # Get the chunk's local path.\n        item_path = self.index_config.get_item_path(item_info)\n\n        # Get the chunk's local path.\n        item_path = self.index_config.get_item_path(item_info)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    # import packages\n    import ray\n    from ray import get, put\n    from ray.util.placement_group import placement_group, remove_placement_group\n    from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy\n\n    # check if distributed environment\n    if ray.util.client.ray.is_connected():\n\n        # get object\n        obj = get(obj)\n\n    # return object\n    return obj"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # create a list of chunk indexes for each node\n    chunk_indexes_per_node = [\n        [\n            chunk_index\n            for chunk_index in range(\n                chunks_per_ranks[rank_index][0], chunks_per_ranks[rank_index][1]\n            )\n        ]\n        for rank_index in range(distributed_env.world_size)\n    ]\n\n    # shuffle the chunk indexes for each node\n    for chunk_indexes in chunk_indexes_per_node:\n        random.Random(seed + current_epoch).shuffle(chunk_indexes)\n\n    # flatten the chunk indexes across all nodes\n    return [\n        chunk_index\n        for chunk_indexes in chunk_indexes_per_node\n        for chunk_index in chunk_indexes\n    ]\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Initialize variables\n    input_dir: Optional[str] = None\n    input_paths: List[str] = []\n\n    # Iterate over inputs\n    for input in inputs:\n\n        # Check if input is a valid file path\n        if isinstance(input, str) and os.path.isfile(input):\n\n            # Append input to list of input paths\n            input_paths.append(input)\n\n    # Check if any input paths were found\n    if len(input_paths) > 0:\n\n        # Determine input directory\n        input_dir = os.path.dirname(os.path.commonpath(input_paths))\n\n    # Return input directory\n    return input_dir\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    import dns.resolver\n    import dns.exception\n    import contextlib\n\n    @contextlib.contextmanager\n    def dns_context(enable: bool):\n        try:\n            if enable:\n                dns.resolver.default_resolver = dns.resolver.Resolver(configure=False)\n                dns.resolver.default_resolver.nameservers = ['8.8.8.8', '8.8.4.4']\n                dns.resolver.default_resolver.cache = dns.resolver.LRUCache()\n            yield\n        finally:\n            dns.resolver.default_resolver = dns.resolver.Resolver(configure=False)\n            dns.resolver.default_resolver.nameservers = ['127.0.0.1']\n            dns.resolver.default_resolver.cache = dns.resolver.LRUCache()\n\n    return dns_context(enable)\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # get the number of items to be distributed\n    num_items = len(indexes)\n\n    # get the number of items to be distributed per rank\n    if drop_last:\n        num_items_per_rank = num_items // distributed_env.world_size\n    else:\n        num_items_per_rank = (num_items + distributed_env.world_size - 1) // distributed_env.world_size\n\n    # get the number of items to be distributed in the last rank\n    num_items_in_last_rank = num_items - (num_items_per_rank * (distributed_env.world_size - 1))\n\n    # get the number of items to be distributed in the other ranks\n    num_items_in_other_ranks = num_items_per_rank * (distributed_env.world_size - 1)\n\n    # get the number of items to be distributed in each rank\n    num_items_per_rank_list = [num_items_per_rank] * (distributed_env.world_size - 1)\n    num_items_per_rank_list.append(num_items_in_last_rank)\n\n    # get the number of items to be distributed in each rank\n    num_items_per_rank_list_other_ranks = [num_items_per_rank] * (distributed_env.world_size - 1)\n    num_items_per_rank_list_other_ranks.append(num_items_in_other_ranks)\n\n    # get the number of items to be distributed in each rank\n    num_items_per_rank_list_other_ranks_cumsum = np.cumsum(num_items_per_rank_list_other_ranks)\n\n    # get the number of items to be distributed in each rank\n    num_items_per_rank_list_cumsum = np.cumsum(num_items_per_rank_list)\n\n    # get the number of items to be distributed in each rank\n    num_items_per_rank_list_cumsum"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        # Prepare the keyword arguments to be passed to the transformation function.\n        kwargs = {}\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        # Prepare the arguments to be passed to the transformation function.\n        args = [item_metadata, output_dir]\n\n        # Call the transformation function.\n        self._fn(*args, **kwargs)"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n            return response\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # TODO: Add support for `num_downloaders` and `num_uploaders`\n\n    # TODO: Add support for `reorder_files`\n\n    # TODO: Add support for `reader`\n\n    # TODO: Add support for `batch_size`\n\n    # TODO: Add support for `fast_dev_run`\n\n    # TODO: Add support for `num_nodes`\n\n    # TODO: Add support for `machine`\n\n    # TODO: Add support for `num_downloaders`\n\n    # TODO: Add support for `num_uploaders`\n\n    # TODO: Add support for `reorder_files`\n\n    # TODO: Add support for `reader`\n\n    # TODO: Add support for `batch_size`\n\n    # TODO: Add support for `fast_dev_run`\n\n    # TODO: Add support for `num_nodes`\n\n    # TODO: Add support for `machine`\n\n    # TODO: Add support for `num_downloaders`\n\n    # TODO: Add support for `num_uploaders`\n\n    # TODO: Add support for `reorder_files`\n\n    # TODO: Add support for `reader`\n\n    # TODO: Add support for `batch_size`\n\n    # TODO: Add support for `fast_dev_run`\n\n    # TODO: Add support for `num_nodes`\n\n    # TODO: Add support for `machine`\n\n    # TODO: Add support for `num_downloaders`\n\n    # TODO: Add support for `num_uploaders`\n\n    # TODO: Add support for `reorder_files`\n\n    # TODO: Add support for `reader`\n\n    # TODO: Add support for `batch_size`\n\n    # TODO: Add support for `fast_dev_run`\n\n    # TODO: Add support for `num_nodes`\n\n    # TODO: Add support for `machine`\n\n    # TODO: Add support for `num_downloaders`\n\n    # TODO: Add support for `num_uploaders`\n\n    # TODO: Add support for `reorder_files`\n\n    # TODO: Add support for `reader"}
{"namespace": "litdata.processing.functions.map", "completion": "    # TODO: Add support for `batch_size`\n    # TODO: Add support for `reorder_files`\n    # TODO: Add support for `error_when_not_empty`\n    # TODO: Add support for `reader`\n\n    # TODO: Add support for `fast_dev_run`\n    # TODO: Add support for `num_workers`\n    # TODO: Add support for `num_nodes`\n    # TODO: Add support for `machine`\n    # TODO: Add support for `num_downloaders`\n    # TODO: Add support for `num_uploaders`\n\n    # TODO: Add support for `weights`\n\n    # TODO: Add support for `output_dir`\n\n    # TODO: Add support for `inputs`\n\n    # TODO: Add support for `fn`\n\n    # TODO: Add support for `fast_dev_run`\n    # TODO: Add support for `num_workers`\n    # TODO: Add support for `num_nodes`\n    # TODO: Add support for `machine`\n    # TODO: Add support for `num_downloaders`\n    # TODO: Add support for `num_uploaders`\n\n    # TODO: Add support for `weights`\n\n    # TODO: Add support for `output_dir`\n\n    # TODO: Add support for `inputs`\n\n    # TODO: Add support for `fn`\n\n    # TODO: Add support for `fast_dev_run`\n    # TODO: Add support for `num_workers`\n    # TODO: Add support for `num_nodes`\n    # TODO: Add support for `machine`\n    # TODO: Add support for `num_downloaders`\n    # TODO: Add support for `num_uploaders`\n\n    # TODO: Add support for `weights`\n\n    # TODO: Add support for `output_dir`\n\n    # TODO: Add support for `inputs`\n\n    # TODO: Add support for `fn`\n\n    # TODO: Add support for `fast_dev_run`\n    # TODO: Add support for `num_workers`\n    # TODO: Add support for `num_nodes"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Import external modules\n    import os\n    import urllib.request\n    import queue\n    import threading\n\n    # Import local modules\n    from . import _utils\n\n    # Initialize variables\n    download_thread_count = 10\n    download_threads = []\n    download_thread_index = 0\n    download_thread_lock = threading.Lock()\n    download_thread_event = threading.Event()\n    download_thread_event.set()\n\n    # Create a function to download a file\n    def _download_file(file_path: str, file_url: str) -> None:\n\n        \"\"\"\n        This function downloads a file from a remote URL to a local path.\n\n        Input-Output Arguments\n        :param file_path: str. The path to the local file to which the file is to be downloaded.\n        :param file_url: str. The URL from which the file is to be downloaded.\n        :return: None. There are no return values as the function's purpose is to perform side effects (downloading files).\n        \"\"\"\n\n        # Import external modules\n        import urllib.request\n\n        # Download the file\n        urllib.request.urlretrieve(file_url, file_path)\n\n    # Create a function to download a file list\n    def _download_file_list(file_list: list) -> None:\n\n        \"\"\"\n        This function downloads a list of files from a remote directory to a local cache directory.\n\n        Input-Output Arguments\n        :param file_list: list. The list of file paths to download.\n        :return: None. There are no return values as the function's purpose is to perform side effects (downloading files).\n        \"\"\"\n\n        # Import external modules\n        import os\n        import urllib.request\n\n        # Import local modules\n        from . import _utils\n\n        # Create a function to download a file\n        def _download_file(file_path: str, file_url: str) -> None:\n\n            \"\"\"\n            This function downloads a file from a remote URL to a local path.\n\n            Input-Output Arguments"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # loop until a termination signal is sent\n    while True:\n\n        # get a chunk to upload\n        chunk = upload_queue.get()\n\n        # check if the termination signal was sent\n        if chunk == 'TERMINATE':\n            break\n\n        # check if the chunk is a file path\n        if isinstance(chunk, str):\n\n            # check if the file path is a local file\n            if chunk.startswith(cache_dir):\n\n                # get the relative file path\n                chunk = chunk[len(cache_dir):]\n\n            # check if the file path is a local file\n            if chunk.startswith('/'):\n\n                # get the relative file path\n                chunk = chunk[1:]\n\n            # upload the file\n            output_dir.upload(chunk)\n\n            # add the file path to the remove queue\n            remove_queue.put(chunk)\n\n        # check if the chunk is a tuple\n        elif isinstance(chunk, tuple):\n\n            # get the temporary directory and the file path\n            temp_dir, chunk = chunk\n\n            # check if the file path is a local file\n            if chunk.startswith('/'):\n\n                # get the relative file path\n                chunk = chunk[1:]\n\n            # upload the file\n            output_dir.upload(chunk, temp_dir)\n\n            # add the file path to the remove queue\n            remove_queue.put(chunk)\n\n        # raise an error if the chunk is not a file path or a tuple\n        else:\n            raise TypeError('upload queue items must be file paths or tuples')\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Print distribution details\n    if file_size:\n        sizes = [os.path.getsize(item) for item in user_items]\n        total_size = sum(sizes)\n        sizes = [size / 1024 / 1024 for size in sizes]\n    else:\n        sizes = [1] * len(user_items)\n        total_size = sum(weights)\n\n    sizes_this_node = [sizes[i] for i in worker_ids_this_node]\n    total_size_this_node = sum(sizes_this_node)\n\n    print(\n        f\"Node {node_rank + 1} out of {num_nodes} nodes with {num_workers} workers:\",\n        f\"{total_size_this_node / total_size:.1%} of total data\",\n        f\"({total_size_this_node / 1024 / 1024:.1f} MB out of {total_size / 1024 / 1024:.1f} MB)\",\n    )\n\n    # Shuffle items\n    worker_items_this_node = []\n    for worker_id in worker_ids_this_node:\n        items = worker_items[worker_id]\n        weights = worker_weights[worker_id]\n        items = [\n            item\n            for _ in range(int(1000 * weights[0] + random.random()))\n            for item in items\n        ]\n        random.shuffle(items)\n        worker_items_this_node.append(items)"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # get the total number of nodes in the environment\n    num_nodes = _get_num_nodes()\n\n    # get the current node's rank\n    node_rank = _get_node_rank()\n\n    # get the total number of workers across all nodes\n    num_workers_total = num_nodes * num_workers\n\n    # get the number of items each worker should process\n    num_items_per_worker = len(user_items) // num_workers_total\n\n    # get the number of items that will be distributed to the last worker\n    remainder = len(user_items) % num_workers_total\n\n    # get the start and end indices for the current worker\n    start_index = node_rank * num_workers + num_items_per_worker * node_rank + min(node_rank, remainder)\n    end_index = start_index + num_items_per_worker + (1 if node_rank < remainder else 0)\n\n    # get the items assigned to the current worker\n    items_for_current_worker = user_items[start_index:end_index]\n\n    # ensure the output list has the correct length\n    if len(items_for_current_worker) != num_workers:\n        raise RuntimeError(f\"The number of items assigned to the current worker ({len(items_for_current_worker)}) does not match the number of workers ({num_workers}).\")\n\n    # return the items assigned to the current worker\n    return items_for_current_worker\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Clean up cache directories\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n        if os.path.exists(self.cache_dir_2):\n            shutil.rmtree(self.cache_dir_2)\n        if os.path.exists(self.cache_dir_3):\n            shutil.rmtree(self.cache_dir_3)\n        if os.path.exists(self.cache_dir_4):\n            shutil.rmtree(self.cache_dir_4)\n        if os.path.exists(self.cache_dir_5):\n            shutil.rmtree(self.cache_dir_5)\n        if os.path.exists(self.cache_dir_6):\n            shutil.rmtree(self.cache_dir_6)\n        if os.path.exists(self.cache_dir_7):\n            shutil.rmtree(self.cache_dir_7)\n        if os.path.exists(self.cache_dir_8):\n            shutil.rmtree(self.cache_dir_8)\n        if os.path.exists(self.cache_dir_9):\n            shutil.rmtree(self.cache_dir_9)\n        if os.path.exists(self.cache_dir_10):\n            shutil.rmtree(self.cache_dir_10)\n        if os.path.exists(self.cache_dir_11):\n            shutil.rmtree(self.cache_dir_11)\n        if os.path.exists(self.cache_dir_12):\n            shutil.rmtree(self.cache_dir_12)\n        if os.path.exists(self.cache_dir_13):\n            shutil.rmtree(self.cache_dir_13)\n        if os.path.exists(self.cache_dir_14):\n            shutil.rmtree(self.cache_dir_14)\n        if os.path.exists(self.cache_dir_15):\n            shutil.rm"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    # Compute the number of workers to use based on the CPU count\n    num_workers = multiprocessing.cpu_count()\n\n    # Create a ThreadPoolExecutor to parallelize the file size retrieval process\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        # Compute the file sizes for each item in the list\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n        # Return the file sizes\n        return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        if input_dir is not None:\n            if element.startswith(input_dir):\n                return True\n        if os.path.exists(element):\n            return True\n    return False\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        # Assert that the number of layers and neurons are greater than 0\n        assert n_layers > 0, \"The number of layers must be greater than 0.\"\n        assert n_neurons > 0, \"The number of neurons must be greater than 0.\"\n\n        # If tinycudann is used, create a network with the specified number of input and output dimensions, the number of layers, and the number of neurons per layer\n        if self.tcnn:\n            # If the number of neurons is less than or equal to 1024, use the fully-connected network\n            if n_neurons <= 1024:\n                return tcnn.Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_layers=n_layers,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n            # If the number of neurons is greater than 1024, use the convolutional network\n            else:\n                return tcnn.NetworkConv2D(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_layers=n_layers,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n\n        # If tinycudnn is not used, create a network with the specified number of input and output dimensions, the number of layers, and the number of neurons per layer\n        else:\n            # Create a list of the number of neurons per layer\n            n_neurons_per_layer = [n_neurons] * n_layers\n\n            # Create a list of the activation functions per layer\n            activation_per_layer = [activation] * n_layers\n\n            # If the number of neurons is less than or equal to 1024, use the fully-connected network"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the rolling median of the signal\n        rolling_median = np.zeros(signal.shape)\n        for i in range(kernel_offset, signal.shape[0] - kernel_offset):\n            rolling_median[i] = np.median(signal[i - kernel_offset:i + kernel_offset])\n\n        # Trim the rolling median array to remove edge effects\n        rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return rolling_median"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Get the number of columns in the template\n    n_cols = template_probe.shape[1]\n\n    # Get the number of columns in the rotation shift\n    n_rot_cols = rotation_shift\n\n    # Get the number of rows in the template\n    n_rows = template_probe.shape[0]\n\n    # Get the number of rows in the rotation shift\n    n_rot_rows = n_rows - n_rot_cols + 1\n\n    # Get the number of columns in the rotation shift\n    n_rot_cols = n_cols - n_rot_cols + 1\n\n    # Get the number of columns in the rotation shift\n    n_rot_rows = n_rows - n_rot_cols + 1\n\n    # Get the number of columns in the rotation shift\n    n_rot_cols = n_cols - n_rot_cols + 1\n\n    # Get the number of columns in the rotation shift\n    n_rot_rows = n_rows - n_rot_cols + 1\n\n    # Get the number of columns in the rotation shift\n    n_rot_cols = n_cols - n_rot_cols + 1\n\n    # Get the number of columns in the rotation shift\n    n_rot_rows = n_rows - n_rot_cols + 1\n\n    # Get the number of columns in the rotation shift\n    n_rot_cols = n_cols - n_rot_cols + 1\n\n    # Get the number of columns in the rotation shift\n    n_rot_rows = n_rows - n_rot_cols + 1\n\n    # Get the number of columns in the rotation shift\n    n_rot_cols = n_cols - n_rot_cols + 1\n\n    # Get the number of columns in the rotation shift\n    n_rot_rows = n_rows - n_rot_cols + 1\n\n    # Get the number of columns in the rotation shift\n    n_rot_cols = n_cols - n_rot_cols + 1\n\n    # Get the number of columns in the rotation shift\n    n_rot_rows = n_rows - n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize the number of iterations to zero.\n        iterations = 0\n\n        # Initialize the number of successful iterations to zero.\n        successful_iterations = 0\n\n        # Initialize the number of bisectors to zero.\n        bisectors_count = 0\n\n        # Initialize the starting points of the perpendicular bisectors to an empty array.\n        starting_points = np.array([])\n\n        # Initialize the ending points of the perpendicular bisectors to an empty array.\n        ending_points = np.array([])\n\n        # Initialize the number of iterations to zero.\n        iterations = 0\n\n        # Initialize the number of successful iterations to zero.\n        successful_iterations = 0\n\n        # Initialize the number of bisectors to zero.\n        bisectors_count = 0\n\n        # Initialize the starting points of the perpendicular bisectors to an empty array.\n        starting_points = np.array([])\n\n        # Initialize the ending points of the perpendicular bisectors to an empty array.\n        ending_points = np.array([])\n\n        # While the number of successful iterations is less than the number of iterations allowed.\n        while successful_iterations < self.max_iterations:\n\n            # Increment the number of iterations.\n            iterations += 1\n\n            # If the number of iterations is greater than the maximum number of iterations allowed.\n            if iterations > self.max_iterations:\n\n                # Raise an exception.\n                raise EyeCentersEstimationError(\n                    \"The function failed to find a sufficient number of point pairs that meet the distance criterion within the maximum number of iterations allowed. This indicates that it may not be possible to accurately estimate the center of the shape.\"\n                )\n\n            # Otherwise, if the number of iterations is less than the maximum number of iterations allowed.\n            else:\n\n                # Generate a random integer between 0 and the number of vertices in the polygon.\n                random_vertex_index = random.randint(0, len(polygon) - 1)\n\n                # Generate a random integer between 0 and the number of vertices in the polygon.\n                random_vertex_index"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output_json = json.loads(output)\n            return self.check_type(output_json, type_definition)\n        except Exception as e:\n            return False"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get the function's signature and type hints\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Get the function's name\n        name = func_object.__name__\n\n        # Get the function's docstring\n        docstring = func_object.__doc__\n\n        # Get the function's input and output type hints\n        input_type_hint = type_hints.get(\"input\", None)\n        output_type_hint = type_hints.get(\"return\", None)\n\n        # Get the function's input and output class definitions\n        input_class_definition = get_class_definition(input_type_hint)\n        output_class_definition = get_class_definition(output_type_hint)\n\n        # Determine the function type based on the output type hint\n        if issubclass(output_class_definition, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        # Create a FunctionDescription instance\n        function_description = FunctionDescription(\n            name=name,\n            docstring=docstring,\n            input_type_hint=input_type_hint,\n            output_type_hint=output_type_hint,\n            input_class_definition=input_class_definition,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n            signature=signature,\n        )\n\n        return function_description"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        # Calculate the hash values of the string\n        hashes = self.hashes(string)\n\n        # Set the bits in the bit array to 1\n        for hash in hashes:\n            self.bit_array[hash] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        # Load the bit array from persistence\n        bit_array = self.persistence.load()\n\n        # Check if the loaded bit array's length matches the expected length based on the BloomFilter size\n        if len(bit_array) != self.size:\n\n            # Log a warning\n            logging.warning(\"The loaded bit array's length does not match the expected length based on the BloomFilter size. It is likely that the BloomFilter has been corrupted. Reinitializing the bit array and indices and saving the new state.\")\n\n            # Reinitialize the bit array and indices\n            self.init_bit_array()\n\n            # Save the new state\n            self.save()\n\n        else:\n\n            # Set the bit array\n            self.bit_array = bit_array\n\n            # Set the indices\n            self.indices = [self.bit_array[i:i + self.num_bits] for i in range(0, self.size, self.num_bits)]\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Check if the string is in the Bloom Filter\n        for i in range(self.hash_count):\n            # Get the hash function and its indices\n            hash_function = self.hash_functions[i]\n            hash_indices = hash_function(string)\n\n            # Check if the bit at each index is set\n            for j in hash_indices:\n                if not self.bit_array[j]:\n                    return False\n\n        # If all bits are set, the string is possibly in the Bloom Filter\n        return True"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        # Load the distilled model\n        if 'distilled_model' in json_dict:\n            self.distilled_model = json_dict['distilled_model']\n\n        # Load the current model stats\n        if 'current_model_stats' in json_dict:\n            self.current_model_stats = json_dict['current_model_stats']\n\n        # Load the last training run\n        if 'last_training_run' in json_dict:\n            self.last_training_run = json_dict['last_training_run']\n\n        # Load the current training run\n        if 'current_training_run' in json_dict:\n            self.current_training_run = json_dict['current_training_run']\n\n        # Load the number of training runs\n        if 'nr_of_training_runs' in json_dict:\n            self.nr_of_training_runs = json_dict['nr_of_training_runs']\n\n        # Load the teacher models\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n\n        return self"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify the API key\n        if not self.verify_api_key():\n            raise Exception(\"Invalid API key\")\n\n        # Validate the parameters\n        self.validate_parameters(model, system_message, prompt, **kwargs)\n\n        # Retry generation up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                # Generate the response\n                response = self.generate_response(model, system_message, prompt, **kwargs)\n\n                # Process the response\n                response = self.process_response(response)\n\n                # Return the response\n                return response\n\n            except Exception as e:\n                # Raise the exception if it is not a rate limit error\n                if \"429\" not in str(e):\n                    raise e\n\n                # Sleep for the rate limit duration\n                time.sleep(int(re.findall(r'\\d+', str(e))[0]))\n\n        # Raise an exception if generation fails after 5 retries\n        raise Exception(\"Generation failed after 5 retries\")\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The given matrix is not symmetric.\")\n\n    # Check if the diagonal elements are close to zero\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0])):\n        raise ValueError(\"The given matrix is not a distance matrix.\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Get the language model to be used for generation\n        model = self.get_model(function_description, llm_parameters)\n\n        # Get the prompt to be used for generation\n        prompt = self.get_prompt(function_description, args, kwargs, llm_parameters)\n\n        # Get the model to be used for generation\n        distill = self.get_distill(function_description, llm_parameters)\n\n        # Get the examples to be used for fine-tuning\n        examples = self.get_examples(function_description, args, kwargs, llm_parameters, func_hash)\n\n        return prompt, model, distill, examples\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    # Check if the input covariance matrix is positive definite\n    if not is_positive_definite(cov):\n\n        # If the input covariance matrix is not positive definite, raise an error\n        raise ValueError(\"The input covariance matrix is not positive definite.\")\n\n    # If the Higham & Nick (2002) algorithm is used, compute the nearest positive definite covariance matrix\n    if higham:\n\n        # Compute the eigendecomposition of the input covariance matrix\n        eig_vals, eig_vecs = np.linalg.eigh(cov)\n\n        # Compute the number of eigenvalues\n        num_eig = len(eig_vals)\n\n        # Compute the maximum eigenvalue\n        max_eig = np.max(eig_vals)\n\n        # Compute the minimum eigenvalue\n        min_eig = np.min(eig_vals)\n\n        # Compute the ratio between the maximum and minimum eigenvalues\n        eig_ratio = max_eig / min_eig\n\n        # Compute the maximum eigenvalue that is allowed\n        max_eig_allowed = max_eig * (1 + np.sqrt(1 + eig_ratio ** 2)) / 2\n\n        # Compute the minimum eigenvalue that is allowed\n        min_eig_allowed = max_eig * (1 - np.sqrt(1 + eig_ratio ** 2)) / 2\n\n        # Compute the ratio between the maximum and minimum allowed eigenvalues\n        eig_ratio_allowed = max_eig_allowed / min_eig_allowed\n\n        # Compute the number of eigenvalues that are allowed\n        num_eig_allowed = int(np.ceil(num_eig / eig_ratio_allowed))\n\n        # Compute the number of eigenvalues that are not allowed\n        num_eig_not_allowed = num_eig - num_eig_allowed\n\n        # Compute the number of iterations\n        num_iterations = 0\n\n        # Compute the maximum number of iterations\n        max_iterations = higham_max_iter"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    if data_home is None:\n        data_home = Path.home() / \"skfolio_data\"\n    else:\n        data_home = Path(data_home)\n\n    if data_home.exists():\n        shutil.rmtree(data_home)\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj, \"str\"), _str_schema\n    elif isinstance(obj, bytes):\n        return (obj, \"bytes\"), _bytes_schema\n    elif isinstance(obj, list):\n        return _flatten_list(obj)\n    elif isinstance(obj, tuple):\n        return _flatten_tuple(obj)\n    elif isinstance(obj, dict):\n        return _flatten_dict(obj)\n    elif isinstance(obj, Instances):\n        return _flatten_instances(obj)\n    elif isinstance(obj, Boxes):\n        return _flatten_boxes(obj)\n    elif isinstance(obj, ROIMasks):\n        return _flatten_roimasks(obj)\n    else:\n        raise ValueError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert to numpy arrays\n    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    # Check if the input is valid\n    if groups.ndim != 2:\n        raise ValueError(f\"{names[0]} must be a 2D array\")\n    if equations.ndim != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array\")\n    if groups.shape[0] != equations.shape[0]:\n        raise ValueError(\n            f\"{names[0]} and {names[1]} must have the same number of rows\"\n        )\n\n    # Create a list of all the groups in the equations\n    all_groups = np.unique(groups)\n\n    # Create a list of all the groups in the equations\n    all_groups = np.unique(groups)\n\n    # Create a list of all the groups in the equations\n    all_groups = np.unique(groups)\n\n    # Create a list of all the groups in the equations\n    all_groups = np.unique(groups)\n\n    # Create a list of all the groups in the equations\n    all_groups = np.unique(groups)\n\n    # Create a list of all the groups in the equations\n    all_groups = np.unique(groups)\n\n    # Create a list of all the groups in the equations\n    all_groups = np.unique(groups)\n\n    # Create a list of all the groups in the equations\n    all_groups = np.unique(groups)\n\n    # Create a list of all the groups in the equations\n    all_groups = np.unique(groups)\n\n    # Create a list of all the groups in the equations\n    all_groups = np.unique(groups)\n\n    # Create a list of all the groups in the equations\n    all_groups = np.unique(groups)\n\n    # Create a list of all the groups in the equations\n    all_groups = np.unique(groups)\n\n    # Create a list of all the groups in the equations\n    all_groups = np.unique(groups)\n\n    # Create a list of all"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # Import the necessary libraries\n    import sys\n    import os\n    import inspect\n    import importlib\n    import tempfile\n    import shutil\n    import torch\n    from detectron2.structures import Instances\n\n    # Get the current working directory\n    cwd = os.getcwd()\n\n    # Get the temporary directory\n    tmp_dir = tempfile.gettempdir()\n\n    # Get the name of the temporary directory\n    tmp_dir_name = os.path.basename(tmp_dir)\n\n    # Get the name of the temporary file\n    tmp_file_name = inspect.getframeinfo(inspect.currentframe()).filename\n\n    # Get the name of the temporary file without the extension\n    tmp_file_name = os.path.splitext(tmp_file_name)[0]\n\n    # Get the name of the temporary file with the extension\n    tmp_file_name = tmp_file_name + \".py\"\n\n    # Get the name of the temporary file with the full path\n    tmp_file_name_full = os.path.join(tmp_dir, tmp_file_name)\n\n    # Get the name of the temporary module\n    tmp_module_name = tmp_file_name_full.replace(os.path.sep, \".\")\n\n    # Get the name of the temporary module without the extension\n    tmp_module_name = os.path.splitext(tmp_module_name)[0]\n\n    # Get the name of the temporary module with the extension\n    tmp_module_name = tmp_module_name + \".py\"\n\n    # Get the name of the temporary module with the full path\n    tmp_module_name_full = os.path.join(tmp_dir, tmp_module_name)\n\n    # Get the name of the temporary class\n    tmp_class_name = \"Instances\"\n\n    # Get the name of the temporary class with the full path\n    tmp_class_name_full = tmp_module_name_full.replace(\".py\", \".\" + tmp_class_name)\n\n    # Get the name of the temporary class with the full path\n    tmp_"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    import numpy as np\n    from PIL import Image\n    from PIL.ExifTags import TAGS\n    from PIL.Image import Image as ImageType\n\n    # Read the image\n    image = Image.open(file_name)\n\n    # Get the image's EXIF data\n    exif_data = image.getexif()\n\n    # Get the orientation of the image\n    orientation = exif_data.get(0x0112)\n\n    # Rotate the image if it is not oriented correctly\n    if orientation == 3:\n        image = image.rotate(180, expand=True)\n    elif orientation == 6:\n        image = image.rotate(270, expand=True)\n    elif orientation == 8:\n        image = image.rotate(90, expand=True)\n\n    # Convert the image to the specified format\n    if format is not None:\n        if format == \"BGR\":\n            image = image.convert(\"RGB\")\n            image = np.array(image)\n            image = image[:, :, ::-1]\n        elif format == \"YUV-BT.601\":\n            image = image.convert(\"YCbCr\")\n            image = np.array(image)\n            image[:, :, 0] = image[:, :, 0] * (100 / 255)\n        else:\n            image = image.convert(format)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Apply transformations to image.\n    image = transforms.apply_image(annotation[\"image\"])\n    annotation[\"image\"] = image\n\n    # Apply transformations to bounding boxes.\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # Note that bbox is 1d (per-instance bounding box)\n    bbox = transforms.apply_box([bbox])[0]\n    # Make sure the bounding box is within the image.\n    bbox = bbox.clip(min=0)\n    bbox = np.minimum(bbox, list(image.shape[1:]))\n    annotation[\"bbox\"] = np.minimum(bbox, list(image.shape[1:]))\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Apply transformations to segmentation polygons.\n    if \"segmentation\" in annotation:\n        # first make sure the segmentation polygons are valid\n        # then apply transformations to each polygon\n        # segmentation = transforms.apply_polygons(annotation[\"segmentation\"])\n        segmentation = annotation[\"segmentation\"]\n        segmentation = transforms.apply_polygons(segmentation)\n        # make sure the segmentation polygons are valid\n        assert osp.isfile(osp.join(osp.dirname(__file__), '../data/coco/annotations/coco_instances.json'))\n        coco = COCO(osp.join(osp.dirname(__file__), '../data/coco/annotations/coco_instances.json'))\n        coco_mask = maskUtils\n        segmentation = [coco_mask.decode(coco_mask.encode(np.asarray(p, dtype=np.uint8, order=\"F\"))).astype(np.bool) for p in segmentation]\n        # clip the segmentation polygons to the image size\n        segmentation = [p.clip(0, 0, image.shape[1], image.shape[0]) for p in segmentation]\n        annotation[\""}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        # If the rotation angle is a multiple of 360 degrees, return the original coordinates\n        if self.angle % 360 == 0:\n            return coords\n\n        # If the input coordinates are empty, return the original coordinates\n        if len(coords) == 0:\n            return coords\n\n        # Apply the rotation transformation to the input coordinates\n        return np.dot(coords, self.rm_coords)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Import libraries\n    import torch\n    from torch.profiler import profile, record_function, ProfilerActivity\n    from collections import defaultdict\n    import time\n\n    # Setup profiler\n    p = profile(\n        activities=[ProfilerActivity.CPU],\n        record_shapes=True,\n        with_stack=True,\n    )\n\n    # Run profiler\n    with p as prof:\n        with record_function(\"model_inference\"):\n            model(inputs)\n\n    # Get flops\n    flops = defaultdict(float)\n    for item in prof.key_averages():\n        if item.is_cpu_time:\n            flops[item.name] += item.cpu_time\n\n    # Return flops\n    return flops\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return None\n\n        if interp is None:\n            interp = self.interp\n\n        if self.angle % 360 == 0:\n            return img\n\n        if self.bound_w is None or self.bound_h is None:\n            self.bound_w = img.shape[1]\n            self.bound_h = img.shape[0]\n\n        # Apply the rotation matrix to the image\n        img_rot = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n\n        return img_rot"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n        masks = predictions.pred_masks.numpy() if predictions.has(\"pred_masks\") else None\n\n        if self.metadata.get(\"thing_colors\"):\n            colors = [\n                self.metadata.thing_colors[i] for i in classes\n            ]\n        else:\n            colors = None\n\n        if self.instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n            colors = [\n                self.metadata.thing_colors[i] for i in classes\n            ]\n        alpha = 0.8\n        if self.instance_mode == ColorMode.SEGMENTATION:\n            composite = image.composite_image(\n                self.image.as_format(self.output_format), masks, alpha=alpha\n            )\n        else:\n            composite = image.draw_instances(\n                self.image.as_format(self.output_format),\n                boxes,\n                classes,\n                masks,\n                keypoints=keypoints,\n                assigned_colors=colors,\n                show_class=self.metadata.get(\"thing_classes\", None),\n            )\n\n        return composite\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Retrieve the visualized image from the VisImage instance.\n        image = self.canvas.get_image()\n\n        # Convert the image from RGBA to RGB format.\n        image = image[:, :, :3]\n\n        # Return the image.\n        return image"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Get the image\n        image = dic[\"image\"]\n\n        # Get the image dimensions\n        image_height, image_width = image.shape[:2]\n\n        # Create a VisImage object\n        vis_image = VisImage(image, image_width, image_height)\n\n        # Get the annotations\n        annos = dic.get(\"annotations\", [])\n\n        # Get the semantic segmentation\n        sem_seg = dic.get(\"sem_seg\", None)\n\n        # Get the panoptic segmentation\n        panoptic_seg = dic.get(\"panoptic_seg\", None)\n\n        # Draw the annotations\n        vis_image = self.draw_instance_predictions(vis_image, annos)\n\n        # Draw the semantic segmentation\n        if sem_seg is not None:\n            vis_image = self.draw_sem_seg(vis_image, sem_seg)\n\n        # Draw the panoptic segmentation\n        if panoptic_seg is not None:\n            vis_image = self.draw_panoptic_seg(vis_image, panoptic_seg,\n                                               metadata.get(\"thing_colors\", None),\n                                               metadata.get(\"stuff_colors\", None))\n\n        # Return the modified image object\n        return vis_image"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # If the mask is not binary, it is drawn as a polygon\n        if not self.is_binary_mask(binary_mask):\n            return self.draw_polygon(binary_mask, color, edge_color, text, alpha, area_threshold)\n\n        # If the mask is binary, it is drawn as a filled polygon\n        binary_mask = np.squeeze(binary_mask)\n        R, C = binary_mask.shape\n        if R < 2 or C < 2:\n            return self\n\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        edge_color = edge_color or color\n\n        # Draw the contours on the image\n        contours, _ = cv2.findContours(binary_mask.astype(\"uint8\"), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        for contour in contours:\n            contour = contour.flatten().tolist()\n            self.draw_polygon([contour], color=color, edge_color=edge_color, alpha=alpha)\n\n        # Draw text on the image\n        if text is not None:\n            xy = (np.random.randint(0, C), np.random.randint(0, R))\n            self.draw_text(text, xy, color=color)\n\n        return self\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg}Expected input to be an Instances object, but got {type(input)}.\"\n    assert isinstance(other, Instances), f\"{msg}Expected other to be an Instances object, but got {type(other)}.\"\n\n    if size_as_tensor:\n        assert isinstance(input.image_size, torch.Tensor), f\"{msg}Expected input.image_size to be a torch.Tensor, but got {type(input.image_size)}.\"\n        assert isinstance(other.image_size, torch.Tensor), f\"{msg}Expected other.image_size to be a torch.Tensor, but got {type(other.image_size)}.\"\n        assert torch.allclose(input.image_size, other.image_size), f\"{msg}Expected input.image_size to be equal to other.image_size, but got {input.image_size} and {other.image_size}.\"\n    else:\n        assert isinstance(input.image_size, tuple), f\"{msg}Expected input.image_size to be a tuple, but got {type(input.image_size)}.\"\n        assert isinstance(other.image_size, tuple), f\"{msg}Expected other.image_size to be a tuple, but got {type(other.image_size)}.\"\n        assert input.image_size == other.image_size, f\"{msg}Expected input.image_size to be equal to other.image_size, but got {input.image_size} and {other.image_size}.\"\n\n    for k, v in input.get_fields().items():\n        other_v = other[k]\n        if isinstance(v, Boxes):\n            assert isinstance(other_v, Boxes), f\"{msg}Expected {k} to be a Boxes object, but got {type(other_v)}.\"\n            assert torch.allclose(v.tensor, other_v.tensor), f\"{msg}Expected {k}.tensor to be equal to other."}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # Compute the area of the boxes\n        area = self.width * self.height\n\n        # Return the area\n        return area"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    from maskrcnn_benchmark.modeling.proposal_generation import make_proposal_generator\n\n    return make_proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        gt_classes = (\n            cat([p.gt_classes for p in proposals], dim=0) if len(proposals) else torch.empty(0)\n        )\n        gt_boxes = cat([p.gt_boxes for p in proposals], dim=0)\n        loss_dict = {}\n\n        # Classification loss (cross-entropy)\n        loss_cls = F.cross_entropy(scores, gt_classes, reduction=\"mean\")\n        loss_dict[\"loss_cls\"] = loss_cls * self.loss_weight.cls\n\n        # Box regression loss (Smooth L1)\n        box_regression_weights = self.loss_weight.box_reg * gt_classes.new_tensor(\n            self.box_reg_loss_type\n        )\n        loss_box_reg = smooth_l1_loss(\n            proposal_deltas,\n            gt_boxes,\n            box_regression_weights,\n            smooth_l1_beta=self.smooth_l1_beta,\n            reduction=\"sum\",\n        )\n        loss_dict[\"loss_box_reg\"] = loss_box_reg\n\n        return loss_dict"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    # Retrieve the tracker name from the configuration\n    tracker_name = cfg.TRACKER_NAME\n\n    # Retrieve the tracker class from the registry\n    tracker_class = registry.TRACKER[tracker_name]\n\n    # Instantiate the tracker using the configuration\n    tracker = tracker_class(cfg)\n\n    return tracker"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # The following tensor operations effectively perform the following:\n        #   1. reshape the deltas from (N, k*4) to (N, 4, k)\n        #   2. expand the boxes dimension from (N, 4) to (N, 1, 4) for broadcasting\n        #   3. concat the expanded boxes and the deltas along the boxes dimension, to obtain a tensor of shape (N, 4, k)\n        #   4. reshape the tensor from (N, 4, k) to (N*k, 4) to prepare for the transform\n        #   5. transform the boxes\n        #   6. reshape the transformed boxes to the output shape (N, k*4)\n        #\n        # The transformation is applied as follows:\n        #   1. boxes[:, :2] + 0.5 * boxes[:, 2:] * deltas[:, :2]\n        #   2. exp(deltas[:, 2:]) * boxes[:, 2:]\n        #\n        # Note that this is different from the original transform implementation.\n\n        deltas = deltas.view(deltas.size(0), -1, 4)\n        boxes = boxes.unsqueeze(2)\n        ctr = boxes\n        ctr = ctr.expand(deltas.size(0), -1, -1)\n        deltas = torch.cat((ctr, deltas), 1)\n        deltas = deltas.view(-1, 4)\n        boxes = self.apply_deltas(deltas, boxes)\n        return boxes.view(deltas.size(0), -1)\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image\n        output = self.process(image)\n\n        # Filter the output\n        if anno_type is not None:\n            if isinstance(anno_type, str):\n                if anno_type in output:\n                    return output[anno_type]\n                else:\n                    raise ValueError(f\"The requested annotation type '{anno_type}' was not found in the output.\")\n            elif isinstance(anno_type, list):\n                output_filtered = {}\n                for anno_type_i in anno_type:\n                    if anno_type_i in output:\n                        output_filtered[anno_type_i] = output[anno_type_i]\n                    else:\n                        raise ValueError(f\"The requested annotation type '{anno_type_i}' was not found in the output.\")\n                return output_filtered\n            else:\n                raise TypeError(f\"The anno_type parameter must be a string or a list of strings.\")\n        else:\n            return output"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize(query)\n\n        # Split the query string into keywords\n        keywords = self.split(query)\n\n        # Initialize a dictionary to store the BM25 scores for each keyword\n        scores = {}\n\n        # Iterate through the keywords\n        for keyword in keywords:\n\n            # Initialize a dictionary to store the BM25 scores for each URL\n            url_scores = {}\n\n            # Iterate through the URLs\n            for url in self.urls:\n\n                # Initialize a list to store the BM25 scores for each document\n                doc_scores = []\n\n                # Iterate through the documents\n                for doc in self.documents:\n\n                    # Calculate the BM25 score for the current document\n                    score = self.bm25(keyword, doc, url)\n\n                    # Add the score to the list\n                    doc_scores.append(score)\n\n                # Calculate the average BM25 score for the current URL\n                avg_score = sum(doc_scores) / len(doc_scores)\n\n                # Add the score to the dictionary\n                url_scores[url] = avg_score\n\n            # Calculate the aggregated BM25 score for the current keyword\n            agg_score = sum(url_scores.values()) / len(url_scores)\n\n            # Add the score to the dictionary\n            scores[keyword] = agg_score\n\n        # Return the dictionary\n        return scores"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        # Iterate over the documents\n        for document in documents:\n\n            # Extract the URL and content of the current document\n            url, content = document\n\n            # Index the current document\n            self.index(url, content)"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles of the boxes to be within the range (-180, 180] degrees\n        self.tensor[:, 4] = torch.clamp(self.tensor[:, 4] % 360, -180, 179.999)\n\n        # Identify the indices of the boxes that are nearly horizontal\n        horizontal_mask = torch.abs(self.tensor[:, 4]) <= clip_angle_threshold\n\n        # Convert the boxes to (x1, y1, x2, y2) format\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2\n        boxes = torch.stack((x1, y1, x2, y2), dim=1)\n\n        # Clamp the x and y coordinates to ensure they do not exceed the specified box_size limits\n        boxes[:, :2] = boxes[:, :2].clamp(min=0, max=box_size[1])\n        boxes[:, 2:] = boxes[:, 2:].clamp(min=0, max=box_size[0])\n\n        # Convert the boxes back to their original representation, ensuring that any numerical errors do not increase their sizes\n        self.tensor[:, 0] = boxes[:, 0] + (boxes[:, 2] - boxes[:, 0]) / 2\n        self.tensor[:, 1] = boxes[:, 1] + (boxes[:, 3] - boxes[:, 1]) / 2\n        self.tensor[:, 2] = boxes[:, 2] - boxes[:, 0]\n        self.tensor[:, 3] = boxes[:, 3] - boxes[:, 1]\n\n        # Set the angle of the clipped boxes"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initializes the statistics dictionary\n        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        # Iterates through the data\n        for item in self.data:\n\n            # Updates the statistics dictionary\n            statistics[item['type']] += 1\n\n        # Returns the statistics dictionary\n        return statistics"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    # Import the specified neck type\n    neck_type = cfg.type\n    neck = NECKS[neck_type](cfg)\n\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] == 'cross_entropy':\n        from .cross_entropy import CrossEntropyLoss\n        return CrossEntropyLoss(cfg)\n    elif cfg['type'] == 'focal_loss':\n        from .focal_loss import FocalLoss\n        return FocalLoss(cfg)\n    elif cfg['type'] == 'dice_loss':\n        from .dice_loss import DiceLoss\n        return DiceLoss(cfg)\n    elif cfg['type'] == 'binary_cross_entropy':\n        from .binary_cross_entropy import BinaryCrossEntropyLoss\n        return BinaryCrossEntropyLoss(cfg)\n    elif cfg['type'] == 'multilabel_cross_entropy':\n        from .multilabel_cross_entropy import MultiLabelCrossEntropyLoss\n        return MultiLabelCrossEntropyLoss(cfg)\n    elif cfg['type'] == 'multilabel_dice_loss':\n        from .multilabel_dice_loss import MultiLabelDiceLoss\n        return MultiLabelDiceLoss(cfg)\n    elif cfg['type'] == 'multilabel_focal_loss':\n        from .multilabel_focal_loss import MultiLabelFocalLoss\n        return MultiLabelFocalLoss(cfg)\n    elif cfg['type'] == 'multilabel_binary_cross_entropy':\n        from .multilabel_binary_cross_entropy import MultiLabelBinaryCrossEntropyLoss\n        return MultiLabelBinaryCrossEntropyLoss(cfg)\n    elif cfg['type'] == 'multilabel_dice_loss':\n        from .multilabel_dice_loss import MultiLabelDiceLoss\n        return MultiLabelDiceLoss(cfg)\n    elif cfg['type'] == 'multilabel_focal_loss':\n        from .multilabel_focal_loss import MultiLabelFocalLoss\n        return MultiLabelFocalLoss(cfg)\n    elif cfg['type'] == 'multilabel_binary_cross_entropy':"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    head_type = cfg.pop('type')\n\n    if head_type in HEADS:\n        head = HEADS[head_type](**cfg)\n    else:\n        head = MMDET_HEADS[head_type](**cfg)\n\n    return head"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    import torch\n    from mmcv.cnn import build_conv_layer, build_norm_layer\n    from mmcv.runner import auto_fp16\n    from torch import nn as nn\n    from torch.nn import functional as F\n\n    from mmdet.models.builder import (\n        TRANSFORMER,\n        build_backbone,\n        build_neck,\n        build_roi_head,\n    )\n    from mmdet.models.utils import ResLayer as ResLayer_\n\n    from .decode_head import build_decode_head\n    from .encoder import build_encoder\n    from .seg_head import build_seg_head\n    from .utils import set_pos_embed\n\n    # Set the training and testing configurations.\n    if train_cfg is not None:\n        cfg.train_cfg = train_cfg\n    if test_cfg is not None:\n        cfg.test_cfg = test_cfg\n\n    # Build the segmentor.\n    # Set the backbone.\n    backbone = build_backbone(cfg.backbone)\n    # Set the neck.\n    if cfg.get(\"neck\", None) is not None:\n        neck = build_neck(cfg.neck)\n    else:\n        neck = None\n    # Set the encoder.\n    if cfg.get(\"encoder\", None) is not None:\n        encoder = build_encoder(cfg.encoder)\n    else:\n        encoder = None\n    # Set the decoder.\n    if cfg.get(\"decoder\", None) is not None:\n        decoder = build_decode_head(cfg.decoder)\n    else:\n        decoder = None\n    # Set the segmentation head.\n    if cfg.get(\"segmentor\", None) is not None:\n        segmentor = build_seg_head(cfg.segmentor)\n    else:\n        segmentor = None\n    # Set the transformer.\n    if cfg.get(\"transformer\", None) is not None:\n        transformer = TRANSFORMER.build(cfg"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    from mmdet.models import build_detector as mmdet_build_detector\n    from mmrotate.models import build_detector as mmrotate_build_detector\n\n    assert train_cfg is None and test_cfg is None, (\n        'train_cfg and test_cfg should not be specified '\n        'in the model configuration. Please specify '\n        'train_cfg and test_cfg in the main config file.')\n\n    assert 'type' in cfg\n    assert isinstance(cfg, dict)\n    cfg_ = cfg.copy()\n\n    if 'train_cfg' in cfg_ or 'test_cfg' in cfg_:\n        assert 'type' in cfg_\n        type_ = cfg_.pop('type')\n        if 'detectors' in cfg_:\n            detector_cfg = cfg_\n        else:\n            detector_cfg = cfg_['detectors'][type_]\n\n        if 'train_cfg' in detector_cfg:\n            train_cfg = detector_cfg.pop('train_cfg')\n        if 'test_cfg' in detector_cfg:\n            test_cfg = detector_cfg.pop('test_cfg')\n\n    detector_type = cfg_.pop('type')\n\n    if detector_type in mmdet_build_detector.registry.module_dict:\n        detector = mmdet_build_detector(cfg_, train_cfg, test_cfg)\n    elif detector_type in mmrotate_build_detector.registry.module_dict:\n        detector = mmrotate_build_detector(cfg_, train_cfg, test_cfg)\n    else:\n        raise KeyError('{} is not in the registry ({}).'.format(\n            detector_type, mmdet_build_detector.registry.module_dict.keys()))\n\n    return detector"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from pcdet.utils import eval_utils\n    from pcdet.utils import object3d_kitti\n\n    def calculate_iou_partly(gt_annos, dt_annos, metric, num_parts=50):\n        \"\"\"\n        This function calculates the IoU (Intersection over Union) for the ground truth and detection annotations.\n\n        Input-Output Arguments:\n        :param gt_annos: list[dict]. Ground truth annotations containing information about the true bounding boxes and their labels.\n        :param dt_annos: list[dict]. Detection annotations containing detected bounding boxes, their labels, and scores.\n        :param metric: list[float]. A list of IoU thresholds used for computing average precisions and recalls.\n        :param num_parts: int, optional. The number of parts the evaluation is divided into.\n        :return: dict[str, float]. A dictionary containing the evaluation results, including class-wise AP and AR for each IoU threshold, as well as overall mAP and mAR.\n\n        \"\"\"\n\n        assert len(gt_annos) == len(dt_annos)\n\n        num_thresholds = len(metric)\n        num_classes = len(label2cat)\n        num_dt = len(dt_annos)\n        num_gt = len(gt_annos)\n        if num_gt == 0:\n            return {metric: 0 for metric in metric}\n\n        gt_num_cars = np.zeros(num_classes, dtype=int)\n        dt_num_cars = np.zeros(num_classes, dtype=int)\n        gt_datas_list = [gt_annos]\n        dt_datas_list = [dt_annos]\n\n        for i in range(len(gt_annos)):\n            for cls_type in label2cat:\n                gt_num_cars[cls_type] += len(gt_annos[i]['gt_boxes_lidar'])\n                dt_num_cars[cls_type] += len(dt_annos[i]['"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    # Import 'Box' class from 'box' module\n    from box import Box\n\n    # Initialize 'box_type' dictionary\n    box_type_dict = {\n        \"LiDAR\": (Box, \"LiDAR\"),\n        \"Camera\": (Box, \"Camera\"),\n        \"Depth\": (Box, \"Depth\")\n    }\n\n    # Check if 'box_type' is in 'box_type_dict'\n    if box_type in box_type_dict:\n\n        # Return the corresponding box type and mode\n        return box_type_dict[box_type]\n\n    # Raise error if 'box_type' is not in 'box_type_dict'\n    else:\n        raise ValueError(\"'box_type' should be one of 'LiDAR', 'Camera', or 'Depth'.\")"}
{"namespace": "ollama._client.Client.chat", "completion": "    # Validate input parameters\n    if not model:\n      raise RequestError('A model is required to initiate a chat.')\n    if messages is not None:\n      if not isinstance(messages, Sequence):\n        raise TypeError('Messages must be a list of Message or dict-like objects.')\n      for message in messages:\n        if not isinstance(message, Message):\n          if not isinstance(message, dict):\n            raise TypeError('Messages must be a list of Message or dict-like objects.')\n          message = Message(**message)\n        if message.role not in ['system', 'user', 'assistant']:\n          raise RequestError('Messages must have a role that is one of \"system\", \"user\", or \"assistant\".')\n        if not message.content:\n          raise RequestError('Messages must have content.')\n        if message.images:\n          message.images = [image.encode() for image in message.images]\n\n    # Create the request\n    request = {\n      'model': model,\n      'messages': messages,\n      'stream': stream,\n      'format': format,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    # Send the request\n    response = self.post('chat', request)\n\n    # Return the response\n    if stream:\n      return (ChatResponse(**response) for response in response)\n    else:\n      return ChatResponse(**response)"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = self.base_url + \"/pull\"\n\n    data = {\n      \"model\": model,\n    }\n\n    response = self.session.post(\n      url,\n      data=data,\n      verify=not insecure,\n    )\n\n    if response.status_code != 200:\n      raise ResponseError(response=response)\n\n    if stream:\n      return self._stream(\n        response=response,\n        model=model,\n      )\n\n    return self._parse(\n      response=response,\n      model=model,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('Model is required.')\n\n    if not context:\n      context = []\n\n    if not options:\n      options = Options()\n\n    if not keep_alive:\n      keep_alive = 0.0\n\n    if not images:\n      images = []\n\n    if not format:\n      format = ''\n\n    if not stream:\n      stream = False\n\n    if not raw:\n      raw = False\n\n    if not template:\n      template = ''\n\n    if not system:\n      system = ''\n\n    if not prompt:\n      prompt = ''\n\n    if not images:\n      images = []\n\n    if not options:\n      options = Options()\n\n    if not keep_alive:\n      keep_alive = 0.0\n\n    if not format:\n      format = ''\n\n    if not stream:\n      stream = False\n\n    if not raw:\n      raw = False\n\n    if not template:\n      template = ''\n\n    if not system:\n      system = ''\n\n    if not prompt:\n      prompt = ''\n\n    if not images:\n      images = []\n\n    if not options:\n      options = Options()\n\n    if not keep_alive:\n      keep_alive = 0.0\n\n    if not format:\n      format = ''\n\n    if not stream:\n      stream = False\n\n    if not raw:\n      raw = False\n\n    if not template:\n      template = ''\n\n    if not system:\n      system = ''\n\n    if not prompt:\n      prompt = ''\n\n    if not images:\n      images = []\n\n    if not options:\n      options = Options()\n\n    if not keep_alive:\n      keep_alive = 0.0\n\n    if not format:\n      format = ''\n\n    if not stream:\n      stream = False\n\n    if not raw:\n      raw = False\n\n    if not template:\n      template = ''\n\n    if not system:\n      system = ''\n\n    if not prompt:\n      prompt"}
{"namespace": "ollama._client.Client.push", "completion": "    url = self._build_url(f'/api/push/{model}')\n    response = self._request(\n      method='POST',\n      url=url,\n      insecure=insecure,\n      stream=stream,\n    )\n\n    if stream:\n      return self._stream_response(response)\n\n    return response"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either 'path' or 'modelfile' is required.\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError(\"Either 'path' or 'modelfile' is required.\")\n\n    url = self.url + \"/models\"\n    data = {\"name\": model, \"model\": modelfile}\n\n    if stream:\n      return self.stream_post(url, data)\n    else:\n      return self.post(url, data)"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Open the file in binary mode\n    with open(path, 'rb') as f:\n\n      # Calculate the SHA-256 checksum of the file\n      sha256 = hashlib.sha256()\n      sha256.update(f.read())\n      digest = 'sha256:' + sha256.hexdigest()\n\n      # Make a HEAD request to check if the blob already exists\n      response = self.head(digest)\n\n      # If the blob does not exist, upload the file\n      if response.status_code == 404:\n\n        # Upload the file\n        with open(path, 'rb') as f:\n          self.upload(digest, f)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    # Validate the model\n    if not model:\n      raise ValueError('Model is required.')\n\n    # Validate the prompt\n    if not prompt:\n      raise ValueError('Prompt is required.')\n\n    # Validate the system\n    if system:\n      system = system.lower()\n      if system not in self.systems:\n        raise ValueError(f'System \"{system}\" is not supported.')\n\n    # Validate the template\n    if template:\n      template = template.lower()\n      if template not in self.templates:\n        raise ValueError(f'Template \"{template}\" is not supported.')\n\n    # Validate the context\n    if context is not None:\n      if not isinstance(context, Sequence):\n        raise TypeError('Context must be a sequence.')\n      for i, c in enumerate(context):\n        if not isinstance(c, int):\n          raise TypeError(f'Context must be a sequence of integers, but context[{i}] is {type(c)}.')\n\n    # Validate the images\n    if images is not None:\n      if not isinstance(images, Sequence):\n        raise TypeError('Images must be a sequence.')\n      for i, image in enumerate(images):\n        if not isinstance(image, (str, bytes)):\n          raise TypeError(f'Images must be a sequence of strings or bytes, but images[{i}] is {type(image)}.')\n\n    # Validate the options\n    if options is not None:\n      if not isinstance(options, Options):\n        raise TypeError('Options must be an instance of the Options class.')\n\n    # Validate the keep-alive parameter\n    if keep_alive is not None:\n      if not isinstance(keep_alive, (float, str)):\n        raise TypeError('Keep-alive must be a float or a string.')\n\n    # Validate the format parameter\n    if format not in ['', 'json']:\n      raise ValueError('Format must be an empty string or \"json\".')\n\n    # Validate the stream parameter\n    if not isinstance(stream, bool):\n      raise TypeError('Stream must be a"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    # Create the request URL.\n    url = self.base_url + model\n\n    # Create the request headers.\n    headers = {\n      \"Authorization\": f\"Bearer {self.token}\",\n      \"Accept\": \"application/json\",\n    }\n\n    # Create the request parameters.\n    params = {\n      \"stream\": stream,\n    }\n\n    # Create the request.\n    request = Request(\n      \"GET\",\n      url,\n      headers=headers,\n      params=params,\n    )\n\n    # Send the request.\n    async with self.session.send(\n      request,\n      verify=not insecure,\n    ) as response:\n\n      # Raise an error if the request was unsuccessful.\n      response.raise_for_status()\n\n      # Return the response.\n      if stream:\n        return response.iter_lines()\n      else:\n        return await response.json()"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate the input parameters\n    if not model:\n      raise ValueError('model is required')\n    if messages is not None:\n      for message in messages:\n        if not isinstance(message, dict):\n          raise TypeError('messages must be a sequence of dictionaries')\n        if not 'role' in message:\n          raise ValueError('messages must contain a role')\n        if not 'content' in message:\n          raise ValueError('messages must contain content')\n        if not 'images' in message:\n          message['images'] = []\n        else:\n          for image in message['images']:\n            if not isinstance(image, str):\n              raise TypeError('images must be a sequence of strings')\n    if not isinstance(stream, bool):\n      raise TypeError('stream must be a boolean')\n    if format not in ['', 'json']:\n      raise ValueError('format must be an empty string or json')\n    if options is not None:\n      if not isinstance(options, Options):\n        raise TypeError('options must be an instance of Options')\n    if keep_alive is not None:\n      if not isinstance(keep_alive, (float, str)):\n        raise TypeError('keep_alive must be a float or a string')\n\n    # Create the request body\n    body = {\n      'model': model,\n      'messages': messages,\n      'format': format,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    # Make the request\n    async with self.session.post(\n      url = self.url + '/chat',\n      json = body,\n      headers = self.headers,\n    ) as response:\n\n      # Check the status code\n      if response.status == 200:\n\n        # Return the response\n        if not stream:\n          return await response.json()\n        else:\n          async for line in response.content:\n            yield json.loads(line)\n\n      else:\n\n        # Raise an error\n        raise AsyncClientError(response)"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    # Create the request URL.\n    url = self.url + \"/api/push\"\n\n    # Create the request body.\n    body = {\"model\": model}\n\n    # Create the request headers.\n    headers = self.headers\n\n    # Create the request.\n    request = Request(\n      method=\"POST\",\n      url=url,\n      headers=headers,\n      json=body,\n    )\n\n    # Send the request.\n    async with self.client.request(\n      request,\n      verify=not insecure,\n    ) as response:\n\n      # Get the response.\n      response = await response.json()\n\n      # If the stream flag is True, return a generator yielding `ProgressResponse` objects.\n      if stream:\n\n        # Create the request URL.\n        url = self.url + \"/api/push/\" + model + \"/stream\"\n\n        # Create the request headers.\n        headers = self.headers\n\n        # Create the request.\n        request = Request(\n          method=\"GET\",\n          url=url,\n          headers=headers,\n        )\n\n        # Send the request.\n        async with self.client.request(\n          request,\n          verify=not insecure,\n        ) as response:\n\n          # Yield `ProgressResponse` objects.\n          async for data in response.content.iter_chunked(1024):\n\n            # Create the response.\n            response = ProgressResponse(\n              data=data,\n              headers=response.headers,\n              status_code=response.status_code,\n            )\n\n            # Yield the response.\n            yield response\n\n      # Otherwise, return a single `ProgressResponse` object.\n      else:\n\n        # Create the response.\n        response = ProgressResponse(\n          data=response,\n          headers=response.headers,\n          status_code=response.status_code,\n        )\n\n        # Return the response.\n        return response"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file.\n    checksum = await self._calculate_checksum(path)\n\n    # Check if a blob with the calculated checksum already exists on the server.\n    response = await self._head_blob(checksum)\n\n    # If the blob does not exist on the server, upload the file in chunks.\n    if response.status_code == 404:\n\n      # Upload the file in chunks.\n      await self._upload_blob(path, checksum)\n\n    # Return the digest of the file.\n    return checksum"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary directory to store the user code and test code.\n        with tempfile.TemporaryDirectory() as tmpdirname:\n\n            # Write the user code and test code to separate files in the temporary directory.\n            with open(os.path.join(tmpdirname, \"user_code.py\"), \"w\") as f:\n                f.write(user_code)\n            with open(os.path.join(tmpdirname, \"test_code.py\"), \"w\") as f:\n                f.write(test_code)\n\n            # Run pyright on the user code and test code.\n            proc = subprocess.run(\n                [\n                    \"pyright\",\n                    \"--useInternalTypeChecker\",\n                    \"--outputjson\",\n                    \"--ignoreexternal\",\n                    \"user_code.py\",\n                    \"test_code.py\",\n                ],\n                cwd=tmpdirname,\n                capture_output=True,\n            )\n\n            # If pyright returns a non-zero exit code, the type check failed.\n            if proc.returncode != 0:\n\n                # Parse the output of pyright to identify the lines with expected type errors.\n                proc_output = proc.stdout.decode(\"utf-8\")\n                expected_type_errors = cls._parse_pyright_output(proc_output)\n\n                # Return a TypeCheckResult object with the type check result and the expected type errors.\n                return TypeCheckResult(\n                    passed=False,\n                    message=proc_output,\n                    expected_type_errors=expected_type_errors,\n                )\n\n            # If pyright returns a zero exit code, the type check passed.\n            else:\n                return TypeCheckResult(passed=True, message=\"\")"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either `path` or `modelfile` must be provided.\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        data = f.read()\n    else:\n      data = modelfile.encode()\n\n    async with self._session.post(\n      f\"{self._url}/models/{model}\",\n      data=data,\n      headers=self._headers,\n    ) as response:\n      if stream:\n        async for line in response.content:\n          yield json.loads(line)\n      else:\n        return json.loads(await response.content.read())"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    import torch\n    import torch.nn as nn\n    from aot.compile.compiler_torch import TorchCompiler\n    from aot.compile.compiler_torch import TorchCompilerOptimized\n    from aot.compile.compiler_torch import TorchCompilerOptimizedNoLoop\n    from aot.compile.compiler_torch import TorchCompilerOptimizedNoLoopNoFuse\n    from aot.compile.compiler_torch import TorchCompilerOptimizedNoLoopNoFuseNoOut\n    from aot.compile.compiler_torch import TorchCompilerOptimizedNoLoopNoFuseNoOutNoUnroll\n    from aot.compile.compiler_torch import TorchCompilerOptimizedNoLoopNoFuseNoOutNoUnrollNoParallel\n    from aot.compile.compiler_torch import TorchCompilerOptimizedNoLoopNoFuseNoOutNoUnrollNoParallelNoOutParallel\n    from aot.compile.compiler_torch import TorchCompilerOptimizedNoLoopNoFuseNoOutNoUnrollNoParallelNoOutParallelNoParallelUnroll\n    from aot.compile.compiler_torch import TorchCompilerOptimizedNoLoopNoFuseNoOutNoUnrollNoParallelNoOutParallelNoParallelUnrollNoParallelUnroll\n    from aot.compile.compiler_torch import TorchCompilerOptimizedNoLoopNoFuseNoOutNoUnrollNoParallelNoOutParallelNoParallelUnrollNoParallelUnrollNoParallelUnroll\n    from aot.compile.compiler_torch import TorchCompilerOptimizedNoLoopNoFuseNoOutNoUnrollNoParallelNoOutParallelNoParallelUnrollNoParallelUnrollNoParallelUnrollNoParallelUnroll\n    from aot.compile.compiler_torch import TorchCompilerOptimizedNoLoopNoFuseNoOutNoUnrollNoParallelNoOutParallelNoParallelUnrollNoParallelUnrollNoParallelUnrollNoParallelUnrollNoParallelUnroll\n    from aot.compile.compiler_torch import TorchCompilerOptimizedNoLoopNoFuseNoOut"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Import libraries\n    import os\n    import pandas as pd\n    import yaml\n\n    # Construct the output path\n    if output_path is not None:\n        output_path = os.path.abspath(output_path)\n\n    # Check if the trial directory exists\n    if not os.path.isdir(trial_path):\n        raise NotADirectoryError(\"The specified trial directory does not exist.\")\n\n    # Check if the trial directory contains a summary file\n    if not os.path.isfile(os.path.join(trial_path, \"summary.csv\")):\n        raise FileNotFoundError(\"The specified trial directory does not contain a summary.csv file.\")\n\n    # Check if the output path is valid\n    if output_path is not None:\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            raise ValueError(\"The output path must be a YAML file with a .yaml or .yml file extension.\")\n\n    # Read the summary file\n    summary = pd.read_csv(os.path.join(trial_path, \"summary.csv\"))\n\n    # Extract the best configuration\n    best_config = summary.iloc[summary[\"mean_test_score\"].idxmax()].to_dict()\n\n    # Remove the mean test score and std test score\n    best_config.pop(\"mean_test_score\")\n    best_config.pop(\"std_test_score\")\n\n    # Save the configuration to a YAML file\n    if output_path is not None:\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config, f)\n\n    # Return the configuration\n    return best_config"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    import inspect\n    import functools\n    import threading\n\n    # Get the tracing function and its arguments\n    if ts_compiler is None:\n        from torch.fx import symbolic_trace as ts_compiler\n    ts_args = inspect.getfullargspec(ts_compiler)[0]\n    ts_args.remove(\"root\")\n    ts_args.remove(\"graph\")\n\n    # Define the tracing function\n    def ts_func(func, *args, **kwargs):\n        # Get the tracing function arguments\n        ts_kwargs = {arg: kwargs[arg] for arg in ts_args if arg in kwargs}\n\n        # Trace the function\n        ts_graph = ts_compiler(func, args[0] if len(args) > 0 else None, **ts_kwargs)\n\n        # Return the traced function\n        return ts_graph\n\n    # Define the tracing function wrapper\n    @functools.wraps(func)\n    def ts_wrapper(*args, **kwargs):\n        # Get the tracing function arguments\n        ts_kwargs = {arg: kwargs[arg] for arg in ts_args if arg in kwargs}\n\n        # Get the traced function\n        ts_func = ts_cache.get(ts_kwargs, None)\n\n        # If the traced function is not cached, trace the function and cache it\n        if ts_func is None:\n            with ts_cache_lock:\n                ts_func = ts_cache.get(ts_kwargs, None)\n                if ts_func is None:\n                    ts_func = ts_compiler(func, args[0] if len(args) > 0 else None, **ts_kwargs)\n                    ts_cache[ts_kwargs] = ts_func\n\n        # Return the traced function\n        return ts_func(*args, **kwargs)\n\n    # Define the tracing function cache\n    ts_cache = {}\n    ts_cache_lock = threading.Lock()\n\n    # Return the tracing function wrapper\n    return ts_wrapper\n\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Extract the best configuration from the trial folder\n        best_config = cls.from_trial_folder_best_config(trial_path)\n\n        # Extract the project directory from the trial folder\n        project_directory = cls.from_trial_folder_project_directory(trial_path)\n\n        # Initialize the Runner with the best configuration and the project directory\n        return cls(best_config, project_directory)\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create a directory for the node line\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node line's results\n    node_line_results_dir = os.path.join(node_line_dir, 'results')\n    os.makedirs(node_line_results_dir, exist_ok=True)\n\n    # Create a directory for the node line's summaries\n    node_line_summaries_dir = os.path.join(node_line_dir, 'summaries')\n    os.makedirs(node_line_summaries_dir, exist_ok=True)\n\n    # Create a directory for the node line's plots\n    node_line_plots_dir = os.path.join(node_line_dir, 'plots')\n    os.makedirs(node_line_plots_dir, exist_ok=True)\n\n    # Create a directory for the node line's tables\n    node_line_tables_dir = os.path.join(node_line_dir, 'tables')\n    os.makedirs(node_line_tables_dir, exist_ok=True)\n\n    # Create a directory for the node line's visualizations\n    node_line_visualizations_dir = os.path.join(node_line_dir, 'visualizations')\n    os.makedirs(node_line_visualizations_dir, exist_ok=True)\n\n    # Create a directory for the node line's logs\n    node_line_logs_dir = os.path.join(node_line_dir, 'logs')\n    os.makedirs(node_line_logs_dir, exist_ok=True)\n\n    # Create a directory for the node line's data\n    node_line_data_dir = os.path.join(node_line_dir, 'data')\n    os.makedirs(node_line_data_dir, exist_ok=True)\n\n    # Create a directory for the node line's figures\n    node_line_figures_dir = os.path.join("}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a directory for the current node line\n    node_line_dir = os.path.join(node_line_dir, 'query_expansion')\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the current module\n    module_dir = os.path.join(node_line_dir, 'query_expansion_module')\n    os.makedirs(module_dir, exist_ok=True)\n\n    # Create a directory for the current module and strategy\n    strategy_dir = os.path.join(module_dir, 'strategy')\n    os.makedirs(strategy_dir, exist_ok=True)\n\n    # Create a directory for the current module, strategy, and metric\n    metric_dir = os.path.join(strategy_dir, 'metric')\n    os.makedirs(metric_dir, exist_ok=True)\n\n    # Create a directory for the current module, strategy, metric, and speed\n    speed_dir = os.path.join(metric_dir, 'speed')\n    os.makedirs(speed_dir, exist_ok=True)\n\n    # Create a directory for the current module, strategy, metric, speed, and threshold\n    threshold_dir = os.path.join(speed_dir, 'threshold')\n    os.makedirs(threshold_dir, exist_ok=True)\n\n    # Create a directory for the current module, strategy, metric, speed, threshold, and other\n    other_dir = os.path.join(threshold_dir, 'other')\n    os.makedirs(other_dir, exist_ok=True)\n\n    # Create a directory for the current module, strategy, metric, speed, threshold, and other, and save the input dataframe\n    input_dir = os.path.join(other_dir, 'input')\n    os.makedirs(input_dir, exist_ok=True)\n    previous_result.to_csv(os.path.join(input_dir, 'input.csv'), index=False)\n\n    # Create a directory for the current module, strategy"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker', 'results'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker', 'results', 'summary'), exist_ok=True)\n\n    # Create a list of prompt maker results\n    prompt_maker_results = []\n\n    # Iterate over the modules and parameters\n    for module, params in zip(modules, module_params):\n\n        # Create a directory for the current module\n        os.makedirs(os.path.join(node_line_dir, 'prompt_maker', module.__name__), exist_ok=True)\n\n        # Create a directory for the current module's results\n        os.makedirs(os.path.join(node_line_dir, 'prompt_maker', module.__name__, 'results'), exist_ok=True)\n\n        # Create a directory for the current module's summary\n        os.makedirs(os.path.join(node_line_dir, 'prompt_maker', module.__name__, 'results', 'summary'), exist_ok=True)\n\n        # Create a logger for the current module\n        logger = create_logger(os.path.join(node_line_dir, 'prompt_maker', module.__name__, 'results', 'summary', 'log.txt'))\n\n        # Create a dictionary for the current module's results\n        module_results = {}\n\n        # Create a dictionary for the current module's summary\n        module_summary = {}\n\n        # Create a dictionary for the current module's execution times\n        module_execution_times = {}\n\n        # Create a dictionary for the current module's evaluation metrics\n        module_evaluation_metrics = {}\n\n        # Create a dictionary for the current module's evaluation metrics\n        module_evaluation_metrics = {}\n\n        # Create a dictionary for the current module"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    # Initialize an empty list to store the values.\n    values: List[str] = []\n\n    # Iterate through the nodes.\n    for node in nodes:\n\n        # Check if the node has a module_params attribute.\n        if hasattr(node, 'module_params'):\n\n            # Check if the module_params attribute has the specified key.\n            if key in node.module_params:\n\n                # Check if the value is not already in the list.\n                if node.module_params[key] not in values:\n\n                    # Add the value to the list.\n                    values.append(node.module_params[key])\n\n    # Return the list of values.\n    return values"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    # If no embedding model is provided, use the default model\n    if not embedding_model:\n        embedding_model = get_default_model()\n\n    # Convert the ground truth strings into embeddings\n    gt_embeddings = embedding_model.encode(generation_gt)\n\n    # Convert the prediction string into an embedding\n    pred_embedding = embedding_model.encode(pred)\n\n    # Calculate the cosine similarity between the prediction and each ground truth string\n    cosine_similarities = [cosine_similarity(pred_embedding, gt_embedding) for gt_embedding in gt_embeddings]\n\n    # Return the maximum cosine similarity\n    return max(cosine_similarities)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        from GFPGAN.inference import GFPGANer\n        from GFPGAN.archs.gfpganv1 import GFPGANv1\n        from GFPGAN.archs.gfpganv1_clean import GFPGANv1_clean\n        from GFPGAN.archs.gfpganv2 import GFPGANv2\n        from GFPGAN.archs.gfpganv2_clean import GFPGANv2_clean\n        from GFPGAN.archs.gfpganv2a import GFPGANv2a\n        from GFPGAN.archs.gfpganv2a_clean import GFPGANv2a_clean\n        from GFPGAN.archs.gfpganv2b import GFPGANv2b\n        from GFPGAN.archs.gfpganv2b_clean import GFPGANv2b_clean\n        from GFPGAN.archs.gfpganv2c import GFPGANv2c\n        from GFPGAN.archs.gfpganv2c_clean import GFPGANv2c_clean\n        from GFPGAN.archs.gfpganv2d import GFPGANv2d\n        from GFPGAN.archs.gfpganv2d_clean import GFPGANv2d_clean\n        from GFPGAN.archs.gfpganv2e import GFPGANv2e\n        from GFPGAN.archs.gfpganv2e_clean import GFPGANv2e_clean\n        from GFPGAN.archs.gfpganv2f import GFPGANv2f\n        from GFPGAN.archs.gfpganv2f_clean import GFPGANv2f_clean\n        from GFPGAN.archs.gfpganv2g import GFPGANv2g\n        from GFPGAN.archs.gf"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from .FaceRestorerCodeFormer import FaceRestorerCodeFormer\n        from .FaceRestorer import FaceRestorer\n        from .utils import get_face_restorer_codeformer_instance\n        from .utils import get_face_restorer_instance\n        from .utils import get_face_restorer_codeformer_instance_from_dir\n        from .utils import get_face_restorer_instance_from_dir\n        from .utils import get_face_restorer_codeformer_instance_from_dir_name\n        from .utils import get_face_restorer_instance_from_dir_name\n        from .utils import get_face_restorer_codeformer_instance_from_dir_name_and_model_name\n        from .utils import get_face_restorer_instance_from_dir_name_and_model_name\n        from .utils import get_face_restorer_codeformer_instance_from_dir_name_and_model_name_and_model_dir\n        from .utils import get_face_restorer_instance_from_dir_name_and_model_name_and_model_dir\n        from .utils import get_face_restorer_codeformer_instance_from_dir_name_and_model_name_and_model_dir_and_model_name\n        from .utils import get_face_restorer_instance_from_dir_name_and_model_name_and_model_dir_and_model_name\n        from .utils import get_face_restorer_codeformer_instance_from_dir_name_and_model_name_and_model_dir_and_model_name_and_model_dir\n        from .utils import get_face_restorer_instance_from_dir_name_and_model_name_and_model_dir_and_model_name_and_model_dir\n        from .utils import get_face_restorer_codeformer_instance_from_dir_name_and_model_name_and_model_dir_and_model_name_and_model_dir_and_model_name\n        from .utils import get_face"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n\n        # Import the GFPGAN model\n        from facexlib.restorers.real_basicvsr import RealBasicVSR\n\n        # Patch the facexlib\n        import sys\n        sys.path.append(dirname)\n\n        # Initialize the GFPGAN face restorer\n        model = RealBasicVSR()\n\n        # Print a success message\n        print('Successfully patched the facexlib and initialized the GFPGAN face restorer.')\n\n    except Exception as e:\n\n        # Print an error message\n        print('Failed to patch the facexlib and initialize the GFPGAN face restorer.')\n        print(e)"}
{"namespace": "quaternion.rotate", "completion": "  # Import the required libraries\n  import numpy as np\n\n  # Convert the vector to a quaternion\n  v_q = np.append(np.zeros(3), 0)\n  v_q[0:3] = v\n\n  # Rotate the quaternion\n  v_q = q * v_q * q.conj()\n\n  # Convert the quaternion back to a vector\n  v_rot = v_q[0:3]\n\n  # Return the rotated vector\n  return v_rot"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Axis-angle to quaternion\n  axis_angle = jnp.asarray(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (angle + eps)\n  s = jnp.sin(angle)\n  c = jnp.cos(angle)\n  q = jnp.array([c, axis[0] * s, axis[1] * s, axis[2] * s])\n\n  return q\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk = model.get_topk(prefix, k)\n\n    # get topk indices\n    topk_indices = [x[0] for x in topk]\n\n    # get topk words\n    topk_words = [model.get_word(x) for x in topk_indices]\n\n    # get topk probabilities\n    topk_probs = [x[1] for x in topk]\n\n    # get topk log probabilities\n    topk_logprobs = [np.log(x) for x in topk_probs]\n\n    # get topk log probabilities\n    topk_logprobs = [np.log(x) for x in topk_probs]\n\n    # get topk log probabilities\n    topk_logprobs = [np.log(x) for x in topk_probs]\n\n    # get topk log probabilities\n    topk_logprobs = [np.log(x) for x in topk_probs]\n\n    # get topk log probabilities\n    topk_logprobs = [np.log(x) for x in topk_probs]\n\n    # get topk log probabilities\n    topk_logprobs = [np.log(x) for x in topk_probs]\n\n    # get topk log probabilities\n    topk_logprobs = [np.log(x) for x in topk_probs]\n\n    # get topk log probabilities\n    topk_logprobs = [np.log(x) for x in topk_probs]\n\n    # get topk log probabilities\n    topk_logprobs = [np.log(x) for x in topk_probs]\n\n    # get topk log probabilities\n    topk_logprobs = [np.log(x) for x in topk_probs]\n\n    # get topk log probabilities\n    topk_logprobs = [np.log(x) for x in topk_probs]\n\n    #"}
{"namespace": "resample.resample_3d", "completion": "  import tensorflow as tf\n  import numpy as np\n\n  # Check if the input data is a tensor\n  if not isinstance(data, tf.Tensor):\n    raise Exception('The input data must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise Exception('The input locations must be a tensor.')\n\n  # Check if the input edge behavior is a string\n  if not isinstance(edge_behavior, str):\n    raise Exception('The input edge behavior must be a string.')\n\n  # Check if the input constant values is a float\n  if not isinstance(constant_values, float):\n    raise Exception('The input constant values must be a float.')\n\n  # Check if the input coordinate order is a string\n  if not isinstance(coordinate_order, str):\n    raise Exception('The input coordinate order must be a string.')\n\n  # Check if the input method is a string\n  if not isinstance(method, str):\n    raise Exception('The input method must be a string.')\n\n  # Check if the input half pixel center is a bool\n  if not isinstance(half_pixel_center, bool):\n    raise Exception('The input half pixel center must be a bool.')\n\n  # Check if the input data is a 4D tensor\n  if len(data.shape) != 4:\n    raise Exception('The input data must be a 4D tensor.')\n\n  # Check if the input locations is a 4D tensor\n  if len(locations.shape) < 3 or len(locations.shape) > 4:\n    raise Exception('The input locations must be a 3D or 4D tensor.')\n\n  # Check if the input edge behavior is valid\n  if edge_behavior != 'CONSTANT_OUTSIDE' and edge_behavior != 'CLAMP':\n    raise Exception('The input edge behavior is invalid.')\n\n  # Check if the input coordinate order is valid\n  if coordinate_order != 'xyz' and coordinate_order != 'zyx':\n    raise Exception('The input coordinate order is"}
{"namespace": "math.plus_eps", "completion": "  # The threshold value is set to 1e-10.\n  eps = 1e-10\n\n  # If x is smaller than the threshold value, return the threshold value.\n  if x < eps:\n    return eps\n\n  # If x is larger than the threshold value, return the next representable floating-point value towards positive infinity.\n  return x + (x - np.nextafter(x, np.inf))\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n\n"}
{"namespace": "math.safe_exp", "completion": "  # import the required libraries\n  import tensorflow as tf\n\n  # define the helper function\n  def exp_helper(x):\n\n    # check if the input is greater than 0\n    if x > 0:\n\n      # return the exponential function\n      return tf.math.exp(x)\n\n    # otherwise, return 0\n    else:\n\n      # return 0\n      return 0\n\n  # apply the custom gradient function\n  with tf.name_scope('safe_exp') as scope:\n\n    # return the safe exponential function\n    return tf.math.exp(tf.clip_by_value(x, -10, 10), name=scope)"}
{"namespace": "math.safe_log", "completion": "  # Import functions from jax.numpy\n  import jax.numpy as jnp\n\n  # Define the safe logarithm function\n  safe_log = generate_safe_fn(jnp.log)\n\n  # Return the result of applying the safe logarithm function to x\n  return safe_log(x)\n"}
{"namespace": "math.safe_sqrt", "completion": "  # Define the helper function for the safe square root function\n  def sqrt_grad(x):\n\n    \"\"\"\n    This function computes the gradient of the square root function. It is used as the gradient function for the safe square root function.\n\n    Input-Output Arguments:\n    :param x: The input value for which the gradient of the square root function will be computed.\n    :return: The gradient of the square root function evaluated at the input x.\n    \"\"\"\n\n    # Compute the gradient of the square root function\n    grad = 1 / (2 * np.sqrt(x))\n\n    return grad\n\n  # Define the safe square root function\n  def safe_sqrt_func(x):\n\n    \"\"\"\n    This function computes the safe square root function. It is used as the main function for the safe square root function.\n\n    Input-Output Arguments:\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n    :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n    \"\"\"\n\n    # Compute the square root of the input\n    sqrt_x = np.sqrt(x)\n\n    # Clamp the input between 0 and a maximum value\n    x_clamped = np.clip(x, 0, 1e+10)\n\n    # Compute the gradient of the square root function\n    grad = sqrt_grad(x_clamped)\n\n    # Return the result of the safe square root function\n    return sqrt_x, grad\n\n  # Return the result of the safe square root function\n  return safe_sqrt_func(x)\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 1\n  elif p == 1:\n    return 1\n  elif p == 2:\n    return 4\n  elif p == 3:\n    return 9\n  elif p == 4:\n    return 16\n  elif p == 5:\n    return 25\n  elif p == 6:\n    return 36\n  elif p == 7:\n    return 49\n  elif p == 8:\n    return 64\n  elif p == 9:\n    return 81\n  elif p == 10:\n    return 100\n  else:\n    return \"Invalid input\"\n\n"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  from scipy.spatial import Delaunay\n  from scipy.spatial.qhull import QhullError\n  from itertools import combinations\n\n  # Check if the input parameters are valid\n  if base_shape not in [\"tetrahedron\", \"icosahedron\", \"octahedron\"]:\n    raise ValueError(\n        \"The specified base shape is not supported. Please choose a base shape from 'tetrahedron', 'icosahedron', or 'octahedron'.\"\n    )\n\n  if angular_tesselation < 1:\n    raise ValueError(\n        \"The angular tesselation must be a positive integer. Please choose a positive integer for the angular tesselation.\"\n    )\n\n  if eps <= 0:\n    raise ValueError(\n        \"The tolerance must be a positive number. Please choose a positive number for the tolerance.\"\n    )\n\n  # Generate the initial basis\n  if base_shape == \"tetrahedron\":\n    vertices = np.array(\n        [\n            [1, 1, 1],\n            [-1, -1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n        ]\n    )\n  elif base_shape == \"icosahedron\":\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = np.array(\n        [\n            [phi, 1, 0],\n            [phi, -1, 0],\n            [-phi, 1, 0],\n            [-phi, -1, 0],\n            [1, 0, phi],\n            [1, 0, -phi],\n            [-1, 0, phi],\n            [-1, 0, -phi],\n            [0, phi, 1],\n            [0, phi, -1],\n            [0, -phi, 1],\n            [0, -phi, -1],\n        ]\n    )\n  elif base_shape == \"octahedron\":\n    vertices = np.array("}
{"namespace": "math.safe_log1p", "completion": "  # If the input value is less than -1, set it to -1.\n  if x < -1:\n    x = -1\n\n  # If the input value is greater than 1, set it to 1.\n  if x > 1:\n    x = 1\n\n  # If the input value is between -1 and 1, return the natural logarithm of 1 plus the input value.\n  return np.log1p(x)\n"}
{"namespace": "math.power_ladder", "completion": "  # Handle pre-multiplication\n  if premult is not None:\n    x = x * premult\n\n  # Handle special cases\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = np.log(x)\n  elif p == -np.inf:\n    y = np.log(x) / p\n  elif p == np.inf:\n    y = np.sign(x) * np.sqrt(np.abs(x))\n\n  # Handle general case\n  else:\n    y = np.sign(x) * np.abs(x) ** p\n\n  # Handle post-multiplication\n  if postmult is not None:\n    y = y * postmult\n\n  return y\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 1:\n    return y\n  elif p == 2:\n    return y ** (1 / 2)\n  elif p == 3:\n    return y ** (1 / 3)\n  elif p == 4:\n    return y ** (1 / 4)\n  elif p == 5:\n    return y ** (1 / 5)\n  elif p == 6:\n    return y ** (1 / 6)\n  elif p == 7:\n    return y ** (1 / 7)\n  elif p == 8:\n    return y ** (1 / 8)\n  elif p == 9:\n    return y ** (1 / 9)\n  elif p == 10:\n    return y ** (1 / 10)\n  elif p == 11:\n    return y ** (1 / 11)\n  elif p == 12:\n    return y ** (1 / 12)\n  elif p == 13:\n    return y ** (1 / 13)\n  elif p == 14:\n    return y ** (1 / 14)\n  elif p == 15:\n    return y ** (1 / 15)\n  elif p == 16:\n    return y ** (1 / 16)\n  elif p == 17:\n    return y ** (1 / 17)\n  elif p == 18:\n    return y ** (1 / 18)\n  elif p == 19:\n    return y ** (1 / 19)\n  elif p == 20:\n    return y ** (1 / 20)\n  elif p == 21:\n    return y ** (1 / 21)\n  elif p == 22:\n    return y ** (1 / 22)\n  elif p == 23:\n    return y ** (1 / 23)\n  elif p == 24:\n    return y ** (1 / 24)\n  elif p == 25:\n    return y ** (1 / 25"}
{"namespace": "math.learning_rate_decay", "completion": "  # If a delay is applied, scale the initial learning rate down by a multiplier\n  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n\n  # If the current step is less than the delay steps, return the scaled initial learning rate\n  if step < lr_delay_steps:\n    return lr_init\n\n  # Otherwise, calculate the learning rate based on the progression of steps\n  else:\n    return lr_init * (lr_final / lr_init) ** (step / max_steps)"}
{"namespace": "utils.dummy_rays", "completion": "  from .generate_random_rays import generate_random_rays\n\n  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert to numpy arrays if needed\n  points = xnp.asarray(points)\n  pixtocams = xnp.asarray(pixtocams)\n  camtoworlds = xnp.asarray(camtoworlds)\n\n  # Convert to jax arrays if needed\n  if xnp == jnp:\n    points = jnp.asarray(points)\n    pixtocams = jnp.asarray(pixtocams)\n    camtoworlds = jnp.asarray(camtoworlds)\n\n  # Check if the input is valid\n  if points.ndim < 2:\n    raise ValueError(\"points must be at least a 2D array\")\n  if pixtocams.ndim < 3:\n    raise ValueError(\"pixtocams must be at least a 3D array\")\n  if camtoworlds.ndim < 3:\n    raise ValueError(\"camtoworlds must be at least a 3D array\")\n  if points.shape[-1] != 3:\n    raise ValueError(\"points must be a Nx3 array\")\n  if pixtocams.shape[-2:] != (3, 3):\n    raise ValueError(\"pixtocams must be a Nx3x3 array\")\n  if camtoworlds.shape[-2:] != (3, 4):\n    raise ValueError(\"camtoworlds must be a Nx3x4 array\")\n  if points.shape[:-2] != pixtocams.shape[:-2]:\n    raise ValueError(\"points and pixtocams must have the same leading dimensions\")\n  if points.shape[:-2] != camtoworlds.shape[:-2]:\n    raise ValueError(\"points and camtoworlds must have the same leading dimensions\")\n\n  # Convert points to homogeneous coordinates\n  points = xnp.concatenate([points, xnp.ones_like(points[..., :1])], axis=-1)\n\n  # Convert points from world coordinates to camera coordinates\n  points = xnp.einsum(\"...ij,...kj->"}
{"namespace": "rigid_body.exp_se3", "completion": "  # The screw axis is encoded as a 6-vector [w, v]\n  w = screw_axis[0:3]\n  v = screw_axis[3:6]\n\n  # The magnitude of the rotation is the norm of the angle-axis vector\n  theta = jnp.linalg.norm(w)\n\n  # If the magnitude of the rotation is small, we can use the small angle approximation\n  if theta < eps:\n    # The rotation matrix is the identity matrix\n    R = jnp.eye(3)\n\n    # The translation is the translation vector\n    t = v\n\n    # The exponential map is the homogeneous transformation matrix\n    exp_se3 = jnp.block([[R, t], [0, 0, 0, 1]])\n\n  else:\n    # The rotation matrix is the Rodrigues formula\n    R = jnp.eye(3) + jnp.sin(theta) * jnp.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]]) + (1 - jnp.cos(theta)) * jnp.array([[w[0] * w[0], w[0] * w[1], w[0] * w[2]], [w[1] * w[0], w[1] * w[1], w[1] * w[2]], [w[2] * w[0], w[2] * w[1], w[2] * w[2]]])\n\n    # The translation is the translation vector\n    t = (1 - jnp.cos(theta)) * jnp.array([w[0] * v[0] + w[1] * v[1] + w[2] * v[2], w[0] * v[1] - w[1] * v[0] + w[2] * v[0], -w[0] * v[0] + w[1] * v[1] - w[2] * v[2]]) + jnp.sin(theta) * jnp"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Avoid division by zero or numerical instability\n  axis_angle = axis_angle + eps\n\n  # Compute the rotation matrix\n  R = jnp.array(\n    [\n      [\n        jnp.cos(axis_angle[2])\n        + jnp.sin(axis_angle[2]) * axis_angle[0] * axis_angle[0]\n        - jnp.sin(axis_angle[2]) * axis_angle[1] * axis_angle[1],\n        -jnp.sin(axis_angle[2]) * axis_angle[0],\n        jnp.sin(axis_angle[2]) * axis_angle[1],\n      ],\n      [\n        jnp.sin(axis_angle[2]) * axis_angle[0],\n        jnp.cos(axis_angle[2])\n        + jnp.sin(axis_angle[2]) * axis_angle[1] * axis_angle[1]\n        - jnp.sin(axis_angle[2]) * axis_angle[0] * axis_angle[0],\n        -jnp.sin(axis_angle[2]) * axis_angle[1],\n      ],\n      [\n        -jnp.sin(axis_angle[2]) * axis_angle[1],\n        jnp.sin(axis_angle[2]) * axis_angle[0],\n        jnp.cos(axis_angle[2])\n        + jnp.sin(axis_angle[2]) * axis_angle[0] * axis_angle[0]\n        - jnp.sin(axis_angle[2]) * axis_angle[1] * axis_angle[1],\n      ],\n    ]\n  )\n\n  return R\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Import libraries\n  import jax.numpy as jnp\n  import math\n\n  # Define helper functions\n  def get_mean(t):\n    return t * d / jnp.linalg.norm(d)\n\n  def get_covariance(t):\n    if diag:\n      return jnp.diag(jnp.array([base_radius, base_radius, base_radius]))\n    else:\n      return jnp.eye(3) * base_radius\n\n  # Calculate the mean and covariance\n  mean = jnp.array([get_mean(t0), get_mean(t1), 0.])\n  covariance = jnp.array([[get_covariance(t0), 0., 0.], [0., get_covariance(t1), 0.], [0., 0., 0.]])\n\n  return mean, covariance\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # Calculate the mean of the Gaussian\n  mean = d * (t1 - t0) / 2\n\n  # Calculate the variance of the Gaussian\n  var = jnp.square(radius)\n\n  # Calculate the covariance of the Gaussian\n  if diag:\n    cov = jnp.diag(jnp.array([var, var, var]))\n  else:\n    cov = jnp.eye(3) * var\n\n  # Lift the Gaussian to the correct dimensionality\n  return lift_gaussian(mean, cov, d)\n\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Compute ray origins and directions in camera coordinates.\n  cam_x_int = xnp.einsum(\"sh,...ij->...si\", pixtocams, xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], axis=-1))\n  cam_x_int = xnp.where(xnp.isfinite(cam_x_int), cam_x_int, xnp.zeros_like(cam_x_int))\n  cam_x_int = xnp.where(xnp.abs(cam_x_int) > 1e6, xnp.zeros_like(cam_x_int), cam_x_int)\n  cam_x_int = xnp.where(xnp.abs(cam_x_int) < 1e-6, xnp.zeros_like(cam_x_int), cam_x_int)\n  cam_x_int = xnp.where(xnp.isfinite(cam_x_int), cam_x_int, xnp.zeros_like(cam_x_int))\n  cam_x_int = xnp.where(xnp.abs(cam_x_int) > 1e6, xnp.zeros_like(cam_x_int), cam_x_int)\n  cam_x_int = xnp.where(xnp.abs(cam_x_int) < 1e-6, xnp.zeros_like(cam_x_int), cam_x_int)\n  cam_x_int = xnp.where(xnp.isfinite(cam_x_int), cam_x_int, xnp.zeros_like(cam_x_int))\n  cam_x_int = xnp.where(xnp.abs(cam_x_int) > 1e6, xnp.zeros_like(cam_x_int), cam_x_int)\n  cam_x_int = xnp.where(xnp.abs(cam_x_int) < 1e-6"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the product of the density and the adjusted distance between points\n  alpha_weights = density * tdist\n\n  # Compute the alpha weights using the helper function\n  alpha_weights = compute_alpha_weights_helper(alpha_weights, **kwargs)\n\n  return alpha_weights\n\n"}
{"namespace": "stepfun.sample", "completion": "  # Compute the bin widths\n  dt = jnp.diff(t)\n\n  # Compute the normalized weights\n  w = jax.nn.softmax(w_logits)\n\n  # Compute the cumulative weights\n  w_cumsum = jnp.cumsum(w)\n\n  # Compute the normalized cumulative weights\n  w_cumsum_norm = w_cumsum / w_cumsum[-1]\n\n  # Compute the normalized bin widths\n  dt_norm = dt / dt[-1]\n\n  # Compute the normalized bin endpoints\n  t_norm = t / t[-1]\n\n  # Compute the normalized sample locations\n  if rng is None:\n    # Use deterministic sampling\n    u = jnp.linspace(0, 1, num_samples, dtype=jnp.float32)\n  else:\n    # Use random sampling\n    if single_jitter:\n      # Use the same jitter for all samples\n      u = jax.random.uniform(rng, (num_samples,), minval=eps, maxval=1.0 - eps)\n    else:\n      # Use different jitter for each sample\n      u = jax.random.uniform(rng, (num_samples,), minval=0, maxval=1.0)\n\n  # Compute the normalized sample locations\n  u = u.reshape(-1, 1)\n\n  # Compute the normalized sample locations\n  if deterministic_center:\n    # Center the samples in each interval\n    u = u - 0.5 * (w_cumsum_norm[1:] - w_cumsum_norm[:-1])\n\n  # Compute the sample locations\n  x = jnp.sum(u * dt_norm * w_cumsum_norm[:-1], axis=0)\n\n  # Compute the sample locations\n  x = x * t[-1]\n\n  # Compute the sample locations\n  x = x + t[0]\n\n  # Compute the sample locations\n  x ="}
{"namespace": "stepfun.sample_intervals", "completion": "  # If no random number generator is provided, use linspace sampling\n  if rng is None:\n    t_samples = jnp.linspace(t[0], t[-1], num_samples)\n\n  # Otherwise, use inverse CDF sampling\n  else:\n    # Calculate the CDF\n    cdf = jnp.cumsum(jnp.exp(w_logits))\n\n    # Normalize the CDF\n    cdf = cdf / cdf[-1]\n\n    # Sample from the CDF\n    t_samples = jnp.interp(\n        rng.uniform(\n            low=0.0,\n            high=1.0,\n            size=(num_samples,),\n        ),\n        cdf,\n        t,\n    )\n\n  # Calculate the midpoints between adjacent samples\n  t_midpoints = (t_samples[:-1] + t_samples[1:]) / 2\n\n  # Calculate the widths of the intervals\n  t_widths = t_samples[1:] - t_samples[:-1]\n\n  # Calculate the left and right edges of the intervals\n  t_left = t_samples[:-1]\n  t_right = t_samples[1:]\n\n  # If single jitter is specified, jitter all samples by the same amount\n  if single_jitter:\n    t_left = t_left + rng.uniform(\n        low=-t_widths / 2,\n        high=t_widths / 2,\n        size=(num_samples,),\n    )\n    t_right = t_right + rng.uniform(\n        low=-t_widths / 2,\n        high=t_widths / 2,\n        size=(num_samples,),\n    )\n\n  # Otherwise, jitter each sample independently\n  else:\n    t_left = t_left + rng.uniform(\n        low=-t_widths / 2,\n        high=t_widths / 2,\n        size=(num_samples,),\n    )\n    t_right = t_right + rng"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  w = w / np.sum(w)\n\n  # Compute the cumulative weights\n  wc = np.cumsum(w)\n\n  # Interpolate into the cumulative weights\n  wc_interp = np.interp(ps, wc, t)\n\n  return wc_interp\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Convert the histogram to a PDF\n  pdf = w / np.sum(w)\n\n  # Blur the PDF\n  pdf_blurred = np.zeros(len(pdf))\n  for i in range(len(pdf)):\n    pdf_blurred[i] = np.sum(pdf[max(0, i - blur_halfwidth):min(len(pdf), i + blur_halfwidth + 1)])\n  pdf_blurred /= np.sum(pdf_blurred)\n\n  # Resample the blurred PDF to match the new time points\n  f = interp1d(t, pdf_blurred, kind='linear', fill_value='extrapolate')\n  return f(tq)"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  import numpy as np\n\n  # check if the input is a numpy array\n  if not isinstance(vectors, np.ndarray):\n    raise RuntimeError('The input data must be a numpy array.')\n\n  # check if the input is a numpy array\n  if not isinstance(transform, np.ndarray):\n    raise RuntimeError('The input data must be a numpy array.')\n\n  # check if the input is a numpy array\n  if not isinstance(transform, np.ndarray):\n    raise RuntimeError('The input data must be a numpy array.')\n\n  # check if the input is a numpy array\n  if not isinstance(transform, np.ndarray):\n    raise RuntimeError('The input data must be a numpy array.')\n\n  # check if the input is a numpy array\n  if not isinstance(transform, np.ndarray):\n    raise RuntimeError('The input data must be a numpy array.')\n\n  # check if the input is a numpy array\n  if not isinstance(transform, np.ndarray):\n    raise RuntimeError('The input data must be a numpy array.')\n\n  # check if the input is a numpy array\n  if not isinstance(transform, np.ndarray):\n    raise RuntimeError('The input data must be a numpy array.')\n\n  # check if the input is a numpy array\n  if not isinstance(transform, np.ndarray):\n    raise RuntimeError('The input data must be a numpy array.')\n\n  # check if the input is a numpy array\n  if not isinstance(transform, np.ndarray):\n    raise RuntimeError('The input data must be a numpy array.')\n\n  # check if the input is a numpy array\n  if not isinstance(transform, np.ndarray):\n    raise RuntimeError('The input data must be a numpy array.')\n\n  # check if the input is a numpy array\n  if not isinstance(transform, np.ndarray):\n    raise RuntimeError('The input data must be a numpy array.')\n\n  # check if the input is a numpy array\n  if not isinstance(transform, np.ndarray):\n    raise Runtime"}
{"namespace": "stepfun.resample", "completion": "  # Resample the step function into new intervals\n  if use_avg:\n    # Compute the width of each interval in t\n    dt = t[1:] - t[:-1]\n    # Compute the average value of the step function in each interval\n    v = (vp[:-1] + vp[1:]) / 2\n    # Resample the step function into new intervals\n    v = v * dt\n  else:\n    # Resample the step function into new intervals\n    v = vp[:-1]\n\n  return v"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # scale the mean and variance\n  scaled_mean = mean * (2 ** min_deg)\n  scaled_var = var * (2 ** min_deg)\n\n  # concatenate the scaled mean and variance\n  scaled_coords = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # get the number of dimensions\n  num_dims = scaled_coords.shape[-1]\n\n  # get the number of degrees\n  num_degs = max_deg - min_deg\n\n  # get the degrees\n  degrees = jnp.arange(num_degs)\n\n  # get the frequencies\n  freqs = 1. / (2 ** degrees)\n\n  # get the frequencies and degrees\n  freqs, degrees = jnp.meshgrid(freqs, degrees)\n\n  # get the frequencies and degrees\n  freqs = freqs.flatten()\n  degrees = degrees.flatten()\n\n  # get the scaled coordinates\n  scaled_coords = scaled_coords.reshape(-1, num_dims)\n\n  # get the encoded coordinates\n  encoded_coords = jnp.concatenate([scaled_coords[:, :, None] * freqs[None, :, None], jnp.sin(scaled_coords[:, :, None] * freqs[None, :, None] + degrees[None, :, None]), jnp.cos(scaled_coords[:, :, None] * freqs[None, :, None] + degrees[None, :, None])], axis=-1)\n\n  # reshape the encoded coordinates\n  encoded_coords = encoded_coords.reshape(scaled_coords.shape[0], -1)\n\n  return encoded_coords\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Import libraries\n  import numpy as np\n  from scipy.special import sph_harm\n\n  # Generate the directional encoding function\n  def dir_enc_fn(points):\n\n    \"\"\"\n    Evaluates the directional encoding for the given input points.\n\n    Input-Output Arguments:\n    :param points: ndarray. The input points to be encoded.\n    :return: ndarray. The directional encoding of the input points.\n\n    \"\"\"\n\n    # Import libraries\n    import numpy as np\n\n    # Generate the directional encoding\n    dir_enc = np.zeros((points.shape[0], deg_view * deg_view))\n\n    # Iterate over the points\n    for i in range(points.shape[0]):\n\n      # Iterate over the spherical harmonics degrees\n      for l in range(deg_view):\n\n        # Iterate over the spherical harmonics orders\n        for m in range(-l, l + 1):\n\n          # Evaluate the spherical harmonics\n          dir_enc[i, l * deg_view + m + deg_view] = sph_harm(m, l, points[i, 0], points[i, 1])\n\n    # Return the directional encoding\n    return dir_enc\n\n  # Return the directional encoding function\n  return dir_enc_fn"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize the result list\n    result = []\n\n    # Initialize the block index\n    block_index = 0\n\n    # Initialize the header block index\n    header_index = 0\n\n    # Initialize the list block index\n    list_index = 0\n\n    # Initialize the list level\n    list_level = 0\n\n    # Initialize the list type\n    list_type = None\n\n    # Initialize the list block\n    list_block = []\n\n    # Initialize the list block index\n    list_block_index = 0\n\n    # Initialize the list block type\n    list_block_type = None\n\n    # Initialize the list block text\n    list_block_text = None\n\n    # Initialize the list block index\n    list_block_index = 0\n\n    # Initialize the list block type\n    list_block_type = None\n\n    # Initialize the list block text\n    list_block_text = None\n\n    # Initialize the list block index\n    list_block_index = 0\n\n    # Initialize the list block type\n    list_block_type = None\n\n    # Initialize the list block text\n    list_block_text = None\n\n    # Initialize the list block index\n    list_block_index = 0\n\n    # Initialize the list block type\n    list_block_type = None\n\n    # Initialize the list block text\n    list_block_text = None\n\n    # Initialize the list block index\n    list_block_index = 0\n\n    # Initialize the list block type\n    list_block_type = None\n\n    # Initialize the list block text\n    list_block_text = None\n\n    # Initialize the list block index\n    list_block_index = 0\n\n    # Initialize the list block type\n    list_block_type = None\n\n    # Initialize the list block text\n    list_block_text = None\n\n    # Initialize the list block index\n    list_block_index = 0\n\n    # Initialize the list block type\n    list_block_type = None\n\n    # Initialize the list block text\n    list_block_text = None\n\n    # Initialize the"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # If the input text is empty or None, return the input as is\n    if not org_texts:\n        return org_texts\n\n    # Normalize the quotation marks\n    text = org_texts.replace(\"\u201c\", \"\\\"\").replace(\"\u201d\", \"\\\"\")\n\n    # Apply the space rule to handle the punctuation at the beginning of the text\n    text = space_rule.sub(\" \", text)\n\n    # Apply the bracket rule to handle the sentences within brackets\n    text = bracket_rule.sub(\"\", text)\n\n    # Apply the rules to handle the paragraphs separated by new lines\n    text = rules.sub(\"\\n\\n\", text)\n\n    # Tokenize the text into sentences\n    sentences = nltk_tokenizer.tokenize(text)\n\n    # Return the tokenized sentences\n    return sentences"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        # If a key is provided, search only that document\n        if key is not None:\n            if key in self.documents.keys():\n                return self.documents[key].positions(token)\n            else:\n                raise KeyError(f\"The key {key} is not in the SearchArray.\")\n\n        # If no key is provided, search all documents\n        else:\n            positions = []\n            for document in self.documents.values():\n                positions.append(document.positions(token))\n            return positions"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # If the 'min should match' specification is an absolute number, return it.\n    if spec.isdigit():\n        return int(spec)\n\n    # If the 'min should match' specification is a percentage, calculate the minimum number of clauses that must match.\n    if spec.endswith('%'):\n        return int(num_clauses * float(spec[:-1]) / 100)\n\n    # If the 'min should match' specification is a conditional expression, calculate the minimum number of clauses that must match.\n    if spec.startswith('<'):\n        return int(num_clauses - float(spec[1:]))\n\n    # If the 'min should match' specification is an invalid expression, raise an exception.\n    raise ValueError('Invalid min should match specification.')"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If the slop is 1 and all tokens are unique, calculate the phrase frequencies directly.\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_adjacent(tokens)\n\n        # Otherwise, calculate the phrase frequencies using the positions of terms.\n        else:\n            return self.phrase_freq_positions(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Initialize the SearchArray instance\n        search_array = cls()\n\n        # Initialize the tokenizer\n        if tokenizer is None:\n            tokenizer = ws_tokenizer\n\n        # Initialize the batch size\n        if batch_size is None:\n            batch_size = 100000\n\n        # Initialize the avoid copies flag\n        if avoid_copies is None:\n            avoid_copies = True\n\n        # Initialize the truncate flag\n        if truncate is None:\n            truncate = False\n\n        # Initialize the term dictionary\n        term_dict = {}\n\n        # Initialize the document lengths\n        doc_lengths = []\n\n        # Initialize the term matrix\n        term_matrix = []\n\n        # Initialize the positions\n        positions = []\n\n        # Initialize the total number of tokens\n        total_tokens = 0\n\n        # Initialize the total number of documents\n        total_docs = 0\n\n        # Initialize the total number of terms\n        total_terms = 0\n\n        # Initialize the total number of positions\n        total_positions = 0\n\n        # Initialize the total number of documents\n        total_docs = 0\n\n        # Initialize the total number of terms\n        total_terms = 0\n\n        # Initialize the total number of positions\n        total_positions = 0\n\n        # Initialize the total number of tokens\n        total_tokens = 0\n\n        # Initialize the total number of documents\n        total_docs = 0\n\n        # Initialize the total number of terms\n        total_terms = 0\n\n        # Initialize the total number of positions\n        total_positions = 0\n\n        # Initialize the total number of tokens\n        total_tokens = 0\n\n        # Initialize the total number of documents\n        total_docs = 0\n\n        # Initialize the total number of terms\n        total_terms = 0\n\n        # Initialize the total number of positions\n        total_positions = 0\n\n        # Initialize the total number of tokens\n        total_tokens = 0\n\n        # Initialize the total number of documents\n        total_docs = 0"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.server.bind((self.config.get(\"server_ip\"), self.config.get(\"server_port\")))\n        self.server.listen(1)\n\n        self.logger.info(\"Server started on port \" + str(self.config.get(\"server_port\")))\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Iterate through each element in the array.\n    for i in range(len(arr)):\n        # Iterate through each bit in the current element.\n        for j in range(64):\n            # If the current bit is set to 1, increment the bit count.\n            if arr[i] & (1 << j):\n                arr[i] += 1\n\n    return arr\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Initialize the searcher\n    searcher = Searcher(frame, similarity)\n\n    # Parse the query\n    query = QueryParser(q, qf, q_op, mm).parse(q)\n\n    # Perform the search\n    results = searcher.search(query)\n\n    # Initialize the explanation\n    explanation = \"The search was performed using the Extended Disjunction Max Query (edismax) approach. The query was parsed as follows:\\n\"\n    explanation += query.explain()\n    explanation += \"\\nThe searcher was initialized as follows:\\n\"\n    explanation += searcher.explain()\n    explanation += \"\\nThe search was performed as follows:\\n\"\n    explanation += results.explain()\n\n    return results.scores, explanation"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections managed by the ProxifierMessageInterceptor instance\n        for connection in self.connections:\n            connection.stop()\n\n        # Stop the server associated with the ProxifierMessageInterceptor instance\n        if self.server is not None:\n            self.server.stop()\n\n        # Reset the ProxifierMessageInterceptor instance's 'connections' and 'server' attributes\n        self.connections = []\n        self.server = None"}
