{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "coord.inv_contract", "completion": "  return z/np.linalg.norm(z)\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n\n        \"\"\"\n        This is the actual decorator function that wraps the original function. It takes the original function as an argument and returns a new function that wraps the original function and adds memoization functionality to it.\n\n        Input-Output Arguments\n        :param func: The original function to be wrapped and memoized.\n        :return: A new function that wraps the original function and adds memoization functionality to it.\n        \"\"\"\n\n        def wrapper(*args, **kwargs):\n\n            \"\"\"\n            This is the wrapper function that wraps the original function and adds memoization functionality to it. It takes the original function's arguments and keyword arguments and checks if the result of the function call is already stored in the SQLite database. If so, it retrieves the result from the database, otherwise, it computes the result, stores it, and then returns it.\n\n            Input-Output Arguments\n            :param args: The arguments to be passed to the original function.\n            :param kwargs: The keyword arguments to be passed to the original function.\n            :return: The result of the function call, either retrieved from the database or computed and stored in the database.\n            \"\"\"\n\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create the table if it doesn't exist\n            c.execute(f\"CREATE TABLE IF NOT EXISTS {func_name} (args TEXT, kwargs TEXT, result TEXT)\")\n\n            # Check if the result is already stored in the database\n            c.execute(f\"SELECT result FROM {func_name} WHERE args = ? AND kwargs = ?\", (str(args), str(kwargs)))\n            result = c.fetchone()\n\n            # If the result is not already stored in the database, compute it and store it in the database\n            if result is None:\n                result = func(*args, **kwargs)\n                c.execute(f\"INSERT INTO {func_name} VALUES (?, ?, ?)\", (str(args), str(kwargs), str(result)))\n                conn.commit()\n\n            # Close the connection to the SQLite database\n            conn.close()\n\n            # Return the result\n            return result\n\n        # Return the wrapper function\n        return wrapper\n\n    # Return the decorator function\n    return decorator\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__} x_min must be less than x_max.\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__} y_min must be less than y_max.\")\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of each column in both matrices\n  sq_norm0 = np.sum(mat0 ** 2, axis=0)\n  sq_norm1 = np.sum(mat1 ** 2, axis=0)\n\n  # Compute the dot product between each column in both matrices\n  dot_prod = np.dot(mat0, mat1.T)\n\n  # Compute the squared distances between each pair of columns\n  sq_dist = sq_norm0[:, None] + sq_norm1[None, :] - 2 * dot_prod\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(SPECIAL_PATH_PREFIXES):\n        return True\n    return False\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"The 'assets_names' argument is required when 'items' is a dictionary.\"\n            )\n        if len(assets_names) != len(items):\n            raise ValueError(\n                f\"The length of 'assets_names' ({len(assets_names)}) does not match the number of assets in 'items' ({len(items)}).\"\n            )\n        items = [items.get(asset, fill_value) for asset in assets_names]\n\n    if isinstance(items, np.ndarray):\n        if items.ndim != dim:\n            raise ValueError(\n                f\"The 'items' array has {items.ndim} dimensions, but it should have {dim} dimensions.\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"The 'items' array has {items.shape[0]} assets, but it should have {n_assets} assets.\"\n            )\n        return items\n\n    if isinstance(items, (list, tuple)):\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"The length of 'items' ({len(items)}) does not match the number of assets ({n_assets}).\"\n            )\n        return np.array(items)\n\n    if isinstance(items, (int, float)):\n        return np.full(n_assets, items)\n\n    if isinstance(items, str):\n        return np.full(n_assets, items)\n\n    if isinstance(items, (np.ndarray, pd.Series)):\n        if items.ndim != 1:\n            raise ValueError(\n                f\"The 'items' array has {items.ndim} dimensions, but it should have 1 dimension.\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"The 'items' array has {items.shape[0]} assets, but it should have {n_assets} assets.\"\n            )\n        return items\n\n    raise ValueError(\n        f\"The 'items' argument must be a dictionary, numpy array, list, tuple, int, float, or str, but it is of type {type(items)}.\"\n    )\n\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent object with the agent lifecycle state and OpenAI wrapper.\n        micro_agent = MicroAgent(agent_lifecycle, openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the dictionary's key-value pairs.\n        micro_agent.id = data.get(\"id\", None)\n        micro_agent.name = data.get(\"name\", None)\n        micro_agent.description = data.get(\"description\", None)\n        micro_agent.version = data.get(\"version\", None)\n        micro_agent.status = data.get(\"status\", None)\n        micro_agent.created_at = data.get(\"created_at\", None)\n        micro_agent.updated_at = data.get(\"updated_at\", None)\n        micro_agent.last_deployed_at = data.get(\"last_deployed_at\", None)\n        micro_agent.last_deployed_version = data.get(\"last_deployed_version\", None)\n        micro_agent.last_deployed_status = data.get(\"last_deployed_status\", None)\n        micro_agent.last_deployed_error = data.get(\"last_deployed_error\", None)\n        micro_agent.last_deployed_error_message = data.get(\"last_deployed_error_message\", None)\n        micro_agent.last_deployed_error_stack_trace = data.get(\"last_deployed_error_stack_trace\", None)\n        micro_agent.last_deployed_error_code = data.get(\"last_deployed_error_code\", None)\n        micro_agent.last_deployed_error_type = data.get(\"last_deployed_error_type\", None)\n        micro_agent.last_deployed_error_message = data.get(\"last_deployed_error_message\", None)\n        micro_agent.last_deployed_error_stack_trace = data.get(\"last_deployed_error_stack_trace\", None)\n        micro_agent.last_deployed_error_code = data.get(\"last_deployed_error_code\", None)\n        micro_agent.last_deployed_error_type = data.get(\"last_deployed_error_type\", None)\n        micro_agent.last_deployed_error_message = data.get(\"last_"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Check if the input is a NumPy array\n  if isinstance(srgb, np.ndarray):\n    # Convert the input to a JAX array\n    srgb = xnp.asarray(srgb)\n\n  # Check if the input is a JAX array\n  if isinstance(srgb, xnp.ndarray):\n    # Check if the input is a float32 array\n    if srgb.dtype == xnp.float32:\n      # Check if the epsilon value is not provided\n      if eps is None:\n        # Set the epsilon value to the machine epsilon for float32 data type\n        eps = xnp.finfo(xnp.float32).eps\n\n      # Define the piecewise function for the conversion\n      def piecewise_function(x):\n        if x <= 0.04045:\n          return x / 12.92\n        else:\n          return ((x + 0.055) / 1.055) ** 2.4\n\n      # Apply the piecewise function to the input\n      linear = xnp.where(srgb <= 0.0031308, srgb * 12.92, piecewise_function(srgb))\n\n      # Return the converted color values\n      return linear\n\n    # Raise an error if the input is not a float32 array\n    else:\n      raise ValueError(\"The input must be a float32 array.\")\n\n  # Raise an error if the input is not a NumPy or JAX array\n  else:\n    raise ValueError(\"The input must be a NumPy or JAX array.\")\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust spline degree to be at most one less than the number of points in x.\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Create a spline interpolator.\n  spline_interpolator = scipy.interpolate.splrep(t_input, x, s=smoothness, k=spline_degree)\n\n  # Interpolate the signal.\n  return scipy.interpolate.splev(t_output, spline_interpolator)\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # if word is all caps\n    if word.isupper():\n        return word\n\n    # if word is all lowercase\n    elif word.islower():\n        return word\n\n    # if word is mixed case\n    else:\n        # if first letter is capitalized\n        if word[0].isupper():\n            # if second letter is capitalized\n            if word[1].isupper():\n                # if third letter is capitalized\n                if word[2].isupper():\n                    # if fourth letter is capitalized\n                    if word[3].isupper():\n                        # if fifth letter is capitalized\n                        if word[4].isupper():\n                            # if sixth letter is capitalized\n                            if word[5].isupper():\n                                # if seventh letter is capitalized\n                                if word[6].isupper():\n                                    # if eighth letter is capitalized\n                                    if word[7].isupper():\n                                        # if ninth letter is capitalized\n                                        if word[8].isupper():\n                                            # if tenth letter is capitalized\n                                            if word[9].isupper():\n                                                # if eleventh letter is capitalized\n                                                if word[10].isupper():\n                                                    # if twelfth letter is capitalized\n                                                    if word[11].isupper():\n                                                        # if thirteenth letter is capitalized\n                                                        if word[12].isupper():\n                                                            # if fourteenth letter is capitalized\n                                                            if word[13].isupper():\n                                                                # if fifteenth letter is capitalized\n                                                                if word[14].isupper():\n                                                                    # if sixteenth letter is capitalized\n                                                                    if word[15].isupper():\n                                                                        # if seventeenth letter is capitalized\n                                                                        if word[16].isupper():\n                                                                            # if eighteenth letter is capitalized\n                                                                            if word[17].isupper():\n                                                                                # if nineteenth letter is capitalized\n                                                                                if word[18].isupper():\n                                                                                    # if twentieth letter is capitalized\n                                                                                    if word[19].isupper():\n                                                                                        # if twenty-first letter is capitalized\n                                                                                        if word[20].isupper():\n                                                                                            # if twenty-second letter is capitalized\n                                                                                            if word[21].isupper():\n                                                                                                # if twenty-third letter is capitalized\n                                                                                                if word[22].isupper():\n                                                                                                   "}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"The field {field.name} in class {cls.__name__} must contain only boolean values, but found {v.dtype}.\")\n\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Apply the isotropic scaling operation\n  return x / norm\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file into a DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Convert specified columns to dictionary objects\n    if dict_columns is None:\n        dict_columns = ['module_params']\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  # Check the mode of computation\n  if mode not in ['fast', 'accurate']:\n    raise ValueError('Invalid mode of computation: {}. Must be either \"fast\" or \"accurate\".'.format(mode))\n\n  # Check the input covariance matrix\n  if not isinstance(cov, np.ndarray):\n    raise TypeError('Input covariance matrix must be a numpy array.')\n  if len(cov.shape) != 2:\n    raise ValueError('Input covariance matrix must be a 2D array.')\n  if cov.shape[0] != cov.shape[1]:\n    raise ValueError('Input covariance matrix must be a square matrix.')\n\n  # Compute the determinant of the input covariance matrix\n  det = np.linalg.det(cov)\n\n  # Check for invalid determinants\n  if det <= 0:\n    raise ValueError('Invalid determinant: {}. Must be positive.'.format(det))\n\n  # Compute the isotropic covariance matrix\n  if mode == 'fast':\n    isotropic_cov = np.diag(np.sqrt(det / np.diag(cov)))\n  elif mode == 'accurate':\n    isotropic_cov = np.diag(np.exp(np.log(det) / np.diag(cov)))\n\n  # Return the isotropic covariance matrix\n  return isotropic_cov\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Task execution program for the CogWorks project.\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", type=bool, default=False, help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", type=bool, help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be retried upon failure.\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file, specifying where to find the configuration settings.\")\n\n    return parser.parse_args()"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape != (None, 2):\n        raise ValueError(f\"The field {field.name} must be a list of 2D points, but it has shape {v.shape}.\")\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n\n    # Convert the integer to a string\n    n_str = str(n)\n\n    # Initialize the encoded character\n    encoded_char = \"\"\n\n    # Iterate over the characters in the string\n    for char in n_str:\n        # Convert the character to an integer\n        char_int = ord(char)\n\n        # Get the corresponding character from the character set\n        encoded_char += char_set[char_int]\n\n    # Return the encoded character\n    return encoded_char\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.clip(x, a_min = eps, a_max = None) + value_at_zero)\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    # Initialize the chunk index and the indexes within the chunks\n    chunk_index = {}\n    indexes_in_chunks = {}\n\n    # Iterate through each worker's intervals\n    for worker_index, intervals in workers_intervals.items():\n        # Initialize the current index and the chunk index for the worker\n        current_index = indexes[worker_index]\n        chunk_index[worker_index] = 0\n        indexes_in_chunks[worker_index] = 0\n\n        # Iterate through each interval\n        for interval in intervals:\n            # Calculate the size of the interval\n            interval_size = interval[1] - interval[0]\n\n            # If the current index is within the interval, update the chunk index and the indexes within the chunk\n            if current_index >= interval[0] and current_index < interval[1]:\n                chunk_index[worker_index] += 1\n                indexes_in_chunks[worker_index] = current_index - interval[0]\n                break\n\n            # Otherwise, update the current index\n            current_index += interval_size\n\n    # Return the updated chunk index and the indexes within the chunks\n    return chunk_index, indexes_in_chunks\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  # Check if the datastructure is valid\n  if datastructure not in ['grid', 'hash']:\n    raise ValueError('Invalid datastructure. Only \"grid\" and \"hash\" are supported.')\n\n  # Check if the coordinates are 2D or higher\n  if coordinates.ndim < 2:\n    raise ValueError('Coordinates should be 2D or higher.')\n\n  # Check if the coordinates are within the bounds of the dimensions they refer to\n  if coordinates.min() < 0 or coordinates.max() > 1:\n    raise ValueError('Coordinates should be within the bounds of the dimensions they refer to.')\n\n  # Check if the coordinates are within the bounds of the dimensions they refer to\n  if coordinates.min() < 0 or coordinates.max() > 1:\n    raise ValueError('Coordinates should be within the bounds of the dimensions they refer to.')\n\n  # Check if the coordinates are within the bounds of the dimensions they refer to\n  if coordinates.min() < 0 or coordinates.max() > 1:\n    raise ValueError('Coordinates should be within the bounds of the dimensions they refer to.')\n\n  # Check if the coordinates are within the bounds of the dimensions they refer to\n  if coordinates.min() < 0 or coordinates.max() > 1:\n    raise ValueError('Coordinates should be within the bounds of the dimensions they refer to.')\n\n  # Check if the coordinates are within the bounds of the dimensions they refer to\n  if coordinates.min() < 0 or coordinates.max() > 1:\n    raise ValueError('Coordinates should be within the bounds of the dimensions they refer to.')\n\n  # Check if the coordinates are within the bounds of the dimensions they refer to\n  if coordinates.min() < 0 or coordinates.max() > 1:\n    raise ValueError('Coordinates should be within the bounds of the dimensions they refer to.')\n\n  # Check if the coordinates are within the bounds of the dimensions they refer to\n  if coordinates.min() < 0 or coordinates.max() > 1:\n    raise ValueError('Coordinates should be within the bounds of the dimensions they refer to.')\n\n  # Check if the coordinates are within the bounds of the dimensions they refer to\n  if coordinates.min() < 0 or coordinates.max() > 1:\n    raise ValueError('Coordinates should be within the bounds of the dimensions they refer to.')\n\n  # Check if the coordinates are within the bounds of the dimensions they refer to\n  if"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check that the tessellation factor is greater than or equal to 1.\n  if v < 1:\n    raise ValueError(\"Tessellation factor must be greater than or equal to 1.\")\n\n  # Generate the integer weights for each vertex of the triangle.\n  w1 = np.arange(0, v + 1)\n  w2 = np.arange(0, v + 1)\n  w3 = np.arange(0, v + 1)\n\n  # Generate the barycentric weights for each vertex of the triangle.\n  b1 = w1 / v\n  b2 = w2 / v\n  b3 = w3 / v\n\n  # Generate the barycentric weights for each point in the tessellated triangle.\n  b = np.zeros((v + 1, v + 1, v + 1))\n  for i in range(v + 1):\n    for j in range(v + 1):\n      for k in range(v + 1):\n        b[i, j, k] = b1[i] + b2[j] + b3[k]\n\n  # Return the barycentric weights for each point in the tessellated triangle.\n  return b\n\n"}
{"namespace": "linspline.query", "completion": "  # Ensure the spline is valid\n  if not is_valid(t, v):\n    raise ValueError(\"The spline is not valid.\")\n\n  # Ensure the query points are sorted\n  tq = np.sort(tq)\n\n  # Ensure the query points are within the range of the spline\n  if np.any(tq < t[0]) or np.any(tq > t[-1]):\n    raise ValueError(\"The query points are outside the range of the spline.\")\n\n  # Find the indices of the query points in the time array\n  i = np.searchsorted(t, tq)\n\n  # Find the indices of the time points in the query array\n  j = np.searchsorted(tq, t)\n\n  # Find the indices of the time points in the query array\n  k = np.searchsorted(tq, t[1:])\n\n  # Find the indices of the time points in the query array\n  l = np.searchsorted(tq, t[:-1])\n\n  # Find the indices of the time points in the query array\n  m = np.searchsorted(tq, t[1:-1])\n\n  # Find the indices of the time points in the query array\n  n = np.searchsorted(tq, t[2:])\n\n  # Find the indices of the time points in the query array\n  o = np.searchsorted(tq, t[:-2])\n\n  # Find the indices of the time points in the query array\n  p = np.searchsorted(tq, t[2:-1])\n\n  # Find the indices of the time points in the query array\n  q = np.searchsorted(tq, t[1:-2])\n\n  # Find the indices of the time points in the query array\n  r = np.searchsorted(tq, t[3:])\n\n  # Find the indices of the time points in the query array\n  s = np.searchsorted(tq, t[:-3])\n\n  # Find the indices of the time points in the query array\n  t = np.searchsorted(tq, t[3:-1])\n\n  # Find the indices of the time points in the query array\n  u = np.searchsorted(tq, t[1:-3])\n\n  # Find the indices of the time points in the query array\n  v = np.searchsorted(tq, t[2:-2])\n\n  # Find the indices of the time points in the query array\n  w ="}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(x, (int, float)) and x > 0 for x in v):\n            raise ValueError(f\"All values in {field.name} must be positive. Found {v}.\")\n    else:\n        if not isinstance(v, (int, float)) or v <= 0:\n            raise ValueError(f\"Value in {field.name} must be positive. Found {v}.\")\n\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert the origins to NDC\n  origins_ndc = xnp.matmul(pixtocam, xnp.concatenate((origins, xnp.ones((origins.shape[0], 1))), axis=1).T).T\n  origins_ndc = origins_ndc[:, :3] / origins_ndc[:, 3:]\n\n  # Calculate the directions in NDC\n  directions_ndc = xnp.matmul(pixtocam, xnp.concatenate((directions, xnp.zeros((directions.shape[0], 1))), axis=1).T).T\n  directions_ndc = directions_ndc[:, :3] / directions_ndc[:, 3:]\n\n  # Adjust the origins to the near plane\n  origins_ndc[:, 2] = origins_ndc[:, 2] / near\n\n  return origins_ndc, directions_ndc\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  return np.abs(np.dot(dir1, dir2)) < 1e-10\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the BLEU score\n    bleu_score = sentence_bleu([reference_tokens], continuation_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n\n    # Apply the brevity penalty if requested\n    if with_penalty:\n        brevity_penalty = brevity_penalty_factor(reference_tokens, continuation_tokens)\n        bleu_score *= brevity_penalty\n\n    return bleu_score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Convert the input vector of weights to a NumPy array.\n  w = np.array(w)\n\n  # Calculate the difference between consecutive elements in the input vector t.\n  diff = np.diff(t)\n\n  # Divide the input vector of weights by the difference between consecutive elements in the input vector t.\n  pdf = w / diff\n\n  # Return the resulting PDF that integrates to 1.\n  return pdf\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for filename in filenames:\n            try:\n                total_size += os.path.getsize(os.path.join(dirpath, filename))\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - offset * period - torch.floor((val - offset * period) / period) * period\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        # Convert the purpose_embedding from a numpy array to a list if necessary, to ensure compatibility with serialization formats.\n        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        # Serialize the MicroAgent object into a dictionary.\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items and weights are the same\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be the same.\")\n\n    # Check that the number of items and weights is greater than zero\n    if len(items) == 0:\n        raise ValueError(\"The number of items and weights must be greater than zero.\")\n\n    # Check that the number of bins is greater than zero\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be greater than zero.\")\n\n    # Check that the weights are all positive\n    if any(weight < 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Create a dictionary to store the items in each bin\n    bin_items = {i: [] for i in range(num_bins)}\n\n    # Create a dictionary to store the total weights of each bin\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Iterate over the sorted items and place them into the bins\n    for item, weight in sorted_items:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n        # Add the item to the bin\n        bin_items[min_bin].append(item)\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    return bin_items, bin_weights\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Create a string representation of the input data.\n        data = func_name + str(args) + str(kwargs)\n\n        # Compute the SHA-256 hash of the input data.\n        hash_object = hashlib.sha256(data.encode())\n\n        # Return the hexadecimal digest of the hash.\n        return hash_object.hexdigest()\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points in the polygon\n    distances = np.linalg.norm(np.diff(polygon, axis=0), axis=1)\n\n    # Remove distances that exceed the maximum point distance\n    distances[distances > max_point_distance] = 0\n\n    # Return the sum of the remaining distances\n    return np.sum(distances)\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Get the largest polygon's area\n    largest_area = max([polygon.area for polygon in polygons])\n\n    # Filter out polygons whose area is below the relative threshold\n    filtered_polygons = [\n        polygon for polygon in polygons if polygon.area >= rel_tr * largest_area\n    ]\n\n    # Filter out polygons whose area is below the absolute threshold\n    filtered_polygons = [\n        polygon for polygon in filtered_polygons if polygon.area >= abs_tr\n    ]\n\n    return filtered_polygons\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker should process.\n    samples_per_worker = num_samples_yielded // num_workers\n    samples_per_worker_remainder = num_samples_yielded % num_workers\n\n    # Create a dictionary to store the number of samples each worker should process.\n    samples_per_worker_dict = {}\n\n    # Distribute the samples evenly among the workers.\n    for worker_index in range(num_workers):\n        samples_per_worker_dict[worker_index] = samples_per_worker\n\n    # Distribute any remaining samples.\n    for worker_index in range(num_workers):\n        if samples_per_worker_remainder > 0:\n            samples_per_worker_dict[worker_index] += 1\n            samples_per_worker_remainder -= 1\n\n    return samples_per_worker_dict\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    # Check if the results and value lists have the same length\n    if len(results) != len(value):\n        raise ValueError(\"The results and value lists must have the same length.\")\n\n    # Check if the metadatas list is provided\n    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    # Check if the metadatas list has the same length as the results list\n    if len(metadatas) != len(results):\n        raise ValueError(\"The metadatas list must have the same length as the results list.\")\n\n    # Filter the results and values based on the threshold\n    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape != (2,):\n        raise ValueError(\"Input array must have shape (_, 2), where _ is the number of points in the polygon.\")\n\n    x = array[0]\n    y = array[1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # check if a is sorted\n    if not torch.all(a[:, :-1] <= a[:, 1:]):\n        raise ValueError(\"a must be sorted\")\n\n    # check if a and v have the same shape\n    if a.shape != v.shape:\n        raise ValueError(\"a and v must have the same shape\")\n\n    # check if a and v have the same dtype\n    if a.dtype != v.dtype:\n        raise ValueError(\"a and v must have the same dtype\")\n\n    # check if a and v have the same device\n    if a.device != v.device:\n        raise ValueError(\"a and v must have the same device\")\n\n    # check if a and v have the same layout\n    if a.layout != v.layout:\n        raise ValueError(\"a and v must have the same layout\")\n\n    # check if a and v have the same storage_offset\n    if a.storage_offset() != v.storage_offset():\n        raise ValueError(\"a and v must have the same storage_offset\")\n\n    # check if a and v have the same is_contiguous\n    if not a.is_contiguous() or not v.is_contiguous():\n        raise ValueError(\"a and v must be contiguous\")\n\n    # check if a and v have the same is_pinned\n    if a.is_pinned() != v.is_pinned():\n        raise ValueError(\"a and v must have the same is_pinned\")\n\n    # check if a and v have the same requires_grad\n    if a.requires_grad != v.requires_grad:\n        raise ValueError(\"a and v must have the same requires_grad\")\n\n    # check if a and v have the same is_leaf\n    if a.is_leaf != v.is_leaf:\n        raise ValueError(\"a and v must have the same is_leaf\")\n\n    # check if a and v have the same is_coalesced\n    if a.is_coalesced() != v.is_coalesced():\n        raise ValueError(\"a and v must have the same is_coalesced\")\n\n    # check if a and v have the same is_shared\n    if a.is_shared() != v.is_shared():\n        raise ValueError(\"a and v must have the same is_shared\")\n\n    # check if a and v have the same is_view\n    if a.is_view"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n    [\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n    ],\n    dtype = xnp.float32,\n  )\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input vector(s)\n  mag_sq = np.sum(x**2, axis=1)\n\n  # Scale the input vector(s) towards the origin\n  return x / np.sqrt(mag_sq)[:, None]\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 1000:\n        return f\"{num_bytes} B\"\n    elif num_bytes < 1000000:\n        return f\"{num_bytes / 1000:.2f} KB\"\n    elif num_bytes < 1000000000:\n        return f\"{num_bytes / 1000000:.2f} MB\"\n    elif num_bytes < 1000000000000:\n        return f\"{num_bytes / 1000000000:.2f} GB\"\n    elif num_bytes < 1000000000000000:\n        return f\"{num_bytes / 1000000000000:.2f} TB\"\n    else:\n        return f\"{num_bytes / 1000000000000000:.2f} PB\"\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls, v, field):\n        if not isinstance(v, np.ndarray):\n            raise ValueError(f\"Field {field.name} must be an array.\")\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"Field {field.name} must have {nb_dimensions} dimensions.\")\n        return v\n\n    return validator\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Extract the x, y, and z coordinates from the input array.\n  x, y, z = onp.atleast_3d(cartesian_vector).T\n\n  # Calculate the radius (r) of the point(s) using the Pythagorean theorem.\n  r = onp.sqrt(x**2 + y**2 + z**2)\n\n  # Calculate the inclination (theta) of the point(s) using the inverse tangent function.\n  theta = onp.arctan2(onp.sqrt(x**2 + y**2), z)\n\n  # Calculate the azimuth (phi) of the point(s) using the inverse tangent function.\n  phi = onp.arctan2(y, x)\n\n  # Return the spherical coordinates as a tuple.\n  return r, theta, phi\n"}
{"namespace": "common.rougeL_score", "completion": "    # Tokenize the generated text continuation and the reference text using the custom tokenizer function based on jieba.\n    generated_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the ROUGE-L score between the generated text continuation and the reference text.\n    rouge_l_score = rouge_l_scorer.score(generated_tokens, reference_tokens)\n\n    return rouge_l_score\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Attempt to locate the object using the standard method.\n    try:\n        return eval(name)\n\n    # If the object cannot be located using the standard method, attempt to locate it using a fallback method.\n    except NameError:\n        return fallback_locate(name)\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to a buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the number of ids and scores match the number of weights\n    if len(ids) != len(scores) or len(ids) != len(weights):\n        raise ValueError(\n            f\"The number of ids ({len(ids)}) and scores ({len(scores)}) must match the number of weights ({len(weights)}).\"\n        )\n\n    # Check if the sum of the weights is equal to 1\n    if sum(weights) != 1:\n        raise ValueError(\n            f\"The sum of the weights ({sum(weights)}) must be equal to 1.\"\n        )\n\n    # Normalize the weights\n    normalized_weights = [w / sum(weights) for w in weights]\n\n    # Initialize the fused ids and scores\n    fused_ids = []\n    fused_scores = []\n\n    # Iterate over the ids and scores\n    for i, s in zip(ids, scores):\n        # Check if the length of the ids and scores match\n        if len(i) != len(s):\n            raise ValueError(\n                f\"The length of the ids ({len(i)}) and scores ({len(s)}) must match.\"\n            )\n\n        # Normalize the scores\n        normalized_scores = [score / sum(s) for score in s]\n\n        # Calculate the fused scores\n        fused_scores.append(\n            [\n                normalized_weights[j] * normalized_scores[k]\n                for j, k in zip(range(len(normalized_weights)), range(len(normalized_scores)))\n            ]\n        )\n\n        # Select the top_k results\n        top_k_ids = [i[j] for j in np.argsort(fused_scores)[-top_k:]]\n        top_k_scores = [fused_scores[j] for j in np.argsort(fused_scores)[-top_k:]]\n\n        # Append the fused ids and scores\n        fused_ids.append(top_k_ids)\n        fused_scores.append(top_k_scores)\n\n    # Return the fused ids and scores\n    return fused_ids, fused_scores\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    else:\n        if percent:\n            x = x * 100\n        if x == 0:\n            return \"0\"\n        else:\n            if x < 0:\n                return f\"{x:,.2f}%\"\n            else:\n                return f\"{x:,.2f}\"\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / 1024 ** 3\n        if free_space < threshold_in_gb:\n            time.sleep(sleep_time)\n        else:\n            break\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in the time or position vector.\n  d = np.diff(t)\n\n  # Multiply the PDF values by the differences to obtain the weights.\n  w = p * d\n\n  # Normalize the weights to sum to 1.\n  w = w / np.sum(w)\n\n  return w\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all spaces from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    line_text = line_text.split(\",\")\n\n    return line_text\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros must not exceed the number of weights.\")\n\n    weights = np.random.rand(n)\n    weights /= np.sum(weights)\n\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n        weights /= np.sum(weights)\n\n    return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict['module_type']\n        module_params = module_dict.copy()\n        del module_params['module_type']\n\n        if module_type == 'linear':\n            return cls.Linear(module_params)\n        elif module_type == 'relu':\n            return cls.ReLU(module_params)\n        elif module_type == 'sigmoid':\n            return cls.Sigmoid(module_params)\n        elif module_type == 'softmax':\n            return cls.Softmax(module_params)\n        elif module_type == 'tanh':\n            return cls.Tanh(module_params)\n        elif module_type == 'batch_norm':\n            return cls.BatchNorm(module_params)\n        elif module_type == 'dropout':\n            return cls.Dropout(module_params)\n        elif module_type == 'conv2d':\n            return cls.Conv2D(module_params)\n        elif module_type == 'max_pool2d':\n            return cls.MaxPool2D(module_params)\n        elif module_type == 'avg_pool2d':\n            return cls.AvgPool2D(module_params)\n        elif module_type == 'flatten':\n            return cls.Flatten(module_params)\n        elif module_type == 'linear_layer':\n            return cls.LinearLayer(module_params)\n        elif module_type == 'conv2d_layer':\n            return cls.Conv2DLayer(module_params)\n        elif module_type == 'max_pool2d_layer':\n            return cls.MaxPool2DLayer(module_params)\n        elif module_type == 'avg_pool2d_layer':\n            return cls.AvgPool2DLayer(module_params)\n        elif module_type == 'flatten_layer':\n            return cls.FlattenLayer(module_params)\n        elif module_type == 'batch_norm_layer':\n            return cls.BatchNormLayer(module_params)\n        elif module_type == 'dropout_layer':\n            return cls.DropoutLayer(module_params)\n        elif module_type == 'linear_block':\n            return cls.LinearBlock(module_params)\n        elif module_type == 'conv2d_block':\n            return cls.Conv2DBlock(module_params)\n        elif module_type == 'max_pool2d"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box coordinates of the instance\n    x1, y1, x2, y2 = instance[\"bbox\"]\n\n    # Calculate the center of the bounding box\n    center_x = (x1 + x2) / 2\n    center_y = (y1 + y2) / 2\n\n    # Calculate the dimensions of the bounding box\n    width = x2 - x1\n    height = y2 - y1\n\n    # Calculate the desired crop size based on the instance's aspect ratio\n    desired_width = crop_size[0]\n    desired_height = int(desired_width * height / width)\n\n    # Calculate the top-left corner of the crop\n    top_left_x = int(center_x - desired_width / 2)\n    top_left_y = int(center_y - desired_height / 2)\n\n    # Adjust the top-left corner to ensure it fits within the image boundaries\n    top_left_x = max(0, top_left_x)\n    top_left_y = max(0, top_left_y)\n    top_left_x = min(image_size[0] - desired_width, top_left_x)\n    top_left_y = min(image_size[1] - desired_height, top_left_y)\n\n    # Calculate the dimensions of the crop\n    crop_width = min(desired_width, image_size[0] - top_left_x)\n    crop_height = min(desired_height, image_size[1] - top_left_y)\n\n    # Create the CropTransform object\n    crop_transform = CropTransform(top_left_x, top_left_y, crop_width, crop_height)\n\n    return crop_transform\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Get the squared norm along the last axis of x\n  squared_norm = jnp.sum(x ** 2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value to prevent division by zero during the backward pass\n  squared_norm = jnp.maximum(squared_norm, grad_eps)\n\n  # Calculate the inverse of the square root of the squared norm\n  inv_norm = 1.0 / jnp.sqrt(squared_norm)\n\n  # Multiply the input array by the inverse of the square root of the squared norm\n  return x * inv_norm\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Extract the agent name and input text from the response string\n        agent_name = response[response.find('Use Agent[') + len('Use Agent['):response.find(']')]\n        input_text = response[response.find(']') + 1:].strip()\n\n        # Return the agent name and input text\n        return agent_name, input_text\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # Initialize an empty list to store the instances\n    instances = []\n\n    # Loop through each annotation in the list\n    for anno in annos:\n\n        # Get the bounding boxes, classes, and segmentation masks from the annotation\n        boxes = anno[\"bbox\"]\n        classes = anno[\"category_id\"]\n        masks = anno[\"segmentation\"]\n\n        # Convert the bounding boxes to a tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n\n        # Convert the classes to a tensor\n        classes = torch.as_tensor(classes, dtype=torch.int64)\n\n        # Convert the segmentation masks to a tensor\n        if mask_format == \"polygon\":\n            masks = [np.asarray(m).reshape(-1, 2) for m in masks]\n            masks = [m.astype(np.float32) for m in masks]\n            masks = torch.as_tensor(masks, dtype=torch.float32)\n        elif mask_format == \"bitmask\":\n            masks = [np.asarray(m).reshape(-1) for m in masks]\n            masks = [m.astype(np.uint8) for m in masks]\n            masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        # Create an empty Instances object\n        instance = Instances(image_size)\n\n        # Set the fields in the Instances object\n        instance.gt_boxes = Boxes(boxes)\n        instance.gt_classes = classes\n        instance.gt_masks = masks\n\n        # Append the Instances object to the list\n        instances.append(instance)\n\n    # Return the list of Instances objects\n    return instances\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert isinstance(cov, np.ndarray), \"Input must be a numpy array.\"\n    assert cov.ndim == 2, \"Input must be a 2D array.\"\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    with torch.no_grad():\n        for name, module in model.named_modules():\n            if hasattr(module, 'training'):\n                module.training = False\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validator(cls: Type[BaseModel]) -> None:\n        \"\"\"\n        This function is the validator function that is generated by are_shapes_equal. It is used within a Pydantic model to ensure the shapes of two specified fields match.\n\n        Input-Output Arguments\n        :param cls: Type[BaseModel], The Pydantic model class that is being validated.\n        :return: None, This function does not return anything. It simply raises a ValueError if the shapes of the two specified fields do not match.\n        \"\"\"\n        if getattr(cls, field1).shape != getattr(cls, field2).shape:\n            raise ValueError(f\"The shapes of {field1} and {field2} do not match.\")\n\n    return validator\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Check if the input is a list of strings or a list of dictionaries\n    if isinstance(metrics, list):\n        # Check if the list contains strings or dictionaries\n        if isinstance(metrics[0], str):\n            # If the list contains strings, convert them to dictionaries\n            metrics = [dict(name=metric) for metric in metrics]\n        elif isinstance(metrics[0], dict):\n            # If the list contains dictionaries, check if they have a 'name' key\n            for metric in metrics:\n                if 'name' not in metric:\n                    raise ValueError(f\"Metric dictionary does not contain a 'name' key: {metric}\")\n        else:\n            raise ValueError(f\"Invalid metric type: {type(metrics[0])}\")\n    else:\n        raise ValueError(f\"Invalid metric type: {type(metrics)}\")\n\n    # Extract metric names and parameters from the list of dictionaries\n    metric_names = [metric['name'] for metric in metrics]\n    metric_params = [metric.get('params', {}) for metric in metrics]\n\n    return metric_names, metric_params\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # If the inverse function is not provided, try to automatically determine it based on a predefined mapping of functions to their inverses.\n  if fn_inv is None:\n    fn_inv = {\n      torch.exp: torch.log,\n      torch.log: torch.exp,\n      torch.log10: torch.pow,\n      torch.pow: torch.log10,\n      torch.sqrt: torch.pow,\n      torch.pow: torch.sqrt,\n      torch.reciprocal: torch.reciprocal,\n      torch.reciprocal: torch.reciprocal,\n      torch.sin: torch.asin,\n      torch.asin: torch.sin,\n      torch.cos: torch.acos,\n      torch.acos: torch.cos,\n      torch.tan: torch.atan,\n      torch.atan: torch.tan,\n      torch.atan2: torch.atan2,\n      torch.atan2: torch.atan2,\n      torch.sinh: torch.asinh,\n      torch.asinh: torch.sinh,\n      torch.cosh: torch.acosh,\n      torch.acosh: torch.cosh,\n      torch.tanh: torch.atanh,\n      torch.atanh: torch.tanh,\n    }.get(fn, None)\n\n  # If the inverse function is still not provided, raise an error.\n  if fn_inv is None:\n    raise ValueError(f\"Could not automatically determine the inverse function for the provided function {fn}. Please provide the inverse function explicitly using the `fn_inv` argument.\")\n\n  # Define the forward mapping from metric to normalized distances.\n  def t_to_s(t):\n    t = torch.clamp(t, t_near, t_far)\n    return fn((t - t_near) / (t_far - t_near))\n\n  # Define the backward mapping from normalized to metric distances.\n  def s_to_t(s):\n    s = torch.clamp(s, 0.0, 1.0)\n    return t_near + (t_far - t_near) * fn_inv(s)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Ensure the input data points are valid for a linear spline.\n  if not is_valid_linear_spline(t, w):\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  # Compute the integral of the data points using the trapezoid rule.\n  integral = 0\n  for i in range(len(t) - 1):\n    integral += (t[i + 1] - t[i]) * (w[i + 1] + w[i]) / 2\n  return integral\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Calculate the weighted sum of scores for each ID\n    weighted_sum = {}\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            if ids[i][j] not in weighted_sum:\n                weighted_sum[ids[i][j]] = scores[i][j] * weights[i]\n            else:\n                weighted_sum[ids[i][j]] += scores[i][j] * weights[i]\n\n    # Normalize the weighted sum of scores for each ID\n    normalized_weighted_sum = {}\n    for id, score in weighted_sum.items():\n        normalized_weighted_sum[id] = score / sum(weighted_sum.values())\n\n    # Sort the IDs by their normalized weighted sum in descending order\n    sorted_ids = sorted(normalized_weighted_sum, key=normalized_weighted_sum.get, reverse=True)\n\n    # Return the top K IDs and their corresponding normalized weighted sums\n    return sorted_ids[:top_k], [normalized_weighted_sum[id] for id in sorted_ids[:top_k]]\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Get the shape of the mean and covariance tensors\n  mean_shape = tf.shape(mean)\n  cov_shape = tf.shape(cov)\n\n  # Get the number of dimensions in the mean and covariance tensors\n  mean_ndims = len(mean_shape)\n  cov_ndims = len(cov_shape)\n\n  # Get the number of dimensions in the mean and covariance tensors\n  mean_ndims = len(mean_shape)\n  cov_ndims = len(cov_shape)\n\n  # Get the number of dimensions in the mean and covariance tensors\n  mean_ndims = len(mean_shape)\n  cov_ndims = len(cov_shape)\n\n  # Get the number of dimensions in the mean and covariance tensors\n  mean_ndims = len(mean_shape)\n  cov_ndims = len(cov_shape)\n\n  # Get the number of dimensions in the mean and covariance tensors\n  mean_ndims = len(mean_shape)\n  cov_ndims = len(cov_shape)\n\n  # Get the number of dimensions in the mean and covariance tensors\n  mean_ndims = len(mean_shape)\n  cov_ndims = len(cov_shape)\n\n  # Get the number of dimensions in the mean and covariance tensors\n  mean_ndims = len(mean_shape)\n  cov_ndims = len(cov_shape)\n\n  # Get the number of dimensions in the mean and covariance tensors\n  mean_ndims = len(mean_shape)\n  cov_ndims = len(cov_shape)\n\n  # Get the number of dimensions in the mean and covariance tensors\n  mean_ndims = len(mean_shape)\n  cov_ndims = len(cov_shape)\n\n  # Get the number of dimensions in the mean and covariance tensors\n  mean_ndims = len(mean_shape)\n  cov_ndims = len(cov_shape)\n\n  # Get the number of dimensions in the mean and covariance tensors\n  mean_ndims = len(mean_shape)\n  cov_ndims = len(cov_shape)\n\n  # Get the number of dimensions in the mean and covariance tensors\n  mean_ndims = len(mean_shape)\n  cov_ndims = len(cov_shape)\n\n  # Get the number of dimensions in the mean and covariance tensors\n  mean_ndims = len(mean_"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[:len(i) // 2], i[len(i) // 2:]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The given matrix is not square.\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Generate the powers of 2 from min_deg to max_deg\n  powers = np.arange(min_deg, max_deg + 1)\n  scales = 2 ** powers\n\n  # Scale the input\n  scaled_input = np.expand_dims(x, axis=-1) * scales\n\n  # Apply sine function\n  encoded_features = np.sin(scaled_input)\n\n  # Concatenate the original input if append_identity is True\n  if append_identity:\n    encoded_features = np.concatenate([encoded_features, np.expand_dims(x, axis=-1)], axis=-1)\n\n  return encoded_features\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validator(cls: Type, values: Dict) -> Dict:\n        \"\"\"\n        This function is the validator function that is called by Pydantic when the validator is used in a Pydantic model. It takes a class type and a dictionary of values, checks for shape equality between arrays in the specified fields, and returns the validated values if the check passes.\n\n        Input-Output Arguments\n        :param cls: Type. The class type of the Pydantic model that is being validated.\n        :param values: Dict. A dictionary of values to be validated.\n        :return: Dict. The validated values, with any changes made to the arrays in the specified fields.\n\n        \"\"\"\n        field1_values = values[field1]\n        field2_values = values[field2]\n        if len(field1_values) != len(field2_values):\n            raise ValueError(f\"The {field1} and {field2} fields must have the same length.\")\n        for field1_value, field2_value in zip(field1_values, field2_values):\n            if field1_value.shape != field2_value.shape:\n                raise ValueError(f\"The {field1} and {field2} fields must have the same shape.\")\n        return values\n\n    return validator\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions.\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings.\n        self.render(camera)\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same settings as the original BertConfig object\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        bos_token_id=bert_config.bos_token_id,\n        eos_token_id=bert_config.eos_token_id,\n        **bert_config.kwargs,\n    )\n\n    # Set the additional attributes specific to the Nomic model\n    nomic_config.num_labels = 1  # Set the number of labels to 1 for a binary classification task\n    nomic_config.num_hidden_layers = 12  # Set the number of hidden layers to 12 for a Nomic model\n    nomic_config.num_attention_heads = 12  # Set the number of attention heads to 12 for a Nomic model\n    nomic_config.intermediate_size = 3072  # Set the intermediate size to 3072 for a Nomic model\n    nomic_config.hidden_act = \"gelu\"  # Set the hidden activation function to gelu for a Nomic model\n    nomic_config.hidden_dropout_prob = 0.1  # Set the hidden dropout probability to 0.1 for a Nomic model\n    nomic_config.attention_probs_dropout_prob = 0.1  #"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.shader_program.bind()\n            self.upload_gl_uniforms(camera)\n            self.vao.bind()\n            glDrawArrays(GL_POINTS, 0, self.num_vertices)\n            self.vao.unbind()\n            self.shader_program.unbind()\n\n        elif self.render_type == RenderType.LINES:\n            self.shader_program.bind()\n            self.upload_gl_uniforms(camera)\n            self.vao.bind()\n            if self.indexed:\n                glDrawElements(GL_LINES, self.num_faces * 2, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_LINES, 0, self.num_faces * 2)\n            self.vao.unbind()\n            self.shader_program.unbind()\n\n        elif self.render_type == RenderType.TRIANGLES:\n            self.shader_program.bind()\n            self.upload_gl_uniforms(camera)\n            self.vao.bind()\n            if self.indexed:\n                glDrawElements(GL_TRIANGLES, self.num_faces * 3, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, self.num_faces * 3)\n            self.vao.unbind()\n            self.shader_program.unbind()\n\n        elif self.render_type == RenderType.QUADS:\n            self.shader_program.bind()\n            self.upload_gl_uniforms(camera)\n            self.vao.bind()\n            if self.indexed:\n                glDrawElements(GL_QUADS, self.num_faces * 4, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_QUADS, 0, self.num_faces * 4)\n            self.vao.unbind()\n            self.shader_program.unbind()\n\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            self.shader_program.bind()\n            self.upload_gl_uniforms(camera)\n            self.vao.bind()\n            if self.indexed:\n                glDrawElements(GL_TRIANGLE"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # If the input is a PyTorch tensor, convert it to a numpy array.\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # If the width and height are not provided, use the object's default dimensions.\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        # Update the texture with the specified data.\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGB, GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate the input shapes\n    assert R.shape == tvec.shape == camera_matrix.shape == image_size.shape, \"Input shapes must match\"\n\n    # Calculate the camera position\n    camera_position = -torch.bmm(R.transpose(1, 2), tvec.unsqueeze(2)).squeeze(2)\n\n    # Calculate the camera rotation\n    camera_rotation = torch.bmm(R, torch.eye(3).unsqueeze(0).expand(R.shape[0], -1, -1)).squeeze(1)\n\n    # Calculate the camera intrinsic parameters\n    focal_length = camera_matrix[:, 0, 0] / (image_size[:, 0] / 2)\n    principal_point = camera_matrix[:, 0, 2] - image_size[:, 0] / 2\n    sensor_width = camera_matrix[:, 0, 0] * image_size[:, 0] / focal_length\n\n    # Create a tensor to store the camera parameters\n    camera_params = torch.cat([camera_position, camera_rotation, focal_length.unsqueeze(1), principal_point.unsqueeze(1), sensor_width.unsqueeze(1)], dim=1)\n\n    return camera_params\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if self.use_quad_draw:\n            # Set up the viewport and scissor box for rendering\n            glViewport(x, y, w, h)\n            glScissor(x, y, w, h)\n\n            # Activate the shader program\n            self.quad_program.use()\n\n            # Bind the texture\n            self.tex.bind()\n\n            # Draw the quadrilateral\n            glBindVertexArray(self.vao)\n            glDrawArrays(GL_TRIANGLE_FAN, 0, 4)\n            glBindVertexArray(0)\n\n            # Restore the viewport and scissor box\n            glViewport(0, 0, self.W, self.H)\n            glScissor(0, 0, self.W, self.H)\n\n        else:\n            # Fall back to a simpler blit method for drawing\n            self.blit(x, y, w, h)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract camera parameters from batch\n    R = batch.R.transpose(1, 2)\n    T = batch.T.transpose(1, 2)\n    K = batch.K.transpose(1, 2)\n    H, W = batch.H, batch.W\n\n    # Adjust rotation and translation matrices to match PyTorch3D's conventions\n    R = torch.matmul(R, torch.tensor([[-1, 0, 0], [0, -1, 0], [0, 0, 1]]).to(R.device))\n    T = torch.matmul(T, torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, -1]]).to(T.device))\n\n    # Compute camera intrinsic matrix for NDC\n    K = torch.matmul(K, torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 0.5]]).to(K.device))\n\n    # Compute camera center in the camera's coordinate system\n    C = -torch.matmul(R.transpose(1, 2), T)\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Bind the Quad instance's framebuffer object (FBO) as the read framebuffer\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n\n        # Perform the pixel copy operation\n        glBlitFramebuffer(x, y, w, h, x, y, w, h, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n\n        # Restore the previously bound read framebuffer\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Check if the target time is greater than the source time\n    if t0 > t1:\n        raise ValueError(\"The target time (t0) must be less than or equal to the source time (t1).\")\n\n    # Check if the target time is less than the source time\n    if t0 < t1:\n        raise ValueError(\"The target time (t0) must be greater than or equal to the source time (t1).\")\n\n    # Check if the target time is equal to the source time\n    if t0 == t1:\n        raise ValueError(\"The target time (t0) must be different from the source time (t1).\")\n\n    # Check if the target time is a scalar\n    if t0.ndim == 0:\n        raise ValueError(\"The target time (t0) must be a tensor with at least one dimension.\")\n\n    # Check if the source time is a scalar\n    if t1.ndim == 0:\n        raise ValueError(\"The source time (t1) must be a tensor with at least one dimension.\")\n\n    # Check if the target time is a scalar\n    if y1.ndim == 0:\n        raise ValueError(\"The values (y1) must be a tensor with at least one dimension.\")\n\n    # Check if the target time is a scalar\n    if t0.shape != t1.shape:\n        raise ValueError(\"The target time (t0) and the source time (t1) must have the same shape.\")\n\n    # Check if the target time is a scalar\n    if t0.shape != y1.shape:\n        raise ValueError(\"The target time (t0) and the values (y1) must have the same shape.\")\n\n    # Check if the target time is a scalar\n    if t1.shape != y1.shape:\n        raise ValueError(\"The source time (t1) and the values (y1) must have the same shape.\")\n\n    # Check if the target time is a scalar\n    if t0.shape != y1.shape:\n        raise ValueError(\"The target time (t0) and the values (y1) must have the same shape.\")\n\n    # Check if the target time is a scalar\n    if t0.shape != y1.shape:\n        raise ValueError(\"The target time (t0) and the values (y1) must have the same shape.\")\n\n    # Check if the target time is a"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_env_upper = torch.max(w_env, w)\n\n    # calculate the loss\n    loss = torch.sum(torch.square(w - w_env_upper))\n\n    # scale the loss\n    loss = loss / (torch.sum(w_env_upper) + eps)\n\n    # return the loss\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Calculate the inter-interval loss\n    inter_interval_loss = torch.sum(w * torch.abs(t[:, 1:] - t[:, :-1]))\n\n    # Calculate the intra-interval loss\n    intra_interval_loss = torch.sum(w * torch.abs(t[:, 1:] - t[:, :-1]) * (t[:, 1:] - t[:, :-1]))\n\n    # Combine the inter- and intra-interval losses\n    distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return distortion_loss\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the weights sum to 1\n    if not torch.isclose(torch.sum(w), torch.ones_like(w)).all():\n        raise ValueError(\"The weights must sum to 1.\")\n\n    # Compute the integrated weights\n    w_int = torch.cumsum(w, dim=0)\n\n    # Compute the weighted percentiles\n    ps_int = torch.tensor(ps).to(w.device)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)\n    ps_int = torch.unsqueeze(ps_int, dim=0)"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Check if t and w are valid tensors\n    if not isinstance(t, torch.Tensor):\n        raise TypeError(\"t must be a torch.Tensor\")\n    if not isinstance(w, torch.Tensor):\n        raise TypeError(\"w must be a torch.Tensor\")\n\n    # Check if t and w have the same shape\n    if t.shape != w.shape:\n        raise ValueError(\"t and w must have the same shape\")\n\n    # Check if t is sorted\n    if not torch.all(t[:-1] <= t[1:]):\n        raise ValueError(\"t must be sorted\")\n\n    # Check if num_samples is a positive integer\n    if not isinstance(num_samples, int):\n        raise TypeError(\"num_samples must be an integer\")\n    if num_samples <= 0:\n        raise ValueError(\"num_samples must be a positive integer\")\n\n    # Check if perturb is a boolean\n    if not isinstance(perturb, bool):\n        raise TypeError(\"perturb must be a boolean\")\n\n    # Check if single_jitter is a boolean\n    if not isinstance(single_jitter, bool):\n        raise TypeError(\"single_jitter must be a boolean\")\n\n    # Check if t and w are on the same device\n    if t.device != w.device:\n        raise ValueError(\"t and w must be on the same device\")\n\n    # Check if t and w are on the same device\n    if t.device != w.device:\n        raise ValueError(\"t and w must be on the same device\")\n\n    # Check if t and w are on the same device\n    if t.device != w.device:\n        raise ValueError(\"t and w must be on the same device\")\n\n    # Check if t and w are on the same device\n    if t.device != w.device:\n        raise ValueError(\"t and w must be on the same device\")\n\n    # Check if t and w are on the same device\n    if t.device != w.device:\n        raise ValueError(\"t and w must be on the same device\")\n\n    # Check if t and w are on the same device\n    if t.device != w.device:\n        raise ValueError(\"t and w must be on the same device\")\n\n    # Check if t and w are on the same device\n    if t.device != w.device:\n        raise ValueError(\"t and w must be on the same device\")\n\n    # Check"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Perform dilation\n    dilated_t = torch.nn.functional.max_pool1d(t.unsqueeze(0).unsqueeze(0), kernel_size=dilation, stride=dilation).squeeze(0).squeeze(0)\n\n    # Clip dilated time steps within the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Adjust weights corresponding to the dilated time steps\n    dilated_w = torch.zeros_like(w)\n    dilated_w[dilated_t.long()] = w[dilated_t.long()]\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check that the query times are in ascending order\n    if not tf.reduce_all(tf.math.less(tq[:-1], tq[1:])):\n        raise ValueError('Query times must be in ascending order.')\n\n    # Check that the step change times are in ascending order\n    if not tf.reduce_all(tf.math.less(t[:-1], t[1:])):\n        raise ValueError('Step change times must be in ascending order.')\n\n    # Check that the step change times are in the range of the query times\n    if not tf.reduce_all(tf.math.logical_and(t[0] <= tq, tq <= t[-1])):\n        raise ValueError('Step change times must be in the range of the query times.')\n\n    # Check that the step change times are not equal to the query times\n    if not tf.reduce_all(tf.math.not_equal(t, tq)):\n        raise ValueError('Step change times must not be equal to the query times.')\n\n    # Check that the step change times are not equal to the query times\n    if not tf.reduce_all(tf.math.not_equal(t, tq)):\n        raise ValueError('Step change times must not be equal to the query times.')\n\n    # Check that the step change times are not equal to the query times\n    if not tf.reduce_all(tf.math.not_equal(t, tq)):\n        raise ValueError('Step change times must not be equal to the query times.')\n\n    # Check that the step change times are not equal to the query times\n    if not tf.reduce_all(tf.math.not_equal(t, tq)):\n        raise ValueError('Step change times must not be equal to the query times.')\n\n    # Check that the step change times are not equal to the query times\n    if not tf.reduce_all(tf.math.not_equal(t, tq)):\n        raise ValueError('Step change times must not be equal to the query times.')\n\n    # Check that the step change times are not equal to the query times\n    if not tf.reduce_all(tf.math.not_equal(t, tq)):\n        raise ValueError('Step change times must not be equal to the query times.')\n\n    # Check that the step change times are not equal to the query times\n    if"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # calculate the distance between adjacent intervals\n    d = t[1:] - t[:-1]\n\n    # calculate the annealing factor for each interval\n    a = 1.0 / (1.0 + torch.exp(anneal_slope * (train_frac - t[:-1]) / d))\n\n    # adjust the weights based on the annealing factor\n    w_adj = w * a\n\n    # apply softmax to the adjusted weights\n    w_adj = torch.softmax(w_adj, dim=-1)\n\n    # handle cases where adjacent intervals have zero distance\n    w_adj[d == 0] = 0.0\n\n    # handle cases where the annealing factor is zero\n    w_adj[a == 0] = 0.0\n\n    # handle cases where the annealing factor is NaN\n    w_adj[torch.isnan(a)] = 0.0\n\n    # handle cases where the adjusted weights are NaN\n    w_adj[torch.isnan(w_adj)] = 0.0\n\n    # handle cases where the adjusted weights are zero\n    w_adj[w_adj == 0] = eps\n\n    # handle cases where the adjusted weights are negative\n    w_adj[w_adj < 0] = eps\n\n    # handle cases where the adjusted weights are greater than one\n    w_adj[w_adj > 1] = 1.0 - eps\n\n    # return the adjusted weights\n    return w_adj\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        return [to_cuda(elem, device, ignore_list) for elem in batch]\n    elif isinstance(batch, dict):\n        return {key: to_cuda(value, device, ignore_list) for key, value in batch.items() if key != \"meta\" and not ignore_list}\n    elif isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor if necessary\n    if v.dim() != f.dim():\n        f = f.expand(v.shape[:-1] + f.shape[dim:])\n\n    # compute the faces normals w.r.t the vertices\n    n = torch.cross(v[f[:, 1]] - v[f[:, 0]], v[f[:, 2]] - v[f[:, 0]], dim=dim)\n\n    # reshaping the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    return n.reshape(f.shape[:-1] + n.shape[dim:])\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, 0)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Create a dotdict instance to store the camera parameters and GUI related elements\n        batch = dotdict()\n\n        # Convert the camera parameters into tensors\n        batch.focal_length = torch.tensor(self.focal_length)\n        batch.principal_point = torch.tensor(self.principal_point)\n        batch.distortion_coefficients = torch.tensor(self.distortion_coefficients)\n        batch.image_size = torch.tensor(self.image_size)\n        batch.image_size_x = torch.tensor(self.image_size_x)\n        batch.image_size_y = torch.tensor(self.image_size_y)\n        batch.image_size_z = torch.tensor(self.image_size_z)\n        batch.image_size_xy = torch.tensor(self.image_size_xy)\n        batch.image_size_yz = torch.tensor(self.image_size_yz)\n        batch.image_size_xz = torch.tensor(self.image_size_xz)\n        batch.image_size_xyz = torch.tensor(self.image_size_xyz)\n        batch.image_size_zyx = torch.tensor(self.image_size_zyx)\n        batch.image_size_yzx = torch.tensor(self.image_size_yzx)\n        batch.image_size_zxy = torch.tensor(self.image_size_zxy)\n        batch.image_size_xzy = torch.tensor(self.image_size_xzy)\n        batch.image_size_zyx_xy = torch.tensor(self.image_size_zyx_xy)\n        batch.image_size_yzx_xy = torch.tensor(self.image_size_yzx_xy)\n        batch.image_size_zxy_xy = torch.tensor(self.image_size_zxy_xy)\n        batch.image_size_xzy_xy = torch.tensor(self.image_size_xzy_xy)\n        batch.image_size_zyx_yz = torch.tensor(self.image_size_zyx_yz)\n        batch.image_size_yzx_yz = torch.tensor(self.image_size_yzx"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent and not agent.is_prime_agent:\n            agent_dict = agent.get_state()\n            self.agent_persistence_manager.save_agent(agent_dict)\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Find the closest agent to the given purpose embedding\n        closest_agent = None\n        similarity_score = -np.inf\n        for agent in self.agents:\n            similarity_score_current = self.calculate_similarity_score(purpose_embedding, agent.purpose_embedding)\n            if similarity_score_current > similarity_score:\n                closest_agent = agent\n                similarity_score = similarity_score_current\n\n        return closest_agent, similarity_score\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        # Create the prime agent\n        prime_agent = Agent(self.prompt, self.name, self.weight, self.prime_flag, self.other_flag)\n\n        # Add the prime agent to the agent list\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent_id = self.get_agent_id(purpose)\n        if agent_id is None:\n            return None\n        agent_json = self.get_agent_json(agent_id)\n        if agent_json is None:\n            return None\n        return self.deserialize_agent(agent_json, agent_lifecycle, openai_wrapper)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Load all agents from the database\n        agents = self.load_all_agents()\n\n        # Initialize a list to store the loaded agents\n        loaded_agents = []\n\n        # Iterate through each agent in the list of agents\n        for agent in agents:\n            # Load the agent based on its purpose\n            if agent.purpose == \"policy\":\n                loaded_agent = self.load_policy_agent(agent, agent_lifecycle, openai_wrapper)\n            elif agent.purpose == \"value\":\n                loaded_agent = self.load_value_agent(agent, agent_lifecycle, openai_wrapper)\n            elif agent.purpose == \"reward\":\n                loaded_agent = self.load_reward_agent(agent, agent_lifecycle, openai_wrapper)\n            elif agent.purpose == \"action\":\n                loaded_agent = self.load_action_agent(agent, agent_lifecycle, openai_wrapper)\n            elif agent.purpose == \"observation\":\n                loaded_agent = self.load_observation_agent(agent, agent_lifecycle, openai_wrapper)\n            elif agent.purpose == \"state\":\n                loaded_agent = self.load_state_agent(agent, agent_lifecycle, openai_wrapper)\n            elif agent.purpose == \"transition\":\n                loaded_agent = self.load_transition_agent(agent, agent_lifecycle, openai_wrapper)\n            elif agent.purpose == \"environment\":\n                loaded_agent = self.load_environment_agent(agent, agent_lifecycle, openai_wrapper)\n            elif agent.purpose == \"model\":\n                loaded_agent = self.load_model_agent(agent, agent_lifecycle, openai_wrapper)\n            elif agent.purpose == \"agent\":\n                loaded_agent = self.load_agent_agent(agent, agent_lifecycle, openai_wrapper)\n            elif agent.purpose == \"agent_group\":\n                loaded_agent = self.load_agent_group_agent(agent, agent_lifecycle, openai_wrapper)\n            elif agent.purpose == \"agent_group_member\":\n                loaded_agent = self.load_agent_group_member_agent(agent, agent_lifecycle, openai_wrapper)\n            elif agent.purpose == \"agent_group_member_agent\":\n                loaded_agent = self.load_agent_group_member_agent"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence_mechanism.save_agent(agent)\n        except Exception as e:\n            self.logger.error(f\"Error saving agent {agent.name} with id {agent.id} to the persistence mechanism. \"\n                              f\"Exception: {e}\")\n            raise e\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Clean up the agents\n        self.cleanup_agents()\n\n        # Return the current list of agents\n        return self.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            # Generate the prompt using the LLM\n            prompt = f\"You are a language learning assistant. You are given a goal: {goal}. You are also given a sample input: {sample_input}. You must generate a prompt that will help you achieve the goal. Your prompt must be a single sentence. Your prompt must be written in English. Your prompt must be written in the past tense. Your prompt must be written in the present tense. Your prompt must be written in the future tense. Your prompt must be written in the present perfect tense. Your prompt must be written in the present continuous tense. Your prompt must be written in the past perfect tense. Your prompt must be written in the future perfect tense. Your prompt must be written in the past continuous tense. Your prompt must be written in the future continuous tense. Your prompt must be written in the present progressive tense. Your prompt must be written in the past progressive tense. Your prompt must be written in the future progressive tense. Your prompt must be written in the present perfect progressive tense. Your prompt must be written in the past perfect progressive tense. Your prompt must be written in the future perfect progressive tense. Your prompt must be written in the past continuous progressive tense. Your prompt must be written in the future continuous progressive tense. Your prompt must be written in the present progressive continuous tense. Your prompt must be written in the past progressive continuous tense. Your prompt must be written in the future progressive continuous tense. Your prompt must be written in the present perfect progressive continuous tense. Your prompt must be written in the past perfect progressive continuous tense. Your prompt must be written in the future perfect progressive continuous tense. Your prompt must be written in the past continuous progressive continuous tense. Your prompt must be written in the future continuous progressive continuous tense. Your prompt must be written in the present progressive perfect tense. Your prompt must be written in the past progressive perfect tense. Your prompt must be written in the future progressive perfect tense. Your prompt must be written in the present perfect progressive perfect tense. Your prompt must be written in the past perfect progressive perfect tense. Your prompt must be written in the future perfect progressive perfect tense. Your prompt must be written in the past continuous progressive perfect tense. Your prompt must be written in the future continuous progressive perfect tense. Your prompt must be written in the present progressive"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Create a connection to the SQLite database.\n        conn = sqlite3.connect(self.db_filename)\n\n        # Create a cursor object to execute SQL queries.\n        cursor = conn.cursor()\n\n        # Check if the agent's record already exists in the database.\n        cursor.execute(\"SELECT * FROM agents WHERE id = ?\", (agent_dict['id'],))\n        result = cursor.fetchone()\n\n        # If the agent's record already exists, update the existing record with the new information.\n        if result:\n            cursor.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\", (agent_dict['purpose'], json.dumps(agent_dict['data']), agent_dict['id']))\n        # If the agent's record does not exist, insert a new record with the agent's information.\n        else:\n            cursor.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict['data'])))\n\n        # Commit the changes to the database and close the connection.\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Fetch the agent data from the database\n        agent_data = self.fetch_agent_data(purpose)\n\n        # If the agent data is found, deserialize it and return it\n        if agent_data is not None:\n            return self.deserialize_agent(agent_data)\n\n        # If the agent data is not found, return None\n        return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Create a cursor object to execute SQL queries\n        cursor = self.conn.cursor()\n\n        # Execute the SQL query to retrieve all purposes from the database\n        cursor.execute(\"SELECT purpose FROM agent_purpose\")\n\n        # Fetch all rows from the result set\n        purposes = cursor.fetchall()\n\n        # Close the cursor and connection\n        cursor.close()\n        self.conn.close()\n\n        # Convert the result set to a list of purposes\n        purposes = [purpose[0] for purpose in purposes]\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Create a connection to the SQLite database.\n        conn = sqlite3.connect(self.db_path)\n\n        # Create a cursor to execute SQL queries.\n        cursor = conn.cursor()\n\n        # Execute the SQL query to fetch the cached result for the provided argument hash.\n        cursor.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n\n        # Fetch the result from the database.\n        result = cursor.fetchone()\n\n        # Close the connection to the database.\n        conn.close()\n\n        # If the result is found, return it.\n        if result:\n            return json.loads(result[0])\n\n        # Otherwise, return None.\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Create a connection to the SQLite database.\n        conn = sqlite3.connect(self.db_path)\n\n        # Create a cursor object to execute SQL commands.\n        cursor = conn.cursor()\n\n        # Create a table to store the cached results if it does not already exist.\n        cursor.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS cache (\n                key TEXT PRIMARY KEY,\n                value TEXT\n            );\n            \"\"\"\n        )\n\n        # Insert a new row into the 'cache' table with the hash of the arguments as the key and the JSON-serialized result as the value.\n        cursor.execute(\n            \"\"\"\n            INSERT OR REPLACE INTO cache (key, value)\n            VALUES (?, ?);\n            \"\"\",\n            (arg_hash, json.dumps(result)),\n        )\n\n        # Commit the changes to the database.\n        conn.commit()\n\n        # Close the connection to the database.\n        conn.close()\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments.\n    global_config.update_config(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of displaying it in the terminal.\n    if quiet_mode:\n        sys.stdout = open(global_config.output_file, \"w\")\n\n    # Execute the command line process based on the provided arguments.\n    if args.command == \"train\":\n        train_model(args)\n    elif args.command == \"predict\":\n        predict_model(args)\n    elif args.command == \"evaluate\":\n        evaluate_model(args)\n    elif args.command == \"visualize\":\n        visualize_model(args)\n    elif args.command == \"clean\":\n        clean_model(args)\n    elif args.command == \"export\":\n        export_model(args)\n    elif args.command == \"import\":\n        import_model(args)\n    elif args.command == \"convert\":\n        convert_model(args)\n    elif args.command == \"convert_to_onnx\":\n        convert_model_to_onnx(args)\n    elif args.command == \"convert_to_coreml\":\n        convert_model_to_coreml(args)\n    elif args.command == \"convert_to_keras\":\n        convert_model_to_keras(args)\n    elif args.command == \"convert_to_pytorch\":\n        convert_model_to_pytorch(args)\n    elif args.command == \"convert_to_tensorflow\":\n        convert_model_to_tensorflow(args)\n    elif args.command == \"convert_to_tensorflow_lite\":\n        convert_model_to_tensorflow_lite(args)\n    elif args.command == \"convert_to_tensorflow_js\":\n        convert_model_to_tensorflow_js(args)\n    elif args.command == \"convert_to_tensorflow_hub\":\n        convert_model_to_tensorflow_hub(args)\n    elif args.command == \"convert_to_c\":\n        convert_model_to_c(args)\n    elif args.command == \"convert_to_c_header\":\n        convert_model_to_c_header(args)\n    elif args.command == \"convert_to_c_source\":\n        convert_model_to_c_source(args)\n    elif args"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Set the default model to use if no model is specified\n        if 'model' not in kwargs:\n            kwargs['model'] = 'gpt-3.5-turbo'\n\n        # Set the default temperature to use if no temperature is specified\n        if 'temperature' not in kwargs:\n            kwargs['temperature'] = 0.7\n\n        # Set the default top_p to use if no top_p is specified\n        if 'top_p' not in kwargs:\n            kwargs['top_p'] = 1\n\n        # Set the default max_tokens to use if no max_tokens is specified\n        if 'max_tokens' not in kwargs:\n            kwargs['max_tokens'] = 100\n\n        # Set the default frequency_penalty to use if no frequency_penalty is specified\n        if 'frequency_penalty' not in kwargs:\n            kwargs['frequency_penalty'] = 0\n\n        # Set the default presence_penalty to use if no presence_penalty is specified\n        if 'presence_penalty' not in kwargs:\n            kwargs['presence_penalty'] = 0\n\n        # Set the default stop to use if no stop is specified\n        if 'stop' not in kwargs:\n            kwargs['stop'] = None\n\n        # Set the default logprobs to use if no logprobs is specified\n        if 'logprobs' not in kwargs:\n            kwargs['logprobs'] = None\n\n        # Set the default echo to use if no echo is specified\n        if 'echo' not in kwargs:\n            kwargs['echo'] = False\n\n        # Set the default messages to use if no messages is specified\n        if 'messages' not in kwargs:\n            kwargs['messages'] = None\n\n        # Set the default stream to use if no stream is specified\n        if 'stream' not in kwargs:\n            kwargs['stream'] = False\n\n        # Set the default logit_bias to use if no logit_bias is specified\n        if 'logit_bias' not in kwargs:\n            kwargs['logit_bias'] = None\n\n        # Set the default logit_bias_all to use if no logit_bias_all is specified\n        if 'logit_bias_all' not in kwargs:\n            kwargs['logit_bias_all'] = None\n\n        # Set the default best_of to use if no best_of is specified\n        if 'best_of' not in kwargs:\n            kwargs['best_of'] ="}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        # Check if the client exists and if the credentials have expired based on the predefined interval.\n        if self.client is None or (time.time() - self.client_creation_time) > self.client_expiration_interval:\n\n            # Create a new S3 client instance.\n            self.client = boto3.client(\n                \"s3\",\n                aws_access_key_id=self.access_key,\n                aws_secret_access_key=self.secret_key,\n                region_name=self.region,\n            )\n\n            # Update the client creation time.\n            self.client_creation_time = time.time()\n\n        # Return the S3 client instance.\n        return self.client\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self.is_worker:\n            raise RuntimeError(\"state_dict() cannot be called from a DataLoader worker process.\")\n\n        state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir_path\": self.input_dir_path,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader is not None else None,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state_dict = state_dict\n\n        # Load the state dictionary into the StreamingDataset instance.\n        self.state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if the state dictionary is empty\n        if not self._state_dict:\n            return\n\n        # Check if the state dictionary contains the correct keys\n        if not set(self._state_dict.keys()).issuperset(set(self._state_dict_keys)):\n            raise ValueError(\n                f\"The state dictionary contains the following keys: {self._state_dict.keys()}. The StreamingDataset instance expects the following keys: {self._state_dict_keys}.\"\n            )\n\n        # Check if the state dictionary contains the correct values\n        if not set(self._state_dict.values()).issuperset(set(self._state_dict_values)):\n            raise ValueError(\n                f\"The state dictionary contains the following values: {self._state_dict.values()}. The StreamingDataset instance expects the following values: {self._state_dict_values}.\"\n            )\n\n        # Check if the state dictionary contains the correct values for the shuffle parameter\n        if self._state_dict[\"shuffle\"] not in self._state_dict_values[\"shuffle\"]:\n            raise ValueError(\n                f\"The state dictionary contains the following value for the shuffle parameter: {self._state_dict['shuffle']}. The StreamingDataset instance expects the following values for the shuffle parameter: {self._state_dict_values['shuffle']}.\"\n            )\n\n        # Check if the state dictionary contains the correct values for the num_workers parameter\n        if self._state_dict[\"num_workers\"] not in self._state_dict_values[\"num_workers\"]:\n            raise ValueError(\n                f\"The state dictionary contains the following value for the num_workers parameter: {self._state_dict['num_workers']}. The StreamingDataset instance expects the following values for the num_workers parameter: {self._state_dict_values['num_workers']}.\"\n            )\n\n        # Check if the state dictionary contains the correct values for the input_dir parameter\n        if self._state_dict[\"input_dir\"] not in self._state_dict_values[\"input_dir\"]:\n            raise ValueError(\n                f\"The state dictionary contains the following value for the input_dir parameter: {self._state_dict['input_dir']}. The StreamingDataset instance expects the following values for the input_dir parameter: {self._state_dict_values['input_dir']}.\"\n            )\n\n        # Check if the state dictionary"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    cache_dir = None\n\n    if \"CACHE_DIR\" in os.environ:\n        cache_dir = os.environ[\"CACHE_DIR\"]\n    else:\n        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"py_cache\")\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    cache_dir = os.path.join(cache_dir, hashlib.md5(input_dir.encode()).hexdigest())\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\"The remote file path must start with 's3://'.\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            raise ValueError(\"The local file already exists.\")\n\n        # Check if the s5cmd command-line tool is available\n        if shutil.which(\"s5cmd\") is None:\n            raise ValueError(\"The s5cmd command-line tool is not available.\")\n\n        # Check if the boto3 library is available\n        if shutil.which(\"aws\") is None:\n            raise ValueError(\"The boto3 library is not available.\")\n\n        # Parse the S3 URL\n        parsed_url = urlparse(remote_filepath)\n\n        # Check if the file lock can be acquired within the specified timeout\n        with self.file_lock:\n\n            # Check if the file already exists\n            if os.path.exists(local_filepath):\n                raise ValueError(\"The local file already exists.\")\n\n            # Check if the s5cmd command-line tool is available\n            if shutil.which(\"s5cmd\") is None:\n                raise ValueError(\"The s5cmd command-line tool is not available.\")\n\n            # Check if the boto3 library is available\n            if shutil.which(\"aws\") is None:\n                raise ValueError(\"The boto3 library is not available.\")\n\n            # Download the file using the s5cmd command-line tool\n            try:\n                subprocess.run(\n                    [\n                        \"s5cmd\",\n                        \"cp\",\n                        f\"{remote_filepath}\",\n                        f\"{local_filepath}\",\n                        \"--recursive\",\n                        \"--force\",\n                        \"--no-progress\",\n                    ],\n                    check=True,\n                )\n            except subprocess.CalledProcessError as e:\n                raise ValueError(f\"Error downloading file using s5cmd: {e}\")\n\n            # Download the file using the boto3 library\n            try:\n                s3_client = boto3.client(\"s3\")\n                s3_client.download_file(\n                    parsed_url.netloc, parsed_url.path[1:], local_filepath\n                )\n            except Exception as e:\n                raise ValueError(f\"Error downloading file using boto3: {e}\")\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Get the world size from the worker environment\n    world_size = worker_env.world_size\n\n    # Get the number of chunks to be distributed\n    num_chunks = len(chunks_replica)\n\n    # Calculate the number of chunks to be assigned to each worker\n    num_chunks_per_worker = num_chunks // world_size\n\n    # Calculate the number of chunks that will be left over after the division\n    num_chunks_left_over = num_chunks % world_size\n\n    # Create a list to store the assigned chunks for each worker\n    assigned_chunks = [[] for _ in range(world_size)]\n\n    # Create a list to store the assigned intervals for each worker\n    assigned_intervals = [[] for _ in range(world_size)]\n\n    # Iterate over the chunks and intervals\n    for i, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n\n        # Calculate the worker index for the current chunk\n        worker_index = i % world_size\n\n        # Calculate the number of chunks to be assigned to the current worker\n        num_chunks_to_assign = num_chunks_per_worker\n\n        # Check if there are any chunks left over\n        if num_chunks_left_over > 0:\n\n            # Decrement the number of chunks left over\n            num_chunks_left_over -= 1\n\n            # Increment the number of chunks to be assigned to the current worker\n            num_chunks_to_assign += 1\n\n        # Append the current chunk and interval to the assigned chunks and intervals lists for the current worker\n        assigned_chunks[worker_index].append(chunk)\n        assigned_intervals[worker_index].append(interval)\n\n    # Return the assigned chunks and intervals\n    return assigned_chunks, assigned_intervals\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Get the image's mode, dimensions, and raw pixel data\n        mode = item.mode\n        size = item.size\n        data = item.tobytes()\n\n        # Convert the image's mode to a bytes object\n        mode_bytes = mode.encode(\"utf-8\")\n\n        # Calculate the length of the mode as a bytes object\n        mode_length = len(mode_bytes).to_bytes(4, \"big\")\n\n        # Create a bytes object containing the image's dimensions, mode length, and mode\n        serialized_data = size + mode_length + mode_bytes\n\n        # Return the serialized data and None\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not isinstance(item, Image):\n            raise TypeError(f\"The item is not an instance of Image class or its subclasses. The item type is {type(item)}.\")\n\n        if isinstance(item, JPEG):\n            if item.filename is not None and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.tobytes(), None\n        elif isinstance(item, PNG):\n            return item.tobytes(), None\n        elif isinstance(item, BMP):\n            return item.tobytes(), None\n        elif isinstance(item, GIF):\n            return item.tobytes(), None\n        elif isinstance(item, TIFF):\n            return item.tobytes(), None\n        elif isinstance(item, PPM):\n            return item.tobytes(), None\n        elif isinstance(item, PGM):\n            return item.tobytes(), None\n        elif isinstance(item, PBM):\n            return item.tobytes(), None\n        elif isinstance(item, PAM):\n            return item.tobytes(), None\n        elif isinstance(item, PNM):\n            return item.tobytes(), None\n        elif isinstance(item, JPEG2000):\n            return item.tobytes(), None\n        elif isinstance(item, WEBP):\n            return item.tobytes(), None\n        elif isinstance(item, HEIF):\n            return item.tobytes(), None\n        elif isinstance(item, AVIF):\n            return item.tobytes(), None\n        elif isinstance(item, ICO):\n            return item.tobytes(), None\n        elif isinstance(item, CUR):\n            return item.tobytes(), None\n        elif isinstance(item, XBM):\n            return item.tobytes(), None\n        elif isinstance(item, XPM):\n            return item.tobytes(), None\n        elif isinstance(item, XWD):\n            return item.tobytes(), None\n        elif isinstance(item, XPM):\n            return item.tobytes(), None\n        elif isinstance(item, XWD):\n            return item.tobytes(), None\n        elif isinstance(item, XPM):\n            return item.tobytes(), None\n        elif isinstance(item, XWD):\n            return item.tobytes(), None\n        elif isinstance(item, XPM):\n            return item.tobytes(), None\n        elif isinstance(item, XWD):\n            return item"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        # Extract the width, height, and mode from the beginning of the byte stream.\n        width = int.from_bytes(data[0:4], byteorder='little')\n        height = int.from_bytes(data[4:8], byteorder='little')\n        mode_length = int.from_bytes(data[8:12], byteorder='little')\n        mode = data[12:12 + mode_length].decode('utf-8')\n\n        # Extract the raw image data from the byte stream.\n        image_data = data[12 + mode_length:]\n\n        # Reconstruct the image from the extracted data.\n        return cls(width, height, mode, image_data)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array.\n        data_type = data[0]\n        shape = []\n        for i in range(1, len(data)):\n            if data[i] == 0:\n                break\n            shape.append(data[i])\n        shape = tuple(shape)\n\n        # Reconstruct the tensor from the remaining bytes.\n        tensor_data = data[len(data_type) + len(shape) + 1:]\n        tensor = torch.from_numpy(np.frombuffer(tensor_data, dtype=data_type)).view(shape)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a numpy array\n        tensor_data = item.numpy()\n\n        # Get the dtype and shape of the tensor\n        tensor_dtype = tensor_data.dtype\n        tensor_shape = tensor_data.shape\n\n        # Serialize the tensor data\n        tensor_data_bytes = tensor_data.tobytes()\n\n        # Create a bytes object to store the serialized data\n        serialized_data = bytes()\n\n        # Add the tensor dtype to the serialized data\n        serialized_data += tensor_dtype.tobytes()\n\n        # Add the tensor shape to the serialized data\n        serialized_data += tensor_shape.tobytes()\n\n        # Add the tensor data to the serialized data\n        serialized_data += tensor_data_bytes\n\n        # Return the serialized data and None as the second element of the tuple\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            if torchvision is not None:\n                image = Image.open(io.BytesIO(data))\n                if image.format == \"JPEG\":\n                    return image\n                else:\n                    image = torchvision.io.decode_jpeg(data)\n                    return image\n            else:\n                image = Image.open(io.BytesIO(data))\n                return image\n        except RuntimeError:\n            image = Image.open(io.BytesIO(data))\n            return image\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the PyTorch tensor to a NumPy array\n        np_array = item.numpy()\n\n        # Convert the NumPy array to bytes\n        bytes_data = np_array.tobytes()\n\n        # Get the data type of the tensor\n        data_type = item.dtype\n\n        # Get the index of the data type\n        data_type_index = self.data_type_to_index[data_type]\n\n        # Create a string representing the data type\n        data_type_str = f\"no_header_tensor:{data_type_index}\"\n\n        return bytes_data, data_type_str\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype))\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        data_type = np.dtype(data[0:4])\n        shape = np.frombuffer(data[4:16], dtype=np.int32)\n\n        # Reconstruct the numpy array based on the extracted data type and shape information\n        array = np.frombuffer(data[16:], dtype=data_type)\n        array = array.reshape(shape)\n\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        # Convert the NumPy array into a bytes object\n        serialized_bytes = item.tobytes()\n\n        # Generate the dtype identifier string\n        dtype_identifier = f\"no_header_numpy:{item.dtype.str}\"\n\n        return serialized_bytes, dtype_identifier\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type index of the array\n        data_type_index = self.data_type_index_map[item.dtype.name]\n\n        # Get the number of dimensions of the array\n        num_dimensions = len(item.shape)\n\n        # Get the shape of the array\n        shape = item.shape\n\n        # Get the binary content of the array\n        binary_content = item.tobytes()\n\n        # Create a bytes object to store the serialized data\n        serialized_data = b''\n\n        # Add the data type index to the serialized data\n        serialized_data += data_type_index.to_bytes(1, byteorder='big')\n\n        # Add the number of dimensions to the serialized data\n        serialized_data += num_dimensions.to_bytes(1, byteorder='big')\n\n        # Add the shape of the array to the serialized data\n        for dimension_size in shape:\n            serialized_data += dimension_size.to_bytes(4, byteorder='big')\n\n        # Add the binary content of the array to the serialized data\n        serialized_data += binary_content\n\n        # Return the serialized bytes object and None as the metadata\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {\n            \"dataset\": self.dataset,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not self._check_dependencies():\n            raise Exception(\"The required dependencies (torchvision and av) are not installed. Please install them and try again.\")\n\n        with tempfile.NamedTemporaryFile(mode='wb', delete=False) as f:\n            f.write(data)\n            video = read_video(f.name)\n\n        return video\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return self._chunks\n\n        if self._should_write:\n            self.write_chunk()\n\n        self.write_chunks_index()\n        self._is_done = True\n        return self._chunks\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        # Update the current epoch and the number of samples yielded\n        self.current_epoch = obj[\"epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n\n        # Update the latest worker index\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n\n        # Update the dataset state if applicable\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset_state\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            for dataset in self.dataset.datasets:\n                dataset.load_state_dict(obj[\"dataset_state\"])\n        else:\n            raise RuntimeError(\"The dataset associated with the StreamingDataLoader is neither a StreamingDataset nor a CombinedStreamingDataset.\")\n\n        # Prepare the DataLoader for resuming by adjusting internal iterators and flags\n        self.prepare_for_resuming()\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\"num_samples_yielded\": num_samples_yielded}\n        else:\n            return self._iterator.state_dict(num_workers, batch_size)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Load the state of each dataset within the CombinedStreamingDataset\n        for dataset in self.datasets:\n            dataset.load_state_dict(state_dict)\n\n        # Update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        self.num_samples_yielded = state_dict['num_samples_yielded']\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(path=dir_path)\n        elif dir_path.startswith(\"gs://\"):\n            return Dir(path=dir_path)\n        elif dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n            return Dir(path=dir_path)\n        elif dir_path.startswith(\"file://\"):\n            return Dir(path=dir_path)\n        elif dir_path.startswith(\"~/\"):\n            return Dir(path=os.path.expanduser(dir_path))\n        elif dir_path.startswith(\"/\"):\n            return Dir(path=dir_path)\n        elif dir_path.startswith(\"./\"):\n            return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n        elif dir_path.startswith(\"../\"):\n            return Dir(path=os.path.join(os.getcwd(), dir_path[3:]))\n        elif dir_path.startswith(\"..\"):\n            return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n        elif dir_path.startswith(\"..\"):\n            return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n        elif dir_path.startswith(\"..\"):\n            return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n        elif dir_path.startswith(\"..\"):\n            return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n        elif dir_path.startswith(\"..\"):\n            return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n        elif dir_path.startswith(\"..\"):\n            return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n        elif dir_path.startswith(\"..\"):\n            return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n        elif dir_path.startswith(\"..\"):\n            return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n        elif dir_path."}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    # Check if the output_dir is a Dir object\n    if not isinstance(output_dir, Dir):\n        raise TypeError(f\"The output_dir argument must be an instance of the Dir class. The type of the argument is {type(output_dir)}.\")\n\n    # Check if the output_dir starts with \"s3://\"\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(f\"The output_dir argument must start with 's3://'. The argument starts with {output_dir.path}.\")\n\n    # Check if the output_dir is empty\n    if len(list(output_dir.list_objects())) > 0:\n        raise ValueError(f\"The output_dir argument must be empty. The output_dir argument contains {len(list(output_dir.list_objects()))} objects.\")\n\n    # Check if appending is allowed\n    if append:\n        raise NotImplementedError(\"Appending data to the output_dir is not currently supported.\")\n\n    # Check if overwriting is allowed\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data in the output_dir is not currently supported.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the directory is an S3 bucket directory.\n    if not output_dir.is_s3_bucket_dir():\n        raise ValueError(f\"The directory {output_dir} is not an S3 bucket directory.\")\n\n    # Check if the directory contains an index file.\n    if output_dir.contains_index_file():\n        raise ValueError(f\"The directory {output_dir} already contains an index file.\")\n\n    # Delete all objects within the specified prefix in the bucket.\n    output_dir.delete_objects_in_bucket()\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Check if the node is the master node (rank 0)\n        if node_rank is not None and node_rank != 0:\n            # Wait until the merged index file is available\n            while not os.path.exists(self.merged_index_path):\n                time.sleep(1)\n\n        # If the node is the master node, proceed with the merge\n        if node_rank is None or node_rank == 0:\n            # Create a list of index files to be merged\n            index_files = [f for f in os.listdir(self.cache_dir) if f.endswith(\".index\")]\n\n            # Check if all index files are available\n            if len(index_files) == num_workers:\n                # Merge the index files\n                self.merge_index_files(index_files)\n\n                # Remove the index files\n                for index_file in index_files:\n                    os.remove(os.path.join(self.cache_dir, index_file))\n            else:\n                # Wait for all index files to be available\n                while len(index_files) < num_workers:\n                    time.sleep(1)\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is available\n    if not is_studio_sdk_available():\n        raise Exception(\n            \"The Studio SDK is not available. Please install the Studio SDK to execute the current operator.\"\n        )\n\n    # Check if the Studio API is available\n    if not is_studio_api_available():\n        raise Exception(\n            \"The Studio API is not available. Please install the Studio API to execute the current operator.\"\n        )\n\n    # Check if the Studio API is authenticated\n    if not is_studio_api_authenticated():\n        raise Exception(\n            \"The Studio API is not authenticated. Please authenticate the Studio API to execute the current operator.\"\n        )\n\n    # Check if the Studio API is authorized\n    if not is_studio_api_authorized():\n        raise Exception(\n            \"The Studio API is not authorized. Please authorize the Studio API to execute the current operator.\"\n        )\n\n    # Check if the Studio API is connected\n    if not is_studio_api_connected():\n        raise Exception(\n            \"The Studio API is not connected. Please connect the Studio API to execute the current operator.\"\n        )\n\n    # Check if the Studio API is connected\n    if not is_studio_api_connected():\n        raise Exception(\n            \"The Studio API is not connected. Please connect the Studio API to execute the current operator.\"\n        )\n\n    # Check if the Studio API is connected\n    if not is_studio_api_connected():\n        raise Exception(\n            \"The Studio API is not connected. Please connect the Studio API to execute the current operator.\"\n        )\n\n    # Check if the Studio API is connected\n    if not is_studio_api_connected():\n        raise Exception(\n            \"The Studio API is not connected. Please connect the Studio API to execute the current operator.\"\n        )\n\n    # Check if the Studio API is connected\n    if not is_studio_api_connected():\n        raise Exception(\n            \"The Studio API is not connected. Please connect the Studio API to execute the current operator.\"\n        )\n\n    # Check if the Studio API is connected\n    if not is_studio_api_connected():\n        raise Exception(\n            \"The Studio API is not connected. Please connect the Studio API to execute the current operator.\"\n        )\n\n    # Check if the Studio API is connected\n    if not is_studio_api_connected():\n        raise Exception(\n            \"The Studio API is not connected. Please connect the Studio API to execute"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        self.delete_queue.extend(chunk_indexes)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the index files are available\n        if not self._check_index_files():\n            return None\n\n        # Load the configuration\n        config = ChunksConfig.load(self._cache_dir)\n\n        # Update the instance's configuration\n        self._config = config\n\n        return config\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        self._to_download_queue.extend(chunk_indexes)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration is not set. Please set the configuration before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        # Check if the index is an instance of ChunkedIndex\n        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index must be an instance of ChunkedIndex.\")\n\n        # Check if the index configuration is defined\n        if self.index_config is None:\n            raise Exception(\"The index configuration is not defined.\")\n\n        # Check if the index is valid\n        if not self.index_config.is_valid(index):\n            raise Exception(\"The index is not valid.\")\n\n        # Check if the index is in memory\n        if self.index_config.is_in_memory(index):\n            return self.index_config.get_item(index)\n\n        # Check if the index is in the cache\n        if self.index_config.is_in_cache(index):\n            return self.index_config.get_item(index)\n\n        # Check if the index is in the prefetch queue\n        if self.index_config.is_in_prefetch_queue(index):\n            return self.index_config.get_item(index)\n\n        # Check if the index is in the download queue\n        if self.index_config.is_in_download_queue(index):\n            return self.index_config.get_item(index)\n\n        # Check if the index is in the download queue\n        if self.index_config.is_in_download_queue(index):\n            return self.index_config.get_item(index)\n\n        # Check if the index is in the download queue\n        if self.index_config.is_in_download_queue(index):\n            return self.index_config.get_item(index)\n\n        # Check if the index is in the download queue\n        if self.index_config.is_in_download_queue(index):\n            return self.index_config.get_item(index)\n\n        # Check if the index is in the download queue\n        if self.index_config.is_in_download_queue(index):\n            return self.index_config.get_item(index)\n\n        # Check if the index is in the download queue\n        if self.index_config.is_in_download_queue(index):\n            return self.index_config.get_item(index)\n\n        # Check if the index is in the download queue\n        if self.index_config"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if not is_distributed():\n        return obj\n\n    if not isinstance(key, str):\n        raise TypeError(\"The key must be a string.\")\n\n    if not isinstance(obj, (int, float, str, bool, list, dict, tuple, set, type(None))):\n        raise TypeError(\"The object to be broadcasted must be of a type that can be serialized.\")\n\n    if not isinstance(obj, (int, float, str, bool, list, dict, tuple, set, type(None))):\n        raise TypeError(\"The object to be broadcasted must be of a type that can be serialized.\")\n\n    if not isinstance(obj, (int, float, str, bool, list, dict, tuple, set, type(None))):\n        raise TypeError(\"The object to be broadcasted must be of a type that can be serialized.\")\n\n    if not isinstance(obj, (int, float, str, bool, list, dict, tuple, set, type(None))):\n        raise TypeError(\"The object to be broadcasted must be of a type that can be serialized.\")\n\n    if not isinstance(obj, (int, float, str, bool, list, dict, tuple, set, type(None))):\n        raise TypeError(\"The object to be broadcasted must be of a type that can be serialized.\")\n\n    if not isinstance(obj, (int, float, str, bool, list, dict, tuple, set, type(None))):\n        raise TypeError(\"The object to be broadcasted must be of a type that can be serialized.\")\n\n    if not isinstance(obj, (int, float, str, bool, list, dict, tuple, set, type(None))):\n        raise TypeError(\"The object to be broadcasted must be of a type that can be serialized.\")\n\n    if not isinstance(obj, (int, float, str, bool, list, dict, tuple, set, type(None))):\n        raise TypeError(\"The object to be broadcasted must be of a type that can be serialized.\")\n\n    if not isinstance(obj, (int, float, str, bool, list, dict, tuple, set, type(None))):\n        raise TypeError(\"The object to be broadcasted must be of a type that can be serialized.\")\n\n    if not isinstance(obj, (int, float, str, bool, list, dict, tuple, set, type(None))):"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the number of nodes and the world size\n    num_nodes = distributed_env.num_nodes\n    world_size = distributed_env.world_size\n\n    # Get the number of chunks per node\n    num_chunks_per_node = len(chunks_per_ranks[0])\n\n    # Calculate the number of chunks per rank\n    num_chunks_per_rank = num_chunks_per_node // num_nodes\n\n    # Calculate the number of chunks left over after dividing evenly\n    num_chunks_left_over = num_chunks_per_node % num_nodes\n\n    # Calculate the number of chunks to be assigned to each node\n    num_chunks_per_node_new = num_chunks_per_rank + (1 if i < num_chunks_left_over else 0)\n\n    # Calculate the number of chunks to be assigned to each rank\n    num_chunks_per_rank_new = num_chunks_per_node_new // num_nodes\n\n    # Calculate the number of chunks left over after dividing evenly\n    num_chunks_left_over_new = num_chunks_per_node_new % num_nodes\n\n    # Calculate the number of chunks to be assigned to each node\n    num_chunks_per_node_new = num_chunks_per_rank_new + (1 if i < num_chunks_left_over_new else 0)\n\n    # Calculate the number of chunks to be assigned to each rank\n    num_chunks_per_rank_new = num_chunks_per_node_new // num_nodes\n\n    # Calculate the number of chunks left over after dividing evenly\n    num_chunks_left_over_new = num_chunks_per_node_new % num_nodes\n\n    # Calculate the number of chunks to be assigned to each node\n    num_chunks_per_node_new = num_chunks_per_rank_new + (1 if i < num_chunks_left_over_new else 0)\n\n    # Calculate the number of chunks to be assigned to each rank\n    num_chunks_per_rank_new = num_chunks_per_node_new // num_nodes\n\n    # Calculate the number of chunks left over after dividing evenly\n    num_chunks_left_over_new = num_chunks_per_node_new %"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Get the first two inputs\n    inputs_0 = inputs[0]\n    inputs_1 = inputs[1]\n\n    # Check if the inputs are valid file paths\n    if not isinstance(inputs_0, str) or not isinstance(inputs_1, str):\n        raise ValueError(f\"Inputs must be file paths, but inputs_0 is {type(inputs_0)} and inputs_1 is {type(inputs_1)}.\")\n\n    # Check if the inputs are consistent file paths\n    if os.path.abspath(inputs_0) != os.path.abspath(inputs_1):\n        raise ValueError(f\"Inputs must be consistent file paths, but inputs_0 is {inputs_0} and inputs_1 is {inputs_1}.\")\n\n    # Get the input directory\n    input_dir = os.path.abspath(inputs_0)\n\n    # Return the input directory\n    return input_dir\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Get the current DNS optimization setting\n    current_setting = get_dns_optimization_setting()\n\n    # Enable or disable DNS optimization based on the input parameter\n    if enable:\n        set_dns_optimization_setting(True)\n    else:\n        set_dns_optimization_setting(False)\n\n    # Ensure DNS optimization is always disabled after the context\n    try:\n        yield\n    finally:\n        set_dns_optimization_setting(current_setting)\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Get the world size and the number of items\n    world_size = distributed_env.world_size\n    num_items = len(indexes)\n\n    # Calculate the number of items each rank should process\n    num_items_per_rank = num_items // world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank - 1\n\n    # Distribute the chunks and their intervals across the ranks\n    chunk_indexes_per_rank = []\n    chunk_intervals_per_rank = []\n    for rank in range(world_size):\n        chunk_indexes_per_rank.append([])\n        chunk_intervals_per_rank.append([])\n        for i in range(num_items_per_rank):\n            chunk_indexes_per_rank[rank].append(indexes[rank * num_items_per_rank + i])\n            chunk_intervals_per_rank[rank].append(chunk_intervals[rank * num_items_per_rank + i])\n        if rank < num_items % world_size:\n            chunk_indexes_per_rank[rank].append(indexes[-1])\n            chunk_intervals_per_rank[rank].append(chunk_intervals[-1])\n\n    return chunk_indexes_per_rank, chunk_intervals_per_rank\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, Sequence):\n        raise TypeError(f\"inputs must be a sequence, but got {type(inputs)}\")\n\n    if not isinstance(output_dir, str):\n        raise TypeError(f\"output_dir must be a string, but got {type(output_dir)}\")\n\n    if not isinstance(weights, (list, tuple, type(None))):\n        raise TypeError(f\"weights must be a list, tuple, or None, but got {type(weights)}\")\n\n    if not isinstance(chunk_size, (int, type(None))):\n        raise TypeError(f\"chunk_size must be an integer or None, but got {type(chunk_size)}\")\n\n    if not isinstance(chunk_bytes, (int, str, type(None))):\n        raise TypeError(f\"chunk_bytes must be an integer, string, or None, but got {type(chunk_bytes)}\")\n\n    if not isinstance(compression, (str, type(None))):\n        raise TypeError(f\"compression must be a string or None, but got {type(compression)}\")\n\n    if not isinstance(num_workers, (int, type(None))):\n        raise TypeError(f\"num_workers must be an integer or None, but got {type(num_workers)}\")\n\n    if not isinstance(fast_dev_run, bool):\n        raise TypeError(f\"fast_dev_run must be a boolean, but got {type(fast_dev_run)}\")\n\n    if not isinstance(num_nodes, (int, type(None))):\n        raise TypeError(f\"num_nodes must be an integer or None, but got {type(num_nodes)}\")\n\n    if not isinstance(machine, (str, type(None))):\n        raise TypeError(f\"machine must be a string or None, but got {type(machine)}\")\n\n    if not isinstance(num_downloaders, (int, type(None))):\n        raise TypeError(f\"num_downloaders must be an integer or None, but got {type(num_downloaders)}\")\n\n    if not isinstance(num_uploaders, (int, type(None))):\n        raise TypeError(f\"num_uploaders must be an integer or None, but got {type(num_uploaders)}\")\n\n    if not isinstance(reorder_files, bool"}
{"namespace": "litdata.processing.functions.map", "completion": "    if not isinstance(inputs, Sequence):\n        raise TypeError(f\"Expected a sequence of inputs, got {type(inputs)}\")\n\n    if not isinstance(output_dir, (str, Dir)):\n        raise TypeError(f\"Expected a string or Dir object for output_dir, got {type(output_dir)}\")\n\n    if not isinstance(weights, (list, tuple, type(None))):\n        raise TypeError(f\"Expected a list or tuple for weights, got {type(weights)}\")\n\n    if not isinstance(num_workers, (int, type(None))):\n        raise TypeError(f\"Expected an integer for num_workers, got {type(num_workers)}\")\n\n    if not isinstance(fast_dev_run, (bool, int)):\n        raise TypeError(f\"Expected a boolean or integer for fast_dev_run, got {type(fast_dev_run)}\")\n\n    if not isinstance(num_nodes, (int, type(None))):\n        raise TypeError(f\"Expected an integer for num_nodes, got {type(num_nodes)}\")\n\n    if not isinstance(machine, (str, type(None))):\n        raise TypeError(f\"Expected a string for machine, got {type(machine)}\")\n\n    if not isinstance(num_downloaders, (int, type(None))):\n        raise TypeError(f\"Expected an integer for num_downloaders, got {type(num_downloaders)}\")\n\n    if not isinstance(num_uploaders, (int, type(None))):\n        raise TypeError(f\"Expected an integer for num_uploaders, got {type(num_uploaders)}\")\n\n    if not isinstance(reorder_files, bool):\n        raise TypeError(f\"Expected a boolean for reorder_files, got {type(reorder_files)}\")\n\n    if not isinstance(error_when_not_empty, bool):\n        raise TypeError(f\"Expected a boolean for error_when_not_empty, got {type(error_when_not_empty)}\")\n\n    if not isinstance(reader, (BaseReader, type(None))):\n        raise TypeError(f\"Expected a BaseReader or None for reader, got {type(reader)}\")\n\n    if not isinstance(batch_size, (int, type(None))):\n        raise TypeError(f\"Expected an integer for batch_size,"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Create a list of file paths to download\n    file_paths = []\n    for file_name in os.listdir(input_dir):\n        file_path = os.path.join(input_dir, file_name)\n        file_paths.append(file_path)\n\n    # Create a list of file paths to download\n    file_paths = []\n    for file_name in os.listdir(input_dir):\n        file_path = os.path.join(input_dir, file_name)\n        file_paths.append(file_path)\n\n    # Create a list of file paths to download\n    file_paths = []\n    for file_name in os.listdir(input_dir):\n        file_path = os.path.join(input_dir, file_name)\n        file_paths.append(file_path)\n\n    # Create a list of file paths to download\n    file_paths = []\n    for file_name in os.listdir(input_dir):\n        file_path = os.path.join(input_dir, file_name)\n        file_paths.append(file_path)\n\n    # Create a list of file paths to download\n    file_paths = []\n    for file_name in os.listdir(input_dir):\n        file_path = os.path.join(input_dir, file_name)\n        file_paths.append(file_path)\n\n    # Create a list of file paths to download\n    file_paths = []\n    for file_name in os.listdir(input_dir):\n        file_path = os.path.join(input_dir, file_name)\n        file_paths.append(file_path)\n\n    # Create a list of file paths to download\n    file_paths = []\n    for file_name in os.listdir(input_dir):\n        file_path = os.path.join(input_dir, file_name)\n        file_paths.append(file_path)\n\n    # Create a list of file paths to download\n    file_paths = []\n    for file_name in os.listdir(input_dir):\n        file_path = os.path.join(input_dir, file_name)\n        file_paths.append(file_path)\n\n    # Create a list of file paths to download\n    file_paths = []\n    for file_"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # Get the output directory's scheme\n    scheme = output_dir.scheme\n\n    # If the output directory is a local directory, use the local filesystem\n    if scheme == \"file\":\n\n        # Get the output directory's path\n        path = output_dir.path\n\n        # Create a local filesystem client\n        client = LocalFileSystemClient()\n\n        # Create a local filesystem directory\n        if not client.exists(path):\n            client.mkdir(path)\n\n        # Create a local filesystem directory for temporary files\n        if not client.exists(os.path.join(path, \"tmp\")):\n            client.mkdir(os.path.join(path, \"tmp\"))\n\n        # Create a local filesystem directory for uploaded files\n        if not client.exists(os.path.join(path, \"uploaded\")):\n            client.mkdir(os.path.join(path, \"uploaded\"))\n\n        # Create a local filesystem directory for failed uploads\n        if not client.exists(os.path.join(path, \"failed\")):\n            client.mkdir(os.path.join(path, \"failed\"))\n\n        # Create a local filesystem directory for failed uploads\n        if not client.exists(os.path.join(path, \"removed\")):\n            client.mkdir(os.path.join(path, \"removed\"))\n\n        # Create a local filesystem directory for failed uploads\n        if not client.exists(os.path.join(path, \"removed\")):\n            client.mkdir(os.path.join(path, \"removed\"))\n\n        # Create a local filesystem directory for failed uploads\n        if not client.exists(os.path.join(path, \"removed\")):\n            client.mkdir(os.path.join(path, \"removed\"))\n\n        # Create a local filesystem directory for failed uploads\n        if not client.exists(os.path.join(path, \"removed\")):\n            client.mkdir(os.path.join(path, \"removed\"))\n\n        # Create a local filesystem directory for failed uploads\n        if not client.exists(os.path.join(path, \"removed\")):\n            client.mkdir(os.path.join(path, \"removed\"))\n\n        # Create a local filesystem directory for failed uploads\n        if not client.exists(os.path.join(path, \"removed\")):\n            client.mkdir(os.path.join(path, \"removed\"))\n\n        # Create a"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes\n    num_workers_total = num_nodes * num_workers\n\n    # Distribute items to workers based on provided weights\n    worker_items = []\n    worker_weights = []\n    for i in range(num_workers_total):\n        worker_items.append([])\n        worker_weights.append(0)\n\n    for i, item in enumerate(user_items):\n        # Get the worker with the highest weight\n        worker_id = worker_ids_this_node[worker_weights.index(max(worker_weights))]\n\n        # Add the item to the worker's list\n        worker_items[worker_id].append(item)\n        worker_weights[worker_id] += weights[i]\n\n    # Shuffle the items for each worker\n    for worker_id in worker_ids_this_node:\n        random.shuffle(worker_items[worker_id])\n\n    # Print the distribution details for workers on the current node\n    print(f\"Worker distribution on node {node_rank} (out of {num_nodes}):\")\n    for worker_id in worker_ids_this_node:\n        print(f\"Worker {worker_id}: {len(worker_items[worker_id])} items\")\n\n    return worker_items, worker_weights\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of nodes in the environment\n    num_nodes = _get_num_nodes()\n\n    # Calculate the total number of workers across all nodes\n    total_workers = num_nodes * num_workers\n\n    # Calculate the number of items each worker should process\n    items_per_worker = len(user_items) // total_workers\n\n    # Calculate the remainder\n    remainder = len(user_items) % total_workers\n\n    # Create a list to store the items assigned to each worker\n    worker_items = [[] for _ in range(total_workers)]\n\n    # Distribute the items to the workers\n    for i in range(total_workers):\n        # Calculate the start and end indices for the current worker\n        start_index = i * items_per_worker\n        end_index = start_index + items_per_worker - 1\n\n        # Handle the remainder\n        if i < remainder:\n            end_index += 1\n\n        # Assign the items to the worker\n        worker_items[i] = user_items[start_index:end_index + 1]\n\n    # Check if the output list has the expected length\n    if len(worker_items) != total_workers:\n        raise RuntimeError(f\"Improper assignment of items to workers. Expected {total_workers} workers, but got {len(worker_items)}.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Remove the cache directory if it exists\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n\n        # Recreate the cache directory\n        os.makedirs(self.cache_dir)\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n        return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        if not isinstance(input_dir, str):\n            raise TypeError(f\"input_dir must be a string, not {type(input_dir)}\")\n        if not os.path.isdir(input_dir):\n            raise ValueError(f\"input_dir {input_dir} does not exist\")\n        input_dir = str(Path(input_dir).absolute())\n        if not element.startswith(input_dir):\n            raise ValueError(f\"element {element} does not start with input_dir {input_dir}\")\n        element = str(Path(element).absolute())\n\n    \"\"\"\n    Determines if a given element is a path that exists or starts with a specified input directory. It specifically checks if the element is a string that represents a path within the input directory or if it exists in the file system. This function also handles absolute path conversion when necessary.\n\n    Input-Output Arguments\n    :param input_dir: Optional[str]. The base directory against which the element is checked. It is used to determine if the element is a subpath of this directory.\n    :param element: Any. The element to be checked if it's a path. It is converted to a string and checked against the input directory or the file system for existence.\n    :return: Bool. Indicates whether the element is a valid path that exists or starts with the specified input directory.\n    \"\"\"\n    if input_dir is not None:\n        if not isinstance(input_dir, str):\n            raise TypeError(f\"input_dir must be a string, not {type(input_dir)}\")\n        if not os.path.isdir(input_dir):\n            raise ValueError(f\"input_dir {input_dir} does not exist\")\n        input_dir = str(Path(input_dir).absolute())\n        if not element.startswith(input_dir):\n            raise ValueError(f\"element {element} does not start with input_dir {input_dir}\")\n        element = str(Path(element).absolute())\n\n    \"\"\"\n    Determines if a given element is a path that exists or starts with a specified input directory. It specifically checks if the element is a string that represents a path within the input directory or if it exists in the file system. This function also handles absolute path conversion when necessary.\n\n    Input-Output Arguments\n    :param input_dir: Optional[str"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"The number of layers must be greater than 0.\"\n        assert n_neurons > 0, \"The number of neurons must be greater than 0.\"\n\n        if self.tcnn:\n            if n_neurons <= 100:\n                network = tinycudann.Network(n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation)\n            else:\n                network = tinycudann.Network(n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation, True)\n        else:\n            network = nn.Sequential()\n            network.add_module(\"input\", nn.Linear(n_input_dims, n_neurons))\n            network.add_module(\"hidden\", nn.Linear(n_neurons, n_neurons))\n            network.add_module(\"output\", nn.Linear(n_neurons, n_output_dims))\n            if activation == \"ReLU\":\n                network.add_module(\"activation\", nn.ReLU())\n            elif activation == \"None\":\n                pass\n            else:\n                raise ValueError(\"Invalid activation function.\")\n            if output_activation == \"ReLU\":\n                network.add_module(\"output_activation\", nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                network.add_module(\"output_activation\", nn.Sigmoid())\n            elif output_activation == \"None\":\n                pass\n            else:\n                raise ValueError(\"Invalid output activation function.\")\n\n        return network\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the rolling median of the signal by shifting it by a range defined by the kernel offset, computing the median of these shifted signals, and then trimming the resulting median array to account for edge effects introduced by the shifting process.\n        rolling_median = np.array([np.median(signal[i : i + kernel_offset * 2]) for i in range(len(signal) - kernel_offset * 2)])\n\n        # Trim the rolling median array to account for edge effects introduced by the shifting process.\n        rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return rolling_median\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance between the two templates\n    hamming_dist = template_probe.hamming_distance(template_gallery, rotation_shift)\n\n    # Calculate the minimum Hamming distance and the corresponding rotation shift\n    min_hamming_dist = np.min(hamming_dist)\n    min_hamming_shift = np.argmin(hamming_dist)\n\n    # Calculate the normalized Hamming distance\n    if nm_dist is not None:\n        norm_hamming_dist = min_hamming_dist / nm_dist\n    else:\n        norm_hamming_dist = None\n\n    # Calculate the weighted Hamming distance\n    if weights is not None:\n        weight_hamming_dist = np.sum(weights * hamming_dist)\n    else:\n        weight_hamming_dist = None\n\n    # Return the minimum Hamming distance and the corresponding rotation shift\n    return min_hamming_dist, min_hamming_shift, norm_hamming_dist, weight_hamming_dist\n\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize the number of iterations and the number of points that meet the distance criterion\n        iterations = 0\n        num_points_that_meet_criterion = 0\n\n        # Initialize the starting and ending points of the perpendicular bisectors\n        starting_points = np.zeros((self.num_bisectors, 2))\n        ending_points = np.zeros((self.num_bisectors, 2))\n\n        # Loop until the number of points that meet the distance criterion reaches the desired number\n        while num_points_that_meet_criterion < self.num_bisectors:\n\n            # Randomly select two points from the polygon\n            point_1 = polygon[np.random.randint(0, polygon.shape[0])]\n            point_2 = polygon[np.random.randint(0, polygon.shape[0])]\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point_1 - point_2)\n\n            # Check if the distance between the two points is greater than the minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n\n                # If the distance is greater than the minimum, add the points to the starting and ending points arrays\n                starting_points[num_points_that_meet_criterion] = point_1\n                ending_points[num_points_that_meet_criterion] = point_2\n\n                # Increment the number of points that meet the distance criterion\n                num_points_that_meet_criterion += 1\n\n            # Increment the number of iterations\n            iterations += 1\n\n            # If the number of iterations exceeds the maximum allowed, raise an exception\n            if iterations > self.max_iterations:\n                raise EyeCentersEstimationError(\n                    \"The function failed to find a sufficient number of point pairs that meet the distance criterion within the maximum number of iterations allowed.\"\n                )\n\n        # Return the starting and ending points of the perpendicular bisectors\n        return starting_points, ending_points\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        # Execute pre-execution hooks\n        for callback in self._callbacks[\"pre\"]:\n            callback(self, *args, **kwargs)\n\n        # Run the algorithm's main run method\n        result = self.run(*args, **kwargs)\n\n        # Execute post-execution hooks\n        for callback in self._callbacks[\"post\"]:\n            callback(self, *args, **kwargs)\n\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get the function's signature and type hints\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Get the function's name and docstring\n        name = func_object.__name__\n        docstring = func_object.__doc__\n\n        # Initialize the function description\n        function_description = FunctionDescription(name=name, docstring=docstring)\n\n        # Iterate over the function's type hints\n        for parameter_name, type_hint in type_hints.items():\n            # Get the parameter's type and default value\n            parameter_type = type_hint\n            parameter_default = signature.parameters[parameter_name].default\n\n            # Check if the parameter is an input or output\n            if parameter_name in signature.parameters:\n                parameter_type = get_class_definition(type_hint)\n                function_description.add_input(parameter_name, parameter_type, parameter_default)\n            else:\n                parameter_type = get_class_definition(type_hint)\n                function_description.add_output(parameter_name, parameter_type, parameter_default)\n\n        # Check if the output type hint is a class or a subclass of a Union\n        if isinstance(type_hints[\"return\"], Union):\n            # Get the output class definition\n            output_class_definition = get_class_definition(type_hints[\"return\"])\n            function_description.set_output_class_definition(output_class_definition)\n\n            # Check if the output type hint is a subclass of an Embedding class\n            if issubclass(type_hints[\"return\"], Embedding):\n                function_description.set_function_type(FunctionDescription.FUNCTION_TYPE_EMBEDDABLE)\n            else:\n                function_description.set_function_type(FunctionDescription.FUNCTION_TYPE_SYMBOLIC)\n        else:\n            # Set the function type to symbolic\n            function_description.set_function_type(FunctionDescription.FUNCTION_TYPE_SYMBOLIC)\n\n        return function_description\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for i in range(self.k):\n            index = self.hash(string, i) % self.m\n            self.bit_array[index] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            with open(self.persistence, 'rb') as f:\n                self.bit_array = pickle.load(f)\n        except:\n            self.init_bit_array()\n            self.save()\n            logging.warning(\"Loaded bit array is corrupted. Reinitialized bit array and saved.\")\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Generate indices for the string using the hash functions\n        indices = [hash_function(string, i, self.size) for i in range(self.hash_count)]\n\n        # Check if all bits at the indices are set in the bit array\n        for index in indices:\n            if not self.bit_array[index]:\n                return False\n\n        # If all bits are set, the string is possibly in the Bloom Filter\n        return True\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if not self.api_key:\n            raise ValueError(\"API key not set. Please set the API key using the set_api_key() method.\")\n\n        # Validate model\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration. Please provide a valid OpenAIConfig instance.\")\n\n        # Validate system message\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message. Please provide a string.\")\n\n        # Validate prompt\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt. Please provide a string.\")\n\n        # Validate additional parameters\n        for key, value in kwargs.items():\n            if not isinstance(key, str):\n                raise ValueError(f\"Invalid parameter name: {key}. Please provide a string.\")\n            if not isinstance(value, (int, float)):\n                raise ValueError(f\"Invalid parameter value: {value}. Please provide an integer or float.\")\n\n        # Generate response\n        response = self.generate_response(model, system_message, prompt, **kwargs)\n\n        # Remove parsing helper tokens\n        response = response.replace(model.start_token, \"\")\n        response = response.replace(model.end_token, \"\")\n\n        return response\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n    # Check if the matrix is symmetric\n    if not np.array_equal(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert x.shape[0] == x.shape[1], \"The matrix is not a square matrix.\"\n    assert np.allclose(x, x.T), \"The matrix is not symmetric.\"\n    assert np.allclose(np.diag(x), 0), \"The diagonal elements of the matrix are not close to zero.\"\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # Initialize the function\n            self.initialize_function(func_hash)\n\n        # Check if the function requires saving examples for fine-tuning\n        if self.initialized_functions[func_hash][\"needs_examples\"]:\n            # Save the examples for fine-tuning\n            self.save_examples(func_hash)\n\n        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if self.initialized_functions[func_hash][\"needs_examples\"]:\n            # Return the prompt, model, and flags indicating that the function is not initialized and does not require saving examples for fine-tuning\n            return self.initialized_functions[func_hash][\"prompt\"], self.initialized_functions[func_hash][\"model\"], False, False\n\n        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if self.initialized_functions[func_hash][\"needs_examples\"]:\n            # Return the prompt, model, and flags indicating that the function is not initialized and does not require saving examples for fine-tuning\n            return self.initialized_functions[func_hash][\"prompt\"], self.initialized_functions[func_hash][\"model\"], False, False\n\n        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if self.initialized_functions[func_hash][\"needs_examples\"]:\n            # Return the prompt, model, and flags indicating that the function is not initialized and does not require saving examples for fine-tuning\n            return self.initialized_functions[func_hash][\"prompt\"], self.initialized_functions[func_hash][\"model\"], False, False\n\n        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if self.initialized_functions[func_hash][\"needs_examples\"]:\n            # Return the prompt, model, and flags indicating that the function is not initialized and does not require saving examples for fine-tuning\n            return self.initialized_functions[func_hash][\"prompt\"], self.initialized_functions[func_hash][\"model\"], False, False\n\n        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if self.initialized_functions[func_hash][\"needs_examples\"]:\n            # Return the prompt, model, and flags indicating that the function"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    # Check if the input matrix is positive definite\n    if np.all(np.linalg.eigvals(cov) > 0):\n        return cov\n\n    # Compute the nearest positive definite matrix using the Higham & Nick (2002) algorithm\n    if higham:\n        cov = higham_nearest(cov, max_iteration=higham_max_iteration)\n\n    # Compute the nearest positive definite matrix by clipping eigenvalues\n    else:\n        # Compute the eigenvalues and eigenvectors of the input matrix\n        eigenvalues, eigenvectors = np.linalg.eig(cov)\n\n        # Clip the eigenvalues to ensure the resulting matrix is positive definite\n        eigenvalues[eigenvalues < 0] = 0\n\n        # Compute the nearest positive definite matrix\n        cov = np.dot(eigenvectors, np.dot(np.diag(eigenvalues), eigenvectors.T))\n\n    return cov\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    if data_home is None:\n        data_home = get_data_home()\n\n    if not os.path.exists(data_home):\n        return\n\n    for root, dirs, files in os.walk(data_home):\n        for file in files:\n            os.remove(os.path.join(root, file))\n        for dir in dirs:\n            shutil.rmtree(os.path.join(root, dir))\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj, lambda x: x)\n\n    if isinstance(obj, (list, tuple)):\n        return (tuple(flatten_to_tuple(x) for x in obj), lambda x: type(obj)(flatten_to_tuple(x) for x in x))\n\n    if isinstance(obj, dict):\n        return (tuple(flatten_to_tuple(x) for x in obj.items()), lambda x: dict(flatten_to_tuple(x) for x in x))\n\n    if isinstance(obj, (Box, BoxList, RotatedBox, RotatedBoxList)):\n        return (tuple(flatten_to_tuple(x) for x in obj.tensor.tolist()), lambda x: obj.from_tensor(torch.tensor(x)))\n\n    if isinstance(obj, (Mask, MaskList)):\n        return (tuple(flatten_to_tuple(x) for x in obj.tensor.tolist()), lambda x: obj.from_tensor(torch.tensor(x)))\n\n    if isinstance(obj, (Instance, Instances)):\n        return (tuple(flatten_to_tuple(x) for x in obj.get_fields().values()), lambda x: obj.from_dict(dict(flatten_to_tuple(x) for x in x)))\n\n    if isinstance(obj, (BoxList, RotatedBoxList, MaskList, Instances)):\n        return (tuple(flatten_to_tuple(x) for x in obj.bbox.tolist()), lambda x: obj.from_tensor(torch.tensor(x)))\n\n    if isinstance(obj, (Box, RotatedBox)):\n        return (tuple(flatten_to_tuple(x) for x in obj.tolist()), lambda x: obj.from_tensor(torch.tensor(x)))\n\n    if isinstance(obj, (Mask,)):\n        return (tuple(flatten_to_tuple(x) for x in obj.tensor.tolist()), lambda x: obj.from_tensor(torch.tensor(x)))\n\n    if isinstance(obj, (int, float, bool)):\n        return (obj, lambda x: x)\n\n    if isinstance(obj, (torch.Tensor, torch.nn.Parameter)):\n        return (obj.tolist(), lambda x: torch.tensor(x))\n\n    if isinstance(obj, (np.ndarray"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    if groups.ndim != 2:\n        raise ValueError(f\"The {names[0]} parameter must be a 2D array, but it is {groups.ndim}D.\")\n\n    if equations.ndim != 1:\n        raise ValueError(f\"The {names[1]} parameter must be a 1D array, but it is {equations.ndim}D.\")\n\n    if groups.shape[1] != equations.shape[0]:\n        raise ValueError(\n            f\"The number of columns in the {names[0]} parameter must match the number of rows in the {names[1]} parameter.\"\n        )\n\n    if sum_to_one:\n        groups = groups / groups.sum(axis=1, keepdims=True)\n\n    groups_set = set(groups.flatten())\n    equations_set = set(re.findall(r\"([a-zA-Z0-9_]+)\", \" \".join(equations)))\n\n    if not groups_set.issuperset(equations_set):\n        if raise_if_group_missing:\n            raise ValueError(\n                f\"The {names[0]} parameter must contain all groups mentioned in the {names[1]} parameter, but it does not.\"\n            )\n        else:\n            warnings.warn(\n                f\"The {names[0]} parameter must contain all groups mentioned in the {names[1]} parameter, but it does not. A warning is issued instead.\"\n            )\n            return None, None\n\n    groups_dict = {group: i for i, group in enumerate(groups_set)}\n    equations_dict = {\n        equation: np.array([groups_dict[group] for group in re.findall(r\"([a-zA-Z0-9_]+)\", equation)])\n        for equation in equations\n    }\n\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n\n    for i, equation in enumerate(equations):\n        left[i, equations_dict[equation]] = 1\n        right[i] = 1\n\n    return left, right\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # Create a temporary file to write the new class to\n    temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False)\n\n    # Write the new class to the temporary file\n    temp_file.write(\n        f\"\"\""}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Read the image using PIL\n    image = Image.open(file_name)\n\n    # Apply any necessary orientation corrections based on the image's EXIF data\n    if hasattr(image, '_getexif'):\n        exif_data = image._getexif()\n        if exif_data is not None:\n            orientation = exif_data.get(0x0112, None)\n            if orientation is not None:\n                if orientation == 3:\n                    image = image.rotate(180, expand=True)\n                elif orientation == 6:\n                    image = image.rotate(270, expand=True)\n                elif orientation == 8:\n                    image = image.rotate(90, expand=True)\n\n    # Convert the image to the specified format\n    if format is not None:\n        if format == \"BGR\":\n            image = image.convert(\"RGB\")\n        elif format == \"YUV-BT.601\":\n            image = image.convert(\"YCbCr\")\n            image = np.array(image)\n            image = image[:, :, 0] / 255.0\n        else:\n            image = image.convert(format)\n\n    # Convert the image to a numpy array\n    image = np.array(image)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Apply transformations to bounding box\n    if \"bbox\" in annotation:\n        annotation[\"bbox\"] = transforms.apply_box(annotation[\"bbox\"], image_size)\n\n    # Apply transformations to segmentation\n    if \"segmentation\" in annotation:\n        if \"bbox\" in annotation:\n            # If bounding box is present, clip segmentation to bounding box\n            annotation[\"segmentation\"] = transforms.apply_segmentation(\n                annotation[\"segmentation\"], annotation[\"bbox\"]\n            )\n        else:\n            # If bounding box is not present, clip segmentation to image size\n            annotation[\"segmentation\"] = transforms.apply_segmentation(\n                annotation[\"segmentation\"], image_size\n            )\n\n    # Apply transformations to keypoints\n    if \"keypoints\" in annotation:\n        if keypoint_hflip_indices is not None:\n            # If keypoint_hflip_indices is provided, apply horizontal flip transformations to keypoints\n            annotation[\"keypoints\"] = transforms.apply_keypoints(\n                annotation[\"keypoints\"], keypoint_hflip_indices\n            )\n        else:\n            # If keypoint_hflip_indices is not provided, apply transformations to keypoints without horizontal flip\n            annotation[\"keypoints\"] = transforms.apply_keypoints(\n                annotation[\"keypoints\"]\n            )\n\n    # Set bbox_mode to XYXY_ABS\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0:\n            return coords\n        elif self.angle % 360 == 0:\n            return coords\n        else:\n            return np.dot(coords, self.rm_coords)\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Create a dummy input to the model\n    dummy_input = torch.randn(1, 3, 224, 224)\n\n    # Create a dummy output to the model\n    dummy_output = model(dummy_input)\n\n    # Create a dummy input to the model\n    dummy_input = torch.randn(1, 3, 224, 224)\n\n    # Create a dummy output to the model\n    dummy_output = model(dummy_input)\n\n    # Create a dummy input to the model\n    dummy_input = torch.randn(1, 3, 224, 224)\n\n    # Create a dummy output to the model\n    dummy_output = model(dummy_input)\n\n    # Create a dummy input to the model\n    dummy_input = torch.randn(1, 3, 224, 224)\n\n    # Create a dummy output to the model\n    dummy_output = model(dummy_input)\n\n    # Create a dummy input to the model\n    dummy_input = torch.randn(1, 3, 224, 224)\n\n    # Create a dummy output to the model\n    dummy_output = model(dummy_input)\n\n    # Create a dummy input to the model\n    dummy_input = torch.randn(1, 3, 224, 224)\n\n    # Create a dummy output to the model\n    dummy_output = model(dummy_input)\n\n    # Create a dummy input to the model\n    dummy_input = torch.randn(1, 3, 224, 224)\n\n    # Create a dummy output to the model\n    dummy_output = model(dummy_input)\n\n    # Create a dummy input to the model\n    dummy_input = torch.randn(1, 3, 224, 224)\n\n    # Create a dummy output to the model\n    dummy_output = model(dummy_input)\n\n    # Create a dummy input to the model\n    dummy_input = torch.randn(1, 3, 224, 224)\n\n    # Create a dummy output to the model\n    dummy_output = model(dummy_input)\n\n    # Create a dummy input to the model\n    dummy_input = torch.randn(1, 3,"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.size == 0:\n            return img\n\n        if self.angle == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        # Get the image dimensions\n        h, w, _ = img.shape\n\n        # Get the bounding dimensions\n        bound_w, bound_h = self.bound_dims\n\n        # Get the rotation matrix\n        rm_image = self.rm_image\n\n        # Rotate the image\n        rotated_img = cv2.warpAffine(img, rm_image, (bound_w, bound_h), flags=interp)\n\n        return rotated_img\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Get the image from the input\n        image = self.image\n\n        # Get the predictions from the input\n        pred_boxes = predictions.pred_boxes\n        pred_classes = predictions.pred_classes\n        scores = predictions.scores\n        pred_masks = predictions.pred_masks\n        pred_masks_rle = predictions.pred_masks_rle\n        pred_keypoints = predictions.pred_keypoints\n\n        # Get the number of instances in the image\n        num_instances = len(pred_boxes)\n\n        # Get the number of classes in the dataset\n        num_classes = self.metadata.get(\"thing_classes\", None)\n\n        # Get the class names from the metadata\n        class_names = self.metadata.get(\"thing_classes\", None)\n\n        # Get the colors for the classes\n        colors = self.metadata.get(\"thing_colors\", None)\n\n        # Get the instance mode\n        instance_mode = self.metadata.get(\"instance_mode\", None)\n\n        # Get the instance segmentation colors\n        instance_colors = self.metadata.get(\"instance_colors\", None)\n\n        # Get the instance segmentation masks\n        instance_masks = self.metadata.get(\"instance_masks\", None)\n\n        # Get the instance segmentation masks RLE\n        instance_masks_rle = self.metadata.get(\"instance_masks_rle\", None)\n\n        # Get the instance segmentation keypoints\n        instance_keypoints = self.metadata.get(\"instance_keypoints\", None)\n\n        # Get the instance segmentation keypoints colors\n        instance_keypoints_colors = self.metadata.get(\"instance_keypoints_colors\", None)\n\n        # Get the instance segmentation keypoints labels\n        instance_keypoints_labels = self.metadata.get(\"instance_keypoints_labels\", None)\n\n        # Get the instance segmentation keypoints labels colors\n        instance_keypoints_labels_colors = self.metadata.get(\"instance_keypoints_labels_colors\", None)\n\n        # Get the instance segmentation keypoints labels font size\n        instance_keypoints_labels_font_size = self.metadata.get(\"instance_keypoints_labels_font_size\", None)\n\n        # Get the instance segmentation keypoints labels font\n        instance_keypoints_labels_font = self.metadata.get(\"instance_keypoints_labels_font\", None)\n\n        # Get the instance segmentation key"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the visualized image from the canvas\n        image = self.canvas.get_image()\n\n        # Convert the image from RGBA to RGB format\n        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n\n        # Return the visualized image\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Get the image from the dictionary\n        image = dic[\"image\"]\n\n        # Get the annotations from the dictionary\n        annotations = dic[\"annotations\"]\n\n        # Get the semantic segmentation from the dictionary\n        semantic_segmentation = dic[\"sem_seg\"]\n\n        # Get the panoptic segmentation from the dictionary\n        panoptic_segmentation = dic[\"panoptic_seg\"]\n\n        # Get the keypoints from the dictionary\n        keypoints = dic[\"keypoints\"]\n\n        # Get the bounding boxes from the dictionary\n        bounding_boxes = dic[\"bbox\"]\n\n        # Get the masks from the dictionary\n        masks = dic[\"masks\"]\n\n        # Get the number of instances in the image\n        num_instances = len(annotations)\n\n        # Get the number of keypoints in the image\n        num_keypoints = len(keypoints)\n\n        # Get the number of bounding boxes in the image\n        num_bounding_boxes = len(bounding_boxes)\n\n        # Get the number of masks in the image\n        num_masks = len(masks)\n\n        # Get the number of semantic segmentation classes in the image\n        num_semantic_classes = len(semantic_segmentation)\n\n        # Get the number of panoptic segmentation classes in the image\n        num_panoptic_classes = len(panoptic_segmentation)\n\n        # Get the number of panoptic segmentation segments in the image\n        num_panoptic_segments = len(panoptic_segmentation)\n\n        # Get the number of panoptic segmentation segments in the image\n        num_panoptic_segments = len(panoptic_segmentation)\n\n        # Get the number of panoptic segmentation segments in the image\n        num_panoptic_segments = len(panoptic_segmentation)\n\n        # Get the number of panoptic segmentation segments in the image\n        num_panoptic_segments = len(panoptic_segmentation)\n\n        # Get the number of panoptic segmentation segments in the image\n        num_panoptic_segments = len(panoptic_segmentation)\n\n        # Get the number of panoptic segmentation segments in the image\n        num_panoptic_segments = len(panoptic_segmentation)\n\n        # Get the number of panoptic segmentation segments in the image\n        num_panoptic_segments = len(panoptic_segmentation)\n\n        # Get the number of panoptic segmentation segments in the image\n        num"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # Check if the binary mask is a valid array\n        if not isinstance(binary_mask, np.ndarray):\n            raise TypeError(\"The binary mask must be a numpy array.\")\n\n        # Check if the binary mask is of uint8 type\n        if binary_mask.dtype != np.uint8:\n            raise TypeError(\"The binary mask must be of uint8 type.\")\n\n        # Check if the binary mask is of shape (H, W)\n        if binary_mask.ndim != 2:\n            raise ValueError(\"The binary mask must be of shape (H, W).\")\n\n        # Check if the binary mask is a valid binary mask\n        if not np.isin(binary_mask, [0, 1]).all():\n            raise ValueError(\"The binary mask must contain only 0s and 1s.\")\n\n        # Check if the color is a valid color\n        if color is not None:\n            try:\n                matplotlib.colors.to_rgba(color)\n            except ValueError:\n                raise ValueError(\"The color must be a valid color.\")\n\n        # Check if the edge color is a valid color\n        if edge_color is not None:\n            try:\n                matplotlib.colors.to_rgba(edge_color)\n            except ValueError:\n                raise ValueError(\"The edge color must be a valid color.\")\n\n        # Check if the text is a string\n        if text is not None and not isinstance(text, str):\n            raise TypeError(\"The text must be a string.\")\n\n        # Check if the alpha is a valid value\n        if not (0 <= alpha <= 1):\n            raise ValueError(\"The alpha value must be between 0 and 1.\")\n\n        # Check if the area threshold is a valid value\n        if not (area_threshold >= 0):\n            raise ValueError(\"The area threshold must be a non-negative value.\")\n\n        # Get the image height and width\n        height, width = binary_mask.shape\n\n        # Create a new figure and axis\n        fig, ax = plt.subplots(1, 1)\n\n        # Set the axis to be invisible\n        ax.axis(\"off\")\n\n        # Set the figure size\n        fig.set_size_inches(width / 100, height / 100)\n\n        # Set the axis limits\n        ax.set_xlim(0, width)\n        ax.set_ylim(height, 0)\n\n        # Set the axis aspect ratio to be equal\n       "}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if the image sizes are the same\n    if size_as_tensor:\n        assert input.image_size == other.image_size, f\"{msg}Image sizes do not match: {input.image_size} != {other.image_size}\"\n    else:\n        assert input.image_size == tuple(other.image_size), f\"{msg}Image sizes do not match: {input.image_size} != {other.image_size}\"\n\n    # Check if the fields are the same\n    for field in input.fields():\n        if field == \"image_size\":\n            continue\n        if field == \"gt_boxes\":\n            assert_boxes_allclose(input.gt_boxes, other.gt_boxes, rtol=rtol, msg=msg)\n        elif field == \"gt_masks\":\n            assert_masks_allclose(input.gt_masks, other.gt_masks, rtol=rtol, msg=msg)\n        elif field == \"gt_keypoints\":\n            assert_keypoints_allclose(input.gt_keypoints, other.gt_keypoints, rtol=rtol, msg=msg)\n        elif field == \"gt_classes\":\n            assert_classes_allclose(input.gt_classes, other.gt_classes, rtol=rtol, msg=msg)\n        elif field == \"gt_fields\":\n            assert_fields_allclose(input.gt_fields, other.gt_fields, rtol=rtol, msg=msg)\n        elif field == \"gt_labels\":\n            assert_labels_allclose(input.gt_labels, other.gt_labels, rtol=rtol, msg=msg)\n        elif field == \"gt_masks_int\":\n            assert_masks_allclose(input.gt_masks_int, other.gt_masks_int, rtol=rtol, msg=msg)\n        elif field == \"gt_masks_int_padded\":\n            assert_masks_allclose(input.gt_masks_int_padded, other.gt_masks_int_padded, rtol=rtol, msg=msg)\n        elif field == \"gt_masks_int_padded_fpn\":\n            assert_masks_allclose(input.gt_masks_int_padded_fpn, other.gt_masks_int_padded"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    proposal_generator_name = cfg.PROPOSAL_GENERATOR.NAME\n    proposal_generator_cfg = cfg.PROPOSAL_GENERATOR\n    proposal_generator_cfg.INPUT_SHAPE = input_shape\n\n    proposal_generator = registry.PROPOSAL_GENERATOR[proposal_generator_name](cfg=proposal_generator_cfg)\n    proposal_generator.initialize()\n\n    return proposal_generator\n\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Extract the scores and proposal deltas from the predictions tuple.\n        scores, proposal_deltas = predictions\n\n        # Extract the ground truth boxes and classes from the Instances objects in the proposals list.\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_classes = [x.gt_classes for x in proposals]\n\n        # Calculate the classification loss.\n        loss_cls = self.loss_cls(scores, gt_classes)\n\n        # Calculate the box regression loss.\n        loss_box_reg = self.loss_box_reg(proposal_deltas, gt_boxes)\n\n        # Scale the losses by their respective weights.\n        loss_cls *= self.loss_weight.get('loss_cls', 1.0)\n        loss_box_reg *= self.loss_weight.get('loss_box_reg', 1.0)\n\n        # Return a dictionary containing the losses.\n        return {'loss_cls': loss_cls, 'loss_box_reg': loss_box_reg}\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.tracker_name\n    tracker_class = TRACKER_REGISTRY.get(tracker_name)\n    if tracker_class is None:\n        raise ValueError(f\"Tracker {tracker_name} is not registered in the TRACKER_REGISTRY.\")\n    tracker = tracker_class(cfg)\n    return tracker\n\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Get the number of boxes and the number of classes\n        num_boxes = boxes.shape[0]\n        num_classes = deltas.shape[1] // 4\n\n        # Reshape the deltas tensor to have the same shape as the boxes tensor\n        deltas = deltas.view(num_boxes, num_classes, 4)\n\n        # Extract the x, y, w, and h components of the boxes\n        x, y, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n\n        # Calculate the center coordinates of the boxes\n        cx = x + 0.5 * w\n        cy = y + 0.5 * h\n\n        # Extract the dx, dy, dw, and dh components of the deltas\n        dx, dy, dw, dh = deltas[:, :, 0], deltas[:, :, 1], deltas[:, :, 2], deltas[:, :, 3]\n\n        # Calculate the new center coordinates of the boxes\n        cx_new = cx + dx\n        cy_new = cy + dy\n\n        # Calculate the new width and height of the boxes\n        w_new = w * torch.exp(dw)\n        h_new = h * torch.exp(dh)\n\n        # Calculate the new x and y coordinates of the boxes\n        x_new = cx_new - 0.5 * w_new\n        y_new = cy_new - 0.5 * h_new\n\n        # Create a new tensor to store the new boxes\n        new_boxes = torch.zeros_like(boxes)\n\n        # Set the new x, y, w, and h components of the new boxes\n        new_boxes[:, 0] = x_new\n        new_boxes[:, 1] = y_new\n        new_boxes[:, 2] = w_new\n        new_boxes[:, 3] = h_new\n\n        # Return the new boxes\n        return new_boxes\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image\n        output = self.process(image)\n\n        # Filter the output\n        if anno_type is None:\n            return output\n        else:\n            return self.filter_output(output, anno_type)\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        normalized_query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = self.split(normalized_query)\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            scores[keyword] = self.bm25(keyword)\n\n        # Aggregate the BM25 scores for each URL\n        aggregated_scores = {}\n        for url, keyword_scores in scores.items():\n            aggregated_score = sum(keyword_scores.values())\n            aggregated_scores[url] = aggregated_score\n\n        return aggregated_scores\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles to be within the range (-180, 180] degrees.\n        self.angles = self.angles % 360\n        self.angles = self.angles - 180 if self.angles > 180 else self.angles + 180\n\n        # Identify the indices of the boxes that are nearly horizontal based on the clip_angle_threshold.\n        horizontal_indices = np.where(np.abs(self.angles) <= clip_angle_threshold)[0]\n\n        # Convert the representation of the boxes to (x1, y1, x2, y2), where (x1, y1) and (x2, y2) are the coordinates of the top-left and bottom-right corners, respectively.\n        self.tensor[:, 0] = self.tensor[:, 0] - self.tensor[:, 2] / 2\n        self.tensor[:, 1] = self.tensor[:, 1] - self.tensor[:, 3] / 2\n        self.tensor[:, 2] = self.tensor[:, 0] + self.tensor[:, 2]\n        self.tensor[:, 3] = self.tensor[:, 1] + self.tensor[:, 3]\n\n        # Clip the x and y coordinates to ensure they do not exceed the specified box_size limits.\n        self.tensor[:, 0] = np.clip(self.tensor[:, 0], 0, box_size[1])\n        self.tensor[:, 1] = np.clip(self.tensor[:, 1], 0, box_size[0])\n        self.tensor[:, 2] = np.clip(self.tensor[:, 2], 0, box_size[1])\n        self.tensor[:, 3] = np.clip(self.tensor[:, 3], 0, box_size[0])\n\n        # Convert the representation of the boxes back to (center x, center y, width, height, angle).\n        self.tensor[:, 0] = self.tensor[:, 0] + self.tensor[:, 2] / 2\n        self.tensor[:, 1] = self.tensor[:, 1] + self.tensor[:, 3] / 2\n        self.tensor[:, 2] = self.tensor[:, 2]"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the dictionary to store the statistics\n        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        # Iterate over the data\n        for item in self.data:\n\n            # Update the statistics\n            statistics[item['type']] += 1\n\n        # Return the statistics\n        return statistics\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    neck_type = cfg.pop('type')\n    if neck_type in NECKS:\n        return NECKS[neck_type](**cfg)\n    else:\n        return MMDET_NECKS[neck_type](**cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    # Get the type of loss function to be built\n    loss_type = cfg.get(\"type\", \"cross_entropy\")\n\n    # Check if the loss function is a predefined loss function from the predefined libraries\n    if loss_type in LOSS_FUNCTIONS:\n        # Get the loss function from the predefined libraries\n        loss_fn = LOSS_FUNCTIONS[loss_type]\n    # Check if the loss function is a custom loss function from a module\n    elif loss_type in CUSTOM_LOSS_FUNCTIONS:\n        # Get the loss function from the custom loss functions module\n        loss_fn = CUSTOM_LOSS_FUNCTIONS[loss_type]\n    # Check if the loss function is a custom loss function from a custom module\n    elif loss_type in CUSTOM_LOSS_MODULES:\n        # Get the loss function from the custom loss functions module\n        loss_fn = CUSTOM_LOSS_MODULES[loss_type]\n    # If the loss function is not a predefined loss function or a custom loss function, raise an error\n    else:\n        raise ValueError(f\"Loss function {loss_type} is not a predefined loss function or a custom loss function.\")\n\n    # Return the loss function\n    return loss_fn\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    head_type = cfg.pop('type')\n    if head_type in HEADS:\n        return HEADS[head_type](**cfg)\n    else:\n        return MMDET_HEADS[head_type](**cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    # Check if the training and testing configurations are specified both in the function arguments and the model configuration.\n    if train_cfg is not None and 'train_cfg' in cfg:\n        warnings.warn('The training configuration is specified both in the function arguments and the model configuration. The configuration in the function arguments will be used.')\n    if test_cfg is not None and 'test_cfg' in cfg:\n        warnings.warn('The testing configuration is specified both in the function arguments and the model configuration. The configuration in the function arguments will be used.')\n\n    # If the training configuration is specified in the function arguments, update the model configuration.\n    if train_cfg is not None:\n        cfg.update(train_cfg)\n\n    # If the testing configuration is specified in the function arguments, update the model configuration.\n    if test_cfg is not None:\n        cfg.update(test_cfg)\n\n    # Build the segmentor model.\n    return build(cfg, SEGMENTOR)\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None:\n        warnings.warn('train_cfg is deprecated, please use the model field in the config file')\n    if test_cfg is not None:\n        warnings.warn('test_cfg is deprecated, please use the model field in the config file')\n\n    # build detector\n    detector_type = cfg.pop('type')\n    if detector_type not in DETECTORS:\n        raise KeyError(f'Invalid detector type {detector_type} for detection')\n    detector_cfg = cfg.copy()\n    detector = DETECTORS.build(detector_cfg)\n\n    return detector"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # Initialize the evaluation results\n    class_wise_ap = {}\n    class_wise_ar = {}\n    overall_ap = 0.0\n    overall_ar = 0.0\n\n    # Iterate over each class\n    for label in label2cat.keys():\n        # Get the ground truth and detection annotations for the current class\n        gt_class_annos = [anno for anno in gt_annos if anno['label'] == label]\n        dt_class_annos = [anno for anno in dt_annos if anno['label'] == label]\n\n        # Convert the 3D bounding boxes to the specified format\n        if box_type_3d is not None:\n            gt_class_annos = [box_type_3d.convert_to(anno, box_mode_3d) for anno in gt_class_annos]\n            dt_class_annos = [box_type_3d.convert_to(anno, box_mode_3d) for anno in dt_class_annos]\n\n        # Compute the average precision and recall for the current class\n        ap, ar = compute_ap_and_ar(gt_class_annos, dt_class_annos, metric)\n\n        # Update the class-wise AP and AR\n        class_wise_ap[label] = ap\n        class_wise_ar[label] = ar\n\n        # Update the overall AP and AR\n        overall_ap += ap\n        overall_ar += ar\n\n    # Compute the overall mAP and mAR\n    overall_ap /= len(label2cat)\n    overall_ar /= len(label2cat)\n\n    # Print the evaluation results\n    if logger is not None:\n        logger.info('Overall mAP: {:.4f}'.format(overall_ap))\n        logger.info('Overall mAR: {:.4f}'.format(overall_ar))\n        for label, ap in class_wise_ap.items():\n            logger.info('Class {} mAP: {:.4f}'.format(label2cat[label], ap))\n        for label, ar in class_wise_ar.items():\n            logger.info('Class {} mAR: {:.4f}'.format(label2cat[label], ar))\n\n    # Return the evaluation results\n    return {\n        'overall_ap': overall_ap,\n        'overall_ar': overall"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        return LiDARBox, \"LiDAR\"\n    elif box_type == \"Camera\":\n        return CameraBox, \"Camera\"\n    elif box_type == \"Depth\":\n        return DepthBox, \"Depth\"\n    else:\n        raise ValueError(f\"Unrecognized box type: {box_type}.\")\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('Model is required.')\n\n    if not messages:\n      raise RequestError('Messages are required.')\n\n    if not isinstance(messages, list):\n      raise TypeError('Messages must be a list of Message or dict-like objects.')\n\n    if not all(isinstance(message, Message) for message in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects.')\n\n    if not isinstance(stream, bool):\n      raise TypeError('Stream must be a boolean.')\n\n    if not isinstance(format, str):\n      raise TypeError('Format must be a string.')\n\n    if not isinstance(options, Options):\n      raise TypeError('Options must be an instance of Options.')\n\n    if not isinstance(keep_alive, (float, str)):\n      raise TypeError('Keep-alive must be a float or a string.')\n\n    if format not in ['', 'json']:\n      raise RequestError('Format must be either \"\" or \"json\".')\n\n    if not isinstance(keep_alive, (float, str)):\n      raise TypeError('Keep-alive must be a float or a string.')\n\n    if keep_alive is not None:\n      if not isinstance(keep_alive, (float, str)):\n        raise TypeError('Keep-alive must be a float or a string.')\n      elif keep_alive < 0:\n        raise RequestError('Keep-alive must be a positive number.')\n\n    if stream:\n      return self._stream_chat(model, messages, format, options, keep_alive)\n    else:\n      return self._chat(model, messages, format, options, keep_alive)\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    if stream:\n      return self._stream_pull(model, insecure)\n    else:\n      return self._pull(model, insecure)\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('Model is required.')\n\n    if not prompt:\n      raise ValueError('Prompt is required.')\n\n    if not system:\n      raise ValueError('System is required.')\n\n    if not template:\n      raise ValueError('Template is required.')\n\n    if not context:\n      context = []\n\n    if not options:\n      options = Options()\n\n    if keep_alive:\n      options.keep_alive = keep_alive\n\n    if images:\n      options.images = images\n\n    if format == 'json':\n      options.format = 'json'\n\n    return self._generate(\n      model=model,\n      prompt=prompt,\n      system=system,\n      template=template,\n      context=context,\n      stream=stream,\n      raw=raw,\n      options=options,\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    # Build the request\n    request = self._build_request(\n      method='POST',\n      path='/api/push',\n      params={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n    )\n\n    # Send the request\n    response = self._send_request(request)\n\n    # Return the response\n    if stream:\n      return self._stream_response(response)\n    else:\n      return self._parse_response(response)\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided.\")\n\n    if path is not None:\n      with open(path, \"r\") as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError(\"modelfile must be provided.\")\n\n    if stream:\n      return self._create_stream(model, modelfile)\n    else:\n      return self._create(model, modelfile)\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Open the file and calculate the SHA-256 checksum\n    with open(path, 'rb') as f:\n        sha256 = hashlib.sha256()\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n\n    # Check if the blob already exists on the server\n    digest = 'sha256:' + sha256.hexdigest()\n    response = requests.head(self.url + digest)\n\n    # If the blob does not exist, upload it\n    if response.status_code == 404:\n        with open(path, 'rb') as f:\n            response = requests.post(self.url, data=f, headers={'Content-Type': 'application/octet-stream'})\n            if response.status_code != 201:\n                raise Exception('Failed to upload blob: ' + response.text)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('Model is required.')\n\n    if not prompt:\n      raise ValueError('Prompt is required.')\n\n    if not isinstance(prompt, str):\n      raise TypeError('Prompt must be a string.')\n\n    if not isinstance(model, str):\n      raise TypeError('Model must be a string.')\n\n    if not isinstance(system, str):\n      raise TypeError('System must be a string.')\n\n    if not isinstance(template, str):\n      raise TypeError('Template must be a string.')\n\n    if context is not None and not isinstance(context, Sequence):\n      raise TypeError('Context must be a sequence.')\n\n    if images is not None and not isinstance(images, Sequence):\n      raise TypeError('Images must be a sequence.')\n\n    if options is not None and not isinstance(options, Options):\n      raise TypeError('Options must be an instance of the Options class.')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError('Keep-alive must be a float or a string.')\n\n    if format not in ('', 'json'):\n      raise ValueError('Format must be empty or \"json\".')\n\n    if format == 'json' and not raw:\n      raise ValueError('Format must be \"json\" if raw is False.')\n\n    if format == 'json' and stream:\n      raise ValueError('Format must be \"json\" if stream is True.')\n\n    if format == 'json' and not raw and not stream:\n      raise ValueError('Format must be \"json\" if raw and stream are False.')\n\n    if format == 'json' and raw and stream:\n      raise ValueError('Format must be \"json\" if raw and stream are True.')\n\n    if format == 'json' and raw and not stream:\n      raise ValueError('Format must be \"json\" if raw and stream are False.')\n\n    if format == 'json' and not raw and stream:\n      raise ValueError('Format must be \"json\" if raw and stream are True.')\n\n    if format == 'json' and not raw and not stream:\n      raise ValueError('Format must be \"json\" if raw and stream are False.')\n\n    if format == 'json' and raw and stream:\n      raise ValueError('Format must be \"json\" if raw and stream are True.')\n\n    if format == 'json' and raw and not stream:\n      raise ValueError('Format must be \"json\" if raw and stream are False.')\n\n    if"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if not self.is_connected:\n      raise ResponseError(\n        \"Cannot pull data for model {}. Client is not connected.\".format(model)\n      )\n\n    if not self.is_authenticated:\n      raise ResponseError(\n        \"Cannot pull data for model {}. Client is not authenticated.\".format(model)\n      )\n\n    if not self.is_authorized:\n      raise ResponseError(\n        \"Cannot pull data for model {}. Client is not authorized.\".format(model)\n      )\n\n    if not self.is_authorized_for_model(model):\n      raise ResponseError(\n        \"Cannot pull data for model {}. Client is not authorized for model.\".format(\n          model\n        )\n      )\n\n    if not self.is_authorized_for_model_version(model, self.version):\n      raise ResponseError(\n        \"Cannot pull data for model {}. Client is not authorized for model version {}.\".format(\n          model, self.version\n        )\n      )\n\n    if not self.is_authorized_for_model_version_stage(\n      model, self.version, self.stage\n    ):\n      raise ResponseError(\n        \"Cannot pull data for model {}. Client is not authorized for model version {} stage {}.\".format(\n          model, self.version, self.stage\n        )\n      )\n\n    if not self.is_authorized_for_model_version_stage_environment(\n      model, self.version, self.stage, self.environment\n    ):\n      raise ResponseError(\n        \"Cannot pull data for model {}. Client is not authorized for model version {} stage {} environment {}.\".format(\n          model, self.version, self.stage, self.environment\n        )\n      )\n\n    if not self.is_authorized_for_model_version_stage_environment_role(\n      model, self.version, self.stage, self.environment, self.role\n    ):\n      raise ResponseError(\n        \"Cannot pull data for model {}. Client is not authorized for model version {} stage {} environment {} role {}.\".format(\n          model, self.version, self.stage, self.environment, self.role\n        )\n      )\n\n    if not self.is_authorized_for_model_version_stage_environment_role_action(\n      model, self.version, self.stage, self.environment, self.role, self.action\n    ):\n      raise ResponseError("}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate the input parameters\n    if not model:\n      raise ValueError('Model identifier is required for chat request.')\n    if messages is None:\n      raise ValueError('Messages are required for chat request.')\n    if not isinstance(messages, list):\n      raise ValueError('Messages must be a list.')\n    if not all(isinstance(message, dict) for message in messages):\n      raise ValueError('Messages must be a list of dictionaries.')\n    if not all(key in message for message in messages for key in ['role', 'content']):\n      raise ValueError('Messages must contain at least role and content keys.')\n    if not all(isinstance(message['role'], str) for message in messages):\n      raise ValueError('Role must be a string.')\n    if not all(isinstance(message['content'], str) for message in messages):\n      raise ValueError('Content must be a string.')\n    if not isinstance(stream, bool):\n      raise ValueError('Stream must be a boolean.')\n    if not isinstance(format, str):\n      raise ValueError('Format must be a string.')\n    if options is not None and not isinstance(options, Options):\n      raise ValueError('Options must be an instance of the Options class.')\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise ValueError('Keep-alive must be a float or a string.')\n\n    # Make the request\n    if stream:\n      async for response in self._chat_stream(\n        model=model,\n        messages=messages,\n        format=format,\n        options=options,\n        keep_alive=keep_alive,\n      ):\n        yield response\n    else:\n      return await self._chat(\n        model=model,\n        messages=messages,\n        format=format,\n        options=options,\n        keep_alive=keep_alive,\n      )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    if stream:\n      return self._stream_push(model, insecure)\n    else:\n      return self._push(model, insecure)\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file.\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n    checksum = sha256.hexdigest()\n\n    # Check if a blob with the same checksum already exists on the server.\n    response = await self.client.head(f'/blobs/sha256:{checksum}')\n    if response.status == 404:\n        # If not found, upload the file in chunks to the server.\n        with open(path, 'rb') as f:\n            while True:\n                data = f.read(65536)\n                if not data:\n                    break\n                response = await self.client.post(f'/blobs/upload', data=data)\n                if response.status != 202:\n                    raise Exception(f'Failed to upload blob: {response.status} {response.reason}')\n    else:\n        # If found, return the digest of the file.\n        return response.headers['Digest']\n\n    # Return the digest of the file.\n    return f'sha256:{checksum}'\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to store the user-provided code.\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\") as user_code_file:\n            user_code_file.write(user_code)\n            user_code_file.flush()\n\n            # Create a temporary file to store the test code.\n            with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\") as test_code_file:\n                test_code_file.write(test_code)\n                test_code_file.flush()\n\n                # Create a temporary directory to store the output of the type check.\n                with tempfile.TemporaryDirectory() as output_dir:\n\n                    # Run the type check using Pyright.\n                    pyright_command = [\n                        \"pyright\",\n                        \"--output\",\n                        output_dir,\n                        \"--python-version\",\n                        \"3.8\",\n                        \"--python-executable\",\n                        sys.executable,\n                        user_code_file.name,\n                        test_code_file.name,\n                    ]\n                    pyright_process = subprocess.run(\n                        pyright_command,\n                        stdout=subprocess.PIPE,\n                        stderr=subprocess.PIPE,\n                        universal_newlines=True,\n                    )\n\n                    # Parse the output of the type check.\n                    pyright_output = pyright_process.stdout\n                    pyright_errors = pyright_output.splitlines()\n\n                    # Check if the type check passed or failed.\n                    if pyright_process.returncode == 0:\n                        return TypeCheckResult(\n                            True,\n                            \"Type check passed.\",\n                            \"The type check passed successfully.\",\n                        )\n                    else:\n                        return TypeCheckResult(\n                            False,\n                            \"Type check failed.\",\n                            \"The type check failed with the following errors:\\n\"\n                            + \"\\n\".join(pyright_errors),\n                        )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      return await self._stream_request(\n        \"POST\",\n        \"/models\",\n        json={\"name\": model, \"data\": modelfile},\n      )\n    else:\n      return await self._request(\n        \"POST\",\n        \"/models\",\n        json={\"name\": model, \"data\": modelfile},\n      )\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn)\n    else:\n        return aot_function(fn)\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Check if the trial directory exists\n    if not os.path.isdir(trial_path):\n        raise ValueError(f\"Trial directory '{trial_path}' does not exist.\")\n\n    # Check if the summary.csv file exists in the trial directory\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    if not os.path.isfile(summary_path):\n        raise ValueError(f\"Summary file '{summary_path}' does not exist in trial directory '{trial_path}'.\")\n\n    # Read the summary file\n    summary_df = pd.read_csv(summary_path)\n\n    # Get the index of the best trial\n    best_trial_index = summary_df[\"trial_id\"].idxmin()\n\n    # Get the best trial ID\n    best_trial_id = summary_df.loc[best_trial_index, \"trial_id\"]\n\n    # Get the best pipeline configuration\n    best_config = summary_df.loc[best_trial_index, \"config\"]\n\n    # Convert the best pipeline configuration to a dictionary\n    best_config_dict = json.loads(best_config)\n\n    # Check if the output path is specified and is a valid file path\n    if output_path is not None:\n        if not os.path.isdir(os.path.dirname(output_path)):\n            raise ValueError(f\"Output directory '{os.path.dirname(output_path)}' does not exist.\")\n\n        # Check if the output file extension is .yaml or .yml\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            raise ValueError(f\"Output file '{output_path}' must have a .yaml or .yml extension.\")\n\n        # Save the best pipeline configuration to a YAML file\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config_dict, f)\n\n    return best_config_dict\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Check if the function or module is a PyTorch module\n    if isinstance(func, torch.nn.Module):\n        # If it is a module, use its forward method\n        func = func.forward\n\n    # Check if the function is a PyTorch module\n    if isinstance(func, torch.nn.Module):\n        # If it is a module, use its forward method\n        func = func.forward\n\n    # Check if the function is a PyTorch module\n    if isinstance(func, torch.nn.Module):\n        # If it is a module, use its forward method\n        func = func.forward\n\n    # Check if the function is a PyTorch module\n    if isinstance(func, torch.nn.Module):\n        # If it is a module, use its forward method\n        func = func.forward\n\n    # Check if the function is a PyTorch module\n    if isinstance(func, torch.nn.Module):\n        # If it is a module, use its forward method\n        func = func.forward\n\n    # Check if the function is a PyTorch module\n    if isinstance(func, torch.nn.Module):\n        # If it is a module, use its forward method\n        func = func.forward\n\n    # Check if the function is a PyTorch module\n    if isinstance(func, torch.nn.Module):\n        # If it is a module, use its forward method\n        func = func.forward\n\n    # Check if the function is a PyTorch module\n    if isinstance(func, torch.nn.Module):\n        # If it is a module, use its forward method\n        func = func.forward\n\n    # Check if the function is a PyTorch module\n    if isinstance(func, torch.nn.Module):\n        # If it is a module, use its forward method\n        func = func.forward\n\n    # Check if the function is a PyTorch module\n    if isinstance(func, torch.nn.Module):\n        # If it is a module, use its forward method\n        func = func.forward\n\n    # Check if the function is a PyTorch module\n    if isinstance(func, torch.nn.Module):\n        # If it is a module, use its forward method\n        func = func.forward\n\n    # Check if the function is a PyTorch module\n    if isinstance(func, torch.nn.Module):\n        # If it is a module, use its forward method\n        func = func.forward\n\n    # Check if the function is a PyTorch module\n    if isinstance(func, torch."}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Extract the best configuration from the trial folder\n        best_config = cls.extract_best_config(trial_path)\n\n        # Initialize the Runner with the best configuration and the project directory set to the parent directory of the trial folder\n        runner = cls(best_config, os.path.dirname(trial_path))\n\n        return runner\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create a directory for this node line\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Create a directory for this node line's results\n    node_line_results_dir = os.path.join(node_line_dir, 'results')\n    if not os.path.exists(node_line_results_dir):\n        os.makedirs(node_line_results_dir)\n\n    # Create a directory for this node line's summaries\n    node_line_summaries_dir = os.path.join(node_line_dir, 'summaries')\n    if not os.path.exists(node_line_summaries_dir):\n        os.makedirs(node_line_summaries_dir)\n\n    # Create a directory for this node line's logs\n    node_line_logs_dir = os.path.join(node_line_dir, 'logs')\n    if not os.path.exists(node_line_logs_dir):\n        os.makedirs(node_line_logs_dir)\n\n    # Create a directory for this node line's evaluation metrics\n    node_line_metrics_dir = os.path.join(node_line_dir, 'metrics')\n    if not os.path.exists(node_line_metrics_dir):\n        os.makedirs(node_line_metrics_dir)\n\n    # Create a directory for this node line's evaluation metrics\n    node_line_speed_dir = os.path.join(node_line_dir, 'speed')\n    if not os.path.exists(node_line_speed_dir):\n        os.makedirs(node_line_speed_dir)\n\n    # Create a directory for this node line's evaluation metrics\n    node_line_eval_dir = os.path.join(node_line_dir, 'eval')\n    if not os.path.exists(node_line_eval_dir):\n        os.makedirs(node_line_eval_dir)\n\n    # Create a directory for this node line's evaluation metrics\n    node_line_eval_dir = os.path.join(node_line_dir, 'eval')\n    if not os.path.exists(node_line_eval_dir):\n        os.makedirs(node_line_eval_dir)\n\n   "}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a directory to save the results and summaries\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a dataframe to store the results and summaries\n    results = pd.DataFrame()\n\n    # Iterate over the modules and their parameters\n    for module, params in zip(modules, module_params):\n\n        # Run the module with the given parameters\n        result = module(previous_result, **params)\n\n        # Save the result to the specified directory\n        result.to_csv(os.path.join(node_line_dir, f\"{module.__name__}.csv\"), index=False)\n\n        # Add the result to the results dataframe\n        results = results.append(result)\n\n    # Save the results dataframe to a CSV file\n    results.to_csv(os.path.join(node_line_dir, \"results.csv\"), index=False)\n\n    # Create a dataframe to store the summary\n    summary = pd.DataFrame()\n\n    # Iterate over the modules and their parameters\n    for module, params in zip(modules, module_params):\n\n        # Run the module with the given parameters\n        summary = summary.append(run_query_expansion_node_summary(module, params, previous_result, node_line_dir, strategies))\n\n    # Save the summary dataframe to a CSV file\n    summary.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    # Select the best result based on the evaluation criteria\n    best_result = select_best_result(summary, strategies)\n\n    # Save the best result to a CSV file\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create the node's output directory\n    node_output_dir = os.path.join(node_line_dir, 'output')\n    if not os.path.exists(node_output_dir):\n        os.makedirs(node_output_dir)\n\n    # Create the node's summary directory\n    node_summary_dir = os.path.join(node_line_dir, 'summary')\n    if not os.path.exists(node_summary_dir):\n        os.makedirs(node_summary_dir)\n\n    # Create the node's evaluation directory\n    node_evaluation_dir = os.path.join(node_output_dir, 'evaluation')\n    if not os.path.exists(node_evaluation_dir):\n        os.makedirs(node_evaluation_dir)\n\n    # Create the node's prompt maker directory\n    node_prompt_maker_dir = os.path.join(node_output_dir, 'prompt_maker')\n    if not os.path.exists(node_prompt_maker_dir):\n        os.makedirs(node_prompt_maker_dir)\n\n    # Create the node's prompt maker summary directory\n    node_prompt_maker_summary_dir = os.path.join(node_summary_dir, 'prompt_maker')\n    if not os.path.exists(node_prompt_maker_summary_dir):\n        os.makedirs(node_prompt_maker_summary_dir)\n\n    # Create the node's prompt maker evaluation directory\n    node_prompt_maker_evaluation_dir = os.path.join(node_evaluation_dir, 'prompt_maker')\n    if not os.path.exists(node_prompt_maker_evaluation_dir):\n        os.makedirs(node_prompt_maker_evaluation_dir)\n\n    # Create the node's prompt maker evaluation summary directory\n    node_prompt_maker_evaluation_summary_dir = os.path.join(node_prompt_maker_summary_dir, 'evaluation')\n    if not os.path.exists(node_prompt_maker_evaluation_summary_dir):\n        os.makedirs(node_prompt_maker_evaluation_summary_dir)\n\n    # Create the node's prompt maker evaluation metrics directory\n    node_prompt_maker_evaluation_metrics_dir = os.path.join(node_prompt_maker_evaluation"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        if node.module_params is not None:\n            if key in node.module_params:\n                values.append(node.module_params[key])\n    return list(set(values))\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = BaseEmbedding('all-mpnet-base-v2')\n\n    # Convert the prediction and ground truth strings into embeddings\n    pred_embedding = embedding_model.encode(pred)\n    gt_embeddings = embedding_model.encode(generation_gt)\n\n    # Calculate the cosine similarity between the prediction and each ground truth string\n    cosine_similarities = cosine_similarity(pred_embedding, gt_embeddings)\n\n    # Return the maximum cosine similarity\n    return np.max(cosine_similarities)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logging.warning(\"GFPGAN face restorer is not set up. Returning the original image.\")\n        return np_image\n\n    try:\n        np_image = gfpgan_face_restorer.restore_image(np_image)\n    except Exception as e:\n        logging.warning(f\"Failed to restore faces in the image. Returning the original image. Error: {e}\")\n        return np_image\n\n    return np_image\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    global face_restorers\n\n    try:\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        # Patch the facexlib with the given directory\n        facexlib.patch_facexlib(dirname)\n\n        # Initialize the GFPGAN face restorer with the model located in the specified directory\n        restorer = GFPGANFaceRestorer(model_dir=dirname)\n\n        # Set the global restorer variable to the initialized GFPGAN face restorer\n        global restorer\n        restorer = restorer\n\n    except Exception as e:\n        # Report any exceptions that occur during the setup process\n        print(f\"Error: {e}\")\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion\n  q_vec = np.array([0, v[0], v[1], v[2]])\n\n  # Apply the rotation\n  q_rot = q * q_vec * q.conj()\n\n  # Convert the rotated quaternion back to a vector\n  v_rot = np.array([q_rot[1], q_rot[2], q_rot[3]])\n\n  return v_rot\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle)\n  axis = axis_angle / jnp.linalg.norm(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n\n  return jnp.array([\n    jnp.sin(angle / 2) * axis[0],\n    jnp.sin(angle / 2) * axis[1],\n    jnp.sin(angle / 2) * axis[2],\n    jnp.cos(angle / 2)\n  ])\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk words\n    topk_words = model.topk(prefix, k=k)\n    topk_words = [word for word, _ in topk_words]\n\n    # get topk indices\n    topk_indices = [model.word_to_index(word) for word in topk_words]\n\n    # get topk log probabilities\n    topk_log_probs = [model.log_prob(prefix, word) for word in topk_words]\n\n    # get topk log probabilities for the target index\n    target_log_prob = topk_log_probs[topk_indices.index(idx)]\n\n    # get topk log probabilities for the target index with the high bias\n    high_log_prob = model.log_prob(prefix, model.index_to_word(idx), high=high)\n\n    # get the number of calls made to the model\n    num_calls = model.num_calls\n\n    # get the number of calls made to the model with the high bias\n    high_num_calls = model.num_calls\n\n    # get the number of calls made to the model with the high bias\n    high_num_calls = model.num_calls\n\n    # get the number of calls made to the model with the high bias\n    high_num_calls = model.num_calls\n\n    # get the number of calls made to the model with the high bias\n    high_num_calls = model.num_calls\n\n    # get the number of calls made to the model with the high bias\n    high_num_calls = model.num_calls\n\n    # get the number of calls made to the model with the high bias\n    high_num_calls = model.num_calls\n\n    # get the number of calls made to the model with the high bias\n    high_num_calls = model.num_calls\n\n    # get the number of calls made to the model with the high bias\n    high_num_calls = model.num_calls\n\n    # get the number of calls made to the model with the high bias\n    high_num_calls = model.num_calls\n\n    # get the number of calls made to the model with the high bias\n    high_num_calls = model.num_calls\n\n    # get the number of calls made to the model with the high bias\n    high_num_calls = model.num"}
{"namespace": "resample.resample_3d", "completion": "  # Check if the input data is a tensor\n  if not isinstance(data, tf.Tensor):\n    raise TypeError('Input data must be a tensor.')\n\n  # Check if the input locations is a tensor\n  if not isinstance(locations, tf.Tensor):\n    raise TypeError('Input locations must be a tensor.')\n\n  # Check if the input edge_behavior is a string\n  if not isinstance(edge_behavior, str):\n    raise TypeError('Input edge_behavior must be a string.')\n\n  # Check if the input coordinate_order is a string\n  if not isinstance(coordinate_order, str):\n    raise TypeError('Input coordinate_order must be a string.')\n\n  # Check if the input method is a string\n  if not isinstance(method, str):\n    raise TypeError('Input method must be a string.')\n\n  # Check if the input half_pixel_center is a bool\n  if not isinstance(half_pixel_center, bool):\n    raise TypeError('Input half_pixel_center must be a bool.')\n\n  # Check if the input data is a 4D tensor\n  if len(data.shape) != 4:\n    raise ValueError('Input data must be a 4D tensor.')\n\n  # Check if the input locations is a 4D tensor\n  if len(locations.shape) != 4:\n    raise ValueError('Input locations must be a 4D tensor.')\n\n  # Check if the input edge_behavior is a valid option\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('Input edge_behavior must be either \"CONSTANT_OUTSIDE\" or \"CLAMP\".')\n\n  # Check if the input coordinate_order is a valid option\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Input coordinate_order must be either \"xyz\" or \"zyx\".')\n\n  # Check if the input method is a valid option\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Input method must be either \"TRILINEAR\" or \"NEAREST\".')\n\n  # Check if the input data is a float tensor\n  if data.dtype != tf.float32:\n    raise ValueError('Input data must be a float tensor.')\n\n  # Check if the input locations is a float tensor\n  if locations.dtype != tf.float32:\n    raise ValueError('Input locations must be a"}
{"namespace": "math.plus_eps", "completion": "  if x < 1e-10:\n    return 1e-10\n  else:\n    return np.nextafter(x, np.inf)\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-100\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n\n"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    \"\"\"\n    The helper function for the safe exponential function. It applies a custom gradient function for backpropagation in automatic differentiation contexts.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function and its custom gradient.\n    :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    if x < -100:\n      return 0\n    elif x > 100:\n      return 1\n    else:\n      return np.exp(x)\n\n  return safe_exp_helper(x)\n"}
{"namespace": "math.safe_log", "completion": "  # If the input is a scalar, return the result of the safe logarithm function applied to the scalar.\n  if isinstance(x, numbers.Number):\n    return safe_fn(x)\n\n  # If the input is an array, return the result of applying the safe logarithm function to each element of the array.\n  elif isinstance(x, np.ndarray):\n    return np.array([safe_fn(xi) for xi in x])\n\n  # If the input is not a scalar or an array, raise an error.\n  else:\n    raise ValueError(f\"Input to safe_log must be a scalar or an array, but got {type(x)}.\")\n"}
{"namespace": "math.safe_sqrt", "completion": "  # Define the maximum value for the input x\n  max_value = 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 1\n  elif p < 0:\n    return 0\n  else:\n    return float('inf')\n"}
{"namespace": "geopoly.generate_basis", "completion": "  # Check if the base shape is valid\n  if base_shape not in [\"tetrahedron\", \"icosahedron\", \"octahedron\"]:\n    raise ValueError(f\"Invalid base shape: {base_shape}.\")\n\n  # Generate the base vertices\n  if base_shape == \"tetrahedron\":\n    base_vertices = np.array(\n      [\n        [0, 0, 0],\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n      ]\n    )\n  elif base_shape == \"icosahedron\":\n    base_vertices = np.array(\n      [\n        [0, 0, 1],\n        [0, 0.9510565162951535, 0.30901699437494745],\n        [0, -0.5877852522924731, 0.8090169943749475],\n        [0.8944271909999159, 0, 0.4472135954999579],\n        [-0.4472135954999579, 0.8944271909999159, 0],\n        [0.5877852522924731, 0.5877852522924731, 0.5877852522924731],\n        [-0.5877852522924731, -0.5877852522924731, 0.5877852522924731],\n        [0.8090169943749474, -0.5877852522924731, -0.090169"}
{"namespace": "math.safe_log1p", "completion": "  # Check if the input value is within the safe range\n  if x < -1.0 or x > 1.0:\n    # If the input value is outside the safe range, return a NaN\n    return np.nan\n  else:\n    # If the input value is within the safe range, return the result of the log1p operation\n    return np.log1p(x)\n"}
{"namespace": "math.power_ladder", "completion": "  # Handle special cases for p.\n  if p == 1:\n    return x\n  elif p == 0:\n    return np.log(x)\n  elif p == -np.inf:\n    return np.sign(x)\n  elif p == np.inf:\n    return np.sign(x) * np.abs(x)\n\n  # Apply pre-multiplication.\n  if premult is not None:\n    x = x * premult\n\n  # Apply Tukey's power ladder.\n  x = np.power(np.abs(x), p)\n\n  # Apply post-multiplication.\n  if postmult is not None:\n    x = x * postmult\n\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 0:\n    return y\n\n  if p == 1:\n    return np.power(y, 1 / p)\n\n  if p == 2:\n    return np.power(y, 1 / p)\n\n  if p == 3:\n    return np.power(y, 1 / p)\n\n  if p == 4:\n    return np.power(y, 1 / p)\n\n  if p == 5:\n    return np.power(y, 1 / p)\n\n  if p == 6:\n    return np.power(y, 1 / p)\n\n  if p == 7:\n    return np.power(y, 1 / p)\n\n  if p == 8:\n    return np.power(y, 1 / p)\n\n  if p == 9:\n    return np.power(y, 1 / p)\n\n  if p == 10:\n    return np.power(y, 1 / p)\n\n  if p == 11:\n    return np.power(y, 1 / p)\n\n  if p == 12:\n    return np.power(y, 1 / p)\n\n  if p == 13:\n    return np.power(y, 1 / p)\n\n  if p == 14:\n    return np.power(y, 1 / p)\n\n  if p == 15:\n    return np.power(y, 1 / p)\n\n  if p == 16:\n    return np.power(y, 1 / p)\n\n  if p == 17:\n    return np.power(y, 1 / p)\n\n  if p == 18:\n    return np.power(y, 1 / p)\n\n  if p == 19:\n    return np.power(y, 1 / p)\n\n  if p == 20:\n    return np.power(y, 1 / p)\n\n  if p == 21:\n    return np.power(y, 1 / p)\n\n  if p == 22:\n    return np.power(y, 1 / p)\n\n  if p == 23:\n    return np.power(y, 1 / p)\n\n  if p == 24:\n    return np.power(y,"}
{"namespace": "math.learning_rate_decay", "completion": "  if step < lr_delay_steps:\n    lr = lr_init * lr_delay_mult\n  else:\n    lr = lr_init + (lr_final - lr_init) * min(1.0, step / max_steps)\n\n  return lr\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  # Generate random rays\n  rays = generate_random_rays(include_exposure_idx, include_exposure_values, include_device_idx)\n\n  return rays\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Check if the input arrays are numpy or jax.numpy arrays\n  if not isinstance(points, xnp.ndarray):\n    raise TypeError(\"points must be a numpy or jax.numpy array\")\n  if not isinstance(pixtocams, xnp.ndarray):\n    raise TypeError(\"pixtocams must be a numpy or jax.numpy array\")\n  if not isinstance(camtoworlds, xnp.ndarray):\n    raise TypeError(\"camtoworlds must be a numpy or jax.numpy array\")\n\n  # Check if the input arrays have the correct shape\n  if points.ndim != 2:\n    raise ValueError(\"points must be a 2D array\")\n  if pixtocams.ndim != 2:\n    raise ValueError(\"pixtocams must be a 2D array\")\n  if camtoworlds.ndim != 2:\n    raise ValueError(\"camtoworlds must be a 2D array\")\n\n  # Check if the input arrays have the correct number of columns\n  if points.shape[1] != 3:\n    raise ValueError(\"points must have 3 columns\")\n  if pixtocams.shape[1] != 9:\n    raise ValueError(\"pixtocams must have 9 columns\")\n  if camtoworlds.shape[1] != 12:\n    raise ValueError(\"camtoworlds must have 12 columns\")\n\n  # Check if the input arrays have the correct number of rows\n  if points.shape[0] != pixtocams.shape[0] or points.shape[0] != camtoworlds.shape[0]:\n    raise ValueError(\"points, pixtocams, and camtoworlds must have the same number of rows\")\n\n  # Check if the camera type is valid\n  if camtype not in ProjectionType:\n    raise ValueError(\"camtype must be a valid ProjectionType\")\n\n  # Check if the distortion parameters are valid\n  if distortion_params is not None:\n    if not isinstance(distortion_params, dict):\n      raise TypeError(\"distortion_params must be a dict\")\n    if \"k1\" not in distortion_params or \"k2\" not in distortion_params:\n      raise ValueError(\"distortion_params must contain k1 and k2\")\n    if \"p1\" not in distortion_params or \"p2\" not in distortion_params:\n      raise ValueError(\""}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the rotation and translation components of the screw axis\n  w = screw_axis[0:3]\n  v = screw_axis[3:6]\n\n  # Compute the exponential map\n  theta = jnp.linalg.norm(w)\n  if theta < eps:\n    return jnp.eye(4)\n  else:\n    w_hat = w / theta\n    w_hat_hat = jnp.array([[0, -w_hat[2], w_hat[1]], [w_hat[2], 0, -w_hat[0]], [-w_hat[1], w_hat[0], 0]])\n    return jnp.eye(3) + w_hat_hat + (1 - jnp.cos(theta)) * w_hat_hat + (theta - jnp.sin(theta)) * w_hat_hat @ w_hat_hat\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Normalize the axis-angle vector\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle, axis=1, keepdims=True)\n\n  # Compute the sine and cosine of the angle of rotation\n  sin_theta = jnp.linalg.norm(axis_angle, axis=1, keepdims=True)\n  cos_theta = jnp.clip(1 - sin_theta ** 2 / (2 * eps), -1, 1)\n\n  # Compute the skew-symmetric matrix of the axis of rotation\n  skew_symmetric_matrix = jnp.array(\n      [\n          [0, -axis_angle[2], axis_angle[1]],\n          [axis_angle[2], 0, -axis_angle[0]],\n          [-axis_angle[1], axis_angle[0], 0],\n      ]\n  )\n\n  # Compute the rotation matrix\n  rotation_matrix = jnp.eye(3) + skew_symmetric_matrix * sin_theta + skew_symmetric_matrix @ skew_symmetric_matrix * (\n      1 - cos_theta\n  )\n\n  return rotation_matrix\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the angle of the cone.\n  theta = jnp.arccos(jnp.dot(d, jnp.array([0, 0, 1])))\n\n  # Calculate the distance of the cone's apex from the origin.\n  apex_distance = jnp.linalg.norm(d)\n\n  # Calculate the distance of the cone's base from the origin.\n  base_distance = apex_distance * jnp.cos(theta)\n\n  # Calculate the distance of the cone's tip from the origin.\n  tip_distance = apex_distance * jnp.sin(theta)\n\n  # Calculate the distance of the cone's base from the origin.\n  base_radius = base_radius * base_distance\n\n  # Calculate the distance of the cone's tip from the origin.\n  tip_radius = base_radius * tip_distance / base_distance\n\n  # Calculate the mean of the Gaussian distribution.\n  mean = d * (t0 + t1) / 2\n\n  # Calculate the covariance of the Gaussian distribution.\n  if diag:\n    # If the covariance is diagonal, the covariance matrix is a diagonal matrix with the variances along the diagonal.\n    # The variances are calculated as the square of the radius at the base and tip of the cone.\n    covariance = jnp.diag(jnp.array([base_radius ** 2, base_radius ** 2, tip_radius ** 2]))\n  else:\n    # If the covariance is full, the covariance matrix is a full matrix with the covariances along the diagonal.\n    # The covariances are calculated as the product of the radius at the base and tip of the cone.\n    covariance = jnp.array([[base_radius ** 2, 0, 0], [0, base_radius ** 2, 0], [0, 0, tip_radius ** 2]])\n\n  return mean, covariance\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # Calculate the mean of the Gaussian based on the cylinder's axis and start and end distances.\n  mean = jnp.array([t0, t0, t0]) + d * t0\n\n  # Calculate the variance of the Gaussian based on the cylinder's radius.\n  variance = jnp.array([radius, radius, radius])\n\n  # Calculate the covariance of the Gaussian based on the cylinder's radius and whether it should be diagonal or full-covariance.\n  if diag:\n    covariance = jnp.diag(variance)\n  else:\n    covariance = jnp.outer(variance, variance)\n\n  # Convert the mean and covariance to a Gaussian distribution using the lift_gaussian function.\n  gaussian = lift_gaussian(mean, covariance)\n\n  return gaussian\n\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Compute pixel coordinates in camera space\n  pix_x = xnp.array(pix_x_int, dtype=xnp.float32)\n  pix_y = xnp.array(pix_y_int, dtype=xnp.float32)\n  pix_xy = xnp.stack([pix_x, pix_y], axis=-1)\n  pix_xy_cam = xnp.matmul(pix_xy, pixtocams)\n\n  # Compute ray origins and directions in world space\n  if camtype == ProjectionType.PERSPECTIVE:\n    origins = xnp.matmul(pix_xy_cam, camtoworlds)\n    directions = xnp.matmul(pix_xy_cam, camtoworlds[...,:3])\n  elif camtype == ProjectionType.FISHEYE:\n    origins = xnp.matmul(pix_xy_cam, camtoworlds)\n    directions = xnp.matmul(pix_xy_cam, camtoworlds[...,:3])\n  elif camtype == ProjectionType.PANORAMIC:\n    origins = xnp.matmul(pix_xy_cam, camtoworlds)\n    directions = xnp.matmul(pix_xy_cam, camtoworlds[...,:3])\n  else:\n    raise ValueError(f\"Unknown camera type: {camtype}\")\n\n  # Compute normalized view directions\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute differential radii\n  radii = xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute image plane coordinates\n  imageplane = xnp.matmul(pix_xy, pixtocams[...,:2])\n\n  # Apply distortion correction if provided\n  if distortion_params is not None:\n    imageplane = apply_distortion(imageplane, distortion_params)\n\n  # Convert to NDC if requested\n  if pixtocam_ndc is not None:\n    origins = xnp.matmul(origins, pixtocam_ndc)\n    directions = xnp.matmul(directions, pixtocam_ndc)\n    viewdirs = xnp.matmul(viewdirs, pixtocam_ndc)\n    imageplane = xnp.matmul"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the product of density and the adjusted distance between points\n  product = density * tdist\n\n  # Call the helper function to compute the alpha weights\n  return compute_alpha_weights_helper(product, dirs, **kwargs)\n\n"}
{"namespace": "stepfun.sample", "completion": "  # Check input arguments\n  assert t.ndim == 1, \"t must be a 1D array\"\n  assert w_logits.ndim == 1, \"w_logits must be a 1D array\"\n  assert t.shape[0] == w_logits.shape[0], \"t and w_logits must have the same length\"\n  assert t.shape[0] >= 2, \"t must have at least 2 elements\"\n  assert num_samples > 0, \"num_samples must be greater than 0\"\n\n  # Get the number of bins\n  num_bins = t.shape[0]\n\n  # Get the bin weights\n  w = jnp.exp(w_logits)\n\n  # Get the bin widths\n  w_sum = jnp.sum(w)\n  w_cumsum = jnp.cumsum(w)\n  w_cumsum = jnp.concatenate([[0], w_cumsum])\n  w_diff = w_cumsum[1:] - w_cumsum[:-1]\n  w_diff = jnp.concatenate([w_diff, [w_sum - w_cumsum[-1]]])\n  w_diff = w_diff / w_sum\n\n  # Get the bin centers\n  t_diff = t[1:] - t[:-1]\n  t_diff = jnp.concatenate([t_diff, [t[-1] - t[-2]]])\n  t_diff = t_diff / 2\n  t_cumsum = jnp.cumsum(t_diff)\n  t_cumsum = jnp.concatenate([[0], t_cumsum])\n  t_cumsum = t_cumsum / t_sum\n  t_cumsum = t_cumsum + t[0]\n  t_cumsum = jnp.concatenate([t_cumsum, [t[-1]]])\n\n  # Get the bin endpoints\n  t_cumsum_diff = t_cumsum[1:] - t_cumsum[:-1]\n  t_cumsum_diff = jnp.concatenate([t_cumsum_diff, [t_cumsum[-1] - t_cumsum[-2]]])\n  t_cumsum_diff = t_cumsum_diff / 2\n  t_cumsum_diff = t_cumsum_"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Check if the domain is valid\n  if domain[0] >= domain[1]:\n    raise ValueError(\"Invalid domain: minval must be less than maxval.\")\n\n  # Check if the number of samples is valid\n  if num_samples < 1:\n    raise ValueError(\"Invalid number of samples: must be greater than 0.\")\n\n  # Check if the number of bins is valid\n  if len(t) < 2:\n    raise ValueError(\"Invalid number of bins: must be greater than 1.\")\n\n  # Check if the number of weights is valid\n  if len(w_logits) != len(t) - 1:\n    raise ValueError(\"Invalid number of weights: must be equal to the number of bins minus 1.\")\n\n  # Check if the weights are valid\n  if not jnp.all(jnp.isfinite(w_logits)):\n    raise ValueError(\"Invalid weights: must be finite.\")\n\n  # Check if the weights are valid\n  if not jnp.all(jnp.isfinite(t)):\n    raise ValueError(\"Invalid bin endpoints: must be finite.\")\n\n  # Check if the bin endpoints are sorted\n  if not jnp.all(jnp.diff(t) >= 0):\n    raise ValueError(\"Invalid bin endpoints: must be sorted.\")\n\n  # Check if the bin endpoints are within the domain\n  if not jnp.all(t >= domain[0]) or not jnp.all(t <= domain[1]):\n    raise ValueError(\"Invalid bin endpoints: must be within the domain.\")\n\n  # Check if the bin endpoints are valid\n  if not jnp.all(jnp.diff(t) > 0):\n    raise ValueError(\"Invalid bin endpoints: must be strictly increasing.\")\n\n  # Check if the bin endpoints are valid\n  if not jnp.all(jnp.diff(t) > 0):\n    raise ValueError(\"Invalid bin endpoints: must be strictly increasing.\")\n\n  # Check if the bin endpoints are valid\n  if not jnp.all(jnp.diff(t) > 0):\n    raise ValueError(\"Invalid bin endpoints: must be strictly increasing.\")\n\n  # Check if the bin endpoints are valid\n  if not jnp.all(jnp.diff(t) > 0):\n    raise ValueError(\"Invalid bin endpoints: must be strictly increasing.\")\n\n  # Check if the bin endpoints are valid\n  if not jnp.all("}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1.\n  w = w / np.sum(w)\n\n  # Integrate the weights to get the cumulative weights.\n  cw = np.cumsum(w)\n\n  # Interpolate the cumulative weights to get the x-coordinates for the given percentiles.\n  x = np.interp(ps, np.linspace(0, 100, len(cw)), cw)\n\n  # Interpolate the x-coordinates to get the corresponding y-coordinates.\n  y = np.interp(x, cw, t)\n\n  return y\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF) by normalizing the weights to sum to 1.\n  pdf = w / np.sum(w)\n\n  # Blur the PDF by convolving it with a Gaussian kernel.\n  kernel = np.exp(-0.5 * np.power(np.arange(-blur_halfwidth, blur_halfwidth + 1) / blur_halfwidth, 2))\n  blurred_pdf = np.convolve(pdf, kernel, mode='same') / np.sum(kernel)\n\n  # Resample the blurred PDF to match the new time points.\n  resampled_weights = np.interp(tq, t, blurred_pdf)\n\n  return resampled_weights\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Get the number of points in the collection\n  num_points = vectors.shape[0]\n\n  # Get the number of dimensions of the points\n  num_dims = vectors.shape[1]\n\n  # Create a new array to hold the transformed points\n  transformed_points = np.zeros((num_points, num_dims))\n\n  # Apply the transformation to each point in the collection\n  for i in range(num_points):\n    transformed_points[i] = apply_homogeneous_transform_to_point(transform, vectors[i])\n\n  # Return the transformed points\n  return transformed_points\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input tensors are valid\n  if not isinstance(t, torch.Tensor):\n    raise TypeError(\"Input `t` must be a torch.Tensor.\")\n  if not isinstance(tp, torch.Tensor):\n    raise TypeError(\"Input `tp` must be a torch.Tensor.\")\n  if not isinstance(vp, torch.Tensor):\n    raise TypeError(\"Input `vp` must be a torch.Tensor.\")\n  if not t.ndim == 1:\n    raise ValueError(\"Input `t` must be a 1-D tensor.\")\n  if not tp.ndim == 1:\n    raise ValueError(\"Input `tp` must be a 1-D tensor.\")\n  if not vp.ndim == 1:\n    raise ValueError(\"Input `vp` must be a 1-D tensor.\")\n  if not t.shape[0] == tp.shape[0] + 1:\n    raise ValueError(\"Input `t` must have the same number of elements as `tp` plus 1.\")\n  if not t.shape[0] == vp.shape[0]:\n    raise ValueError(\"Input `t` must have the same number of elements as `vp`.\")\n\n  # Resample the step function\n  if use_avg:\n    # Compute the width of each interval\n    w = t[1:] - t[:-1]\n    # Compute the average value of the step function for each interval\n    vp_avg = (vp[1:] + vp[:-1]) / 2\n    # Compute the average value of the step function for each interval\n    vp_avg = (vp_avg * w).sum() / w.sum()\n    # Return the average value\n    return vp_avg\n  else:\n    # Compute the width of each interval\n    w = t[1:] - t[:-1]\n    # Compute the sum of the step function for each interval\n    vp_sum = (vp[1:] + vp[:-1]) * w\n    # Return the sum of the step function\n    return vp_sum.sum()\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance of the coordinates\n  scaled_mean = mean / 2 ** (max_deg - 1)\n  scaled_var = var / 2 ** (max_deg - 1)\n\n  # Concatenate the scaled mean and variance\n  concat_mean_var = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply sinusoidal encoding to the concatenated mean and variance\n  encoded_variables = jnp.sin(concat_mean_var * 2 ** jnp.arange(min_deg, max_deg))\n\n  return encoded_variables\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Create a function that evaluates the directional encoding for given inputs.\n  def dir_enc_fn(points):\n\n    # Get the number of points.\n    num_points = points.shape[0]\n\n    # Create a matrix to store the directional encoding of each point.\n    dir_enc = np.zeros((num_points, 1))\n\n    # Iterate over each point.\n    for i in range(num_points):\n\n      # Get the direction of the point.\n      point_dir = points[i, :]\n\n      # Get the directional encoding of the point.\n      dir_enc[i, 0] = dir_enc_fn_int(point_dir, deg_view)\n\n    # Return the directional encoding of each point.\n    return dir_enc\n\n  # Return the directional encoding function.\n  return dir_enc_fn\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Remove duplicate lines\n    lines = list(dict.fromkeys(lines))\n\n    # Remove empty lines\n    lines = [line for line in lines if line.strip()]\n\n    # Remove lines that are only numbers\n    lines = [line for line in lines if not line.strip().isnumeric()]\n\n    # Remove lines that are only punctuation\n    lines = [line for line in lines if not line.strip().isalpha()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip().isspace()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip().isspace()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip().isspace()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip().isspace()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip().isspace()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip().isspace()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip().isspace()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip().isspace()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip().isspace()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip().isspace()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip().isspace()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip().isspace()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip().isspace()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip().isspace()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip().isspace()]\n\n    # Remove lines that are only whitespace\n    lines = [line for line in lines if not line.strip"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # If the input text is empty or None, return the input as is\n    if not org_texts:\n        return org_texts\n\n    # Tokenize the text into sentences using the nltk_tokenizer\n    sentences = nltk_tokenizer.tokenize(org_texts)\n\n    # Normalize quotation marks within the text\n    sentences = [quotation_pattern.sub(r'\"\\1\"', sentence) for sentence in sentences]\n\n    # Apply the rules and patterns to normalize the sentences\n    sentences = [space_rule.sub(' ', sentence) for sentence in sentences]\n    sentences = [bracket_rule.sub('', sentence) for sentence in sentences]\n    sentences = [rules.sub('', sentence) for sentence in sentences]\n\n    # Return the tokenized sentences\n    return sentences\n\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        # If a key is provided, return the positions for that document only.\n        if key is not None:\n            return self.positions_by_key(token, key)\n\n        # If no key is provided, return the positions across all documents.\n        else:\n            return self.positions_across_all_docs(token)\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # If the spec is a percentage, convert it to an absolute number of clauses\n    if spec.endswith('%'):\n        spec = str(int(num_clauses * float(spec[:-1]) / 100))\n\n    # If the spec is a conditional expression, evaluate it and return the appropriate value\n    if '<' in spec:\n        parts = spec.split('<')\n        if len(parts) == 2:\n            if eval(parts[0]):\n                return int(parts[1])\n            else:\n                return 0\n        else:\n            raise ValueError('Invalid conditional expression in min should match specification: ' + spec)\n\n    # If the spec is an absolute number, return it\n    try:\n        return int(spec)\n    except ValueError:\n        raise ValueError('Invalid min should match specification: ' + spec)\n\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_unique(tokens)\n        else:\n            return self.phrase_freq_nonunique(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Initialize the SearchArray instance\n        search_array = cls()\n\n        # Initialize the term dictionary\n        term_dict = {}\n\n        # Initialize the term matrix\n        term_matrix = []\n\n        # Initialize the positions\n        positions = []\n\n        # Initialize the document lengths\n        doc_lengths = []\n\n        # Initialize the average document length\n        avg_doc_len = 0\n\n        # Initialize the number of documents\n        num_docs = 0\n\n        # Initialize the number of terms\n        num_terms = 0\n\n        # Initialize the number of tokens\n        num_tokens = 0\n\n        # Initialize the number of unique tokens\n        num_unique_tokens = 0\n\n        # Initialize the number of batches\n        num_batches = 0\n\n        # Initialize the number of documents in the current batch\n        num_docs_in_batch = 0\n\n        # Initialize the number of tokens in the current batch\n        num_tokens_in_batch = 0\n\n        # Initialize the number of unique tokens in the current batch\n        num_unique_tokens_in_batch = 0\n\n        # Initialize the number of terms in the current batch\n        num_terms_in_batch = 0\n\n        # Initialize the number of documents in the current batch\n        num_docs_in_batch = 0\n\n        # Initialize the number of tokens in the current batch\n        num_tokens_in_batch = 0\n\n        # Initialize the number of unique tokens in the current batch\n        num_unique_tokens_in_batch = 0\n\n        # Initialize the number of terms in the current batch\n        num_terms_in_batch = 0\n\n        # Initialize the number of documents in the current batch\n        num_docs_in_batch = 0\n\n        # Initialize the number of tokens in the current batch\n        num_tokens_in_batch = 0\n\n        # Initialize the number of unique tokens in the current batch\n        num_unique_tokens_in_batch = 0\n\n        # Initialize the number of terms in the current batch\n        num_terms_in_batch = 0\n\n        # Initialize the number of documents in the current batch\n        num_docs_in_batch = 0\n\n        # Initialize the number of tokens in the current batch\n        num_tokens_in_batch = 0\n\n        # Initialize the number of unique tokens in the current batch\n        num_unique_tokens_in_batch = 0\n\n        # Initialize the number of terms in the current"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Set up the server\n        self.server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.server.bind((self.config.get('server_ip'), self.config.get('server_port')))\n        self.server.listen(self.config.get('server_backlog'))\n\n        # Set up the dictionary to keep track of connections\n        self.connections = {}\n\n        # Set up the lock for thread safety\n        self.lock = threading.Lock()\n\n        # Start the server\n        self.server.setblocking(False)\n        self.server.settimeout(0.0)\n        self.server_thread = threading.Thread(target=self.server_loop)\n        self.server_thread.start()\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.array(arr, dtype=np.uint64)\n    arr = np.right_shift(arr, 32)\n    arr = np.bitwise_or(arr, np.left_shift(np.bitwise_and(arr, 0xffffffff), 32))\n    arr = np.right_shift(arr, 16)\n    arr = np.bitwise_or(arr, np.left_shift(np.bitwise_and(arr, 0xffffffff), 16))\n    arr = np.right_shift(arr, 8)\n    arr = np.bitwise_or(arr, np.left_shift(np.bitwise_and(arr, 0xffffffff), 8))\n    arr = np.right_shift(arr, 4)\n    arr = np.bitwise_or(arr, np.left_shift(np.bitwise_and(arr, 0xffffffff), 4))\n    arr = np.right_shift(arr, 2)\n    arr = np.bitwise_or(arr, np.left_shift(np.bitwise_and(arr, 0xffffffff), 2))\n    arr = np.right_shift(arr, 1)\n    arr = np.bitwise_or(arr, np.left_shift(np.bitwise_and(arr, 0xffffffff), 1))\n    return arr\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Initialize the explanation string\n    explanation = \"\"\n\n    # Initialize the scores array\n    scores = np.zeros(len(frame))\n\n    # Perform the search\n    for i, row in frame.iterrows():\n        # Initialize the score for the current document\n        score = 0.0\n\n        # Check if the document matches the query\n        if q_op == \"OR\":\n            if q in row.values:\n                score += 1.0\n        else:\n            if q in row.values:\n                score += 1.0\n\n        # Check if the document matches the query fields\n        for field in qf:\n            if q in row[field]:\n                score += 1.0\n\n        # Check if the document matches the minimum match specification\n        if mm is not None:\n            if mm in row.values:\n                score += 1.0\n\n        # Check if the document matches the phrase matches\n        if pf is not None:\n            for field in pf:\n                if q in row[field]:\n                    score += 1.0\n\n        # Check if the document matches the bigram matches\n        if pf2 is not None:\n            for field in pf2:\n                if q in row[field]:\n                    score += 1.0\n\n        # Check if the document matches the trigram matches\n        if pf3 is not None:\n            for field in pf3:\n                if q in row[field]:\n                    score += 1.0\n\n        # Update the scores array\n        scores[i] = score\n\n    # Sort the scores in descending order\n    sorted_indices = np.argsort(scores)[::-1]\n\n    # Update the explanation string\n    explanation += f\"The scores were calculated using the following parameters: \\n\"\n    explanation += f\"Query: {q} \\n\"\n    explanation += f\"Query Fields: {qf} \\n\"\n    explanation += f\"Minimum Match: {mm} \\n\"\n    explanation += f\"Phrase Fields: {pf} \\n\"\n    explanation += f\"Bigram Fields: {pf2} \\n\"\n    explanation += f\"Trigram Fields: {pf3} \\n\"\n    explanation += f\"Query Operator: {q_op} \\n\"\n    explanation += f\"Similarity Measure: {similarity} \\n\"\n\n    # Return the scores and the explanation\n    return scores[sorted_indices], explanation\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.close()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop the server if it is running.\n        if self.server is not None:\n            self.server.stop()\n\n        # Stop all connections.\n        for connection in self.connections:\n            connection.stop()\n\n        # Clear the connections list.\n        self.connections.clear()\n"}
