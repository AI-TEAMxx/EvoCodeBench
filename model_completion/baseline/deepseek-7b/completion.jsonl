{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"\\n{cmd}\\n\")\n    return system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    exit_status = os.system(cmd)\n\n    return exit_status\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    return x.shape[0] == x.shape[1]"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        return False\n\n    # Check if the matrix is Hermitian\n    if not np.allclose(x, x.T):\n        return False\n\n    # Check if the matrix is positive definite\n    try:\n        np.linalg.cholesky(x)\n    except np.linalg.LinAlgError:\n        return False\n\n    return True"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    if x.shape[0] != x.shape[1]:\n        return False\n    else:\n        try:\n            np.linalg.cholesky(x)\n            return True\n        except np.linalg.LinAlgError:\n            return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        return False\n\n    # Check if the matrix is Hermitian\n    if not np.allclose(x, x.conj().T):\n        return False\n\n    # Check if the matrix is positive definite\n    if not np.all(np.linalg.eigvals(x) > 0):\n        return False\n\n    return True\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n    except:\n        return False\n    else:\n        return True\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    if x.shape[0] != x.shape[1]:\n        return False\n    else:\n        try:\n            np.linalg.cholesky(x)\n            return True\n        except:\n            return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        return False\n\n    # Check if the matrix is Hermitian\n    if not np.allclose(x, x.T):\n        return False\n\n    # Check if the matrix is positive definite\n    if not np.all(np.linalg.eigvals(x) > 0):\n        return False\n\n    # If all the above conditions are met, the matrix is suitable for Cholesky decomposition\n    return True\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        return False\n\n    # Check if the matrix is Hermitian\n    if not is_hermitian(x):\n        return False\n\n    # Check if the matrix is positive definite\n    if not is_positive_definite(x):\n        return False\n\n    # If all checks pass, the matrix is suitable for Cholesky decomposition\n    return True\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    return x.shape[0] == x.shape[1]\n\n"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n  z_norm = np.linalg.norm(z)\n  if z_norm > 1:\n    z_scaled = z / z_norm\n  else:\n    z_scaled = z\n  return z_scaled"}
{"namespace": "coord.inv_contract", "completion": "  ### Initialization\n  import numpy as np\n\n  ### Computation\n  z_inv = np.zeros(z.shape)\n  z_inv[0] = z[0]\n  z_inv[1] = z[1]\n  z_inv[2] = z[2]\n  z_inv[3] = z[3]\n  z_inv[4] = z[4]\n  z_inv[5] = z[5]\n  z_inv[6] = z[6]\n  z_inv[7] = z[7]\n  z_inv[8] = z[8]\n  z_inv[9] = z[9]\n  z_inv[10] = z[10]\n  z_inv[11] = z[11]\n  z_inv[12] = z[12]\n  z_inv[13] = z[13]\n  z_inv[14] = z[14]\n  z_inv[15] = z[15]\n  z_inv[16] = z[16]\n  z_inv[17] = z[17]\n  z_inv[18] = z[18]\n  z_inv[19] = z[19]\n  z_inv[20] = z[20]\n  z_inv[21] = z[21]\n  z_inv[22] = z[22]\n  z_inv[23] = z[23]\n  z_inv[24] = z[24]\n  z_inv[25] = z[25]\n  z_inv[26] = z[26]\n  z_inv[27] = z[27]\n  z_inv[28] = z[28]\n  z_inv[29] = z[29]\n  z_inv[30] = z[30]\n  z_inv[31] = z[31]\n  z_"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  z_inv = np.zeros(len(z))\n\n  for i in range(len(z)):\n    if z[i] > 0:\n      z_inv[i] = z[i]**2\n    elif z[i] < 0:\n      z_inv[i] = -(z[i]**2)\n    else:\n      z_inv[i] = 0\n\n  return z_inv"}
{"namespace": "coord.inv_contract", "completion": "  z_inv = np.zeros(z.shape)\n  for i in range(z.shape[0]):\n    if z[i] < 0:\n      z_inv[i] = -np.sqrt(z[i]**2)\n    else:\n      z_inv[i] = np.sqrt(z[i]**2)\n  return z_inv"}
{"namespace": "coord.inv_contract", "completion": "  # Input-Output Arguments\n  # z : Array-like. The input vector for which the inverse operation of contract is to be computed. \n  # return : Array-like. The scaled vector after applying the inverse operation of the contract function.\n\n  # Computes the inverse of the contract function for a given input. It scales the input vector in a specific way, ensuring the output vector has a magnitude that correctly inverses the operation of the contract function within a certain domain.\n\n  # Input-Output Arguments\n  # z : Array-like. The input vector for which the inverse operation of contract is to be computed. \n  # return : Array-like. The scaled vector after applying the inverse operation of the contract function.\n\n  # Computes the inverse of the contract function for a given input. It scales the input vector in a specific way, ensuring the output vector has a magnitude that correctly inverses the operation of the contract function within a certain domain.\n\n  # Input-Output Arguments\n  # z : Array-like. The input vector for which the inverse operation of contract is to be computed. \n  # return : Array-like. The scaled vector after applying the inverse operation of the contract function.\n\n  # Computes the inverse of the contract function for a given input. It scales the input vector in a specific way, ensuring the output vector has a magnitude that correctly inverses the operation of the contract function within a certain domain.\n\n  # Input-Output Arguments\n  # z : Array-like. The input vector for which the inverse operation of contract is to be computed. \n  # return : Array-like. The scaled vector after applying the inverse operation of the contract function.\n\n  # Computes the inverse of the contract function for a given input. It scales the input vector in a specific way, ensuring the output vector has a magnitude that correctly inverses the operation of the contract function within a certain domain.\n\n  # Input-Output Arguments\n  # z : Array-like. The input vector for which the inverse operation of contract is to be computed. \n  # return : Array-like. The scaled vector after applying the inverse operation of the contract function.\n\n  # Computes the inverse of the contract function for a given input. It scales the"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  # Compute the norm of the input vector\n  norm_z = np.linalg.norm(z)\n\n  # Compute the scaling factor based on the norm of the input vector\n  scale_factor = np.sqrt(1 / (1 + norm_z ** 2))\n\n  # Scale the input vector using the scaling factor\n  scaled_z = scale_factor * z\n\n  return scaled_z\n"}
{"namespace": "coord.inv_contract", "completion": "  # Import required libraries\n  import numpy as np\n\n  # Compute the inverse of the contract function\n  z_inv = np.zeros(z.shape)\n  for i in range(z.shape[0]):\n    z_inv[i] = np.sign(z[i]) * (np.sqrt(np.abs(z[i])) - 1)\n\n  return z_inv"}
{"namespace": "coord.inv_contract", "completion": "  z_inv = np.zeros(len(z))\n\n  for i in range(len(z)):\n    z_inv[i] = z[i]\n\n  z_inv = z_inv / np.linalg.norm(z_inv)\n\n  return z_inv"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  z_mag = np.linalg.norm(z)\n  if z_mag > 1:\n    z_scaled = z / z_mag\n  else:\n    z_scaled = z\n\n  return z_scaled\n\n"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  # Define the domain of the contract function\n  domain = np.array([[-1, 1], [-1, 1], [-1, 1]])\n\n  # Compute the inverse of the contract function\n  z_inv = z * (domain[0][1] - domain[0][0]) + domain[0][0]\n\n  return z_inv"}
{"namespace": "coord.inv_contract", "completion": "  # Compute the inverse of the contract function\n  z_inv = np.zeros(z.shape)\n  for i in range(z.shape[0]):\n    if z[i] > 0:\n      z_inv[i] = 1 / z[i]\n    else:\n      z_inv[i] = -1 / z[i]\n\n  # Scale the vector to ensure the output vector has a magnitude that correctly inverses the operation of the contract function within a certain domain\n  z_inv_scaled = z_inv / np.linalg.norm(z_inv)\n\n  return z_inv_scaled\n"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  # Compute the norm of the input vector\n  z_norm = np.linalg.norm(z)\n\n  # Compute the scaling factor based on the norm of the input vector\n  if z_norm > 0.5:\n    scale_factor = 1 / (1 + z_norm)\n  else:\n    scale_factor = 1 / (1 - z_norm)\n\n  # Scale the input vector using the computed scaling factor\n  z_scaled = scale_factor * z\n\n  # Return the scaled vector\n  return z_scaled"}
{"namespace": "coord.inv_contract", "completion": "  # The input vector z is scaled in a specific way to ensure the output vector has a magnitude that correctly inverses the operation of the contract function within a certain domain.\n  z_scaled = (z / np.sqrt(np.sum(z**2))) * np.sqrt(np.sum(z**2) / np.sum(z**2 - 1))\n\n  return z_scaled"}
{"namespace": "coord.inv_contract", "completion": "  z_norm = np.linalg.norm(z)\n  if z_norm > 1.0:\n    z_norm = 1.0\n  if z_norm < 0.0:\n    z_norm = 0.0\n  z_norm = np.sqrt(1.0 - z_norm**2)\n  z_inv = z_norm * z\n  return z_inv"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n  from scipy.linalg import norm\n\n  # Define the domain of the contract function\n  domain = np.array([[0, 1], [0, 1], [0, 1]])\n\n  # Compute the maximum magnitude of the domain\n  domain_max = norm(domain.max(axis=0) - domain.min(axis=0))\n\n  # Compute the maximum magnitude of the input vector\n  z_max = norm(z)\n\n  # Compute the scaling factor\n  scale_factor = domain_max / z_max\n\n  # Scale the input vector\n  z_scaled = z * scale_factor\n\n  # Return the scaled vector\n  return z_scaled"}
{"namespace": "coord.inv_contract", "completion": "  ### Importing Libraries\n  import numpy as np\n\n  ### Computing Inverse of Contract Function\n  z = np.array(z)\n  z_mag = np.linalg.norm(z)\n  z_inv = z / z_mag\n  z_inv = z_inv * (1 / (1 + z_mag))\n\n  return z_inv"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  z = np.array(z)\n\n  if z.ndim == 1:\n    z = np.array([z])\n\n  z_scaled = np.zeros(z.shape)\n\n  for i in range(z.shape[0]):\n    for j in range(z.shape[1]):\n      if z[i,j] < 0:\n        z_scaled[i,j] = -np.sqrt(z[i,j])\n      elif z[i,j] > 0:\n        z_scaled[i,j] = np.sqrt(z[i,j])\n\n  return z_scaled"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  # Define the domain of the contract function\n  domain = np.array([[1, 1, 1], [1, 1, -1], [1, -1, 1], [1, -1, -1], [-1, 1, 1], [-1, 1, -1], [-1, -1, 1], [-1, -1, -1]])\n\n  # Scale the input vector to the domain\n  z_scaled = z / np.linalg.norm(z) * np.linalg.norm(domain)\n\n  # Compute the inverse of the contract function\n  z_inv = np.zeros_like(z)\n  for i in range(len(domain)):\n    z_inv += np.dot(z_scaled, domain[i]) * domain[i]\n\n  # Scale the inverse vector back to the original domain\n  z_inv_scaled = z_inv / np.linalg.norm(z_inv) * np.linalg.norm(z)\n\n  return z_inv_scaled\n"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n\n  ## Checking if the input is a numpy array\n  if type(z) is not np.ndarray:\n    z = np.array(z)\n\n  ## Computing the magnitude of the input vector\n  z_mag = np.linalg.norm(z)\n\n  ## Checking if the magnitude of the input vector is within the domain of the contract function\n  if z_mag > 1:\n    raise ValueError(\"The magnitude of the input vector should be less than or equal to 1.\")\n\n  ## Computing the inverse of the contract function\n  z_inv = z / (1 + z_mag)\n\n  ## Scaling the inverse of the contract function to ensure the output vector has a magnitude of 1\n  z_inv_mag = np.linalg.norm(z_inv)\n  z_inv_scaled = z_inv / z_inv_mag\n\n  return z_inv_scaled\n\n"}
{"namespace": "coord.inv_contract", "completion": "  import numpy as np\n  from scipy.special import j1\n\n  # Compute the inverse of the contract function for a given input\n  z_inv = np.zeros(z.shape)\n  for i in range(z.shape[0]):\n    z_inv[i] = (z[i] - 2 * np.pi * np.floor(z[i] / (2 * np.pi))) / (2 * np.pi)\n\n  # Scale the input vector in a specific way\n  z_inv = z_inv * 2 * np.pi\n  z_inv = z_inv - 2 * np.pi * np.floor(z_inv / (2 * np.pi))\n\n  return z_inv\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        import sqlite3\n\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n            cursor.execute(\n                f\"CREATE TABLE IF NOT EXISTS {func_name} (args TEXT, kwargs TEXT, result TEXT)\"\n            )\n            conn.commit()\n            cursor.execute(\n                f\"SELECT result FROM {func_name} WHERE args=? AND kwargs=?\",\n                (str(args), str(kwargs)),\n            )\n            result = cursor.fetchone()\n            if result:\n                return eval(result[0])\n            else:\n                result = func(*args, **kwargs)\n                cursor.execute(\n                    f\"INSERT INTO {func_name} (args, kwargs, result) VALUES (?, ?, ?)\",\n                    (str(args), str(kwargs), str(result)),\n                )\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        import sqlite3\n\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n            cursor.execute(\n                \"CREATE TABLE IF NOT EXISTS cache (func_name text, args text, kwargs text, result text)\"\n            )\n            conn.commit()\n            cursor.execute(\n                \"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = cursor.fetchone()\n            if result:\n                return result[0]\n            else:\n                result = func(*args, **kwargs)\n                cursor.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, str(args), str(kwargs), str(result)),\n                )\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    from functools import wraps\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\n                \"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\"\n            )\n            conn.commit()\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = c.fetchone()\n            if result:\n                return eval(result[0])\n            else:\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, str(args), str(kwargs), str(result)),\n                )\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        import sqlite3\n\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\n                f\"CREATE TABLE IF NOT EXISTS {func_name} (args TEXT, kwargs TEXT, result TEXT)\"\n            )\n            c.execute(\n                f\"SELECT result FROM {func_name} WHERE args = ? AND kwargs = ?\",\n                (str(args), str(kwargs)),\n            )\n            result = c.fetchone()\n            if result:\n                return eval(result[0])\n            else:\n                result = func(*args, **kwargs)\n                c.execute(\n                    f\"INSERT INTO {func_name} (args, kwargs, result) VALUES (?, ?, ?)\",\n                    (str(args), str(kwargs), str(result)),\n                )\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    from functools import wraps\n    import sqlite3\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n            cursor.execute(\"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\")\n            conn.commit()\n            cursor.execute(\"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\", (func_name, str(args), str(kwargs)))\n            result = cursor.fetchone()\n            if result:\n                return eval(result[0])\n            else:\n                result = func(*args, **kwargs)\n                cursor.execute(\"INSERT INTO cache VALUES (?, ?, ?, ?)\", (func_name, str(args), str(kwargs), str(result)))\n                conn.commit()\n                return result\n        return wrapper\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        import sqlite3\n        import pickle\n\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n            cursor.execute(\n                \"SELECT result FROM cache WHERE func_name = ? AND args = ? AND kwargs = ?\",\n                (func_name, pickle.dumps(args), pickle.dumps(kwargs)),\n            )\n            result = cursor.fetchone()\n            if result:\n                result = pickle.loads(result[0])\n            else:\n                result = func(*args, **kwargs)\n                cursor.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, pickle.dumps(args), pickle.dumps(kwargs), pickle.dumps(result)),\n                )\n                conn.commit()\n            conn.close()\n            return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        import sqlite3\n\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Check if the result of the function call is already stored in the database\n            cursor.execute(\"SELECT result FROM cache WHERE func_name = ?\", (func_name,))\n            result = cursor.fetchone()\n\n            if result:\n                # If the result is already stored, return it\n                return result[0]\n            else:\n                # If the result is not stored, compute it and store it in the database\n                result = func(*args, **kwargs)\n                cursor.execute(\"INSERT INTO cache (func_name, result) VALUES (?, ?)\", (func_name, result))\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        import sqlite3\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\n                f\"CREATE TABLE IF NOT EXISTS {func_name} (args TEXT, kwargs TEXT, result TEXT)\"\n            )\n            conn.commit()\n            c.execute(\n                f\"SELECT result FROM {func_name} WHERE args=? AND kwargs=?\",\n                (str(args), str(kwargs)),\n            )\n            result = c.fetchone()\n            if result:\n                return eval(result[0])\n            else:\n                result = func(*args, **kwargs)\n                c.execute(\n                    f\"INSERT INTO {func_name} VALUES (?, ?, ?)\",\n                    (str(args), str(kwargs), str(result)),\n                )\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import functools\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Check if the result of the function call is already stored in the database\n            c.execute(\n                f\"SELECT result FROM cache WHERE func_name = '{func_name}' AND args = '{str(args)}' AND kwargs = '{str(kwargs)}'\"\n            )\n            result = c.fetchone()\n\n            # If the result is not stored in the database, compute it and store it\n            if result is None:\n                result = func(*args, **kwargs)\n                c.execute(\n                    f\"INSERT INTO cache (func_name, args, kwargs, result) VALUES ('{func_name}', '{str(args)}', '{str(kwargs)}', '{str(result)}')\"\n                )\n                conn.commit()\n\n            # Close the database connection\n            conn.close()\n\n            # Return the result\n            return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\n                \"CREATE TABLE IF NOT EXISTS cache (func_name text, args text, kwargs text, result text)\"\n            )\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = c.fetchone()\n            if result is None:\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, str(args), str(kwargs), str(result)),\n                )\n                conn.commit()\n            else:\n                result = result[0]\n            conn.close()\n            return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import functools\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Check if the result is already stored in the database\n            cursor.execute(\n                \"SELECT result FROM cache WHERE func_name = ? AND args = ? AND kwargs = ?\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = cursor.fetchone()\n            if result:\n                # If the result is stored, return it\n                return result[0]\n\n            # If the result is not stored, compute it\n            result = func(*args, **kwargs)\n\n            # Store the result in the database\n            cursor.execute(\n                \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                (func_name, str(args), str(kwargs), result),\n            )\n            conn.commit()\n\n            return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        import sqlite3\n\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Check if the result of the function call is already stored in the database\n            cursor.execute(\"SELECT result FROM cache WHERE func_name = ?\", (func_name,))\n            result = cursor.fetchone()\n\n            if result:\n                # If the result is already stored, return it from the database\n                return result[0]\n            else:\n                # If the result is not stored, compute it and store it in the database\n                result = func(*args, **kwargs)\n                cursor.execute(\"INSERT INTO cache (func_name, result) VALUES (?, ?)\", (func_name, result))\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n            cursor.execute(\n                \"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\"\n            )\n            conn.commit()\n\n            # Check if the result of the function call is already stored in the database\n            cursor.execute(\n                \"SELECT result FROM cache WHERE func_name = ? AND args = ? AND kwargs = ?\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = cursor.fetchone()\n\n            # If the result is not stored, compute it and store it in the database\n            if result is None:\n                result = func(*args, **kwargs)\n                cursor.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, str(args), str(kwargs), str(result)),\n                )\n                conn.commit()\n\n            # Return the result\n            return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        import sqlite3\n\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create a table if it doesn't exist\n            c.execute(\n                \"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\"\n            )\n\n            # Check if the result is already in the database\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\",\n                (func_name, str(args), str(kwargs)),\n            )\n            result = c.fetchone()\n            if result:\n                # If the result is in the database, return it\n                return result[0]\n            else:\n                # If the result is not in the database, compute it and store it\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, str(args), str(kwargs), result),\n                )\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        def wrapper(*args, **kwargs):\n            import sqlite3\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(f\"CREATE TABLE IF NOT EXISTS {func_name} (args TEXT, kwargs TEXT, result TEXT)\")\n            conn.commit()\n            c.execute(f\"SELECT * FROM {func_name} WHERE args=? AND kwargs=?\", (str(args), str(kwargs)))\n            result = c.fetchone()\n            if result:\n                return result[2]\n            else:\n                result = func(*args, **kwargs)\n                c.execute(f\"INSERT INTO {func_name} (args, kwargs, result) VALUES (?, ?, ?)\", (str(args), str(kwargs), result))\n                conn.commit()\n                return result\n        return wrapper\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create the table if it doesn't exist\n            c.execute(\n                \"\"\"CREATE TABLE IF NOT EXISTS cache (\n                    func_name TEXT PRIMARY KEY,\n                    result TEXT\n                )\"\"\"\n            )\n\n            # Check if the result is already stored in the database\n            c.execute(\"SELECT result FROM cache WHERE func_name = ?\", (func_name,))\n            result = c.fetchone()\n\n            if result:\n                # If the result is found, return it\n                return result[0]\n            else:\n                # If the result is not found, compute it and store it in the database\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, result) VALUES (?, ?)\",\n                    (func_name, result),\n                )\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import functools\n    import inspect\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Get the function's arguments and default values\n            signature = inspect.signature(func)\n            bound_args = signature.bind(*args, **kwargs)\n            bound_args.apply_defaults()\n            arg_values = tuple(bound_args.arguments.values())\n\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Check if the function's output is already stored in the database\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name = ? AND args = ?\",\n                (func_name, str(arg_values)),\n            )\n            result = c.fetchone()\n\n            # If the function's output is not stored, compute it and store it in the database\n            if result is None:\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, args, result) VALUES (?, ?, ?)\",\n                    (func_name, str(arg_values), result),\n                )\n                conn.commit()\n\n            # Close the database connection\n            conn.close()\n\n            # Return the function's output\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import functools\n    from datetime import datetime\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Check if the result is already stored in the database\n            c.execute(\"SELECT result FROM cache WHERE func_name = ?\", (func_name,))\n            result = c.fetchone()\n            if result:\n                # If the result is already stored, return it\n                return result[0]\n            else:\n                # If the result is not stored, compute it and store it in the database\n                result = func(*args, **kwargs)\n                c.execute(\"INSERT INTO cache (func_name, result, timestamp) VALUES (?, ?, ?)\", (func_name, result, datetime.now()))\n                conn.commit()\n                conn.close()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    import sqlite3\n    import functools\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a connection to the SQLite database\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Check if the result of the function call is already stored in the database\n            cursor.execute(\"SELECT * FROM cache WHERE func_name = ? AND args = ? AND kwargs = ?\", (func_name, str(args), str(kwargs)))\n            result = cursor.fetchone()\n\n            if result is not None:\n                # If the result is found, retrieve it from the database\n                result = result[1]\n            else:\n                # If the result is not found, compute it and store it in the database\n                result = func(*args, **kwargs)\n                cursor.execute(\"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\", (func_name, str(args), str(kwargs), result))\n                conn.commit()\n\n            # Close the connection to the SQLite database\n            conn.close()\n\n            return result\n        return wrapper\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        def wrapper(*args, **kwargs):\n            import sqlite3\n            import pickle\n            import os\n            import uuid\n            import time\n\n            # Create the database file if it doesn't exist\n            if not os.path.exists(filename):\n                os.mknod(filename)\n\n            # Connect to the database\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Create the table if it doesn't exist\n            cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS cache (\n                id TEXT PRIMARY KEY,\n                func_name TEXT,\n                args TEXT,\n                kwargs TEXT,\n                result TEXT,\n                timestamp TEXT\n            )\n            \"\"\")\n\n            # Generate a unique ID for the function call\n            call_id = str(uuid.uuid4())\n\n            # Check if the function call has already been cached\n            cursor.execute(\"SELECT result FROM cache WHERE id = ?\", (call_id,))\n            cached_result = cursor.fetchone()\n\n            if cached_result is not None:\n                # If the function call is cached, retrieve the result from the database\n                result = pickle.loads(cached_result[0])\n            else:\n                # If the function call is not cached, compute the result and store it in the database\n                result = func(*args, **kwargs)\n                cursor.execute(\"INSERT INTO cache (id, func_name, args, kwargs, result, timestamp) VALUES (?, ?, ?, ?, ?, ?)\",\n                               (call_id, func_name, pickle.dumps(args), pickle.dumps(kwargs), pickle.dumps(result), time.time()))\n                conn.commit()\n\n            # Close the database connection\n            conn.close()\n\n            # Return the result\n            return result\n        return wrapper\n    return decorator\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max\")\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n    if x_min >= x_max or y_min >= y_max:\n        raise ValueError(f\"Invalid bounding box for {cls.__name__}: x_min ({x_min}) >= x_max ({x_max}) or y_min ({y_min}) >= y_max ({y_max})\")\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: x_min ({values['x_min']}) >= x_max ({values['x_max']})\"\n        )\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: y_min ({values['y_min']}) >= y_max ({values['y_max']})\"\n        )\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__} x_min ({values['x_min']}) must be less than x_max ({values['x_max']})\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__} y_min ({values['y_min']}) must be less than y_max ({values['y_max']})\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"Invalid bounding box for {cls.__name__}: x_min ({values['x_min']}) >= x_max ({values['x_max']}) or y_min ({values['y_min']}) >= y_max ({values['y_max']})\")\n    return values\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__} x_min must be less than x_max\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__} y_min must be less than y_max\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__} x_min must be less than x_max\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__} y_min must be less than y_max\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min should be less than x_max\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min should be less than y_max\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: x_min must be less than x_max, but got {values['x_min']} >= {values['x_max']}\"\n        )\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: y_min must be less than y_max, but got {values['y_min']} >= {values['y_max']}\"\n        )\n    return values\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"Invalid bounding box for {cls.__name__}: {values}\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"Invalid bounding box for {cls.__name__}. x_min ({values['x_min']}) must be less than x_max ({values['x_max']})\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"Invalid bounding box for {cls.__name__}. y_min ({values['y_min']}) must be less than y_max ({values['y_max']})\")\n    return values\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__} x_min ({x_min}) must be less than x_max ({x_max})\")\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__} y_min ({y_min}) must be less than y_max ({y_max})\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__} - Invalid bounding box x_min ({values['x_min']}) >= x_max ({values['x_max']}).\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__} - Invalid bounding box y_min ({values['y_min']}) >= y_max ({values['y_max']}).\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: Invalid bounding box: x_min must be less than x_max\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: Invalid bounding box: y_min must be less than y_max\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: x_min must be less than x_max, but got x_min={values['x_min']} and x_max={values['x_max']}\"\n        )\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: y_min must be less than y_max, but got y_min={values['y_min']} and y_max={values['y_max']}\"\n        )\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min ({values['x_min']}) must be less than x_max ({values['x_max']})\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min ({values['y_min']}) must be less than y_max ({values['y_max']})\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if not (values[\"x_min\"] < values[\"x_max\"]):\n        raise ValueError(f\"{cls.__name__}: Invalid bounding box: x_min must be less than x_max\")\n    if not (values[\"y_min\"] < values[\"y_max\"]):\n        raise ValueError(f\"{cls.__name__}: Invalid bounding box: y_min must be less than y_max\")\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  import numpy as np\n\n  if mat1 is None:\n    mat1 = mat0\n\n  norm0 = np.sum(mat0**2, axis=0)\n  norm1 = np.sum(mat1**2, axis=0)\n  dot_prod = np.dot(mat0.T, mat1)\n\n  dist = norm0[:, None] + norm1[None, :] - 2 * dot_prod\n  dist[dist < 0] = 0\n\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  mat0_norm = np.sum(np.square(mat0), axis=0, keepdims=True)\n  mat1_norm = np.sum(np.square(mat1), axis=0, keepdims=True)\n  dist_mat = np.dot(mat0.T, mat1)\n  dist_mat = mat0_norm + mat1_norm.T - 2 * dist_mat\n  dist_mat[dist_mat < 0] = 0\n\n  return dist_mat"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  mat0_sq = (mat0 * mat0).sum(axis=0)\n  mat1_sq = (mat1 * mat1).sum(axis=0)\n\n  mat01_sq_dist = mat0_sq[:, None] + mat1_sq[None, :] - 2 * mat0 @ mat1.T\n  mat01_sq_dist[mat01_sq_dist < 0] = 0\n\n  return mat01_sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  norm0 = np.sum(mat0**2, axis=0)\n  norm1 = np.sum(mat1**2, axis=0)\n  dot = np.dot(mat0.T, mat1)\n  dist = np.maximum(norm0[:, np.newaxis] + norm1[np.newaxis, :] - 2 * dot, 0)\n\n  return dist\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # check input\n  if mat1 is None:\n    mat1 = mat0\n\n  # compute squared distances\n  return np.maximum(0, np.sum(mat0**2, axis=0)[:, None] + np.sum(mat1**2, axis=0)[None, :] - 2*np.dot(mat0.T, mat1))"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # Compute the squared distance between all pairs of columns in the matrices\n  if mat1 is None:\n    mat1 = mat0\n  dist = -2 * np.dot(mat0, mat1.T) + np.sum(mat0**2, axis=0) + np.sum(mat1**2, axis=0)\n\n  # Set negative distances to zero\n  dist[dist < 0] = 0\n\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of each vector.\n  norm0 = np.sum(mat0**2, axis=0)\n  norm1 = np.sum(mat1**2, axis=0)\n\n  # Compute the dot product of each pair of vectors.\n  dot_prod = np.dot(mat0.T, mat1)\n\n  # Compute the squared Euclidean distance.\n  dist = norm0[:, np.newaxis] + norm1[np.newaxis, :] - 2*dot_prod\n\n  # Set negative distances to zero.\n  dist[dist < 0] = 0\n\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of the columns of the matrices\n  norm0 = (mat0**2).sum(axis=0)\n  norm1 = (mat1**2).sum(axis=0)\n\n  # Compute the dot product of the matrices\n  dot_product = np.dot(mat0.T, mat1)\n\n  # Compute the squared Euclidean distance between the columns\n  dist = norm0[:, np.newaxis] + norm1[np.newaxis, :] - 2 * dot_product\n\n  # Set negative distances to zero\n  dist[dist < 0] = 0\n\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # Check input\n  if mat1 is not None:\n    assert mat0.shape[0] == mat1.shape[0]\n\n  # Compute squared distance\n  if mat1 is None:\n    mat1 = mat0\n  dist = (mat0**2).sum(axis=0, keepdims=True) + (mat1**2).sum(axis=0) - 2 * mat0.dot(mat1)\n  dist[dist < 0] = 0\n\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # Check input arguments\n  if not isinstance(mat0, np.ndarray):\n    raise ValueError('mat0 must be a numpy.ndarray.')\n  if mat1 is not None and not isinstance(mat1, np.ndarray):\n    raise ValueError('mat1 must be a numpy.ndarray.')\n\n  # Compute the squared distance\n  if mat1 is None:\n    mat1 = mat0\n  dist = np.sum(mat0**2, axis=0)[:, None] + np.sum(mat1**2, axis=0)[None, :] - 2 * np.dot(mat0.T, mat1)\n\n  # Set negative distances to zero\n  dist[dist < 0] = 0\n\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  dist = np.sum(mat0**2, axis=0, keepdims=True) + np.sum(mat1**2, axis=0, keepdims=True).T - 2*np.dot(mat0, mat1.T)\n  dist[dist < 0] = 0\n  return dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  import numpy as np\n\n  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the dot product between all pairs of columns in mat0 and mat1\n  dot_product = np.dot(mat0.T, mat1)\n\n  # Compute the squared norms of all columns in mat0 and mat1\n  norm_sq0 = np.sum(mat0 ** 2, axis=0)\n  norm_sq1 = np.sum(mat1 ** 2, axis=0)\n\n  # Compute the squared Euclidean distance between all pairs of columns\n  sq_dist = norm_sq0[:, np.newaxis] + norm_sq1[np.newaxis, :] - 2 * dot_product\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # Check if the input matrices are valid\n  if mat0 is None:\n    raise ValueError(\"Input matrix mat0 cannot be None.\")\n  if mat1 is not None and mat1.shape[0] != mat0.shape[0]:\n    raise ValueError(\"Input matrices mat0 and mat1 must have the same number of rows.\")\n\n  # Compute the squared Euclidean distance between all pairs of columns in the input matrices\n  if mat1 is None:\n    mat1 = mat0\n  sq_dist_mat = -2 * np.dot(mat0, mat1.T) + np.sum(mat0**2, axis=0) + np.sum(mat1**2, axis=0)[np.newaxis, :]\n  sq_dist_mat = np.maximum(sq_dist_mat, 0)  # Set negative distances to zero\n\n  return sq_dist_mat"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # If mat1 is not provided, use mat0 as mat1\n  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of the columns of mat0 and mat1\n  norm0 = np.sum(mat0 ** 2, axis=0)\n  norm1 = np.sum(mat1 ** 2, axis=0)\n\n  # Compute the dot product between the columns of mat0 and mat1\n  dot = np.dot(mat0.T, mat1)\n\n  # Compute the squared Euclidean distance matrix\n  dist = norm0[:, None] + norm1[None, :] - 2 * dot\n\n  # Set negative distances to zero\n  dist[dist < 0] = 0\n\n  return dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # check if the input matrices are valid\n  if mat0 is None:\n    raise ValueError(\"Input matrix cannot be None.\")\n\n  # compute the squared Euclidean distance between all pairs of columns in the input matrices\n  if mat1 is not None:\n    dist_mat = (np.sum(mat0**2, axis=0, keepdims=True) + np.sum(mat1**2, axis=0, keepdims=True).T) - 2 * np.dot(mat0, mat1.T)\n  else:\n    dist_mat = (np.sum(mat0**2, axis=0, keepdims=True) + np.sum(mat0**2, axis=0, keepdims=True).T) - 2 * np.dot(mat0, mat0.T)\n\n  # set negative distances to zero\n  dist_mat[dist_mat < 0] = 0\n\n  return dist_mat"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # Check if both matrices are provided\n  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the dot product between the columns of the two matrices\n  dot_prod = np.dot(mat0, mat1.T)\n\n  # Compute the squared norms of the columns of the two matrices\n  norm_sq0 = np.sum(mat0**2, axis=0)\n  norm_sq1 = np.sum(mat1**2, axis=0)\n\n  # Compute the squared Euclidean distance between all pairs of columns\n  sq_dist = norm_sq0[:, np.newaxis] + norm_sq1[np.newaxis, :] - 2 * dot_prod\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # Compute the squared norm of each column in mat0\n  norm0 = np.sum(mat0 ** 2, axis=0)\n\n  # If mat1 is provided, compute the squared norm of each column in mat1\n  if mat1 is not None:\n    norm1 = np.sum(mat1 ** 2, axis=0)\n\n  # If mat1 is not provided, set norm1 to norm0 to compute distances within mat0\n  else:\n    norm1 = norm0\n\n  # Compute the dot product of each column in mat0 with each column in mat1\n  dot_prod = np.dot(mat0.T, mat1)\n\n  # Compute the squared distance matrix by subtracting the dot product from the sum of the squared norms\n  sq_dist = norm0[:, np.newaxis] + norm1[np.newaxis, :] - 2 * dot_prod\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # Check if the input matrices are valid\n  if mat0.ndim != 2:\n    raise ValueError(\"The input matrix mat0 must be a 2D array.\")\n  if mat1 is not None:\n    if mat1.ndim != 2:\n      raise ValueError(\"The input matrix mat1 must be a 2D array.\")\n    if mat0.shape[1] != mat1.shape[1]:\n      raise ValueError(\"The input matrices mat0 and mat1 must have the same number of columns.\")\n\n  # Compute the norms of the columns of the input matrices\n  norm0 = np.linalg.norm(mat0, axis=0)\n  if mat1 is None:\n    norm1 = norm0\n  else:\n    norm1 = np.linalg.norm(mat1, axis=0)\n\n  # Compute the dot products between the columns of the input matrices\n  dot_prod = np.dot(mat0.T, mat1) if mat1 is not None else np.dot(mat0.T, mat0)\n\n  # Compute the squared Euclidean distance between the columns of the input matrices\n  sq_dist = norm0[:, np.newaxis] + norm1[np.newaxis, :] - 2 * dot_prod\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  # If mat1 is not provided, compute the squared distance between all columns of mat0\n  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the dot product between the columns of mat0 and mat1\n  dot_prod = np.dot(mat0.T, mat1)\n\n  # Compute the norms of the columns of mat0 and mat1\n  norm0 = np.linalg.norm(mat0, axis=0)\n  norm1 = np.linalg.norm(mat1, axis=0)\n\n  # Compute the squared distance between the columns of mat0 and mat1\n  sq_dist = np.maximum(0, np.outer(norm0, norm1) - 2 * dot_prod)\n\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  mat0_sq_norm = np.sum(mat0**2, axis=0)\n  mat1_sq_norm = np.sum(mat1**2, axis=0)\n  mat01_dot_prod = np.dot(mat0.T, mat1)\n\n  dist = mat0_sq_norm[:, None] + mat1_sq_norm[None, :] - 2*mat01_dot_prod\n  dist[dist < 0] = 0\n\n  return dist\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http\") or path.startswith(\"https\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"/\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"~/\"):\n        return True\n    if path.startswith(\"~\"):\n        return True\n    if path.startswith(\".\"):\n        return True\n    if path.startswith(\"..\"):\n        return True\n    if path.startswith(\"/\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"~/\"):\n        return True\n    if path.startswith(\"~\"):\n        return True\n    if path.startswith(\".\"):\n        return True\n    if path.startswith(\"..\"):\n        return True\n    if path.startswith(\"/\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"~/\"):\n        return True\n    if path.startswith(\"~\"):\n        return True\n    if path.startswith(\".\"):\n        return True\n    if path.startswith(\"..\"):\n        return True\n    if path.startswith(\"/\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"~/\"):\n        return True\n    if path.startswith(\"~\"):\n        return True\n    if path.startswith(\".\"):\n        return True\n    if path.startswith(\"..\"):\n        return True\n    if path.startswith(\"/\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startsw"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"/\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return True\n    if path.startswith(\"./\"):\n        return True\n    if path.startswith(\"../\"):\n        return"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"/\"):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"<\") or path.startswith(\">\"):\n        return True\n    return False\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"/\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n\n    if not path:\n        return True\n\n    if path.startswith(\"/\"):\n        return True\n\n    if path.startswith(\"./\"):\n        return True\n\n    if path.startswith(\"../\"):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\":\n        return True\n\n    if path.startswith(\"file://\"):\n        return True\n\n    if path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n\n    if path.startswith(\"ftp://\"):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\") or path.startswith(\"ftp://\") or path.startswith(\"s3://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\":\n        return True\n\n    if path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n\n    if path.startswith(\"ftp://\"):\n        return True\n\n    if path.startswith(\"s3://\"):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"/\"):\n        return True\n    if path.startswith(\"~\"):\n        return True\n    if path.startswith(\".\"):\n        return True\n    if path.startswith(\"..\"):\n        return True\n    return False\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\"'assets_names' must be provided when 'items' is a dictionary.\")\n        if len(items) != len(assets_names):\n            raise ValueError(\n                f\"The number of keys in 'items' ({len(items)}) does not match the number of assets ({len(assets_names)})\"\n            )\n        items = np.array([items.get(asset, fill_value) for asset in assets_names])\n    else:\n        items = np.array(items)\n\n    if dim == 1:\n        if items.ndim != 1:\n            raise ValueError(f\"Expected a 1D array for '{name}', but got a {items.ndim}D array.\")\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"Expected '{name}' to have {n_assets} elements, but got {items.shape[0]}.\"\n            )\n    elif dim == 2:\n        if items.ndim != 2:\n            raise ValueError(f\"Expected a 2D array for '{name}', but got a {items.ndim}D array.\")\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"Expected '{name}' to have {n_assets} columns, but got {items.shape[1]}.\"\n            )\n    else:\n        raise ValueError(f\"Invalid dimension '{dim}' for '{name}'.\")\n\n    return items\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} must be a dictionary when assets_names is None.\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"Number of assets in {name} ({len(assets_names)}) must match the number of assets in assets_names ({n_assets}).\"\n            )\n        items_array = np.full(n_assets, fill_value=fill_value)\n        for i, asset_name in enumerate(assets_names):\n            if asset_name in items:\n                items_array[i] = items[asset_name]\n        return items_array\n    elif isinstance(items, np.ndarray):\n        if items.ndim != dim:\n            raise ValueError(\n                f\"Number of dimensions in {name} ({items.ndim}) must match the specified dimension ({dim}).\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"Number of assets in {name} ({items.shape[0]}) must match the number of assets in assets_names ({n_assets}).\"\n            )\n        return items\n    else:\n        raise ValueError(f\"{name} must be a dictionary or numpy array.\")"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"The 'assets_names' argument is required when 'items' is a dictionary.\"\n            )\n\n        # Convert the dictionary to a numpy array\n        array = np.full(n_assets, fill_value=fill_value)\n        for i, asset_name in enumerate(assets_names):\n            if asset_name in items:\n                array[i] = items[asset_name]\n\n    else:\n        array = np.array(items)\n\n    # Verify the shape of the array\n    if dim == 1:\n        if array.shape != (n_assets,):\n            raise ValueError(\n                f\"The shape of '{name}' should be ({n_assets},) but is {array.shape}.\"\n            )\n    elif dim == 2:\n        if array.shape != (n_assets, n_assets):\n            raise ValueError(\n                f\"The shape of '{name}' should be ({n_assets}, {n_assets}) but is {array.shape}.\"\n            )\n    else:\n        raise ValueError(\n            f\"Invalid value for 'dim': {dim}. It should be either 1 or 2.\"\n        )\n\n    return array"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"{name} is a dictionary but no assets_names were provided.\")\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"{name} is a dictionary but the number of assets in assets_names does not match the expected number of assets ({n_assets}).\"\n            )\n        array = np.full(n_assets, fill_value)\n        for i, asset in enumerate(assets_names):\n            if asset in items:\n                array[i] = items[asset]\n        if dim == 1:\n            array = array.reshape((n_assets,))\n        elif dim == 2:\n            array = array.reshape((1, n_assets))\n        else:\n            raise ValueError(f\"Invalid dim value: {dim}\")\n    elif isinstance(items, np.ndarray):\n        if dim == 1 and items.shape != (n_assets,):\n            raise ValueError(\n                f\"{name} is a numpy array but its shape does not match the expected shape ({n_assets},).\"\n            )\n        elif dim == 2 and items.shape != (1, n_assets):\n            raise ValueError(\n                f\"{name} is a numpy array but its shape does not match the expected shape (1, {n_assets}).\"\n            )\n        else:\n            array = items\n    else:\n        raise ValueError(f\"Invalid type for {name}: {type(items)}\")\n\n    return array"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"When 'items' is a dictionary, 'assets_names' must be provided.\"\n            )\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"When 'items' is a dictionary, its length must match the expected number of assets ({n_assets}).\"\n            )\n        if dim == 1:\n            array = np.full(n_assets, fill_value)\n            for i, asset_name in enumerate(assets_names):\n                if asset_name in items:\n                    array[i] = items[asset_name]\n                else:\n                    raise ValueError(\n                        f\"When 'items' is a dictionary, all assets must be present. Asset '{asset_name}' is missing.\"\n                    )\n        elif dim == 2:\n            array = np.full((n_assets, n_assets), fill_value)\n            for i, asset_name in enumerate(assets_names):\n                if asset_name in items:\n                    array[i, :] = items[asset_name]\n                else:\n                    raise ValueError(\n                        f\"When 'items' is a dictionary, all assets must be present. Asset '{asset_name}' is missing.\"\n                    )\n        else:\n            raise ValueError(\n                f\"Invalid dimension '{dim}'. It must be either 1 or 2.\"\n            )\n    elif isinstance(items, (np.ndarray, list, tuple)):\n        if dim == 1:\n            array = np.array(items, dtype=float)\n            if len(array) != n_assets:\n                raise ValueError(\n                    f\"When 'items' is an array-like structure, its length must match the expected number of assets ({n_assets}).\"\n                )\n        elif dim == 2:\n            array = np.array(items, dtype=float)\n            if array.shape != (n_assets, n_assets):\n                raise ValueError(\n                    f\"When 'items' is an array"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Convert items to numpy array\n    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"assets_names must be provided when {name} is a dictionary\"\n            )\n        items_array = np.array([items.get(asset, fill_value) for asset in assets_names])\n    elif isinstance(items, (np.ndarray, list)):\n        items_array = np.array(items)\n    else:\n        raise ValueError(f\"Invalid input type for {name}: {type(items)}\")\n\n    # Verify dimensions and shape\n    if dim == 1:\n        if items_array.ndim != 1:\n            raise ValueError(f\"{name} must be a 1-dimensional array\")\n        if items_array.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} elements, but has {items_array.shape[0]}\"\n            )\n    elif dim == 2:\n        if items_array.ndim != 2:\n            raise ValueError(f\"{name} must be a 2-dimensional array\")\n        if items_array.shape[1] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} elements in each row, but has {items_array.shape[1]}\"\n            )\n    else:\n        raise ValueError(f\"Invalid dimension for {name}: {dim}\")\n\n    return items_array\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"assets_names must be provided when 'items' is a dictionary.\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"The number of assets in 'assets_names' ({len(assets_names)}) does not match the expected number of assets ({n_assets}).\"\n            )\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"The number of assets in 'items' ({len(items)}) does not match the expected number of assets ({n_assets}).\"\n            )\n        arr = np.full(n_assets, fill_value)\n        for i, name in enumerate(assets_names):\n            if name in items:\n                arr[i] = items[name]\n            else:\n                raise ValueError(\n                    f\"Asset '{name}' is missing from the 'items' dictionary.\"\n                )\n    elif isinstance(items, np.ndarray):\n        arr = items\n    else:\n        arr = np.array(items)\n\n    if dim == 1:\n        if arr.ndim != 1:\n            raise ValueError(\n                f\"The dimension of the '{name}' array ({arr.ndim}) is not 1.\"\n            )\n        if arr.shape[0] != n_assets:\n            raise ValueError(\n                f\"The shape of the '{name}' array ({arr.shape}) does not match the expected number of assets ({n_assets}).\"\n            )\n    elif dim == 2:\n        if arr.ndim != 2:\n            raise ValueError(\n                f\"The dimension of the '{name}' array ({arr.ndim}) is not 2.\"\n            )\n        if arr.shape[1] != n_assets:\n            raise ValueError(\n                f\"The shape of the '{name}' array ({arr.shape}) does not match the expected number of assets ({n_assets}).\"\n            )\n    else:"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"assets_names must be provided when items is a dictionary.\"\n            )\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"The number of assets ({len(items)}) does not match the expected number ({n_assets}).\"\n            )\n        if dim == 1:\n            array = np.full(n_assets, fill_value)\n            for i, asset in enumerate(assets_names):\n                if asset in items:\n                    array[i] = items[asset]\n        elif dim == 2:\n            array = np.full((n_assets, n_assets), fill_value)\n            for i, asset in enumerate(assets_names):\n                if asset in items:\n                    array[i, :] = items[asset]\n        else:\n            raise ValueError(f\"Invalid dimension ({dim}) for {name}.\")\n    elif isinstance(items, np.ndarray):\n        if dim == 1:\n            if len(items) != n_assets:\n                raise ValueError(\n                    f\"The length of the array ({len(items)}) does not match the expected number of assets ({n_assets}).\"\n                )\n            array = items\n        elif dim == 2:\n            if items.shape != (n_assets, n_assets):\n                raise ValueError(\n                    f\"The shape of the array ({items.shape}) does not match the expected shape ({n_assets}, {n_assets}).\"\n                )\n            array = items\n        else:\n            raise ValueError(f\"Invalid dimension ({dim}) for {name}.\")\n    else:\n        raise ValueError(f\"Invalid input type ({type(items)}) for {name}.\")\n\n    return array"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Convert items to array\n    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"assets_names must be provided when 'items' is a dictionary\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"Length of assets_names ({len(assets_names)}) must match n_assets ({n_assets})\"\n            )\n        array = np.full((n_assets,), fill_value)\n        for i, key in enumerate(assets_names):\n            if key in items:\n                array[i] = items[key]\n        if dim == 1:\n            array = array.reshape((n_assets,))\n        elif dim == 2:\n            array = array.reshape((1, n_assets))\n    elif isinstance(items, np.ndarray):\n        if items.ndim != dim:\n            raise ValueError(\n                f\"Dimension of 'items' ({items.ndim}) must match 'dim' ({dim})\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"First dimension of 'items' ({items.shape[0]}) must match n_assets ({n_assets})\"\n            )\n        array = items\n    else:\n        raise ValueError(f\"Unsupported type for 'items': {type(items)}\")\n\n    # Verify array shape\n    if dim == 1:\n        if array.shape != (n_assets,):\n            raise ValueError(\n                f\"Shape of '{name}' ({array.shape}) must match ({n_assets},)\"\n            )\n    elif dim == 2:\n        if array.shape != (1, n_assets):\n            raise ValueError(\n                f\"Shape of '{name}' ({array.shape}) must match (1, {n_assets})\"\n            )\n\n    return array"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Convert items to an array if it's a dictionary\n    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"If 'items' is a dictionary, 'assets_names' must be provided.\"\n            )\n\n        # Create an empty array with the expected shape\n        array = np.full((n_assets,), fill_value, dtype=np.float64)\n\n        # Fill in values from the dictionary\n        for i, asset_name in enumerate(assets_names):\n            if asset_name in items:\n                array[i] = items[asset_name]\n\n    # Convert items to an array if it's an array-like\n    elif isinstance(items, (np.ndarray, list, tuple)):\n        array = np.array(items, dtype=np.float64)\n\n    # Raise an error if items is not an array-like or a dictionary\n    else:\n        raise ValueError(f\"Invalid type for '{name}': {type(items)}.\")\n\n    # Verify the shape of the array\n    if dim == 1:\n        if array.shape != (n_assets,):\n            raise ValueError(\n                f\"Invalid shape for '{name}': expected ({n_assets},), got {array.shape}.\"\n            )\n    elif dim == 2:\n        if array.shape != (n_assets, 1):\n            raise ValueError(\n                f\"Invalid shape for '{name}': expected ({n_assets}, 1), got {array.shape}.\"\n            )\n        array = array.squeeze()\n    else:\n        raise ValueError(f\"Invalid dimension for '{name}': {dim}.\")\n\n    return array"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Convert items to numpy array\n    if isinstance(items, dict):\n        # If items is a dictionary, convert to numpy array\n        if assets_names is None:\n            raise ValueError(\n                \"assets_names must be provided when 'items' is a dictionary\"\n            )\n        items_array = np.zeros(n_assets)\n        items_array[:] = fill_value\n        for i, asset_name in enumerate(assets_names):\n            if asset_name in items:\n                items_array[i] = items[asset_name]\n        if dim == 1:\n            items_array = items_array.reshape((n_assets,))\n    elif isinstance(items, npt.ArrayLike):\n        # If items is array-like, convert to numpy array\n        items_array = np.array(items)\n        if dim == 1:\n            items_array = items_array.reshape((n_assets,))\n    else:\n        raise ValueError(f\"Invalid input for '{name}'\")\n\n    # Verify the shape of the array\n    if dim == 1:\n        if items_array.shape != (n_assets,):\n            raise ValueError(\n                f\"Invalid shape for '{name}': expected ({n_assets},), got {items_array.shape}\"\n            )\n    elif dim == 2:\n        if items_array.shape != (n_assets, n_assets):\n            raise ValueError(\n                f\"Invalid shape for '{name}': expected ({n_assets}, {n_assets}), got {items_array.shape}\"\n            )\n\n    return items_array\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"The '{name}' is a dictionary but 'assets_names' is not provided.\"\n            )\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"The number of items in the '{name}' dictionary ({len(items)}) does not match the expected number of assets ({n_assets}).\"\n            )\n        items_array = np.zeros(n_assets)\n        for i, asset_name in enumerate(assets_names):\n            if asset_name in items:\n                items_array[i] = items[asset_name]\n            else:\n                items_array[i] = fill_value\n    elif isinstance(items, np.ndarray):\n        if items.shape == (n_assets,):\n            items_array = items\n        else:\n            raise ValueError(\n                f\"The shape of the '{name}' array ({items.shape}) does not match the expected shape ({n_assets},).\"\n            )\n    else:\n        raise ValueError(f\"Invalid input type for '{name}'.\")\n\n    if dim == 1:\n        if items_array.ndim != 1:\n            raise ValueError(\n                f\"The '{name}' array has an invalid number of dimensions ({items_array.ndim}).\"\n            )\n    elif dim == 2:\n        if items_array.ndim != 2:\n            raise ValueError(\n                f\"The '{name}' array has an invalid number of dimensions ({items_array.ndim}).\"\n            )\n        if items_array.shape[1] != n_assets:\n            raise ValueError(\n                f\"The '{name}' array has an invalid number of assets ({items_array.shape[1]}).\"\n            )\n\n    return items_array"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Convert items to a numpy array\n    if isinstance(items, dict):\n        # Convert items to a numpy array\n        if assets_names is None:\n            raise ValueError(\"assets_names must be provided when items is a dictionary\")\n        arr = np.full(n_assets, fill_value)\n        for i, asset_name in enumerate(assets_names):\n            if asset_name in items:\n                arr[i] = items[asset_name]\n        if dim == 2:\n            arr = np.expand_dims(arr, axis=0)\n    else:\n        arr = np.array(items)\n\n    # Verify the shape of the array\n    if dim == 2:\n        if arr.ndim != 2:\n            raise ValueError(f\"{name} must be a 2D array\")\n        if arr.shape[1] != n_assets:\n            raise ValueError(f\"{name} must have shape (n_groups, n_assets)\")\n    elif dim == 1:\n        if arr.ndim != 1:\n            raise ValueError(f\"{name} must be a 1D array\")\n        if arr.shape[0] != n_assets:\n            raise ValueError(f\"{name} must have shape (n_assets,)\")\n\n    return arr\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Convert items to numpy array\n    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"assets_names must be provided when items is a dictionary.\")\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"The number of assets ({len(assets_names)}) does not match the expected number of assets ({n_assets}).\"\n            )\n        array = np.full((n_assets,), fill_value)\n        for i, asset in enumerate(assets_names):\n            if asset in items:\n                array[i] = items[asset]\n            else:\n                raise ValueError(\n                    f\"Missing value for asset '{asset}' in {name} dictionary. Please provide a value for all assets.\"\n                )\n    elif isinstance(items, np.ndarray):\n        array = items\n    else:\n        array = np.array(items)\n\n    # Verify array shape\n    if dim == 1:\n        if array.ndim != 1:\n            raise ValueError(\n                f\"{name} must be a 1-dimensional array or dictionary with {n_assets} keys.\"\n            )\n        if array.shape[0] != n_assets:\n            raise ValueError(\n                f\"The shape of {name} ({array.shape}) does not match the expected shape ({n_assets},).\"\n            )\n    elif dim == 2:\n        if array.ndim != 2:\n            raise ValueError(\n                f\"{name} must be a 2-dimensional array or dictionary with {n_assets} keys.\"\n            )\n        if array.shape[1] != n_assets:\n            raise ValueError(\n                f\"The shape of {name} ({array.shape}) does not match the expected shape (?, {n_assets}).\"\n            )\n    else:\n        raise ValueError(f\"Invalid dim value: {dim}. Expected 1 or 2.\")\n\n    return array\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"{name} must be a dictionary if assets_names is None\")\n        if not all(isinstance(key, str) for key in items.keys()):\n            raise ValueError(f\"{name} keys must be strings\")\n        if not all(isinstance(value, (int, float)) for value in items.values()):\n            raise ValueError(f\"{name} values must be numeric\")\n        if not all(isinstance(asset_name, str) for asset_name in assets_names):\n            raise ValueError(\"assets_names must be a list of strings\")\n\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"Number of assets in {name} ({len(assets_names)}) does not match the expected number of assets ({n_assets})\"\n            )\n\n        array = np.zeros(n_assets)\n        array[:] = fill_value\n        for key, value in items.items():\n            if key not in assets_names:\n                raise ValueError(f\"{name} key '{key}' not found in assets_names\")\n            array[assets_names == key] = value\n\n    elif isinstance(items, np.ndarray):\n        if len(items.shape) != dim:\n            raise ValueError(\n                f\"{name} must be a {dim}D array with shape (n_assets,) or (n_groups, n_assets)\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"Number of assets in {name} ({items.shape[0]}) does not match the expected number of assets ({n_assets})\"\n            )\n        array = items\n\n    else:\n        raise ValueError(f\"{name} must be a dictionary or a numpy array\")\n\n    return array\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"assets_names must be provided when items is a dictionary\"\n            )\n\n        if len(items) != len(assets_names):\n            raise ValueError(\n                f\"Number of items ({len(items)}) does not match number of assets ({len(assets_names)})\"\n            )\n\n        items = np.array(\n            [items.get(asset, fill_value) for asset in assets_names],\n            dtype=float,\n        )\n\n    items = np.array(items, dtype=float)\n\n    if items.ndim != dim:\n        raise ValueError(\n            f\"{name} must have {dim} dimensions, but has {items.ndim}\"\n        )\n\n    if items.shape[0] != n_assets:\n        raise ValueError(\n            f\"{name} must have {n_assets} assets, but has {items.shape[0]}\"\n        )\n\n    return items\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Verify that the input items are a dictionary or an array-like structure\n    if isinstance(items, dict):\n        # If the input items are a dictionary, convert it to a numpy array\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} is a dictionary but assets_names is None. Please provide a list of asset names to match keys.\"\n            )\n\n        # Convert the dictionary to a numpy array\n        array = np.array(\n            [items.get(asset_name, fill_value) for asset_name in assets_names],\n            dtype=np.float64,\n        )\n\n    elif isinstance(items, (list, tuple, np.ndarray)):\n        # If the input items are a list, tuple, or numpy array, convert it to a numpy array\n        array = np.array(items, dtype=np.float64)\n\n    else:\n        raise ValueError(\n            f\"{name} must be a dictionary or an array-like structure, but got {type(items)}.\"\n        )\n\n    # Verify that the final array has the expected shape\n    if dim == 1:\n        if array.ndim != 1 or array.shape != (n_assets,):\n            raise ValueError(\n                f\"{name} must be a 1D array with shape (n_assets,), but got {array.shape}.\"\n            )\n    elif dim == 2:\n        if array.ndim != 2 or array.shape != (n_assets, n_assets):\n            raise ValueError(\n                f\"{name} must be a 2D array with shape (n_assets, n_assets), but got {array.shape}.\"\n            )\n    else:\n        raise ValueError(\n            f\"Invalid value for dim: {dim}. Expected 1 or 2, but got {dim}.\"\n        )\n\n    # Return the final array\n    return array"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # If 'items' is a dictionary, convert it to a numpy array\n    if isinstance(items, dict):\n\n        # If assets_names is not provided, raise an error\n        if assets_names is None:\n            raise ValueError(\n                \"assets_names must be provided when 'items' is a dictionary\"\n            )\n\n        # Create a numpy array filled with fill_value\n        array = np.full(n_assets, fill_value=fill_value)\n\n        # Loop over the keys and values in 'items'\n        for key, value in items.items():\n\n            # Find the index of the asset with the matching name\n            index = np.where(assets_names == key)[0]\n\n            # If the asset is not found, raise an error\n            if len(index) == 0:\n                raise ValueError(f\"Asset '{key}' not found in assets_names\")\n\n            # Set the value at the matching index\n            array[index] = value\n\n        # Reshape the array to (n_assets,)\n        array = array.reshape(-1)\n\n    # If 'items' is not a dictionary, convert it to a numpy array\n    else:\n\n        # Convert 'items' to a numpy array\n        array = np.array(items)\n\n        # Reshape the array to (n_assets,)\n        array = array.reshape(-1)\n\n    # Verify the shape of the array\n    if array.shape != (n_assets,) and array.shape != (n_assets, 1):\n        raise ValueError(\n            f\"{name} must be a dictionary or an array with shape (n_assets,) or (n_assets, 1)\"\n        )\n\n    # If dim is 2, reshape the array to (1, n_assets)\n    if dim == 2:\n        array = array.reshape(1, -1)\n\n    # Return the array\n    return array"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Convert items to a numpy array if it is not already\n    if isinstance(items, dict):\n        # If items is a dictionary, create an array with the expected shape and fill missing values\n        if assets_names is None:\n            raise ValueError(\n                f\"assets_names is required when {name} is a dictionary.\"\n            )\n\n        # Create an array with the expected shape and fill missing values\n        array = np.full((len(assets_names), n_assets), fill_value)\n        for i, asset in enumerate(assets_names):\n            if asset in items:\n                array[i, :] = items[asset]\n\n    elif isinstance(items, np.ndarray):\n        # If items is a numpy array, ensure it has the expected shape\n        if items.shape == (n_assets,):\n            array = items\n        elif items.shape == (n_assets, 1):\n            array = items.reshape(n_assets)\n        elif items.shape == (1, n_assets):\n            array = items.reshape(n_assets)\n        elif items.shape == (n_assets, 1, 1):\n            array = items.reshape(n_assets)\n        elif items.shape == (1, 1, n_assets):\n            array = items.reshape(n_assets)\n        else:\n            raise ValueError(\n                f\"The shape of {name} is not compatible with the expected shape ({n_assets},).\"\n            )\n    else:\n        # If items is not a dictionary or a numpy array, raise an error\n        raise ValueError(\n            f\"{name} must be a dictionary or a numpy array with the expected shape ({n_assets},).\"\n        )\n\n    # Verify the final array's dimensions and shape against expected values\n    if dim == 1:\n        if array.shape != (n_assets,):\n            raise ValueError(\n                f\"The shape of {name} is not compatible with the expected shape ({n_assets},).\"\n            )\n    elif dim == 2:\n        if array.shape"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Convert items to an array\n    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} must be a dictionary when assets_names is not provided\"\n            )\n        # Convert dictionary to array\n        arr = np.full(n_assets, fill_value)\n        for i, asset in enumerate(assets_names):\n            if asset in items:\n                arr[i] = items[asset]\n        # Verify array shape\n        if arr.shape != (n_assets,):\n            raise ValueError(\n                f\"{name} dictionary must have exactly {n_assets} elements\"\n            )\n    elif isinstance(items, npt.ArrayLike):\n        # Convert array-like to array\n        arr = np.array(items)\n        # Verify array shape\n        if dim == 1:\n            if arr.shape != (n_assets,):\n                raise ValueError(\n                    f\"{name} must be a 1D array of length {n_assets}\"\n                )\n        elif dim == 2:\n            if arr.shape != (n_assets, n_assets):\n                raise ValueError(\n                    f\"{name} must be a 2D array of shape ({n_assets}, {n_assets})\"\n                )\n    else:\n        raise ValueError(f\"{name} must be a dictionary or array-like\")\n\n    return arr\n\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        if 'agent_lifecycle' not in data:\n            data['agent_lifecycle'] = agent_lifecycle\n        if 'openai_wrapper' not in data:\n            data['openai_wrapper'] = openai_wrapper\n        return MicroAgent(**data)"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent()\n        agent.agent_lifecycle = agent_lifecycle\n        agent.openai_wrapper = openai_wrapper\n        agent.id = data.get(\"id\", None)\n        agent.name = data.get(\"name\", None)\n        agent.description = data.get(\"description\", None)\n        agent.owner = data.get(\"owner\", None)\n        agent.tags = data.get(\"tags\", None)\n        agent.agent_type = data.get(\"agent_type\", None)\n        agent.agent_version = data.get(\"agent_version\", None)\n        agent.agent_version_id = data.get(\"agent_version_id\", None)\n        agent.agent_version_name = data.get(\"agent_version_name\", None)\n        agent.agent_version_description = data.get(\"agent_version_description\", None)\n        agent.agent_version_owner = data.get(\"agent_version_owner\", None)\n        agent.agent_version_tags = data.get(\"agent_version_tags\", None)\n        agent.agent_version_agent_type = data.get(\"agent_version_agent_type\", None)\n        agent.agent_version_agent_version = data.get(\"agent_version_agent_version\", None)\n        agent.agent_version_agent_version_id = data.get(\"agent_version_agent_version_id\", None)\n        agent.agent_version_agent_version_name = data.get(\"agent_version_agent_version_name\", None)\n        agent.agent_version_agent_version_description = data.get(\"agent_version_agent_version_description\", None)\n        agent.agent_version_agent_version_owner = data.get(\"agent_version_agent_version_owner\", None)\n        agent.agent_version_agent_version_tags = data.get(\"agent_version_agent_version_tags\", None)\n        agent.agent_version_agent_version_agent_type = data.get(\"agent_version_agent_version_agent_type\", None)\n        agent.agent_version_agent_version"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        micro_agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        micro_agent.id = data.get('id', None)\n        micro_agent.name = data.get('name', None)\n        micro_agent.description = data.get('description', None)\n        micro_agent.lifecycle = data.get('lifecycle', None)\n        micro_agent.openai_wrapper = data.get('openai_wrapper', None)\n        micro_agent.agent_lifecycle = data.get('agent_lifecycle', None)\n        micro_agent.agent_type = data.get('agent_type', None)\n        micro_agent.agent_type_id = data.get('agent_type_id', None)\n        micro_agent.agent_type_name = data.get('agent_type_name', None)\n        micro_agent.agent_type_description = data.get('agent_type_description', None)\n        micro_agent.agent_type_lifecycle = data.get('agent_type_lifecycle', None)\n        micro_agent.agent_type_openai_wrapper = data.get('agent_type_openai_wrapper', None)\n        micro_agent.agent_type_agent_lifecycle = data.get('agent_type_agent_lifecycle', None)\n        micro_agent.agent_type_agent_type = data.get('agent_type_agent_type', None)\n        micro_agent.agent_type_agent_type_id = data.get('agent_type_agent_type_id', None)\n        micro_agent.agent_type_agent_type_name = data.get('agent_type_agent_type_name', None)\n        micro_agent.agent_type_agent_type_description = data.get('agent_type_agent_type_description', None)\n        micro_agent.agent_type_agent_type_lifecycle = data.get('agent_type_agent_type_lifecycle', None)\n        micro_agent.agent_type_agent_type"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        micro_agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        micro_agent.agent_id = data.get(\"agent_id\", None)\n        micro_agent.agent_type = data.get(\"agent_type\", None)\n        micro_agent.agent_name = data.get(\"agent_name\", None)\n        micro_agent.agent_description = data.get(\"agent_description\", None)\n        micro_agent.agent_version = data.get(\"agent_version\", None)\n        micro_agent.agent_language = data.get(\"agent_language\", None)\n        micro_agent.agent_language_version = data.get(\"agent_language_version\", None)\n        micro_agent.agent_location = data.get(\"agent_location\", None)\n        micro_agent.agent_location_type = data.get(\"agent_location_type\", None)\n        micro_agent.agent_location_url = data.get(\"agent_location_url\", None)\n        micro_agent.agent_location_path = data.get(\"agent_location_path\", None)\n        micro_agent.agent_location_file = data.get(\"agent_location_file\", None)\n        micro_agent.agent_location_line = data.get(\"agent_location_line\", None)\n        micro_agent.agent_location_column = data.get(\"agent_location_column\", None)\n        micro_agent.agent_location_function = data.get(\"agent_location_function\", None)\n        micro_agent.agent_location_class = data.get(\"agent_location_class\", None)\n        micro_agent.agent_location_package = data.get(\"agent_location_package\", None)\n        micro_agent.agent_location_module = data.get(\"agent_location_module\", None)\n        micro_agent.agent_location_namespace = data.get(\"agent_location_namespace\", None)\n        micro_agent.agent_location_package_version = data.get(\"agent_location_package_version\", None)\n        micro_agent.agent_location_package_name = data.get(\"agent_location_package"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        if \"agent_id\" in data:\n            agent_id = data[\"agent_id\"]\n        else:\n            agent_id = None\n        if \"agent_type\" in data:\n            agent_type = data[\"agent_type\"]\n        else:\n            agent_type = None\n        if \"agent_name\" in data:\n            agent_name = data[\"agent_name\"]\n        else:\n            agent_name = None\n        if \"agent_description\" in data:\n            agent_description = data[\"agent_description\"]\n        else:\n            agent_description = None\n        if \"agent_version\" in data:\n            agent_version = data[\"agent_version\"]\n        else:\n            agent_version = None\n        if \"agent_language\" in data:\n            agent_language = data[\"agent_language\"]\n        else:\n            agent_language = None\n        if \"agent_license\" in data:\n            agent_license = data[\"agent_license\"]\n        else:\n            agent_license = None\n        if \"agent_author\" in data:\n            agent_author = data[\"agent_author\"]\n        else:\n            agent_author = None\n        if \"agent_email\" in data:\n            agent_email = data[\"agent_email\"]\n        else:\n            agent_email = None\n        if \"agent_url\" in data:\n            agent_url = data[\"agent_url\"]\n        else:\n            agent_url = None\n        if \"agent_tags\" in data:\n            agent_tags = data[\"agent_tags\"]\n        else:\n            agent_tags = None\n        if \"agent_lifecycle\" in data:\n            agent_lifecycle = data[\"agent_lifecycle\"]\n        else:\n            agent_lifecycle = None\n        if \"agent_state\" in data:\n            agent_state = data[\"agent_state\"]\n        else:\n            agent_state = None\n        if \"agent_state_reason\" in data:\n            agent_state_reason = data[\"agent_state_reason\"]\n        else:\n            agent_state_reason = None\n        if \""}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        try:\n            # Deserialize the MicroAgent object from the dictionary\n            agent = MicroAgent.from_dict(data, agent_lifecycle, openai_wrapper)\n        except Exception as e:\n            # If the deserialization fails, log the error and return None\n            logger.error(f\"Failed to deserialize MicroAgent: {e}\")\n            return None\n        # Return the deserialized MicroAgent object\n        return agent"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        micro_agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        micro_agent.name = data.get('name', None)\n        micro_agent.description = data.get('description', None)\n        micro_agent.version = data.get('version', None)\n        micro_agent.language = data.get('language', None)\n        micro_agent.language_version = data.get('language_version', None)\n        micro_agent.license = data.get('license', None)\n        micro_agent.author = data.get('author', None)\n        micro_agent.author_email = data.get('author_email', None)\n        micro_agent.url = data.get('url', None)\n        micro_agent.tags = data.get('tags', None)\n        micro_agent.lifecycle = data.get('lifecycle', None)\n        micro_agent.agent_type = data.get('agent_type', None)\n        micro_agent.agent_version = data.get('agent_version', None)\n        micro_agent.agent_version_date = data.get('agent_version_date', None)\n        micro_agent.agent_version_author = data.get('agent_version_author', None)\n        micro_agent.agent_version_author_email = data.get('agent_version_author_email', None)\n        micro_agent.agent_version_url = data.get('agent_version_url', None)\n        micro_agent.agent_version_description = data.get('agent_version_description', None)\n        micro_agent.agent_version_tags = data.get('agent_version_tags', None)\n        micro_agent.agent_version_lifecycle = data.get('agent_version_lifecycle', None)\n        micro_agent.agent_version_agent_type = data.get('agent_version_agent_type', None)\n        micro_agent.agent_version_agent_version = data.get('agent_version_agent_version', None)\n        micro_agent.agent_version_agent_version_date"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Create a new MicroAgent object\n        micro_agent = MicroAgent()\n        # Set the MicroAgent's lifecycle state\n        micro_agent.lifecycle = agent_lifecycle\n        # Set the MicroAgent's OpenAI wrapper\n        micro_agent.openai_wrapper = openai_wrapper\n        # Set the MicroAgent's name\n        micro_agent.name = data.get('name', micro_agent.name)\n        # Set the MicroAgent's description\n        micro_agent.description = data.get('description', micro_agent.description)\n        # Set the MicroAgent's tags\n        micro_agent.tags = data.get('tags', micro_agent.tags)\n        # Set the MicroAgent's version\n        micro_agent.version = data.get('version', micro_agent.version)\n        # Set the MicroAgent's model_id\n        micro_agent.model_id = data.get('model_id', micro_agent.model_id)\n        # Set the MicroAgent's model_version\n        micro_agent.model_version = data.get('model_version', micro_agent.model_version)\n        # Set the MicroAgent's model_name\n        micro_agent.model_name = data.get('model_name', micro_agent.model_name)\n        # Set the MicroAgent's model_description\n        micro_agent.model_description = data.get('model_description', micro_agent.model_description)\n        # Set the MicroAgent's model_tags\n        micro_agent.model_tags = data.get('model_tags', micro_agent.model_tags)\n        # Set the MicroAgent's model_version\n        micro_agent.model_version = data.get('model_version', micro_agent.model_version)\n        # Set the MicroAgent's model_version\n        micro_agent.model_version = data.get('model_version', micro_agent.model_version)\n        # Set the MicroAgent's model_version\n        micro_agent.model_version = data.get('model_version', micro_agent.model_version)"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        agent.name = data.get(\"name\", \"\")\n        agent.description = data.get(\"description\", \"\")\n        agent.language = data.get(\"language\", \"\")\n        agent.agent_type = data.get(\"agent_type\", \"\")\n        agent.agent_version = data.get(\"agent_version\", \"\")\n        agent.agent_config = data.get(\"agent_config\", {})\n        agent.agent_metadata = data.get(\"agent_metadata\", {})\n        agent.agent_state = data.get(\"agent_state\", {})\n        agent.agent_state_history = data.get(\"agent_state_history\", [])\n        agent.agent_state_history_size = data.get(\"agent_state_history_size\", 0)\n        agent.agent_state_history_index = data.get(\"agent_state_history_index\", 0)\n        agent.agent_state_history_count = data.get(\"agent_state_history_count\", 0)\n        agent.agent_state_history_max_size = data.get(\"agent_state_history_max_size\", 0)\n        agent.agent_state_history_max_size_reached = data.get(\"agent_state_history_max_size_reached\", False)\n        agent.agent_state_history_max_size_reached_at = data.get(\"agent_state_history_max_size_reached_at\", None)\n        agent.agent_state_history_max_size_reached_at_index = data.get(\"agent_state_history_max_size_reached_at_index\", 0)\n        agent.agent_state_history_max_size_reached_at_count = data.get(\"agent_state_history_max_size_reached_at_count\", 0)\n        agent.agent_state_history_max_size_reached_at_count_max = data.get(\"agent_state_history_max_size_reached_at_count_max\", 0)"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        agent.id = data.get(\"id\", None)\n        agent.name = data.get(\"name\", None)\n        agent.description = data.get(\"description\", None)\n        agent.agent_type = data.get(\"agent_type\", None)\n        agent.agent_version = data.get(\"agent_version\", None)\n        agent.agent_state = data.get(\"agent_state\", None)\n        agent.agent_status = data.get(\"agent_status\", None)\n        agent.agent_status_reason = data.get(\"agent_status_reason\", None)\n        agent.agent_status_timestamp = data.get(\"agent_status_timestamp\", None)\n        agent.agent_config = data.get(\"agent_config\", None)\n        agent.agent_config_signature = data.get(\"agent_config_signature\", None)\n        agent.agent_config_timestamp = data.get(\"agent_config_timestamp\", None)\n        agent.agent_config_signature_timestamp = data.get(\"agent_config_signature_timestamp\", None)\n        agent.agent_config_signature_valid = data.get(\"agent_config_signature_valid\", None)\n        agent.agent_config_signature_valid_timestamp = data.get(\"agent_config_signature_valid_timestamp\", None)\n        agent.agent_config_signature_valid_reason = data.get(\"agent_config_signature_valid_reason\", None)\n        agent.agent_config_signature_valid_reason_timestamp = data.get(\"agent_config_signature_valid_reason_timestamp\", None)\n        agent.agent_config_signature_valid_reason_code = data.get(\"agent_config_signature_valid_reason_code\", None)\n        agent.agent_config_signature_valid_reason_code_timestamp = data.get(\"agent_config_signature_valid_reason_code_timestamp\", None)\n        agent.agent_config_signature_valid_reason_code_message = data.get(\"agent_config_signature_valid_reason_code_message\", None)\n        agent.agent"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        micro_agent = MicroAgent(agent_lifecycle=agent_lifecycle, openai_wrapper=openai_wrapper)\n        micro_agent.id = data.get('id')\n        micro_agent.name = data.get('name')\n        micro_agent.description = data.get('description')\n        micro_agent.agent_type = data.get('agent_type')\n        micro_agent.agent_version = data.get('agent_version')\n        micro_agent.agent_lifecycle = data.get('agent_lifecycle')\n        micro_agent.agent_lifecycle_transition_time = data.get('agent_lifecycle_transition_time')\n        micro_agent.agent_lifecycle_transition_reason = data.get('agent_lifecycle_transition_reason')\n        micro_agent.agent_lifecycle_transition_details = data.get('agent_lifecycle_transition_details')\n        micro_agent.agent_lifecycle_transition_message = data.get('agent_lifecycle_transition_message')\n        micro_agent.agent_lifecycle_transition_message_details = data.get('agent_lifecycle_transition_message_details')\n        micro_agent.agent_lifecycle_transition_message_details_code = data.get('agent_lifecycle_transition_message_details_code')\n        micro_agent.agent_lifecycle_transition_message_details_target = data.get('agent_lifecycle_transition_message_details_target')\n        micro_agent.agent_lifecycle_transition_message_details_target_type = data.get('agent_lifecycle_transition_message_details_target_type')\n        micro_agent.agent_lifecycle_transition_message_details_target_id = data.get('agent_lifecycle_transition_message_details_target_id')\n        micro_agent.agent_lifecycle_transition_message_details_target_name = data.get('agent_lifecycle"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        agent.name = data.get(\"name\", \"\")\n        agent.description = data.get(\"description\", \"\")\n        agent.lifecycle = data.get(\"lifecycle\", \"\")\n        agent.version = data.get(\"version\", \"\")\n        agent.last_modified = data.get(\"last_modified\", \"\")\n        agent.created = data.get(\"created\", \"\")\n        agent.agent_type = data.get(\"agent_type\", \"\")\n        agent.agent_id = data.get(\"agent_id\", \"\")\n        agent.agent_lifecycle = data.get(\"agent_lifecycle\", \"\")\n        agent.agent_version = data.get(\"agent_version\", \"\")\n        agent.agent_last_modified = data.get(\"agent_last_modified\", \"\")\n        agent.agent_created = data.get(\"agent_created\", \"\")\n        agent.agent_description = data.get(\"agent_description\", \"\")\n        agent.agent_name = data.get(\"agent_name\", \"\")\n        agent.agent_type = data.get(\"agent_type\", \"\")\n        agent.agent_id = data.get(\"agent_id\", \"\")\n        agent.agent_lifecycle = data.get(\"agent_lifecycle\", \"\")\n        agent.agent_version = data.get(\"agent_version\", \"\")\n        agent.agent_last_modified = data.get(\"agent_last_modified\", \"\")\n        agent.agent_created = data.get(\"agent_created\", \"\")\n        agent.agent_description = data.get(\"agent_description\", \"\")\n        agent.agent_name = data.get(\"agent_name\", \"\")\n        agent.agent_type = data.get(\"agent_type\", \"\")\n        agent.agent_id = data.get(\"agent_id\", \"\")\n        agent.agent_lifecycle = data.get(\"agent_lifecycle\", \"\")\n        agent.agent_version = data.get(\"agent"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        if data is None:\n            return None\n        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        agent.id = data.get(\"id\")\n        agent.name = data.get(\"name\")\n        agent.description = data.get(\"description\")\n        agent.language = data.get(\"language\")\n        agent.tags = data.get(\"tags\")\n        agent.code = data.get(\"code\")\n        agent.code_type = data.get(\"code_type\")\n        agent.code_language = data.get(\"code_language\")\n        agent.code_language_version = data.get(\"code_language_version\")\n        agent.code_language_runtime = data.get(\"code_language_runtime\")\n        agent.code_language_runtime_version = data.get(\"code_language_runtime_version\")\n        agent.code_language_runtime_environment = data.get(\"code_language_runtime_environment\")\n        agent.code_language_runtime_environment_version = data.get(\"code_language_runtime_environment_version\")\n        agent.code_language_runtime_environment_architecture = data.get(\"code_language_runtime_environment_architecture\")\n        agent.code_language_runtime_environment_operating_system = data.get(\"code_language_runtime_environment_operating_system\")\n        agent.code_language_runtime_environment_operating_system_version = data.get(\"code_language_runtime_environment_operating_system_version\")\n        agent.code_language_runtime_environment_operating_system_architecture = data.get(\"code_language_runtime_environment_operating_system_architecture\")\n        agent.code_language_runtime_environment_operating_system_kernel_version = data.get(\"code_language_runtime_environment_operating_system_kernel_version\")\n        agent.code_language_runtime_environment_operating_system_kernel_architecture = data.get(\"code_language_runtime_environment_operating_system_kernel_architecture\")\n        agent.code_language_runtime_environment_operating_system_kernel_version_major"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Create a new MicroAgent object\n        micro_agent = MicroAgent(agent_lifecycle, openai_wrapper)\n    \n        # Set the MicroAgent's attributes based on the input dictionary\n        micro_agent.agent_id = data.get(\"agent_id\", None)\n        micro_agent.agent_name = data.get(\"agent_name\", None)\n        micro_agent.agent_type = data.get(\"agent_type\", None)\n        micro_agent.agent_version = data.get(\"agent_version\", None)\n        micro_agent.agent_description = data.get(\"agent_description\", None)\n        micro_agent.agent_parameters = data.get(\"agent_parameters\", None)\n        micro_agent.agent_metadata = data.get(\"agent_metadata\", None)\n        micro_agent.agent_lifecycle = data.get(\"agent_lifecycle\", None)\n        micro_agent.agent_lifecycle_status = data.get(\"agent_lifecycle_status\", None)\n        micro_agent.agent_lifecycle_transition_time = data.get(\"agent_lifecycle_transition_time\", None)\n        micro_agent.agent_lifecycle_transition_reason = data.get(\"agent_lifecycle_transition_reason\", None)\n        micro_agent.agent_lifecycle_transition_details = data.get(\"agent_lifecycle_transition_details\", None)\n        micro_agent.agent_lifecycle_transition_metadata = data.get(\"agent_lifecycle_transition_metadata\", None)\n        micro_agent.agent_lifecycle_transition_agent = data.get(\"agent_lifecycle_transition_agent\", None)\n        micro_agent.agent_lifecycle_transition_agent_id = data.get(\"agent_lifecycle_transition_agent_id\", None)\n        micro_agent.agent_lifecycle_transition_agent_name = data.get(\"agent_lifecycle_transition_agent_name\", None)\n        micro_agent.agent_lifec"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        agent.name = data.get(\"name\", \"\")\n        agent.description = data.get(\"description\", \"\")\n        agent.agent_type = data.get(\"agent_type\", \"\")\n        agent.agent_subtype = data.get(\"agent_subtype\", \"\")\n        agent.agent_version = data.get(\"agent_version\", \"\")\n        agent.agent_id = data.get(\"agent_id\", \"\")\n        agent.agent_key = data.get(\"agent_key\", \"\")\n        agent.agent_lifecycle = data.get(\"agent_lifecycle\", \"\")\n        agent.agent_lifecycle_state = data.get(\"agent_lifecycle_state\", \"\")\n        agent.agent_lifecycle_transition_time = data.get(\"agent_lifecycle_transition_time\", \"\")\n        agent.agent_lifecycle_transition_reason = data.get(\"agent_lifecycle_transition_reason\", \"\")\n        agent.agent_lifecycle_transition_message = data.get(\"agent_lifecycle_transition_message\", \"\")\n        agent.agent_lifecycle_transition_details = data.get(\"agent_lifecycle_transition_details\", \"\")\n        agent.agent_lifecycle_transition_details_type = data.get(\"agent_lifecycle_transition_details_type\", \"\")\n        agent.agent_lifecycle_transition_details_url = data.get(\"agent_lifecycle_transition_details_url\", \"\")\n        agent.agent_lifecycle_transition_details_code = data.get(\"agent_lifecycle_transition_details_code\", \"\")\n        agent.agent_lifecycle_transition_details_message = data.get(\"agent_lifecycle_transition_details_message\", \"\")\n        agent.agent_lifecycle_transition_details_description = data.get(\"agent_lifecycle_transition_details_description\", \"\")"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        agent.agent_id = data.get(\"agent_id\", None)\n        agent.agent_name = data.get(\"agent_name\", None)\n        agent.agent_type = data.get(\"agent_type\", None)\n        agent.agent_version = data.get(\"agent_version\", None)\n        agent.agent_status = data.get(\"agent_status\", None)\n        agent.agent_description = data.get(\"agent_description\", None)\n        agent.agent_input_type = data.get(\"agent_input_type\", None)\n        agent.agent_output_type = data.get(\"agent_output_type\", None)\n        agent.agent_input_params = data.get(\"agent_input_params\", None)\n        agent.agent_output_params = data.get(\"agent_output_params\", None)\n        agent.agent_input_example = data.get(\"agent_input_example\", None)\n        agent.agent_output_example = data.get(\"agent_output_example\", None)\n        agent.agent_input_validation_pattern = data.get(\"agent_input_validation_pattern\", None)\n        agent.agent_output_validation_pattern = data.get(\"agent_output_validation_pattern\", None)\n        agent.agent_input_validation_failure_message = data.get(\"agent_input_validation_failure_message\", None)\n        agent.agent_output_validation_failure_message = data.get(\"agent_output_validation_failure_message\", None)\n        agent.agent_input_validation_regex = data.get(\"agent_input_validation_regex\", None)\n        agent.agent_output_validation_regex = data.get(\"agent_output_validation_regex\", None)\n        agent.agent_input_validation_failure_regex = data.get(\"agent_input_validation_failure_regex\", None)\n        agent.agent_output_validation_failure_regex = data.get(\"agent_output_validation_failure_regex\", None)\n        agent.agent_input_validation"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent_id = data.get(\"agent_id\", None)\n        agent_type = data.get(\"agent_type\", None)\n        agent_name = data.get(\"agent_name\", None)\n        agent_description = data.get(\"agent_description\", None)\n        agent_version = data.get(\"agent_version\", None)\n        agent_tags = data.get(\"agent_tags\", None)\n        agent_metadata = data.get(\"agent_metadata\", None)\n        agent_lifecycle = data.get(\"agent_lifecycle\", agent_lifecycle)\n        agent_openai_wrapper = data.get(\"agent_openai_wrapper\", openai_wrapper)\n        agent_owner = data.get(\"agent_owner\", None)\n        agent_openai_wrapper.set_agent_id(agent_id)\n        agent = MicroAgent(agent_id=agent_id, agent_type=agent_type, agent_name=agent_name, agent_description=agent_description,\n                           agent_version=agent_version, agent_tags=agent_tags, agent_metadata=agent_metadata,\n                           agent_lifecycle=agent_lifecycle, agent_openai_wrapper=agent_openai_wrapper, agent_owner=agent_owner)\n        return agent"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        micro_agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        if \"id\" in data:\n            micro_agent.id = data[\"id\"]\n        if \"name\" in data:\n            micro_agent.name = data[\"name\"]\n        if \"description\" in data:\n            micro_agent.description = data[\"description\"]\n        if \"type\" in data:\n            micro_agent.type = data[\"type\"]\n        if \"version\" in data:\n            micro_agent.version = data[\"version\"]\n        if \"language\" in data:\n            micro_agent.language = data[\"language\"]\n        if \"language_version\" in data:\n            micro_agent.language_version = data[\"language_version\"]\n        if \"language_runtime\" in data:\n            micro_agent.language_runtime = data[\"language_runtime\"]\n        if \"language_runtime_version\" in data:\n            micro_agent.language_runtime_version = data[\"language_runtime_version\"]\n        if \"language_runtime_url\" in data:\n            micro_agent.language_runtime_url = data[\"language_runtime_url\"]\n        if \"language_runtime_files\" in data:\n            micro_agent.language_runtime_files = data[\"language_runtime_files\"]\n        if \"language_runtime_dependencies\" in data:\n            micro_agent.language_runtime_dependencies = data[\"language_runtime_dependencies\"]\n        if \"language_runtime_environment_variables\" in data:\n            micro_agent.language_runtime_environment_variables = data[\"language_runtime_environment_variables\"]\n        if \"language_runtime_executable\" in data:\n            micro_agent.language_runtime_executable = data[\"language_runtime_executable\"]\n        if \"language_runtime_command_line_arguments\" in data:\n            micro_agent.language_runtime_command_line_arguments = data[\"language_runtime_command_line_arguments\"]\n        if \"language_runtime_command_line_arguments_example\" in data:\n            micro_agent.language_runtime_command_line_arguments_example = data[\"language_"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent_id = data.get('agent_id', None)\n        agent_type = data.get('agent_type', None)\n        agent_lifecycle = data.get('agent_lifecycle', None)\n        agent_lifecycle = agent_lifecycle if agent_lifecycle is not None else agent_lifecycle\n        agent_lifecycle = AgentLifecycle.from_str(agent_lifecycle)\n        agent_name = data.get('agent_name', None)\n        agent_name = agent_name if agent_name is not None else agent_name\n        agent_description = data.get('agent_description', None)\n        agent_description = agent_description if agent_description is not None else agent_description\n        agent_version = data.get('agent_version', None)\n        agent_version = agent_version if agent_version is not None else agent_version\n        agent_image = data.get('agent_image', None)\n        agent_image = agent_image if agent_image is not None else agent_image\n        agent_url = data.get('agent_url', None)\n        agent_url = agent_url if agent_url is not None else agent_url\n        agent_license = data.get('agent_license', None)\n        agent_license = agent_license if agent_license is not None else agent_license\n        agent_license_url = data.get('agent_license_url', None)\n        agent_license_url = agent_license_url if agent_license_url is not None else agent_license_url\n        agent_license_type = data.get('agent_license_type', None)\n        agent_license_type = agent_license_type if agent_license_type is not None else agent_license_type\n        agent_license_type = LicenseType.from_str(agent_license_type)\n        agent_author = data.get('agent_author', None)\n        agent_author = agent_author if agent_author is not None else agent_author\n        agent_author_email = data.get('agent_author_email', None"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        if 'name' in data:\n            agent.name = data['name']\n        if 'description' in data:\n            agent.description = data['description']\n        if 'version' in data:\n            agent.version = data['version']\n        if 'tags' in data:\n            agent.tags = data['tags']\n        if 'properties' in data:\n            agent.properties = data['properties']\n        if 'capabilities' in data:\n            agent.capabilities = data['capabilities']\n        if 'default_language' in data:\n            agent.default_language = data['default_language']\n        if 'languages' in data:\n            agent.languages = data['languages']\n        if 'default_model' in data:\n            agent.default_model = data['default_model']\n        if 'models' in data:\n            agent.models = data['models']\n        if 'default_prompt' in data:\n            agent.default_prompt = data['default_prompt']\n        if 'prompts' in data:\n            agent.prompts = data['prompts']\n        if 'default_chat_prompt' in data:\n            agent.default_chat_prompt = data['default_chat_prompt']\n        if 'chat_prompts' in data:\n            agent.chat_prompts = data['chat_prompts']\n        if 'default_chat_model' in data:\n            agent.default_chat_model = data['default_chat_model']\n        if 'chat_models' in data:\n            agent.chat_models = data['chat_models']\n        if 'default_chat_system_message' in data:\n            agent.default_chat_system_message = data['default_chat_system_message']\n        if 'chat_system_messages' in data:\n            agent.chat_system_messages = data['chat_system_messages']\n        if 'default_chat_history' in data:\n            agent.default_chat_history = data['default_chat_history']"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = jnp.finfo(jnp.float32).eps\n\n  linear = xnp.where(\n    srgb <= 0.04045,\n    srgb / 12.92,\n    xnp.power((srgb + 0.055) / 1.055, 2.4)\n  )\n\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = jnp.finfo(jnp.float32).eps\n\n  # Define the piecewise function for sRGB to linear conversion\n  def srgb_to_linear_piecewise(x):\n    return xnp.where(\n        x <= 0.04045,\n        x / 12.92,\n        xnp.power((x + 0.055) / 1.055, 2.4)\n    )\n\n  # Apply the piecewise function to the input sRGB values\n  return srgb_to_linear_piecewise(srgb)\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  # Define the piecewise function for sRGB to linear conversion\n  def srgb_to_linear_piecewise(x):\n    return xnp.where(\n      x <= 0.04045,\n      x / 12.92,\n      xnp.power((x + 0.055) / 1.055, 2.4)\n    )\n\n  # Apply the piecewise function to the input sRGB values\n  return srgb_to_linear_piecewise(srgb)\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Define the piecewise function for sRGB to linear conversion\n  def srgb_to_linear_piecewise(x):\n    return xnp.where(\n        x <= 0.04045,\n        x / 12.92,\n        xnp.power((x + 0.055) / 1.055, 2.4)\n    )\n\n  # Convert sRGB values to linear space\n  linear = srgb_to_linear_piecewise(srgb)\n\n  # Return the linear color values\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  # Define the piecewise function\n  def piecewise_function(x):\n    return xnp.where(\n        x <= 0.04045,\n        x / 12.92,\n        xnp.power((x + 0.055) / 1.055, 2.4)\n    )\n\n  # Apply the piecewise function to the sRGB values\n  linear = piecewise_function(srgb)\n\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Define the piecewise function for sRGB to linear conversion\n  def srgb_to_linear_piecewise(x):\n    return xnp.where(\n        x <= 0.04045,\n        x / 12.92,\n        xnp.power((x + 0.055) / 1.055, 2.4)\n    )\n\n  # Convert sRGB to linear space\n  linear = srgb_to_linear_piecewise(srgb)\n\n  # Return the converted color values in linear space\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Define the piecewise function for sRGB to linear conversion\n  def srgb_to_linear_piecewise(x, eps):\n    return xnp.where(\n        x <= 0.04045,\n        x / 12.92,\n        xnp.power((x + 0.055) / 1.055, 2.4)\n    )\n\n  # Convert the input sRGB values to linear space\n  linear = srgb_to_linear_piecewise(srgb, eps)\n\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  srgb = xnp.where(srgb <= 0.04045, srgb / 12.92, xnp.power((srgb + 0.055) / 1.055, 2.4))\n  return xnp.where(srgb <= eps, eps, srgb)\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = jnp.finfo(jnp.float32).eps\n\n  return jnp.where(\n    srgb <= eps,\n    srgb * 12.92,\n    1.055 * jnp.power(srgb, 1.0 / 2.4) - 0.055\n  )\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = jnp.finfo(jnp.float32).eps\n\n  # Define the piecewise function\n  def piecewise_function(x):\n    return x / 12.92 if x <= 0.04045 else xnp.power((x + 0.055) / 1.055, 2.4)\n\n  # Apply the piecewise function to each element of the input array\n  linear = xnp.where(srgb <= eps, eps, piecewise_function(srgb))\n\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  # Define the piecewise function to convert sRGB to linear\n  def srgb_to_linear_piecewise(x):\n    return xnp.where(\n        x <= 0.04045,\n        x / 12.92,\n        xnp.power((x + 0.055) / 1.055, 2.4)\n    )\n\n  # Convert the sRGB values to linear space\n  return srgb_to_linear_piecewise(srgb)\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Define the piecewise function for sRGB to linear conversion\n  def piecewise_function(x):\n    return xnp.where(x <= 0.04045, x / 12.92, xnp.power((x + 0.055) / 1.055, 2.4))\n\n  # Convert srgb to linear\n  linear = piecewise_function(srgb)\n\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Set default epsilon value if not provided\n  if eps is None:\n    eps = jnp.finfo(jnp.float32).eps\n\n  # Convert sRGB to linear\n  linear = jnp.where(\n    srgb <= 0.04045,\n    srgb / 12.92,\n    jnp.power((srgb + 0.055) / 1.055, 2.4)\n  )\n\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Define the piecewise function for sRGB to linear conversion\n  def srgb_to_linear_piecewise(x):\n    return xnp.where(x <= 0.04045, x / 12.92, xnp.power((x + 0.055) / 1.055, 2.4))\n\n  # Convert srgb to linear\n  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear = srgb_to_linear_piecewise(srgb + eps)\n\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Set default values\n  if eps is None:\n    eps = jnp.finfo(jnp.float32).eps\n\n  # Convert sRGB to linear space\n  srgb = xnp.array(srgb)\n  linear = xnp.where(\n    srgb <= 0.04045,\n    srgb / 12.92,\n    xnp.power((srgb + 0.055) / 1.055, 2.4)\n  )\n\n  return linear\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Define the epsilon value\n  eps = eps if eps is not None else jnp.finfo(jnp.float32).eps\n\n  # Define the piecewise function for sRGB to linear conversion\n  def srgb_to_linear_piecewise(x):\n    return xnp.where(x <= 0.04045, x / 12.92, xnp.power((x + 0.055) / 1.055, 2.4))\n\n  # Apply the piecewise function to convert sRGB values to linear space\n  return srgb_to_linear_piecewise(srgb)\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  # Define piecewise function\n  def piecewise(x):\n    return xnp.where(\n        x <= 0.04045,\n        x / 12.92,\n        xnp.power((x + 0.055) / 1.055, 2.4)\n    )\n\n  # Convert sRGB to linear space\n  return xnp.where(\n      srgb <= eps,\n      eps,\n      piecewise(srgb)\n  )\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Check if eps is provided or not\n  if eps is None:\n    eps = jnp.finfo(jnp.float32).eps\n\n  # Convert sRGB values to linear space\n  linear = xnp.where(srgb <= (12.92 * eps), srgb / 12.92, xnp.power((srgb + (11.0 / 255.0)) / 1.055, 2.4))\n\n  return linear"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = jnp.finfo(jnp.float32).eps\n\n  return xnp.where(\n    srgb <= (4.045 * eps),\n    srgb / 12.92,\n    xnp.power((srgb + (15.0 * eps) / 16.0) / 1.055, 2.4)\n  )\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  return xnp.where(\n    srgb <= (4.045 * eps),\n    srgb / 12.92,\n    xnp.power((srgb + (0.055 * eps)) / (1.055 * eps), 2.4)\n  )\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  ###\n  #"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  ### BEGIN SOLUTION\n\n  ### END SOLUTION\n\n  return y"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  from scipy.interpolate import splrep, splev\n\n  # Adjust spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit spline to the signal\n  tck = splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times\n  y = splev(t_output, tck)\n\n  return y"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check if the input and output times are sorted\n  if not np.all(np.diff(t_input) >= 0):\n    raise ValueError(\"Input times must be sorted.\")\n  if not np.all(np.diff(t_output) >= 0):\n    raise ValueError(\"Output times must be sorted.\")\n\n  # Check if the input and output times are within the range of the signal\n  if t_input[0] > t_output[0] or t_input[-1] < t_output[-1]:\n    raise ValueError(\"Output times must be within the range of the signal.\")\n\n  # Check if the input and output times are not too close together\n  if np.min(np.diff(t_output)) < 1e-6:\n    raise ValueError(\"Output times must be at least 1e-6 apart.\")\n\n  # Check if the input and output times are not too close together\n  if np.min(np.diff(t_input)) < 1e-6:\n    raise ValueError(\"Input times must be at least 1e-6 apart.\")\n\n  # Check if the input and output times are not too close together\n  if np.min(np.diff(t_output)) < 1e-6:\n    raise ValueError(\"Output times must be at least 1e-6 apart.\")\n\n  # Check if the input and output times are not too close together\n  if np.min(np.diff(t_input)) < 1e-6:\n    raise ValueError(\"Input times must be at least 1e-6 apart.\")\n\n  # Check if the input and output times are not too close together\n  if np.min(np.diff(t_output)) < 1e-6:\n    raise ValueError(\"Output times must be at least 1e-6 apart.\")\n\n  # Check if the input and output times are not too close together\n  if np.min(np.diff(t_input)) < 1e-6:\n    raise ValueError(\"Input times must be at least 1e-6 apart.\")\n\n  # Check if the input and output times are not too close together\n  if np.min(np"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check if the input and output times are sorted\n  if not np.all(np.diff(t_input) >= 0):\n    raise ValueError(\"Input times must be sorted.\")\n  if not np.all(np.diff(t_output) >= 0):\n    raise ValueError(\"Output times must be sorted.\")\n\n  # Check if the input and output times are within the range of the signal\n  if np.any(t_input < t_input[0]) or np.any(t_input > t_input[-1]):\n    raise ValueError(\"Input times must be within the range of the signal.\")\n  if np.any(t_output < t_input[0]) or np.any(t_output > t_input[-1]):\n    raise ValueError(\"Output times must be within the range of the signal.\")\n\n  # Check if the input and output times are unique\n  if not np.all(np.diff(t_input) > 0):\n    raise ValueError(\"Input times must be unique.\")\n  if not np.all(np.diff(t_output) > 0):\n    raise ValueError(\"Output times must be unique.\")\n\n  # Check if the input and output times are not too close together\n  if np.any(np.diff(t_input) < 1e-6):\n    raise ValueError(\"Input times must be at least 1e-6 apart.\")\n  if np.any(np.diff(t_output) < 1e-6):\n    raise ValueError(\"Output times must be at least 1e-6 apart.\")\n\n  # Check if the input and output times are not too far apart\n  if np.any(np.diff(t_input) > 1000):\n    raise ValueError(\"Input times must be at most 1000 apart.\")\n  if np.any(np.diff(t_output) > 1000):\n    raise ValueError(\"Output times must be at most 1000 apart.\")\n\n  # Check if the input and output times are not too close to the edges of the signal\n  if np.any(t_input < t_input[0] + 1"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy import interpolate\n\n  # Adjust spline degree\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit spline\n  spline = interpolate.UnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate spline at output times\n  x_interpolated = spline(t_output)\n\n  return x_interpolated"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check if the input and output times are sorted\n  if not np.all(np.diff(t_input) >= 0):\n    raise ValueError(\"Input times must be sorted.\")\n  if not np.all(np.diff(t_output) >= 0):\n    raise ValueError(\"Output times must be sorted.\")\n\n  # Check if the input and output times are within the range of the signal\n  if t_input[0] > t_output[0] or t_input[-1] < t_output[-1]:\n    raise ValueError(\"Output times must be within the range of the input times.\")\n\n  # Check if the input and output times are not too close together\n  if np.min(np.diff(t_input)) < np.min(np.diff(t_output)):\n    raise ValueError(\"Output times must be spaced further apart than input times.\")\n\n  # Check if the input and output times are not too close together\n  if np.min(np.diff(t_output)) < np.min(np.diff(t_input)):\n    raise ValueError(\"Input times must be spaced further apart than output times.\")\n\n  # Check if the input and output times are not too close together\n  if np.min(np.diff(t_output)) < np.min(np.diff(t_input)):\n    raise ValueError(\"Input times must be spaced further apart than output times.\")\n\n  # Check if the input and output times are not too close together\n  if np.min(np.diff(t_output)) < np.min(np.diff(t_input)):\n    raise ValueError(\"Input times must be spaced further apart than output times.\")\n\n  # Check if the input and output times are not too close together\n  if np.min(np.diff(t_output)) < np.min(np.diff(t_input)):\n    raise ValueError(\"Input times must be spaced further apart than output times.\")\n\n  # Check if the input and output times are not too close together\n  if np.min(np.diff(t_output)) < np.min(np.diff(t_input))"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Check if the input and output times are sorted\n  if not np.all(np.diff(t_input) >= 0):\n    raise ValueError(\"Input times must be sorted.\")\n  if not np.all(np.diff(t_output) >= 0):\n    raise ValueError(\"Output times must be sorted.\")\n\n  # Check if the input and output times are within the range of the signal\n  if t_input[0] > t_output[0] or t_input[-1] < t_output[-1]:\n    raise ValueError(\"Input and output times must be within the range of the signal.\")\n\n  # Check if the input and output times are unique\n  if not np.all(np.diff(t_input) != 0):\n    raise ValueError(\"Input times must be unique.\")\n  if not np.all(np.diff(t_output) != 0):\n    raise ValueError(\"Output times must be unique.\")\n\n  # Check if the input and output times are not the same\n  if np.all(t_input == t_output):\n    raise ValueError(\"Input and output times must be different.\")\n\n  # Check if the input and output times are not the same\n  if np.all(t_input == t_output):\n    raise ValueError(\"Input and output times must be different.\")\n\n  # Check if the input and output times are not the same\n  if np.all(t_input == t_output):\n    raise ValueError(\"Input and output times must be different.\")\n\n  # Check if the input and output times are not the same\n  if np.all(t_input == t_output):\n    raise ValueError(\"Input and output times must be different.\")\n\n  # Check if the input and output times are not the same\n  if np.all(t_input == t_output):\n    raise ValueError(\"Input and output times must be different.\")\n\n  # Check if the input and output times are not the same\n  if np.all(t_input == t_output):\n    raise ValueError(\"Input and"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check that the input and output times are sorted\n  if not np.all(np.diff(t_input) >= 0):\n    raise ValueError(\"Input times must be sorted.\")\n  if not np.all(np.diff(t_output) >= 0):\n    raise ValueError(\"Output times must be sorted.\")\n\n  # Check that the input and output times are within the range of the input times\n  if np.any(t_output < t_input[0]) or np.any(t_output > t_input[-1]):\n    raise ValueError(\"Output times must be within the range of the input times.\")\n\n  # Check that the input and output times are not too close together\n  if np.any(np.diff(t_input) < 1e-6):\n    raise ValueError(\"Input times must be at least 1e-6 apart.\")\n  if np.any(np.diff(t_output) < 1e-6):\n    raise ValueError(\"Output times must be at least 1e-6 apart.\")\n\n  # Check that the input and output times are not too far apart\n  if np.any(np.diff(t_input) > 100):\n    raise ValueError(\"Input times must be at most 100 apart.\")\n  if np.any(np.diff(t_output) > 100):\n    raise ValueError(\"Output times must be at most 100 apart.\")\n\n  # Check that the input and output times are not too close to the edges\n  if np.any(t_output < t_input[1]) or np.any(t_output > t_input[-2]):\n    raise ValueError(\"Output times must be at least 1e-6 apart from the edges of the input times.\")\n\n  # Check that the input and output times are not too far from the edges\n  if np.any(t_output < t_input[0]) or np.any(t_output > t_input[-1]):\n    raise ValueError(\"Output times must be at most 100 apart from the edges of the input times.\")\n\n  # Check that the input and output times are not too"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy import interpolate\n\n  if spline_degree > len(x) - 1:\n    spline_degree = len(x) - 1\n\n  if spline_degree < 1:\n    spline_degree = 1\n\n  spline_degree = int(spline_degree)\n\n  tck = interpolate.splrep(t_input, x, s=smoothness, k=spline_degree)\n  x_interp = interpolate.splev(t_output, tck)\n\n  return x_interp"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  from scipy.interpolate import InterpolatedUnivariateSpline\n\n  # Adjust spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit spline to x\n  spline = InterpolatedUnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate spline at t_output times\n  return spline(t_output)\n\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check if x and t_input are numpy arrays\n  if not isinstance(x, np.ndarray):\n    x = np.array(x)\n  if not isinstance(t_input, np.ndarray):\n    t_input = np.array(t_input)\n\n  # Check if t_output is a numpy array\n  if not isinstance(t_output, np.ndarray):\n    t_output = np.array(t_output)\n\n  # Check if t_input and t_output are 1-dimensional arrays\n  if x.ndim != 1:\n    raise ValueError(\"x must be a 1-dimensional array\")\n  if t_input.ndim != 1:\n    raise ValueError(\"t_input must be a 1-dimensional array\")\n  if t_output.ndim != 1:\n    raise ValueError(\"t_output must be a 1-dimensional array\")\n\n  # Check if x and t_input have the same number of elements\n  if x.shape[0] != t_input.shape[0]:\n    raise ValueError(\"x and t_input must have the same number of elements\")\n\n  # Check if t_input is sorted in ascending order\n  if not np.all(np.diff(t_input) >= 0):\n    raise ValueError(\"t_input must be sorted in ascending order\")\n\n  # Check if t_output is sorted in ascending order\n  if not np.all(np.diff(t_output) >= 0):\n    raise ValueError(\"t_output must be sorted in ascending order\")\n\n  # Check if t_output is within the range of t_input\n  if np.any(t_output < t_input[0]) or np.any(t_output > t_input[-1]):\n    raise ValueError(\"t_output must be within the range of t_input\")\n\n  # Check if t_input and t_output have the same number of elements\n  if t_input.shape[0] != t_output.shape[0]:\n    raise ValueError(\"t_input and t_output"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  import numpy as np\n  from scipy.interpolate import splev, splrep\n\n  if len(x) < 2:\n    raise ValueError(\"x must have at least two elements\")\n\n  if len(t_input) != len(x):\n    raise ValueError(\"t_input must have the same length as x\")\n\n  if len(t_output) < 1:\n    raise ValueError(\"t_output must have at least one element\")\n\n  if spline_degree < 1:\n    raise ValueError(\"spline_degree must be at least 1\")\n\n  if spline_degree > len(x) - 1:\n    spline_degree = len(x) - 1\n\n  if smoothness <= 0:\n    raise ValueError(\"smoothness must be positive\")\n\n  tck = splrep(t_input, x, k=spline_degree, s=smoothness)\n  return splev(t_output, tck)\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  from scipy.interpolate import PPoly\n\n  if spline_degree < 1:\n    raise ValueError(\"spline_degree must be at least 1\")\n  if spline_degree > len(x) - 1:\n    spline_degree = len(x) - 1\n\n  spline_degree = min(spline_degree, len(x) - 1)\n  spline_degree = max(spline_degree, 1)\n\n  ppoly = PPoly.fit(t_input, x, spline_degree, w=smoothness)\n  return ppoly(t_output)\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check if x and t_input are 1-dimensional arrays\n  if len(x.shape) != 1 or len(t_input.shape) != 1:\n    raise ValueError(\"x and t_input must be 1-dimensional arrays.\")\n\n  # Check if t_input and t_output are monotonically increasing\n  if not np.all(np.diff(t_input) > 0):\n    raise ValueError(\"t_input must be monotonically increasing.\")\n  if not np.all(np.diff(t_output) > 0):\n    raise ValueError(\"t_output must be monotonically increasing.\")\n\n  # Check if t_input and t_output are of the same length\n  if len(t_input) != len(x):\n    raise ValueError(\"t_input and x must have the same length.\")\n\n  # Check if spline_degree is an integer\n  if not isinstance(spline_degree, int):\n    raise ValueError(\"spline_degree must be an integer.\")\n\n  # Check if smoothness is a float\n  if not isinstance(smoothness, float):\n    raise ValueError(\"smoothness must be a float.\")\n\n  # Check if spline_degree is at most one less than the number of points in x\n  if spline_degree >= len(x) - 1:\n    raise ValueError(\"spline_degree must be at most one less than the number of points in x.\")\n\n  # Compute the spline coefficients using the splrep function\n  tck = splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Interpolate the signal at the output times using the splev function\n  y = splev(t_output, tck)\n\n  return y"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check input arguments\n  if len(x) != len(t_input):\n    raise ValueError(\"The length of x must be the same as the length of t_input\")\n  if len(t_input) != len(t_output):\n    raise ValueError(\"The length of t_input must be the same as the length of t_output\")\n  if spline_degree < 0:\n    raise ValueError(\"The spline degree must be non-negative\")\n  if smoothness <= 0:\n    raise ValueError(\"The smoothness must be positive\")\n\n  # Sort the input times\n  t_input = np.sort(t_input)\n\n  # Compute the spline coefficients\n  coeffs = splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Interpolate the signal at the output times\n  x_output = splev(t_output, coeffs)\n\n  return x_output"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check input arguments\n  if len(x) != len(t_input):\n    raise ValueError('The number of points in x and t_input must be the same.')\n\n  # Convert input arguments to numpy arrays\n  x = np.array(x)\n  t_input = np.array(t_input)\n  t_output = np.array(t_output)\n\n  # Adjust spline degree if necessary\n  if spline_degree >= len(x):\n    spline_degree = len(x) - 1\n\n  # Sort input times\n  sort_indices = np.argsort(t_input)\n  t_input = t_input[sort_indices]\n  x = x[sort_indices]\n\n  # Compute spline coefficients\n  spline_coefficients = spline_coefficients_1d(x, spline_degree, smoothness)\n\n  # Interpolate signal at output times\n  y = spline_interpolate_1d(spline_coefficients, t_input, t_output)\n\n  return y\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check input arguments\n  if not isinstance(x, (list, tuple, np.ndarray)):\n    raise ValueError(\"x must be a list, tuple or numpy array\")\n  if not isinstance(t_input, (list, tuple, np.ndarray)):\n    raise ValueError(\"t_input must be a list, tuple or numpy array\")\n  if not isinstance(t_output, (list, tuple, np.ndarray)):\n    raise ValueError(\"t_output must be a list, tuple or numpy array\")\n  if not isinstance(spline_degree, int):\n    raise ValueError(\"spline_degree must be an integer\")\n  if not isinstance(smoothness, float):\n    raise ValueError(\"smoothness must be a float\")\n  if len(x) != len(t_input):\n    raise ValueError(\"x and t_input must have the same length\")\n  if len(t_output) == 0:\n    raise ValueError(\"t_output must not be empty\")\n  if spline_degree > len(x) - 1:\n    raise ValueError(\"spline_degree must be at most one less than the number of points in x\")\n\n  # Check if t_output is in the range of t_input\n  if np.any(t_output < t_input[0]) or np.any(t_output > t_input[-1]):\n    raise ValueError(\"t_output must be in the range of t_input\")\n\n  # Check if t_input is sorted\n  if np.any(np.diff(t_input) < 0):\n    raise ValueError(\"t_input must be sorted\")\n\n  # Check if t_input contains unique values\n  if len(t_input) != len(np.unique(t_input)):\n    raise ValueError(\"t_input must contain unique values\")\n\n  # Check if t_output contains unique values\n  if len(t_output) != len(np.unique(t_output)):\n    raise ValueError(\"t_output must contain unique values\")\n\n  # Check if t_input and t_"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check that the input and output times are sorted\n  assert t_input.ndim == 1\n  assert t_output.ndim == 1\n  assert t_input.shape == t_output.shape\n  assert np.all(t_input[1:] >= t_input[:-1])\n  assert np.all(t_output[1:] >= t_output[:-1])\n\n  # Check that the signal is 1-dimensional\n  assert x.ndim == 1\n\n  # Check that the spline degree is valid\n  assert spline_degree >= 1\n  assert spline_degree <= t_input.shape[0] - 1\n\n  # Check that the smoothness parameter is positive\n  assert smoothness > 0\n\n  # Check that the input and output times are within the range of the signal\n  assert np.all(t_input[0] <= t_output)\n  assert np.all(t_input[-1] >= t_output)\n\n  # Interpolate the signal using spline interpolation\n  spline_interpolator = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n  x_interpolated = scipy.interpolate.splev(t_output, spline_interpolator)\n\n  return x_interpolated\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # check input arguments\n  if len(t_input) != len(x):\n    raise ValueError(\"The number of input times must be the same as the number of input values.\")\n  if len(t_output) != len(t_input):\n    raise ValueError(\"The number of output times must be the same as the number of input times.\")\n  if spline_degree < 0:\n    raise ValueError(\"The spline degree must be non-negative.\")\n  if spline_degree >= len(t_input):\n    raise ValueError(\"The spline degree must be smaller than the number of input times.\")\n  if smoothness <= 0:\n    raise ValueError(\"The smoothness parameter must be positive.\")\n\n  # sort input times and values\n  t_input, x = zip(*sorted(zip(t_input, x)))\n\n  # interpolate signal\n  x_interp = interpolate_1d(\n      x,\n      spline_degree,\n      smoothness,\n      t_input,\n      t_output,\n  )\n\n  # check if interpolation succeeded\n  if x_interp is None:\n    raise ValueError(\"Interpolation failed.\")\n\n  return x_interp\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    elif word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    else:\n        return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word\n    elif word[0].islower() and word[1].islower():\n        return word.capitalize()\n    elif word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    else:\n        return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # if lower no uppers after\n    # if upper no\n\n    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word\n    elif word[0].isupper() and word[1].islower():\n        return word\n    elif word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    elif word.isupper():\n        return word\n    else:\n        if word[0].islower() and word[1].isupper():\n            return word[0].upper() + word[1:].lower()\n        elif word[0].isupper() and word[1].isupper():\n            return word.lower()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        else:\n            return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].islower() and word[1].isupper():\n        return word[0].upper() + word[1:]\n    if word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word\n    elif word[0].islower() and word[1].islower():\n        return word\n    elif word[0].islower() and word[1].isupper():\n        return word[0].upper() + word[1:]\n    elif word[0].isupper() and word[1].islower():\n        return word[0].lower() + word[1:]\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    elif word.isupper():\n        return word\n    else:\n        if word[0].islower() and word[1].isupper():\n            return word[0].upper() + word[1:].lower()\n        elif word[0].isupper() and word[1].islower():\n            return word[0].lower() + word[1:].capitalize()\n        else:\n            return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    if word.isupper():\n        return word\n\n    if word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # if lower no uppers after\n    # if upper no\n    if word.islower():\n        return word\n    elif word.isupper():\n        return word\n    else:\n        # if mixed case, convert to lowercase\n        word = word.lower()\n        # capitalize first letter of first two words\n        word = word[0].upper() + word[1:]\n        return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    elif word.isupper():\n        return word\n    else:\n        if word[0].islower() and word[1].islower():\n            return word.lower()\n        elif word[0].isupper() and word[1].isupper():\n            return word.lower()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        elif word[0].islower() and word[1].isupper():\n            return word[0] + word[1:].lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0].islower():\n            return word.lower()\n        elif word[0].isupper() and word[1].isupper():\n            return word.lower()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        else:\n            return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # if lower no uppers after\n    # if upper no\n\n    if word.islower():\n        return word\n    elif word.isupper():\n        return word\n    else:\n        if word[0].islower() and word[1].isupper():\n            return word.capitalize()\n        else:\n            return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    if word[0].islower() and word[1].islower():\n        return word.capitalize()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0:2].isupper():\n            return word.capitalize()\n        else:\n            return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    elif word.isupper():\n        return word\n    else:\n        if word[0].islower() and word[1].islower():\n            return word.lower()\n        elif word[0].isupper() and word[1].isupper():\n            return word.lower()\n        else:\n            return word.capitalize()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # if lower no uppers after\n    # if upper no\n    if word.islower():\n        return word\n    elif word.isupper():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word.lower()\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be binary\")\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__} field {field.name} must be binary, found {v.dtype}\"\n        )\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a binary array, but found {v.dtype}\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}.{field.name} must be a binary array. Found {v.dtype} instead.\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a binary array, but got {v.dtype}\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__} field {field.name} must be binary (only True or False)\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a binary array\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__} - {field.name} must be binary\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__} has a field {field.name} that is not binary\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a binary array (dtype=bool), but got {v.dtype}\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be binary, but contains data type {v.dtype}.\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be binary, but got {v.dtype}\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__} field {field.name} must be a binary array\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a binary array (containing only True or False values). Found {v.dtype} instead.\")\n    return v\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a binary array, but got {v.dtype}\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be binary\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be binary. Found {v.dtype}\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__} field {field.name} must be a binary array.\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}.{field.name} must be a binary array, got {v.dtype} instead.\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be binary (boolean)\")\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = np.linalg.norm(x)\n  if x_norm == 0:\n    return x\n  else:\n    return x / x_norm"}
{"namespace": "coord.contract3_isoscale", "completion": "  import numpy as np\n\n  norm = np.linalg.norm(x)\n  if norm == 0:\n    return x\n  else:\n    return x / norm"}
{"namespace": "coord.contract3_isoscale", "completion": "  import numpy as np\n\n  norm = np.linalg.norm(x)\n  if norm == 0:\n    return x\n  else:\n    return x / norm"}
{"namespace": "coord.contract3_isoscale", "completion": "  import numpy as np\n\n  # Get the norm of the input array\n  x_norm = np.linalg.norm(x)\n\n  # Calculate the scaling factor based on the norm\n  if x_norm > 0:\n    scale_factor = 1 / x_norm\n  else:\n    scale_factor = 0\n\n  # Apply the scaling operation to the input array\n  x_scaled = scale_factor * x\n\n  return x_scaled"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Compute the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Compute the scaling factor based on the norm\n  scale = np.sqrt(1 / (norm ** 2 + 1))\n\n  # Apply the scaling operation to the input array\n  scaled_x = scale * x\n\n  return scaled_x"}
{"namespace": "coord.contract3_isoscale", "completion": "  import numpy as np\n  from numpy import linalg as LA\n\n  # Calculate the norm of the input array\n  norm = LA.norm(x)\n\n  # If the norm is zero, return the input array\n  if norm == 0:\n    return x\n\n  # Calculate the scaling factor based on the norm\n  scale = 1 / norm\n\n  # Apply the scaling operation to the input array\n  return x * scale"}
{"namespace": "coord.contract3_isoscale", "completion": "  from numpy import sqrt\n\n  # Compute the norm of the input array\n  norm = sqrt(x[0]**2 + x[1]**2 + x[2]**2)\n\n  # Apply the isotropic scaling operation\n  x[0] = x[0] / norm\n  x[1] = x[1] / norm\n  x[2] = x[2] / norm\n\n  # Return the scaled array\n  return x"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # If the norm is zero, return the input array\n  if norm == 0:\n      return x\n\n  # Calculate the scaling factor based on the norm\n  scale_factor = np.sqrt(1 / norm)\n\n  # Scale the input array using the scaling factor\n  scaled_x = x * scale_factor\n\n  # Return the scaled array\n  return scaled_x"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Compute the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Perform the isotropic scaling operation\n  y = x / norm\n\n  # Return the scaled array\n  return y\n\n\n\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  import numpy as np\n\n  # Calculate the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Check if the norm is greater than zero\n  if norm > 0:\n    # Calculate the scaling factor based on the norm\n    scale = np.sqrt(3) / norm\n    # Apply the scaling operation to the input array\n    x = scale * x\n\n  # Return the scaled input array\n  return x"}
{"namespace": "coord.contract3_isoscale", "completion": "  import numpy as np\n  import math\n\n  norm = np.linalg.norm(x)\n  if norm == 0:\n    return x\n  else:\n    return x / norm\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Imports\n  import numpy as np\n  import math\n\n  # Compute norm\n  norm = np.linalg.norm(x)\n\n  # Compute scaling factor\n  scale_factor = 1 / math.sqrt(norm)\n\n  # Apply scaling operation\n  y = scale_factor * x\n\n  return y"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input array\n  x_norm = np.linalg.norm(x, axis=-1)\n\n  # Perform the isotropic scaling operation\n  x_scaled = x / x_norm[:, None]\n\n  return x_scaled\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Importing the necessary libraries\n  import numpy as np\n  from numba import jit\n\n  # Checking if the input array is 3-dimensional\n  if len(x.shape) != 3:\n    raise ValueError(\"Input array must be 3-dimensional\")\n\n  # Calculating the norm of the input array\n  norm = np.sqrt(np.sum(x ** 2))\n\n  # Applying the isotropic scaling operation\n  x /= norm\n\n  # Returning the scaled array\n  return x\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  from numpy import sqrt\n  return x * sqrt(x.size)\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = np.linalg.norm(x, axis=-1, keepdims=True)\n  x_scaled = x / x_norm\n\n  return x_scaled\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  norm = np.linalg.norm(x, axis=-1)\n  norm_inv = 1.0 / norm\n  norm_inv3 = norm_inv * norm_inv * norm_inv\n  x_scaled = x * norm_inv3[..., np.newaxis]\n  return x_scaled\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Get the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Calculate the scaling factor based on the norm\n  scale = np.sqrt(1 / (1 + norm**2))\n\n  # Scale the input array\n  return x * scale\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  import numpy as np\n  import math\n\n  # Compute the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Calculate the scaling factor based on the norm\n  if norm > 0:\n    scale_factor = math.sqrt(1 / norm)\n  else:\n    scale_factor = 0\n\n  # Apply the scaling operation to the input array\n  x_scaled = x * scale_factor\n\n  return x_scaled\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # If the norm is zero, return an array of zeros of the same shape as the input\n  if norm == 0:\n    return np.zeros(x.shape)\n\n  # If the norm is not zero, calculate the scaled version of the input array\n  else:\n    return x / norm\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    df = pd.read_csv(summary_path)\n\n    for col in dict_columns:\n        df[col] = df[col].apply(lambda x: ast.literal_eval(x))\n\n    return df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    summary_df = pd.read_csv(summary_path)\n\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file into a DataFrame\n    df = pd.read_csv(summary_path)\n\n    # Convert the specified dictionary-like columns into actual dictionaries\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    for col in dict_columns:\n        df[col] = df[col].apply(lambda x: eval(x))\n\n    return df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file into a DataFrame\n    summary_df = pd.read_csv(summary_path, sep='\\t')\n\n    # If the dictionary columns are not provided, use the default\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Convert the dictionary-like strings in the specified columns to actual dictionaries\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file into a DataFrame\n    df = pd.read_csv(summary_path)\n\n    # If no dictionary columns are specified, use the default ['module_params']\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Convert the specified dictionary columns to dictionaries\n    for col in dict_columns:\n        df[col] = df[col].apply(lambda x: ast.literal_eval(x))\n\n    return df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(eval)\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Read the summary file into a DataFrame\n    summary_df = pd.read_csv(summary_path, sep='\\t', header=0, index_col=0)\n\n    # Convert the specified dictionary-like columns into actual dictionaries\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Check if the summary file exists and is not empty\n    if not os.path.exists(summary_path) or os.stat(summary_path).st_size == 0:\n        raise FileNotFoundError(f\"Summary file not found at {summary_path}\")\n\n    # Read the summary file into a pandas DataFrame\n    df = pd.read_csv(summary_path)\n\n    # Convert dictionary-like strings in specified columns to actual dictionaries\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    for col in dict_columns:\n        if col in df.columns:\n            df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path, sep='\\t')\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file as a pandas DataFrame\n    summary_df = pd.read_csv(summary_path, sep='\\t')\n\n    # Convert specified dictionary-like columns to actual dictionaries\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(eval)\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Read the summary file into a DataFrame\n    summary_df = pd.read_csv(summary_path, sep='\\t')\n\n    # Convert the specified columns into dictionaries\n    if dict_columns is None:\n        dict_columns = ['module_params']\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(eval)\n\n    return summary_df\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # If no dictionary columns are provided, use the default\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Load the summary file into a DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Convert dictionary-like strings in the specified columns into actual dictionaries\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(\n            lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Read the summary file into a pandas DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Convert the specified columns that contain dictionary-like strings into actual dictionaries\n    if dict_columns is None:\n        dict_columns = ['module_params']\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Read the summary file into a DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # If dictionary columns are not provided, use the default\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Convert dictionary-like strings in specified columns to actual dictionaries\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    summary_df = pd.read_csv(summary_path)\n\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Check if the summary file exists\n    if not os.path.exists(summary_path):\n        raise FileNotFoundError(f\"Summary file not found at {summary_path}\")\n\n    # Load the summary file into a DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Convert the specified columns to dictionaries\n    if dict_columns is None:\n        dict_columns = ['module_params']\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(eval)\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file into a pandas DataFrame\n    df = pd.read_csv(summary_path)\n\n    # If no dictionary columns are provided, use the default\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Convert dictionary-like strings in specified columns to actual dictionaries\n    for col in dict_columns:\n        df[col] = df[col].apply(eval)\n\n    return df\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    return cov * np.trace(cov) / np.linalg.det(cov)\n  elif mode == 'accurate':\n    return cov * np.exp(np.log(np.linalg.det(cov)) / cov.shape[0])\n  else:\n    raise ValueError(\"Invalid mode. Must be either 'fast' or 'accurate'.\")"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  if mode == 'fast':\n    isotropic_cov = cov * np.trace(cov) / np.linalg.det(cov)\n  elif mode == 'accurate':\n    isotropic_cov = cov * np.exp(np.log(np.linalg.det(cov)) / 2)\n  else:\n    raise ValueError(\"Invalid mode. Choose either 'fast' or 'accurate'.\")\n\n  return isotropic_cov"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  if mode == 'fast':\n    det = np.linalg.det(cov)\n    cov = cov * np.sqrt(det)\n  elif mode == 'accurate':\n    logdet = np.log(np.linalg.det(cov))\n    cov = cov * np.exp(logdet / 2)\n  else:\n    raise ValueError(\"Invalid mode: {}\".format(mode))\n\n  return cov"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  if mode == 'fast':\n    # Compute the isotropic covariance matrices using the determinant directly\n    det = np.linalg.det(cov)\n    cov_iso = np.sqrt(det) * np.eye(cov.shape[0])\n  elif mode == 'accurate':\n    # Compute the isotropic covariance matrices using the logarithm of the determinant for stability\n    logdet = np.log(np.linalg.det(cov))\n    cov_iso = np.sqrt(np.exp(logdet)) * np.eye(cov.shape[0])\n  else:\n    raise ValueError(\"Invalid mode. Must be either 'fast' or 'accurate'.\")\n\n  return cov_iso"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the covariance matrix\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is zero or negative\n  if det <= 0:\n    # If the determinant is zero or negative, return a zero covariance matrix\n    return np.zeros_like(cov)\n\n  # Compute the isotropic covariance matrix\n  if mode == 'fast':\n    # If the mode is 'fast', use the determinant directly\n    iso_cov = cov * np.sqrt(det)\n  elif mode == 'accurate':\n    # If the mode is 'accurate', use the logarithm of the determinant for stability\n    iso_cov = cov * np.exp(0.5 * np.log(det))\n  else:\n    # If the mode is not recognized, raise an error\n    raise ValueError(\"Invalid mode. Must be either 'fast' or 'accurate'.\")\n\n  # Return the isotropic covariance matrix\n  return iso_cov"}
{"namespace": "coord.isotropize", "completion": "  # Check if the input is a matrix or a collection of matrices\n  if len(cov.shape) == 2:\n    cov = [cov]\n\n  # Compute the isotropic covariance matrices\n  isotropic_cov = []\n  for c in cov:\n    if mode == 'fast':\n      # Compute the isotropic covariance matrix using the determinant directly\n      isotropic_cov.append(c * np.trace(c) / np.linalg.det(c))\n    elif mode == 'accurate':\n      # Compute the isotropic covariance matrix using the logarithm of the determinant for stability\n      isotropic_cov.append(c * np.trace(c) / np.exp(np.linalg.slogdet(c)[1]))\n    else:\n      raise ValueError(\"Invalid mode: {}\".format(mode))\n\n  # Return the isotropic covariance matrix or matrices\n  if len(isotropic_cov) == 1:\n    return isotropic_cov[0]\n  else:\n    return isotropic_cov"}
{"namespace": "coord.isotropize", "completion": "  # Check if the input is a 2D array\n  if len(cov.shape) != 2:\n    raise ValueError(\"Input array must be 2D\")\n\n  # Check if the input is a square matrix\n  if cov.shape[0] != cov.shape[1]:\n    raise ValueError(\"Input array must be square\")\n\n  # Compute the determinant of the input covariance matrix\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is positive\n  if det <= 0:\n    raise ValueError(\"Input covariance matrix must have positive determinant\")\n\n  # Compute the isotropic covariance matrix\n  if mode == 'fast':\n    # Use the determinant directly\n    cov_iso = cov * det\n  elif mode == 'accurate':\n    # Use the logarithm of the determinant for stability\n    cov_iso = cov * np.exp(0.5 * np.log(det))\n  else:\n    raise ValueError(\"Invalid mode\")\n\n  # Return the isotropic covariance matrix\n  return cov_iso"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  # Compute the determinant of the input covariance matrix\n  det = np.linalg.det(cov)\n\n  # Check if the determinant is non-zero\n  if det == 0:\n    raise ValueError(\"Determinant of input covariance matrix is zero.\")\n\n  # Compute the isotropic covariance matrix\n  if mode == 'fast':\n    # Use the determinant directly\n    isotropic_cov = det * np.eye(cov.shape[0])\n  elif mode == 'accurate':\n    # Use the logarithm of the determinant for stability\n    isotropic_cov = np.exp(np.log(det) / cov.shape[0]) * np.eye(cov.shape[0])\n  else:\n    raise ValueError(\"Invalid mode specified.\")\n\n  return isotropic_cov"}
{"namespace": "coord.isotropize", "completion": "  # Check if the input is a matrix or a list of matrices\n  if cov.ndim == 2:\n    cov = [cov]\n\n  # Compute the isotropic covariance matrices\n  if mode == 'fast':\n    isotropic_cov = [(1 / cov[i].shape[0]) * cov[i] for i in range(len(cov))]\n  elif mode == 'accurate':\n    isotropic_cov = [(1 / cov[i].shape[0]) * cov[i] * cov[i].shape[0] for i in range(len(cov))]\n  else:\n    raise ValueError(\"Invalid mode. Must be either 'fast' or 'accurate'.\")\n\n  # Check if the determinant of the isotropic covariance matrices is valid\n  for i in range(len(isotropic_cov)):\n    if np.linalg.det(isotropic_cov[i]) <= 0:\n      raise ValueError(\"Invalid determinant for isotropic covariance matrix.\")\n\n  # Return the isotropic covariance matrices\n  if len(isotropic_cov) == 1:\n    return isotropic_cov[0]\n  else:\n    return isotropic_cov"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'accurate':\n    logdet = np.log(np.linalg.det(cov))\n    if logdet < 0:\n      logdet = logdet * -1\n    isotropic_cov = cov * np.exp(logdet / 2)\n  elif mode == 'fast':\n    isotropic_cov = cov * np.trace(cov) / np.linalg.det(cov)\n  else:\n    raise ValueError('Invalid mode. Must be either \"fast\" or \"accurate\".')\n\n  return isotropic_cov"}
{"namespace": "coord.isotropize", "completion": "  # Check if the input is a matrix or a list of matrices\n  if cov.ndim == 2:\n    cov = [cov]\n\n  # Compute the determinant of the covariance matrices\n  if mode == 'fast':\n    dets = [np.linalg.det(c) for c in cov]\n  elif mode == 'accurate':\n    dets = [np.log(np.linalg.det(c)) for c in cov]\n  else:\n    raise ValueError(\"Invalid mode. Please use 'fast' or 'accurate'.\")\n\n  # Compute the isotropic covariance matrices\n  isotropic_covs = []\n  for i in range(len(cov)):\n    if dets[i] > 0:\n      isotropic_covs.append(cov[i] / np.sqrt(dets[i]))\n    else:\n      isotropic_covs.append(np.zeros_like(cov[i]))\n\n  # Return the isotropic covariance matrices\n  if len(isotropic_covs) == 1:\n    return isotropic_covs[0]\n  else:\n    return isotropic_covs"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n  from numpy import linalg as LA\n  from numpy import log as ln\n  from numpy import exp as e\n\n  # Compute the determinant of the covariance matrix or matrices\n  det = LA.det(cov)\n\n  # Check if the determinant is valid\n  if det <= 0:\n    raise ValueError(\"The determinant of the covariance matrix or matrices is not valid.\")\n\n  # Compute the isotropic covariance matrix or matrices using the specified mode\n  if mode == 'fast':\n    isotropic_cov = det * np.eye(cov.shape[0])\n  elif mode == 'accurate':\n    isotropic_cov = e(0.5 * ln(det)) * np.eye(cov.shape[0])\n  else:\n    raise ValueError(\"Invalid mode. Please choose either 'fast' or 'accurate'.\")\n\n  return isotropic_cov"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n  from numpy.linalg import det\n  from numpy.linalg import inv\n\n  # Compute the determinant of the covariance matrix\n  det_cov = det(cov)\n\n  # Check if the determinant is negative\n  if det_cov < 0:\n    raise ValueError('The determinant of the covariance matrix is negative.')\n\n  # Compute the isotropic covariance matrix\n  if mode == 'fast':\n    # Compute the isotropic covariance matrix using the determinant directly\n    isotropic_cov = np.sqrt(det_cov) * np.eye(cov.shape[0])\n  elif mode == 'accurate':\n    # Compute the isotropic covariance matrix using the logarithm of the determinant for stability\n    isotropic_cov = np.sqrt(det_cov) * inv(cov)\n  else:\n    raise ValueError('Invalid mode. Please use either \"fast\" or \"accurate\".')\n\n  return isotropic_cov"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n  from numpy.linalg import det, slogdet\n\n  # Check if input is a single covariance matrix or a list of covariance matrices\n  if cov.ndim == 2:\n    cov = [cov]\n\n  # Compute the determinant of the covariance matrix or matrices\n  if mode == 'fast':\n    det_cov = det(cov)\n  elif mode == 'accurate':\n    det_cov = slogdet(cov)[1]\n\n  # Compute the isotropic covariance matrix or matrices\n  cov_iso = []\n  for i in range(len(cov)):\n    if det_cov > 0:\n      cov_iso.append(np.sqrt(det_cov) * cov[i] / np.trace(cov[i]) * np.trace(cov[i]) / det_cov)\n    else:\n      cov_iso.append(np.zeros_like(cov[i]))\n\n  # Return the isotropic covariance matrix or matrices\n  if len(cov_iso) == 1:\n    return cov_iso[0]\n  else:\n    return cov_iso"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  if cov.ndim == 2:\n    cov = cov[None, :, :]\n\n  if mode == 'accurate':\n    det = np.linalg.det(cov)\n    det_sqrt = np.sqrt(det)\n    cov_isotropic = det_sqrt * np.linalg.inv(det_sqrt * cov)\n\n  elif mode == 'fast':\n    det = np.linalg.det(cov)\n    cov_isotropic = cov / det\n\n  else:\n    raise ValueError('Mode must be either \"fast\" or \"accurate\".')\n\n  if np.any(np.isnan(cov_isotropic)):\n    raise ValueError('Invalid determinant or logarithm of determinant.')\n\n  return cov_isotropic"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    isotropic_cov = cov * np.sqrt(np.linalg.det(cov))\n  elif mode == 'accurate':\n    isotropic_cov = cov * np.exp(0.5 * np.log(np.linalg.det(cov)))\n  else:\n    raise ValueError('Invalid mode')\n\n  return isotropic_cov\n\n"}
{"namespace": "coord.isotropize", "completion": "  # Check if the input is a matrix or a batch of matrices\n  if len(cov.shape) == 2:\n    # If it's a matrix, compute the determinant\n    det = np.linalg.det(cov)\n  elif len(cov.shape) == 3:\n    # If it's a batch of matrices, compute the determinant for each matrix\n    det = np.linalg.det(cov)\n  else:\n    raise ValueError('Input must be a matrix or a batch of matrices.')\n\n  # Check if the determinant is valid\n  if np.any(det <= 0):\n    raise ValueError('Determinant of covariance matrix is not positive.')\n\n  # Compute the isotropic covariance matrices\n  if mode == 'fast':\n    # If the mode is 'fast', use the determinant directly\n    cov_isotropic = np.sqrt(det) * np.eye(cov.shape[0])\n  elif mode == 'accurate':\n    # If the mode is 'accurate', use the logarithm of the determinant for stability\n    cov_isotropic = np.sqrt(np.exp(np.log(det) / cov.shape[0])) * np.eye(cov.shape[0])\n  else:\n    raise ValueError('Invalid mode. Must be either \"fast\" or \"accurate\".')\n\n  return cov_isotropic"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  # Check if the input is a list of covariance matrices\n  if isinstance(cov, list):\n    cov_list = cov\n  else:\n    cov_list = [cov]\n\n  # Compute the isotropic covariance matrices in the specified mode\n  if mode == 'fast':\n    isotropic_cov_list = [np.diag(np.diag(cov)) for cov in cov_list]\n  elif mode == 'accurate':\n    isotropic_cov_list = [np.diag(np.diag(np.exp(0.5 * np.diag(cov)))) for cov in cov_list]\n  else:\n    raise ValueError(\"Invalid mode. Choose either 'fast' or 'accurate'.\")\n\n  # Check if the input is a list of covariance matrices\n  if isinstance(cov, list):\n    return isotropic_cov_list\n  else:\n    return isotropic_cov_list[0]\n\n\n"}
{"namespace": "coord.isotropize", "completion": "  # Check if the input is a matrix or a list of matrices\n  if cov.ndim == 2:\n    cov = [cov]\n  else:\n    cov = cov.tolist()\n\n  # Compute the determinant of the covariance matrix or matrices\n  dets = np.array([np.linalg.det(c) for c in cov])\n\n  # Check if the determinant is valid\n  if not np.all(dets > 0):\n    raise ValueError('Invalid determinant for covariance matrix or matrices')\n\n  # Compute the isotropic covariance matrix or matrices\n  if mode == 'fast':\n    isotropic_cov = [c / np.sqrt(det) for c, det in zip(cov, dets)]\n  elif mode == 'accurate':\n    isotropic_cov = [c / np.sqrt(np.log(det)) for c, det in zip(cov, dets)]\n  else:\n    raise ValueError('Invalid mode for computing isotropic covariance matrix or matrices')\n\n  return isotropic_cov"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  # Check if the input is a matrix or a list of matrices\n  if cov.ndim == 2:\n    # If it is a matrix, compute the isotropic covariance matrix\n    cov_isotropic = cov * np.trace(cov) / np.linalg.det(cov)\n  else:\n    # If it is a list of matrices, compute the isotropic covariance matrices\n    cov_isotropic = []\n    for c in cov:\n      # Compute the isotropic covariance matrix for each matrix in the list\n      cov_isotropic.append(c * np.trace(c) / np.linalg.det(c))\n    cov_isotropic = np.array(cov_isotropic)\n\n  # Check if the mode is 'fast'\n  if mode == 'fast':\n    # If the mode is 'fast', use the determinant directly\n    cov_isotropic = cov_isotropic * np.linalg.det(cov)\n  else:\n    # If the mode is 'accurate', use the logarithm of the determinant for stability\n    cov_isotropic = cov_isotropic * np.exp(np.log(np.linalg.det(cov)) / cov.shape[0])\n\n  # Check if the computed isotropic covariance matrix is positive semi-definite\n  if np.all(np.linalg.eigvals(cov_isotropic) >= 0):\n    # If the matrix is positive semi-definite, return it\n    return cov_isotropic\n  else:\n    # If the matrix is not positive semi-definite, return a matrix with the same determinant as the input matrix\n    return cov * np.linalg.det(cov) / np.linalg.det(cov_isotropic)"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Run a task.\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Execute a task.\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Parse command line arguments for the task execution.\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"A tool for managing and executing tasks.\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts,"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Execute a task.')\n\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode, which can be \"auto\" or \"manual\", specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, spec"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command line interface for the task execution system.')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times',"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Run a task')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode, which can be \"auto\" or \"manual\", specifying how the task should be executed')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, specifying how many times"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser()\n\n    # Input-Output Arguments\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='A tool for executing tasks and managing their execution.')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Run a task')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='List of files to upload, allowing multiple files to be specified')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode, which can be \"auto\" or \"manual\", specifying how the task should be executed')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, specifying how many times a task can be retried upon failure')"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Task execution arguments.')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Run the task')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode, which can be \"auto\" or \"manual\", specifying how the task should be executed')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, specifying how many times a task can be retried upon failure')"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Run a task.\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"Files to upload.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts.\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file.\")\n    args = parser.parse_args()\n    return args"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"--task\", type=str, required=True)\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", default=[])\n    parser.add_argument(\"--model\", type=str, default=None)\n    parser.add_argument(\"--record-dir\", type=str, default=None)\n    parser.add_argument(\"--mode\", type=str, default=\"auto\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", default=False)\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, default=10)\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", default=False)\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, default=10)\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, default=10)\n    parser.add_argument(\"--max-plan-tree-width\", type=int, default=10)\n    parser.add_argument(\"--max-retry-times\", type=int, default=3)\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"))\n\n    args = parser.parse_args()\n\n    return args"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Execute tasks.')\n    parser.add_argument('--task', type=str, required=True, help='Task description.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='Files to upload.')\n    parser.add_argument('--model', type=str, help='Model identifier.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs.')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode.')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts.')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file.')\n\n    args = parser.parse_args()\n\n    return args"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Run a task')\n    parser.add_argument('--task', type=str, required=True, help='The task description')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier for the task')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode, can be auto or manual')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n    args = parser.parse_args()\n    return args"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(\n        description=\"A command-line interface for running tasks on the Taskmaster platform.\")\n\n    parser.add_argument(\"--task\", type=str, required=True,\n                        help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\",\n                        help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str,\n                        help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str,\n                        help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\",\n                        choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\",\n                        help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int,\n                        help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\",\n                        help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int,\n                        help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int,\n                        help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int,\n                        help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"A tool for executing tasks.\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, can be 'auto' or 'manual'\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode with minimal output\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance during task execution\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts\")\n    parser.add_argument(\"--config-file\", type=str, default=os.getenv(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file\")\n\n    return parser.parse_args()"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"A program for executing tasks in the context of a task execution chain.\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Execute a task using the provided arguments.\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual'\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode with minimal output\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance during task execution\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file\")\n\n    return parser.parse_args()"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points\")\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points\")\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points.\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points.\")\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if not v.shape[1] == 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points, but got shape {v.shape}.\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points, but has shape {v.shape}\")\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points (shape (_, 2))\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points, but {v.shape} was given.\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points, but shape is {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points, but has shape {v.shape}.\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points, but shape {v.shape} was given.\")\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points, but it has shape {v.shape}\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if not v.shape[1] == 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must have a shape of (_, 2)\")\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if not isinstance(v, np.ndarray):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a numpy array, not {type(v)}\")\n    if v.shape[-1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must have shape (_, 2), not {v.shape}\")\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points\")\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points.\")\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n    # Calculate the number of characters in the character set\n    char_set_len = len(char_set)\n    # Encode the integer into a single character\n    encoded_char = char_set[n % char_set_len]\n    return encoded_char\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = '_abcdefghijklmnopqrstuvwxyz0123456789'\n\n    # Calculate the number of digits in the integer\n    num_digits = len(str(n))\n\n    # Calculate the index of the character in the character set\n    char_index = n % len(char_set)\n\n    # Return the encoded character\n    return char_set[char_index]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    character_set = \"_abcdefghijklmnopqrstuvwxyz0123456789\"\n\n    # Encode the integer into a single character\n    encoded_char = character_set[n]\n\n    return encoded_char\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = \"_abcdefghijklmnopqrstuvwxyz0123456789\"\n\n    # Calculate the length of the character set\n    char_set_len = len(char_set)\n\n    # Initialize an empty string to store the encoded character\n    encoded_char = \"\"\n\n    # Encode the integer using the character set\n    while n > 0:\n        encoded_char = char_set[n % char_set_len] + encoded_char\n        n //= char_set_len\n\n    # Return the encoded character\n    return encoded_char\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    characters = \"_abcdefghijklmnopqrstuvwxyz0123456789\"\n\n    # Calculate the number of digits in the integer\n    digits = len(str(n))\n\n    # Calculate the index of the character in the character set\n    index = n % (len(characters) ** digits)\n\n    # Convert the index to a string and pad it with leading zeros\n    index_str = str(index).zfill(digits)\n\n    # Convert the index string to a list of integers\n    index_list = [int(c) for c in index_str]\n\n    # Encode the integer using the character set\n    encoded_char = \"\".join([characters[i] for i in index_list])\n\n    return encoded_char\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    characters = \"_abcdefghijklmnopqrstuvwxyz0123456789\"\n\n    # Calculate the number of characters in the character set\n    num_characters = len(characters)\n\n    # Initialize the encoded string\n    encoded_string = \"\"\n\n    # Encode the integer into a single character\n    while n > 0:\n        # Get the remainder of the integer division by the number of characters\n        remainder = n % num_characters\n\n        # Add the corresponding character to the encoded string\n        encoded_string = characters[remainder] + encoded_string\n\n        # Divide the integer by the number of characters\n        n //= num_characters\n\n    # Return the encoded string\n    return encoded_string\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = \"_abcdefghijklmnopqrstuvwxyz0123456789\"\n\n    # Check if the input integer is within the range of the character set\n    if n >= len(char_set):\n        raise ValueError(\"Input integer is too large for the character set\")\n\n    # Encode the integer into a single character\n    encoded_char = char_set[n]\n\n    return encoded_char\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    if n < 0:\n        raise ValueError(\"Input integer must be non-negative.\")\n    if n == 0:\n        return \"_\"\n    char_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n    encoded_char = \"\"\n    while n > 0:\n        encoded_char = char_set[n % 36] + encoded_char\n        n //= 36\n    return encoded_char\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = '_abcdefghijklmnopqrstuvwxyz0123456789'\n    # Calculate the number of characters in the character set\n    char_set_len = len(char_set)\n    # Initialize the encoded string\n    encoded_str = ''\n    # Encode the integer into a single character\n    while n > 0:\n        # Get the remainder of n divided by the number of characters in the character set\n        remainder = n % char_set_len\n        # Append the corresponding character to the encoded string\n        encoded_str = char_set[remainder] + encoded_str\n        # Update n by dividing it by the number of characters in the character set\n        n = n // char_set_len\n    # Return the encoded string\n    return encoded_str\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    character_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n\n    # Calculate the number of characters in the character set\n    num_chars = len(character_set)\n\n    # Calculate the number of digits in the integer\n    num_digits = len(str(n))\n\n    # Calculate the base of the integer representation\n    base = num_chars ** num_digits\n\n    # Calculate the index of the character in the character set\n    index = n % base\n\n    # Select the character from the character set\n    char = character_set[index]\n\n    # Return the encoded character\n    return char\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = '_abcdefghijklmnopqrstuvwxyz0123456789'\n\n    # Check if the input integer is within the range of the character set\n    if n < 0 or n >= len(char_set):\n        raise ValueError(\"Input integer is out of range for character set.\")\n\n    # Encode the integer using the character set\n    return char_set[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = 'abcdefghijklmnopqrstuvwxyz0123456789_'\n\n    # Calculate the number of digits in the integer\n    num_digits = len(str(n))\n\n    # Calculate the index of the character in the character set\n    char_index = n % len(char_set)\n\n    # Encode the integer into a single character\n    encoded_char = char_set[char_index]\n\n    # Return the encoded character\n    return encoded_char\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n\n    # Calculate the length of the character set\n    char_set_len = len(char_set)\n\n    # Initialize the encoded string\n    encoded_str = \"\"\n\n    # Encode the integer into a single character\n    while n > 0:\n        # Calculate the remainder when n is divided by the length of the character set\n        remainder = n % char_set_len\n\n        # Append the character corresponding to the remainder to the encoded string\n        encoded_str += char_set[remainder]\n\n        # Divide n by the length of the character set and round down to the nearest integer\n        n = n // char_set_len\n\n    # Reverse the encoded string\n    encoded_str = encoded_str[::-1]\n\n    return encoded_str\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    chars = \"_abcdefghijklmnopqrstuvwxyz0123456789\"\n\n    # Convert the integer to a string and pad it with leading zeros to make it 3 characters long\n    n = str(n).zfill(3)\n\n    # Initialize an empty string to store the encoded character\n    encoded_char = \"\"\n\n    # Loop through each character in the input string\n    for i in range(3):\n        # Get the character at the current index in the character set\n        char = chars[int(n[i])]\n\n        # Append the character to the encoded character string\n        encoded_char += char\n\n    # Return the encoded character\n    return encoded_char\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    chars = \"_abcdefghijklmnopqrstuvwxyz0123456789\"\n    # Calculate the number of characters in the character set\n    num_chars = len(chars)\n    # Calculate the number of digits in the integer\n    num_digits = len(str(n))\n    # Calculate the base for the integer representation\n    base = num_chars ** (num_digits - 1)\n    # Calculate the index of the character in the character set\n    index = n // base\n    # Return the encoded character\n    return chars[index]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    char_set = \"_abcdefghijklmnopqrstuvwxyz0123456789\"\n\n    # Check if the input integer is within the valid range\n    if n < 0 or n >= len(char_set):\n        raise ValueError(\"Input integer is out of range\")\n\n    # Convert the integer to a string and return the corresponding character from the character set\n    return char_set[n]\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = '_abcdefghijklmnopqrstuvwxyz0123456789'\n\n    # Convert the integer to a string and reverse it\n    s = str(n)[::-1]\n\n    # Initialize the encoded string\n    encoded_str = ''\n\n    # Loop through each digit of the reversed integer\n    for i, c in enumerate(s):\n        # Calculate the index of the character in the character set\n        index = int(c) + 26 * i\n\n        # Append the character to the encoded string\n        encoded_str += char_set[index]\n\n    # Return the encoded string\n    return encoded_str\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = 'abcdefghijklmnopqrstuvwxyz0123456789_'\n\n    # Calculate the number of characters in the character set\n    char_set_len = len(char_set)\n\n    # Calculate the number of digits in the integer\n    num_digits = len(str(n))\n\n    # Calculate the maximum possible value for the integer based on the number of digits\n    max_val = char_set_len ** num_digits\n\n    # Check if the input integer is within the range of the character set\n    if n < 0 or n >= max_val:\n        raise ValueError(f\"Integer {n} is out of range for encoding\")\n\n    # Convert the integer to a string and pad it with leading zeros if necessary\n    n_str = str(n).zfill(num_digits)\n\n    # Initialize the encoded string\n    encoded_str = ''\n\n    # Iterate through the digits of the integer in reverse order\n    for i in range(num_digits - 1, -1, -1):\n        # Calculate the index of the character to be added to the encoded string\n        char_idx = int(n_str[i])\n\n        # Add the character to the encoded string\n        encoded_str = char_set[char_idx] + encoded_str\n\n    # Return the encoded string\n    return encoded_str\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = '_abcdefghijklmnopqrstuvwxyz0123456789'\n\n    # Calculate the number of characters in the character set\n    num_chars = len(char_set)\n\n    # Calculate the number of digits in the integer\n    num_digits = len(str(n))\n\n    # Calculate the index of the character in the character set\n    index = n % num_chars\n\n    # Calculate the number of times the integer needs to be divided by the number of characters\n    num_divisions = num_digits // num_chars\n\n    # Calculate the remainder of the integer divided by the number of characters\n    remainder = num_digits % num_chars\n\n    # If there is a remainder, add an additional character to the index\n    if remainder > 0:\n        index += 1\n\n    # If there are any divisions, add the appropriate number of underscores to the character\n    char = char_set[index]\n    if num_divisions > 0:\n        char = '_' * num_divisions + char\n\n    # Return the encoded character\n    return char\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    characters = \"_abcdefghijklmnopqrstuvwxyz0123456789\"\n\n    # Calculate the number of digits in the integer\n    num_digits = len(str(n))\n\n    # Calculate the number of characters in the character set\n    num_characters = len(characters)\n\n    # Calculate the number of characters needed to represent the integer\n    num_needed = num_digits * num_characters\n\n    # Calculate the number of times the integer needs to be divided by the number of characters\n    num_divisions = num_needed // num_characters\n\n    # Calculate the remainder of the division\n    remainder = num_needed % num_characters\n\n    # Calculate the integer part of the division\n    int_part = n // (num_characters ** num_divisions)\n\n    # Calculate the remainder of the division\n    remainder_part = n % (num_characters ** num_divisions)\n\n    # Encode the integer\n    encoded = characters[int_part]\n\n    # Encode the remainder\n    for i in range(num_divisions):\n        encoded += characters[remainder_part // (num_characters ** (num_divisions - i - 1))]\n        remainder_part %= num_characters ** (num_divisions - i - 1)\n\n    # Return the encoded character\n    return encoded\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, value_at_zero))"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, value_at_zero))"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, value_at_zero))"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, value_at_zero))"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, value_at_zero))"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, eps))"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, eps))"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.clip(x, a_min = eps, a_max = None))"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.clip(x, a_min = eps, a_max = None))"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, value_at_zero))"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, value_at_zero))"}
{"namespace": "spin_math.safe_log", "completion": "  x = jnp.where(x < eps, value_at_zero, x)\n  return jnp.log(x)"}
{"namespace": "spin_math.safe_log", "completion": "  x = jnp.clip(x, eps, None)\n  return jnp.log(x)"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.clip(x, a_min=eps, a_max=None))"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, eps))\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, eps))\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, eps))\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.clip(x, a_min = eps, a_max = None))\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, value_at_zero))\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, value_at_zero))\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker_index, intervals in workers_intervals.items():\n        for interval in intervals:\n            indexes[worker_index] += 1\n            if indexes[worker_index] == interval[1]:\n                indexes[worker_index] = 0\n\n    return indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    current_indexes = {}\n\n    for worker_index, intervals in workers_intervals.items():\n        current_index = indexes[worker_index]\n        chunk_index = indexes[worker_index] // len(intervals)\n\n        chunk_indexes[worker_index] = chunk_index\n        current_indexes[worker_index] = current_index % len(intervals)\n\n    return chunk_indexes, current_indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    indexes_ = {}\n    for worker_id, intervals in workers_intervals.items():\n        if len(intervals) == 0:\n            continue\n        chunk_indexes[worker_id] = indexes[worker_id] // len(intervals)\n        indexes_[worker_id] = indexes[worker_id] % len(intervals)\n    return chunk_indexes, indexes_\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker_id, worker_intervals in workers_intervals.items():\n        current_index = indexes[worker_id]\n        chunk_index = indexes[worker_id] // len(worker_intervals)\n        indexes[worker_id] = current_index % len(worker_intervals)\n        indexes[worker_id] = indexes[worker_id] + worker_intervals[chunk_index][0]\n    return indexes, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker_index, intervals in workers_intervals.items():\n        for interval in intervals:\n            if len(interval) > 1:\n                indexes[worker_index] += 1\n                if indexes[worker_index] >= interval[1] - interval[0]:\n                    indexes[worker_index] = 0\n\n    return indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker_index, intervals in workers_intervals.items():\n        current_index = indexes[worker_index]\n        chunk_index = indexes[worker_index] // len(intervals)\n        indexes[worker_index] = current_index % len(intervals)\n        indexes[worker_index] = indexes[worker_index] + (\n            intervals[chunk_index][1] - intervals[chunk_index][0]\n        )\n\n    return indexes, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker, intervals in workers_intervals.items():\n        for interval in intervals:\n            if interval[0] <= indexes[worker] < interval[1]:\n                chunk_indexes[worker] = indexes[worker]\n                indexes[worker] += 1\n                break\n\n    return chunk_indexes, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker, intervals in workers_intervals.items():\n        for interval in intervals:\n            indexes[worker] += 1\n            if indexes[worker] == interval[1]:\n                indexes[worker] = 0\n                workers_intervals[worker][0] += 1\n                if workers_intervals[worker][0] == len(workers_intervals[worker]):\n                    workers_intervals[worker] = []\n\n    return workers_intervals, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker_id, worker_intervals in workers_intervals.items():\n        worker_index = indexes[worker_id]\n        for interval in worker_intervals:\n            worker_index += 1\n            indexes[worker_id] = worker_index\n            if worker_index == interval[1]:\n                worker_index = 0\n                indexes[worker_id] = worker_index\n\n    return indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker, intervals in workers_intervals.items():\n        current_index = indexes[worker]\n        chunk_index = indexes[worker] // len(intervals)\n        indexes[worker] = current_index % len(intervals)\n        indexes[worker] = indexes[worker] + len(intervals[chunk_index])\n    return indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker_id, intervals in workers_intervals.items():\n        if len(intervals) > 0:\n            chunk_index = indexes[worker_id]\n            current_index = intervals[chunk_index][0]\n            while current_index >= intervals[chunk_index][1]:\n                chunk_index += 1\n                if chunk_index == len(intervals):\n                    chunk_index = 0\n                    current_index = intervals[chunk_index][0]\n                else:\n                    current_index = intervals[chunk_index][0]\n            indexes[worker_id] = chunk_index\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    # Initialize the chunk index and indexes for each worker\n    chunk_index = {worker: 0 for worker in workers_intervals}\n    new_indexes = {worker: 0 for worker in workers_intervals}\n\n    # Iterate through each worker's intervals\n    for worker, intervals in workers_intervals.items():\n        # Iterate through each interval for the current worker\n        for interval in intervals:\n            # Calculate the size of the current interval\n            size = interval[-1] - interval[0]\n\n            # Update the chunk index for the current worker\n            chunk_index[worker] = (chunk_index[worker] + 1) % len(intervals)\n\n            # Update the indexes for the current worker\n            new_indexes[worker] = (new_indexes[worker] + size) % len(interval)\n\n    # Return the updated chunk index and indexes for each worker\n    return chunk_index, new_indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker_index, intervals in workers_intervals.items():\n        if len(intervals) == 0:\n            continue\n\n        if indexes[worker_index] >= len(intervals):\n            indexes[worker_index] = 0\n\n        current_index = indexes[worker_index]\n        current_interval = intervals[current_index]\n\n        if current_interval[1] - current_interval[0] > CHUNK_SIZE:\n            indexes[worker_index] += 1\n        else:\n            indexes[worker_index] = 0\n\n    return indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker_idx, intervals in workers_intervals.items():\n        current_index = indexes[worker_idx]\n        current_chunk_index = indexes[worker_idx] // len(intervals)\n        indexes[worker_idx] = current_index % len(intervals)\n        indexes[worker_idx] = indexes[worker_idx] + len(intervals[current_chunk_index])\n\n    return indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    indexes_ = {}\n    for worker_idx in workers_intervals:\n        for interval in workers_intervals[worker_idx]:\n            if indexes[worker_idx] < interval[0]:\n                indexes_[worker_idx] = interval[0]\n                chunk_indexes[worker_idx] = interval[0]\n            elif interval[0] <= indexes[worker_idx] <= interval[1]:\n                indexes_[worker_idx] = indexes[worker_idx]\n                chunk_indexes[worker_idx] = indexes[worker_idx]\n            else:\n                indexes_[worker_idx] = interval[0]\n                chunk_indexes[worker_idx] = interval[0]\n    return chunk_indexes, indexes_"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    new_indexes = {}\n    new_chunk_indexes = {}\n\n    for worker_index, intervals in workers_intervals.items():\n        current_index = indexes[worker_index]\n        current_chunk_index = indexes[worker_index]\n        for interval in intervals:\n            if current_index >= interval[0] and current_index < interval[1]:\n                new_indexes[worker_index] = current_index\n                new_chunk_indexes[worker_index] = current_chunk_index\n            current_index += 1\n            current_chunk_index += 1\n\n    return new_chunk_indexes, new_indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker_id, intervals in workers_intervals.items():\n        for interval in intervals:\n            indexes[worker_id] += 1\n            if indexes[worker_id] >= len(interval):\n                indexes[worker_id] = 0\n                workers_intervals[worker_id].pop(0)\n                if not workers_intervals[worker_id]:\n                    del workers_intervals[worker_id]\n                    del indexes[worker_id]\n                    break\n    return workers_intervals, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_index, intervals in workers_intervals.items():\n        chunk_index = indexes[worker_index]\n        chunk_indexes[worker_index] = chunk_index\n        if chunk_index == len(intervals):\n            indexes[worker_index] = 0\n            continue\n        indexes[worker_index] += 1\n\n    return chunk_indexes, indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    # Initialize empty dictionaries to store updated chunk index and indexes\n    chunk_indexes = {}\n    indexes_chunk = {}\n\n    # Iterate through each worker's intervals\n    for worker_id, intervals in workers_intervals.items():\n\n        # Get the current index for this worker\n        current_index = indexes[worker_id]\n\n        # Initialize the chunk index for this worker\n        chunk_index = 0\n\n        # Iterate through each interval for this worker\n        for interval in intervals:\n\n            # Calculate the size of the current interval\n            interval_size = interval[1] - interval[0]\n\n            # If the current index is within the current interval\n            if current_index >= interval[0] and current_index < interval[1]:\n\n                # Calculate the chunk index for this interval\n                chunk_index = (current_index - interval[0]) // interval_size\n\n            # If the current index is greater than or equal to the end of the current interval\n            elif current_index >= interval[1]:\n\n                # Calculate the chunk index for the next interval\n                chunk_index = (current_index - interval[0]) // interval_size + 1\n\n            # If the current index is less than the start of the current interval\n            else:\n\n                # Calculate the chunk index for the previous interval\n                chunk_index = (current_index - interval[0]) // interval_size - 1\n\n        # Update the chunk index for this worker\n        chunk_indexes[worker_id] = chunk_index\n\n        # Update the indexes within the current chunk for this worker\n        indexes_chunk[worker_id] = (\n            current_index - (chunk_index * intervals[0][1] - intervals[0][0])\n        )\n\n    # Return the updated chunk index and indexes\n    return chunk_indexes, indexes_chunk"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_indexes = dict()\n    for worker_index, worker_intervals in workers_intervals.items():\n        worker_indexes = indexes[worker_index]\n        if worker_indexes < len(worker_intervals):\n            chunk_index = worker_indexes\n        else:\n            chunk_index = worker_indexes % len(worker_intervals)\n        chunks_indexes[worker_index] = chunk_index\n        indexes[worker_index] = worker_indexes + 1\n\n    return chunks_indexes, indexes\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError('Invalid datastructure.')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f\"Unsupported datastructure: {datastructure}\")"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError('Unsupported datastructure')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust the coordinates to be within the grid bounds\n    coordinates = np.clip(coordinates, 0, 1)\n\n    # Perform trilinear interpolation\n    x, y, z = coordinates.T\n    x0, x1 = np.floor(x).astype(int), np.ceil(x).astype(int)\n    y0, y1 = np.floor(y).astype(int), np.ceil(y).astype(int)\n    z0, z1 = np.floor(z).astype(int), np.ceil(z).astype(int)\n\n    x0 = np.clip(x0, 0, values.shape[0] - 1)\n    x1 = np.clip(x1, 0, values.shape[0] - 1)\n    y0 = np.clip(y0, 0, values.shape[1] - 1)\n    y1 = np.clip(y1, 0, values.shape[1] - 1)\n    z0 = np.clip(z0, 0, values.shape[2] - 1)\n    z1 = np.clip(z1, 0, values.shape[2] - 1)\n\n    v000 = values[x0, y0, z0]\n    v001 = values[x0, y0, z1]\n    v010 = values[x0, y1, z0]\n    v011 = values[x0, y1, z1]\n    v100 = values[x1, y0, z0]\n    v101 = values[x1, y0, z1]\n    v110 = values[x1, y1, z0]\n    v111 = values[x1, y1, z1]\n\n    w00 = (x1 - x) * (y1 - y) * (z1 - z)\n    w01 = (x1 - x) * (y1 - y) * (z - z"}
{"namespace": "grid_utils.trilerp", "completion": "  import numpy as np\n\n  if datastructure == 'grid':\n    # Adjust the coordinates to be within the bounds of the grid\n    coordinates = np.clip(coordinates, 0, np.array(values.shape[:3]) - 1)\n\n    # Perform trilinear interpolation on the grid\n    x, y, z = coordinates[..., 0], coordinates[..., 1], coordinates[..., 2]\n    x0, y0, z0 = np.floor(x).astype(int), np.floor(y).astype(int), np.floor(z).astype(int)\n    x1, y1, z1 = x0 + 1, y0 + 1, z0 + 1\n    w000 = (x1 - x) * (y1 - y) * (z1 - z)\n    w001 = (x1 - x) * (y1 - y) * (z - z0)\n    w010 = (x1 - x) * (y - y0) * (z1 - z)\n    w011 = (x1 - x) * (y - y0) * (z - z0)\n    w100 = (x - x0) * (y1 - y) * (z1 - z)\n    w101 = (x - x0) * (y1 - y) * (z - z0)\n    w110 = (x - x0) * (y - y0) * (z1 - z)\n    w111 = (x - x0) * (y - y0) * (z - z0)\n    interpolated = w000 * values[x0, y0, z0] + w001 * values[x0, y0, z1] + w010 * values[x0, y1, z0] + w011 * values[x0, y1, z1] + w100 * values[x1, y0, z0] + w101 * values[x1, y0, z1] + w11"}
{"namespace": "grid_utils.trilerp", "completion": "  # TODO: Implement trilinear interpolation for 3D voxel grid\n  # TODO: Implement trilinear interpolation for hash data structure\n\n  if datastructure == 'grid':\n    # TODO: Implement trilinear interpolation for 3D voxel grid\n    pass\n  elif datastructure == 'hash':\n    # TODO: Implement trilinear interpolation for hash data structure\n    pass\n  else:\n    raise ValueError(\n        f'Invalid datastructure {datastructure}. Only \"grid\" and \"hash\" are supported.')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == \"grid\":\n    return trilerp_grid(values, coordinates)\n  elif datastructure == \"hash\":\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(\"Invalid datastructure. Must be 'grid' or 'hash'\")\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f\"Invalid datastructure: {datastructure}. Expected 'grid' or 'hash'.\")\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f'Datastructure {datastructure} not supported.')\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilinear_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilinear_hash(values, coordinates)\n  else:\n    raise ValueError('Invalid datastructure')\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f'Unsupported datastructure: {datastructure}')\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(\"Invalid datastructure. Must be 'grid' or 'hash'.\")\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust coordinates to be within the bounds of the voxel grid\n    coordinates = np.clip(coordinates, 0, np.array(values.shape[:-1]) - 1)\n\n    # Get the integer coordinates\n    coordinates_int = coordinates.astype(int)\n\n    # Compute the fractional coordinates\n    coordinates_frac = coordinates - coordinates_int\n\n    # Compute the interpolated values using trilinear interpolation\n    values_interp = np.zeros(coordinates.shape[:-1] + (values.shape[-1],))\n    for i in range(coordinates.shape[-1]):\n      # Get the values at the integer coordinates\n      values_int = values[coordinates_int[:, 0], coordinates_int[:, 1], coordinates_int[:, 2], i]\n\n      # Compute the interpolated values using trilinear interpolation\n      values_interp[:, i] = (1 - coordinates_frac[:, 0]) * (1 - coordinates_frac[:, 1]) * (1 - coordinates_frac[:, 2]) * values_int[..., 0] + \\\n                            (1 - coordinates_frac[:, 0]) * (1 - coordinates_frac[:, 1]) * coordinates_frac[:, 2] * values_int[..., 1] + \\\n                            (1 - coordinates_frac[:, 0]) * coordinates_frac[:, 1] * (1 - coordinates_frac[:, 2]) * values_int[..., 2] + \\\n                            (1 - coordinates_frac[:, 0]) * coordinates_frac[:, 1] * coordinates_frac[:, 2] * values_int[..., 3] + \\\n                            coordinates_frac[:, 0] * (1 - coordinates_frac[:, 1]) * (1 - coordinates_frac[:, 2]) * values_int[..., 4] + \\\n                            coordinates_frac[:, 0] * (1 - coordinates_frac[:, 1]) * coordinates_frac[:, 2] * values_int[..., 5"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust coordinates to be within the bounds of the grid\n    coordinates = np.clip(coordinates, 0, 1)\n\n    # Sample from the grid\n    values = trilinear_interpolation(values, coordinates)\n\n  elif datastructure == 'hash':\n    # Adjust coordinates to be within the bounds of the hash\n    coordinates = np.clip(coordinates, 0, 1)\n\n    # Sample from the hash\n    values = trilinear_interpolation_hash(values, coordinates)\n\n  else:\n    raise ValueError(\"Invalid datastructure. Only 'grid' or 'hash' are supported.\")\n\n  return values\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  import numpy as np\n\n  if datastructure == 'grid':\n\n    # Adjust coordinates to be within the grid bounds\n    coordinates = np.clip(coordinates, 0, 1)\n\n    # Get dimensions of the grid\n    dims = values.shape[:-1]\n\n    # Compute the fractional coordinates\n    frac_coords = coordinates * np.array(dims)\n\n    # Compute the integer coordinates\n    int_coords = np.floor(frac_coords).astype(np.int32)\n\n    # Compute the fractional offsets\n    frac_offsets = frac_coords - int_coords\n\n    # Compute the lower and upper bounds for each dimension\n    lower_bounds = np.clip(int_coords, 0, dims - 1)\n    upper_bounds = np.clip(int_coords + 1, 0, dims - 1)\n\n    # Compute the weights for each dimension\n    weights = np.zeros(values.shape)\n    weights[lower_bounds[..., 0], lower_bounds[..., 1], lower_bounds[..., 2], :] = 1 - frac_offsets[..., 0]\n    weights[upper_bounds[..., 0], lower_bounds[..., 1], lower_bounds[..., 2], :] = frac_offsets[..., 0]\n    weights[lower_bounds[..., 0], upper_bounds[..., 1], lower_bounds[..., 2], :] = 1 - frac_offsets[..., 1]\n    weights[upper_bounds[..., 0], upper_bounds[..., 1], lower_bounds[..., 2], :] = frac_offsets[..., 1]\n    weights[lower_bounds[..., 0], lower_bounds[..., 1], upper_bounds[..., 2], :] = 1 - frac_offsets[..., 2]\n    weights[upper"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError('Invalid datastructure.')\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  # Adjust coordinates to be within the bounds of the dimensions they refer to\n  coordinates = np.clip(coordinates, 0, np.array(values.shape[:3]) - 1)\n\n  # Check if the datastructure is valid\n  if datastructure not in ['grid', 'hash']:\n    raise ValueError(f\"Invalid datastructure: {datastructure}\")\n\n  # Perform trilinear interpolation based on the datastructure type\n  if datastructure == 'grid':\n    # Convert coordinates to integers\n    coordinates = np.floor(coordinates).astype(np.int32)\n\n    # Extract the values at the specified coordinates\n    values = values[coordinates[:, 0], coordinates[:, 1], coordinates[:, 2], :]\n\n    # Compute the fractional part of the coordinates\n    fractional_coords = coordinates - np.floor(coordinates)\n\n    # Perform trilinear interpolation\n    v000 = values[:, 0]\n    v001 = values[:, 1]\n    v010 = values[:, 2]\n    v011 = values[:, 3]\n    v100 = values[:, 4]\n    v101 = values[:, 5]\n    v110 = values[:, 6]\n    v111 = values[:, 7]\n\n    # Compute the interpolated values\n    interpolated_values = (1 - fractional_coords[:, 0]) * (1 - fractional_coords[:, 1]) * (1 - fractional_coords[:, 2]) * v000 + \\\n                          (1 - fractional_coords[:, 0]) * (1 - fractional_coords[:, 1]) * fractional_coords[:, 2] * v001 + \\\n                          (1 - fractional_coords[:, 0]) * fractional_coords[:, 1] * (1 - fractional_coords[:, 2]) * v010 + \\\n                          (1 - fractional_coords[:, 0"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f'Unknown datastructure: {datastructure}')\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f\"Unsupported datastructure: {datastructure}\")\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n\n    # Adjust coordinates to be within the bounds of the voxel grid\n    coordinates = np.clip(coordinates, 0, 1)\n\n    # Calculate the integer coordinates\n    int_coordinates = np.floor(coordinates * np.array(values.shape[:-1]))\n\n    # Calculate the fractional coordinates\n    frac_coordinates = coordinates - int_coordinates / np.array(values.shape[:-1])\n\n    # Calculate the indices for the 8 corners of the voxel grid\n    indices = np.stack([\n        int_coordinates[:, 0],\n        int_coordinates[:, 1],\n        int_coordinates[:, 2],\n        np.zeros(int_coordinates.shape[0], dtype=np.int32),\n    ], axis=-1)\n\n    # Calculate the weights for each corner\n    weights = np.zeros(indices.shape)\n    for i in range(8):\n        weights[i] = np.prod(1 - np.abs(frac_coordinates - indices[i] / np.array(values.shape[:-1])))\n\n    # Perform trilinear interpolation\n    result = np.sum(values[tuple(indices.T)] * weights, axis=0)\n\n    return result\n\n  elif datastructure == 'hash':\n\n    # Adjust coordinates to be within the bounds of the hash data structure\n    coordinates = np.clip(coordinates, 0, 1)\n\n    # Calculate the integer coordinates\n    int_coordinates = np.floor(coordinates * np.array(values.shape[:-1]))\n\n    # Calculate the fractional coordinates\n    frac_coordinates = coordinates - int_coordinates / np.array(values.shape[:-1])\n\n    # Calculate the indices for the 8 corners of the hash data structure\n    indices = np.stack([\n        int_coordinates[:, 0],\n        int_coordinates[:, 1],\n        int_coordinates[:, 2],\n        np."}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the tessellation factor is valid\n  if v < 1:\n    raise ValueError(\"Tessellation factor must be greater than or equal to 1\")\n\n  # Compute the barycentric weights for each point in the tessellated triangle\n  weights = []\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      weights.append([i, j, v - i - j])\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = np.array(weights)\n  weights = weights / np.sum(weights, axis=1)[:, np.newaxis]\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the tessellation factor is valid\n  if v < 1:\n    raise ValueError(\"Tessellation factor must be greater than or equal to 1.\")\n\n  # Compute the barycentric weights for each vertex of the triangle\n  weights = np.zeros((3, v, v))\n  for i in range(v):\n    for j in range(v):\n      weights[0, i, j] = (v - i) * (v - j) / v**2\n      weights[1, i, j] = (i) * (v - j) / v**2\n      weights[2, i, j] = (i) * (j) / v**2\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / np.sum(weights, axis=0)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the input is valid\n  if v < 1:\n    raise ValueError(\"Tessellation factor must be greater than or equal to 1.\")\n\n  # Generate the weights for each vertex of the triangle\n  weights = []\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      weights.append((i, j, v - i - j))\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = np.array(weights)\n  weights = weights / np.sum(weights, axis=1, keepdims=True)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the input is valid\n  if v < 1:\n    raise ValueError(\"The tessellation factor must be greater than or equal to 1.\")\n\n  # Compute the barycentric weights for each point in the tessellated triangle\n  weights = np.zeros((3, v + 1, v + 1))\n  for i in range(v + 1):\n    for j in range(v + 1):\n      if i + j <= v:\n        weights[0, i, j] = (v - i) * (v - j) / v\n        weights[1, i, j] = i * (v - j) / v\n        weights[2, i, j] = i * j / v\n\n  # Normalize the weights to get the barycentric coordinates\n  weights /= np.sum(weights, axis=0)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the tessellation factor is valid\n  if v < 1:\n    raise ValueError(\"Tessellation factor must be greater than or equal to 1\")\n\n  # Compute the barycentric weights for each vertex of the triangle\n  weights = np.zeros((3, v, v))\n  for i in range(v):\n    for j in range(v):\n      weights[0, i, j] = (v - i) * (v - j) / v**2\n      weights[1, i, j] = (v - i) * j / v**2\n      weights[2, i, j] = i * (v - j) / v**2\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / np.sum(weights, axis=0)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the tessellation factor is valid\n  if v < 1:\n    raise ValueError('The tessellation factor must be greater than or equal to 1.')\n\n  # Compute the barycentric weights\n  weights = np.zeros((v+1, v+1))\n  for i in range(v+1):\n    for j in range(v+1):\n      if i + j <= v:\n        weights[i, j] = 1\n  weights /= np.sum(weights)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the tessellation factor is greater than or equal to 1\n  if v < 1:\n    raise ValueError('Tessellation factor must be greater than or equal to 1.')\n\n  # Compute the barycentric weights for each point in the tessellated triangle\n  weights = np.zeros((v + 1, v + 1))\n  for i in range(v + 1):\n    for j in range(v + 1):\n      if i + j <= v:\n        weights[i, j] = (v - i - j + 1) / v\n\n  # Normalize the weights to get the barycentric coordinates\n  weights /= np.sum(weights)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check that the tessellation factor is greater than or equal to 1\n  if v < 1:\n    raise ValueError(\"The tessellation factor must be greater than or equal to 1.\")\n\n  # Generate the integer weights for each vertex of the triangle\n  weights = np.array([[v, v, v], [0, v, 0], [0, 0, v]])\n\n  # Normalize the weights to get the barycentric coordinates\n  weights /= np.sum(weights, axis=1)[:, None]\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # check input\n  if v < 1:\n    raise ValueError(\"The tessellation factor must be greater than or equal to 1.\")\n\n  # compute weights\n  weights = np.zeros((v+1, v+1))\n  for i in range(v+1):\n    for j in range(v+1):\n      weights[i, j] = np.gcd(i, v) * np.gcd(j, v)\n  weights /= np.sum(weights)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the tessellation factor is valid\n  if v < 1:\n    raise ValueError(\"Tessellation factor must be greater than or equal to 1.\")\n\n  # Compute the barycentric weights for the vertices of the triangle\n  weights = np.zeros((3, v, v))\n  for i in range(3):\n    for j in range(v):\n      for k in range(v):\n        weights[i, j, k] = (v - j) * (v - k) / (v * v) + (j * (v - k) / (v * v)) + (k * (v - j) / (v * v))\n\n  # Normalize the weights to get the barycentric coordinates\n  weights /= np.sum(weights, axis=0)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check that the tessellation factor is valid.\n  if v < 1:\n    raise ValueError(\"The tessellation factor must be greater than or equal to 1.\")\n\n  # Compute the barycentric weights for each point in the tessellated triangle.\n  weights = np.zeros((3 * v, 3 * v, 3))\n  for i in range(3 * v):\n    for j in range(3 * v):\n      weights[i, j, 0] = (i + 1) / (v * 3)\n      weights[i, j, 1] = (j + 1) / (v * 3)\n      weights[i, j, 2] = 1 - weights[i, j, 0] - weights[i, j, 1]\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights /= np.sum(weights, axis=-1, keepdims=True)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check the input\n  assert v >= 1, \"The tessellation factor must be greater than or equal to 1.\"\n\n  # Compute the barycentric weights for each point in the tessellated triangle\n  weights = np.zeros((v + 1) ** 2, 3)\n  for i in range(v + 1):\n    for j in range(v + 1):\n      weights[i * (v + 1) + j] = [i / v, j / v, 1 - i / v - j / v]\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / np.sum(weights, axis=1)[:, None]\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the tessellation factor is valid\n  if v < 1:\n    raise ValueError(\"The tessellation factor must be greater than or equal to 1.\")\n\n  # Generate the barycentric weights for each vertex of the triangle\n  w = np.zeros((v + 1) * (v + 2) // 2, dtype=int)\n  for i in range(v + 1):\n    for j in range(i + 1):\n      w[i * (i + 1) // 2 + j] = (v + 1 - i) * (v + 2 - i) // 2 - j\n\n  # Normalize the weights to get the barycentric coordinates\n  w = w / np.sum(w)\n\n  return w"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  import numpy as np\n\n  # Check if the input is valid\n  if v < 1:\n    raise ValueError(\"Tessellation factor must be greater than or equal to 1.\")\n\n  # Generate the weights for the vertices of the triangle\n  weights = np.zeros((v, v, 3))\n  for i in range(v):\n    for j in range(v):\n      weights[i, j, 0] = i\n      weights[i, j, 1] = j\n      weights[i, j, 2] = v - i - j\n\n  # Normalize the weights to get the barycentric coordinates\n  weights /= np.sum(weights, axis=-1, keepdims=True)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the tessellation factor is valid\n  if v < 1:\n    raise ValueError(\"Invalid tessellation factor. Must be greater than or equal to 1.\")\n\n  # Generate the integer weights for each vertex of the triangle\n  w = np.arange(v+1)\n  w = np.tile(w, (v+1, 1))\n\n  # Normalize the weights to get the barycentric coordinates\n  w = w / v\n\n  return w"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the tessellation factor is valid\n  if v < 1:\n    raise ValueError(\"Invalid tessellation factor. It must be greater than or equal to 1.\")\n\n  # Compute the barycentric weights\n  weights = np.zeros((v + 1, v + 1))\n  for i in range(v + 1):\n    for j in range(v + 1):\n      weights[i, j] = (i + j) / (v * (v + 1))\n\n  return weights\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check the input\n  if v < 1:\n    raise ValueError('The tessellation factor must be greater than or equal to 1.')\n\n  # Compute the barycentric weights\n  weights = np.zeros((v + 1, v + 1))\n  for i in range(v + 1):\n    for j in range(v + 1):\n      weights[i, j] = (i * j) / (v * (v + 1))\n\n  return weights\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # check the tessellation factor\n  if v < 1:\n    raise ValueError('Tessellation factor must be greater than or equal to 1.')\n\n  # compute the barycentric weights\n  weights = []\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      weights.append([i, j, v - i - j])\n  weights = np.array(weights)\n  weights = weights / v\n\n  return weights\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Compute the barycentric weights for the triangle\n  weights = np.zeros((v, v, 3))\n  for i in range(v):\n    for j in range(v):\n      weights[i, j, 0] = i / (v - 1)\n      weights[i, j, 1] = j / (v - 1)\n      weights[i, j, 2] = 1 - weights[i, j, 0] - weights[i, j, 1]\n\n  # Normalize the weights\n  weights = weights / np.sum(weights, axis=2, keepdims=True)\n\n  return weights\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  import numpy as np\n\n  # Check the input\n  if v < 1:\n    raise ValueError('The tessellation factor must be greater than or equal to 1.')\n\n  # Generate the weights for the triangle\n  weights = np.zeros((v+1, v+1))\n  for i in range(v+1):\n    for j in range(v+1):\n      if i+j <= v:\n        weights[i, j] = (v-i)*(v-j)\n\n  # Normalize the weights\n  weights = weights / np.sum(weights)\n\n  return weights\n"}
{"namespace": "linspline.query", "completion": "  # Check if t and v have the same length\n  if len(t) != len(v):\n    raise ValueError('t and v must have the same length')\n\n  # Check if t is sorted\n  if not np.all(np.diff(t) >= 0):\n    raise ValueError('t must be sorted')\n\n  # Check if tq is sorted\n  if not np.all(np.diff(tq) >= 0):\n    raise ValueError('tq must be sorted')\n\n  # Check if tq is within the range of t\n  if np.any(tq < t[0]) or np.any(tq > t[-1]):\n    raise ValueError('tq must be within the range of t')\n\n  # Check if tq is within the range of t\n  if np.any(tq < t[0]) or np.any(tq > t[-1]):\n    raise ValueError('tq must be within the range of t')\n\n  # Check if tq is within the range of t\n  if np.any(tq < t[0]) or np.any(tq > t[-1]):\n    raise ValueError('tq must be within the range of t')\n\n  # Check if tq is within the range of t\n  if np.any(tq < t[0]) or np.any(tq > t[-1]):\n    raise ValueError('tq must be within the range of t')\n\n  # Check if tq is within the range of t\n  if np.any(tq < t[0]) or np.any(tq > t[-1]):\n    raise ValueError('tq must be within the range of t')\n\n  # Check if tq is within the range of t\n  if np.any(tq < t[0]) or np.any(tq > t[-1]):\n    raise ValueError('tq must be within the range of t')\n\n  # Check if tq is within the range of t\n  if np.any(tq < t[0]) or np.any(tq > t[-1]"}
{"namespace": "linspline.query", "completion": "  # Ensure that the time points are sorted\n  t = np.sort(t)\n\n  # Ensure that the time points are unique\n  t = np.unique(t)\n\n  # Ensure that the time points are not repeated\n  if len(t) != len(v):\n    raise ValueError('The number of time points must be equal to the number of values.')\n\n  # Ensure that the time points are not repeated\n  if len(t) != len(np.unique(t)):\n    raise ValueError('The time points must be unique.')\n\n  # Ensure that the time points are not repeated\n  if len(t) != len(np.unique(t)):\n    raise ValueError('The time points must be unique.')\n\n  # Ensure that the time points are not repeated\n  if len(t) != len(np.unique(t)):\n    raise ValueError('The time points must be unique.')\n\n  # Ensure that the time points are not repeated\n  if len(t) != len(np.unique(t)):\n    raise ValueError('The time points must be unique.')\n\n  # Ensure that the time points are not repeated\n  if len(t) != len(np.unique(t)):\n    raise ValueError('The time points must be unique.')\n\n  # Ensure that the time points are not repeated\n  if len(t) != len(np.unique(t)):\n    raise ValueError('The time points must be unique.')\n\n  # Ensure that the time points are not repeated\n  if len(t) != len(np.unique(t)):\n    raise ValueError('The time points must be unique.')\n\n  # Ensure that the time points are not repeated\n  if len(t) != len(np.unique(t)):\n    raise ValueError('The time points must be unique.')\n\n  # Ensure that the time points are not repeated\n  if len(t) != len(np.unique(t)):\n    raise ValueError('The time points must be unique.')\n\n  # En"}
{"namespace": "linspline.query", "completion": "  # Check if the spline is valid\n  if not is_valid(t, v):\n    raise ValueError('The spline is not valid')\n\n  # Check if the query points are within the range of the spline\n  if not np.all(tq >= t[0]) or not np.all(tq <= t[-1]):\n    raise ValueError('The query points are outside the range of the spline')\n\n  # Interpolate the values at the query points\n  vq = np.interp(tq, t, v)\n\n  # Set extrapolated values to 0\n  vq[tq < t[0]] = 0\n  vq[tq > t[-1]] = 0\n\n  return vq"}
{"namespace": "linspline.query", "completion": "  # check if t and v are the same length\n  if len(t) != len(v):\n    raise ValueError('t and v must be the same length')\n\n  # check if t is strictly increasing\n  if not np.all(np.diff(t) > 0):\n    raise ValueError('t must be strictly increasing')\n\n  # check if tq is strictly increasing\n  if not np.all(np.diff(tq) > 0):\n    raise ValueError('tq must be strictly increasing')\n\n  # check if tq is within the range of t\n  if not np.all(tq >= t[0]) or not np.all(tq <= t[-1]):\n    raise ValueError('tq must be within the range of t')\n\n  # check if tq is within the range of t\n  if not np.all(tq >= t[0]) or not np.all(tq <= t[-1]):\n    raise ValueError('tq must be within the range of t')\n\n  # check if tq is within the range of t\n  if not np.all(tq >= t[0]) or not np.all(tq <= t[-1]):\n    raise ValueError('tq must be within the range of t')\n\n  # check if tq is within the range of t\n  if not np.all(tq >= t[0]) or not np.all(tq <= t[-1]):\n    raise ValueError('tq must be within the range of t')\n\n  # check if tq is within the range of t\n  if not np.all(tq >= t[0]) or not np.all(tq <= t[-1]):\n    raise ValueError('tq must be within the range of t')\n\n  # check if tq is within the range of t\n  if not np.all(tq >= t[0]) or not np.all(tq <= t[-1]):\n    raise ValueError('tq must be within the range of t')\n\n  # check if tq is within the range of t\n  if not np.all(tq >="}
{"namespace": "linspline.query", "completion": "  # Check if the time points are valid\n  if not np.all(np.diff(t) > 0):\n    raise ValueError('Time points must be strictly increasing')\n\n  # Check if the time points and values are the same length\n  if len(t) != len(v):\n    raise ValueError('Time points and values must be the same length')\n\n  # Interpolate the values at the query points\n  vq = np.interp(tq, t, v, left=0, right=0)\n\n  return vq"}
{"namespace": "linspline.query", "completion": "  # Check if the time points are sorted\n  if not np.all(np.diff(t) >= 0):\n    raise ValueError('Time points must be sorted')\n\n  # Check if the query points are within the range of the time points\n  if np.any(tq < t[0]) or np.any(tq > t[-1]):\n    raise ValueError('Query points must be within the range of the time points')\n\n  # Check if the time points are unique\n  if not np.all(np.diff(t) != 0):\n    raise ValueError('Time points must be unique')\n\n  # Check if the query points are unique\n  if not np.all(np.diff(tq) != 0):\n    raise ValueError('Query points must be unique')\n\n  # Check if the time points and values have the same length\n  if len(t) != len(v):\n    raise ValueError('Time points and values must have the same length')\n\n  # Check if the query points and values have the same length\n  if len(tq) != len(v):\n    raise ValueError('Query points and values must have the same length')\n\n  # Check if the query points are within the range of the time points\n  if np.any(tq < t[0]) or np.any(tq > t[-1]):\n    raise ValueError('Query points must be within the range of the time points')\n\n  # Check if the time points and query points are sorted\n  if not np.all(np.diff(t) >= 0) or not np.all(np.diff(tq) >= 0):\n    raise ValueError('Time points and query points must be sorted')\n\n  # Check if the time points and query points are unique\n  if not np.all(np.diff(t) != 0) or not np.all(np.diff(tq) != 0):\n    raise ValueError('Time points and query points must be unique')\n\n  # Check if the time points and values have the same length\n  if len(t) != len(v):\n    raise ValueError('Time points and values"}
{"namespace": "linspline.query", "completion": "  # Ensure the spline is valid\n  if not len(t) == len(v):\n    raise ValueError('Length of time points (t) and values (v) must be the same.')\n  if not np.all(np.diff(t) > 0):\n    raise ValueError('Time points (t) must be strictly increasing.')\n\n  # Find the indices of the query points in the time points\n  i = np.searchsorted(t, tq, side='left')\n\n  # If the query point is outside the range of the time points, set the value to 0\n  i[i == 0] = 1\n  i[i == len(t)] = len(t) - 1\n\n  # Interpolate the values at the query points\n  return np.interp(tq, t[i - 1], v[i - 1])"}
{"namespace": "linspline.query", "completion": "  import numpy as np\n  import scipy.interpolate\n\n  # Make sure the time points are sorted\n  t = np.sort(t)\n\n  # Make sure the time points are unique\n  if np.any(np.diff(t) == 0):\n    raise ValueError(\"The time points must be unique.\")\n\n  # Make sure the time points are within the range of the original data\n  if np.any(t < t[0]) or np.any(t > t[-1]):\n    raise ValueError(\"The time points must be within the range of the original data.\")\n\n  # Make sure the time points are within the range of the original data\n  if np.any(t < t[0]) or np.any(t > t[-1]):\n    raise ValueError(\"The time points must be within the range of the original data.\")\n\n  # Make sure the time points are within the range of the original data\n  if np.any(t < t[0]) or np.any(t > t[-1]):\n    raise ValueError(\"The time points must be within the range of the original data.\")\n\n  # Make sure the time points are within the range of the original data\n  if np.any(t < t[0]) or np.any(t > t[-1]):\n    raise ValueError(\"The time points must be within the range of the original data.\")\n\n  # Make sure the time points are within the range of the original data\n  if np.any(t < t[0]) or np.any(t > t[-1]):\n    raise ValueError(\"The time points must be within the range of the original data.\")\n\n  # Make sure the time points are within the range of the original data\n  if np.any(t < t[0]) or np.any(t > t[-1]):\n    raise ValueError(\"The time points must be within the range of the original data.\")\n\n  # Make sure the time points are within the range of the original data\n  if np.any(t < t[0]) or np.any(t > t[-1]):\n    raise ValueError(\"The time points must be within the range"}
{"namespace": "linspline.query", "completion": "  # Ensure t and v are numpy arrays\n  t = np.array(t)\n  v = np.array(v)\n\n  # Ensure t is sorted\n  if not np.all(t[:-1] <= t[1:]):\n    raise ValueError('t must be sorted')\n\n  # Ensure t and v have the same length\n  if len(t) != len(v):\n    raise ValueError('t and v must have the same length')\n\n  # Ensure tq is a numpy array\n  tq = np.array(tq)\n\n  # Ensure tq is not empty\n  if len(tq) == 0:\n    raise ValueError('tq cannot be empty')\n\n  # Ensure tq is sorted\n  if not np.all(tq[:-1] <= tq[1:]):\n    raise ValueError('tq must be sorted')\n\n  # Ensure tq is within the range of t\n  if tq[0] < t[0] or tq[-1] > t[-1]:\n    raise ValueError('tq must be within the range of t')\n\n  # Ensure tq is not a single point\n  if len(tq) == 1:\n    raise ValueError('tq must be at least two points')\n\n  # Ensure tq is not a duplicate point\n  if np.all(tq == tq[0]):\n    raise ValueError('tq must be at least two unique points')\n\n  # Ensure tq is not a duplicate point\n  if np.all(t == t[0]):\n    raise ValueError('t must be at least two unique points')\n\n  # Ensure tq is not a duplicate point\n  if np.all(v == v[0]):\n    raise ValueError('v must be at least two unique points')\n\n  # Ensure tq is not a duplicate point\n  if np.all(v == v[-1]):\n    raise ValueError('v must be at least two unique points')\n\n  # Ensure tq is not a duplicate point\n  if np.all(t =="}
{"namespace": "linspline.query", "completion": "  # Ensure the spline is valid\n  if not is_valid(t, v):\n    raise ValueError('Spline is not valid.')\n\n  # Get the number of query points\n  nq = len(tq)\n\n  # Initialize the output array\n  vq = np.zeros(nq)\n\n  # Loop over the query points\n  for i in range(nq):\n\n    # Get the query point\n    tqi = tq[i]\n\n    # Find the index of the knot just before the query point\n    j = np.searchsorted(t, tqi, side='left')\n\n    # If the query point is before the first knot, set the value to 0\n    if j == 0:\n      vq[i] = 0\n\n    # If the query point is after the last knot, set the value to 0\n    elif j == len(t):\n      vq[i] = 0\n\n    # Otherwise, interpolate the value using the two adjacent knots\n    else:\n      vq[i] = v[j - 1] + (v[j] - v[j - 1]) * (tqi - t[j - 1]) / (t[j] - t[j - 1])\n\n  # Return the interpolated values\n  return vq"}
{"namespace": "linspline.query", "completion": "  # Check the input arguments\n  if len(t) != len(v):\n    raise Exception('Length of time points (t) and values (v) must be the same.')\n  if len(t) < 2:\n    raise Exception('Number of time points (t) must be at least 2.')\n  if len(tq) < 1:\n    raise Exception('Number of query points (tq) must be at least 1.')\n\n  # Ensure the time points are sorted\n  t = np.sort(t)\n\n  # Ensure the time points are unique\n  if not np.all(np.diff(t) > 0):\n    raise Exception('Time points (t) must be unique and sorted.')\n\n  # Find the indices of the query points\n  tq_idx = np.searchsorted(t, tq)\n\n  # Ensure the query points are within the range of the time points\n  tq_idx[tq_idx == 0] = 1\n  tq_idx[tq_idx == len(t)] = len(t) - 1\n\n  # Interpolate the values at the query points\n  vq = np.interp(tq, t, v, left=0, right=0)\n\n  return vq"}
{"namespace": "linspline.query", "completion": "  # Ensure that the spline is valid\n  if len(t) != len(v):\n    raise ValueError('The number of time points and values must be the same.')\n  if len(t) < 2:\n    raise ValueError('The number of time points must be at least 2.')\n  if t[0] > t[-1]:\n    raise ValueError('The time points must be in ascending order.')\n\n  # If the query points are outside the range of the spline, set the values to 0\n  vq = np.zeros(len(tq))\n  tq = np.asarray(tq)\n  t = np.asarray(t)\n  v = np.asarray(v)\n  for i in range(len(tq)):\n    if tq[i] < t[0]:\n      vq[i] = 0\n    elif tq[i] > t[-1]:\n      vq[i] = 0\n    else:\n      # Find the index of the first time point that is greater than the query point\n      j = np.searchsorted(t, tq[i], side='right')\n      # Interpolate between the two time points\n      vq[i] = v[j-1] + (v[j] - v[j-1]) * (tq[i] - t[j-1]) / (t[j] - t[j-1])\n\n  return vq"}
{"namespace": "linspline.query", "completion": "  # Check if the input is valid\n  if (len(t) != len(v)):\n    raise ValueError('The length of the time points (t) and values (v) must be the same.')\n\n  # Ensure the spline is valid\n  t, v = check_spline(t, v)\n\n  # Find the indices of the points to interpolate\n  indices = np.searchsorted(t, tq, side='left')\n\n  # Interpolate the values\n  vq = np.interp(tq, t, v, left=0, right=0)\n\n  return vq"}
{"namespace": "linspline.query", "completion": "  # Ensure the spline is valid\n  if not (tq.size > 0 and t.size > 0 and v.size > 0 and t.size == v.size):\n    raise ValueError('Invalid spline input.')\n  if not np.all(tq >= t[0]) or not np.all(tq <= t[-1]):\n    raise ValueError('Query points outside of the spline range.')\n  if not np.all(np.diff(t) > 0):\n    raise ValueError('Invalid spline input.')\n\n  # Interpolate\n  tq = np.asarray(tq)\n  t = np.asarray(t)\n  v = np.asarray(v)\n  vq = np.interp(tq, t, v)\n\n  # Extrapolate\n  vq[tq < t[0]] = 0\n  vq[tq > t[-1]] = 0\n\n  return vq"}
{"namespace": "linspline.query", "completion": "  # Check that t and v have the same length\n  if len(t) != len(v):\n    raise ValueError(\"The time and value arrays must have the same length.\")\n\n  # Check that t is strictly increasing\n  if not all(t[i] < t[i+1] for i in range(len(t)-1)):\n    raise ValueError(\"The time array must be strictly increasing.\")\n\n  # Check that tq is strictly increasing\n  if not all(tq[i] < tq[i+1] for i in range(len(tq)-1)):\n    raise ValueError(\"The query time array must be strictly increasing.\")\n\n  # Check that tq is within the range of t\n  if tq[0] < t[0] or tq[-1] > t[-1]:\n    raise ValueError(\"The query time array must be within the range of the time array.\")\n\n  # Find the indices of the time points in t that are closest to the query points in tq\n  indices = np.searchsorted(t, tq)\n\n  # Extrapolate the values at the endpoints of the spline\n  v_extrap = np.zeros_like(tq)\n  v_extrap[:indices[0]] = v[0]\n  v_extrap[indices[-1]:] = v[-1]\n\n  # Interpolate the values at the query points\n  v_interp = np.interp(tq, t, v, left=v_extrap[0], right=v_extrap[-1])\n\n  return v_interp"}
{"namespace": "linspline.query", "completion": "  # Check that the input arrays are valid.\n  if not isinstance(tq, (list, tuple, np.ndarray)):\n    raise ValueError('The query points must be a list, tuple, or numpy array.')\n  if not isinstance(t, (list, tuple, np.ndarray)):\n    raise ValueError('The time points must be a list, tuple, or numpy array.')\n  if not isinstance(v, (list, tuple, np.ndarray)):\n    raise ValueError('The values must be a list, tuple, or numpy array.')\n  if len(t) != len(v):\n    raise ValueError('The number of time points must be equal to the number of values.')\n  if len(tq) == 0:\n    raise ValueError('The query points cannot be empty.')\n  if len(t) < 2:\n    raise ValueError('The time points must be at least 2 points.')\n\n  # Convert the input arrays to numpy arrays.\n  tq = np.array(tq)\n  t = np.array(t)\n  v = np.array(v)\n\n  # Check that the query points are within the range of the time points.\n  if np.any(tq < t[0]) or np.any(tq > t[-1]):\n    raise ValueError('The query points must be within the range of the time points.')\n\n  # Check that the time points are sorted.\n  if not np.all(np.diff(t) >= 0):\n    raise ValueError('The time points must be sorted.')\n\n  # Check that the time points are unique.\n  if len(t) != len(np.unique(t)):\n    raise ValueError('The time points must be unique.')\n\n  # Check that the values are not NaN.\n  if np.any(np.isnan(v)):\n    raise ValueError('The values cannot be NaN.')\n\n  # Check that the query points are not NaN.\n  if np.any(np.isnan(tq)):\n    raise ValueError('"}
{"namespace": "linspline.query", "completion": "  # Check that t and v are the same length\n  if len(t) != len(v):\n    raise ValueError(\"'t' and 'v' must be the same length.\")\n\n  # Check that t is sorted\n  if not np.all(np.diff(t) >= 0):\n    raise ValueError(\"'t' must be sorted.\")\n\n  # Check that tq is sorted\n  if not np.all(np.diff(tq) >= 0):\n    raise ValueError(\"'tq' must be sorted.\")\n\n  # Check that tq is within the range of t\n  if tq[0] < t[0] or tq[-1] > t[-1]:\n    raise ValueError(\"'tq' must be within the range of 't'.\")\n\n  # Check that tq is not a duplicate\n  if np.any(np.diff(tq) == 0):\n    raise ValueError(\"'tq' must not contain duplicate values.\")\n\n  # Check that tq is not a duplicate\n  if np.any(np.diff(t) == 0):\n    raise ValueError(\"'t' must not contain duplicate values.\")\n\n  # Initialize the output array\n  vq = np.zeros_like(tq)\n\n  # Iterate over each query point\n  for i, tqi in enumerate(tq):\n    # Find the indices of the time points that bracket the query point\n    ti = np.searchsorted(t, tqi, side='left')\n    tj = np.searchsorted(t, tqi, side='right')\n\n    # If the query point is outside the range of the time points, set the output value to 0\n    if ti == 0 or tj == len(t):\n      vq[i] = 0\n    else:\n      # Compute the interpolated value using linear interpolation\n      vq[i] = v[ti] + (tqi - t[ti]) / (t[ti+1] - t[ti]) * (v[ti+1] - v[ti])\n\n  # Return the interpol"}
{"namespace": "linspline.query", "completion": "  # Check the inputs\n  if len(t) != len(v):\n    raise ValueError(\"The number of time points must match the number of values.\")\n  if np.any(np.diff(t) <= 0):\n    raise ValueError(\"The time points must be strictly increasing.\")\n\n  # Find the indices of the query points in the time points\n  tq_indices = np.searchsorted(t, tq)\n\n  # Ensure the query points are within the range of the time points\n  tq_indices = np.clip(tq_indices, 1, len(t) - 1)\n\n  # Calculate the slope of the line between the query points and the time points\n  slope = (v[tq_indices] - v[tq_indices - 1]) / (t[tq_indices] - t[tq_indices - 1])\n\n  # Calculate the value at the query points using the line equation\n  return slope * (tq - t[tq_indices - 1]) + v[tq_indices - 1]\n"}
{"namespace": "linspline.query", "completion": "  # Check that the input arrays are valid.\n  if len(t) != len(v):\n    raise ValueError(\"'t' and 'v' must have the same length\")\n  if len(tq) < 1:\n    raise ValueError(\"'tq' must have at least one element\")\n\n  # Ensure that the time points are sorted in ascending order.\n  t = np.sort(t)\n\n  # Check that the time points are increasing.\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"'t' must be sorted in ascending order\")\n\n  # Find the indices of the time points that are closest to the query points.\n  i = np.searchsorted(t, tq)\n\n  # Ensure that the indices are within the range of the time points.\n  i = np.clip(i, 0, len(t) - 1)\n\n  # Interpolate the values at the query points using the indices.\n  vq = np.interp(tq, t, v, left=0, right=0)\n\n  return vq\n\n"}
{"namespace": "linspline.query", "completion": "  import numpy as np\n  import sys\n  # check if t and v are valid\n  if not np.all(np.diff(t) > 0):\n    sys.exit('ERROR: query: t must be strictly increasing')\n  if not len(t) == len(v):\n    sys.exit('ERROR: query: t and v must have the same length')\n\n  # ensure tq is sorted\n  tq = np.sort(tq)\n\n  # find the indices of the tq values in t\n  idx = np.searchsorted(t, tq, side='left')\n\n  # find the values of the spline at the query points\n  vq = np.zeros(len(tq))\n  for i in range(len(tq)):\n    if idx[i] == 0:\n      vq[i] = 0\n    elif idx[i] == len(t):\n      vq[i] = 0\n    else:\n      vq[i] = v[idx[i] - 1] + (tq[i] - t[idx[i] - 1]) * (v[idx[i]] - v[idx[i] - 1]) / (t[idx[i]] - t[idx[i] - 1])\n\n  return vq\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not isinstance(v, Iterable):\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}.{field.name} must be positive\")\n    else:\n        for value in v:\n            if value <= 0:\n                raise ValueError(f\"{cls.__name__}.{field.name} must be positive\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(isinstance(x, (int, float)) and x > 0 for x in v):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be positive\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(x > 0 for x in v):\n        raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(x > 0 for x in v):\n        raise ValueError(f\"{cls.__name__} must have only positive values in field {field.name}\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(x > 0 for x in v):\n        raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive.\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(x > 0 for x in v):\n        raise ValueError(f\"{cls.__name__} must have positive values for field {field.name}\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(x > 0 for x in v):\n        raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__} must have positive values for {field.name}\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__} must have positive values for {field.name}\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(x > 0 for x in v):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be positive\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(x > 0 for x in v):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be positive\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(isinstance(x, numbers.Number) and x > 0 for x in v):\n        raise ValueError(\n            f\"All values in {cls.__name__}.{field.name} must be positive.\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be positive\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(x > 0 for x in v):\n        raise ValueError(f\"{cls.__name__} must have only positive values for {field.name}\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(isinstance(x, numbers.Number) and x > 0 for x in v):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be positive\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(isinstance(x, numbers.Number) and x > 0 for x in v):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be positive\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(value > 0 for value in v):\n        raise ValueError(\n            f\"All values in {cls.__name__}.{field.name} must be positive.\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(isinstance(x, (int, float)) and x > 0 for x in v):\n        raise ValueError(\n            f\"All values in {cls.__name__}.{field.name} must be positive\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(isinstance(i, (int, float)) for i in v):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be positive.\")\n    if not all(i > 0 for i in v):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be positive.\")\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(x, Number) for x in v):\n            raise ValueError(f\"{cls.__name__}: All values must be numbers\")\n        if not all(x > 0 for x in v):\n            raise ValueError(\n                f\"{cls.__name__}: All values in field '{field.name}' must be positive\"\n            )\n    else:\n        if not isinstance(v, Number):\n            raise ValueError(f\"{cls.__name__}: All values must be numbers\")\n        if v <= 0:\n            raise ValueError(\n                f\"{cls.__name__}: Value in field '{field.name}' must be positive\"\n            )\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(item, numbers.Number) for item in v):\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be numbers\")\n        if not all(item > 0 for item in v):\n            raise ValueError(f\"All values in {cls.__name__}.{field.name} must be positive\")\n    else:\n        if not isinstance(v, numbers.Number):\n            raise ValueError(f\"{cls.__name__}.{field.name} must be a number\")\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}.{field.name} must be positive\")\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins + near * directions\n\n  # Calculate ray directions in NDC\n  directions = xnp.einsum('ij,kj->ki', pixtocam, directions)\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins - near * directions\n\n  # Calculate ray directions in NDC\n  directions = xnp.einsum(\"...ij,...j->...i\", pixtocam, directions)\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins + near * directions\n\n  # Calculate ray directions in NDC\n  directions = xnp.einsum('ij,jkl->ikl', pixtocam, directions)\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins / origins[..., 2:3] * near\n\n  # Calculate ray directions in NDC\n  directions = xnp.einsum(\"...ij,...j->...i\", pixtocam, directions)\n  directions = directions / directions[..., 2:3]\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins + near * directions\n\n  # Calculate directions in NDC\n  directions = xnp.dot(pixtocam, xnp.concatenate([directions, xnp.ones_like(directions[:, :1])], -1))\n  directions = directions[:, :3] / directions[:, 3:4]\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins * near\n\n  # Calculate ray directions in NDC\n  directions = xnp.dot(directions, pixtocam)\n\n  # Normalize ray directions\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins - directions * near\n\n  # Calculate ray directions in NDC\n  directions = xnp.einsum('ij,nj->ni', pixtocam, directions)\n\n  # Normalize ray directions\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins * near\n\n  # Calculate directions in NDC\n  directions = xnp.matmul(pixtocam, xnp.concatenate([directions, xnp.ones_like(directions[:, :1])], axis=-1))\n  directions = directions[:, :3] / directions[:, 3:4]\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins + near * directions\n\n  # Calculate ray directions in NDC\n  directions = xnp.einsum('ij,nj->ni', pixtocam, directions)\n\n  # Normalize ray directions\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.array(origins)\n  directions = xnp.array(directions)\n  pixtocam = xnp.array(pixtocam)\n\n  # Adjust ray origins to the near plane.\n  origins = origins + directions * near\n\n  # Calculate the corresponding directions in NDC.\n  directions = xnp.einsum('ij,...j->...i', pixtocam, directions)\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane.\n  origins = origins - near * directions\n\n  # Calculate the NDC directions.\n  directions = xnp.dot(pixtocam, directions.T).T\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins + near * directions\n\n  # Calculate ray directions in NDC\n  directions = xnp.einsum(\"ij,...j->...i\", pixtocam, directions)\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust origins to the near plane\n  origins = origins + near * directions\n\n  # Calculate directions in NDC\n  directions = xnp.linalg.inv(pixtocam) @ directions[:, :3]\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert origins to NDC\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[:, :1])], axis=-1)\n  origins = xnp.einsum(\"ij,kj->ki\", pixtocam, origins)\n  origins = origins[..., :3] / origins[..., 3:4]\n\n  # Convert directions to NDC\n  directions = xnp.concatenate([directions, xnp.zeros_like(directions[:, :1])], axis=-1)\n  directions = xnp.einsum(\"ij,kj->ki\", pixtocam, directions)\n  directions = directions[..., :3] / directions[..., 3:4]\n\n  # Adjust origins to near plane\n  origins = origins * near\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.array(origins)\n  directions = xnp.array(directions)\n  pixtocam = xnp.array(pixtocam)\n\n  # Adjust ray origins to the near plane\n  origins = origins + near * directions\n\n  # Calculate ray directions in NDC\n  directions = xnp.matmul(pixtocam, directions[..., :3].T).T\n  directions = directions / directions[..., 2:3]\n\n  return origins, directions\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[..., :1])], axis=-1)\n  origins = origins[..., :3] - origins[..., 2][..., None] * directions\n\n  # Calculate ray directions in NDC\n  directions = xnp.matmul(pixtocam, directions[..., None])[..., 0]\n  directions = directions[..., :3] / directions[..., 2][..., None]\n\n  # Clip ray directions to the near plane\n  directions = xnp.where(directions[..., 2][..., None] < 0, directions, xnp.zeros_like(directions))\n\n  return origins, directions"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # adjust origins to near plane\n  origins = origins - directions * near\n\n  # convert to NDC\n  origins = xnp.einsum(\"ij,jkl->ikl\", pixtocam, origins)\n  directions = xnp.einsum(\"ij,jkl->ikl\", pixtocam, directions)\n\n  return origins, directions\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.array(origins)\n  directions = xnp.array(directions)\n  pixtocam = xnp.array(pixtocam)\n\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[..., :1])], -1)\n  directions = xnp.concatenate([directions, xnp.zeros_like(directions[..., :1])], -1)\n\n  origins = origins[..., None, :]\n  directions = directions[..., None, :]\n\n  origins = origins / origins[..., -1:]\n  directions = directions / directions[..., -1:]\n\n  origins = xnp.concatenate([origins[..., :2], near * origins[..., 2:3]], -1)\n  directions = xnp.concatenate([directions[..., :2], directions[..., 2:3]], -1)\n\n  origins = xnp.einsum('...ij,...j->...i', pixtocam, origins)\n  directions = xnp.einsum('...ij,...j->...i', pixtocam, directions)\n\n  origins = origins[..., :2]\n  directions = directions[..., :2]\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert the origins to NDC using the perspective projection\n  origins_ndc = xnp.einsum(\"ij,kj->ki\", pixtocam, origins)\n  origins_ndc = origins_ndc / origins_ndc[:, 2:3]\n  origins_ndc = origins_ndc[:, :2]\n\n  # Calculate the directions in NDC using the perspective projection\n  directions_ndc = xnp.einsum(\"ij,kj->ki\", pixtocam, directions)\n  directions_ndc = directions_ndc / directions_ndc[:, 2:3]\n  directions_ndc = directions_ndc[:, :2]\n\n  # Adjust the origins to the near plane\n  origins_ndc = origins_ndc * near\n\n  return origins_ndc, directions_ndc\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins + directions * near\n\n  # Calculate the homogeneous coordinates of the origins\n  homo_origins = xnp.concatenate([origins, xnp.ones_like(origins[:, :1])], axis=-1)\n\n  # Calculate the homogeneous coordinates of the directions\n  homo_directions = xnp.concatenate([directions, xnp.zeros_like(directions[:, :1])], axis=-1)\n\n  # Perform the perspective projection\n  homo_origins = homo_origins @ pixtocam.T\n  homo_directions = homo_directions @ pixtocam.T\n\n  # Normalize the homogeneous coordinates\n  homo_origins = homo_origins / homo_origins[..., -1:, :]\n  homo_directions = homo_directions / homo_directions[..., -1:, :]\n\n  # Convert the homogeneous coordinates back to cartesian coordinates\n  origins = homo_origins[..., :3]\n  directions = homo_directions[..., :3]\n\n  return origins, directions"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compare the dot product of the normalized direction vectors\n  return np.isclose(np.dot(dir1, dir2), 1.0)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compare the dot product of the normalized direction vectors\n  return np.isclose(np.dot(dir1, dir2), 1.0)\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 or -1 (within a small epsilon)\n  return np.isclose(dot_product, 1) or np.isclose(dot_product, -1)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compare the dot product of the normalized direction vectors\n  return np.isclose(np.dot(dir1, dir2), 1.0, atol=1e-6)\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Check if the dot product is close to 1\n  if np.isclose(np.dot(dir1, dir2), 1.0):\n    return True\n  else:\n    return False"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 (considering a small epsilon)\n  return np.isclose(dot_product, 1, atol=1e-8)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 (parallel) or -1 (antiparallel)\n  return np.isclose(dot_product, 1.0) or np.isclose(dot_product, -1.0)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Calculate the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is within a small epsilon of 1 (considering numerical precision)\n  return np.isclose(dot_product, 1.0, atol=1e-6)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 or -1, considering a small epsilon to account for numerical precision issues\n  return np.isclose(dot_product, 1.0) or np.isclose(dot_product, -1.0)\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = np.array(dir1)\n  dir2 = np.array(dir2)\n\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  return np.abs(np.dot(dir1, dir2)) > 1 - 1e-10\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 or -1\n  return np.isclose(dot_product, 1) or np.isclose(dot_product, -1)\n\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n  return np.abs(np.dot(dir1, dir2)) > 1 - 1e-10\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compare the dot product of the normalized direction vectors\n  return np.isclose(np.dot(dir1, dir2), 1.0)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  eps = 1e-10\n  return abs(dir1.dot(dir2)) >= 1 - eps\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Check if the dot product of the normalized direction vectors is close to 1 (considering numerical precision)\n  return np.isclose(np.dot(dir1, dir2), 1)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  return np.dot(dir1, dir2) > 1 - 1e-12\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1_normalized = dir1 / np.linalg.norm(dir1)\n  dir2_normalized = dir2 / np.linalg.norm(dir2)\n\n  dot_prod = np.dot(dir1_normalized, dir2_normalized)\n\n  return abs(dot_prod) >= 1 - 1e-10\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compare the dot product of the normalized direction vectors\n  return np.isclose(np.dot(dir1, dir2), 1.0)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is within a small epsilon of 1, indicating parallel lines\n  return abs(dot_product - 1) < 1e-6\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # check if the dot product of the normalized direction vectors is within a small epsilon of 1\n  return np.abs(np.dot(dir1, dir2) - 1) < 1e-6\n\n"}
{"namespace": "common.bleu4_score", "completion": "    from nltk.tokenize import word_tokenize\n    from nltk.translate.bleu_score import sentence_bleu\n\n    # Tokenize the input texts\n    continuation_tokens = word_tokenize(continuation)\n    reference_tokens = word_tokenize(reference)\n\n    # Compute the BLEU-4 score\n    bleu_score = sentence_bleu([reference_tokens], continuation_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n\n    # Adjust the score for brevity penalty if requested\n    if with_penalty:\n        penalty = max(0, 1 - len(reference_tokens) / len(continuation_tokens)) ** 2\n        bleu_score *= penalty\n\n    return bleu_score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the BLEU-4 score\n    score = bleu_score(continuation_tokens, reference_tokens)\n\n    # If the brevity penalty is enabled, adjust the score\n    if with_penalty:\n        score = adjust_score_for_brevity(score, continuation_tokens, reference_tokens)\n\n    return score\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts using a custom tokenizer function\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Initialize variables to store the n-gram counts and the maximum n-gram length\n    continuation_n_grams = {}\n    reference_n_grams = {}\n    max_continuation_n_gram_length = 0\n    max_reference_n_gram_length = 0\n\n    # Compute the n-gram counts for the continuation and reference texts\n    for n in range(1, 5):\n        for i in range(len(continuation_tokens) - n + 1):\n            n_gram = tuple(continuation_tokens[i:i + n])\n            continuation_n_grams[n_gram] = continuation_n_grams.get(n_gram, 0) + 1\n            max_continuation_n_gram_length = max(max_continuation_n_gram_length, n)\n\n        for i in range(len(reference_tokens) - n + 1):\n            n_gram = tuple(reference_tokens[i:i + n])\n            reference_n_grams[n_gram] = reference_n_grams.get(n_gram, 0) + 1\n            max_reference_n_gram_length = max(max_reference_n_gram_length, n)\n\n    # Compute the precision values for each n-gram length\n    precision_values = []\n    for n in range(1, max_continuation_n_gram_length + 1):\n        continuation_n_gram_count = sum(continuation_n_grams.values())\n        reference_n_gram_count = sum(reference_n_grams.values())\n\n        continuation_n_gram_count = continuation_n_gram_count if continuation_n_gram_count > 0 else 1\n        reference_n_gram_count = reference_n_gram_count if reference_n_gram_count > 0 else 1\n\n        precision_values.append"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the BLEU-4 score\n    score = bleu_score(continuation_tokens, reference_tokens)\n\n    # If the brevity penalty is requested, adjust the score\n    if with_penalty:\n        score = brevity_penalty(score, continuation_tokens, reference_tokens)\n\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the BLEU-4 score\n    score = bleu_score(\n        continuation_tokens,\n        reference_tokens,\n        n=4,\n        with_penalty=with_penalty\n    )\n\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the n-gram overlaps\n    overlaps = {}\n    for n in range(1, 5):\n        continuation_ngrams = get_ngrams(continuation_tokens, n)\n        reference_ngrams = get_ngrams(reference_tokens, n)\n        overlaps[n] = len(continuation_ngrams & reference_ngrams)\n\n    # Calculate the brevity penalty\n    if with_penalty:\n        ratio = len(continuation_tokens) / len(reference_tokens)\n        penalty = 1 if ratio > 1 else math.exp(1 - ratio)\n    else:\n        penalty = 1\n\n    # Calculate the BLEU-4 score\n    score = math.exp(sum(math.log(overlap + 1) for overlap in overlaps.values()) / 4) * penalty\n\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # tokenize the input texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # calculate n-gram overlaps\n    ngram_overlaps = {}\n    for n in range(1, 5):\n        continuation_ngrams = get_ngrams(continuation_tokens, n)\n        reference_ngrams = get_ngrams(reference_tokens, n)\n        ngram_overlaps[n] = len(set(continuation_ngrams) & set(reference_ngrams))\n\n    # calculate brevity penalty\n    if with_penalty:\n        brevity_penalty = min(1, len(reference_tokens) / len(continuation_tokens)) ** 0.25\n    else:\n        brevity_penalty = 1\n\n    # calculate BLEU-4 score\n    bleu_4_score = (\n        ngram_overlaps[1] * ngram_overlaps[2] * ngram_overlaps[3] * ngram_overlaps[4]\n    ) ** 0.25 * brevity_penalty\n\n    return bleu_4_score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate n-gram overlaps\n    ngram_overlaps = {}\n    for n in range(1, 5):\n        continuation_ngrams = get_ngrams(continuation_tokens, n)\n        reference_ngrams = get_ngrams(reference_tokens, n)\n        ngram_overlaps[n] = len(continuation_ngrams.intersection(reference_ngrams))\n\n    # Calculate brevity penalty\n    if with_penalty:\n        reference_length = len(reference_tokens)\n        continuation_length = len(continuation_tokens)\n        penalty = min(1, reference_length / continuation_length)\n    else:\n        penalty = 1\n\n    # Calculate BLEU-4 score\n    score = (\n        penalty\n        * (\n            ngram_overlaps[1] / len(continuation_tokens)\n            + ngram_overlaps[2] / len(continuation_tokens) ** 2\n            + ngram_overlaps[3] / len(continuation_tokens) ** 3\n            + ngram_overlaps[4] / len(continuation_tokens) ** 4\n        )\n    ) / 4\n\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the n-gram overlaps\n    n_gram_overlaps = compute_n_gram_overlaps(continuation_tokens, reference_tokens)\n\n    # Compute the brevity penalty\n    if with_penalty:\n        brevity_penalty = compute_brevity_penalty(continuation_tokens, reference_tokens)\n    else:\n        brevity_penalty = 1\n\n    # Compute the BLEU-4 score\n    bleu_score = (\n        brevity_penalty\n        * n_gram_overlaps[0]\n        * n_gram_overlaps[1]\n        * n_gram_overlaps[2]\n        * n_gram_overlaps[3]\n    )\n\n    return bleu_score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input text\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the n-gram overlap between the continuation and reference texts\n    ngram_overlap = ngram_overlap(continuation_tokens, reference_tokens)\n\n    # Compute the BLEU-4 score\n    bleu_score = ngram_overlap / len(continuation_tokens)\n\n    # Apply the brevity penalty if specified\n    if with_penalty:\n        bleu_score *= brevity_penalty(continuation_tokens, reference_tokens)\n\n    return bleu_score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate n-gram overlaps\n    continuation_ngrams = get_ngrams(continuation_tokens, 1, 4)\n    reference_ngrams = get_ngrams(reference_tokens, 1, 4)\n    overlaps = calculate_overlaps(continuation_ngrams, reference_ngrams)\n\n    # Calculate BLEU-4 score\n    precision = sum(overlaps) / len(continuation_ngrams)\n    recall = sum(overlaps) / len(reference_ngrams)\n    bleu_score = precision * recall\n\n    # Apply brevity penalty\n    if with_penalty:\n        brevity_penalty = calculate_brevity_penalty(continuation_tokens, reference_tokens)\n        bleu_score *= brevity_penalty\n\n    return bleu_score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input text\n    continuation_tokens = custom_tokenizer(continuation)\n    reference_tokens = custom_tokenizer(reference)\n\n    # Calculate n-gram overlaps\n    ngram_overlaps = compute_ngram_overlaps(continuation_tokens, reference_tokens, 4)\n\n    # Calculate the BLEU-4 score\n    score = bleu_score(ngram_overlaps, with_penalty)\n\n    return score\n"}
{"namespace": "common.bleu4_score", "completion": "    def tokenize(text: str) -> list:\n        \"\"\"\n        Tokenizes the input text by splitting it into individual words or punctuation marks.\n\n        Input-Output Arguments\n        :param text: str, the input text to be tokenized.\n        :return: list, a list of tokens extracted from the input text.\n        \"\"\"\n\n        # Define a list of punctuation marks to be considered as separate tokens\n        punctuation = [\",\", \".\", \"?\", \"!\"]\n\n        # Split the text into a list of words and punctuation marks\n        tokens = text.split() + punctuation\n\n        return tokens\n\n    # Tokenize the continuation and reference texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the BLEU-4 score\n    score = 0\n    for n in range(1, 5):\n        ngrams = [tuple(continuation_tokens[i:i + n]) for i in range(len(continuation_tokens) - n + 1)]\n        reference_ngrams = [tuple(reference_tokens[i:i + n]) for i in range(len(reference_tokens) - n + 1)]\n        score += len(set(ngrams) & set(reference_ngrams)) / len(ngrams)\n\n    score /= 4\n\n    # Calculate the brevity penalty\n    if with_penalty:\n        penalty = min(1, len(continuation_tokens) / len(reference_tokens)) ** (1 / 4)\n        score *= penalty\n\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the BLEU-4 score\n    score = bleu_score(continuation_tokens, reference_tokens)\n\n    # Apply the brevity penalty if specified\n    if with_penalty:\n        score = apply_penalty(score, continuation_tokens, reference_tokens)\n\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Compute n-gram overlaps\n    overlaps = compute_ngram_overlaps(continuation_tokens, reference_tokens)\n\n    # Compute the BLEU-4 score\n    bleu4 = (\n        overlaps[0] * overlaps[1] * overlaps[2] * overlaps[3]\n    ) ** (1 / 4)\n\n    # Compute the brevity penalty\n    if with_penalty:\n        penalty = compute_brevity_penalty(continuation_tokens, reference_tokens)\n        bleu4 *= penalty\n\n    return bleu4\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the BLEU-4 score\n    score = bleu_score(continuation_tokens, reference_tokens)\n\n    # Apply the brevity penalty if specified\n    if with_penalty:\n        score = apply_brevity_penalty(score, continuation_tokens, reference_tokens)\n\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Initialize the BLEU score variables\n    score = 0\n    n_grams = [1, 2, 3, 4]\n    n_gram_counts = [0] * 4\n    reference_length = len(reference_tokens)\n    continuation_length = len(continuation_tokens)\n\n    # Calculate n-gram counts for the continuation and reference texts\n    for n in n_grams:\n        for i in range(len(continuation_tokens) - n + 1):\n            n_gram = continuation_tokens[i:i+n]\n            n_gram_counts[n-1] += continuation_tokens.count(n_gram)\n\n        for i in range(len(reference_tokens) - n + 1):\n            n_gram = reference_tokens[i:i+n]\n            n_gram_counts[n-1] -= reference_tokens.count(n_gram)\n\n    # Calculate the BLEU score\n    for i in range(4):\n        if n_gram_counts[i] > 0:\n            score += math.log(n_gram_counts[i] / (continuation_length - n_grams[i] + 1))\n\n    # Calculate the brevity penalty\n    if with_penalty:\n        if continuation_length > reference_length:\n            penalty = 1\n        else:\n            penalty = math.exp(1 - reference_length / continuation_length)\n    else:\n        penalty = 1\n\n    # Return the final BLEU-4 score\n    return penalty * math.exp(score / 4)\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the BLEU-4 score using the tokenized texts\n    score = bleu4_score_tokenized(continuation_tokens, reference_tokens)\n\n    # If the with_penalty flag is set, adjust the score for brevity penalty\n    if with_penalty:\n        score = adjust_for_brevity(continuation_tokens, reference_tokens, score)\n\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts using a custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the n-gram overlap between the continuation and reference texts\n    ngram_overlap = ngram_overlap_score(continuation_tokens, reference_tokens)\n\n    # Calculate the brevity penalty factor\n    brevity_penalty = brevity_penalty_factor(continuation_tokens, reference_tokens)\n\n    # Calculate the BLEU-4 score\n    bleu_score = (ngram_overlap ** 0.25) * brevity_penalty\n\n    # Optionally adjust the score for the brevity penalty\n    if with_penalty:\n        bleu_score *= brevity_penalty\n\n    return bleu_score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts using a custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Initialize variables to store the n-gram overlaps and the cumulative log probabilities\n    ngram_overlaps = [0] * 4\n    cumulative_log_probs = 0.0\n\n    # Compute the n-gram overlaps and the cumulative log probabilities\n    for n in range(1, 5):\n        ngram_overlaps[n - 1] = len(set(ngrams(continuation_tokens, n)).intersection(ngrams(reference_tokens, n)))\n        cumulative_log_probs += ngram_overlaps[n - 1] / len(continuation_tokens)\n\n    # Compute the brevity penalty\n    if with_penalty:\n        penalty = max(0, 1 - len(reference_tokens) / len(continuation_tokens)) ** 4\n    else:\n        penalty = 1.0\n\n    # Compute the BLEU-4 score\n    bleu_4_score = penalty * math.exp(cumulative_log_probs / 4)\n\n    return bleu_4_score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), jnp.sqrt(value_at_zero))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.maximum(x, eps * value_at_zero))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.maximum(x, eps))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), value_at_zero)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  return w / np.diff(t)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  return w / np.diff(t)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  return w / (t[1:] - t[:-1])"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  return w / (t[1:] - t[:-1])"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Check if the input vectors are of the same length\n  if len(t) != len(w):\n    raise ValueError(\"Input vectors must be of the same length\")\n\n  # Calculate the difference between consecutive elements in the input vector t\n  dt = t[1:] - t[:-1]\n\n  # Divide the weights by the difference between consecutive elements in the input vector t\n  pdf = w / dt\n\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Check if the input vectors are valid\n  if len(t) != len(w):\n    raise ValueError(\"The input vectors must have the same length.\")\n\n  # Calculate the difference between consecutive elements in the input vector t\n  dt = t[1:] - t[:-1]\n\n  # Divide the weights by the difference between consecutive elements in the input vector t\n  w = w / dt\n\n  # Return the resulting PDF\n  return w"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Check that t and w have the same length\n  if len(t) != len(w):\n    raise ValueError(\"t and w must have the same length\")\n\n  # Compute the difference between consecutive elements in t\n  dt = np.diff(t)\n\n  # Divide the weights by the difference between consecutive elements in t\n  pdf = w / dt\n\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Check if the input vectors are valid\n  if len(t) != len(w):\n    raise ValueError(\"Input vectors must have the same length.\")\n\n  # Check if the input vectors are sorted\n  if not np.all(np.diff(t) >= 0):\n    raise ValueError(\"Input vectors must be sorted.\")\n\n  # Check if the weights sum to 1\n  if not np.isclose(np.sum(w), 1):\n    raise ValueError(\"Weights must sum to 1.\")\n\n  # Calculate the PDF by dividing the weights by the difference between consecutive elements in the input vector t\n  pdf = w / np.diff(t)\n\n  # Normalize the PDF so that it integrates to 1\n  pdf = pdf / np.sum(pdf)\n\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  return w / np.diff(t)\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Ensure that the input weights sum to 1\n  assert np.isclose(np.sum(w), 1)\n\n  # Ensure that the input weights are sorted in ascending order\n  assert np.all(np.diff(t) >= 0)\n\n  # Compute the PDF by dividing the weights by the difference between consecutive elements in the input vector t\n  pdf = w / np.diff(t)\n\n  # Return the resulting PDF\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Check if the input vectors are valid\n  if len(t) != len(w):\n    raise ValueError(\"Input vectors must have the same length\")\n\n  # Check if the sum of the weights is 1\n  if not np.isclose(np.sum(w), 1.0):\n    raise ValueError(\"Sum of weights must be 1\")\n\n  # Calculate the PDF by dividing the weights by the difference between consecutive elements in the input vector t\n  pdf = w / np.diff(t)\n\n  # Return the PDF\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  return w / np.diff(t)\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Convert the input weights to a probability density function (PDF) by dividing the weights by the difference between consecutive elements in the input vector t.\n  w = w / np.diff(t)\n\n  # Normalize the PDF so that its integral (sum of weights) is equal to 1.\n  w = w / np.sum(w)\n\n  return w"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  return w / (t[1:] - t[:-1])\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  if len(t) != len(w):\n    raise Exception('The length of t and w must be the same.')\n\n  if len(t) < 2:\n    raise Exception('The length of t and w must be at least 2.')\n\n  if len(t) == 2:\n    return w\n\n  if len(t) == 3:\n    return w / (t[2] - t[1])\n\n  return w / (t[1:-1] - t[:-2])"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Convert the input arrays to numpy arrays\n  t = np.array(t)\n  w = np.array(w)\n\n  # Calculate the PDF by dividing the weights by the difference between consecutive elements in the input vector t\n  pdf = w / np.diff(t)\n\n  # Return the resulting PDF\n  return pdf\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Check if the input arrays are 1-dimensional\n  if len(t.shape) != 1:\n    raise ValueError(\"Input array t must be 1-dimensional.\")\n  if len(w.shape) != 1:\n    raise ValueError(\"Input array w must be 1-dimensional.\")\n\n  # Check if the input arrays have the same length\n  if len(t) != len(w):\n    raise ValueError(\"Input arrays t and w must have the same length.\")\n\n  # Check if the input arrays are sorted\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"Input array t must be sorted.\")\n\n  # Check if the input arrays are not empty\n  if len(t) == 0:\n    raise ValueError(\"Input arrays t and w cannot be empty.\")\n\n  # Check if the input arrays are not all zeros\n  if np.all(w == 0):\n    raise ValueError(\"Input array w cannot be all zeros.\")\n\n  # Check if the input arrays are not all equal\n  if np.all(w == w[0]):\n    raise ValueError(\"Input array w cannot be all equal.\")\n\n  # Check if the input arrays are not all equal\n  if np.all(w == w[0]):\n    raise ValueError(\"Input array w cannot be all equal.\")\n\n  # Check if the input arrays are not all equal\n  if np.all(w == w[0]):\n    raise ValueError(\"Input array w cannot be all equal.\")\n\n  # Check if the input arrays are not all equal\n  if np.all(w == w[0]):\n    raise ValueError(\"Input array w cannot be all equal.\")\n\n  # Check if the input arrays are not all equal\n  if np.all(w == w[0]):\n    raise ValueError(\"Input array w cannot be all equal.\")\n\n  # Check if the input arrays are not all equal\n  if np.all(w == w[0]):\n    raise ValueError(\"Input array w cannot be all equal.\")\n\n  # Check if the input arrays are not all equal\n  if np.all(w"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Check if the input vectors are of the same length\n  if len(t) != len(w):\n    raise ValueError(\"Input vectors must be of the same length\")\n\n  # Check if the sum of the weights is 1\n  if sum(w) != 1:\n    raise ValueError(\"Sum of weights must be 1\")\n\n  # Divide the weights by the difference between consecutive elements in the input vector t\n  pdf = [w[i] / (t[i+1] - t[i]) for i in range(len(t)-1)]\n\n  # Add the last weight to the PDF\n  pdf.append(w[-1])\n\n  return pdf\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  t = np.array(t)\n  w = np.array(w)\n  if len(t.shape) == 1:\n    t = t[:, np.newaxis]\n  if len(w.shape) == 1:\n    w = w[:, np.newaxis]\n  return w / np.diff(t, axis=0)\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  return w / np.diff(t)\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    except FileNotFoundError:\n        pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    except FileNotFoundError:\n        pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    try:\n        size = sum(os.path.getsize(os.path.join(path, f)) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)))\n    except FileNotFoundError:\n        size = 0\n\n    return size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    except NotADirectoryError:\n        return os.path.getsize(path)\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    total_size = 0\n\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                continue\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n    import errno\n\n    total_size = 0\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except OSError as e:\n                    if e.errno != errno.ENOENT:\n                        raise\n    except FileNotFoundError:\n        pass\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n    from pathlib import Path\n\n    try:\n        total_size = sum(f.stat().st_size for f in Path(path).glob('**/*') if f.is_file())\n    except FileNotFoundError:\n        total_size = 0\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n    import sys\n    from pathlib import Path\n\n    if not os.path.isdir(path):\n        return 0\n\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    try:\n        size = sum(os.path.getsize(os.path.join(path, filename)) for filename in os.listdir(path))\n    except FileNotFoundError:\n        size = 0\n\n    return size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n    import time\n\n    size = 0\n\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            try:\n                size += os.path.getsize(os.path.join(root, file))\n            except FileNotFoundError:\n                time.sleep(0.1)\n                size += os.path.getsize(os.path.join(root, file))\n\n    return size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n    import time\n    import logging\n\n    try:\n        size = sum(os.path.getsize(os.path.join(path, file)) for file in os.listdir(path) if os.path.isfile(os.path.join(path, file)))\n    except FileNotFoundError:\n        logging.info(\"FileNotFoundError: {}\".format(path))\n        time.sleep(1)\n        size = sum(os.path.getsize(os.path.join(path, file)) for file in os.listdir(path) if os.path.isfile(os.path.join(path, file)))\n\n    return size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n    import errno\n\n    try:\n        total_size = sum(os.path.getsize(os.path.join(path, file_name)) for file_name in os.listdir(path))\n    except FileNotFoundError as e:\n        if e.errno == errno.ENOENT:\n            total_size = 0\n        else:\n            raise\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    try:\n        total_size = 0\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n        return total_size\n    except:\n        return 0\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n\n    size = 0\n    try:\n        for root, dirs, files in os.walk(path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                size += os.path.getsize(file_path)\n    except FileNotFoundError:\n        pass\n    return size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n    import time\n    import logging\n\n    logging.basicConfig(level=logging.INFO)\n\n    start_time = time.time()\n\n    total_size = 0\n\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                try:\n                    fp = os.path.join(dirpath, f)\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    logging.warning(f\"FileNotFoundError: {fp}\")\n                    pass\n    except FileNotFoundError:\n        logging.warning(f\"FileNotFoundError: {path}\")\n        pass\n\n    end_time = time.time()\n\n    logging.info(f\"Folder size: {total_size} bytes\")\n    logging.info(f\"Time taken: {end_time - start_time} seconds\")\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n    import time\n\n    total_size = 0\n    start_time = time.time()\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    end_time = time.time()\n    print(f'Folder size: {total_size:,} bytes')\n    print(f'Time elapsed: {end_time - start_time:.2f} seconds')\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n    import pathlib\n    import time\n\n    path = pathlib.Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"The specified path does not exist: {path}\")\n\n    if not path.is_dir():\n        raise NotADirectoryError(f\"The specified path is not a directory: {path}\")\n\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for filename in filenames:\n            try:\n                file_path = os.path.join(dirpath, filename)\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                continue\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n    from pathlib import Path\n    from typing import List\n\n    # Get the size of the folder\n    size = sum(f.stat().st_size for f in Path(path).glob('**/*') if f.is_file())\n\n    # Get the size of the subdirectories\n    subdirectories = [f.path for f in os.scandir(path) if f.is_dir()]\n    for subdirectory in subdirectories:\n        try:\n            size += _get_folder_size(subdirectory)\n        except FileNotFoundError:\n            pass\n\n    return size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val % period\n    val = torch.where(val < -offset * period + offset * period, val + period, val)\n    val = torch.where(val > (1 - offset) * period, val - period, val)\n    return val"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val - offset * period\n    val = val - period * (val // period)\n    val = val + offset * period\n    return val"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = torch.atan2(torch.sin(val), torch.cos(val))\n    val = val - offset * period\n    val = val - torch.floor(val / period) * period\n    val = val + offset * period\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val - 2 * np.pi * np.floor((val + np.pi * offset) / (2 * np.pi))\n    val = val - np.pi * offset + ((val < -np.pi * offset) * 2 - 1) * np.pi\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if val is None:\n        return val\n    val = val % period\n    if val < -offset * period:\n        val += period\n    elif val > (1 - offset) * period:\n        val -= period\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = torch.atan2(torch.sin(val), torch.cos(val))\n    val = val - 2 * torch.pi * torch.floor(val / (2 * torch.pi))\n    val = val - offset * period\n    val = val % (2 * period)\n    val = val + offset * period\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + 0.5) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val % period\n    val = torch.where(val < -offset * period, val + period, val)\n    val = torch.where(val > (1 - offset) * period, val - period, val)\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val - offset * period\n    val = val - period * torch.floor(val / period + 0.5)\n    return val\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return agent.__dict__"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n\n        return agent_dict"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            'dynamic_prompt': agent.dynamic_prompt,\n            'purpose': agent.purpose,\n            'purpose_embedding': agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            'depth': agent.depth,\n            'max_depth': agent.max_depth,\n            'usage_count': agent.usage_count,\n            'id': agent.id,\n            'parent_id': agent.parent_id,\n            'working_agent': agent.working_agent,\n            'is_prime': agent.is_prime,\n            'evolve_count': agent.evolve_count,\n            'number_of_code_executions': agent.number_of_code_executions,\n            'last_input': agent.last_input,\n        }\n\n        return agent_dict"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            purpose_embedding = agent.purpose_embedding.tolist()\n        else:\n            purpose_embedding = None\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            purpose_embedding = agent.purpose_embedding.tolist()\n        else:\n            purpose_embedding = None\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if type(agent.purpose_embedding) == np.ndarray:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        # Convert the purpose_embedding from a numpy array to a list if necessary\n        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        # Convert the dynamic_prompt from a numpy array to a list if necessary\n        if agent.dynamic_prompt is not None:\n            agent.dynamic_prompt = agent.dynamic_prompt.tolist()\n\n        # Convert the last_input from a numpy array to a list if necessary\n        if agent.last_input is not None:\n            agent.last_input = agent.last_input.tolist()\n\n        # Convert the working_agent from a numpy array to a list if necessary\n        if agent.working_agent is not None:\n            agent.working_agent = agent.working_agent.tolist()\n\n        # Convert the working_agent from a numpy array to a list if necessary\n        if agent.working_agent is not None:\n            agent.working_agent = agent.working_agent.tolist()\n\n        # Convert the working_agent from a numpy array to a list if necessary\n        if agent.working_agent is not None:\n            agent.working_agent = agent.working_agent.tolist()\n\n        # Convert the working_agent from a numpy array to a list if necessary\n        if agent.working_agent is not None:\n            agent.working_agent = agent.working_agent.tolist()\n\n        # Convert the working_agent from a numpy array to a list if necessary\n        if agent.working_agent is not None:\n            agent.working_agent = agent.working_agent.tolist()\n\n        # Convert the working_agent from a numpy array to a list if necessary\n        if agent.working_agent is not None:\n            agent.working_agent = agent.working_agent.tolist()\n\n        # Convert the working_agent from a numpy array to a list if necessary\n        if agent.working_agent is not None:\n            agent.working_agent = agent.working_agent.tolist()"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return agent.__dict__\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = agent.__dict__\n        if agent_dict['purpose_embedding'] is not None:\n            agent_dict['purpose_embedding'] = agent_dict['purpose_embedding'].tolist()\n        return agent_dict\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            if isinstance(agent.purpose_embedding, np.ndarray):\n                agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return agent.__dict__\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        return agent_dict\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            if type(agent.purpose_embedding) == np.ndarray:\n                agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = [[] for _ in range(num_bins)]\n    bin_weights = [0] * num_bins\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        bin_idx = min(range(num_bins), key=lambda i: bin_weights[i])\n        bins[bin_idx].append(item)\n        bin_weights[bin_idx] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive\")\n\n    # Check that the number of items and weights match\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must match\")\n\n    # Check that all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive\")\n\n    # Sort the items by weight in descending order\n    sorted_items = [item for _, item in sorted(zip(weights, items), reverse=True)]\n\n    # Initialize a dictionary to store the items in each bin\n    bins = {i: [] for i in range(num_bins)}\n\n    # Initialize a dictionary to store the total weight in each bin\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item into the bin with the current lowest total weight\n    for item in sorted_items:\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weights[item]\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items and weights are equal\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be equal.\")\n\n    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_indices = sorted(range(len(weights)), key=lambda i: weights[i], reverse=True)\n    sorted_items = [items[i] for i in sorted_indices]\n    sorted_weights = [weights[i] for i in sorted_indices]\n\n    # Initialize the bins and bin weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Distribute the items into the bins\n    for item, weight in zip(sorted_items, sorted_weights):\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin with the current lowest total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n\n    # Check that the number of items and weights are equal\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be equal.\")\n\n    # Check that all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    # Return the bins and their weights\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of bins is valid\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive.\")\n\n    # Check if the number of items and weights match\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must match.\")\n\n    # Sort the items by weight in descending order\n    sorted_indices = np.argsort(-np.array(weights))\n    sorted_items = [items[i] for i in sorted_indices]\n    sorted_weights = [weights[i] for i in sorted_indices]\n\n    # Initialize the bins and weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Distribute the items greedily\n    for item, weight in zip(sorted_items, sorted_weights):\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Place the item in the bin with the lowest total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of bins is valid\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive.\")\n\n    # Check if the number of items and weights match\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must match.\")\n\n    # Check if the weights are all positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive\")\n\n    # Check that the number of items and weights match\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must match\")\n\n    # Check that all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and weights dictionaries\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    # Return the bins and weights dictionaries\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive\")\n\n    # Check that the number of items and weights is equal\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must be equal\")\n\n    # Check that the weights are all positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"Weights must be positive\")\n\n    # Sort the items by weight in descending order\n    sorted_indices = sorted(range(len(weights)), key=lambda i: weights[i], reverse=True)\n    sorted_items = [items[i] for i in sorted_indices]\n    sorted_weights = [weights[i] for i in sorted_indices]\n\n    # Initialize the bins and total weights\n    bins = {i: [] for i in range(num_bins)}\n    total_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in zip(sorted_items, sorted_weights):\n        bin_index = min(total_weights, key=total_weights.get)\n        bins[bin_index].append(item)\n        total_weights[bin_index] += weight\n\n    return bins, total_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive.\")\n\n    # Check that the number of items and weights is equal\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must be equal.\")\n\n    # Check that all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Initialize a dictionary to store the items in each bin\n    bins = {bin_idx: [] for bin_idx in range(num_bins)}\n\n    # Initialize a dictionary to store the total weight in each bin\n    bin_weights = {bin_idx: 0 for bin_idx in range(num_bins)}\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin_idx = min(bin_weights, key=bin_weights.get)\n        bins[min_bin_idx].append(item)\n        bin_weights[min_bin_idx] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive.\")\n\n    # Check that the number of items and weights are the same\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must be the same.\")\n\n    # Sort the items by weight in descending order\n    sorted_indices = np.argsort(weights)[::-1]\n    sorted_items = [items[i] for i in sorted_indices]\n    sorted_weights = [weights[i] for i in sorted_indices]\n\n    # Initialize the bins and weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Greedily distribute the items into the bins\n    for item, weight in zip(sorted_items, sorted_weights):\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin with the lowest total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive\")\n\n    # Check that the number of items and weights match\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must match\")\n\n    # Check that all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive\")\n\n    # Create a list of tuples containing the item and its weight\n    items_with_weights = list(zip(items, weights))\n\n    # Sort the items by weight in descending order\n    items_with_weights.sort(key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Iterate over the items and distribute them to the bins\n    for item, weight in items_with_weights:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin with the lowest total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    # Return the bins and their weights\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of bins is less than or equal to zero.\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive.\")\n\n    # Check if the number of items is less than the number of bins.\n    if len(items) < num_bins:\n        raise ValueError(\"Number of items must be greater than or equal to the number of bins.\")\n\n    # Check if the number of weights is equal to the number of items.\n    if len(weights) != len(items):\n        raise ValueError(\"Number of weights must be equal to the number of items.\")\n\n    # Check if any weight is negative.\n    if any(weight < 0 for weight in weights):\n        raise ValueError(\"Weights must be non-negative.\")\n\n    # Sort the items by weight in descending order.\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and the weights of the bins.\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item into the bin with the current lowest total weight.\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    # Return the bins and the weights of the bins.\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be the same.\")\n    if num_bins < 1:\n        raise ValueError(\"The number of bins must be at least 1.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = [(item, weight) for item, weight in zip(items, weights)]\n    sorted_items.sort(key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Greedily place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin_weight = min(bin_weights.values())\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive\")\n\n    # Check if the number of items and weights are the same\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must be the same\")\n\n    # Check if all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive\")\n\n    # Create a list of tuples containing the item and its weight\n    item_weights = [(item, weight) for item, weight in zip(items, weights)]\n\n    # Sort the items by weight in descending order\n    item_weights.sort(key=lambda x: x[1], reverse=True)\n\n    # Initialize a dictionary to store the items in each bin\n    bins = {i: [] for i in range(num_bins)}\n\n    # Initialize a dictionary to store the total weight of each bin\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Iterate over the items and place them into the bins\n    for item, weight in item_weights:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin\n        bins[min_bin].append(item)\n\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    # Return the dictionaries containing the items and total weights in each bin\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items and weights match\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must match.\")\n\n    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive.\")\n\n    # Create a list of tuples containing the item and its weight\n    item_weights = [(item, weight) for item, weight in zip(items, weights)]\n\n    # Sort the list of tuples by weight in descending order\n    item_weights.sort(key=lambda x: x[1], reverse=True)\n\n    # Create a dictionary to store the items in each bin\n    bins = {i: [] for i in range(num_bins)}\n\n    # Create a dictionary to store the total weight of each bin\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Iterate over the sorted list of tuples\n    for item, weight in item_weights:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin with the lowest total weight\n        bins[min_bin].append(item)\n\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    # Return the dictionaries\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive\")\n\n    # Check that the number of items and weights match\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must match\")\n\n    # Create a list of tuples, where each tuple contains an item and its weight\n    item_weights = list(zip(items, weights))\n\n    # Sort the list of tuples in descending order of weight\n    item_weights.sort(key=lambda x: x[1], reverse=True)\n\n    # Initialize a dictionary to store the items in each bin\n    bin_items = {i: [] for i in range(num_bins)}\n\n    # Initialize a dictionary to store the total weight of each bin\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Iterate over the sorted list of tuples\n    for item, weight in item_weights:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin with the lowest total weight\n        bin_items[min_bin].append(item)\n\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    # Return the dictionaries\n    return bin_items, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    assert len(items) == len(weights)\n    assert num_bins > 0\n    assert all(w > 0 for w in weights)\n\n    # Sort items by weight in descending order\n    sorted_indices = sorted(range(len(weights)), key=lambda i: weights[i], reverse=True)\n    sorted_items = [items[i] for i in sorted_indices]\n    sorted_weights = [weights[i] for i in sorted_indices]\n\n    # Initialize empty bins\n    bins = [[] for _ in range(num_bins)]\n    bin_weights = [0] * num_bins\n\n    # Greedily distribute items into bins\n    for item, weight in zip(sorted_items, sorted_weights):\n        # Find bin with lowest total weight\n        min_bin_index = min(range(num_bins), key=lambda i: bin_weights[i])\n\n        # Add item to bin\n        bins[min_bin_index].append(item)\n        bin_weights[min_bin_index] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of bins is valid\n    if num_bins <= 0:\n        raise ValueError(\"Invalid number of bins.\")\n\n    # Check if the number of items and weights match\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights do not match.\")\n\n    # Check if the weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"Weights must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = [item for _, item in sorted(zip(weights, items), reverse=True)]\n\n    # Initialize the bins and weights dictionaries\n    bins = {i: [] for i in range(num_bins)}\n    weights_dict = {i: 0 for i in range(num_bins)}\n\n    # Place each item into the bin with the current lowest total weight\n    for item in sorted_items:\n        min_bin = min(weights_dict, key=weights_dict.get)\n        bins[min_bin].append(item)\n        weights_dict[min_bin] += weights[sorted_items.index(item)]\n\n    return bins, weights_dict\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive\")\n\n    # Check if the number of items and weights are equal\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must be equal\")\n\n    # Check if all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive\")\n\n    # Initialize the bin weights dictionary\n    bin_weights = {bin_idx: 0 for bin_idx in range(num_bins)}\n\n    # Initialize the bin items dictionary\n    bin_items = {bin_idx: [] for bin_idx in range(num_bins)}\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin_idx = min(bin_weights, key=bin_weights.get)\n        bin_weights[min_bin_idx] += weight\n        bin_items[min_bin_idx].append(item)\n\n    # Return the bin items and bin weights dictionaries\n    return bin_items, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Create a list of tuples containing the item and its weight\n    item_weight_pairs = list(zip(items, weights))\n\n    # Sort the items by weight in descending order\n    item_weight_pairs.sort(key=lambda x: x[1], reverse=True)\n\n    # Initialize a dictionary to store the items in each bin\n    bins = {i: [] for i in range(num_bins)}\n\n    # Initialize a dictionary to store the total weight of each bin\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Iterate over the items and distribute them to the bins\n    for item, weight in item_weight_pairs:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin with the lowest total weight\n        bins[min_bin].append(item)\n\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    # Return the dictionaries\n    return bins, bin_weights\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Convert the function name, arguments, and keyword arguments to a string representation\n        data = func_name + str(args) + str(kwargs)\n\n        # Compute the SHA-256 hash of the data\n        hash_obj = hashlib.sha256()\n        hash_obj.update(data.encode())\n        hash_value = hash_obj.hexdigest()\n\n        return hash_value"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = func_name + str(args) + str(kwargs)\n        hash_object = hashlib.sha256(data.encode())\n        return hash_object.hexdigest()\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = func_name + str(args) + str(kwargs)\n        return hashlib.sha256(data.encode()).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Concatenate the function name, arguments, and keyword arguments into a single string\n        data = func_name + str(args) + str(kwargs)\n\n        # Compute the SHA-256 hash of the data\n        hash_object = hashlib.sha256(data.encode())\n\n        # Return the hexadecimal digest of the hash\n        return hash_object.hexdigest()\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Create a hash object\n        h = hashlib.sha256()\n\n        # Update the hash object with the function name\n        h.update(func_name.encode())\n\n        # Update the hash object with the positional arguments\n        for arg in args:\n            h.update(str(arg).encode())\n\n        # Update the hash object with the keyword arguments\n        for k, v in kwargs.items():\n            h.update(str(k).encode())\n            h.update(str(v).encode())\n\n        # Return the hexadecimal digest of the hash\n        return h.hexdigest()\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Compute the hash of the function name, its arguments, and keyword arguments\n        hash_data = func_name.encode() + str(args).encode() + str(kwargs).encode()\n        hash_object = hashlib.sha256(hash_data)\n        return hash_object.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Create a hash object\n        hash_obj = hashlib.sha256()\n\n        # Update the hash object with the function name\n        hash_obj.update(func_name.encode())\n\n        # Update the hash object with the positional arguments\n        for arg in args:\n            hash_obj.update(str(arg).encode())\n\n        # Update the hash object with the keyword arguments\n        for k, v in kwargs.items():\n            hash_obj.update(str(k).encode())\n            hash_obj.update(str(v).encode())\n\n        # Return the hexadecimal digest of the hash\n        return hash_obj.hexdigest()\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Create a hash object to generate the hash\n        hash_obj = hashlib.sha256()\n\n        # Convert the function name to bytes and update the hash object with it\n        hash_obj.update(func_name.encode())\n\n        # Convert the positional arguments to bytes and update the hash object with them\n        for arg in args:\n            hash_obj.update(str(arg).encode())\n\n        # Convert the keyword arguments to bytes and update the hash object with them\n        for k, v in kwargs.items():\n            hash_obj.update(str(k).encode())\n            hash_obj.update(str(v).encode())\n\n        # Return the hexadecimal digest of the hash\n        return hash_obj.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Convert the function name, arguments, and keyword arguments to a string\n        data = str(func_name) + str(args) + str(kwargs)\n\n        # Compute the SHA-256 hash of the data\n        hash_obj = hashlib.sha256()\n        hash_obj.update(data.encode())\n        hash_value = hash_obj.hexdigest()\n\n        return hash_value\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Convert the function name, arguments, and keyword arguments to a string representation\n        data = func_name + str(args) + str(kwargs)\n\n        # Compute the SHA-256 hash of the data\n        hash_object = hashlib.sha256(data.encode())\n\n        # Return the hexadecimal representation of the hash\n        return hash_object.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Create a hash object\n        hash_obj = hashlib.sha256()\n\n        # Convert the function name to bytes and update the hash object\n        hash_obj.update(func_name.encode())\n\n        # Convert the positional arguments to bytes and update the hash object\n        for arg in args:\n            hash_obj.update(str(arg).encode())\n\n        # Convert the keyword arguments to bytes and update the hash object\n        for kwarg in kwargs:\n            hash_obj.update(str(kwarg).encode())\n\n        # Return the hexadecimal digest of the hash\n        return hash_obj.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Combine the function name, arguments, and keyword arguments into a single string\n        data = f\"{func_name}{args}{kwargs}\"\n\n        # Compute the SHA-256 hash of the data\n        hash_obj = hashlib.sha256()\n        hash_obj.update(data.encode())\n        hash_value = hash_obj.hexdigest()\n\n        return hash_value\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Compute the hash of the function name, arguments, and keyword arguments\n        data = func_name + str(args) + str(kwargs)\n        hash_obj = hashlib.sha256(data.encode())\n        return hash_obj.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Combine the function name, arguments, and keyword arguments into a string\n        data = func_name + str(args) + str(kwargs)\n\n        # Compute the SHA-256 hash of the data\n        hash_value = hashlib.sha256(data.encode()).hexdigest()\n\n        return hash_value\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Generate the input data for the hash\n        input_data = func_name + str(args) + str(kwargs)\n\n        # Compute the SHA-256 hash of the input data\n        hash_object = hashlib.sha256(input_data.encode())\n        hex_digest = hash_object.hexdigest()\n\n        return hex_digest\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Create a hash object\n        hash_obj = hashlib.sha256()\n\n        # Convert the function name, arguments, and keyword arguments to a string\n        data = str(func_name) + str(args) + str(kwargs)\n\n        # Update the hash object with the data\n        hash_obj.update(data.encode())\n\n        # Return the hexadecimal digest of the hash\n        return hash_obj.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = func_name + str(args) + str(kwargs)\n        return hashlib.sha256(data.encode()).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Convert the function name, arguments, and keyword arguments to a string representation\n        data = str(func_name) + str(args) + str(kwargs)\n\n        # Compute the SHA-256 hash of the data\n        hash_obj = hashlib.sha256()\n        hash_obj.update(data.encode())\n        hash_value = hash_obj.hexdigest()\n\n        return hash_value\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Convert the function name, arguments, and keyword arguments to a string representation\n        data = func_name + str(args) + str(kwargs)\n\n        # Compute the SHA-256 hash of the data\n        hash_obj = hashlib.sha256()\n        hash_obj.update(data.encode())\n        return hash_obj.hexdigest()\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = func_name\n        for arg in args:\n            data += str(arg)\n        for kwarg in kwargs:\n            data += str(kwargs[kwarg])\n        return hashlib.sha256(data.encode()).hexdigest()\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    return np.sum(\n        np.linalg.norm(np.diff(polygon, axis=0), axis=1)\n        < max_point_distance\n    )"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    total_length = 0.0\n    for i in range(polygon.shape[0] - 1):\n        distance = np.linalg.norm(polygon[i + 1] - polygon[i])\n        if distance <= max_point_distance:\n            total_length += distance\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    total_length = 0\n    for i in range(polygon.shape[0] - 1):\n        distance = np.linalg.norm(polygon[i + 1] - polygon[i])\n        if distance <= max_point_distance:\n            total_length += distance\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    polygon_length = 0\n    for i in range(len(polygon) - 1):\n        point1 = polygon[i]\n        point2 = polygon[i + 1]\n        distance = np.linalg.norm(point2 - point1)\n        if distance <= max_point_distance:\n            polygon_length += distance\n    return polygon_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    return np.sum(\n        np.sqrt(\n            np.sum(\n                np.square(np.diff(polygon, axis=0)),\n                axis=1,\n            )\n        )\n    )\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    polygon_length = 0\n    for i in range(polygon.shape[0] - 1):\n        point1 = polygon[i]\n        point2 = polygon[i + 1]\n        distance = np.linalg.norm(point2 - point1)\n        if distance > max_point_distance:\n            continue\n        polygon_length += distance\n    return polygon_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    total_length = 0\n    for i in range(len(polygon) - 1):\n        distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n        if distance > max_point_distance:\n            continue\n        total_length += distance\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    length = 0\n    for i in range(len(polygon) - 1):\n        distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n        if distance > max_point_distance:\n            break\n        length += distance\n    return length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the total length to 0\n    total_length = 0\n\n    # Loop through the points in the polygon\n    for i in range(len(polygon)):\n        # Get the current point and the next point\n        point = polygon[i]\n        next_point = polygon[(i + 1) % len(polygon)]\n\n        # Compute the distance between the current point and the next point\n        distance = np.linalg.norm(point - next_point)\n\n        # If the distance is below the maximum distance, add it to the total length\n        if distance <= max_point_distance:\n            total_length += distance\n\n    # Return the total length\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    length = 0\n    for i in range(polygon.shape[0] - 1):\n        point1 = polygon[i]\n        point2 = polygon[i + 1]\n        distance = np.linalg.norm(point2 - point1)\n        if distance <= max_point_distance:\n            length += distance\n    return length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    length = 0\n    for i in range(len(polygon) - 1):\n        distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n        if distance > max_point_distance:\n            continue\n        length += distance\n    return length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    return sum([np.linalg.norm(polygon[i] - polygon[i + 1]) for i in range(len(polygon) - 1) if\n                np.linalg.norm(polygon[i] - polygon[i + 1]) <= max_point_distance])\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    length = 0\n    for i in range(polygon.shape[0] - 1):\n        point_distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n        if point_distance > max_point_distance:\n            continue\n        length += point_distance\n    return length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    return np.sum(np.linalg.norm(np.diff(polygon, axis=0), axis=1) < max_point_distance)\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the length variable to 0\n    length = 0\n\n    # Iterate over the points in the polygon\n    for i in range(len(polygon) - 1):\n        # Calculate the distance between the current point and the next point\n        distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n\n        # If the distance is below the maximum distance, add it to the total length\n        if distance <= max_point_distance:\n            length += distance\n\n    # Return the total length\n    return length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    polygon_length = 0\n    for i in range(polygon.shape[0] - 1):\n        point_distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n        if point_distance <= max_point_distance:\n            polygon_length += point_distance\n    return polygon_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    return np.sum(np.linalg.norm(polygon[1:] - polygon[:-1], axis=1))\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the length variable to 0\n    length = 0\n\n    # Iterate over the points in the polygon\n    for i in range(len(polygon) - 1):\n        # Calculate the distance between the current point and the next point\n        distance = np.linalg.norm(polygon[i + 1] - polygon[i])\n\n        # If the distance is below the maximum distance, add it to the total length\n        if distance < max_point_distance:\n            length += distance\n\n    # Return the total length\n    return length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    total_length = 0\n    for i in range(len(polygon)):\n        j = (i + 1) % len(polygon)\n        distance = np.linalg.norm(polygon[i] - polygon[j])\n        if distance <= max_point_distance:\n            total_length += distance\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon) < 2:\n        return 0.0\n    total_length = 0.0\n    for i in range(len(polygon) - 1):\n        point_distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n        if point_distance > max_point_distance:\n            continue\n        total_length += point_distance\n    return total_length\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Calculate the largest polygon's area\n    largest_polygon_area = max(polygon.area for polygon in polygons)\n\n    # Filter out polygons based on their area\n    filtered_polygons = [\n        polygon\n        for polygon in polygons\n        if polygon.area >= rel_tr * largest_polygon_area or polygon.area >= abs_tr\n    ]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Get the largest polygon's area\n    largest_area = max(polygon.area for polygon in polygons)\n\n    # Filter out polygons based on their area\n    filtered_polygons = [\n        polygon\n        for polygon in polygons\n        if polygon.area > rel_tr * largest_area or polygon.area > abs_tr\n    ]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    area_threshold = max(abs_tr, np.max(polygon_areas(polygons)) * rel_tr)\n    return [polygon for polygon in polygons if polygon_area(polygon) > area_threshold]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Calculate the largest polygon's area\n    largest_area = max(polygon.area for polygon in polygons)\n\n    # Filter out polygons based on their area\n    filtered_polygons = []\n    for polygon in polygons:\n        if polygon.area >= rel_tr * largest_area or polygon.area >= abs_tr:\n            filtered_polygons.append(polygon)\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return []\n\n    if rel_tr < 0.0 or abs_tr < 0.0:\n        raise ValueError(\"Relative and absolute thresholds must be non-negative.\")\n\n    largest_area = max(polygon.area for polygon in polygons)\n    filtered_polygons = [polygon for polygon in polygons if polygon.area >= max(rel_tr * largest_area, abs_tr)]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    max_area = max([polygon_area(polygon) for polygon in polygons])\n    rel_tr_area = rel_tr * max_area\n    return [polygon for polygon in polygons if polygon_area(polygon) > max(rel_tr_area, abs_tr)]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    if rel_tr < 0 or abs_tr < 0:\n        raise ValueError(\"Relative and absolute thresholds must be non-negative.\")\n\n    if rel_tr == 0 and abs_tr == 0:\n        raise ValueError(\"At least one threshold must be non-zero.\")\n\n    largest_polygon_area = max(polygon.area for polygon in polygons)\n    rel_tr_area = largest_polygon_area * rel_tr\n    filtered_polygons = [polygon for polygon in polygons if polygon.area >= rel_tr_area or polygon.area >= abs_tr]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Calculate the area of each polygon in the list\n    areas = [polygon.area for polygon in polygons]\n\n    # Calculate the relative threshold as a fraction of the largest polygon's area\n    rel_threshold = rel_tr * max(areas)\n\n    # Filter out polygons whose area is below either the absolute threshold or the relative threshold\n    filtered_polygons = [polygon for polygon, area in zip(polygons, areas) if area > abs_tr or area > rel_threshold]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    if rel_tr < 0 or abs_tr < 0:\n        raise ValueError(\"Thresholds must be non-negative.\")\n\n    max_area = np.max([p.area for p in polygons])\n    rel_tr_area = max_area * rel_tr\n    abs_tr_area = abs_tr\n\n    polygons = [p for p in polygons if p.area >= rel_tr_area or p.area >= abs_tr_area]\n\n    return polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    largest_polygon_area = max(\n        [\n            np.linalg.norm(np.diff(polygon, axis=0), axis=1).sum()\n            for polygon in polygons\n        ]\n    )\n\n    polygons = [\n        polygon\n        for polygon in polygons\n        if np.linalg.norm(np.diff(polygon, axis=0), axis=1).sum()\n        > max(rel_tr * largest_polygon_area, abs_tr)\n    ]\n\n    return polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    areas = [polygon_area(polygon) for polygon in polygons]\n    rel_tr_area = np.max(areas) * rel_tr\n    abs_tr_area = abs_tr\n\n    return [polygons[i] for i in range(len(polygons)) if areas[i] > max(rel_tr_area, abs_tr_area)]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    max_area = np.max([polygon_area(p) for p in polygons])\n\n    return [p for p in polygons if (polygon_area(p) > rel_tr * max_area) or (polygon_area(p) > abs_tr)]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    if rel_tr < 0.0 or abs_tr < 0.0:\n        raise ValueError(\"Threshold values must be non-negative.\")\n\n    # Calculate the area of each polygon\n    areas = [cv2.contourArea(polygon) for polygon in polygons]\n\n    # Calculate the relative threshold as a fraction of the largest polygon's area\n    rel_threshold = max(areas) * rel_tr\n\n    # Filter out polygons with an area below either the absolute or relative threshold\n    filtered_polygons = [polygon for polygon, area in zip(polygons, areas) if area >= rel_threshold or area >= abs_tr]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    max_area = np.max([np.abs(polygon_area(polygon)) for polygon in polygons])\n\n    if max_area == 0:\n        return polygons\n\n    return [\n        polygon\n        for polygon in polygons\n        if abs_tr < np.abs(polygon_area(polygon)) or rel_tr * max_area < np.abs(polygon_area(polygon))\n    ]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return []\n\n    max_area = max(polygon.area for polygon in polygons)\n    filtered_polygons = [polygon for polygon in polygons if polygon.area >= max(rel_tr * max_area, abs_tr)]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return []\n\n    rel_tr = np.clip(rel_tr, 0.0, 1.0)\n    abs_tr = np.clip(abs_tr, 0.0, np.inf)\n\n    areas = [polygon_area(polygon) for polygon in polygons]\n    max_area = np.max(areas)\n    rel_tr = rel_tr * max_area\n\n    if rel_tr < abs_tr:\n        rel_tr = abs_tr\n\n    return [polygon for polygon, area in zip(polygons, areas) if area >= rel_tr]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    rel_tr = max(rel_tr, 0.0)\n    abs_tr = max(abs_tr, 0.0)\n\n    if rel_tr == 0.0 and abs_tr == 0.0:\n        return polygons\n\n    areas = [polygon_area(polygon) for polygon in polygons]\n    largest_area = max(areas)\n\n    if largest_area == 0.0:\n        return polygons\n\n    rel_tr_area = largest_area * rel_tr\n    abs_tr_area = max(abs_tr, rel_tr_area)\n\n    filtered_polygons = [polygons[i] for i in range(len(polygons)) if areas[i] >= abs_tr_area]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    areas = np.array([polygon_area(polygon) for polygon in polygons])\n    largest_area = np.max(areas)\n\n    if largest_area == 0:\n        return polygons\n\n    rel_threshold = rel_tr * largest_area\n    abs_threshold = abs_tr\n\n    valid_indices = np.where(np.logical_and(areas >= abs_threshold, areas >= rel_threshold))[0]\n    valid_polygons = [polygons[i] for i in valid_indices]\n\n    return valid_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    rel_tr = np.clip(rel_tr, 0.0, 1.0)\n    abs_tr = np.clip(abs_tr, 0.0, np.inf)\n\n    if rel_tr == 0.0 and abs_tr == 0.0:\n        return polygons\n\n    if rel_tr > 0.0:\n        largest_area = np.max([p.area for p in polygons])\n        rel_tr = largest_area * rel_tr\n\n    if rel_tr == 0.0:\n        rel_tr = np.inf\n\n    return [p for p in polygons if p.area > min(rel_tr, abs_tr)]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Get the area of each polygon\n    areas = [np.abs(polygon_area(p)) for p in polygons]\n\n    # Get the largest area\n    largest_area = max(areas)\n\n    # Filter out polygons based on area\n    polygons = [polygons[i] for i in range(len(polygons)) if areas[i] >= rel_tr * largest_area or areas[i] >= abs_tr]\n\n    return polygons\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    num_samples_per_worker_dict = {i: num_samples_per_worker for i in range(num_workers)}\n\n    for i in range(num_samples_remaining):\n        num_samples_per_worker_dict[i] += 1\n\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n    num_samples_per_worker_batch = num_samples_per_worker // batch_size\n    num_samples_per_worker_remaining = num_samples_per_worker % batch_size\n    num_samples_per_worker_dict = {}\n    for worker_idx in range(num_workers):\n        num_samples_per_worker_dict[worker_idx] = num_samples_per_worker_batch * batch_size\n    for worker_idx in range(num_samples_remaining):\n        num_samples_per_worker_dict[worker_idx] += num_samples_per_worker_remaining\n    return num_samples_per_worker_dict"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    num_samples_per_worker_dict = {i: num_samples_per_worker for i in range(num_workers)}\n\n    for i in range(num_samples_remaining):\n        num_samples_per_worker_dict[i] += 1\n\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that will be left over after distributing the samples evenly among workers\n    num_samples_left_over = num_samples_yielded % num_workers\n\n    # Create a dictionary to store the number of samples each worker has processed\n    num_samples_per_worker_dict = {}\n\n    # Distribute the remaining samples evenly among workers\n    for i in range(num_samples_left_over):\n        num_samples_per_worker_dict[i] = num_samples_per_worker + 1\n\n    # Distribute the remaining samples evenly among workers\n    for i in range(num_samples_left_over, num_workers):\n        num_samples_per_worker_dict[i] = num_samples_per_worker\n\n    return num_samples_per_worker_dict"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    num_samples_per_worker_dict = {worker_idx: num_samples_per_worker for worker_idx in range(num_workers)}\n\n    for worker_idx in range(num_samples_remaining):\n        num_samples_per_worker_dict[worker_idx] += 1\n\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples to be distributed to each worker\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples to be distributed to each worker, taking into account the batch size\n    num_samples_per_worker = max(num_samples_per_worker // batch_size, 1) * batch_size\n\n    # Calculate the number of samples to be distributed to each worker, taking into account the batch size and the number of workers\n    num_samples_per_worker = max(num_samples_per_worker // num_workers, 1) * num_workers\n\n    # Calculate the number of samples to be distributed to each worker, taking into account the batch size and the number of workers\n    num_samples_per_worker = max(num_samples_per_worker // batch_size, 1) * batch_size\n\n    # Calculate the number of samples to be distributed to each worker, taking into account the batch size and the number of workers\n    num_samples_per_worker = max(num_samples_per_worker // num_workers, 1) * num_workers\n\n    # Calculate the number of samples to be distributed to each worker, taking into account the batch size and the number of workers\n    num_samples_per_worker = max(num_samples_per_worker // batch_size, 1) * batch_size\n\n    # Calculate the number of samples to be distributed to each worker, taking into account the batch size and the number of workers\n    num_samples_per_worker = max(num_samples_per_worker // num_workers, 1) * num_workers\n\n    # Calculate the number of samples to be distributed to each worker, taking into account the batch size and the number of workers\n    num_samples_per_worker = max(num_samples_per_worker // batch_size, 1) * batch_size\n\n    # Calculate the number of samples to be distributed to each worker, taking into account the batch size and the number of workers\n    num_s"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples that each worker should process\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that should be distributed to each worker\n    samples_per_worker_distributed = samples_per_worker * batch_size\n\n    # Calculate the number of samples that should be distributed to the last worker\n    samples_last_worker = num_samples_yielded - samples_per_worker_distributed\n\n    # Create a dictionary to store the number of samples processed by each worker\n    samples_per_worker_dict = {}\n\n    # Distribute the samples to the workers\n    for i in range(num_workers):\n        if i == num_workers - 1:\n            samples_per_worker_dict[i] = samples_last_worker\n        else:\n            samples_per_worker_dict[i] = samples_per_worker_distributed\n\n    return samples_per_worker_dict"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    samples_per_worker = num_samples_yielded // num_workers\n    samples_per_worker_remainder = num_samples_yielded % num_workers\n\n    samples_per_worker_dict = {i: samples_per_worker for i in range(num_workers)}\n\n    if samples_per_worker_remainder > 0:\n        for i in range(samples_per_worker_remainder):\n            samples_per_worker_dict[i] += 1\n\n    return samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples to be processed by each worker\n    num_samples_per_worker = num_samples_yielded // num_workers\n    # Calculate the number of samples that will be processed by the last worker\n    num_samples_last_worker = num_samples_yielded - num_samples_per_worker * (num_workers - 1)\n\n    # Create a dictionary to store the number of samples processed by each worker\n    num_samples_per_worker_dict = {}\n\n    # Distribute the samples to the workers\n    for worker_index in range(num_workers - 1):\n        num_samples_per_worker_dict[worker_index] = num_samples_per_worker\n\n    # Distribute the remaining samples to the last worker\n    num_samples_per_worker_dict[num_workers - 1] = num_samples_last_worker\n\n    return num_samples_per_worker_dict"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    samples_per_worker = num_samples_yielded // num_workers\n    num_samples_left = num_samples_yielded % num_workers\n\n    samples_per_worker_dict = {}\n    for worker_idx in range(num_workers):\n        samples_per_worker_dict[worker_idx] = samples_per_worker + (num_samples_left > 0)\n        num_samples_left -= 1\n\n    return samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples to be distributed to each worker\n    samples_per_worker = num_samples_yielded // num_workers\n    samples_per_worker_remainder = num_samples_yielded % num_workers\n\n    # Initialize the dictionary to store the number of samples each worker has processed\n    samples_per_worker_dict = {}\n\n    # Distribute the samples to each worker\n    for worker_index in range(num_workers):\n        # If there are remaining samples, distribute one more sample to the current worker\n        if samples_per_worker_remainder > 0:\n            samples_per_worker_dict[worker_index] = samples_per_worker + 1\n            samples_per_worker_remainder -= 1\n        # Otherwise, distribute the regular number of samples to the current worker\n        else:\n            samples_per_worker_dict[worker_index] = samples_per_worker\n\n    return samples_per_worker_dict"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_remainder = num_samples_yielded % num_workers\n    num_samples_per_worker_per_batch = num_samples_per_worker // batch_size\n    num_samples_per_worker_per_batch_remainder = num_samples_per_worker % batch_size\n\n    num_samples_per_worker_per_batch_dict = {\n        worker_idx: num_samples_per_worker_per_batch\n        for worker_idx in range(num_workers)\n    }\n\n    if num_samples_per_worker_remainder > 0:\n        for worker_idx in range(num_samples_per_worker_remainder):\n            num_samples_per_worker_per_batch_dict[worker_idx] += 1\n\n    if num_samples_per_worker_per_batch_remainder > 0:\n        for worker_idx in range(num_workers):\n            num_samples_per_worker_per_batch_dict[worker_idx] += 1\n\n    return num_samples_per_worker_per_batch_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n    num_samples_per_worker_per_batch = num_samples_per_worker // batch_size\n    num_samples_remaining_per_worker = num_samples_per_worker % batch_size\n    num_samples_per_worker_per_batch_remaining = num_samples_remaining // batch_size\n    num_samples_remaining_per_worker_remaining = num_samples_remaining % batch_size\n\n    num_samples_per_worker_per_batch_remaining_dict = {\n        worker_idx: num_samples_per_worker_per_batch_remaining\n        for worker_idx in range(num_workers)\n    }\n    num_samples_remaining_per_worker_remaining_dict = {\n        worker_idx: num_samples_remaining_per_worker_remaining\n        for worker_idx in range(num_workers)\n    }\n\n    for worker_idx in range(num_workers):\n        num_samples_per_worker_per_batch_remaining_dict[\n            worker_idx\n        ] += num_samples_remaining_per_worker_remaining_dict[worker_idx]\n        num_samples_remaining_per_worker_remaining_dict[\n            worker_idx\n        ] = min(\n            num_samples_remaining_per_worker_remaining_dict[worker_idx],\n            batch_size - num_samples_per_worker_per_batch_remaining_dict[worker_idx],\n        )\n\n    num_samples_per_worker_dict = {\n        worker_idx: num_samples_per_worker_per_batch * batch_size\n        + num_samples_per_worker_per_batch_remaining_dict[worker_idx]\n        for worker_idx in range(num_workers)"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker will process\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that will be left over after distributing the samples evenly among workers\n    num_samples_left_over = num_samples_yielded % num_workers\n\n    # Initialize a dictionary to store the number of samples each worker has processed\n    num_samples_per_worker_dict = {}\n\n    # Distribute the remaining samples evenly among workers\n    for worker_idx in range(num_workers):\n        # If there are remaining samples, distribute one more sample to the current worker\n        if num_samples_left_over > 0:\n            num_samples_per_worker_dict[worker_idx] = num_samples_per_worker + 1\n            num_samples_left_over -= 1\n        # Otherwise, distribute the regular number of samples to the current worker\n        else:\n            num_samples_per_worker_dict[worker_idx] = num_samples_per_worker\n\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples per worker\n    samples_per_worker = num_samples_yielded // num_workers\n    # Calculate the number of samples that will be left over after distributing the samples evenly\n    remainder = num_samples_yielded % num_workers\n\n    # Initialize a dictionary to store the number of samples each worker has processed\n    samples_per_worker_dict = {}\n\n    # Distribute the samples evenly among the workers\n    for i in range(num_workers):\n        # If there are still samples left, distribute one more sample to the current worker\n        if remainder > 0:\n            samples_per_worker_dict[i] = samples_per_worker + 1\n            remainder -= 1\n        else:\n            samples_per_worker_dict[i] = samples_per_worker\n\n    return samples_per_worker_dict"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed\n    samples_per_worker = num_samples_yielded // num_workers\n    samples_per_worker = max(1, samples_per_worker)\n    samples_per_worker = min(samples_per_worker, batch_size)\n    samples_per_worker = int(samples_per_worker)\n\n    # Calculate the number of remaining samples\n    remaining_samples = num_samples_yielded % num_workers\n\n    # Distribute the remaining samples evenly among the workers\n    samples_per_worker += remaining_samples // num_workers\n\n    # Create a dictionary to store the number of samples each worker has processed\n    samples_per_worker_dict = {}\n    for i in range(num_workers):\n        samples_per_worker_dict[i] = samples_per_worker\n\n    # Distribute any remaining samples among the workers\n    for i in range(remaining_samples % num_workers):\n        samples_per_worker_dict[i] += 1\n\n    return samples_per_worker_dict"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples per worker, taking into account the batch size and the number of workers\n    samples_per_worker = num_samples_yielded // batch_size // num_workers\n\n    # Calculate the number of remaining samples after distributing the samples evenly among workers\n    remaining_samples = num_samples_yielded - samples_per_worker * batch_size * num_workers\n\n    # Create a dictionary to store the number of samples processed by each worker\n    samples_per_worker_dict = {}\n\n    # Distribute the remaining samples evenly among the workers\n    for i in range(num_workers):\n        # Calculate the number of samples to be distributed to the current worker\n        samples_to_distribute = samples_per_worker + (remaining_samples > 0)\n\n        # Update the dictionary with the number of samples processed by the current worker\n        samples_per_worker_dict[i] = samples_to_distribute\n\n        # Decrement the remaining samples counter\n        remaining_samples -= 1\n\n    # Return the dictionary containing the number of samples processed by each worker\n    return samples_per_worker_dict"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker should process\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that will be left over\n    leftover_samples = num_samples_yielded % num_workers\n\n    # Create a dictionary to store the number of samples each worker should process\n    samples_per_worker_dict = {}\n\n    # Distribute the samples evenly among the workers\n    for i in range(num_workers):\n        # Calculate the number of samples to assign to the current worker\n        num_samples = samples_per_worker\n        if leftover_samples > 0:\n            num_samples += 1\n            leftover_samples -= 1\n        samples_per_worker_dict[i] = num_samples\n\n    # Return the dictionary of samples per worker\n    return samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    num_samples_per_worker = num_samples_per_worker // batch_size\n    num_samples_remaining = num_samples_remaining // batch_size\n\n    num_samples_per_worker += num_samples_remaining\n\n    num_samples_per_worker_dict = {i: num_samples_per_worker for i in range(num_workers)}\n\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    samples_per_worker = num_samples_yielded // num_workers\n    samples_per_worker_remainder = num_samples_yielded % num_workers\n\n    # Distribute the remainder samples evenly among the workers\n    samples_per_worker_remainder_per_worker = samples_per_worker_remainder // num_workers\n    samples_per_worker_remainder_remainder = samples_per_worker_remainder % num_workers\n\n    samples_per_worker_dict = {}\n    for worker_index in range(num_workers):\n        num_samples_for_worker = samples_per_worker + samples_per_worker_remainder_per_worker\n        if worker_index < samples_per_worker_remainder_remainder:\n            num_samples_for_worker += 1\n        samples_per_worker_dict[worker_index] = num_samples_for_worker\n\n    return samples_per_worker_dict\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            if metadatas is not None:\n                filtered_metadatas.append(metadatas[i])\n            else:\n                filtered_metadatas.append(None)\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if not metadatas:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if not isinstance(results, list):\n        raise TypeError(\"results must be a list\")\n    if not isinstance(value, list):\n        raise TypeError(\"value must be a list\")\n    if not isinstance(threshold, (int, float)):\n        raise TypeError(\"threshold must be a numeric value\")\n    if metadatas is not None and not isinstance(metadatas, list):\n        raise TypeError(\"metadatas must be a list\")\n\n    if len(results) != len(value):\n        raise ValueError(\"results and value must have the same length\")\n    if metadatas is not None and len(results) != len(metadatas):\n        raise ValueError(\"results and metadatas must have the same length\")\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            if metadatas is not None:\n                filtered_metadatas.append(metadatas[i])\n            else:\n                filtered_metadatas.append(None)\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            if metadatas is not None:\n                filtered_metadatas.append(metadatas[i])\n            else:\n                filtered_metadatas.append(None)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if not metadatas:\n        metadatas = [None] * len(results)\n    filtered_results = []\n    filtered_metadatas = []\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            filtered_metadatas.append(metadatas[i])\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if len(results) != len(value):\n        raise ValueError(\"Results and values must have the same length.\")\n    if metadatas is None:\n        metadatas = [None] * len(results)\n    if len(results) != len(metadatas):\n        raise ValueError(\"Results and metadatas must have the same length.\")\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if not isinstance(results, list):\n        raise ValueError(\"results must be a list\")\n    if not isinstance(value, list):\n        raise ValueError(\"value must be a list\")\n    if not isinstance(threshold, (int, float)):\n        raise ValueError(\"threshold must be a numeric value\")\n    if metadatas is not None and not isinstance(metadatas, list):\n        raise ValueError(\"metadatas must be a list\")\n    if len(results) != len(value):\n        raise ValueError(\"results and value must have the same length\")\n    if metadatas is not None and len(results) != len(metadatas):\n        raise ValueError(\"results and metadatas must have the same length\")\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            if metadatas is not None:\n                filtered_metadatas.append(metadatas[i])\n            else:\n                filtered_metadatas.append(None)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    assert len(results) == len(value) == len(metadatas), \"The input lists must have the same length\"\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, val, metadata in zip(results, value, metadatas):\n        if val <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if not results:\n        return [], []\n    if not metadatas:\n        metadatas = [None] * len(results)\n    filtered_results, filtered_metadatas = [], []\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadatas[i])\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    # Initialize empty lists to store the filtered results and metadatas\n    filtered_results = []\n    filtered_metadatas = []\n\n    # Iterate over the results and values\n    for result, val, metadata in zip(results, value, metadatas):\n        # Check if the value is less than or equal to the threshold\n        if val <= threshold:\n            # If the value is less than or equal to the threshold, append the result and metadata to the filtered lists\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    # Return the filtered lists\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if not metadatas:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if len(results) != len(value):\n        raise ValueError(\"The length of the results list must be equal to the length of the value list.\")\n\n    if metadatas is None:\n        metadatas = [None] * len(results)\n    elif len(metadatas) != len(results):\n        raise ValueError(\"The length of the metadatas list must be equal to the length of the results list.\")\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    # Check if the input lists have the same length\n    if len(results) != len(value):\n        raise ValueError(\"The input lists 'results' and 'value' must have the same length.\")\n\n    # If metadatas are not provided, create a list of None values with the same length as results\n    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    # Check if the input lists have the same length\n    if len(results) != len(metadatas):\n        raise ValueError(\"The input lists 'results' and 'metadatas' must have the same length.\")\n\n    # Filter the results and metadatas based on the threshold\n    filtered_results = [result for result, value, metadata in zip(results, value, metadatas) if value <= threshold]\n    filtered_metadatas = [metadata for result, value, metadata in zip(results, value, metadatas) if value <= threshold]\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    assert len(results) == len(value) == len(metadatas)\n\n    filtered_results = []\n    filtered_metadatas = []\n    for i, v in enumerate(value):\n        if v <= threshold:\n            filtered_results.append(results[i])\n            filtered_metadatas.append(metadatas[i])\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if not results or not value:\n        return [], []\n\n    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    if len(results) != len(value) or len(results) != len(metadatas):\n        raise ValueError(\"Input lists must have the same length.\")\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    # Ensure that the input lists have the same length\n    assert len(results) == len(value), \"The results and value lists must have the same length.\"\n\n    # Initialize the output lists\n    filtered_results = []\n    filtered_metadatas = []\n\n    # Iterate through the results and values\n    for result, val, metadata in zip(results, value, metadatas):\n        # If the value is less than or equal to the threshold, add it to the output lists\n        if val <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    # Return the filtered lists\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the expected shape\n    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    # Calculate the area using the Shoelace formula\n    area = 0.5 * np.abs(\n        np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1))\n    )\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x, y = array.T\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the expected shape\n    if array.shape[1] != 2:\n        raise ValueError(\n            \"Input array must have shape (_, 2), where _ can be any number of points.\"\n        )\n\n    # Calculate the area using the Shoelace formula\n    n = len(array)\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area = abs(area) / 2.0\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the correct shape\n    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    # Calculate the area using the Shoelace formula\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i, 0] * array[j, 1]\n        area -= array[j, 0] * array[i, 1]\n    area = abs(area) / 2.0\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2).\")\n\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area = abs(area) / 2.0\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"Input array must have shape (_, 2), but has shape {array.shape}\")\n\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i, 0] * array[j, 1]\n        area -= array[j, 0] * array[i, 1]\n    area = abs(area) / 2.0\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the expected shape\n    if array.shape[1] != 2:\n        raise ValueError(\n            \"The input array must have shape (_, 2), where _ can be any number of points.\"\n        )\n\n    # Calculate the area using the Shoelace formula\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area = abs(area) / 2.0\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the correct shape\n    if array.shape[1] != 2:\n        raise ValueError(\n            f\"Input array must have shape (_, 2), but has shape {array.shape}\"\n        )\n\n    # Calculate the area using the Shoelace formula\n    n = len(array)\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area = abs(area) / 2.0\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2), where _ can be any number of points.\")\n\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i, 0] * array[j, 1]\n        area -= array[j, 0] * array[i, 1]\n    area = abs(area) / 2.0\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the correct shape\n    if array.shape[1] != 2:\n        raise ValueError(\n            f\"Input array must have shape (_, 2), but has shape {array.shape}.\"\n        )\n\n    # Calculate the area using the Shoelace formula\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area = abs(area) / 2.0\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f'Input array must have shape (_, 2), where _ can be any number of points.')\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Invalid input shape. Expected shape: (_, 2).\")\n\n    # Calculate the area using the Shoelace formula\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area /= 2.0\n\n    return abs(area)"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"Invalid shape: {array.shape}\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2).\")\n\n    x, y = array[:, 0], array[:, 1]\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2).\")\n\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i, 0] * array[j, 1]\n        area -= array[j, 0] * array[i, 1]\n    area = abs(area) / 2.0\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2), where _ can be any number of points.\")\n\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area = abs(area) / 2.0\n    return area\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\n            \"Input array must have shape (_, 2), where _ is any number of points.\"\n        )\n\n    # Calculate the area using the Shoelace formula\n    area = 0.5 * np.abs(\n        np.dot(array[:, 0], np.roll(array[:, 1], 1))\n        - np.dot(array[:, 1], np.roll(array[:, 0], 1))\n    )\n\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if not array.shape[1] == 2:\n        raise ValueError(f\"array.shape[1] must be 2, but it is {array.shape[1]}\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\n            f\"The input array must have shape (_, 2), but it has shape {array.shape}.\"\n        )\n\n    # Calculate the area using the Shoelace formula\n    area = 0.5 * np.abs(\n        np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1))\n    )\n\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the expected shape\n    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    # Calculate the area using the Shoelace formula\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area = abs(area) / 2.0\n\n    return area\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Create a copy of the input tensor a\n    a_copy = a.clone()\n\n    # Initialize the output tensors\n    idx_lo = torch.zeros(v.shape[:-1], dtype=torch.long, device=a.device)\n    idx_hi = torch.zeros(v.shape[:-1], dtype=torch.long, device=a.device)\n\n    # Iterate over the last dimension of v\n    for i in range(v.shape[-1]):\n        # Find the indices where the element in v at position i should be inserted into a\n        idx = torch.searchsorted(a_copy, v[..., i])\n\n        # Update the output tensors\n        idx_lo[..., i] = idx\n        idx_hi[..., i] = idx + 1\n\n        # Update the copy of a\n        a_copy = torch.cat((a_copy[:idx], a_copy[idx:]), dim=-1)\n\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Define the shape of the output tensors\n    shape = list(v.shape)\n    shape[-1] = 1\n    idx_lo = torch.zeros(shape, dtype=torch.long, device=v.device)\n    idx_hi = torch.zeros(shape, dtype=torch.long, device=v.device)\n\n    # Iterate over the last dimension of v\n    for i in range(v.shape[-1]):\n\n        # Get the current query point\n        query = v[..., i]\n\n        # Find the indices where the query point could be inserted into a\n        idx_lo[..., i] = torch.searchsorted(a, query, side=\"left\")\n        idx_hi[..., i] = torch.searchsorted(a, query, side=\"right\")\n\n    # Return the indices where the query points could be inserted into a\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if v is a scalar\n    if v.ndim == 0:\n        v = v.unsqueeze(0)\n\n    # Check if v is a vector\n    if v.ndim == 1:\n        v = v.unsqueeze(1)\n\n    # Check if a and v have the same shape\n    if a.shape[:-1] != v.shape[:-1]:\n        raise ValueError(\"a and v must have the same shape, except for the last dimension\")\n\n    # Check if a is a vector\n    if a.ndim == 1:\n        a = a.unsqueeze(1)\n\n    # Check if a and v have the same shape\n    if a.shape[:-1] != v.shape[:-1]:\n        raise ValueError(\"a and v must have the same shape, except for the last dimension\")\n\n    # Check if a and v have the same type\n    if a.dtype != v.dtype:\n        raise ValueError(\"a and v must have the same type\")\n\n    # Check if a and v have the same device\n    if a.device != v.device:\n        raise ValueError(\"a and v must have the same device\")\n\n    # Check if a and v have the same dtype\n    if a.dtype != v.dtype:\n        raise ValueError(\"a and v must have the same dtype\")\n\n    # Check if a and v have the same device\n    if a.device != v.device:\n        raise ValueError(\"a and v must have the same device\")\n\n    # Check if a and v have the same dtype\n    if a.dtype != v.dtype:\n        raise ValueError(\"a and v must have the same dtype\")\n\n    # Check if a and v have the same device\n    if a.device != v.device:\n        raise ValueError(\"a and v must have the same device\")\n\n    # Check if a and v have the same dtype\n    if a.dtype != v.dtype:\n        raise ValueError(\"a and v must have the same dtype\")"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    assert a.ndim == v.ndim\n    assert a.shape[:-1] == v.shape[:-1]\n    assert a.shape[-1] >= 1\n    assert v.shape[-1] >= 1\n\n    v_expand = v.unsqueeze(-2)\n    a_expand = a.unsqueeze(-1)\n\n    # find the indices where elements of v should be inserted into a\n    idx_lo = torch.searchsorted(a_expand, v_expand, right=False)\n    idx_hi = torch.searchsorted(a_expand, v_expand, right=True)\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the input tensors are of the correct shape and type\n    if not isinstance(a, torch.Tensor):\n        raise TypeError(\"Input tensor a must be a torch.Tensor\")\n    if not isinstance(v, torch.Tensor):\n        raise TypeError(\"Input tensor v must be a torch.Tensor\")\n    if len(a.shape) != len(v.shape):\n        raise ValueError(\"Input tensors a and v must have the same number of dimensions\")\n    if a.shape[-1] != v.shape[-1]:\n        raise ValueError(\"The last dimension of input tensors a and v must be equal\")\n    if a.dtype != v.dtype:\n        raise ValueError(\"Input tensors a and v must have the same data type\")\n\n    # Check if the input tensors are sorted\n    if not torch.all(torch.diff(a, dim=-1) >= 0):\n        raise ValueError(\"Input tensor a must be sorted\")\n\n    # Find the indices where elements of v should be inserted into a\n    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n\n    # If an element in v is out of the range of a, set idx_lo and idx_hi to either the first or last index of a\n    idx_lo[idx_lo == a.shape[-1]] = 0\n    idx_hi[idx_hi == a.shape[-1]] = a.shape[-1]\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Get the shape of the input tensors\n    a_shape = a.shape\n    v_shape = v.shape\n\n    # Check if the last dimension of v is compatible with a\n    if v_shape[-1] != a_shape[-1]:\n        raise ValueError(\"The last dimension of v is not compatible with a.\")\n\n    # Reshape a and v to 2D tensors\n    a = a.reshape(-1, a_shape[-1])\n    v = v.reshape(-1, v_shape[-1])\n\n    # Initialize idx_lo and idx_hi tensors\n    idx_lo = torch.zeros(v.shape[0], dtype=torch.int64, device=a.device)\n    idx_hi = torch.zeros(v.shape[0], dtype=torch.int64, device=a.device)\n\n    # Iterate over the elements in v\n    for i in range(v.shape[0]):\n        # Get the query point\n        query = v[i]\n\n        # Find the indices where the query point should be inserted into a\n        idx = torch.searchsorted(a, query, side='right')\n\n        # Update idx_lo and idx_hi\n        idx_lo[i] = idx - 1\n        idx_hi[i] = idx\n\n    # Reshape idx_lo and idx_hi to the original shape of a and v\n    idx_lo = idx_lo.reshape(v_shape[:-1] + a_shape[:-1])\n    idx_hi = idx_hi.reshape(v_shape[:-1] + a_shape[:-1])\n\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the input tensors are of the correct shape\n    if a.ndim != 1:\n        raise ValueError(\"Input tensor a must be 1D.\")\n    if v.ndim != 1:\n        raise ValueError(\"Input tensor v must be 1D.\")\n    if a.shape[0] != v.shape[0]:\n        raise ValueError(\"Input tensors a and v must have the same number of elements.\")\n\n    # Create a copy of a and v to avoid modifying the original tensors\n    a_copy = a.clone()\n    v_copy = v.clone()\n\n    # Sort the copy of a in ascending order\n    a_copy.sort()\n\n    # Create two tensors to store the indices where elements of v should be inserted into a\n    idx_lo = torch.zeros(v.shape[0], dtype=torch.int64, device=a.device)\n    idx_hi = torch.zeros(v.shape[0], dtype=torch.int64, device=a.device)\n\n    # Iterate over each element in v\n    for i in range(v.shape[0]):\n        # Find the indices where the element should be inserted into a\n        idx_lo[i], idx_hi[i] = torch.searchsorted(a_copy, v_copy[i], side=\"left\")\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    if not isinstance(a, torch.Tensor):\n        raise TypeError(\"Input a must be a torch.Tensor.\")\n    if not isinstance(v, torch.Tensor):\n        raise TypeError(\"Input v must be a torch.Tensor.\")\n    if len(a.shape) < 1:\n        raise ValueError(\"Input a must have at least 1 dimension.\")\n    if len(v.shape) < 1:\n        raise ValueError(\"Input v must have at least 1 dimension.\")\n    if a.shape[-1] != v.shape[-1]:\n        raise ValueError(\"The last dimension of a and v must be equal.\")\n    if not a.is_cuda:\n        raise ValueError(\"Input a must be a CUDA tensor.\")\n    if not v.is_cuda:\n        raise ValueError(\"Input v must be a CUDA tensor.\")\n\n    # Compute the number of elements in a and v\n    num_a = a.shape[0]\n    num_v = v.shape[0]\n\n    # Create a tensor to store the indices of the lower and upper bounds\n    idx_lo = torch.zeros(num_v, dtype=torch.int64, device=a.device)\n    idx_hi = torch.zeros(num_v, dtype=torch.int64, device=a.device)\n\n    # Compute the indices of the lower and upper bounds using CUDA\n    _searchsorted.searchsorted(a, v, idx_lo, idx_hi)\n\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the input tensors are sorted\n    if not torch.all(a[:-1] <= a[1:]):\n        raise ValueError(\"Input tensor a must be sorted.\")\n\n    # Check if the shapes of a and v are compatible\n    if len(a.shape) != len(v.shape):\n        raise ValueError(\"Input tensors a and v must have the same number of dimensions.\")\n\n    # Compute the number of dimensions\n    ndim = len(a.shape)\n\n    # Initialize the output tensors\n    idx_lo = torch.zeros(v.shape[:-1] + (1,), dtype=torch.long, device=a.device)\n    idx_hi = torch.zeros(v.shape[:-1] + (1,), dtype=torch.long, device=a.device)\n\n    # Iterate over the last dimension of v\n    for i in range(v.shape[-1]):\n        # Extract the current element of v\n        curr_v = v[..., i]\n\n        # Find the indices where the element should be inserted into a\n        idx_lo[..., 0] = torch.searchsorted(a, curr_v, side=\"left\")\n        idx_hi[..., 0] = torch.searchsorted(a, curr_v, side=\"right\")\n\n        # Update the output tensors\n        idx_lo = idx_lo.expand(v.shape[:-1] + (ndim,))\n        idx_hi = idx_hi.expand(v.shape[:-1] + (ndim,))\n\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Initialize the output tensors\n    idx_lo = torch.zeros(v.shape[:-1], dtype=torch.int64, device=a.device)\n    idx_hi = torch.zeros(v.shape[:-1], dtype=torch.int64, device=a.device)\n\n    # Check if the input tensors are valid\n    if v.shape[-1] != a.shape[-1]:\n        raise ValueError(\"The last dimension of v must be the same as the last dimension of a.\")\n\n    # Check if a is sorted\n    if not torch.all(torch.diff(a, dim=-1) >= 0):\n        raise ValueError(\"a must be sorted along the last dimension.\")\n\n    # Check if v is sorted\n    if not torch.all(torch.diff(v, dim=-1) >= 0):\n        raise ValueError(\"v must be sorted along the last dimension.\")\n\n    # Iterate over the last dimension of v\n    for i in range(v.shape[-1]):\n        # Find the indices where v[:, i] should be inserted into a\n        idx = torch.searchsorted(a, v[:, i], right=True)\n\n        # Update the output tensors\n        idx_lo += idx\n        idx_hi += idx\n\n    # Return the output tensors\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the input tensors have the same shape except for the last dimension\n    if a.shape[:-1] != v.shape[:-1]:\n        raise ValueError(\"Input tensors must have the same shape except for the last dimension.\")\n\n    # Check if the input tensor is sorted along the last dimension\n    if not torch.all(torch.diff(a, dim=-1) >= 0):\n        raise ValueError(\"Input tensor must be sorted along the last dimension.\")\n\n    # Compute the lower and upper bounds for each element in v\n    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the input tensors are sorted\n    if not torch.all(a[:-1] <= a[1:]):\n        raise ValueError(\"Input tensor a is not sorted.\")\n\n    # Check if the input tensors have the same shape except for the last dimension\n    if v.shape[:-1] != a.shape[:-1]:\n        raise ValueError(\"Input tensors do not have the same shape except for the last dimension.\")\n\n    # Check if the input tensors have the same device and dtype\n    if a.device != v.device or a.dtype != v.dtype:\n        raise ValueError(\"Input tensors do not have the same device and dtype.\")\n\n    # Create a tensor of ones with the same shape as v\n    ones = torch.ones(v.shape, device=v.device, dtype=v.dtype)\n\n    # Use binary search to find the indices where elements of v should be inserted into a\n    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n\n    # If an element in v is out of the range of a, idx_lo and idx_hi will point to either the first or last index of a\n    idx_lo = torch.where(idx_lo >= a.shape[-1], a.shape[-1] - ones, idx_lo)\n    idx_hi = torch.where(idx_hi <= 0, ones, idx_hi)\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the input tensors are valid\n    if not isinstance(a, torch.Tensor) or not isinstance(v, torch.Tensor):\n        raise TypeError(\"Both input tensors must be torch.Tensor objects.\")\n    if a.dim() < 1 or v.dim() < 1:\n        raise ValueError(\"Both input tensors must have at least one dimension.\")\n    if a.shape[-1] != v.shape[-1]:\n        raise ValueError(\"The last dimension of a and v must be the same.\")\n\n    # Get the shape of the input tensors\n    a_shape = a.shape\n    v_shape = v.shape\n\n    # Reshape the input tensors to 2D tensors\n    a = a.reshape(-1, a_shape[-1])\n    v = v.reshape(-1, v_shape[-1])\n\n    # Initialize the output tensors\n    idx_lo = torch.zeros(v.shape[0], dtype=torch.long, device=a.device)\n    idx_hi = torch.zeros(v.shape[0], dtype=torch.long, device=a.device)\n\n    # Iterate over the elements of v\n    for i in range(v.shape[0]):\n        # Get the element of v\n        element = v[i]\n\n        # Find the indices where element could be inserted into a\n        idx_lo[i], idx_hi[i] = torch.searchsorted(a, element, side=\"left\")\n\n        # If element is out of the range of a, set idx_lo and idx_hi to the first or last index of a\n        if idx_lo[i] == a.shape[0]:\n            idx_lo[i] -= 1\n            idx_hi[i] -= 1\n\n    # Reshape the output tensors back to the original shape of the input tensors\n    idx_lo = idx_lo.reshape(v_shape[:-1])\n    idx_hi = idx_hi.reshape(v_shape[:-1])\n\n   "}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check the shapes of a and v\n    if a.ndim != v.ndim:\n        raise ValueError(\"a and v must have the same number of dimensions\")\n    if a.shape[-1] != v.shape[-1]:\n        raise ValueError(\"the last dimension of a and v must be equal\")\n\n    # Create a new tensor to store the indices\n    idx_lo = torch.zeros(v.shape[:-1], dtype=torch.int64, device=a.device)\n    idx_hi = torch.zeros(v.shape[:-1], dtype=torch.int64, device=a.device)\n\n    # Iterate over the last dimension of v\n    for i in range(v.shape[-1]):\n        # Compare the elements of v to the elements of a along the last dimension\n        idx_lo += torch.where(v[..., i] <= a[..., i], torch.ones_like(a[..., i]), torch.zeros_like(a[..., i]))\n        idx_hi += torch.where(v[..., i] >= a[..., i], torch.ones_like(a[..., i]), torch.zeros_like(a[..., i]))\n\n    # Return the indices\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the input tensors are 1D or 2D\n    if a.dim() == 1:\n        a = a.unsqueeze(0)\n    if v.dim() == 1:\n        v = v.unsqueeze(0)\n\n    # Check if the shapes of the input tensors are compatible\n    if a.shape[-1] != v.shape[-1]:\n        raise ValueError(\"The last dimension of a and v must be the same.\")\n\n    # Compute the indices where the elements of v should be inserted into a\n    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n\n    # Clip the indices to ensure they are within the valid range of a\n    idx_lo = torch.clamp(idx_lo, 0, a.shape[0] - 1)\n    idx_hi = torch.clamp(idx_hi, 0, a.shape[0] - 1)\n\n    return idx_lo, idx_hi\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check input tensors\n    if not isinstance(a, torch.Tensor):\n        raise TypeError(\"Input 'a' must be a torch.Tensor.\")\n    if not isinstance(v, torch.Tensor):\n        raise TypeError(\"Input 'v' must be a torch.Tensor.\")\n    if a.ndim < 1:\n        raise ValueError(\"Input 'a' must have at least 1 dimension.\")\n    if v.ndim < 1:\n        raise ValueError(\"Input 'v' must have at least 1 dimension.\")\n    if a.shape[-1] != v.shape[-1]:\n        raise ValueError(\"The last dimension of 'a' and 'v' must be the same.\")\n\n    # Check if 'a' is sorted\n    if not torch.all(a[:-1] <= a[1:]):\n        raise ValueError(\"Input 'a' must be sorted.\")\n\n    # Check if 'v' is sorted\n    if not torch.all(v[:-1] <= v[1:]):\n        raise ValueError(\"Input 'v' must be sorted.\")\n\n    # Create a copy of 'a' and 'v' to avoid modifying the original tensors\n    a_copy = a.clone()\n    v_copy = v.clone()\n\n    # Expand 'a' and 'v' to the same shape\n    if a_copy.shape != v_copy.shape:\n        max_dim = max(a_copy.ndim, v_copy.ndim)\n        for _ in range(max_dim - a_copy.ndim):\n            a_copy = a_copy.unsqueeze(-1)\n        for _ in range(max_dim - v_copy.ndim):\n            v_copy = v_copy.unsqueeze(-1)\n\n    # Create a mask to indicate the positions where 'v' is greater than 'a'\n    mask = v_copy > a_copy\n\n    # Initialize 'idx_lo' and 'idx_hi' tensors with zeros\n    idx_lo = torch.zeros(v_copy.shape[:"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the input tensors are of the correct shape and type\n    assert a.ndim >= 1 and v.ndim >= 1 and a.dtype == v.dtype, \"Invalid input tensors\"\n    assert a.shape[:-1] == v.shape[:-1], \"Input tensors must have the same shape except for the last dimension\"\n\n    # Get the shape of the input tensors\n    a_shape = a.shape\n    v_shape = v.shape\n\n    # Reshape the input tensors to 2D tensors\n    a = a.reshape(-1, a_shape[-1])\n    v = v.reshape(-1, v_shape[-1])\n\n    # Create a new tensor to store the indices\n    idx = torch.zeros(v.shape[0], dtype=torch.int64, device=a.device)\n\n    # Iterate over the elements of v\n    for i in range(v.shape[0]):\n        # Find the indices where the element of v should be inserted into a\n        idx_lo = torch.searchsorted(a, v[i], side='left')\n        idx_hi = torch.searchsorted(a, v[i], side='right')\n\n        # Store the indices in the idx tensor\n        idx[i] = idx_lo\n\n    # Reshape the idx tensor to the original shape of the input tensors\n    idx = idx.reshape(v_shape[:-1])\n\n    return idx\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ], dtype = xnp.float32)"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n    [\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n    ],\n    dtype = xnp.float32,\n  )"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1]\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ], dtype=np.float32)"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ], dtype=np.float32)"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n    [\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n    ],\n    dtype = xnp.float64,\n  )"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # Generate the intrinsic matrix\n  intrinsic_matrix = xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])\n\n  return intrinsic_matrix"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # Generate the matrix\n  matrix = xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n\n  # Return the matrix\n  return matrix"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # Generate the intrinsic matrix\n  intrinsics = xnp.array(\n    [\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n    ],\n    dtype=xnp.float32,\n  )\n\n  return intrinsics"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ], dtype=np.float32)\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # Initialize the matrix with zeros\n  matrix = xnp.zeros((3, 3), dtype=xnp.float32)\n\n  # Set the focal lengths and optical center\n  matrix[0, 0] = fx\n  matrix[1, 1] = fy\n  matrix[0, 2] = cx\n  matrix[1, 2] = cy\n\n  # Set the remaining values\n  matrix[2, 2] = 1\n\n  return matrix"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # Generate the intrinsic matrix\n  intrinsic_matrix = xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n\n  # Return the matrix\n  return intrinsic_matrix\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # Create the matrix\n  intrinsic_matrix = xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ], dtype = xnp.float32)\n\n  # Return the matrix\n  return intrinsic_matrix\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ], dtype = xnp.float32)\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=xnp.float32)\n\n"}
{"namespace": "coord.contract", "completion": "  return x / (1 + np.linalg.norm(x, axis=-1, keepdims=True))**2"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the points\n  mag_squared = np.sum(x ** 2, axis=-1, keepdims=True)\n\n  # Calculate the scaling factor\n  scaling_factor = np.sqrt(mag_squared)\n\n  # Scale the points towards the origin\n  x_contracted = x / scaling_factor\n\n  return x_contracted"}
{"namespace": "coord.contract", "completion": "  return x * (1 - np.sqrt(np.sum(x**2, axis=-1, keepdims=True)))\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input points\n  mag_sq = np.sum(x**2, axis=-1, keepdims=True)\n\n  # Calculate the scaling factor based on the magnitude squared\n  scale_factor = np.sqrt(1 - 1 / (mag_sq + 1))\n\n  # Scale the input points towards the origin\n  x_contracted = x * scale_factor\n\n  return x_contracted"}
{"namespace": "coord.contract", "completion": "  return x * (1 - np.linalg.norm(x, axis=-1, keepdims=True) ** 2) ** 0.5\n"}
{"namespace": "coord.contract", "completion": "  x_mag_squared = np.sum(x ** 2, axis=-1)\n  x_mag = np.sqrt(x_mag_squared)\n  x_mag_3 = x_mag ** 3\n  x_mag_4 = x_mag ** 4\n  x_mag_5 = x_mag ** 5\n  x_mag_6 = x_mag ** 6\n  x_mag_7 = x_mag ** 7\n  x_mag_8 = x_mag ** 8\n  x_mag_9 = x_mag ** 9\n  x_mag_10 = x_mag ** 10\n  x_mag_11 = x_mag ** 11\n  x_mag_12 = x_mag ** 12\n  x_mag_13 = x_mag ** 13\n  x_mag_14 = x_mag ** 14\n  x_mag_15 = x_mag ** 15\n  x_mag_16 = x_mag ** 16\n  x_mag_17 = x_mag ** 17\n  x_mag_18 = x_mag ** 18\n  x_mag_19 = x_mag ** 19\n  x_mag_20 = x_mag ** 20\n  x_mag_21 = x_mag ** 21\n  x_mag_22 = x_mag ** 22\n  x_mag_23 = x_mag ** 23\n  x_mag_24 = x_mag ** 24\n  x_mag_25 = x_mag ** 25\n  x_mag_26 = x_mag ** 26\n  x_mag_27 = x_mag ** 27\n  x_mag_28 = x_mag ** 28\n  x_mag_29 = x_mag ** 29\n  x_mag_30 = x_mag ** 30\n  x_mag_31 = x_mag ** 31\n  x_mag_32 = x"}
{"namespace": "coord.contract", "completion": "  return x * (1 - (1 / (1 + np.linalg.norm(x)**2)))\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input points\n  magnitude_squared = np.sum(x ** 2, axis=-1, keepdims=True)\n\n  # Calculate the scaling factor based on the magnitude squared\n  scaling_factor = np.sqrt(1 - magnitude_squared)\n\n  # Scale the input points towards the origin\n  contracted_points = x * scaling_factor\n\n  return contracted_points\n"}
{"namespace": "coord.contract", "completion": "  return x * (1 - np.linalg.norm(x, axis=-1, keepdims=True) ** 2) ** 0.5\n\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input vector(s)\n  x_mag_squared = np.sum(x * x, axis=-1, keepdims=True)\n\n  # Calculate the scaling factor\n  scale_factor = np.sqrt(1.0 / (x_mag_squared + 1e-12))\n\n  # Scale the input vector(s) towards the origin\n  x_contracted = x * scale_factor\n\n  return x_contracted\n\n"}
{"namespace": "coord.contract", "completion": "  return x * (1.0 - np.sqrt(np.sum(x ** 2, axis=1)) ** 2)\n\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input vector(s)\n  magnitude_squared = np.sum(x**2, axis=-1, keepdims=True)\n\n  # Calculate the scaling factor for each point based on the magnitude squared\n  scaling_factor = np.maximum(1.0 - magnitude_squared, 0.0)\n\n  # Scale the input vector(s) towards the origin based on the scaling factor\n  return x * scaling_factor"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the points\n  x_mag_sq = np.sum(x**2, axis=-1, keepdims=True)\n\n  # Calculate the scaling factor based on the formula\n  scaling_factor = 1 / (1 + np.sqrt(x_mag_sq))\n\n  # Scale the points towards the origin\n  x_contracted = x * scaling_factor\n\n  return x_contracted\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input points\n  x_mag_squared = np.sum(np.square(x), axis=-1)\n\n  # Calculate the scaling factor for each point based on the formula in the paper\n  scaling_factor = np.sqrt(1 - 1 / (1 + x_mag_squared))\n\n  # Scale each point in the input array by the scaling factor\n  x_contracted = x * scaling_factor[:, np.newaxis]\n\n  return x_contracted\n\n"}
{"namespace": "coord.contract", "completion": "  x_norm = np.linalg.norm(x, axis=-1, keepdims=True)\n  return x / (1.0 + x_norm)\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input vector(s)\n  x_mag_squared = np.sum(x**2, axis=-1, keepdims=True)\n\n  # Calculate the scaling factor for each point based on the magnitude squared\n  scaling_factor = np.sqrt(x_mag_squared) / (1 + x_mag_squared)\n\n  # Scale the points towards the origin using the scaling factor\n  x_contracted = x * scaling_factor\n\n  return x_contracted"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input array\n  mag_sq = np.sum(np.square(x), axis=-1)\n  # Calculate the scaling factor based on the magnitude squared and the formula in the paper\n  scaling_factor = (1 - np.sqrt(mag_sq)) / mag_sq\n  # Scale the input array by the scaling factor\n  x_contracted = x * scaling_factor[:, np.newaxis]\n  return x_contracted\n"}
{"namespace": "coord.contract", "completion": "  return x * (1 - (np.linalg.norm(x, axis=1) ** 2) / 2)\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input vector(s)\n  mag_squared = np.sum(x**2, axis=-1)\n\n  # Calculate the scaling factor based on the formula\n  scaling_factor = (1 - np.exp(-mag_squared)) / mag_squared\n\n  # Scale the input vector(s) towards the origin\n  x_contracted = x * scaling_factor[:, None]\n\n  return x_contracted\n\n"}
{"namespace": "coord.contract", "completion": "  return x * (1 - np.sqrt(np.sum(x ** 2, axis=-1, keepdims=True)))\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 1000:\n        return f\"{num_bytes} B\"\n    elif num_bytes < 1000000:\n        return f\"{num_bytes / 1000} KB\"\n    elif num_bytes < 1000000000:\n        return f\"{num_bytes / 1000000} MB\"\n    elif num_bytes < 1000000000000:\n        return f\"{num_bytes / 1000000000} GB\"\n    elif num_bytes < 1000000000000000:\n        return f\"{num_bytes / 1000000000000} TB\"\n    else:\n        return f\"{num_bytes / 1000000000000000} PB\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 1000:\n        return f\"{num_bytes} B\"\n    elif num_bytes < 1000 ** 2:\n        return f\"{num_bytes / 1000:.2f} KB\"\n    elif num_bytes < 1000 ** 3:\n        return f\"{num_bytes / 1000 ** 2:.2f} MB\"\n    elif num_bytes < 1000 ** 4:\n        return f\"{num_bytes / 1000 ** 3:.2f} GB\"\n    elif num_bytes < 1000 ** 5:\n        return f\"{num_bytes / 1000 ** 4:.2f} TB\"\n    elif num_bytes < 1000 ** 6:\n        return f\"{num_bytes / 1000 ** 5:.2f} PB\"\n    else:\n        return f\"{num_bytes / 1000 ** 6:.2f} EB\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 0:\n        raise ValueError(\"!!! number of bytes cannot be negative\")\n    step = 1000.0\n    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    for unit in units:\n        if num_bytes < step:\n            return f\"{num_bytes:.2f}{unit}\"\n        num_bytes /= step\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 1000:\n        return f\"{num_bytes}B\"\n    elif num_bytes < 1000000:\n        return f\"{num_bytes / 1000:.2f}KB\"\n    elif num_bytes < 1000000000:\n        return f\"{num_bytes / 1000000:.2f}MB\"\n    elif num_bytes < 1000000000000:\n        return f\"{num_bytes / 1000000000:.2f}GB\"\n    elif num_bytes < 1000000000000000:\n        return f\"{num_bytes / 1000000000000:.2f}TB\"\n    else:\n        return f\"{num_bytes / 1000000000000000:.2f}PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 0:\n        raise ValueError(\"!!! number of bytes cannot be negative\")\n\n    step = 1000.\n\n    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    idx = 0\n    while num_bytes >= step and idx < len(units):\n        num_bytes /= step\n        idx += 1\n    return f\"{num_bytes:.2f} {units[idx]}\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n\n    suffixes = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    num_bytes = float(num_bytes)\n    suffix_index = 0\n\n    while num_bytes >= 1000 and suffix_index < len(suffixes) - 1:\n        num_bytes /= 1000.0\n        suffix_index += 1\n\n    return f\"{num_bytes:.2f} {suffixes[suffix_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    # Convert the number of bytes into a human-readable format with appropriate units (B, KB, MB, GB, TB, PB).\n    if num_bytes < 1024:\n        return f'{num_bytes:.2f} B'\n    elif num_bytes < 1024 * 1024:\n        return f'{num_bytes / 1024:.2f} KB'\n    elif num_bytes < 1024 * 1024 * 1024:\n        return f'{num_bytes / 1024 / 1024:.2f} MB'\n    elif num_bytes < 1024 * 1024 * 1024 * 1024:\n        return f'{num_bytes / 1024 / 1024 / 1024:.2f} GB'\n    elif num_bytes < 1024 * 1024 * 1024 * 1024 * 1024:\n        return f'{num_bytes / 1024 / 1024 / 1024 / 1024:.2f} TB'\n    else:\n        return f'{num_bytes / 1024 / 1024 / 1024 / 1024 / 1024:.2f} PB'\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0B\"\n    elif num_bytes < 1024:\n        return f\"{num_bytes:.2f}B\"\n    elif num_bytes < 1024 ** 2:\n        return f\"{num_bytes / 1024:.2f}KB\"\n    elif num_bytes < 1024 ** 3:\n        return f\"{num_bytes / 1024 ** 2:.2f}MB\"\n    elif num_bytes < 1024 ** 4:\n        return f\"{num_bytes / 1024 ** 3:.2f}GB\"\n    elif num_bytes < 1024 ** 5:\n        return f\"{num_bytes / 1024 ** 4:.2f}TB\"\n    else:\n        return f\"{num_bytes / 1024 ** 5:.2f}PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0B\"\n\n    # Convert the number of bytes into a human-readable format with appropriate units\n    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    num_bytes_scaled = num_bytes\n    unit_index = 0\n    while num_bytes_scaled >= 1000 and unit_index < len(units) - 1:\n        num_bytes_scaled /= 1000\n        unit_index += 1\n\n    # Return the number of bytes converted into a human-readable format with appropriate units\n    return f\"{num_bytes_scaled:.2f}{units[unit_index]}\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n\n    unit_list = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    unit_list_short = [\"B\", \"K\", \"M\", \"G\", \"T\", \"P\"]\n    unit_index = 0\n\n    while num_bytes >= 1000:\n        num_bytes /= 1000\n        unit_index += 1\n\n    if unit_index == 0:\n        return f\"{num_bytes:.2f} {unit_list[unit_index]}\"\n    else:\n        return f\"{num_bytes:.2f} {unit_list_short[unit_index]} {unit_list[unit_index]}\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n\n    suffixes = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    i = 0\n    while num_bytes >= 1000 and i < len(suffixes) - 1:\n        num_bytes /= 1000.0\n        i += 1\n\n    return f\"{num_bytes:.2f} {suffixes[i]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 1024:\n        return f\"{num_bytes} B\"\n    elif num_bytes < 1024 ** 2:\n        return f\"{num_bytes / 1024:.2f} KB\"\n    elif num_bytes < 1024 ** 3:\n        return f\"{num_bytes / 1024 ** 2:.2f} MB\"\n    elif num_bytes < 1024 ** 4:\n        return f\"{num_bytes / 1024 ** 3:.2f} GB\"\n    elif num_bytes < 1024 ** 5:\n        return f\"{num_bytes / 1024 ** 4:.2f} TB\"\n    else:\n        return f\"{num_bytes / 1024 ** 5:.2f} PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    # Scale the number down by 1000 until it is less than 1000\n    while num_bytes >= 1000:\n        num_bytes /= 1000\n\n    # Change units from bytes to petabytes\n    while num_bytes >= 1000:\n        num_bytes /= 1000\n\n    # Add the appropriate unit to the number\n    if num_bytes < 1000:\n        return f\"{num_bytes:.2f} B\"\n    elif num_bytes < 1000000:\n        return f\"{num_bytes / 1000:.2f} KB\"\n    elif num_bytes < 1000000000:\n        return f\"{num_bytes / 1000000:.2f} MB\"\n    elif num_bytes < 1000000000000:\n        return f\"{num_bytes / 1000000000:.2f} GB\"\n    elif num_bytes < 1000000000000000:\n        return f\"{num_bytes / 1000000000000:.2f} TB\"\n    else:\n        return f\"{num_bytes / 1000000000000000:.2f} PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    # Define the units of measurement\n    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n\n    # Initialize the index for the units\n    index = 0\n\n    # Loop through the units until the number of bytes is less than 1000 or the index is at the end of the units list\n    while num_bytes >= 1000 and index < len(units) - 1:\n        # Divide the number of bytes by 1000 to get the next unit\n        num_bytes /= 1000\n        # Increment the index to the next unit\n        index += 1\n\n    # Return the number of bytes with the appropriate unit\n    return f\"{num_bytes:.2f} {units[index]}\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    # If the number of bytes is less than 1000, the number of bytes is returned as a string with no units.\n    if num_bytes < 1000:\n        return str(num_bytes) + \" B\"\n\n    # If the number of bytes is greater than or equal to 1000, the number of bytes is scaled down by 1000 and the units are changed from bytes to petabytes as necessary until a human-friendly format is achieved.\n    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n        if abs(num_bytes) < 1000.0:\n            return f\"{num_bytes:3.1f}{unit}B\"\n        num_bytes /= 1000.0\n\n    # If the number of bytes is greater than or equal to 1000, the number of bytes is scaled down by 1000 and the units are changed from bytes to petabytes as necessary until a human-friendly format is achieved.\n    return f\"{num_bytes:3.1f}PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    # If the number of bytes is 0, return 0 B\n    if num_bytes == 0:\n        return \"0 B\"\n\n    # Define the units of measurement\n    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n\n    # Initialize the index of the unit\n    i = 0\n\n    # Loop until the number of bytes is less than 1000\n    while num_bytes >= 1000:\n        # Divide the number of bytes by 1000\n        num_bytes /= 1000\n        # Increment the index of the unit\n        i += 1\n\n    # Return the number of bytes converted into a human-readable format with the appropriate unit\n    return f\"{num_bytes:.2f} {units[i]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 0:\n        raise ValueError(\"!!! number of bytes cannot be negative\")\n    step = 1000.0\n    prefixes = {\n        0: \"\",\n        1: \"K\",\n        2: \"M\",\n        3: \"G\",\n        4: \"T\",\n        5: \"P\",\n    }\n    prefix_index = 0\n    while num_bytes >= step:\n        prefix_index += 1\n        if prefix_index > 5:\n            break\n        num_bytes /= step\n    return f\"{num_bytes:.1f}{prefixes[prefix_index]}B\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    # The units of measurement\n    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n\n    # The number of bytes\n    num = num_bytes\n\n    # The unit index\n    unit_index = 0\n\n    # While the number of bytes is greater than 1000\n    while num > 1000:\n\n        # Divide the number of bytes by 1000\n        num = num / 1000\n\n        # Increment the unit index\n        unit_index += 1\n\n    # Return the number of bytes converted into a human-readable format with the appropriate unit\n    return f\"{num:.2f} {units[unit_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    # Check if the number of bytes is zero\n    if num_bytes == 0:\n        return \"0B\"\n\n    # Define the list of units\n    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n\n    # Define the base for scaling\n    base = 1024\n\n    # Initialize the index for the units list\n    i = 0\n\n    # Loop until the number of bytes is less than the base\n    while num_bytes < base:\n        # Increment the index\n        i += 1\n        # Check if the index is greater than the length of the units list\n        if i >= len(units):\n            break\n        # Divide the number of bytes by the base\n        num_bytes /= base\n\n    # Return the number of bytes with the appropriate unit\n    return f\"{num_bytes:.2f}{units[i]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 0:\n        raise ValueError(\"!!! number of bytes cannot be negative\")\n\n    step = 1000.\n\n    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < step:\n            return f\"{num_bytes:3.1f}{unit}\"\n        num_bytes /= step\n\n    return f\"{num_bytes:3.1f}PB\"\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_dimensions(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{field.name} must be a {nb_dimensions}D array\")\n        return v\n\n    return validate_array_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_dimensions(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if not isinstance(v, np.ndarray):\n            raise ValueError(f\"{field.name} must be a numpy array\")\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{field.name} must be a {nb_dimensions}D array\")\n        return v\n\n    return validate_array_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be a {nb_dimensions}D array, not {v.ndim}D\"\n            )\n        return v\n\n    return validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be a {nb_dimensions}D array, but is {v.ndim}D\"\n            )\n        return v\n\n    return validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if not isinstance(v, np.ndarray):\n            raise ValueError(\n                f\"{field.name} must be a numpy array, not {type(v)}\"\n            )\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be a {nb_dimensions}D array, not {v.ndim}D\"\n            )\n        return v\n\n    return validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if not isinstance(v, np.ndarray):\n            raise ValueError(f\"{field.name} must be a numpy array\")\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be a numpy array with {nb_dimensions} dimensions\"\n            )\n        return v\n\n    return validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_dimensions(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be a {nb_dimensions}D array, but got {v.ndim}D array instead.\"\n            )\n        return v\n\n    return validate_array_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def array_n_dimensions(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be a {nb_dimensions} dimensional array\"\n            )\n        return v\n\n    return array_n_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls, v, field):\n        if not isinstance(v, np.ndarray):\n            raise ValueError(f\"{field.name} must be a numpy array\")\n\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{field.name} must be a {nb_dimensions}D array\")\n\n        return v\n\n    return validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_dimensions(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if not isinstance(v, np.ndarray):\n            raise ValueError(f\"{field.name} must be an array\")\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{field.name} must be an array with {nb_dimensions} dimensions\")\n        return v\n\n    return validate_array_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_dimensions(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be a {nb_dimensions}D array, but got a {v.ndim}D array\"\n            )\n        return v\n\n    return validate_array_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{field.name} must be a {nb_dimensions}D array\")\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls, v, field):\n        if not isinstance(v, np.ndarray):\n            raise TypeError(f\"{field.name} must be a numpy array\")\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be a {nb_dimensions}D array, but got {v.ndim}D\"\n            )\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{field.name} must be a {nb_dimensions}D array\")\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def array_n_dimensions(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if not isinstance(v, np.ndarray):\n            raise ValueError(f\"{field.name} must be a numpy array\")\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be a {nb_dimensions}D array, found {v.ndim}D\"\n            )\n        return v\n\n    return array_n_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be a {nb_dimensions}D array, got {v.ndim}D array instead.\"\n            )\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validate_array_dimensions(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if not isinstance(v, np.ndarray):\n            raise ValueError(\"The field must be an array\")\n\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"The field must be an array of {nb_dimensions} dimensions\")\n\n        return v\n\n    return validate_array_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _is_array_n_dimensions(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if not isinstance(v, np.ndarray):\n            raise ValueError(\"The value is not an array\")\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"The array must have {nb_dimensions} dimensions\")\n        return v\n\n    return _is_array_n_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if not isinstance(v, np.ndarray):\n            raise ValueError(\n                f\"{field.name} must be a numpy array, got {type(v)} instead.\"\n            )\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be a {nb_dimensions}D array, got {v.ndim}D array instead.\"\n            )\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _is_array_n_dimensions(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"Expected {field.name} to be {nb_dimensions}D array\")\n        return v\n\n    return _is_array_n_dimensions\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = onp.sqrt(x ** 2 + y ** 2 + z ** 2)\n  theta = onp.arccos(z / (r + eps))\n  phi = onp.arctan2(y, x)\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Convert cartesian coordinates to spherical coordinates\n  r = onp.linalg.norm(cartesian_vector, axis=-1)\n  theta = onp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = onp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Extract the x, y, and z coordinates from the input array\n  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  # Calculate the radius (r) using the Pythagorean theorem\n  r = onp.sqrt(x ** 2 + y ** 2 + z ** 2)\n\n  # Calculate the inclination (theta) using the arctangent function\n  theta = onp.arctan2(onp.sqrt(x ** 2 + y ** 2), z)\n\n  # Calculate the azimuth (phi) using the arctangent function\n  phi = onp.arctan2(y, x)\n\n  # Return the spherical coordinates as a tuple\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Extract the x, y, and z coordinates from the input array\n  x = cartesian_vector[..., 0]\n  y = cartesian_vector[..., 1]\n  z = cartesian_vector[..., 2]\n\n  # Calculate the radius (r) using the Pythagorean theorem\n  r = onp.sqrt(x**2 + y**2 + z**2)\n\n  # Calculate the inclination angle (theta) using the arccosine function\n  theta = onp.arccos(z / (onp.maximum(r, eps)))\n\n  # Calculate the azimuth angle (phi) using the arctangent function\n  phi = onp.arctan2(y, x)\n\n  # Return the spherical coordinates as a tuple\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Check if the input array is of the expected shape\n  if cartesian_vector.shape[-1] != 3:\n    raise ValueError(\"Input array must have shape (..., 3)\")\n\n  # Extract the x, y, and z coordinates from the input array\n  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  # Calculate the radius (r) using the Pythagorean theorem\n  r = onp.sqrt(x**2 + y**2 + z**2)\n\n  # Calculate the inclination (theta) using the arccosine function\n  theta = onp.arccos(z / (r + eps))\n\n  # Calculate the azimuth (phi) using the arctan2 function\n  phi = onp.arctan2(y, x)\n\n  # Return the spherical coordinates as a tuple\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Check if the input array has the expected shape\n  if cartesian_vector.shape[-1] != 3:\n    raise ValueError(\"Input array must have shape (..., 3)\")\n\n  # Compute the radius, inclination, and azimuth\n  r = onp.linalg.norm(cartesian_vector, axis=-1)\n  theta = onp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = onp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  # Return the spherical coordinates as a tuple\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Convert cartesian vector to spherical vector\n  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = onp.sqrt(onp.square(x) + onp.square(y) + onp.square(z))\n  theta = onp.arccos(z / (r + eps))\n  phi = onp.arctan2(y, x)\n\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Calculate the radius (r)\n  r = onp.linalg.norm(cartesian_vector, axis = -1)\n\n  # Calculate the inclination (theta)\n  theta = onp.arccos(cartesian_vector[..., 2] / (r + eps))\n\n  # Calculate the azimuth (phi)\n  phi = onp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Check if the input array has the expected shape\n  if len(cartesian_vector.shape) != 2 or cartesian_vector.shape[-1] != 3:\n    raise ValueError(\"Input array must have shape (..., 3)\")\n\n  # Extract the x, y, and z coordinates from the input array\n  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  # Calculate the radius (r) using the Pythagorean theorem\n  r = onp.sqrt(x**2 + y**2 + z**2)\n\n  # Calculate the inclination (theta) using the arccosine function\n  theta = onp.arccos(z / (onp.sqrt(x**2 + y**2) + eps))\n\n  # Calculate the azimuth (phi) using the arctan2 function\n  phi = onp.arctan2(y, x)\n\n  # Return the spherical coordinates as a tuple\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Check if the input array has the expected shape\n  if cartesian_vector.shape[-1] != 3:\n    raise ValueError(\"Input array must have shape (..., 3)\")\n\n  # Extract the x, y, and z components of the input array\n  x = cartesian_vector[..., 0]\n  y = cartesian_vector[..., 1]\n  z = cartesian_vector[..., 2]\n\n  # Calculate the radius (r) using the Pythagorean theorem\n  r = onp.sqrt(x**2 + y**2 + z**2)\n\n  # Calculate the inclination (theta) using the arccosine function\n  theta = onp.arccos(z / (r + eps))\n\n  # Calculate the azimuth (phi) using the arctangent function\n  phi = onp.arctan2(y, x)\n\n  # Return the spherical coordinates as a tuple\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Convert cartesian coordinates to spherical coordinates\n  radius = onp.linalg.norm(cartesian_vector, axis = -1)\n  inclination = onp.arccos(cartesian_vector[..., 2] / (radius + eps))\n  azimuth = onp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return radius, inclination, azimuth\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Calculate the radius\n  r = onp.linalg.norm(cartesian_vector, axis = -1)\n\n  # Calculate the inclination angle (theta)\n  theta = onp.arccos(cartesian_vector[..., 2] / (r + eps))\n\n  # Calculate the azimuth angle (phi)\n  phi = onp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  # Return the spherical coordinates as a tuple\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Check if the input is a valid array\n  if not isinstance(cartesian_vector, onp.ndarray):\n    raise TypeError(\"Input must be a numpy array\")\n\n  # Check if the input array has the expected shape\n  if cartesian_vector.shape[-1] != 3:\n    raise ValueError(\"Input array must have shape (..., 3)\")\n\n  # Convert the input array to float32\n  cartesian_vector = cartesian_vector.astype(onp.float32)\n\n  # Calculate the radius\n  r = onp.linalg.norm(cartesian_vector, axis=-1)\n\n  # Calculate the inclination angle (theta)\n  theta = onp.arccos(cartesian_vector[..., 2] / (r + eps))\n\n  # Calculate the azimuth angle (phi)\n  phi = onp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  # Return the spherical coordinates as a tuple\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Get the x, y, and z coordinates of the input vector\n  x = cartesian_vector[..., 0]\n  y = cartesian_vector[..., 1]\n  z = cartesian_vector[..., 2]\n\n  # Calculate the radius of the point\n  r = onp.sqrt(x**2 + y**2 + z**2)\n\n  # Calculate the inclination angle (theta)\n  theta = onp.arccos(z / (r + eps))\n\n  # Calculate the azimuth angle (phi)\n  phi = onp.arctan2(y, x)\n\n  # Return the spherical coordinates as a tuple\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Check if the input array has the correct shape\n  if cartesian_vector.shape[-1] != 3:\n    raise ValueError(\"Input array must have shape (..., 3)\")\n\n  # Extract the x, y, and z coordinates from the input array\n  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  # Calculate the radius (r) using the Pythagorean theorem\n  r = onp.sqrt(x ** 2 + y ** 2 + z ** 2)\n\n  # Calculate the inclination angle (theta) using the arccosine function\n  theta = onp.arccos(z / (r + eps))\n\n  # Calculate the azimuth angle (phi) using the arctangent function\n  phi = onp.arctan2(y, x)\n\n  # Return the spherical coordinates as a tuple\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Check if the input array is of the expected shape\n  if not cartesian_vector.shape[-1] == 3:\n    raise ValueError(f'The input array must have a shape of (..., 3) but got {cartesian_vector.shape}')\n\n  # Extract the x, y, and z coordinates from the input array\n  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  # Calculate the radius (r) using the Pythagorean theorem\n  r = onp.sqrt(x**2 + y**2 + z**2)\n\n  # Calculate the inclination (theta) using the arccosine function\n  theta = onp.arccos(z / (r + eps))\n\n  # Calculate the azimuth (phi) using the arctangent function\n  phi = onp.arctan2(y, x)\n\n  # Return the spherical coordinates as a tuple\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Convert the input vector to a NumPy array\n  cartesian_vector = onp.array(cartesian_vector)\n\n  # Extract the x, y, and z components of the vector\n  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  # Calculate the radius using the Pythagorean theorem\n  r = onp.sqrt(x**2 + y**2 + z**2)\n\n  # Calculate the inclination (theta) using the arccosine function\n  theta = onp.arccos(z / (onp.sqrt(x**2 + y**2) + eps))\n\n  # Calculate the azimuth (phi) using the arctangent function\n  phi = onp.arctan2(y, x)\n\n  # Return the spherical coordinates as a tuple\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Check if the input is a valid numpy array\n  if not isinstance(cartesian_vector, onp.ndarray):\n    raise TypeError(\"Input must be a numpy array\")\n\n  # Check if the input array has the correct shape\n  if cartesian_vector.shape[-1] != 3:\n    raise ValueError(\"Input array must have shape (..., 3)\")\n\n  # Calculate the radius\n  radius = onp.sqrt(onp.sum(cartesian_vector ** 2, axis=-1))\n\n  # Calculate the inclination angle (theta)\n  theta = onp.arccos(cartesian_vector[..., 2] / (radius + eps))\n\n  # Calculate the azimuth angle (phi)\n  phi = onp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return radius, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = onp.linalg.norm(cartesian_vector, axis = -1)\n  theta = onp.arccos(z / (r + eps))\n  phi = onp.arctan2(y, x)\n\n  return r, theta, phi\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Convert input to array\n  cartesian_vector = onp.asarray(cartesian_vector)\n\n  # Extract x, y, z coordinates\n  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  # Calculate radius\n  r = onp.sqrt(x ** 2 + y ** 2 + z ** 2)\n\n  # Calculate inclination\n  theta = onp.arccos(z / (r + eps))\n\n  # Calculate azimuth\n  phi = onp.arctan2(y, x)\n\n  return r, theta, phi\n"}
{"namespace": "common.rougeL_score", "completion": "    from rouge_score import rouge_scorer\n    from rouge_score import scoring\n\n    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    score = scorer.score(reference, continuation)\n\n    return score['rougeL'].fmeasure\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Tokenize the continuation and reference texts using a custom tokenizer function based on jieba\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Compute the longest common subsequence (LCS) between the tokenized continuation and reference texts\n    lcs = longest_common_subsequence(continuation_tokens, reference_tokens)\n\n    # Compute the ROUGE-L score using the LCS length and the lengths of the tokenized continuation and reference texts\n    rouge_l_score = lcs / len(continuation_tokens) if len(continuation_tokens) > 0 else 0\n\n    return rouge_l_score"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n    from rouge_score import rouge_scorer\n\n    def tokenize(text: str) -> list:\n        \"\"\"\n        Tokenizes a given text using the jieba library.\n\n        Input-Output Arguments\n        :param text: str, The text to be tokenized.\n        :return: list, A list of tokens extracted from the input text.\n        \"\"\"\n        return jieba.lcut(text)\n\n    # Tokenize the input texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the ROUGE-L score\n    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    score = scorer.score(continuation, reference)['rougeL']\n\n    return score.fmeasure"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n    import jieba.posseg as pseg\n    from rouge_score import rouge_scorer\n\n    # Tokenize the continuation and reference texts using jieba\n    tokenizer = lambda text: [token.word for token in pseg.cut(text)]\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Calculate the ROUGE-L score\n    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    score = scorer.score(reference_tokens, continuation_tokens)['rougeL']\n\n    return score.fmeasure"}
{"namespace": "common.rougeL_score", "completion": "    # Tokenize the continuation and reference texts using a custom tokenizer function based on jieba\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the ROUGE-L score between the continuation and reference texts\n    rouge_l_score = rouge_l(continuation_tokens, reference_tokens)\n\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n    from rouge_score import rouge_scorer\n\n    # Tokenize the input texts using jieba\n    continuation_tokens = list(jieba.cut(continuation))\n    reference_tokens = list(jieba.cut(reference))\n\n    # Compute the ROUGE-L score between the input texts\n    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    scores = scorer.score(reference, continuation)\n    rouge_l_score = scores['rougeL'].fmeasure\n\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n    import jieba.posseg as pseg\n    from rouge_score import rouge_scorer\n\n    def tokenize(text):\n        \"\"\"\n        Tokenizes a given text using the jieba library and returns a list of tokens.\n\n        Input-Output Arguments\n        :param text: str, The text to be tokenized.\n        :return: list, A list of tokens extracted from the text.\n        \"\"\"\n        return [token for token, _ in pseg.cut(text)]\n\n    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    scores = scorer.score(reference, continuation)\n\n    return scores['rougeL'].fmeasure\n\n"}
{"namespace": "common.rougeL_score", "completion": "    from jieba import lcut\n    from rouge_score import rouge_n_sentence_level\n\n    def tokenizer(text: str) -> list:\n        \"\"\"\n        Tokenizes a given text using the jieba library.\n\n        Input-Output Arguments\n        :param text: str, The text to be tokenized.\n        :return: list, A list of tokens extracted from the text.\n        \"\"\"\n        return lcut(text)\n\n    return rouge_n_sentence_level(\n        [continuation],\n        [reference],\n        tokenizer=tokenizer,\n        n=2\n    ).fmeasure\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Tokenize the continuation and reference texts using the custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the ROUGE-L score using the custom ROUGE-L function\n    rouge_l_score = rouge_l(continuation_tokens, reference_tokens)\n\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n    import numpy as np\n\n    # Tokenize the text using jieba\n    def tokenize(text):\n        return [token for token in jieba.cut(text) if token.strip()]\n\n    # Tokenize the continuation and reference texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Initialize the ROUGE-L score variables\n    lcs_matrix = np.zeros((len(continuation_tokens) + 1, len(reference_tokens) + 1))\n    max_lcs = 0\n\n    # Compute the LCS matrix\n    for i in range(1, len(continuation_tokens) + 1):\n        for j in range(1, len(reference_tokens) + 1):\n            if continuation_tokens[i - 1] == reference_tokens[j - 1]:\n                lcs_matrix[i][j] = lcs_matrix[i - 1][j - 1] + 1\n                if lcs_matrix[i][j] > max_lcs:\n                    max_lcs = lcs_matrix[i][j]\n\n    # Compute the ROUGE-L score\n    rouge_l_score = max_lcs / min(len(continuation_tokens), len(reference_tokens))\n\n    return rouge_l_score"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n    import jieba.posseg as pseg\n    from rouge import Rouge\n\n    # Tokenize the input texts using a custom tokenizer function\n    def tokenize(text):\n        return [word for word, flag in pseg.cut(text) if flag not in ['x', 'c', 'u', 'd', 'p', 't', 'uj', 'm', 'f', 'r']]\n\n    # Tokenize the input texts using the custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the ROUGE-L score using the Rouge package\n    rouge = Rouge()\n    scores = rouge.get_scores(continuation_tokens, reference_tokens)\n    rouge_l_score = scores[0]['rouge-l']['f']\n\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Tokenize the continuation and reference texts using a custom tokenizer function based on jieba\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the ROUGE-L score using the tokenized texts\n    rouge_l_score = rouge_l(continuation_tokens, reference_tokens)\n\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Tokenize the continuation and reference texts using a custom tokenizer function based on jieba\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Initialize variables to store the lengths of the tokenized texts\n    continuation_length = len(continuation_tokens)\n    reference_length = len(reference_tokens)\n\n    # Initialize a matrix to store the lengths of the longest common subsequences between the tokenized texts\n    lcs_matrix = [[0 for _ in range(reference_length + 1)] for _ in range(continuation_length + 1)]\n\n    # Compute the lengths of the longest common subsequences between the tokenized texts using dynamic programming\n    for i in range(1, continuation_length + 1):\n        for j in range(1, reference_length + 1):\n            if continuation_tokens[i - 1] == reference_tokens[j - 1]:\n                lcs_matrix[i][j] = lcs_matrix[i - 1][j - 1] + 1\n            else:\n                lcs_matrix[i][j] = max(lcs_matrix[i - 1][j], lcs_matrix[i][j - 1])\n\n    # Compute the ROUGE-L score as the ratio of the length of the longest common subsequence to the length of the shorter text\n    rouge_l_score = lcs_matrix[continuation_length][reference_length] / min(continuation_length, reference_length)\n\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Tokenize the text using a custom tokenizer function based on jieba\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Initialize variables for the ROUGE-L score\n    score = 0\n    longest_common_subsequence_length = 0\n\n    # Compute the longest common subsequence length between the two tokenized texts\n    for i in range(1, len(continuation_tokens) + 1):\n        for j in range(1, len(reference_tokens) + 1):\n            if continuation_tokens[i - 1] == reference_tokens[j - 1]:\n                longest_common_subsequence_length = max(longest_common_subsequence_length, i, j)\n\n    # Calculate the ROUGE-L score\n    score = longest_common_subsequence_length / min(len(continuation_tokens), len(reference_tokens))\n\n    return score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Tokenize the continuation and reference texts using a custom tokenizer function based on jieba\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Compute the ROUGE-L score between the tokenized continuation and reference texts\n    rouge_l_score = rouge_l(continuation_tokens, reference_tokens)\n\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n    import jieba.posseg as pseg\n\n    # tokenize the texts\n    continuation_tokens = list(pseg.cut(continuation))\n    reference_tokens = list(pseg.cut(reference))\n\n    # initialize the ROUGE-L score\n    score = 0.0\n\n    # compute the ROUGE-L score\n    for i in range(len(continuation_tokens)):\n        for j in range(len(reference_tokens)):\n            if continuation_tokens[i].word == reference_tokens[j].word:\n                score += 1\n                reference_tokens[j] = None\n                break\n\n    # return the ROUGE-L score\n    return score / len(continuation_tokens)\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Tokenize the continuation and reference texts using the custom tokenizer function\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Calculate the longest common subsequence (LCS) between the two tokenized texts\n    lcs = longest_common_subsequence(continuation_tokens, reference_tokens)\n\n    # Calculate the ROUGE-L score using the LCS length and the lengths of the two tokenized texts\n    rouge_l_score = lcs / len(continuation_tokens) if len(continuation_tokens) != 0 else 0\n    rouge_l_score = rouge_l_score if rouge_l_score > lcs / len(reference_tokens) else lcs / len(reference_tokens)\n\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Tokenize the continuation and reference using a custom tokenizer function\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the longest common subsequence (LCS) between the tokenized continuation and reference texts\n    lcs = longest_common_subsequence(continuation_tokens, reference_tokens)\n\n    # Compute the ROUGE-L score using the LCS and the lengths of the tokenized continuation and reference texts\n    rouge_l_score = len(lcs) / (len(continuation_tokens) + len(reference_tokens) - len(lcs))\n\n    return rouge_l_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # imports\n    from nltk.tokenize import word_tokenize\n    from nltk.util import ngrams\n\n    # tokenize the input texts\n    continuation_tokens = word_tokenize(continuation)\n    reference_tokens = word_tokenize(reference)\n\n    # convert the token lists to n-grams\n    continuation_ngrams = list(ngrams(continuation_tokens, 2))\n    reference_ngrams = list(ngrams(reference_tokens, 2))\n\n    # initialize the ROUGE-L score variables\n    intersection_count = 0\n    continuation_count = len(continuation_ngrams)\n    reference_count = len(reference_ngrams)\n\n    # compute the ROUGE-L score\n    for continuation_ngram in continuation_ngrams:\n        for reference_ngram in reference_ngrams:\n            if continuation_ngram == reference_ngram:\n                intersection_count += 1\n                reference_ngrams.remove(reference_ngram)\n                break\n\n    # compute the ROUGE-L score\n    recall = intersection_count / reference_count\n    precision = intersection_count / continuation_count\n    f1_score = 2 * (precision * recall) / (precision + recall)\n\n    return f1_score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    import jieba\n    import jieba.posseg as pseg\n    import jieba.analyse\n    import numpy as np\n\n    def tokenizer(text):\n        \"\"\"\n        Tokenizes a given text using the jieba library.\n\n        :param text: str, The text to be tokenized.\n        :return: list, A list of tokens extracted from the text.\n        \"\"\"\n        # \u4f7f\u7528jieba\u8fdb\u884c\u5206\u8bcd\n        seg_list = jieba.cut(text, cut_all=False)\n        # \u53bb\u9664\u505c\u7528\u8bcd\n        stop_words = {}.fromkeys(\n            [line.rstrip() for line in open('./data/stopwords.txt',\n                                            encoding='UTF-8')]\n        )\n        return [w for w in seg_list if not w in stop_words]\n\n    # \u8ba1\u7b97ROUGE-L\n    def rouge_l(continuation, reference):\n        \"\"\"\n        Calculates the ROUGE-L score between a generated text continuation and a reference text.\n\n        :param continuation: str, The generated text whose quality is to be evaluated.\n        :param reference: str, The reference text against which the generated text is compared.\n        :return: float. The ROUGE-L score indicating the quality of the generated text in relation to the reference text.\n        \"\"\"\n        # \u4f7f\u7528jieba\u8fdb\u884c\u5206\u8bcd\n        continuation_token = tokenizer(continuation)\n        reference_token = tokenizer(reference)\n\n        # \u8ba1\u7b97ROUGE-L\n        lcs = max([len(x) for x in\n                   [list(jieba.lcs.lcs(continuation_token, reference_token))]],\n                  key=len)\n        rl = float(lcs) / (len(continuation_token) + len(reference_token) - lcs)\n        return rl\n\n    # \u8ba1\u7b97ROUGE-L\n    rl = rouge_l(continuation, reference)\n    return rl\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_standard(name)\n    except:\n        return locate_fallback(name)"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_object(name)\n    except:\n        return locate_object_fallback(name)"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_object(name)\n    except:\n        return locate_object_fallback(name)"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_using_standard_method(name)\n    except:\n        return locate_using_fallback_method(name)"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate(name)\n    except:\n        try:\n            return locate(name)\n        except:\n            raise Exception(f\"Could not locate object with name {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate(name)\n    except Exception as e:\n        pass\n\n    try:\n        return locate(name)\n    except Exception as e:\n        pass\n\n    raise Exception(\"Object not found\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_object(name)\n    except ModuleNotFoundError:\n        return locate_object_with_fallback(name)\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate(name)\n    except:\n        return locate_fallback(name)\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_object(name)\n    except:\n        return locate_object_fallback(name)\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_object(name)\n    except Exception:\n        return locate_object_fallback(name)\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_object_by_name(name)\n    except Exception as e:\n        try:\n            return locate_object_by_name_fallback(name)\n        except Exception as e:\n            raise Exception(f\"Failed to locate object with name '{name}'.\") from e"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_by_standard_method(name)\n    except ImportError:\n        return locate_by_fallback_method(name)\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_standard(name)\n    except NameError:\n        return locate_fallback(name)\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Attempt to locate the object using the standard method\n    try:\n        module_name, _, object_name = name.rpartition('.')\n        module = __import__(module_name)\n        for submodule_name in object_name.split('.'):\n            module = getattr(module, submodule_name)\n        return module\n    except Exception as e:\n        pass\n\n    # Attempt to locate the object using the fallback method\n    try:\n        module_name, _, object_name = name.rpartition('.')\n        module = __import__(module_name)\n        for submodule_name in object_name.split('.'):\n            module = getattr(module, submodule_name)\n        return module\n    except Exception as e:\n        raise Exception(f\"Failed to locate object: {name}\") from e\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Locate the object using the standard method\n    try:\n        module_name, class_name = name.rsplit('.', 1)\n        module = importlib.import_module(module_name)\n        return getattr(module, class_name)\n    except (ImportError, AttributeError):\n        pass\n\n    # Locate the object using the fallback method\n    try:\n        return locate_object(name)\n    except (ImportError, AttributeError):\n        raise ImportError(f\"Could not locate object '{name}'\")\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Attempt to locate the object using the standard method\n    try:\n        obj = __import__(name)\n        for part in name.split(\".\")[1:]:\n            obj = getattr(obj, part)\n        return obj\n    except ImportError:\n        pass\n\n    # Attempt to locate the object using the fallback method\n    try:\n        obj = locate_fallback(name)\n        return obj\n    except Exception as e:\n        raise Exception(f\"Failed to locate object '{name}'\") from e\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        module_name, _, class_name = name.rpartition('.')\n        module = __import__(module_name, fromlist=[class_name])\n        return getattr(module, class_name)\n    except (AttributeError, ImportError):\n        return fallback_locate(name)\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Locate the object using the standard method\n    try:\n        module_name, _, class_name = name.rpartition('.')\n        module = importlib.import_module(module_name)\n        return getattr(module, class_name)\n    except (ImportError, AttributeError):\n        pass\n\n    # Locate the object using the fallback method\n    try:\n        return locate_object_using_fallback_method(name)\n    except Exception as e:\n        raise Exception(f\"Failed to locate object '{name}': {e}\") from e\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    if not isinstance(name, str):\n        raise TypeError(\"'name' must be a string\")\n\n    try:\n        module_name, object_name = name.rsplit('.', 1)\n        module = importlib.import_module(module_name)\n        return getattr(module, object_name)\n    except (ImportError, AttributeError):\n        pass\n\n    # Fallback method\n    try:\n        return locate_object(name)\n    except (ImportError, AttributeError):\n        raise ImportError(f\"Failed to locate object '{name}'\")\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    if name == \"\":\n        raise Exception(\"The name of the object to locate cannot be empty.\")\n\n    if name.startswith(\".\"):\n        raise Exception(\"The name of the object to locate cannot start with a period.\")\n\n    if name.endswith(\".\"):\n        raise Exception(\"The name of the object to locate cannot end with a period.\")\n\n    if \".\" not in name:\n        raise Exception(\"The name of the object to locate must contain at least one period.\")\n\n    if name.count(\".\") > 1:\n        raise Exception(\"The name of the object to locate cannot contain more than one period.\")\n\n    if name.count(\".\") == 1:\n        module_name, object_name = name.split(\".\")\n    else:\n        module_name = \"\"\n        object_name = name\n\n    if module_name == \"\":\n        try:\n            obj = globals()[object_name]\n        except KeyError:\n            obj = locate_object_using_fallback_method(object_name)\n    else:\n        try:\n            module = __import__(module_name)\n            obj = getattr(module, object_name)\n        except (ImportError, AttributeError):\n            obj = locate_object_using_fallback_method(object_name)\n\n    if obj is None:\n        raise Exception(\"The object could not be located.\")\n\n    return obj\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the in-memory buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the in-memory buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the in-memory buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer.\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the in-memory buffer.\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer.\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the in-memory buffer.\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer.\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the in-memory buffer.\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to a buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to a buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the buffer\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    import io\n    import torch\n\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    import io\n    import torch\n\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # save the module to a buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # load the module from the buffer\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n\n    # Load the module from the in-memory buffer\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n\n    # Load the module from the in-memory buffer\n    module = torch.jit.load(buffer)\n\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the script module to an in-memory buffer.\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the script module from the in-memory buffer.\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module into a buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n\n    # Load the module from the buffer\n    module = torch.jit.load(buffer)\n\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer.\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the in-memory buffer.\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n\n    # Return the reloaded module.\n    return module\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the in-memory buffer\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n\n    return module\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # save the script module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # load the script module from the in-memory buffer\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n\n    return module\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights)\n\n    ids_fused = []\n    scores_fused = []\n    for i in range(len(ids)):\n        ids_fused.extend(ids[i])\n        scores_fused.extend(scores[i])\n\n    ids_fused, scores_fused = zip(*sorted(zip(ids_fused, scores_fused), key=lambda x: x[1], reverse=True))\n\n    ids_fused = list(ids_fused)[:top_k]\n    scores_fused = list(scores_fused)[:top_k]\n\n    return ids_fused, scores_fused"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights)\n\n    # Normalize the scores\n    scores = [\n        [score / sum(scores[i]) for score in scores[i]]\n        for i in range(len(scores))\n    ]\n\n    # Combine the scores\n    combined_scores = [\n        [sum(scores[i][j] * weights[i] for i in range(len(scores)))\n         for j in range(len(scores[0]))]\n        for _ in range(len(scores[0]))\n    ]\n\n    # Sort the combined scores\n    combined_scores, combined_ids = zip(*sorted(\n        zip(combined_scores, ids), reverse=True, key=lambda x: x[0]))\n\n    # Select the top k scores\n    combined_scores = combined_scores[:top_k]\n    combined_ids = combined_ids[:top_k]\n\n    return combined_ids, combined_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Normalize the scores\n    scores = [[(score / len(scores)) for score in score_list] for score_list in scores]\n\n    # Combine the scores using a convex combination method\n    fused_scores = [sum([scores[i][j] * weights[i] for i in range(len(scores))]) for j in range(len(scores[0]))]\n\n    # Sort the fused scores in descending order\n    fused_scores, fused_ids = zip(*sorted(zip(fused_scores, ids[0]), reverse=True))\n\n    # Select the top_k results\n    fused_scores = fused_scores[:top_k]\n    fused_ids = fused_ids[:top_k]\n\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores), \"The length of the ids tuple must match the length of the scores tuple.\"\n    assert len(ids) == len(weights), \"The length of the ids tuple must match the length of the weights tuple.\"\n\n    # Normalize scores\n    for i in range(len(ids)):\n        scores[i] = scores[i] / sum(scores[i])\n\n    # Convex combination\n    ids_fused = []\n    scores_fused = []\n    for i in range(len(ids)):\n        ids_fused += ids[i]\n        scores_fused += scores[i]\n\n    # Sort by scores\n    ids_fused, scores_fused = zip(*sorted(zip(ids_fused, scores_fused), key=lambda x: x[1], reverse=True))\n\n    # Select top_k results\n    ids_fused = list(ids_fused)[:top_k]\n    scores_fused = list(scores_fused)[:top_k]\n\n    return ids_fused, scores_fused"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the input arguments are valid\n    if len(ids) != len(scores):\n        raise ValueError(\"The length of ids and scores must match\")\n    if len(ids) != len(weights):\n        raise ValueError(\"The length of ids and weights must match\")\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of weights must equal 1\")\n\n    # Normalize the scores\n    normalized_scores = []\n    for score in scores:\n        normalized_scores.append(score / sum(score))\n\n    # Combine the scores using the convex combination method\n    fused_scores = []\n    for i in range(len(normalized_scores)):\n        fused_scores.append(normalized_scores[i] * weights[i])\n\n    # Select the top k results\n    fused_ids = []\n    fused_scores = []\n    for i in range(top_k):\n        max_score = max(fused_scores)\n        max_index = fused_scores.index(max_score)\n        fused_ids.append(ids[max_index][i])\n        fused_scores.append(scores[max_index][i])\n\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the input arguments are valid\n    if len(ids) != len(scores):\n        raise ValueError(\n            \"The length of the ids tuple must match the length of the scores tuple\")\n    if len(weights) != len(ids):\n        raise ValueError(\n            \"The length of the weights tuple must match the length of the ids tuple\")\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1\")\n\n    # Initialize the fused ids and scores lists\n    fused_ids = []\n    fused_scores = []\n\n    # Iterate over the ids and scores tuples\n    for i in range(len(ids)):\n        # Normalize the scores\n        scores[i] = scores[i] / sum(scores[i])\n        # Combine the scores using the convex combination method\n        fused_scores.append(\n            [sum(weights[i] * scores[i][j] for i in range(len(scores))) for j in range(len(scores[0]))])\n        # Select the top_k results\n        top_k_indices = np.argsort(fused_scores[i])[-top_k:]\n        # Append the fused ids and scores to the lists\n        fused_ids.append([ids[i][j] for j in top_k_indices])\n        fused_scores.append([fused_scores[i][j] for j in top_k_indices])\n\n    # Return the fused ids and scores\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the number of ids and scores tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\"The number of ids and scores tuples must match.\")\n\n    # Check if the number of weights matches the number of ids and scores tuples\n    if len(weights) != len(ids):\n        raise ValueError(\n            \"The number of weights must match the number of ids and scores tuples.\")\n\n    # Check if the sum of the weights is 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    # Normalize the scores of each retrieval result\n    normalized_scores = []\n    for i in range(len(ids)):\n        normalized_scores.append(\n            [score / sum(scores[i]) for score in scores[i]])\n\n    # Combine the normalized scores of each retrieval result using a convex combination method\n    fused_scores = []\n    fused_ids = []\n    for i in range(len(ids)):\n        fused_scores.extend(\n            [score * weights[i] for score in normalized_scores[i]])\n        fused_ids.extend([id for id in ids[i]])\n\n    # Select the top_k results based on the fused scores\n    top_k_indices = np.argsort(fused_scores)[-top_k:]\n    top_k_ids = [fused_ids[i] for i in top_k_indices]\n    top_k_scores = [fused_scores[i] for i in top_k_indices]\n\n    return top_k_ids, top_k_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the input arguments are valid\n    if len(ids) != len(scores):\n        raise ValueError(\"The ids and scores tuples must have the same length.\")\n    if len(ids) != len(weights):\n        raise ValueError(\"The weights tuple must have the same length as the ids and scores tuples.\")\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    # Normalize the scores and combine them using the weights\n    normalized_scores = []\n    for score_list in scores:\n        score_sum = sum(score_list)\n        normalized_scores.append([score / score_sum for score in score_list])\n    combined_scores = [sum(a * b for a, b in zip(score_list, weights)) for score_list in zip(*normalized_scores)]\n\n    # Select the top_k results\n    top_ids = []\n    top_scores = []\n    for score_list, id_list in zip(combined_scores, ids):\n        top_k_indices = np.argsort(score_list)[::-1][:top_k]\n        top_ids.append([id_list[i] for i in top_k_indices])\n        top_scores.append([score_list[i] for i in top_k_indices])\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights), \"The length of the ids, scores and weights tuples must be the same.\"\n\n    assert abs(sum(weights) - 1) < 1e-3, \"The sum of the weights must be equal to 1.\"\n\n    # Normalize the scores\n    scores = [[(score - min(score)) / (max(score) - min(score)) for score in score_list] for score_list in scores]\n\n    # Combine the scores using a convex combination method\n    fused_scores = [sum([score * weight for score, weight in zip(score_list, weights)]) for score_list in scores]\n\n    # Sort the fused scores and retrieve the top_k results\n    top_k_indices = sorted(range(len(fused_scores)), key=lambda i: fused_scores[i], reverse=True)[:top_k]\n\n    # Retrieve the fused ids and scores for the top_k results\n    fused_ids = [ids[i][j] for i in range(len(ids)) for j in top_k_indices]\n    fused_scores = [fused_scores[i][j] for i in range(len(fused_scores)) for j in top_k_indices]\n\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights), \"Length of ids, scores, and weights must be equal\"\n    assert sum(weights) == 1, \"Sum of weights must equal 1\"\n\n    # Normalize scores\n    normalized_scores = []\n    for score in scores:\n        normalized_scores.append([s / sum(score) for s in score])\n\n    # Fuse scores\n    fused_scores = []\n    for i in range(len(normalized_scores)):\n        fused_scores.append([normalized_scores[i][j] * weights[i] for j in range(len(normalized_scores[i]))])\n\n    # Sum scores\n    summed_scores = [sum(fused_scores[i]) for i in range(len(fused_scores))]\n\n    # Sort scores\n    sorted_indices = sorted(range(len(summed_scores)), key=lambda k: summed_scores[k], reverse=True)\n\n    # Select top_k results\n    top_ids = []\n    top_scores = []\n    for i in range(top_k):\n        index = sorted_indices[i]\n        top_ids.append(ids[index])\n        top_scores.append(scores[index])\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Initialize the fused ids and scores lists\n    fused_ids = []\n    fused_scores = []\n\n    # Loop through the ids and scores tuples\n    for i in range(len(ids)):\n        # Get the ids and scores for the current retrieval result\n        ids_i = ids[i]\n        scores_i = scores[i]\n\n        # Normalize the scores\n        scores_i = scores_i / np.sum(scores_i)\n\n        # Compute the fused scores\n        fused_scores_i = scores_i * weights[i]\n\n        # Add the fused scores to the fused scores list\n        fused_scores.extend(fused_scores_i)\n\n        # Add the ids to the fused ids list\n        fused_ids.extend(ids_i)\n\n    # Sort the fused ids and scores in descending order\n    fused_ids, fused_scores = zip(*sorted(zip(fused_ids, fused_scores), key=lambda x: x[1], reverse=True))\n\n    # Select the top k results\n    fused_ids = fused_ids[:top_k]\n    fused_scores = fused_scores[:top_k]\n\n    # Return the fused ids and scores\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the input arguments are valid\n    assert len(ids) == len(scores) == len(weights), \"The number of ids, scores and weights must match\"\n    assert sum(weights) == 1, \"The sum of the weights must equal 1\"\n\n    # Normalize the scores\n    normalized_scores = []\n    for score_list, weight in zip(scores, weights):\n        normalized_scores.append([score * weight for score in score_list])\n\n    # Concatenate the ids and scores\n    concatenated_ids = []\n    concatenated_scores = []\n    for id_list, score_list in zip(ids, normalized_scores):\n        concatenated_ids.extend(id_list)\n        concatenated_scores.extend(score_list)\n\n    # Sort the concatenated ids and scores by score\n    sorted_indices = sorted(range(len(concatenated_scores)), key=lambda k: concatenated_scores[k], reverse=True)\n    concatenated_ids = [concatenated_ids[i] for i in sorted_indices]\n    concatenated_scores = [concatenated_scores[i] for i in sorted_indices]\n\n    # Select the top_k ids and scores\n    top_ids = concatenated_ids[:top_k]\n    top_scores = concatenated_scores[:top_k]\n\n    return top_ids, top_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the lengths of the input tuples match\n    assert len(ids) == len(scores) == len(weights), \"The lengths of the input tuples do not match.\"\n\n    # Check that the sum of the weights is equal to 1\n    assert sum(weights) == 1, \"The sum of the weights must equal 1.\"\n\n    # Initialize the fused ids and scores lists\n    fused_ids = []\n    fused_scores = []\n\n    # Iterate over the ids and scores tuples\n    for i in range(len(ids)):\n        # Normalize the scores of the current retrieval result\n        scores[i] = [score / sum(scores[i]) for score in scores[i]]\n\n        # Combine the normalized scores with the weights\n        combined_scores = [score * weights[i] for score in scores[i]]\n\n        # Combine the ids with the combined scores\n        for j in range(len(ids[i])):\n            fused_ids.append(ids[i][j])\n            fused_scores.append(combined_scores[j])\n\n    # Sort the fused ids and scores based on their scores\n    fused_ids, fused_scores = zip(*sorted(zip(fused_ids, fused_scores), key=lambda x: x[1], reverse=True))\n\n    # Select the top_k ids and scores\n    fused_ids = list(fused_ids)[:top_k]\n    fused_scores = list(fused_scores)[:top_k]\n\n    # Return the fused ids and scores\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the lengths of the ids and scores tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\n            \"The length of the ids tuple must match the length of the scores tuple\")\n\n    # Check that the lengths of the ids and scores tuples match the lengths of the weights tuple\n    if len(ids) != len(weights):\n        raise ValueError(\n            \"The length of the ids tuple must match the length of the weights tuple\")\n\n    # Check that the sum of the weights is 1\n    if not math.isclose(sum(weights), 1):\n        raise ValueError(\"The sum of the weights must equal 1\")\n\n    # Initialize the fused ids and scores lists\n    fused_ids = []\n    fused_scores = []\n\n    # Loop over the ids and scores tuples\n    for ids_list, scores_list in zip(ids, scores):\n        # Normalize the scores\n        scores_list = [score / sum(scores_list) for score in scores_list]\n        # Combine the scores using the convex combination method\n        fused_score = [\n            sum([w * s for w, s in zip(weights, scores_list)])]\n        # Select the top_k results\n        top_k_indices = sorted(\n            range(len(fused_score)), key=lambda i: fused_score[i], reverse=True)[:top_k]\n        # Append the fused ids and scores to the lists\n        fused_ids.extend([ids_list[i] for i in top_k_indices])\n        fused_scores.extend([fused_score[i] for i in top_k_indices])\n\n    # Return the fused ids and scores\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the input arguments are valid\n    assert len(ids) == len(scores), \"The ids and scores tuples must have the same length.\"\n    assert len(ids) == len(weights), \"The ids and weights tuples must have the same length.\"\n\n    # Normalize the scores of each retrieval result\n    norm_scores = [scores[i] / sum(scores[i]) for i in range(len(scores))]\n\n    # Create a list of lists to store the fused results\n    fused_ids = []\n    fused_scores = []\n\n    # Loop through the top_k results\n    for i in range(top_k):\n        # Initialize the fused id and score\n        fused_id = \"\"\n        fused_score = 0\n\n        # Loop through the retrieval results\n        for j in range(len(ids)):\n            # Check if the current retrieval result has enough results to fuse\n            if len(ids[j]) > i:\n                # Fuse the id and score of the current retrieval result\n                fused_id += ids[j][i]\n                fused_score += norm_scores[j][i] * weights[j]\n\n        # Add the fused id and score to the lists\n        fused_ids.append(fused_id)\n        fused_scores.append(fused_score)\n\n    return fused_ids, fused_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the lengths of the input arguments are consistent\n    assert len(ids) == len(scores) == len(weights)\n\n    # Initialize the fused ids and scores\n    fused_ids = []\n    fused_scores = []\n\n    # Iterate through the ids and scores tuples\n    for i in range(len(ids)):\n        # Normalize the scores\n        scores[i] = scores[i] / sum(scores[i])\n        # Combine the scores using the convex combination method\n        fused_scores.append(scores[i] * weights[i])\n        fused_ids.append(ids[i])\n\n    # Select the top_k results\n    top_k_ids, top_k_scores = select_top_k(fused_ids, fused_scores, top_k)\n\n    return top_k_ids, top_k_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    if len(ids) != len(scores):\n        raise ValueError(\"The length of ids and scores must be the same.\")\n    if len(ids) != len(weights):\n        raise ValueError(\"The length of weights must be the same as the length of ids and scores.\")\n    if not isinstance(top_k, int) or top_k <= 0:\n        raise ValueError(\"top_k must be a positive integer.\")\n    if not all(isinstance(w, float) for w in weights):\n        raise ValueError(\"Weights must be floats.\")\n    if not math.isclose(sum(weights), 1):\n        raise ValueError(\"The sum of weights must be 1.\")\n\n    fused_ids = []\n    fused_scores = []\n\n    # Normalize scores\n    normalized_scores = []\n    for i in range(len(ids)):\n        normalized_scores.append([score / sum(scores[i]) for score in scores[i]])\n\n    # Fuse scores\n    for i in range(len(ids)):\n        fused_scores.extend([weight * score for score in normalized_scores[i]])\n\n    # Sort and select top k results\n    fused_ids, fused_scores = zip(*sorted(zip(fused_ids, fused_scores), key=lambda x: x[1], reverse=True))\n    fused_ids = fused_ids[:top_k]\n    fused_scores = fused_scores[:top_k]\n\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the lengths of the input tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\"The lengths of the ids and scores tuples must match.\")\n\n    # Check that the lengths of the weights tuple match the lengths of the ids and scores tuples\n    if len(weights) != len(ids):\n        raise ValueError(\"The length of the weights tuple must match the length of the ids and scores tuples.\")\n\n    # Check that the sum of the weights is equal to 1\n    if not math.isclose(sum(weights), 1.0):\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    # Initialize empty lists for the fused ids and scores\n    fused_ids = []\n    fused_scores = []\n\n    # Loop through each retrieval result\n    for i in range(len(ids)):\n\n        # Normalize the scores of this retrieval result\n        scores[i] = [score / sum(scores[i]) for score in scores[i]]\n\n        # Multiply the scores of this retrieval result by their corresponding weight\n        scores[i] = [score * weights[i] for score in scores[i]]\n\n        # Combine the ids and scores of this retrieval result with the fused ids and scores\n        fused_ids.extend(ids[i])\n        fused_scores.extend(scores[i])\n\n    # Combine the fused ids and scores into a single list of tuples\n    fused_results = list(zip(fused_ids, fused_scores))\n\n    # Sort the fused results by score in descending order\n    fused_results.sort(key=lambda x: x[1], reverse=True)\n\n    # Select the top_k results from the fused results\n    fused_results = fused_results[:top_k]\n\n    # Unpack the fused ids and scores into separate lists\n    fused_ids, fused_scores = zip(*fused_results)\n\n    # Return"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Initialize variables\n    ids_fused = []\n    scores_fused = []\n\n    # Check if the length of ids and scores tuples are the same\n    if len(ids) != len(scores):\n        raise ValueError(\"Length of ids and scores tuples must be the same.\")\n\n    # Check if the length of weights is the same as the length of ids and scores tuples\n    if len(weights) != len(ids):\n        raise ValueError(\"Length of weights must match the length of ids and scores tuples.\")\n\n    # Check if the sum of weights is equal to 1\n    if sum(weights) != 1:\n        raise ValueError(\"Sum of weights must equal 1.\")\n\n    # Normalize the scores of each retrieval result\n    scores_norm = []\n    for score in scores:\n        scores_norm.append([s / sum(score) for s in score])\n\n    # Fuse the scores of each retrieval result using a convex combination method\n    for i in range(len(ids[0])):\n        score_fused = 0\n        for j in range(len(ids)):\n            score_fused += scores_norm[j][i] * weights[j]\n        ids_fused.append(ids[0][i])\n        scores_fused.append(score_fused)\n\n    # Sort the fused results in descending order of scores\n    scores_fused, ids_fused = zip(*sorted(zip(scores_fused, ids_fused), reverse=True))\n\n    # Select the top_k fused results\n    ids_fused = ids_fused[:top_k]\n    scores_fused = scores_fused[:top_k]\n\n    return ids_fused, scores_fused\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Initialize the output lists\n    ids_out = []\n    scores_out = []\n\n    # Check that the number of ids and scores lists in the tuple is the same\n    if len(ids) != len(scores):\n        raise ValueError(\n            \"The number of ids and scores lists in the tuple must be the same.\")\n\n    # Check that the number of weights is the same as the number of ids and scores lists\n    if len(weights) != len(ids):\n        raise ValueError(\n            \"The number of weights must be the same as the number of ids and scores lists.\")\n\n    # Check that the sum of the weights is 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    # Initialize a list of indices to be used for sorting\n    indices = list(range(len(ids[0])))\n\n    # Loop over each retrieval result\n    for i in range(len(ids)):\n        # Normalize the scores of the current retrieval result\n        scores[i] = scores[i] / sum(scores[i])\n\n        # Combine the scores of the current retrieval result with the previous results\n        if i == 0:\n            scores_out = scores[i]\n        else:\n            scores_out = [scores_out[j] + scores[i][j]\n                          for j in range(len(scores_out))]\n\n    # Sort the indices based on the combined scores\n    indices.sort(key=lambda x: scores_out[x], reverse=True)\n\n    # Select the top_k indices\n    indices = indices[:top_k]\n\n    # Extract the ids and scores of the top_k indices\n    ids_out = [ids[i][indices[j]] for i in range(len(ids)) for j in range(top_k)]\n    scores_out = [scores[i][indices[j]] for i in range(len(ids)) for j in range(top_k)]\n\n    # Return the ids and scores of"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x == x:\n        if percent:\n            return f\"{x * 100:.1f}%\"\n        else:\n            if x < 1:\n                return f\"{x:.5f}\"\n            elif x < 10:\n                return f\"{x:.4f}\"\n            elif x < 100:\n                return f\"{x:.3f}\"\n            elif x < 1000:\n                return f\"{x:.2f}\"\n            else:\n                return f\"{x:.1f}\"\n    else:\n        return \"NaN\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if x == 0:\n        return '0'\n\n    if x < 0.0001:\n        return '<0.0001'\n\n    if x < 0.001:\n        return '<0.001'\n\n    if x < 0.01:\n        return '<0.01'\n\n    if x < 0.1:\n        return '<0.1'\n\n    if x < 1:\n        return '<1'\n\n    if x < 10:\n        return '<10'\n\n    if x < 100:\n        return '<100'\n\n    if x < 1000:\n        return '<1000'\n\n    if x < 10000:\n        return '<10000'\n\n    if x < 100000:\n        return '<100000'\n\n    if x < 1000000:\n        return '<1000000'\n\n    if x < 10000000:\n        return '<10000000'\n\n    if x < 100000000:\n        return '<100000000'\n\n    if x < 1000000000:\n        return '<1000000000'\n\n    if x < 10000000000:\n        return '<10000000000'\n\n    if x < 100000000000:\n        return '<100000000000'\n\n    if x < 1000000000000:\n        return '<1000000000000'\n\n    if x < 100000000"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x == float('nan'):\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if x < 1e-1:\n        return f'{x:.1e}'\n    elif x < 1e1:\n        return f'{x:.1f}'\n    elif x < 1e2:\n        return f'{x:.2f}'\n    elif x < 1e3:\n        return f'{x:.3f}'\n    elif x < 1e4:\n        return f'{x:.4f}'\n    elif x < 1e5:\n        return f'{x:.5f}'\n    elif x < 1e6:\n        return f'{x:.6f}'\n    elif x < 1e7:\n        return f'{x:.7f}'\n    elif x < 1e8:\n        return f'{x:.8f}'\n    elif x < 1e9:\n        return f'{x:.9f}'\n    elif x < 1e10:\n        return f'{x:.10f}'\n    else:\n        return f'{x:.11f}'"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x == x:\n\n        if percent:\n            x *= 100\n\n        if x < 1e-3:\n            return f\"{x:.3f}%\" if percent else f\"{x:.3f}\"\n        elif x < 1e-2:\n            return f\"{x:.2f}%\" if percent else f\"{x:.2f}\"\n        elif x < 1e-1:\n            return f\"{x:.1f}%\" if percent else f\"{x:.1f}\"\n        else:\n            return f\"{x:.0f}%\" if percent else f\"{x:.0f}\"\n\n    return \"NaN\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if x < 1e-2:\n        return f\"{x:.2e}\"\n    elif x < 1e-1:\n        return f\"{x:.3f}\"\n    elif x < 1:\n        return f\"{x:.4f}\"\n    elif x < 10:\n        return f\"{x:.5f}\"\n    elif x < 100:\n        return f\"{x:.6f}\"\n    else:\n        return f\"{x:.7f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n\n    if percent:\n        x *= 100\n\n    if x >= 1e3:\n        return f\"{x / 1e3:.0f}k\"\n\n    if x >= 1:\n        return f\"{x:.0f}\"\n\n    if x >= 1e-3:\n        return f\"{x:.3f}\"\n\n    if x >= 1e-6:\n        return f\"{x:.6f}\"\n\n    return f\"{x:.9f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    # If the input number is NaN, return the string representation of NaN\n    if np.isnan(x):\n        return str(x)\n\n    # If the number is a percentage, multiply it by 100 and add a percentage sign\n    if percent:\n        x *= 100\n        return f\"{x:.0f}%\"\n\n    # Determine the number of decimal places based on the value of the number\n    if x == 0:\n        decimal_places = 0\n    elif x < 1:\n        decimal_places = -int(np.floor(np.log10(x)))\n    else:\n        decimal_places = -int(np.floor(np.log10(x))) + 1\n\n    # Format the number with the determined number of decimal places\n    return f\"{x:.{decimal_places}f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    else:\n        if percent:\n            x = x * 100\n        if x >= 1:\n            return f\"{x:.0f}%\"\n        elif x >= 0.001:\n            return f\"{x:.2f}%\"\n        elif x >= 0.00001:\n            return f\"{x:.4f}%\"\n        else:\n            return f\"{x:.6f}%\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n\n    if percent:\n        x *= 100\n\n    if x < 1e-3:\n        return f\"{x:.2e}\"\n    elif x < 1e-2:\n        return f\"{x:.3f}\"\n    elif x < 1e-1:\n        return f\"{x:.4f}\"\n    elif x < 1e0:\n        return f\"{x:.5f}\"\n    elif x < 1e1:\n        return f\"{x:.6f}\"\n    elif x < 1e2:\n        return f\"{x:.7f}\"\n    elif x < 1e3:\n        return f\"{x:.8f}\"\n    elif x < 1e4:\n        return f\"{x:.9f}\"\n    elif x < 1e5:\n        return f\"{x:.0f}\"\n    elif x < 1e6:\n        return f\"{x / 1e3:.1f}K\"\n    elif x < 1e7:\n        return f\"{x / 1e3:.1f}K\"\n    elif x < 1e8:\n        return f\"{x / 1e6:.1f}M\"\n    elif x < 1e9:\n        return f\"{x / 1e6:.1f}M\"\n    elif x < 1e10:\n        return f\"{x / 1e9:.1f}B\"\n    elif x < 1e11:\n        return f\"{x / 1e9:.1f}B\"\n    elif x < 1e12:\n        return f\"{x / 1e12:.1f}T\"\n    else:\n        return f\"{x / 1e12:.1f}T\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x == x:\n\n        if percent:\n            return f'{x * 100:.2f}%'\n        else:\n            return f'{x:.2f}'\n\n    else:\n        return str(x)\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x == x:\n        if percent:\n            x *= 100\n            return f'{x:.0f}%'\n        else:\n            return f'{x:.0f}'\n    else:\n        return 'NaN'\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if x < 1e-4:\n        return f\"{x:.2e}\"\n    elif x < 1e-2:\n        return f\"{x:.3f}\"\n    elif x < 1:\n        return f\"{x:.4f}\"\n    elif x < 10:\n        return f\"{x:.5f}\"\n    elif x < 100:\n        return f\"{x:.6f}\"\n    elif x < 1000:\n        return f\"{x:.7f}\"\n    elif x < 10000:\n        return f\"{x:.8f}\"\n    else:\n        return f\"{x:.9f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n\n    if percent:\n        x *= 100\n        suffix = \"%\"\n    else:\n        suffix = \"\"\n\n    if x >= 1000000000:\n        x /= 1000000000\n        suffix += \"B\"\n    elif x >= 1000000:\n        x /= 1000000\n        suffix += \"M\"\n    elif x >= 1000:\n        x /= 1000\n        suffix += \"k\"\n\n    if x < 1:\n        return f\"{x:.2f}{suffix}\"\n    elif x < 10:\n        return f\"{x:.1f}{suffix}\"\n    else:\n        return f\"{x:.0f}{suffix}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    # If the input is NaN, return the string representation of NaN\n    if x != x:\n        return str(x)\n\n    # If the input is zero, return the string representation of zero\n    if x == 0:\n        return str(x)\n\n    # If the input is positive and less than or equal to 1, format it as a percentage with two decimal places\n    if x > 0 and x <= 1:\n        if percent:\n            return f\"{x * 100:.2f}%\"\n        else:\n            return f\"{x:.2f}\"\n\n    # If the input is positive and greater than 1, format it with three decimal places\n    if x > 1:\n        if percent:\n            return f\"{x * 100:.3f}%\"\n        else:\n            return f\"{x:.3f}\"\n\n    # If the input is negative and less than or equal to -1, format it as a percentage with two decimal places\n    if x < 0 and x >= -1:\n        if percent:\n            return f\"{x * 100:.2f}%\"\n        else:\n            return f\"{x:.2f}\"\n\n    # If the input is negative and greater than -1, format it with three decimal places\n    if x < -1:\n        if percent:\n            return f\"{x * 100:.3f}%\"\n        else:\n            return f\"{x:.3f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x == float(\"nan\"):\n        return \"NaN\"\n\n    if percent:\n        x *= 100\n\n    if x >= 1000000000000:\n        return f\"{x/1000000000000:.1f}T\"\n    elif x >= 1000000000:\n        return f\"{x/1000000000:.1f}B\"\n    elif x >= 1000000:\n        return f\"{x/1000000:.1f}M\"\n    elif x >= 1000:\n        return f\"{x/1000:.1f}K\"\n    else:\n        if x % 1 == 0:\n            return f\"{int(x)}\"\n        else:\n            return f\"{x:.1f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n        return f\"{x:.0f}%\"\n    if x < 1e-3:\n        return f\"{x:.3e}\"\n    if x < 1:\n        return f\"{x:.3f}\"\n    return f\"{x:.0f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    else:\n        if percent:\n            x *= 100\n        if x < 1e-3:\n            return f\"{x:.3e}\"\n        elif x < 1:\n            return f\"{x:.2f}\"\n        else:\n            return f\"{x:.0f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if x >= 1e3:\n        x /= 1e3\n        if x >= 1e3:\n            x /= 1e3\n            if x >= 1e3:\n                x /= 1e3\n                return f'{x:.0f}T'\n            return f'{x:.0f}B'\n        return f'{x:.0f}M'\n    return f'{x:.2f}'\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    import numpy as np\n\n    if np.isnan(x):\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if x < 1e-6:\n        return f\"{x:1.2e}\"\n    elif x < 1e-3:\n        return f\"{x:1.3f}\"\n    elif x < 1:\n        return f\"{x:1.4f}\"\n    elif x < 10:\n        return f\"{x:1.5f}\"\n    elif x < 100:\n        return f\"{x:1.6f}\"\n    else:\n        return f\"{x:1.7f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    else:\n        if percent:\n            x *= 100\n            x = np.round(x, 1)\n            return str(x) + '%'\n        else:\n            x = np.round(x, 2)\n            if x % 1 == 0:\n                return str(int(x))\n            else:\n                return str(x)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        disk_usage = os.statvfs(input_dir)\n        free_space = disk_usage.f_bavail * disk_usage.f_frsize\n        free_space_in_gb = free_space / (1024 ** 3)\n        if free_space_in_gb > threshold_in_gb:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        disk_usage = os.statvfs(input_dir)\n        free_space_in_gb = disk_usage.f_bfree * disk_usage.f_frsize / 1024 / 1024 / 1024\n        if free_space_in_gb > threshold_in_gb:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        free_space = os.statvfs(input_dir).f_bavail * os.statvfs(input_dir).f_frsize\n        free_space_gb = free_space / (1024 ** 3)\n        if free_space_gb < threshold_in_gb:\n            print(f\"Free disk space is lower than {threshold_in_gb} GB. Waiting for {sleep_time} seconds...\")\n            time.sleep(sleep_time)\n        else:\n            print(f\"Free disk space is higher than {threshold_in_gb} GB. Continuing...\")\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        free_space_in_gb = os.statvfs(input_dir).f_bavail * os.statvfs(input_dir).f_frsize / 1024 / 1024 / 1024\n        if free_space_in_gb > threshold_in_gb:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        # Get the free disk space in gigabytes\n        free_space_in_gb = os.statvfs(input_dir).f_bfree * os.statvfs(input_dir).f_frsize / 1024 / 1024 / 1024\n\n        # Check if the free space is lower than the threshold\n        if free_space_in_gb < threshold_in_gb:\n            # Wait for a while before checking the disk usage again\n            time.sleep(sleep_time)\n        else:\n            # Exit the loop when the free space is higher than the threshold\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        disk_usage = os.statvfs(input_dir)\n        free_space = disk_usage.f_bavail * disk_usage.f_frsize / (1024 ** 3)\n        if free_space > threshold_in_gb:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        free_space = os.statvfs(input_dir).f_bavail * os.statvfs(input_dir).f_frsize\n        free_space_in_gb = free_space / (1024 ** 3)\n        if free_space_in_gb < threshold_in_gb:\n            break\n        else:\n            time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import time\n    import shutil\n\n    while shutil.disk_usage(input_dir).free / (1024 ** 3) > threshold_in_gb:\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import time\n    import shutil\n\n    while True:\n        free_space_in_gb = shutil.disk_usage(input_dir).free / (1024 ** 3)\n        if free_space_in_gb < threshold_in_gb:\n            print(f\"Free space is below {threshold_in_gb} GB. Waiting for {sleep_time} seconds before checking again.\")\n            time.sleep(sleep_time)\n        else:\n            print(f\"Free space is above {threshold_in_gb} GB. Continuing with the execution.\")\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n    while True:\n        disk_usage = os.statvfs(input_dir)\n        free_space_in_gb = disk_usage.f_bavail * disk_usage.f_frsize / 1024 / 1024 / 1024\n        if free_space_in_gb > threshold_in_gb:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n    from hurry.filesize import size\n\n    while True:\n        # Get the free disk space in bytes\n        free_space = os.statvfs(input_dir).f_bavail * os.statvfs(input_dir).f_frsize\n\n        # Convert the free space to gigabytes\n        free_space_gb = free_space / (1024 ** 3)\n\n        # Print the free space in gigabytes\n        print(f\"Free space in {input_dir}: {size(free_space)}\")\n\n        # Check if the free space is lower than the threshold\n        if free_space_gb > threshold_in_gb:\n            break\n\n        # Sleep for the specified time before checking the disk usage again\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n    from hurry.filesize import size\n\n    while True:\n        disk_usage = os.statvfs(input_dir)\n        free_space = disk_usage.f_bsize * disk_usage.f_bavail\n        free_space_gb = free_space / (1024 ** 3)\n        print(f\"Free space: {size(free_space)} ({free_space_gb:.2f} GB)\")\n        if free_space_gb > threshold_in_gb:\n            print(f\"Free space is higher than {threshold_in_gb} GB. Waiting for {sleep_time} seconds...\")\n            time.sleep(sleep_time)\n        else:\n            print(f\"Free space is lower than {threshold_in_gb} GB. Continuing...\")\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n    import psutil\n\n    while psutil.disk_usage(input_dir).free / 1024**3 > threshold_in_gb:\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        free_space_in_gb = _get_free_space_in_gb(input_dir)\n        if free_space_in_gb < threshold_in_gb:\n            print(f\"Free disk space is lower than the threshold ({threshold_in_gb} GB). Waiting for {sleep_time} seconds...\")\n            time.sleep(sleep_time)\n        else:\n            print(f\"Free disk space is higher than the threshold ({threshold_in_gb} GB). Continuing...\")\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n    while True:\n        free_space = _get_free_space_in_gb(input_dir)\n        if free_space < threshold_in_gb:\n            time.sleep(sleep_time)\n        else:\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        free_space = os.statvfs(input_dir).f_bavail * os.statvfs(input_dir).f_frsize / 1024 / 1024 / 1024\n        if free_space < threshold_in_gb:\n            time.sleep(sleep_time)\n        else:\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        free_space = os.statvfs(input_dir).f_bfree * os.statvfs(input_dir).f_frsize\n        free_space_in_gb = free_space / (1024 ** 3)\n        if free_space_in_gb > threshold_in_gb:\n            break\n        else:\n            time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        disk_usage = os.statvfs(input_dir)\n        free_space_in_gb = disk_usage.f_bavail * disk_usage.f_frsize / 1024 / 1024 / 1024\n        if free_space_in_gb < threshold_in_gb:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        free_space = os.statvfs(input_dir).f_bavail * os.statvfs(input_dir).f_frsize\n        if free_space > threshold_in_gb * 1024 * 1024 * 1024:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n    import psutil\n\n    while True:\n        disk_usage = psutil.disk_usage(input_dir)\n        free_space = disk_usage.free / (1024 ** 3)\n        if free_space < threshold_in_gb:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  return p * np.diff(t)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  return p * np.diff(t)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  return p * np.diff(t)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  return p * np.diff(t)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  return p * np.diff(t)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  return p * np.diff(t)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  return p * (t[1:] - t[:-1])"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  return p * np.diff(t)\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  return p * np.diff(t)\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  w = np.diff(t) * p[:-1]\n  w = np.append(w, 1 - np.sum(w))\n  return w"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  return p * np.diff(t)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Check if the input arrays have the same length\n  if len(t) != len(p):\n    raise ValueError(\"Input arrays 't' and 'p' must have the same length.\")\n\n  # Calculate the differences between consecutive elements in 't'\n  dt = np.diff(t)\n\n  # Multiply the PDF values by the differences to obtain the weights\n  w = p * dt\n\n  # Normalize the weights to sum to 1\n  w /= np.sum(w)\n\n  return w"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  return p * np.diff(t)\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in the time or position vector\n  dt = t[1:] - t[:-1]\n  # Multiply the PDF values by the differences between consecutive elements\n  w = p[:-1] * dt\n  # Normalize the weights to sum to 1\n  w = w / w.sum()\n  return w\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Calculate the differences between consecutive elements in 't'\n  diff = np.diff(t)\n\n  # Multiply the PDF values by the differences to obtain the weights\n  weights = p * diff\n\n  # Normalize the weights to sum to 1\n  weights = weights / weights.sum()\n\n  return weights\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Check if t and p have the same length\n  if len(t) != len(p):\n    raise ValueError(\"t and p must have the same length\")\n\n  # Calculate the differences between consecutive elements in t\n  dt = np.diff(t)\n\n  # Multiply the PDF values in p by the differences in t\n  w = p * dt\n\n  # Normalize the weights to sum to 1\n  w /= np.sum(w)\n\n  return w\n\n\n\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  if len(t) != len(p):\n    raise ValueError(\"t and p must have the same length.\")\n  if not np.isclose(np.sum(p), 1):\n    raise ValueError(\"The sum of p must be 1.\")\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"The elements of t must be strictly increasing.\")\n  return p * np.diff(t)\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  p = np.asarray(p)\n  t = np.asarray(t)\n  if len(p) != len(t):\n    raise ValueError('t and p must have the same length')\n  if not np.isclose(np.sum(p), 1):\n    raise ValueError('p must integrate to 1')\n  if len(p) < 2:\n    raise ValueError('p must have at least 2 elements')\n  weights = np.zeros_like(p)\n  weights[0] = p[0] * (t[1] - t[0])\n  weights[-1] = p[-1] * (t[-1] - t[-2])\n  weights[1:-1] = p[1:-1] * (t[2:] - t[:-2])\n  return weights / np.sum(weights)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  import numpy as np\n\n  # Calculate the differences between consecutive elements in the time or position vector\n  dt = np.diff(t)\n\n  # Multiply the PDF values by the differences between consecutive elements\n  w = p * dt\n\n  # Normalize the weights to sum to 1\n  w = w / np.sum(w)\n\n  return w\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  import numpy as np\n  import warnings\n\n  if np.isscalar(t):\n    t = np.array([t])\n    p = np.array([p])\n  else:\n    t = np.array(t)\n    p = np.array(p)\n\n  if len(t) != len(p):\n    raise ValueError('t and p must have the same length.')\n\n  if np.any(p < 0):\n    raise ValueError('All values in p must be non-negative.')\n\n  if not np.isclose(np.sum(p), 1):\n    warnings.warn('The sum of p is not close to 1.')\n\n  if np.any(np.diff(t) <= 0):\n    raise ValueError('The differences between consecutive elements in t must be positive.')\n\n  diff = np.diff(t)\n  weights = p * diff\n  return weights / np.sum(weights)\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text.\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens.\n    tokens = line_text.split(\",\")\n\n    return tokens"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    line_text = [line_text[i:i + 2] for i in range(0, len(line_text), 2)]\n\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text.\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens.\n    line_text = list(line_text)\n\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all spaces from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    line_text = list(line_text)\n\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all spaces from the text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the text into tokens\n    tokens = []\n    for i in range(0, len(line_text)):\n        tokens.append(line_text[i])\n\n    return tokens"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    line_text = line_text.split()\n\n    return line_text\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text.\n    line_text = line_text.replace('\\n', '').replace('\\r', '').replace('\\t', '').replace(' ', '')\n\n    # Segment the modified text into smaller parts or tokens.\n    line_text = [line_text[i:i + 10] for i in range(0, len(line_text), 10)]\n\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Removing all whitespace characters\n    line_text = line_text.replace('\\n', '')\n    line_text = line_text.replace('\\t', '')\n    line_text = line_text.replace('\\r', '')\n    line_text = line_text.replace(' ', '')\n\n    # Segmenting the text into tokens\n    tokens = line_text.split(',')\n\n    return tokens"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the text into smaller parts or tokens\n    tokens = [line_text[i:i + 2] for i in range(0, len(line_text), 2)]\n\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters\n    line_text = line_text.replace(' ', '')\n    line_text = line_text.replace('\\t', '')\n    line_text = line_text.replace('\\n', '')\n\n    # Segment the modified text into smaller parts or tokens\n    line_text_list = line_text.split('|')\n\n    return line_text_list"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all spaces from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the text into smaller parts or tokens\n    tokens = []\n    token = \"\"\n    for char in line_text:\n        if char.isalpha():\n            token += char\n        else:\n            if token:\n                tokens.append(token)\n            token = \"\"\n            tokens.append(char)\n    if token:\n        tokens.append(token)\n\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = []\n    for i in range(0, len(line_text), 2):\n        tokens.append(line_text[i:i + 2])\n\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Removes all spaces from the input text.\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segments the modified text into smaller parts or tokens.\n    line_text = line_text.split(\".\")\n\n    # Returns the segmented parts of the modified text.\n    return line_text\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = line_text.split()\n\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"-\", \" \")\n    line_text = line_text.replace(\"'\", \"\")\n\n    return line_text.split()\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text.\n    line_text = ''.join(line_text.split())\n\n    # Segment the modified text into smaller parts or tokens.\n    tokens = [line_text[i:i + 3] for i in range(0, len(line_text), 3)]\n\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(' ', '')\n\n    # Segment the modified text into smaller parts or tokens\n    line_text = line_text.split()\n\n    return line_text\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # remove all whitespace characters\n    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"\\n\", \"\")\n\n    # segment the text into smaller parts or tokens\n    tokens = []\n    start = 0\n    end = 0\n    for i in range(len(line_text)):\n        if line_text[i].isalpha() or line_text[i] == \"'\":\n            end += 1\n        else:\n            if end > start:\n                tokens.append(line_text[start:end])\n            start = i + 1\n            end = i + 1\n    if end > start:\n        tokens.append(line_text[start:end])\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Removing all whitespace characters from the input text\n    line_text = line_text.replace(' ', '')\n    line_text = line_text.replace('\\t', '')\n    line_text = line_text.replace('\\n', '')\n\n    # Segmenting the modified text into smaller parts or tokens\n    tokens = line_text.split('\\r')\n\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text.\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens.\n    line_text = [line_text[i:i + 10] for i in range(0, len(line_text), 10)]\n\n    return line_text\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    weights = np.ones(n)\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n    return weights / np.sum(weights)"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros must not exceed the total number of weights.\")\n    weights = np.random.uniform(0, 1, n)\n    weights = weights / np.sum(weights)\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    weights = np.random.uniform(size=n)\n    weights /= weights.sum()\n    if zeros > 0:\n        zeros_idx = np.random.choice(np.arange(n), zeros, replace=False)\n        weights[zeros_idx] = 0\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the total number of weights.\")\n    # Generate random weights\n    weights = np.random.rand(n)\n    # Normalize weights\n    weights /= weights.sum()\n    # Set zeros\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check inputs\n    if zeros > n:\n        raise ValueError(f\"The number of zeros ({zeros}) cannot exceed the number of weights ({n}).\")\n\n    # Generate random weights\n    weights = np.random.rand(n)\n\n    # Normalize weights\n    weights = weights / np.sum(weights)\n\n    # Set zeros\n    if zeros > 0:\n        # Randomly select indices to set to zero\n        zero_indices = np.random.choice(np.arange(n), zeros, replace=False)\n\n        # Set weights to zero\n        weights[zero_indices] = 0\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    weights = np.ones(n)\n    if zeros > 0:\n        weights = np.concatenate((weights, np.zeros(zeros)))\n    np.random.shuffle(weights)\n    return weights / np.sum(weights)\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check input\n    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the number of weights.\")\n\n    # Generate random weights and normalize\n    weights = np.random.rand(n)\n    weights = weights / sum(weights)\n\n    # Set zeros\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\n            \"The number of zeros cannot exceed the total number of weights to generate.\"\n        )\n\n    # Generate n-zeros\n    zeros_arr = np.zeros(zeros)\n\n    # Generate n-1-zeros\n    n_1_zeros_arr = np.zeros(n - zeros)\n\n    # Concatenate the arrays\n    weights = np.concatenate((zeros_arr, n_1_zeros_arr))\n\n    # Shuffle the array\n    np.random.shuffle(weights)\n\n    # Normalize the array\n    weights = weights / np.sum(weights)\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Create a list of weights with the specified number of zeros\n    weights = [1] * (n - zeros) + [0] * zeros\n    # Shuffle the list of weights\n    random.shuffle(weights)\n    # Convert the list to a numpy array and return it\n    return np.array(weights) / np.sum(weights)\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    assert n > 0, \"n must be greater than zero.\"\n    assert zeros <= n, \"zeros must be less than or equal to n.\"\n\n    # Generate n-zeros and n-1-ones\n    rand = np.concatenate([np.zeros(zeros), np.ones(n-zeros)])\n\n    # Shuffle and return\n    np.random.shuffle(rand)\n    return rand"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Generate a numpy array of n random weights that sum up to one\n    weights = np.random.rand(n)\n    weights /= weights.sum()\n\n    # If zeros is not zero, set zeros random weights to zero\n    if zeros:\n        indices = np.random.choice(n, zeros, replace=False)\n        weights[indices] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(f\"The number of zeros ({zeros}) cannot be greater than the number of weights ({n}).\")\n\n    weights = np.random.random(n)\n    weights /= weights.sum()\n    if zeros > 0:\n        weights[np.random.choice(np.arange(n), zeros, replace=False)] = 0\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check if zeros is valid\n    if zeros > n:\n        raise ValueError(\"zeros must not exceed n.\")\n\n    # Generate n random weights\n    weights = np.random.rand(n)\n\n    # Normalize the weights\n    weights /= weights.sum()\n\n    # Set zeros random weights to zero\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the total number of weights.\")\n\n    # Create a list of n-1 random weights that sum to 1\n    weights = np.random.uniform(0, 1, n - 1)\n    weights = weights / sum(weights)\n\n    # If zeros is not zero, randomly select zeros indices and set them to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(n - 1, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Initialize the output array with zeros\n    weights = np.zeros(n)\n\n    # If the number of zeros is zero, return the array with all weights set to 1\n    if zeros == 0:\n        return np.ones(n)\n\n    # Generate a random permutation of the indices of the array\n    indices = np.random.permutation(n)\n\n    # Set the first zeros indices to zero\n    weights[indices[:zeros]] = 0\n\n    # Normalize the weights to sum to one\n    weights /= np.sum(weights)\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check if zeros is valid\n    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the total number of weights.\")\n\n    # Generate a numpy array of n random weights that sum to one\n    weights = np.random.dirichlet(np.ones(n))\n\n    # Set zeros random weights to zero\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros must not exceed the total number of weights.\")\n    w = np.random.rand(n)\n    w = w / sum(w)\n    if zeros > 0:\n        w[np.random.choice(range(n), zeros, replace=False)] = 0\n    return w\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check input arguments\n    if not isinstance(n, int):\n        raise ValueError('n must be an integer.')\n    if not isinstance(zeros, int):\n        raise ValueError('zeros must be an integer.')\n    if n < 1:\n        raise ValueError('n must be at least 1.')\n    if zeros > n:\n        raise ValueError('zeros must be at most n.')\n\n    # Generate random weights and normalize them\n    weights = np.random.random(n)\n    weights = weights / weights.sum()\n\n    # Set specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Generate random weights that sum up to one\n    w = np.random.rand(n)\n    w /= w.sum()\n\n    # Set the specified number of weights to zero\n    if zeros:\n        # Create a mask of indices to zero out\n        mask = np.zeros(n, dtype=bool)\n        mask[np.random.choice(n, zeros, replace=False)] = True\n\n        # Zero out the specified weights\n        w[mask] = 0\n\n    return w\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Create an array of n random weights that sum up to one\n    weights = np.random.rand(n)\n    weights /= weights.sum()\n\n    # If zeros is not zero, then set zeros of the weights array to zero\n    if zeros:\n        # Get the indices of the weights that should be set to zero\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        # Set the weights at the zero indices to zero\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict['module_type']\n        module_params = {k: v for k, v in module_dict.items() if k != 'module_type'}\n        return cls(module_type, **module_params)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict['module_type']\n        module_params = {k: v for k, v in module_dict.items() if k != 'module_type'}\n        return cls(module_type, **module_params)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict['module_type']\n        module_params = {k: v for k, v in module_dict.items() if k != 'module_type'}\n        return cls(module_type, **module_params)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict['module_type']\n        module_params = {k: v for k, v in module_dict.items() if k != 'module_type'}\n        return cls(module_type, **module_params)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module = cls(**module_dict)\n        module.module_type = module_type\n        return module"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module = cls(**module_dict)\n        module.module_type = module_type\n        return module"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict['module_type']\n        module_params = {k: v for k, v in module_dict.items() if k != 'module_type'}\n        return cls(module_type, **module_params)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module = cls(module_type=module_type, **module_dict)\n        return module\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_params = module_dict\n\n        return cls(module_type, **module_params)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module = cls(module_type=module_type, **module_dict)\n        return module\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict['module_type']\n        module_params = {k: v for k, v in module_dict.items() if k != 'module_type'}\n        return cls(module_type, **module_params)\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict['module_type']\n        module_params = {key: value for key, value in module_dict.items() if key != 'module_type'}\n        return cls(module_type, **module_params)\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_dict['module_type'] = module_type\n        return cls(**module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict['module_type']\n        module_params = module_dict\n        module_params.pop('module_type')\n        return cls(module_type, **module_params)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_params = module_dict\n\n        return cls(module_type=module_type, **module_params)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict['module_type']\n        module_params = {k: v for k, v in module_dict.items() if k != 'module_type'}\n\n        return cls(module_type, **module_params)\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict['module_type']\n        module_params = {key: value for key, value in module_dict.items() if key != 'module_type'}\n        return cls(module_type, **module_params)\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_obj = cls.get_module_class(module_type)\n        return module_obj(**module_dict)\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module = cls.get_module(module_type)\n        module.from_dict(module_dict)\n        return module\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_params = module_dict\n        module_params['module_type'] = module_type\n        return cls(**module_params)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    if instance['mode'] == 'box':\n        bbox = instance['bbox']\n        x1, y1, x2, y2 = bbox\n        x1 = max(x1, 0)\n        y1 = max(y1, 0)\n        x2 = min(x2, image_size[1])\n        y2 = min(y2, image_size[0])\n        w = x2 - x1\n        h = y2 - y1\n        if w < crop_size[1] or h < crop_size[0]:\n            x1 = max(0, x1 - (crop_size[1] - w) // 2)\n            y1 = max(0, y1 - (crop_size[0] - h) // 2)\n            x2 = min(image_size[1], x1 + crop_size[1])\n            y2 = min(image_size[0], y1 + crop_size[0])\n        return CropTransform(x1, y1, x2, y2)\n    elif instance['mode'] == 'polygon':\n        polygon = instance['polygon']\n        xs = [pt[0] for pt in polygon]\n        ys = [pt[1] for pt in polygon]\n        x1 = min(xs)\n        y1 = min(ys)\n        x2 = max(xs)\n        y2 = max(ys)\n        w = x2 - x1\n        h = y2 - y1\n        if w < crop_size[1] or h < crop_size[0]:\n            x1 = max(0, x1 - (crop_size[1] - w) // 2)\n            y1 = max(0, y1 - (crop_size[0] - h) // 2)\n            x2 = min(image_size[1], x1 + crop_size[1])\n            y2 = min(image_size[0], y1 + crop_size[0])\n        return CropTransform(x1, y1, x2, y2)\n    else:\n       "}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Crop the image to the region that contains the instance\n    crop_transform = CropTransform(0, 0, image_size[0], image_size[1])\n    x1, y1, x2, y2 = instance[\"bbox\"]\n    w, h = x2 - x1, y2 - y1\n    x = max(0, x1 + w // 2 - crop_size[0] // 2)\n    y = max(0, y1 + h // 2 - crop_size[1] // 2)\n    x = min(x, image_size[0] - crop_size[0])\n    y = min(y, image_size[1] - crop_size[1])\n    crop_transform = CropTransform(x, y, crop_size[0], crop_size[1])\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    from detectron2.data.transforms import CropTransform\n\n    bbox = instance[\"bbox\"]\n    mode = instance[\"mode\"]\n\n    # Calculate the center of the bounding box\n    bbox_center_x = (bbox[0] + bbox[2]) / 2\n    bbox_center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the maximum distance from the center of the bounding box to the edges of the image\n    max_distance_x = min(bbox_center_x, image_size[1] - bbox_center_x)\n    max_distance_y = min(bbox_center_y, image_size[0] - bbox_center_y)\n\n    # Calculate the maximum distance from the center of the bounding box to the edges of the crop\n    max_crop_distance_x = min(max_distance_x, crop_size[1] / 2)\n    max_crop_distance_y = min(max_distance_y, crop_size[0] / 2)\n\n    # Calculate the top-left corner of the crop\n    crop_top_left_x = int(bbox_center_x - max_crop_distance_x)\n    crop_top_left_y = int(bbox_center_y - max_crop_distance_y)\n\n    # Calculate the dimensions of the crop\n    crop_width = int(2 * max_crop_distance_x)\n    crop_height = int(2 * max_crop_distance_y)\n\n    # Create the CropTransform object\n    crop_transform = CropTransform(\n        crop_size=crop_size,\n        crop_region=(crop_top_left_x, crop_top_left_y, crop_width, crop_height),\n        mode=mode,\n    )\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    crop_transform = CropTransform(0, 0, image_size[0], image_size[1])\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    if bbox_mode == BoxMode.XYXY_ABS:\n        # crop with a bounding box\n        crop_transform = crop_transform.crop(bbox, crop_size)\n    elif bbox_mode == BoxMode.XYWH_ABS:\n        # crop with a bounding box\n        crop_transform = crop_transform.crop(\n            [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]], crop_size\n        )\n    elif bbox_mode == BoxMode.XYWH_REL:\n        # crop with a bounding box\n        crop_transform = crop_transform.crop(\n            [bbox[0] * image_size[1], bbox[1] * image_size[0], bbox[2] * image_size[1], bbox[3] * image_size[0]],\n            crop_size\n        )\n    elif bbox_mode == BoxMode.XYXY_REL:\n        # crop with a bounding box\n        crop_transform = crop_transform.crop(\n            [bbox[0] * image_size[1], bbox[1] * image_size[0], bbox[2] * image_size[1], bbox[3] * image_size[0]],\n            crop_size\n        )\n    else:\n        raise ValueError(\"Unknown bbox_mode: {}\".format(bbox_mode))\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Calculate the center of the instance\n    instance_center = [(instance['bbox'][0] + instance['bbox'][2]) / 2, (instance['bbox'][1] + instance['bbox'][3]) / 2]\n\n    # Calculate the crop size based on the aspect ratio of the image\n    crop_aspect_ratio = crop_size[1] / crop_size[0]\n    image_aspect_ratio = image_size[1] / image_size[0]\n    if crop_aspect_ratio > image_aspect_ratio:\n        crop_width = int(image_size[1] * crop_aspect_ratio)\n        crop_height = image_size[1]\n    else:\n        crop_width = image_size[0]\n        crop_height = int(image_size[0] / crop_aspect_ratio)\n\n    # Calculate the top-left corner of the crop region\n    crop_x = int(instance_center[0] - crop_width / 2)\n    crop_y = int(instance_center[1] - crop_height / 2)\n\n    # Adjust the crop region to ensure it fits within the image boundaries\n    if crop_x < 0:\n        crop_x = 0\n    if crop_y < 0:\n        crop_y = 0\n    if crop_x + crop_width > image_size[0]:\n        crop_x = image_size[0] - crop_width\n    if crop_y + crop_height > image_size[1]:\n        crop_y = image_size[1] - crop_height\n\n    # Create a CropTransform object with the calculated parameters\n    crop_transform = CropTransform(crop_x, crop_y, crop_width, crop_height)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance['bbox']\n    bbox_mode = instance['bbox_mode']\n    crop_h, crop_w = crop_size\n    image_h, image_w = image_size\n\n    # Calculate the center of the instance's bounding box\n    bbox_center_x = bbox[0] + bbox[2] / 2\n    bbox_center_y = bbox[1] + bbox[3] / 2\n\n    # Calculate the top-left corner of the crop based on the center of the instance's bounding box\n    crop_x = max(0, bbox_center_x - crop_w // 2)\n    crop_y = max(0, bbox_center_y - crop_h // 2)\n\n    # Adjust the crop's dimensions to ensure it fits within the image boundaries\n    crop_x = min(crop_x, image_w - crop_w)\n    crop_y = min(crop_y, image_h - crop_h)\n\n    # Create the CropTransform object\n    crop_transform = CropTransform(crop_x, crop_y, crop_w, crop_h, bbox_mode)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Extract the bounding box coordinates from the instance\n    bbox = instance['bbox']\n\n    # Calculate the center of the bounding box\n    bbox_center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n\n    # Calculate the desired crop size\n    crop_size = (crop_size, crop_size)\n\n    # Calculate the top-left corner of the crop region\n    top_left_x = max(0, int(bbox_center[0] - crop_size[0] / 2))\n    top_left_y = max(0, int(bbox_center[1] - crop_size[1] / 2))\n\n    # Calculate the bottom-right corner of the crop region\n    bottom_right_x = min(image_size[1], int(bbox_center[0] + crop_size[0] / 2))\n    bottom_right_y = min(image_size[0], int(bbox_center[1] + crop_size[1] / 2))\n\n    # Create a CropTransform object with the calculated parameters\n    crop_transform = CropTransform(\n        crop_size=crop_size,\n        top_left_x=top_left_x,\n        top_left_y=top_left_y,\n        bottom_right_x=bottom_right_x,\n        bottom_right_y=bottom_right_y\n    )\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    if instance[\"mode\"] == \"box\":\n        bbox = instance[\"bbox\"]\n        crop_x1 = max(bbox[0], 0)\n        crop_y1 = max(bbox[1], 0)\n        crop_x2 = min(bbox[2], image_size[1])\n        crop_y2 = min(bbox[3], image_size[0])\n        crop_w = crop_x2 - crop_x1\n        crop_h = crop_y2 - crop_y1\n\n        if crop_w < crop_size[1] or crop_h < crop_size[0]:\n            crop_x1 = max(bbox[0] - (crop_size[1] - crop_w) // 2, 0)\n            crop_y1 = max(bbox[1] - (crop_size[0] - crop_h) // 2, 0)\n            crop_x2 = min(bbox[2] + (crop_size[1] - crop_w) // 2, image_size[1])\n            crop_y2 = min(bbox[3] + (crop_size[0] - crop_h) // 2, image_size[0])\n\n        return CropTransform(crop_x1, crop_y1, crop_x2, crop_y2)\n\n    elif instance[\"mode\"] == \"polygon\":\n        bbox = instance[\"bbox\"]\n        poly = Polygon(bbox)\n        poly_crop = poly.buffer(crop_size[0] // 2).buffer(-crop_size[1] // 2)\n        x1, y1, x2, y2 = poly_crop.bounds\n        crop_x1 = max(x1, 0)\n        crop_y1 = max(y1, 0)\n        crop_x2 = min(x2, image_size[1])\n        crop_y2 = min(y2, image_size[0])\n        return CropTransform(crop_x1, crop_y1, crop_"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    crop_transform = CropTransform(crop_size, image_size, bbox, bbox_mode)\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]\n\n    # Adjust the crop size to ensure it fits within the image boundaries\n    crop_size = [\n        min(crop_size[0], image_size[0]),\n        min(crop_size[1], image_size[1]),\n    ]\n\n    # Calculate the center of the instance's bounding box\n    center_x = (bbox[0] + bbox[2]) / 2\n    center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the top-left corner of the crop region\n    top_left_x = int(center_x - crop_size[0] / 2)\n    top_left_y = int(center_y - crop_size[1] / 2)\n\n    # Ensure the crop region does not extend beyond the image boundaries\n    top_left_x = max(0, top_left_x)\n    top_left_y = max(0, top_left_y)\n    top_left_x = min(top_left_x, image_size[0] - crop_size[0])\n    top_left_y = min(top_left_y, image_size[1] - crop_size[1])\n\n    # Create the CropTransform object\n    crop_transform = CropTransform(\n        crop_size=crop_size,\n        top_left_x=top_left_x,\n        top_left_y=top_left_y,\n    )\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box coordinates of the instance\n    bbox = instance['bbox']\n\n    # Calculate the center of the bounding box\n    bbox_center_x = (bbox[0] + bbox[2]) / 2\n    bbox_center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the offset from the center of the bounding box to the desired crop size\n    offset_x = (crop_size[1] - bbox[2]) / 2\n    offset_y = (crop_size[0] - bbox[3]) / 2\n\n    # Calculate the top-left corner of the crop\n    x1 = int(max(0, bbox_center_x - offset_x))\n    y1 = int(max(0, bbox_center_y - offset_y))\n\n    # Calculate the bottom-right corner of the crop\n    x2 = int(min(image_size[1], bbox_center_x + offset_x))\n    y2 = int(min(image_size[0], bbox_center_y + offset_y))\n\n    # Create a CropTransform object with the calculated crop region\n    crop_transform = CropTransform(x1, y1, x2, y2)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Convert the crop size to a tuple if it is not already\n    crop_size = (crop_size, crop_size)\n\n    # Get the instance's bounding box coordinates\n    bbox = instance[\"bbox\"]\n\n    # Compute the center of the bounding box\n    bbox_center = (bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2)\n\n    # Compute the top-left corner of the crop region\n    crop_top_left = (\n        max(0, bbox_center[0] - crop_size[1] // 2),\n        max(0, bbox_center[1] - crop_size[0] // 2),\n    )\n\n    # Compute the bottom-right corner of the crop region\n    crop_bottom_right = (\n        min(image_size[1], bbox_center[0] + crop_size[1] // 2),\n        min(image_size[0], bbox_center[1] + crop_size[0] // 2),\n    )\n\n    # Compute the dimensions of the crop region\n    crop_size = (\n        crop_bottom_right[1] - crop_top_left[1],\n        crop_bottom_right[0] - crop_top_left[0],\n    )\n\n    # Create a CropTransform object with the computed parameters\n    crop_transform = CropTransform(crop_top_left, crop_size)\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = instance[\"bbox\"]\n\n    # Calculate the center of the bounding box\n    bbox_center_x = (bbox[0] + bbox[2]) / 2\n    bbox_center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the desired crop size\n    crop_width = crop_size[0]\n    crop_height = crop_size[1]\n\n    # Calculate the offset from the center of the bounding box to the top-left corner of the crop\n    offset_x = (crop_width - bbox_center_x) / 2\n    offset_y = (crop_height - bbox_center_y) / 2\n\n    # Calculate the top-left corner of the crop\n    top_left_x = max(bbox_center_x - offset_x, 0)\n    top_left_y = max(bbox_center_y - offset_y, 0)\n\n    # Calculate the dimensions of the crop\n    crop_width = min(crop_width, image_size[0] - top_left_x)\n    crop_height = min(crop_height, image_size[1] - top_left_y)\n\n    # Create the CropTransform object\n    crop_transform = CropTransform(\n        top_left_x=int(top_left_x),\n        top_left_y=int(top_left_y),\n        crop_width=int(crop_width),\n        crop_height=int(crop_height),\n    )\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Extract the bounding box coordinates from the instance\n    x1, y1, x2, y2 = instance[\"bbox\"]\n    # Calculate the center of the instance\n    center_x = (x1 + x2) / 2\n    center_y = (y1 + y2) / 2\n    # Calculate the size of the crop\n    crop_width = crop_size[0]\n    crop_height = crop_size[1]\n    # Calculate the top-left corner of the crop\n    x_min = max(0, int(center_x - crop_width / 2))\n    y_min = max(0, int(center_y - crop_height / 2))\n    # Calculate the bottom-right corner of the crop\n    x_max = min(image_size[1], int(center_x + crop_width / 2))\n    y_max = min(image_size[0], int(center_y + crop_height / 2))\n    # Create a CropTransform object\n    crop_transform = CropTransform(x_min, y_min, x_max, y_max)\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box coordinates and mode for the instance\n    bbox = instance[\"bbox\"]\n    mode = instance[\"mode\"]\n\n    # Compute the coordinates of the crop region based on the instance's bounding box and the desired crop size\n    x1 = max(0, bbox[0] - crop_size[1] // 2)\n    y1 = max(0, bbox[1] - crop_size[0] // 2)\n    x2 = min(image_size[1], bbox[2] + crop_size[1] // 2)\n    y2 = min(image_size[0], bbox[3] + crop_size[0] // 2)\n\n    # Adjust the crop region to ensure it fits within the image boundaries\n    x1 = max(0, x1)\n    y1 = max(0, y1)\n    x2 = min(image_size[1], x2)\n    y2 = min(image_size[0], y2)\n\n    # Create a CropTransform object with the computed crop region\n    crop_transform = CropTransform(x1, y1, x2 - x1, y2 - y1, mode)\n\n    return crop_transform\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box coordinates and mode of the instance\n    bbox = instance['bbox']\n    mode = instance['mode']\n\n    # Compute the center of the bounding box\n    bbox_center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n\n    # Compute the maximum distance from the center of the bounding box to the edges of the image\n    max_distance = max(image_size[0] - bbox_center[0], bbox_center[0],\n                       image_size[1] - bbox_center[1], bbox_center[1])\n\n    # Compute the top-left corner of the crop region\n    top_left = (int(bbox_center[0] - max_distance), int(bbox_center[1] - max_distance))\n\n    # Compute the dimensions of the crop region\n    crop_size = (int(2 * max_distance), int(2 * max_distance))\n\n    # Create a CropTransform object with the computed parameters\n    return CropTransform(top_left, crop_size, image_size, mode)\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box coordinates of the instance\n    bbox = instance[\"bbox\"]\n\n    # Calculate the center of the bounding box\n    bbox_center_x = (bbox[0] + bbox[2]) / 2\n    bbox_center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the crop size\n    crop_height, crop_width = crop_size\n\n    # Calculate the top-left corner of the crop\n    crop_top = int(bbox_center_y - crop_height / 2)\n    crop_left = int(bbox_center_x - crop_width / 2)\n\n    # Adjust the crop region to ensure it does not go beyond the image boundaries\n    crop_top = max(0, crop_top)\n    crop_left = max(0, crop_left)\n    crop_bottom = min(image_size[0], crop_top + crop_height)\n    crop_right = min(image_size[1], crop_left + crop_width)\n\n    # Create the CropTransform object\n    crop_transform = CropTransform(crop_top, crop_left, crop_bottom, crop_right)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    import detectron2.data.transforms as T\n    from detectron2.data.transforms.augmentation import Augmentation\n    from detectron2.data.transforms.crop_transform import CropTransform\n    import numpy as np\n\n    # Extract the bounding box coordinates from the instance\n    bbox = instance[\"bbox\"]\n\n    # Calculate the center of the bounding box\n    bbox_center = np.array([bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2])\n\n    # Calculate the size of the crop\n    crop_size = np.array(crop_size)\n\n    # Calculate the size of the image\n    image_size = np.array(image_size)\n\n    # Calculate the offset of the crop center from the image center\n    offset = (image_size - crop_size) / 2\n\n    # Calculate the new center of the crop\n    new_center = bbox_center - offset\n\n    # Calculate the new top-left corner of the crop\n    new_top_left = new_center - crop_size / 2\n\n    # Calculate the new bottom-right corner of the crop\n    new_bottom_right = new_center + crop_size / 2\n\n    # Calculate the new top-left corner of the crop\n    new_top_left = np.maximum(new_top_left, 0)\n\n    # Calculate the new bottom-right corner of the crop\n    new_bottom_right = np.minimum(new_bottom_right, image_size)\n\n    # Calculate the new height and width of the crop\n    new_height = new_bottom_right[1] - new_top_left[1]\n    new_width = new_bottom_right[0] - new_top_left[0]\n\n    # Create a CropTransform object with the new cropping region\n    crop = CropTransform(new_top_left[0], new_top_left[1], new_width, new_height)\n\n    # Create an Augment"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the instance's bounding box\n    bbox = instance['bbox']\n\n    # Calculate the center of the instance's bounding box\n    center_x = bbox[0] + (bbox[2] - bbox[0]) / 2\n    center_y = bbox[1] + (bbox[3] - bbox[1]) / 2\n\n    # Calculate the top-left corner of the crop region\n    top_left_x = int(center_x - crop_size[0] / 2)\n    top_left_y = int(center_y - crop_size[1] / 2)\n\n    # Adjust the top-left corner to ensure that it fits within the image boundaries\n    top_left_x = max(0, top_left_x)\n    top_left_y = max(0, top_left_y)\n    top_left_x = min(image_size[1] - crop_size[1], top_left_x)\n    top_left_y = min(image_size[0] - crop_size[0], top_left_y)\n\n    # Create the CropTransform object\n    crop_transform = CropTransform(\n        top_left_x, top_left_y, crop_size[0], crop_size[1])\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    import numpy as np\n    from detectron2.data import transforms as T\n\n    # Get the bounding box coordinates and mode\n    bbox = instance['bbox']\n    mode = instance['mode']\n\n    # Calculate the center of the instance\n    center = np.array([(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2])\n\n    # Calculate the crop size based on the desired crop size and the instance's mode\n    if mode == 'whole':\n        crop_size = (min(crop_size[0], image_size[0]), min(crop_size[1], image_size[1]))\n    elif mode == 'partial':\n        crop_size = (min(crop_size[0], bbox[2] - bbox[0]), min(crop_size[1], bbox[3] - bbox[1]))\n\n    # Calculate the top-left corner of the crop based on the center and the desired crop size\n    top_left = (center[0] - crop_size[0] / 2, center[1] - crop_size[1] / 2)\n\n    # Ensure the crop does not extend beyond the image boundaries\n    top_left = (max(0, top_left[0]), max(0, top_left[1]))\n    top_left = (min(image_size[0] - crop_size[0], top_left[0]), min(image_size[1] - crop_size[1], top_left[1]))\n\n    # Create a CropTransform object with the calculated parameters\n    crop_transform = T.CropTransform(top_left, crop_size)\n\n    return crop_transform\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm_clamped = jnp.maximum(norm, grad_eps)\n  return x / norm_clamped"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm_clamped = jnp.maximum(x_norm, grad_eps)\n  return x / x_norm_clamped"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm_clamped = jnp.maximum(x_norm, grad_eps)\n  return x / x_norm_clamped"}
{"namespace": "ref_utils.l2_normalize", "completion": "  squared_norm = jnp.sum(jnp.square(x), axis=-1, keepdims=True)\n  safe_norm = jnp.sqrt(jnp.maximum(squared_norm, grad_eps))\n  return x / safe_norm"}
{"namespace": "ref_utils.l2_normalize", "completion": "  squared_norm = jnp.sum(jnp.square(x), axis=-1, keepdims=True)\n  safe_norm = jnp.sqrt(jnp.maximum(squared_norm, grad_eps))\n  return x / safe_norm"}
{"namespace": "ref_utils.l2_normalize", "completion": "  return x / jnp.maximum(jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True)), grad_eps)"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.linalg.norm(x, ord=2, axis=-1, keepdims=True)\n  x_norm_clamped = jnp.maximum(x_norm, grad_eps)\n  return x / x_norm_clamped"}
{"namespace": "ref_utils.l2_normalize", "completion": "  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n  clamped_norm = jnp.clip(squared_norm, a_min=grad_eps, a_max=None)\n  return x / jnp.sqrt(clamped_norm)"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm_clamped = jnp.clip(norm, a_min=grad_eps, a_max=None)\n  return x / norm_clamped\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm_clamped = jnp.clip(x_norm, a_min=grad_eps, a_max=None)\n  return x / x_norm_clamped"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.sqrt(jnp.sum(jnp.square(x), axis=-1, keepdims=True))\n  x_norm = jnp.clip(x_norm, a_min=grad_eps, a_max=None)\n  return x / x_norm"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm of x along the last axis\n  squared_norm = jnp.sum(x ** 2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to prevent exploding gradients during the backward pass\n  squared_norm_clamped = jnp.maximum(squared_norm, grad_eps)\n\n  # Calculate the normalization factor\n  norm = jnp.sqrt(squared_norm_clamped)\n\n  # Divide x by the normalization factor to obtain the normalized output\n  x_normalized = x / norm\n\n  return x_normalized"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.sqrt(jnp.sum(jnp.square(x), axis=-1, keepdims=True))\n  norm_clamped = jnp.maximum(norm, grad_eps)\n  return x / norm_clamped\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm_clamped = jnp.clip(norm, a_min=grad_eps, a_max=None)\n  return x / norm_clamped\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm_clamped = jnp.maximum(x_norm, grad_eps)\n  return x / x_norm_clamped\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm along the last axis of x\n  norm = jnp.sum(x ** 2, axis=-1, keepdims=True)\n\n  # Clamp the norm to prevent exploding gradients during the backward pass\n  norm = jnp.clip(norm, a_min=grad_eps, a_max=None)\n\n  # Calculate the normalization factor\n  norm_factor = jnp.sqrt(norm)\n\n  # Clamp the normalization factor to prevent exploding gradients during the backward pass\n  norm_factor = jnp.clip(norm_factor, a_min=grad_eps, a_max=None)\n\n  # Divide x by the normalization factor to obtain the normalized array\n  x_normalized = x / norm_factor\n\n  return x_normalized"}
{"namespace": "ref_utils.l2_normalize", "completion": "  return x / jnp.maximum(jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True)), grad_eps)\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.linalg.norm(x, ord=2, axis=-1, keepdims=True)\n  x_norm_clamped = jnp.maximum(x_norm, grad_eps)\n  return x / x_norm_clamped\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm_clamped = jnp.maximum(norm, grad_eps)\n  return x / norm_clamped\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.sqrt(jnp.sum(jnp.square(x), axis=-1))\n  norm = jnp.maximum(norm, grad_eps)\n  return x / norm\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0]\n        if len(agent_info.split(':')) > 1:\n            input_text = agent_info.split(':')[1]\n        else:\n            input_text = ''\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name, *input_text = agent_info.split(':')\n        return agent_name.strip(), ':'.join(input_text).strip()"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0]\n        agent_input = agent_info.split(':')[1] if len(agent_info.split(':')) > 1 else ''\n\n        return agent_name, agent_input"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0].strip()\n        input_text = agent_info.split(':')[1].strip() if len(agent_info.split(':')) > 1 else ''\n\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = re.search(r'Use Agent\\[(.*?)\\]:(.*?)$', response)\n        if agent_info:\n            agent_name = agent_info.group(1)\n            input_text = agent_info.group(2).strip()\n            return agent_name, input_text\n        else:\n            return None, None"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0]\n        if len(agent_info.split(':')) > 1:\n            agent_input = agent_info.split(':')[1]\n        else:\n            agent_input = ''\n        return agent_name, agent_input\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = re.search(r'Use Agent\\[(.*?)\\]', response)\n        if agent_info:\n            agent_name = agent_info.group(1).split(':')[0].strip()\n            input_text = ''\n            if len(agent_info.group(1).split(':')) > 1:\n                input_text = agent_info.group(1).split(':')[1].strip()\n            return agent_name, input_text\n        else:\n            return '', ''"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name, agent_input = agent_info.split(':')\n        return agent_name.strip(), agent_input.strip()\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Extract the agent information from the response string\n        agent_info = response.split('Use Agent[')[1].split(']')[0]\n\n        # Split the agent information into the agent name and input text\n        agent_name, _, input_text = agent_info.partition(':')\n\n        # Return the agent name and input text as a tuple\n        return agent_name.strip(), input_text.strip()\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Extract the agent information from the response string\n        agent_info = re.search(r'Use Agent\\[(.*?)\\]', response).group(1)\n\n        # Split the agent information into the agent name and input text\n        agent_name, input_text = agent_info.split(':')\n\n        # Remove leading and trailing whitespace from the agent name and input text\n        agent_name = agent_name.strip()\n        input_text = input_text.strip()\n\n        # Return the agent name and input text as a tuple\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Extract the agent information using a regular expression pattern\n        pattern = r\"Use Agent\\[(.*?)\\](?::(.*))?\"\n        match = re.search(pattern, response)\n        if match:\n            agent_name = match.group(1).strip()\n            input_text = match.group(2).strip() if match.group(2) else \"\"\n            return agent_name, input_text\n        else:\n            return None, None"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        if 'Use Agent[' in response:\n            response = response[response.index('Use Agent['):]\n            agent_name = response[response.index('[')+1:response.index(']')]\n            if ':' in response:\n                input_text = response[response.index(':')+1:]\n            else:\n                input_text = ''\n            return agent_name, input_text\n        else:\n            return None, ''"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name, *input_text = agent_info.split(':')\n        input_text = ':'.join(input_text) if input_text else ''\n        return agent_name.strip(), input_text.strip()\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Split the response string into lines\n        lines = response.split('\\n')\n\n        # Find the line that starts with 'Use Agent['\n        agent_line = None\n        for line in lines:\n            if line.startswith('Use Agent['):\n                agent_line = line\n                break\n\n        # If no agent line was found, return None\n        if agent_line is None:\n            return None\n\n        # Split the agent line into the agent name and input text\n        parts = agent_line.split(']')\n        agent_name = parts[0].split('[')[1]\n        input_text = parts[1].strip() if len(parts) > 1 else ''\n\n        # Return the agent name and input text as a tuple\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = re.search(r'Use Agent\\[(.*?)\\](.*?)$', response)\n        if agent_info:\n            agent_name = agent_info.group(1)\n            input_text = agent_info.group(2).strip()\n            return agent_name, input_text\n        else:\n            return None, None\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info_start = response.find('Use Agent[')\n        agent_info_end = response.find(']', agent_info_start)\n        if agent_info_start != -1 and agent_info_end != -1:\n            agent_info = response[agent_info_start + 10:agent_info_end]\n            if ':' in agent_info:\n                agent_name, input_text = agent_info.split(':')\n            else:\n                agent_name = agent_info\n                input_text = ''\n            return agent_name.strip(), input_text.strip()\n        else:\n            return '', ''\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        try:\n            agent_info = re.search(r'Use Agent\\[(.*?)\\](.*?)$', response).groups()\n            return agent_info[0], agent_info[1] if agent_info[1] else ''\n        except AttributeError:\n            return '', ''\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0]\n        agent_input = agent_info.split(':')[1] if len(agent_info.split(':')) > 1 else ''\n        return agent_name, agent_input\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0]\n        if len(agent_info.split(':')) > 1:\n            input_text = agent_info.split(':')[1]\n        else:\n            input_text = ''\n        return agent_name, input_text\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = \"\"\n        agent_input = \"\"\n\n        if \"Use Agent[\" in response:\n            start_index = response.find(\"Use Agent[\") + len(\"Use Agent[\")\n            end_index = response.find(\"]\", start_index)\n            agent_info = response[start_index:end_index]\n\n            # Split the agent information by the colon to separate the name and input text\n            agent_info_parts = agent_info.split(\":\")\n            agent_name = agent_info_parts[0].strip()\n\n            if len(agent_info_parts) > 1:\n                agent_input = agent_info_parts[1].strip()\n\n        return agent_name, agent_input\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    from detectron2.structures import Boxes, BoxMode, Instances\n\n    boxes = [BoxMode.convert(anno[\"bbox\"], ann[\"bbox_mode\"], BoxMode.XYXY_ABS) for ann in annos]\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(boxes)\n\n    classes = [ann[\"category_id\"] for ann in annos]\n    classes = [int(c) for c in classes]\n    target.gt_classes = torch.tensor(classes)\n\n    if \"segmentation\" in annos[0]:\n        segms = [ann[\"segmentation\"] for ann in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        elif mask_format == \"bitmask\":\n            masks = BitMasks(segms)\n        else:\n            raise ValueError(\n                \"Unknown segmentation format: {}\".format(mask_format)\n            )\n        target.gt_masks = masks\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [ann[\"keypoints\"] for ann in annos]\n        keypoints = [torch.as_tensor(k).reshape(-1, 3) for k in keypoints]\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    from detectron2.structures import Boxes, BoxMode, Instances\n\n    boxes = [BoxMode.convert(anno[\"bbox\"], annobox_format, BoxMode.XYXY_ABS) for anno in annos]\n    target = Boxes(boxes)\n    target = target.clip(image_size)\n\n    classes = [anno[\"category_id\"] for anno in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n\n    if \"segmentation\" in annos[0]:\n        segms = [anno[\"segmentation\"] for anno in annos]\n        masks = SegmentationMask(segms, image_size, mode=mask_format)\n    else:\n        masks = None\n\n    keypoints = None\n    if \"keypoints\" in annos[0]:\n        keypoints = [anno[\"keypoints\"] for anno in annos]\n        keypoints = Keypoints(keypoints, image_size)\n\n    if \"iscrowd\" in annos[0]:\n        iscrowd = [anno[\"iscrowd\"] for anno in annos]\n        iscrowd = torch.tensor(iscrowd, dtype=torch.int64)\n    else:\n        iscrowd = None\n\n    instance = Instances(image_size, **{\"gt_boxes\": target, \"gt_classes\": classes, \"gt_masks\": masks, \"gt_keypoints\": keypoints, \"gt_iscrowd\": iscrowd})\n    return instance"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    if len(annos) == 0:\n        return None\n    boxes = [obj[\"bbox\"] for obj in annos]\n    boxes = torch.as_tensor(boxes).reshape(-1, 4)  # guard against no boxes\n    target = {}\n    target[\"boxes\"] = boxes\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes)\n    one_hot_vector = torch.zeros(\n        (len(classes), max(classes) + 1), device=classes.device\n    )\n    one_hot_vector.scatter_(1, classes.long().view(-1, 1), 1)\n    target[\"labels\"] = one_hot_vector\n\n    if \"segmentation\" in annos[0]:\n        masks = [obj[\"segmentation\"] for obj in annos]\n        target[\"masks\"] = masks\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = torch.as_tensor(keypoints).reshape(-1, 3)\n        target[\"keypoints\"] = keypoints\n\n    if \"area\" in annos[0]:\n        area = torch.tensor([obj[\"area\"] for obj in annos])\n        target[\"area\"] = area\n\n    if \"iscrowd\" in annos[0]:\n        iscrowd = torch.tensor([obj[\"iscrowd\"] for obj in annos])\n        target[\"iscrowd\"] = iscrowd\n\n    image_size = torch.as_tensor(image_size)\n    target[\"size\"] = image_size\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    from detectron2.structures import Boxes, BoxMode, Instances\n\n    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(boxes)\n    target.gt_classes = [obj[\"category_id\"] for obj in annos]\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            segmentations = [obj[\"segmentation\"] for obj in annos]\n        elif mask_format == \"bitmask\":\n            raise NotImplementedError\n        else:\n            raise ValueError(\n                \"Unknown mask format: {} for annotations in {}\".format(mask_format, annos[0][\"file_name\"])\n            )\n        target.gt_masks = segmentations\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        target.gt_keypoints = keypoints\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    if annos:\n        boxes = [box for box in annos[\"bbox\"]]\n        classes = [annos[\"category_id\"]]\n        if \"segmentation\" in annos:\n            if mask_format == \"polygon\":\n                masks = [annos[\"segmentation\"]]\n            elif mask_format == \"bitmask\":\n                masks = annos[\"segmentation\"]\n        else:\n            masks = None\n        if \"keypoints\" in annos:\n            keypoints = [annos[\"keypoints\"]]\n        else:\n            keypoints = None\n    else:\n        boxes = []\n        classes = []\n        masks = None\n        keypoints = None\n    return Instances(image_size, boxes=boxes, classes=classes, masks=masks, keypoints=keypoints)"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    if len(annos) == 0:\n        return None\n\n    boxes = [Boxes(anno[\"bbox\"]) for anno in annos]\n    boxes = [b.scale(1.0 / image_size[0], 1.0 / image_size[1]) for b in boxes]\n    boxes = cat(boxes, dim=0)\n\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            segms = [anno[\"segmentation\"] for anno in annos]\n            masks = PolygonMasks(segms)\n        elif mask_format == \"bitmask\":\n            masks = BitMasks(\n                [anno[\"segmentation\"] for anno in annos],\n                height=image_size[0],\n                width=image_size[1],\n            )\n        else:\n            raise ValueError(f\"Unknown mask format: {mask_format}\")\n    else:\n        masks = None\n\n    keypoints = None\n    if \"keypoints\" in annos[0]:\n        keypoints = [anno[\"keypoints\"] for anno in annos]\n        keypoints = Keypoints(keypoints)\n\n    gt_classes = [anno[\"category_id\"] for anno in annos]\n    gt_classes = cat(gt_classes)\n\n    if \"iscrowd\" in annos[0]:\n        iscrowd = [anno[\"iscrowd\"] for anno in annos]\n        iscrowd = cat(iscrowd)\n    else:\n        iscrowd = None\n\n    if \"area\" in annos[0]:\n        area = [anno[\"area\"] for anno in annos]\n        area = cat(area)\n    else:\n        area = None\n\n    if \"bbox_mode\" in annos[0]:\n        bbox_mode = annos[0][\"bbox_mode\"]\n    else:\n        bbox_mode = BoxMode.XYXY_ABS"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    from detectron2.structures import Boxes, BoxMode, Instances\n\n    # Convert annotations to a list of dictionaries\n    annos = [anno for anno in annos]\n\n    # Extract the fields of interest from the annotations\n    boxes = [BoxMode.convert(anno[\"bbox\"], annos[0][\"bbox_mode\"], BoxMode.XYXY_ABS) for anno in annos]\n    classes = [anno[\"category_id\"] for anno in annos]\n    segmentation = [anno[\"segmentation\"] for anno in annos]\n    keypoints = [anno[\"keypoints\"] for anno in annos]\n\n    # Create an Instances object\n    instances = Instances(image_size)\n\n    # Set the fields of the Instances object\n    instances.gt_boxes = Boxes(boxes)\n    instances.gt_classes = classes\n    instances.gt_masks = segmentation\n    instances.gt_keypoints = keypoints\n\n    return instances\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    from detectron2.structures import Boxes, BoxMode, Instances\n\n    # Convert the annotations into a list of dictionaries\n    annos = [anno for anno in annos]\n\n    # Extract the bounding boxes, classes, segmentation, and keypoints from the annotations\n    boxes = [anno[\"bbox\"] for anno in annos]\n    classes = [anno[\"category_id\"] for anno in annos]\n    segmentation = [anno[\"segmentation\"] for anno in annos]\n    keypoints = [anno[\"keypoints\"] for anno in annos]\n\n    # Convert the bounding boxes to a Boxes object\n    boxes = Boxes(boxes)\n\n    # Convert the segmentation masks to a tensor\n    if mask_format == \"polygon\":\n        masks = PolygonMasks(segmentation)\n    elif mask_format == \"bitmask\":\n        masks = BitMasks(segmentation)\n    else:\n        raise ValueError(\n            \"Invalid mask format. Must be either 'polygon' or 'bitmask'.\"\n        )\n\n    # Create an Instances object and populate it with the extracted information\n    instances = Instances(image_size)\n    instances.gt_boxes = boxes\n    instances.gt_classes = classes\n    instances.gt_masks = masks\n    instances.gt_keypoints = keypoints\n\n    return instances"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    if not annos:\n        return None\n    boxes = [anno[\"bbox\"] for anno in annos]\n    boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [anno[\"category_id\"] for anno in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n\n    segmentation = [anno[\"segmentation\"] for anno in annos]\n    if mask_format == \"polygon\":\n        masks = PolygonMasks(segmentation)\n    elif mask_format == \"bitmask\":\n        masks = BitMasks(segmentation)\n    else:\n        raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    keypoints = [anno[\"keypoints\"] for anno in annos]\n    keypoints = Keypoints(keypoints)\n\n    return Instances(image_size, boxes=boxes, classes=classes, masks=masks, keypoints=keypoints)\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    \"\"\"\n    This function converts instance annotations from a dataset dictionary into an Instances object that is compatible with the models. It processes annotations related to bounding boxes, classes, segmentation masks, and keypoints, and organizes them into a structured format that the models can directly use.\n\n    Input-Output Arguments\n    :param annos: list[dict]. A list of dictionaries where each dictionary contains information about one instance in an image, including bounding boxes, classes, segmentation, and keypoints.\n    :param image_size: tuple. The size of the image as a tuple (height, width), used to scale and format the annotations correctly.\n    :param mask_format: str, optional. The format of the segmentation masks in the annotations. It defaults to \"polygon\" and can also be \"bitmask\". This parameter determines how segmentation annotations are processed and converted.\n    :return: Instances. An object containing structured fields like \"gt_boxes\", \"gt_classes\", \"gt_masks\", and \"gt_keypoints\", which are derived from the input annotations. This object is ready to be used by the models.\n\n    \"\"\"\n    if len(annos) == 0:\n        return None\n\n    boxes = [box for box in annos[\"bbox\"]]\n    if len(boxes) == 0:\n        return None\n\n    # Add gt_classes.\n    classes = [int(c) for c in annos[\"category_id\"]]\n\n    # Add gt_masks.\n    if \"segmentation\" in annos:\n        masks = annos[\"segmentation\"]\n        if mask_format == \"polygon\":\n            masks = [\n                [p for p in m if len(p) == 2] for m in masks\n            ]  # remove invalid polygons\n            masks = PolygonMasks(masks, image_size)\n        elif mask_format == \"bitmask\":\n            masks = [\n                BitMasks.from_polygon_mask(\n                    PolygonMasks([m], image_size), image_size\n                )\n                for m in masks\n            ]"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    if len(annos) == 0:\n        return None\n\n    boxes = [anno[\"bbox\"] for anno in annos]\n    if len(boxes) == 0:\n        return None\n    boxes = np.array(boxes, dtype=np.float32)\n    boxes[:, 2] += boxes[:, 0]\n    boxes[:, 3] += boxes[:, 1]\n\n    # If the annotations are in COCO format, convert them to the required format.\n    if annos[0].get(\"segmentation\", None) is not None:\n        segms = [anno[\"segmentation\"] for anno in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms, image_size)\n        elif mask_format == \"bitmask\":\n            masks = BitMasks(segms, image_size)\n        else:\n            raise ValueError(\n                \"Only 'polygon' and 'bitmask' segmentation masks are supported.\"\n            )\n    else:\n        masks = None\n\n    # Get the keypoints and convert them to the required format.\n    keypoints = None\n    if annos[0].get(\"keypoints\", None) is not None:\n        keypoints = [anno[\"keypoints\"] for anno in annos]\n        keypoints = Keypoints(keypoints, image_size)\n\n    # Get the classes and convert them to the required format.\n    classes = [anno[\"category_id\"] for anno in annos]\n    classes = np.array(classes, dtype=np.int64)\n\n    # Create an Instances object and populate it with the structured fields.\n    instances = Instances(image_size)\n    instances.gt_boxes = Boxes(boxes)\n    instances.gt_classes = classes\n    instances.gt_masks = masks\n    instances.gt_keypoints = keypoints\n\n    return instances"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    from detectron2.structures import Boxes, BoxMode, Instances\n\n    boxes = [BoxMode.convert(anno[\"bbox\"], annobox_format, BoxMode.XYXY_ABS) for anno in annos]\n    boxes = [box if box[0] < box[2] and box[1] < box[3] else box[::-1] for box in boxes]\n    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n    boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [anno[\"category_id\"] for anno in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n\n    if \"segmentation\" in annos[0]:\n        segms = [anno[\"segmentation\"] for anno in annos]\n        masks = PolygonMasks(segms, image_size)\n    elif \"keypoints\" in annos[0]:\n        keypoints = [anno[\"keypoints\"] for anno in annos]\n        keypoints = torch.as_tensor(keypoints, dtype=torch.float32).view(len(annos), -1, 3)\n        keypoints = PersonKeypoints(keypoints)\n    else:\n        masks = None\n        keypoints = None\n\n    return Instances(image_size, **{\"gt_boxes\": boxes, \"gt_classes\": classes, \"gt_masks\": masks, \"gt_keypoints\": keypoints})\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # The following keys are used in the dicts\n    # \"id\": int. The ID of the instance.\n    # \"image_id\": int. The ID of the image.\n    # \"category_id\": int. The ID of the category.\n    # \"segmentation\": list[list[float]]. The segmentation mask of the instance.\n    # \"area\": float. The area of the instance.\n    # \"bbox\": list[float]. The bounding box of the instance.\n    # \"iscrowd\": int. A flag indicating if the instance is a crowd instance (1) or not (0).\n    # \"keypoints\": list[float]. The keypoints of the instance.\n    # \"num_keypoints\": int. The number of keypoints of the instance.\n\n    if len(annos) == 0:\n        return None\n\n    boxes = [Boxes(anno[\"bbox\"]) for anno in annos]\n    boxes = cat(boxes, dim=0)\n    boxes.clip(image_size)\n\n    classes = [anno[\"category_id\"] for anno in annos]\n    classes = cat(classes, dim=0)\n\n    if mask_format == \"polygon\":\n        segmentations = [\n            [\n                np.array(\n                    [\n                        [p[0], p[1]]\n                        for p in anno[\"segmentation\"][0]\n                    ],\n                    dtype=np.float32,\n                )\n            ]\n            for anno in annos\n        ]\n    elif mask_format == \"bitmask\":\n        masks = [\n            PolygonMasks(\n                [\n                    np.array(\n                        [\n                            [p[0], p[1]]\n                            for p in anno[\"segmentation\"][0]\n                        ],\n                        dtype=np.float32,\n                    )\n                ]\n            )\n            for anno in annos\n        ]\n        segmentations = [m.get_mask_tensor() for m in masks]\n    else:\n        raise ValueError(\n            \"Only polygon and"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # The following keys are used in the dicts\n    # \"id\", \"category_id\", \"bbox\", \"area\", \"iscrowd\", \"keypoints\", \"num_keypoints\"\n    if not annos:\n        return None\n    boxes = [box[:4] for box in annos]\n    # guard against no boxes via resizing\n    if len(boxes) == 0:\n        boxes = np.zeros((0, 4))\n    boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [int(anno[\"category_id\"]) for anno in annos]\n    classes = np.array(classes, dtype=np.int64)\n\n    segmentation = None\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            polygons = [anno[\"segmentation\"] for anno in annos]\n            # filter out invalid polygons (< 3 points)\n            valid_polygons = []\n            for poly in polygons:\n                poly = np.array(poly).reshape(-1, 2)\n                if len(poly) >= 3:\n                    valid_polygons.append(poly)\n            if len(valid_polygons) == 0:\n                segmentation = None\n            else:\n                segmentation = PolygonMasks(valid_polygons)\n        elif mask_format == \"bitmask\":\n            masks = [anno[\"segmentation\"] for anno in annos]\n            masks = [np.array(mask).reshape(-1) for mask in masks]\n            masks = BitMasks(masks)\n        else:\n            raise ValueError(\n                \"mask_format must be one of 'polygon' or 'bitmask', got {}\".format(\n                    mask_format\n                )\n            )\n\n    keypoints = None\n    if \"keypoints\" in annos[0]:\n        keypoints = [anno[\"keypoints\"] for anno in annos]\n        keypoints = np.array"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [Boxes(anno[\"bbox\"]) for anno in annos]\n    target = Instances(image_size, **{\"gt_boxes\": boxes})\n\n    if len(annos) == 0:\n        return target\n\n    classes = [anno[\"category_id\"] for anno in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if \"segmentation\" in annos[0]:\n        masks = [\n            polygon_to_bitmask(\n                [anno[\"segmentation\"]],\n                image_size[0],\n                image_size[1],\n            )\n            for anno in annos\n        ]\n        masks = torch.stack(masks, dim=0)\n        target.gt_masks = masks\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [anno[\"keypoints\"] for anno in annos]\n        keypoints = torch.tensor(keypoints, dtype=torch.float32)\n        keypoints = keypoints.reshape(-1, len(keypoints), 3)\n        target.gt_keypoints = keypoints\n\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # Get the height and width of the image\n    height, width = image_size\n\n    # Create an empty list to store the instances\n    instances = []\n\n    # Iterate over the annotations\n    for anno in annos:\n        # Get the bounding box coordinates and class label\n        bbox = anno[\"bbox\"]\n        x, y, x2, y2 = bbox\n        x, y, x2, y2 = x / width, y / height, x2 / width, y2 / height\n        class_label = anno[\"category_id\"]\n\n        # Get the segmentation mask if it exists\n        if \"segmentation\" in anno:\n            segm = anno[\"segmentation\"]\n            if mask_format == \"polygon\":\n                segm = [\n                    [p for p in poly if p % width != 0 and p // width < height]\n                    for poly in segm\n                ]\n            elif mask_format == \"bitmask\":\n                raise NotImplementedError\n            else:\n                raise ValueError(\n                    \"Unknown mask format: {}\".format(mask_format)\n                )\n        else:\n            segm = None\n\n        # Get the keypoints if they exist\n        if \"keypoints\" in anno:\n            keypoints = anno[\"keypoints\"]\n            keypoints = [\n                [p for p in keypoint if p % width != 0 and p // width < height]\n                for keypoint in keypoints\n            ]\n        else:\n            keypoints = None\n\n        # Create an instance dictionary\n        instance = {\n            \"bbox\": [x, y, x2, y2],\n            \"bbox_mode\": BoxMode.XYXY_ABS,\n            \"segmentation\": segm,\n            \"keypoints\": keypoints,\n            \"category_id\": class_label,\n        }\n\n        # Append the instance to the list of instances\n        instances.append(instance)\n\n    # Return the list of instances\n    return instances"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # The following keys are common to all annotations that can be found in an annotation dict\n    if \"segmentation\" not in annos[0]:\n        annos = [obj for obj in annos if \"iscrowd\" not in obj or obj[\"iscrowd\"] == 0]\n\n    boxes = [obj[\"bbox\"] for obj in annos]\n    # guard against no boxes via resizing\n    if len(boxes) == 0:\n        boxes = np.zeros((0, 4))\n    boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n\n    if mask_format == \"polygon\":\n        if \"segmentation\" in annos[0]:\n            segms = [obj[\"segmentation\"] for obj in annos]\n            masks = PolygonMasks(segms)\n        else:\n            masks = None\n    elif mask_format == \"bitmask\":\n        masks = None\n        if \"segmentation\" in annos[0:]:\n            segms = [obj[\"segmentation\"] for obj in annos]\n            masks = BitMasks(segms)\n    else:\n        raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    keypoints = None\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = Keypoints(keypoints)\n\n    return Instances(\n        image_size,\n        boxes=boxes,\n        classes=classes,\n        masks=masks,\n        keypoints=keypoints,\n    )\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # Convert the annotations into a list of Instances\n    instances = []\n    for anno in annos:\n        # Extract the bounding box coordinates and class label\n        bbox = anno[\"bbox\"]\n        x, y, w, h = bbox\n        x1, y1, x2, y2 = x, y, x + w, y + h\n        class_label = anno[\"category_id\"]\n\n        # Convert the segmentation mask to a BitMasks object\n        if mask_format == \"polygon\":\n            segm = anno[\"segmentation\"]\n            if len(segm) == 0:\n                # If there is no segmentation mask, create an empty BitMasks object\n                masks = BitMasks([])\n            else:\n                # Convert the segmentation mask to a BitMasks object\n                masks = PolygonMasks(segm, image_size)\n        elif mask_format == \"bitmask\":\n            # Convert the segmentation mask to a BitMasks object\n            masks = BitMasks(anno[\"segmentation\"])\n        else:\n            raise ValueError(f\"Unknown mask format: {mask_format}\")\n\n        # Extract the keypoints and visibility\n        keypoints = anno.get(\"keypoints\", [])\n        keypoints = np.array(keypoints).reshape(-1, 3)\n        keypoints = Keypoints(keypoints)\n        keypoints.set_visibility(anno.get(\"keypoints_visible\", []))\n\n        # Create an Instance object and add it to the list of instances\n        instance = Instance(\n            bbox=Boxes(BoxMode.XYXY_ABS, [[x1, y1, x2, y2]]),\n            gt_classes=[class_label],\n            gt_masks=masks,\n            gt_keypoints=keypoints,\n        )\n        instances.append(instance)\n\n    # Convert the list of instances into a single Instances object\n    instances = Instances(image"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    from detectron2.structures import Boxes, BoxMode, Instances, PolygonMasks, BitMasks\n    from detectron2.utils.visualizer import _create_text_labels\n\n    # Create a list of dictionaries with the following keys:\n    # \"id\": the instance ID\n    # \"bbox\": the bounding box of the instance in the format (x1, y1, x2, y2)\n    # \"bbox_mode\": the bounding box mode, which is always set to \"XYXY_ABS\"\n    # \"segmentation\": the segmentation mask of the instance in the format of a list of polygons\n    # \"category_id\": the class ID of the instance\n    # \"iscrowd\": a flag indicating if the instance is a crowd instance (not used in this function)\n    # \"keypoints\": the keypoints of the instance in the format of a list of (x, y) coordinates\n    # \"num_keypoints\": the number of keypoints in the instance (not used in this function)\n    # \"score\": the score of the instance (not used in this function)\n    instances_annos = []\n    for anno in annos:\n        instances_annos.append(\n            {\n                \"id\": anno[\"id\"],\n                \"bbox\": anno[\"bbox\"],\n                \"bbox_mode\": BoxMode.XYXY_ABS,\n                \"segmentation\": anno[\"segmentation\"],\n                \"category_id\": anno[\"category_id\"],\n                \"iscrowd\": 0,\n                \"keypoints\": anno.get(\"keypoints\", []),\n                \"num_keypoints\": anno.get(\"num_keypoints\", 0),\n                \"score\": 1.0,\n            }\n        )\n\n    # Create an Instances object from the list of dictionaries\n    instances = Instances.from_dict(instances_annos)\n\n    # Convert the bounding box coordinates from absolute to relative (0 to 1)\n    instances.gt_boxes = instances.gt_boxes."}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # The following keys are used in the model\n    keys = [\"gt_boxes\", \"gt_classes\", \"gt_masks\", \"gt_keypoints\"]\n    # The number of instances in this image\n    num_instances = len(annos)\n    # The number of classes in the dataset\n    num_classes = len(annos[0][\"classes\"])\n    # The number of keypoints in the dataset\n    num_keypoints = len(annos[0][\"keypoints\"])\n\n    # The fields that will be used to create the instances\n    # See https://detectron2.readthedocs.io/en/latest/modules/structures.html#instances for all possible fields\n    # We use only the following fields:\n    # - gt_boxes: bounding boxes in XYXY_ABS format\n    # - gt_classes: class IDs\n    # - gt_masks: segmentation masks in binary format\n    # - gt_keypoints: keypoint annotations in XY format\n    # - gt_keypoint_weights: weights for each keypoint (1.0 for visible, 0.0 for occluded)\n    fields = keys + [\"gt_keypoint_weights\"]\n\n    # Create a list of empty arrays for each field\n    values = [np.empty((num_instances, 0), dtype=dtype) for dtype in fields]\n\n    # Extract information from the annotations\n    for anno in annos:\n        # Get the bounding boxes in XYXY_ABS format\n        bbox = anno[\"bbox\"]\n        bbox = np.array([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n        # Get the class IDs\n        class_ids = anno[\"classes\"]\n        # Get the segmentation masks in binary format\n        masks = anno[\"segmentation\"]\n        # Get the keypoint annotations in XY format\n        keypoints = anno"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path(\"~/skfolio_data\").expanduser())\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path(\"~/skfolio_data\").expanduser())\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path(\"~/skfolio_data\").expanduser())\n    data_home = Path(data_home).expanduser()\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    from pathlib import Path\n    import os\n\n    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n\n    data_home = Path(data_home).expanduser().absolute()\n    data_home.mkdir(parents=True, exist_ok=True)\n\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home.resolve()"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", \"~/skfolio_data\")\n    data_home = Path(data_home).expanduser()\n    if not data_home.exists():\n        data_home.mkdir(parents=True, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get('SKFOLIO_DATA', Path.home() / 'skfolio_data')\n    data_home = Path(data_home).expanduser().absolute()\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get('SKFOLIO_DATA', Path.home() / 'skfolio_data')\n\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home).expanduser().absolute()\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    # Check if data_home is provided as a string or a Path object\n    if isinstance(data_home, str):\n        data_home = Path(data_home)\n\n    # Check if data_home is provided as a Path object\n    if isinstance(data_home, Path):\n        # Convert data_home to a string\n        data_home = str(data_home)\n\n    # Check if data_home is provided as a string\n    if isinstance(data_home, str):\n        # Check if data_home is an absolute path\n        if not os.path.isabs(data_home):\n            # Convert data_home to an absolute path\n            data_home = os.path.abspath(data_home)\n\n    # Check if data_home is provided as a string\n    if isinstance(data_home, str):\n        # Check if data_home is a valid path\n        if not os.path.exists(data_home):\n            # Create the data_home directory if it does not exist\n            os.makedirs(data_home)\n\n    # Return the data_home path\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    if isinstance(data_home, str):\n        data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    # If data_home is None, check for an environment variable\n    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", None)\n\n    # If data_home is still None, set it to the default path\n    if data_home is None:\n        data_home = Path.home() / \"skfolio_data\"\n\n    # If data_home is a string, convert it to a Path object\n    if isinstance(data_home, str):\n        data_home = Path(data_home)\n\n    # Ensure the data directory exists\n    data_home.mkdir(parents=True, exist_ok=True)\n\n    return str(data_home)\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    cov = np.atleast_2d(cov)\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    cov = np.atleast_2d(cov)\n\n    # Calculate the standard deviation for each variable\n    std_dev = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std_dev, std_dev)\n\n    return corr, std_dev\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Check if the input is a 2D array\n    if not isinstance(cov, np.ndarray) or len(cov.shape) != 2:\n        raise ValueError(\"Input must be a 2D numpy array\")\n\n    # Calculate the standard deviations for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure that the input is a 2D array\n    if len(cov.shape) != 2 or cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"Input must be a 2D square array\")\n\n    # Calculate the standard deviations for each variable\n    stds = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(stds, stds)\n\n    return corr, stds"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    cov = np.atleast_2d(cov)\n\n    # Calculate the standard deviation for each variable\n    sd = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(sd, sd)\n\n    return corr, sd\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    cov = np.array(cov)\n    if cov.ndim != 2:\n        raise ValueError(\"Input must be a 2D array.\")\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    cov = np.array(cov)\n    assert cov.ndim == 2, \"cov must be a 2D array\"\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Check if the input is a 2D array\n    if len(cov.shape) != 2 or cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"Input must be a 2D square array\")\n\n    # Calculate the standard deviation for each variable\n    std_dev = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std_dev, std_dev)\n\n    return corr, std_dev\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    if not isinstance(cov, np.ndarray) or len(cov.shape) != 2:\n        raise ValueError(\"Input must be a 2D numpy array\")\n\n    # Calculate the standard deviations for each variable\n    std_devs = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std_devs, std_devs)\n\n    return corr, std_devs\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    if len(cov.shape) != 2:\n        raise ValueError(\"Input must be a 2D array\")\n\n    # Calculate the standard deviations for each variable\n    std_devs = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std_devs, std_devs)\n\n    return corr, std_devs\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert cov.ndim == 2, \"Covariance matrix must be 2D\"\n    assert cov.shape[0] == cov.shape[1], \"Covariance matrix must be square\"\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure input is a 2D array\n    cov = np.atleast_2d(cov)\n\n    # Calculate the standard deviation for each variable\n    sigma = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(sigma, sigma)\n\n    # Return the correlation matrix and standard deviations\n    return corr, sigma\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure input is a 2D array\n    if not isinstance(cov, np.ndarray) or cov.ndim != 2:\n        raise ValueError(\"Input must be a 2D numpy array\")\n\n    # Calculate standard deviations\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure input is a 2D array\n    cov = np.atleast_2d(cov)\n\n    # Calculate the standard deviation for each variable\n    sd = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(sd, sd)\n\n    return corr, sd\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure input is 2D array\n    cov = np.atleast_2d(cov)\n\n    # Calculate the standard deviation for each variable\n    std_devs = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr_matrix = np.zeros_like(cov)\n    for i in range(cov.shape[0]):\n        for j in range(i, cov.shape[1]):\n            corr_matrix[i, j] = cov[i, j] / (std_devs[i] * std_devs[j])\n            corr_matrix[j, i] = corr_matrix[i, j]\n\n    return corr_matrix, std_devs\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    cov = np.atleast_2d(cov)\n\n    # Calculate the standard deviation for each variable\n    sigma = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(sigma, sigma)\n\n    return corr, sigma\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    cov = np.array(cov)\n    assert cov.ndim == 2, \"Input must be a 2D array\"\n    std = np.sqrt(np.diag(cov))\n    corr = np.outer(std, std)\n    corr = cov / corr\n    np.fill_diagonal(corr, 1)\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    cov = np.atleast_2d(cov)\n\n    # Calculate the standard deviations for each variable\n    sd = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(sd, sd)\n\n    return corr, sd\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure input is a 2D array\n    if not isinstance(cov, np.ndarray) or cov.ndim != 2:\n        raise ValueError(\"Input must be a 2D numpy array\")\n\n    # Calculate standard deviations\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    cov = np.atleast_2d(cov)\n\n    # Calculate the standard deviations for each variable\n    s = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(s, s)\n\n    return corr, s\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    for submodule in model.modules():\n        if hasattr(submodule, \"training\"):\n            submodule.training = False\n\n    return model"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    for submodule in model.modules():\n        if hasattr(submodule, 'training'):\n            submodule.training = True\n\n    yield\n\n    for submodule in model.modules():\n        if hasattr(submodule, 'training'):\n            submodule.training = False"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def freeze_module(module):\n        for name, child in module.named_children():\n            if hasattr(child, \"training\"):\n                child.training = False\n            freeze_module(child)\n\n    freeze_module(model)"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _freeze_training_mode(module):\n        if hasattr(module, \"training\"):\n            module.training = False\n\n    model.apply(_freeze_training_mode)"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    for submodule in model.modules():\n        submodule.__class__.__setattr__('training', True)\n\n    @contextlib.contextmanager\n    def freeze_training_mode(model):\n        for submodule in model.modules():\n            submodule.__class__.__setattr__('training', True)\n        yield\n        for submodule in model.modules():\n            submodule.__class__.__setattr__('training', False)\n\n    return freeze_training_mode"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    for module in model.modules():\n        module.__class__.__setattr__('training', True)\n\n    @contextmanager\n    def freeze_training_mode():\n        for module in model.modules():\n            module.__class__.__setattr__('training', True)\n        yield\n        for module in model.modules():\n            module.__class__.__setattr__('training', False)\n\n    return freeze_training_mode\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def freeze_training_mode_recursive(model, frozen_training_mode):\n        for submodule in model.children():\n            if hasattr(submodule, 'training'):\n                submodule.training = frozen_training_mode\n            freeze_training_mode_recursive(submodule, frozen_training_mode)\n\n    frozen_training_mode = False\n    freeze_training_mode_recursive(model, frozen_training_mode)\n    return model"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules of the model\n    for module in model.modules():\n        # Check if the module has a \"training\" attribute\n        if hasattr(module, \"training\"):\n            # Create a context manager to temporarily freeze the \"training\" attribute\n            with torch.no_grad():\n                # Set the \"training\" attribute to a constant value\n                module.training = True\n\n    # Return None\n    return None"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    import torch\n    import torch.nn as nn\n\n    def freeze_training_mode_helper(module):\n        for submodule in module.children():\n            if isinstance(submodule, nn.Module):\n                freeze_training_mode_helper(submodule)\n                submodule.training = False\n\n    freeze_training_mode_helper(model)\n\n    return"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    import torch\n\n    for submodule in model.modules():\n        if hasattr(submodule, \"training\"):\n            submodule.training = torch.jit.Attribute(submodule.training, torch.jit.AttributeKind.CONST)\n\n    return model\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules of the model\n    for module in model.modules():\n\n        # If the module has a \"training\" attribute\n        if hasattr(module, \"training\"):\n\n            # Create a context manager that temporarily sets the \"training\" attribute to a constant value\n            with torch.no_grad():\n\n                # Set the \"training\" attribute to a constant value\n                module.training = False\n\n                # Check if the \"training\" attribute is a constant\n                assert not module.training\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _freeze_training_mode(module):\n        for name, child in module.named_children():\n            if hasattr(child, 'training'):\n                child.training = False\n            _freeze_training_mode(child)\n\n    with torch.no_grad():\n        _freeze_training_mode(model)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules of the model\n    for module in model.modules():\n\n        # Define a context manager that temporarily sets the \"training\" attribute of the module to a constant value\n        @contextlib.contextmanager\n        def freeze_training_mode_context_manager(module):\n            # Store the original value of the \"training\" attribute\n            original_training_value = module.training\n\n            # Set the \"training\" attribute to a constant value\n            module.training = False\n\n            # Yield control to the context manager\n            yield\n\n            # Revert the \"training\" attribute back to its original value\n            module.training = original_training_value\n\n        # Use the context manager to freeze the training mode for the module\n        with freeze_training_mode_context_manager(module):\n            # Perform any operations that require the frozen training mode\n            pass\n\n    # Return the modified model\n    return model"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    for module in model.modules():\n        if hasattr(module, \"training\"):\n            module.training = True\n\n    return model\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules of the model\n    for submodule in model.modules():\n        # Get the class definition of the submodule\n        submodule_class = submodule.__class__\n\n        # Define a new class with the same name as the original class, but with a frozen \"training\" attribute\n        frozen_class = type(submodule_class.__name__, (submodule_class,), {\"training\": False})\n\n        # Replace the original class with the frozen class in the module\n        setattr(submodule_class, submodule.__class__.__name__, frozen_class)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    for name, module in model.named_modules():\n        if hasattr(module, 'training'):\n            module.training = False\n\n    return model\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    for name, module in model.named_modules():\n        if hasattr(module, 'training'):\n            original_training = module.training\n            module.training = False\n\n            def freeze_training(module):\n                for name, param in module.named_parameters():\n                    param.requires_grad = False\n\n            module.apply(freeze_training)\n\n            module.training = original_training\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Define the context manager\n    @contextlib.contextmanager\n    def freeze_training_mode_ctx(model):\n        # Save the original class definitions of the submodules\n        original_classes = {name: cls for name, cls in model.named_modules()}\n        # Iterate through all submodules and modify their class definitions\n        for name, cls in model.named_modules():\n            # Define a new class that inherits from the original class and overrides the \"training\" attribute\n            class FrozenModule(cls):\n                @property\n                def training(self):\n                    return False\n            # Replace the original class with the modified class\n            setattr(model, name, FrozenModule)\n        try:\n            yield\n        finally:\n            # Revert the class definitions of the submodules back to their original state\n            for name, cls in original_classes.items():\n                setattr(model, name, cls)\n\n    # Use the context manager to freeze the training mode\n    with freeze_training_mode_ctx(model):\n        # Perform any operations that require the frozen training mode\n        pass"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Define a context manager to temporarily freeze the training mode\n    @contextlib.contextmanager\n    def freeze_training_mode_context(model):\n        # Store the original class definitions of the submodules\n        original_classes = {name: module.__class__ for name, module in model.named_modules()}\n\n        # Iterate through all submodules and modify their class definitions\n        for name, module in model.named_modules():\n            # Define a new class that inherits from the original class and overrides the \"training\" attribute\n            class FrozenModule(module.__class__):\n                @property\n                def training(self):\n                    return False\n\n            # Replace the module's class with the new class\n            setattr(model, name, FrozenModule())\n\n        try:\n            # Yield control to the caller, allowing for code to be executed within the context\n            yield\n        finally:\n            # Revert the original class definitions of the submodules after the context manager exits\n            for name, module in model.named_modules():\n                setattr(model, name, original_classes[name])\n\n    # Use the context manager to freeze the training mode for the given model\n    with freeze_training_mode_context(model):\n        # Code to be executed within the context\n        pass\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules in the model\n    for name, module in model.named_modules():\n        # Define a context manager to temporarily set the \"training\" attribute to a constant value\n        @contextlib.contextmanager\n        def training_mode(model):\n            # Save the original training mode of the module\n            original_training_mode = module.training\n            # Set the training mode to a constant value\n            module.training = False\n            try:\n                # Yield the module to the context manager\n                yield module\n            finally:\n                # Restore the original training mode of the module\n                module.training = original_training_mode\n\n        # Use the context manager to modify the class definition of the module\n        with training_mode(module):\n            # Check if the module has a \"training\" attribute\n            if hasattr(module, \"training\"):\n                # Get the original type of the \"training\" attribute\n                original_type = type(module.training)\n                # Define a new type that inherits from the original type and overrides the __setattr__ method to raise an AttributeError\n                class FrozenTrainingMode(original_type):\n                    def __setattr__(self, name, value):\n                        raise AttributeError(\n                            f\"The 'training' attribute of {type(module).__name__} is frozen and cannot be set.\"\n                        )\n\n                # Replace the type of the \"training\" attribute with the new type\n                module.training = FrozenTrainingMode()\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def are_shapes_equal_validator(cls, values):\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(f\"{field1} and {field2} must have the same shape\")\n        return values\n\n    return are_shapes_equal_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def are_shapes_equal_validator(cls, values):\n        if values.get(field1).shape != values.get(field2).shape:\n            raise ValueError(f\"{field1} and {field2} must have the same shape\")\n        return values\n\n    return are_shapes_equal_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shapes(cls, values: Dict) -> Dict:\n        if values.get(field1).shape != values.get(field2).shape:\n            raise ValueError(f\"{field1} and {field2} must have the same shape\")\n        return values\n\n    return validate_shapes"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def _are_shapes_equal(cls, v, values):\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(f\"{field1} and {field2} must have the same shape.\")\n        return v\n\n    return _are_shapes_equal"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shapes(cls, values: dict) -> dict:\n        if values.get(field1).shape != values.get(field2).shape:\n            raise ValueError(\n                f\"Shape mismatch between {field1} and {field2}: {values.get(field1).shape} != {values.get(field2).shape}\"\n            )\n        return values\n\n    return validate_shapes"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shapes(cls, values):\n        field1_shape = values.get(field1).shape\n        field2_shape = values.get(field2).shape\n\n        if field1_shape != field2_shape:\n            raise ValueError(f\"The shapes of {field1} and {field2} do not match.\")\n\n        return values\n\n    return validate_shapes"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shapes(cls, v):\n        if v[field1].shape != v[field2].shape:\n            raise ValueError(\n                f\"Shape mismatch between {field1} and {field2}. {field1} has shape {v[field1].shape} and {field2} has shape {v[field2].shape}.\"\n            )\n        return v\n\n    return validate_shapes"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def are_shapes_equal_validator(cls, values: Dict) -> Dict:\n        if values.get(field1).shape != values.get(field2).shape:\n            raise ValueError(\n                f\"Field {field1} and {field2} must have the same shape.\"\n            )\n        return values\n\n    return are_shapes_equal_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def are_shapes_equal_validator(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        field1_value = values.get(field1)\n        field2_value = values.get(field2)\n        if field1_value is not None and field2_value is not None:\n            if field1_value.shape != field2_value.shape:\n                raise ValueError(\n                    f\"Shape mismatch between {field1} and {field2}. \"\n                    f\"Expected shape: {field1_value.shape}, \"\n                    f\"Actual shape: {field2_value.shape}\"\n                )\n        return values\n\n    return are_shapes_equal_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shapes(cls, values: Dict) -> Dict:\n        if values.get(field1).shape != values.get(field2).shape:\n            raise ValueError(\n                f\"Field shapes do not match: {field1} shape is {values.get(field1).shape}, but {field2} shape is {values.get(field2).shape}\"\n            )\n        return values\n\n    return validate_shapes"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shapes(cls, values: dict) -> None:\n        \"\"\"\n        This function is a Pydantic validator that checks if two specified fields within a model have the same shape.\n\n        Input Arguments\n        :param cls: The Pydantic model class being validated.\n        :param values: dict, A dictionary containing the values of the fields being validated.\n        :return: None, If the shapes of the two fields match, the function does not return anything. If the shapes do not match, it raises a ValueError indicating the mismatch.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"Shapes of {field1} and {field2} do not match: {values[field1].shape} vs {values[field2].shape}\"\n            )\n\n    return validate_shapes"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shapes(cls, v):\n        if v[field1].shape != v[field2].shape:\n            raise ValueError(\n                f\"The shapes of {field1} and {field2} do not match.\"\n            )\n        return v\n\n    return validate_shapes\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shapes(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        This function is a validator for a Pydantic model. It compares the shapes of two fields within the model and raises a ValueError if they do not match.\n\n        Input Arguments\n        :param cls: The class of the Pydantic model.\n        :param values: Dict[str, Any], A dictionary containing the values of the fields in the model.\n        :return: Dict[str, Any], The same dictionary of values passed as input.\n        \"\"\"\n        field1_value = values.get(field1)\n        field2_value = values.get(field2)\n        if field1_value.shape != field2_value.shape:\n            raise ValueError(\n                f\"The shapes of {field1} and {field2} do not match: {field1_value.shape} vs {field2_value.shape}\"\n            )\n        return values\n\n    return validate_shapes"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shapes(cls, values):\n        \"\"\"\n        This function is a validator for Pydantic models. It checks if the shapes of two specified fields match within the model.\n\n        Input Arguments\n        :param cls: The class of the Pydantic model being validated.\n        :param values: A dictionary containing the values of the fields being validated.\n        :return: None, if the shapes of the two fields match. If the shapes do not match, it raises a ValueError indicating the mismatch.\n        \"\"\"\n        v1 = values.get(field1)\n        v2 = values.get(field2)\n        if v1.shape != v2.shape:\n            raise ValueError(f\"Shape mismatch: {field1} and {field2} must have the same shape\")\n\n    return validate_shapes"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shapes(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        This function is used as a validator for a Pydantic model. It compares the shapes of two specified fields within the model.\n\n        Input Arguments\n        :param cls: Any, The class of the Pydantic model.\n        :param values: Dict[str, Any], A dictionary containing the values of the fields being validated.\n        :return: Dict[str, Any], The same dictionary of values passed as input.\n        \"\"\"\n        field1_shape = values[field1].shape\n        field2_shape = values[field2].shape\n        if field1_shape != field2_shape:\n            raise ValueError(\n                f\"The shapes of {field1} and {field2} do not match: {field1_shape} vs {field2_shape}\"\n            )\n        return values\n\n    return validate_shapes"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def are_shapes_equal_validator(cls, v, values):\n        field1_value = values[field1]\n        field2_value = values[field2]\n\n        if not np.shape(field1_value) == np.shape(field2_value):\n            raise ValueError(\n                f\"{field1} and {field2} must have the same shape\"\n            )\n        return v\n\n    return are_shapes_equal_validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def check_shape(cls, values: dict) -> None:\n        \"\"\"\n        This function is a Pydantic validator that checks if the shapes of two specified fields match.\n\n        Input Arguments\n        :param cls: The class of the model being validated.\n        :param values: dict, A dictionary containing the values of the fields being compared.\n        :return: None, If the shapes of the two fields match, the function does not return anything.\n        :raise ValueError: If the shapes of the two fields do not match, it raises a ValueError indicating the mismatch.\n        \"\"\"\n        shape1 = values.get(field1).shape\n        shape2 = values.get(field2).shape\n        if shape1 != shape2:\n            raise ValueError(f\"{field1} and {field2} must have the same shape\")\n\n    return check_shape"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def _are_shapes_equal(cls, v: Any, values: Dict[str, Any]) -> Any:\n        if not np.shape(v) == np.shape(values[field2]):\n            raise ValueError(\n                f\"{field1} and {field2} must have the same shape, {np.shape(v)} != {np.shape(values[field2])}\"\n            )\n        return v\n\n    return _are_shapes_equal\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def validate_shapes(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        This function is a validator function that is used within a Pydantic model to ensure the shapes of two fields match.\n\n        Input-Output Arguments\n        :param cls: Type[BaseModel], The class of the Pydantic model that contains the fields to be checked.\n        :param values: Dict[str, Any], A dictionary containing the values of the fields to be checked.\n        :return: Dict[str, Any], The same dictionary of values that was passed in as input.\n        \"\"\"\n        field_1 = values.get(field1)\n        field_2 = values.get(field2)\n        if not (field_1 is None or field_2 is None):\n            if not field_1.shape == field_2.shape:\n                raise ValueError(\n                    f\"Shape mismatch between {field1} and {field2} fields.\"\n                )\n        return values\n\n    return validate_shapes"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def _are_shapes_equal(cls, v: Any, values: Dict[str, Any]) -> Any:\n        \"\"\"\n        This function is a validator function that is used within a Pydantic model to ensure the shapes of two specified fields match.\n\n        Input-Output Arguments\n        :param cls: Any, The class instance that the validator is being called on.\n        :param v: Any, The value of the field that is being validated.\n        :param values: Dict[str, Any], A dictionary containing all the values of the model instance. It is used to access the value of the second field being compared to the shape of the first field.\n        :return: Any, The value of the field that is being validated if the shapes match. If the shapes do not match, it raises a ValueError indicating the mismatch.\n        \"\"\"\n\n        if not isinstance(v, np.ndarray):\n            raise ValueError(\"The field must be a NumPy array.\")\n\n        if not isinstance(values[field2], np.ndarray):\n            raise ValueError(\"The field must be a NumPy array.\")\n\n        if v.shape != values[field2].shape:\n            raise ValueError(\"The shapes of the fields do not match.\")\n\n        return v\n\n    return _are_shapes_equal"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list) and all(isinstance(metric, str) for metric in metrics):\n        return metrics, []\n    elif isinstance(metrics, list) and all(isinstance(metric, dict) for metric in metrics):\n        return [metric[\"name\"] for metric in metrics], metrics\n    else:\n        raise ValueError(\"Metrics must be a list of strings or dictionaries.\")"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            return metrics, [{}] * len(metrics)\n        elif isinstance(metrics[0], dict):\n            return [m[\"name\"] for m in metrics], metrics\n        else:\n            raise ValueError(\"Metrics must be either a list of strings or a list of dictionaries\")\n    else:\n        raise ValueError(\"Metrics must be a list\")\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if len(metrics) == 0:\n            return [], []\n        elif isinstance(metrics[0], str):\n            return metrics, [{} for _ in metrics]\n        elif isinstance(metrics[0], dict):\n            return [metric[\"name\"] for metric in metrics], metrics\n        else:\n            raise ValueError(\"Metrics must be a list of strings or dictionaries.\")\n    else:\n        raise ValueError(\"Metrics must be a list.\")"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        return metrics, [{} for _ in metrics]\n\n    return [metric[\"name\"] for metric in metrics], [metric[\"params\"] for metric in metrics]\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n    if isinstance(metrics, list):\n        if all(isinstance(metric, str) for metric in metrics):\n            return metrics, [{} for _ in metrics]\n        elif all(isinstance(metric, dict) for metric in metrics):\n            return [metric[\"name\"] for metric in metrics], metrics\n        else:\n            raise ValueError(\"Invalid metric format\")\n    else:\n        raise ValueError(\"Invalid metric format\")\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            return metrics, [{\"name\": m} for m in metrics]\n        elif isinstance(metrics[0], dict):\n            return [m[\"name\"] for m in metrics], metrics\n        else:\n            raise ValueError(\"Metrics must be either a list of strings or a list of dictionaries.\")\n    else:\n        raise ValueError(\"Metrics must be a list.\")\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if not isinstance(metrics, list):\n        raise ValueError(\"Metrics must be a list of strings or dictionaries\")\n\n    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_name = list(metric.keys())[0]\n            metric_params.append(metric[metric_name])\n            metric_names.append(metric_name)\n        else:\n            raise ValueError(\"Metrics must be a list of strings or dictionaries\")\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if not metrics:\n        return [], []\n    if isinstance(metrics[0], str):\n        return metrics, [{}] * len(metrics)\n    if isinstance(metrics[0], dict):\n        return [metric[\"name\"] for metric in metrics], metrics\n    raise ValueError(\"Invalid input format for metrics.\")"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list) and all(isinstance(metric, str) for metric in metrics):\n        return metrics, [{}] * len(metrics)\n    elif isinstance(metrics, list) and all(isinstance(metric, dict) for metric in metrics):\n        return [metric[\"name\"] for metric in metrics], metrics\n    else:\n        raise ValueError(\n            \"Metrics should be either a list of strings or a list of dictionaries\"\n        )"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            return metrics, [{} for _ in metrics]\n        elif isinstance(metrics[0], dict):\n            return [m[\"name\"] for m in metrics], metrics\n        else:\n            raise ValueError(\n                \"Metrics must be a list of strings or a list of dictionaries\"\n            )\n    else:\n        raise ValueError(\"Metrics must be a list of strings or a list of dictionaries\")\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            metric_names = metrics\n            metric_params = [{}] * len(metric_names)\n        elif isinstance(metrics[0], dict):\n            metric_names = [metric[\"name\"] for metric in metrics]\n            metric_params = metrics\n        else:\n            raise ValueError(\"Metrics must be either a list of strings or a list of dictionaries\")\n    else:\n        raise ValueError(\"Metrics must be a list\")\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Cast metrics to a list of strings if necessary\n    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    # Cast metrics to a list of dictionaries if necessary\n    if isinstance(metrics[0], str):\n        metrics = [{\"name\": metric} for metric in metrics]\n\n    # Extract metric names and parameters\n    metric_names = [metric[\"name\"] for metric in metrics]\n    metric_params = [metric.get(\"params\", {}) for metric in metrics]\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Convert input to list if it is not a list\n    if not isinstance(metrics, list):\n        metrics = [metrics]\n\n    # Extract metric names and parameters\n    metric_names = []\n    metric_params = []\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(metric[\"name\"])\n            metric_params.append(metric)\n        else:\n            raise ValueError(f\"Invalid metric format: {metric}\")\n\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if not isinstance(metrics, list):\n        raise TypeError(\"Metrics must be a list\")\n\n    if not metrics:\n        raise ValueError(\"Metrics list cannot be empty\")\n\n    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            if \"name\" not in metric:\n                raise ValueError(\"Metric dictionary must contain 'name' key\")\n            if not isinstance(metric[\"name\"], str):\n                raise TypeError(\"Metric name must be a string\")\n            metric_names.append(metric[\"name\"])\n            metric_params.append(metric)\n        else:\n            raise TypeError(\"Metrics must be either strings or dictionaries\")\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if not isinstance(metrics, list):\n        raise ValueError(\"Metrics must be a list of strings or dictionaries.\")\n\n    if not all(isinstance(metric, str) or isinstance(metric, dict) for metric in metrics):\n        raise ValueError(\"Metrics must be a list of strings or dictionaries.\")\n\n    if all(isinstance(metric, str) for metric in metrics):\n        return metrics, [{} for _ in metrics]\n\n    metric_names, metric_params = zip(*[(metric[\"name\"], metric) for metric in metrics])\n    return list(metric_names), list(metric_params)\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if not isinstance(metrics, list):\n        raise ValueError(\"Metrics must be a list.\")\n\n    if not metrics:\n        return [], []\n\n    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            if \"name\" not in metric:\n                raise ValueError(\"Each metric must have a name.\")\n            metric_names.append(metric[\"name\"])\n            metric_params.append(metric)\n        else:\n            raise ValueError(\n                \"Metrics must be strings or dictionaries with 'name' key.\"\n            )\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    metrics_names = []\n    metrics_params = []\n    for metric in metrics:\n        if isinstance(metric, str):\n            metrics_names.append(metric)\n            metrics_params.append({})\n        elif isinstance(metric, dict):\n            metrics_names.append(metric[\"name\"])\n            metrics_params.append(metric)\n        else:\n            raise ValueError(\n                \"Invalid metric format. Please provide either a string or a dictionary.\"\n            )\n    return metrics_names, metrics_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n    elif isinstance(metrics, dict):\n        metrics = [metrics]\n    elif not isinstance(metrics, list):\n        raise ValueError(f\"Invalid type for metrics: {type(metrics)}\")\n\n    # Check if all elements in the list are either strings or dictionaries\n    if not all(isinstance(m, (str, dict)) for m in metrics):\n        raise ValueError(\"All elements in metrics must be either strings or dictionaries\")\n\n    # Convert all metric names to lowercase\n    metric_names = [m.lower() if isinstance(m, str) else m[\"name\"].lower() for m in metrics]\n\n    # Extract metric parameters\n    metric_params = []\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_params.append({\"name\": metric})\n        elif isinstance(metric, dict):\n            metric_params.append(metric)\n\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # If the input is a list of strings, create a list of dictionaries with the metric name as the key and an empty dictionary as the value.\n    if isinstance(metrics, list) and all(isinstance(metric, str) for metric in metrics):\n        metrics = [{\"name\": metric, \"params\": {}} for metric in metrics]\n\n    # If the input is a list of dictionaries, check that each dictionary has the correct format.\n    if isinstance(metrics, list) and all(isinstance(metric, dict) for metric in metrics):\n        for metric in metrics:\n            if not isinstance(metric, dict):\n                raise ValueError(\"All metrics must be dictionaries.\")\n            if not isinstance(metric.get(\"name\"), str):\n                raise ValueError(\"All metrics must have a 'name' key.\")\n            if not isinstance(metric.get(\"params\"), dict):\n                raise ValueError(\"All metrics must have a 'params' key.\")\n\n    # Extract the metric names and parameters from the input list of dictionaries.\n    metric_names = [metric[\"name\"] for metric in metrics]\n    metric_params = [metric[\"params\"] for metric in metrics]\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if not isinstance(metrics, list):\n        raise ValueError(\"metrics should be a list\")\n    if len(metrics) == 0:\n        raise ValueError(\"metrics should not be empty\")\n\n    # Check if all elements are strings or dictionaries\n    if all(isinstance(metric, str) for metric in metrics):\n        metric_names = metrics\n        metric_params = [{} for _ in range(len(metric_names))]\n    elif all(isinstance(metric, dict) for metric in metrics):\n        metric_names = [metric.get(\"name\") for metric in metrics]\n        metric_params = metrics\n    else:\n        raise ValueError(\"All elements in metrics should be strings or dictionaries\")\n\n    return metric_names, metric_params\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == lambda t: t:\n      fn_inv = lambda s: s\n    elif fn == lambda t: 1 / t:\n      fn_inv = lambda s: 1 / s\n    elif fn == lambda t: t ** 2:\n      fn_inv = lambda s: s ** 0.5\n    elif fn == lambda t: t ** 3:\n      fn_inv = lambda s: s ** (1 / 3)\n    elif fn == lambda t: t ** 4:\n      fn_inv = lambda s: s ** 0.25\n    elif fn == lambda t: t ** 5:\n      fn_inv = lambda s: s ** 0.2\n    elif fn == lambda t: t ** 6:\n      fn_inv = lambda s: s ** 0.15\n    elif fn == lambda t: t ** 7:\n      fn_inv = lambda s: s ** 0.14285714285714285\n    elif fn == lambda t: t ** 8:\n      fn_inv = lambda s: s ** 0.125\n    elif fn == lambda t: t ** 9:\n      fn_inv = lambda s: s ** 0.1111111111111111\n    elif fn == lambda t: t ** 10:\n      fn_inv = lambda s: s ** 0.1\n    elif fn == lambda t: t ** 11:\n      fn_inv = lambda s: s ** 0.09090909090909091\n    elif fn == lambda t: t ** 12:\n      fn_inv = lambda s: s ** 0.08333333333333333\n    elif fn == lambda t: t ** 13:\n      fn_inv = lambda s: s ** 0.07692307"}
{"namespace": "coord.construct_ray_warps", "completion": "  # If the inverse function is not provided, try to automatically determine it\n  if fn_inv is None:\n    # Define a dictionary mapping functions to their inverses\n    fn_inv_map = {\n      lambda t: t: lambda s: s,\n      lambda t: 1.0 - t: lambda s: 1.0 - s,\n      lambda t: t ** 2: lambda s: s ** 0.5,\n      lambda t: t ** 3: lambda s: s ** 0.3333333333333333,\n      lambda t: t ** 4: lambda s: s ** 0.25,\n      lambda t: t ** 5: lambda s: s ** 0.2,\n      lambda t: t ** 6: lambda s: s ** 0.16666666666666666,\n      lambda t: t ** 7: lambda s: s ** 0.14285714285714285,\n      lambda t: t ** 8: lambda s: s ** 0.125,\n      lambda t: t ** 9: lambda s: s ** 0.1111111111111111,\n      lambda t: t ** 10: lambda s: s ** 0.1,\n      lambda t: t ** 11: lambda s: s ** 0.09090909090909091,\n      lambda t: t ** 12: lambda s: s ** 0.08333333333333333,\n      lambda t: t ** 13: lambda s: s ** 0.07692307692307693,\n      lambda t: t ** 14: lambda s: s ** 0.0714285714285"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: 1.0 - x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0 - x,\n      lambda x: x,\n      lambda x: 1.0"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Construct the forward mapping from metric to normalized distances.\n  t_to_s = lambda t: (t - t_near) / (t_far - t_near)\n\n  # Construct the backward mapping from normalized to metric distances.\n  if fn_inv is not None:\n    s_to_t = fn_inv\n  else:\n    # Attempt to automatically determine the inverse based on a predefined mapping of functions to their inverses.\n    inverse_mappings = {\n      lambda x: x ** 2: lambda x: x ** 0.5,\n      lambda x: x ** 3: lambda x: x ** (1 / 3),\n      lambda x: x ** 4: lambda x: x ** 0.25,\n      lambda x: x ** 5: lambda x: x ** 0.2,\n      lambda x: x ** 6: lambda x: x ** 0.17,\n      lambda x: x ** 7: lambda x: x ** 0.143,\n      lambda x: x ** 8: lambda x: x ** 0.125,\n      lambda x: x ** 9: lambda x: x ** 0.111,\n      lambda x: x ** 10: lambda x: x ** 0.1,\n      lambda x: x ** 11: lambda x: x ** 0.091,\n      lambda x: x ** 12: lambda x: x ** 0.083,\n      lambda x: x ** 13: lambda x: x ** 0.077,\n      lambda x: x ** 14: lambda x: x ** 0.071,\n      lambda x: x ** 15: lambda x: x ** 0.067,\n      lambda x: x ** 16: lambda x: x ** 0.063,\n      lambda x: x ** 17: lambda x: x ** 0.06,\n      l"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Define a dictionary of known functions and their inverses\n  known_functions = {\n    'exp': lambda x: torch.exp(x),\n    'log': lambda x: torch.log(x),\n    'sqrt': lambda x: torch.sqrt(x),\n    'rsqrt': lambda x: 1.0 / torch.sqrt(x),\n    'tanh': lambda x: torch.tanh(x),\n    'atanh': lambda x: torch.atanh(x),\n    'sigmoid': lambda x: torch.sigmoid(x),\n    'asinh': lambda x: torch.asinh(x),\n    'acosh': lambda x: torch.acosh(x),\n    'sinh': lambda x: torch.sinh(x),\n    'cosh': lambda x: torch.cosh(x),\n    'tanhshrink': lambda x: x - torch.tanh(x),\n    'softplus': lambda x: torch.nn.functional.softplus(x),\n    'softshrink': lambda x: torch.nn.functional.softshrink(x),\n    'softsign': lambda x: torch.nn.functional.softsign(x),\n    'softmin': lambda x: torch.nn.functional.softmin(x),\n    'softmax': lambda x: torch.nn.functional.softmax(x),\n    'logsigmoid': lambda x: torch.nn.functional.logsigmoid(x),\n    'logit': lambda x: torch.nn.functional.logit(x),\n    'log1p': lambda x: torch.log1p(x),\n    'expm1': lambda x: torch.expm1(x),\n    'erf': lambda x: torch.erf(x),\n    'erfc': lambda x: torch.erfc(x),\n    'erfinv': l"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Construct the forward mapping from metric to normalized distances\n  def t_to_s(t):\n    return (t - t_near) / (t_far - t_near)\n\n  # Construct the backward mapping from normalized to metric distances\n  def s_to_t(s):\n    return s * (t_far - t_near) + t_near\n\n  # If an inverse function is provided, use it\n  if fn_inv is not None:\n    return t_to_s, fn_inv\n\n  # Otherwise, try to automatically determine the inverse based on a predefined mapping of functions to their inverses\n  fn_inv_map = {\n      lambda x: x ** 2: lambda x: x ** 0.5,\n      lambda x: x ** 3: lambda x: x ** (1 / 3),\n      lambda x: x ** 4: lambda x: x ** 0.25,\n      lambda x: x ** 0.5: lambda x: x ** 2,\n      lambda x: x ** (1 / 3): lambda x: x ** 3,\n      lambda x: x ** 0.25: lambda x: x ** 4,\n      lambda x: x ** 0.5 * (1 - x ** 0.5): lambda x: x ** 2 * (1 - x ** 2) ** 0.5,\n      lambda x: x ** 0.5 * (1 - x ** 0.5) ** 0.5: lambda x: x ** 2 * (1 - x ** 2),\n      lambda x: x ** 0.5 * (1 - x ** 0.5) ** 0.25: lambda x: x ** 2 * (1 - x ** 2) ** 0.5,\n      lambda x: x ** 0.5 * (1 - x ** 0.5) ** 0.5: lambda x: x ** 2 * (1 - x ** 2),\n      lambda x: x ** 0.5 * (1 - x"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Attempt to automatically determine the inverse function based on a predefined mapping\n    fn_inv = {\n        lambda t: t: lambda s: s,\n        lambda t: t ** 2: lambda s: s ** 0.5,\n        lambda t: t ** 3: lambda s: s ** (1. / 3.),\n        lambda t: t ** 4: lambda s: s ** 0.25,\n        lambda t: t ** (1. / 3.): lambda s: s ** 3,\n        lambda t: t ** (2. / 3.): lambda s: s ** (2. / 3.),\n        lambda t: t ** (3. / 2.): lambda s: s ** 2,\n        lambda t: t ** 0.5: lambda s: s ** 2,\n        lambda t: t ** 0.25: lambda s: s ** 4,\n        lambda t: t ** 0.5: lambda s: s ** 2,\n        lambda t: t ** 0.25: lambda s: s ** 4,\n        lambda t: t ** 0.25: lambda s: s ** 4,\n        lambda t: t ** 0.25: lambda s: s ** 4,\n        lambda t: t ** 0.25: lambda s: s ** 4,\n        lambda t: t ** 0.25: lambda s: s ** 4,\n        lambda t: t ** 0.25: lambda s: s ** 4,\n        lambda t: t ** 0.25: lambda s: s ** 4,\n        lambda t: t ** 0.25: lambda s: s ** 4,\n        lambda t: t ** 0.25: lambda s: s ** 4,\n        lambda t: t ** 0.25: lambda s: s ** 4,\n        lambda t:"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Construct the forward mapping from metric to normalized distances\n  t_to_s = lambda t: torch.clip((t - t_near) / (t_far - t_near), 0., 1.)\n\n  # Construct the backward mapping from normalized to metric distances\n  if fn_inv is not None:\n    s_to_t = fn_inv\n  else:\n    # Define a dictionary of known functions and their inverses\n    fn_inverses = {\n      'exp': lambda x: torch.log(x) / torch.log(torch.tensor(2.)),\n      'sigmoid': lambda x: torch.log(x / (1 - x)) / torch.log(torch.tensor(2.)),\n      'tanh': lambda x: torch.atanh(x) / torch.atanh(torch.tensor(1.)),\n      'softplus': lambda x: torch.log(torch.exp(x) - 1.) / torch.log(torch.tensor(2.)),\n      'relu': lambda x: torch.log(x) / torch.log(torch.tensor(2.)),\n      'leaky_relu': lambda x: torch.log(torch.maximum(x, torch.tensor(0.))) / torch.log(torch.tensor(2.)),\n      'elu': lambda x: torch.log(torch.maximum(x, torch.tensor(0.)) + torch.exp(torch.minimum(x, torch.tensor(0.)))) / torch.log(torch.tensor(2.)),\n      'selu': lambda x: torch.log(torch.maximum(x, torch.tensor(1.6732632423543772848170429916717)) + torch.exp(torch.minimum(x, torch.tensor(-1.758099340847376696"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n        'exp': lambda s: torch.exp(s),\n        'log': lambda s: torch.log(s),\n        'tanh': lambda s: torch.tanh(s),\n        'sin': lambda s: torch.sin(s),\n        'cos': lambda s: torch.cos(s),\n        'asin': lambda s: torch.asin(s),\n        'acos': lambda s: torch.acos(s),\n        'atan': lambda s: torch.atan(s),\n        'softplus': lambda s: torch.log(torch.exp(s) + 1),\n        'softmax': lambda s: torch.log(torch.exp(s) / torch.sum(torch.exp(s))),\n        'softmin': lambda s: -torch.log(torch.exp(-s) / torch.sum(torch.exp(-s))),\n        'sigmoid': lambda s: 1 / (1 + torch.exp(-s)),\n        'tanh_exp': lambda s: torch.tanh(torch.exp(s)),\n        'tanh_log': lambda s: torch.tanh(torch.log(s)),\n        'tanh_sin': lambda s: torch.tanh(torch.sin(s)),\n        'tanh_cos': lambda s: torch.tanh(torch.cos(s)),\n        'tanh_asin': lambda s: torch.tanh(torch.asin(s)),\n        'tanh_acos': lambda s: torch.tanh(torch.acos(s)),\n        'tanh_atan': lambda s: torch.tanh(torch.atan(s)),\n        'tanh_softplus': lambda s: torch.tanh(torch.log(torch.exp(s) + 1)),\n        'tanh_softmax': lambda"}
{"namespace": "coord.construct_ray_warps", "completion": "  # NOTE: This function is based on the following reference:\n  # https://www.pbr-book.org/3ed-2018/Monte_Carlo_Integration/2D_Sampling_with_Multidimensional_Transformations#Ray-to-Disk-Transformation\n  #\n  # NOTE: The following paper provides a detailed explanation of the ray-to-disk transformation:\n  # https://www.pbr-book.org/3ed-2018/Monte_Carlo_Integration/2D_Sampling_with_Multidimensional_Transformations#Ray-to-Disk-Transformation\n  #\n  # NOTE: The following paper provides a detailed explanation of the ray-to-disk transformation:\n  # https://www.pbr-book.org/3ed-2018/Monte_Carlo_Integration/2D_Sampling_with_Multidimensional_Transformations#Ray-to-Disk-Transformation\n  #\n  # NOTE: The following paper provides a detailed explanation of the ray-to-disk transformation:\n  # https://www.pbr-book.org/3ed-2018/Monte_Carlo_Integration/2D_Sampling_with_Multidimensional_Transformations#Ray-to-Disk-Transformation\n  #\n  # NOTE: The following paper provides a detailed explanation of the ray-to-disk transformation:\n  # https://www.pbr-book.org/3ed-2018/Monte_Carlo_Integration/2D_Sampling_with_Multidimensional_Transformations#Ray-to-Disk-Transformation\n  #\n  # NOTE: The following paper provides a detailed explanation of the ray-to-disk transformation:\n  # https://www.pbr-book.org/3ed-2018/Monte_Carlo_Integration/2D_Sampling_with_Multidimensional_Transformations#Ray-to-Disk-Transformation\n  #\n  # NOTE: The following paper provides a detailed explanation of the ray-to-disk transformation"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == torch.exp:\n      fn_inv = torch.log\n    elif fn == torch.pow:\n      fn_inv = lambda x, y: torch.pow(x, 1. / y)\n    elif fn == torch.sqrt:\n      fn_inv = lambda x, y: torch.pow(x, 2.)\n    elif fn == torch.rsqrt:\n      fn_inv = lambda x, y: torch.pow(x, -2.)\n    elif fn == torch.sinh:\n      fn_inv = torch.asinh\n    elif fn == torch.cosh:\n      fn_inv = torch.acosh\n    elif fn == torch.tanh:\n      fn_inv = torch.atanh\n    elif fn == torch.asinh:\n      fn_inv = torch.sinh\n    elif fn == torch.acosh:\n      fn_inv = torch.cosh\n    elif fn == torch.atanh:\n      fn_inv = torch.tanh\n    elif fn == torch.sigmoid:\n      fn_inv = lambda x, y: torch.log(x / (1 - x))\n    elif fn == torch.softplus:\n      fn_inv = lambda x, y: torch.log(torch.exp(x) - 1)\n    elif fn == torch.softmax:\n      fn_inv = lambda x, y: torch.log(x / (1 - x))\n    else:\n      raise ValueError(f\"Unsupported function: {fn}\")\n\n  def t_to_s(t):\n    t = torch.clamp(t, t_near, t_far)\n    return fn(t)\n\n  def s_to_t(s):\n    return fn_inv(s, t_near)\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == torch.exp:\n      fn_inv = torch.log\n    elif fn == torch.sqrt:\n      fn_inv = lambda x: x ** 2\n    elif fn == torch.pow:\n      fn_inv = lambda x, p: x ** (1 / p)\n    else:\n      raise ValueError(f\"Cannot automatically determine inverse function for {fn}.\")\n\n  def t_to_s(t):\n    t = torch.clamp(t, min=t_near, max=t_far)\n    return (fn(t) - fn(t_near)) / (fn(t_far) - fn(t_near))\n\n  def s_to_t(s):\n    return fn_inv((s * (fn(t_far) - fn(t_near))) + fn(t_near))\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    t = tf.clip_by_value(t, t_near, t_far)\n    return fn(t)\n\n  def s_to_t(s):\n    if fn_inv is not None:\n      return fn_inv(s)\n\n    # Invert the function using a lookup table.\n    # This is a bit hacky, but it works for the functions we care about.\n    if fn == tf.math.log:\n      return tf.math.exp(s)\n    elif fn == tf.math.rsqrt:\n      return 1. / (s ** 2)\n    else:\n      raise ValueError(f'Unsupported function: {fn}')\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Construct the inverse of the function if not provided\n  if fn_inv is None:\n    # Check if the function is one of the supported functions\n    if fn == torch.exp:\n      fn_inv = torch.log\n    elif fn == torch.sqrt:\n      fn_inv = lambda x: x ** 2\n    elif fn == lambda x: x ** 2:\n      fn_inv = torch.sqrt\n    elif fn == lambda x: x ** 3:\n      fn_inv = lambda x: x ** (1 / 3)\n    else:\n      raise ValueError(f'Unsupported function {fn}.')\n\n  # Construct the functions for mapping distances\n  def t_to_s(t):\n    t = torch.clamp(t, t_near, t_far)\n    return fn(t) / (fn(t_far) - fn(t_near))\n\n  def s_to_t(s):\n    s = torch.clamp(s, 0.0, 1.0)\n    return fn_inv(s * (fn(t_far) - fn(t_near)) + fn(t_near))\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Try to automatically determine the inverse function based on a predefined mapping\n    if fn == torch.exp:\n      fn_inv = torch.log\n    elif fn == torch.sqrt:\n      fn_inv = lambda x: x ** 2\n    elif fn == lambda x: 1 - torch.exp(-x):\n      fn_inv = lambda x: -torch.log(1 - x)\n    else:\n      raise ValueError(f\"Unsupported function: {fn}\")\n\n  def t_to_s(t):\n    t = torch.clamp(t, t_near, t_far)\n    return fn(t)\n\n  def s_to_t(s):\n    return fn_inv(s)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Check if fn_inv is provided or automatically determine its inverse\n  if fn_inv is None:\n    # Define a dictionary of functions and their inverses\n    fn_inv_dict = {\n      \"exp\": lambda t: torch.exp(t),\n      \"log\": lambda s: torch.log(s),\n      \"sqrt\": lambda t: torch.sqrt(t),\n      \"rsqrt\": lambda t: 1.0 / torch.sqrt(t),\n      \"sin\": lambda t: torch.asin(t),\n      \"asin\": lambda s: torch.sin(s),\n      \"cos\": lambda t: torch.acos(t),\n      \"acos\": lambda s: torch.cos(s),\n      \"tan\": lambda t: torch.atan(t),\n      \"atan\": lambda s: torch.tan(s),\n      \"sigmoid\": lambda t: 1.0 / (1.0 + torch.exp(-t)),\n      \"logit\": lambda s: torch.log(s / (1.0 - s)),\n      \"relu\": lambda t: torch.relu(t),\n      \"softplus\": lambda t: torch.log(1.0 + torch.exp(t)),\n      \"softmax\": lambda t: torch.exp(t) / torch.sum(torch.exp(t), dim=-1, keepdim=True),\n      \"softmin\": lambda t: -torch.exp(t) / torch.sum(torch.exp(t), dim=-1, keepdim=True),\n      \"softsign\": lambda t: t / (1.0 + torch.abs(t)),\n      \"softshrink\": lambda t: torch.where(torch.abs(t) < 1.0, 0.0, t),\n      \"softthrush\": lambda t: torch.where(torch.abs(t) < 1.0, t, 0.0),\n      \"softstaircase\": lambda"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    return (t - t_near) / (t_far - t_near)\n\n  def s_to_t(s):\n    return s * (t_far - t_near) + t_near\n\n  if fn_inv is None:\n    # Try to automatically determine the inverse function\n    try:\n      fn_inv = {\n          lambda x: x ** 2: lambda x: x ** 0.5,\n          lambda x: -x ** 2 + 2 * x: lambda x: -(x - 1) ** 0.5 + 1,\n          lambda x: 1 - np.exp(-x): lambda x: np.log(1 - x) + np.log(x),\n          lambda x: 1 - np.exp(-x): lambda x: np.log(1 - x) + np.log(x),\n          lambda x: 2 / (1 + np.exp(-x)) - 1: lambda x: np.log(x / (1 - x) + 1)\n      }[fn]\n    except KeyError:\n      raise ValueError(f\"Cannot automatically determine the inverse function for {fn}\")\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Construct the forward mapping from metric to normalized distances.\n  def t_to_s(t):\n    # Clip the distances to ensure they fall within the valid range.\n    t = tf.clip_by_value(t, t_near, t_far)\n    # Apply the provided function to the distances.\n    s = fn(t)\n    # Normalize the distances to the range [0, 1].\n    s = (s - t_near) / (t_far - t_near)\n    return s\n\n  # Construct the backward mapping from normalized to metric distances.\n  def s_to_t(s):\n    # Clip the normalized distances to ensure they fall within the valid range.\n    s = tf.clip_by_value(s, 0., 1.)\n    # Denormalize the distances to the range [t_near, t_far].\n    s = s * (t_far - t_near) + t_near\n    # If an inverse function is provided, use it to map the denormalized distances back to metric distances.\n    if fn_inv is not None:\n      t = fn_inv(s)\n    # Otherwise, try to automatically determine the inverse based on a predefined mapping of functions to their inverses.\n    else:\n      if fn == tf.math.log:\n        t = tf.exp(s)\n      elif fn == tf.math.sqrt:\n        t = s * s\n      elif fn == tf.math.rsqrt:\n        t = 1. / (tf.sqrt(s) + 1e-8)\n      elif fn == tf.math.square:\n        t = tf.sqrt(s)\n      elif fn == tf.math.rsqrt:\n        t = tf.sqrt(s)\n      elif fn == tf.math.log1p:\n        t = tf.exp(s) - 1.\n      else:\n        raise ValueError(f\"No inverse function defined for function {fn}.\")\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Check if the provided function is one of the supported functions\n  if fn in [\n      lambda x: x,\n      lambda x: 1.0 / x,\n      lambda x: x * x,\n      lambda x: 1.0 / (x * x),\n      lambda x: torch.exp(x),\n      lambda x: 1.0 / torch.exp(x),\n      lambda x: torch.exp(x * x),\n      lambda x: 1.0 / torch.exp(x * x),\n  ]:\n    # If the function is one of the supported functions, use the predefined inverse\n    if fn == lambda x: x:\n      fn_inv = lambda x: x\n    elif fn == lambda x: 1.0 / x:\n      fn_inv = lambda x: 1.0 / x\n    elif fn == lambda x: x * x:\n      fn_inv = lambda x: torch.sqrt(x)\n    elif fn == lambda x: 1.0 / (x * x):\n      fn_inv = lambda x: 1.0 / (x * x)\n    elif fn == lambda x: torch.exp(x):\n      fn_inv = lambda x: torch.log(x)\n    elif fn == lambda x: 1.0 / torch.exp(x):\n      fn_inv = lambda x: -torch.log(x)\n    elif fn == lambda x: torch.exp(x * x):\n      fn_inv = lambda x: torch.sqrt(torch.log(x))\n    elif fn == lambda x: 1.0 / torch.exp(x * x):\n      fn_inv = lambda x: 1.0 / torch.sqrt(torch.log(x))\n    else:\n      raise ValueError(\n        f\"Function {fn} is not supported. Please provide a valid function or an inverse function.\"\n      )\n  elif fn_inv is None:\n    # If the function is not supported and no inverse function is provided"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n      \"exp\": lambda x: torch.exp(x),\n      \"log\": lambda x: torch.log(x),\n      \"pow\": lambda x: torch.pow(x, 2),\n      \"sqrt\": lambda x: torch.sqrt(x),\n      \"sigmoid\": lambda x: 1.0 / (1.0 + torch.exp(-x)),\n      \"tanh\": lambda x: torch.tanh(x),\n      \"softplus\": lambda x: torch.log(1.0 + torch.exp(x)),\n      \"relu\": lambda x: torch.relu(x),\n      \"elu\": lambda x: torch.nn.functional.elu(x),\n      \"softmax\": lambda x: torch.softmax(x, dim=-1),\n    }.get(fn.__name__)\n\n  if fn_inv is None:\n    raise ValueError(f\"No inverse function found for the provided function {fn.__name__}. Please provide an inverse function as the `fn_inv` argument.\")\n\n  def t_to_s(t):\n    return torch.clamp((t - t_near) / (t_far - t_near), min=0.0, max=1.0)\n\n  def s_to_t(s):\n    return t_near + (t_far - t_near) * s\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n  import math\n\n  x = r * math.sin(theta) * math.cos(phi)\n  y = r * math.sin(theta) * math.sin(phi)\n  z = r * math.cos(theta)\n\n  return np.array([x, y, z])"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n  from math import sin, cos\n\n  x = r * sin(theta) * cos(phi)\n  y = r * sin(theta) * sin(phi)\n  z = r * cos(theta)\n\n  return np.array([x, y, z])"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  # Calculate the x, y, and z components of the cartesian coordinates\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  # Return the cartesian coordinates\n  return np.array([x, y, z])"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  # Convert the spherical coordinates to cartesian coordinates\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  # importing the required libraries\n  import numpy as np\n\n  # converting the spherical coordinates to cartesian coordinates\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  # returning the cartesian coordinates\n  return np.array([x, y, z])"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  # Check if the input arguments are numeric types.\n  if not (isinstance(r, int) or isinstance(r, float)):\n    raise Exception(\"The radius must be a numeric type.\")\n  if not (isinstance(theta, int) or isinstance(theta, float)):\n    raise Exception(\"The elevation angle must be a numeric type.\")\n  if not (isinstance(phi, int) or isinstance(phi, float)):\n    raise Exception(\"The azimuth angle must be a numeric type.\")\n\n  # Check if the input arguments are positive.\n  if r < 0:\n    raise Exception(\"The radius must be a positive number.\")\n  if theta < 0 or theta > 180:\n    raise Exception(\"The elevation angle must be between 0 and 180 degrees.\")\n  if phi < 0 or phi > 360:\n    raise Exception(\"The azimuth angle must be between 0 and 360 degrees.\")\n\n  # Convert the elevation angle to radians.\n  theta = theta * (3.14 / 180)\n\n  # Convert the azimuth angle to radians.\n  phi = phi * (3.14 / 180)\n\n  # Calculate the cartesian coordinates.\n  x = r * sin(theta) * cos(phi)\n  y = r * sin(theta) * sin(phi)\n  z = r * cos(theta)\n\n  # Return the cartesian coordinates.\n  return [x, y, z]"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if not (len(t) > 1 and len(w) > 1 and len(t) == len(w)):\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  # Calculate the integral using the trapezoid rule\n  integral = 0\n  for i in range(1, len(t)):\n    integral += (t[i] - t[i - 1]) * (w[i] + w[i - 1]) / 2\n\n  return integral"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) < 2 or len(w) < 2:\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  # Calculate the integral using the trapezoid rule\n  integral = 0\n  for i in range(len(t) - 1):\n    integral += (w[i + 1] + w[i]) * (t[i + 1] - t[i]) / 2\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if not is_valid_linear_spline(t, w):\n    raise ValueError(\"Input data points are not valid for a linear spline.\")\n\n  # Initialize the integral to zero\n  integral = 0\n\n  # Compute the integral using the trapezoid rule\n  for i in range(len(t) - 1):\n    integral += (w[i + 1] + w[i]) * (t[i + 1] - t[i]) / 2\n\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if not (isinstance(t, list) and isinstance(w, list) and len(t) == len(w)):\n    raise ValueError(\"Input data points are not valid for a linear spline\")\n\n  # Calculate the integral using the trapezoid rule\n  n = len(t)\n  integral = 0\n  for i in range(1, n):\n    integral += (t[i] - t[i - 1]) * (w[i] + w[i - 1]) / 2\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if not is_valid(t, w):\n    raise ValueError(\"Input data points are not valid for a linear spline.\")\n\n  # Calculate the integral using the trapezoid rule\n  integral = 0.0\n  for i in range(len(t) - 1):\n    integral += (w[i + 1] + w[i]) * (t[i + 1] - t[i]) / 2.0\n\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the data points are valid for a linear spline\n  if not check_data_points(t, w):\n    raise ValueError(\"The data points are not valid for a linear spline.\")\n\n  # Calculate the integral using the trapezoid rule\n  integral = 0\n  for i in range(len(t) - 1):\n    integral += (w[i + 1] + w[i]) * (t[i + 1] - t[i]) / 2\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  # Ensure the input data points are valid for a linear spline\n  if len(t) < 2 or len(w) < 2:\n    raise ValueError('At least two data points are required to compute the integral.')\n\n  # Initialize the integral\n  integral = 0\n\n  # Compute the integral using the trapezoid rule\n  for i in range(len(t) - 1):\n    integral += (t[i + 1] - t[i]) * (w[i] + w[i + 1]) / 2\n\n  return integral"}
{"namespace": "linspline.integrate", "completion": "  if len(t) != len(w):\n    raise ValueError(\"Input data points must have the same number of elements\")\n  if len(t) < 2:\n    raise ValueError(\"Input data points must have at least 2 elements\")\n  if t[0] > t[1]:\n    raise ValueError(\"Input data points must be sorted in increasing order\")\n  if t[-1] < t[-2]:\n    raise ValueError(\"Input data points must be sorted in increasing order\")\n\n  integral = 0\n  for i in range(1, len(t)):\n    integral += (t[i] - t[i - 1]) * (w[i] + w[i - 1]) / 2\n  return integral"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline.\n  if not is_valid_spline(t, w):\n    raise Exception(\"The input data points are not valid for a linear spline.\")\n\n  # Calculate the integral of the data points using the trapezoid rule.\n  return np.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if not check_validity(t, w):\n    return None\n\n  # Perform the integration using the trapezoid rule\n  integral = 0\n  for i in range(len(t) - 1):\n    integral += (t[i+1] - t[i]) * (w[i] + w[i+1]) / 2\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  # Ensure the input data points are valid for a linear spline.\n  if not is_valid_linear_spline(t, w):\n    raise ValueError(\"The input data points are not valid for a linear spline.\")\n\n  # Calculate the integral using the trapezoid rule.\n  return np.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline.\n  if not (len(t) == len(w) and len(t) > 1):\n    raise ValueError('Input data points are not valid for a linear spline.')\n\n  # Calculate the integral of the data points using the trapezoid rule.\n  integral = 0\n  for i in range(len(t) - 1):\n    integral += (t[i + 1] - t[i]) * (w[i] + w[i + 1]) / 2\n\n  return integral\n\n"}
{"namespace": "linspline.integrate", "completion": "  if len(t) == 0 or len(w) == 0:\n    raise ValueError('The input data points are empty.')\n\n  if len(t) != len(w):\n    raise ValueError('The input data points are not valid.')\n\n  if not np.all(np.diff(t) > 0):\n    raise ValueError('The input data points are not valid.')\n\n  return np.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline.\n  if not is_valid(t, w):\n    raise ValueError(\"Invalid input data points for a linear spline.\")\n\n  # Calculate the integral of the data points using the trapezoid rule.\n  integral = 0\n  for i in range(len(t) - 1):\n    integral += (t[i + 1] - t[i]) * (w[i] + w[i + 1]) / 2\n  return integral\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if len(t) < 2 or len(w) < 2:\n    return None\n\n  # Calculate the integral using the trapezoid rule\n  integral = w[0]\n  for i in range(1, len(t)):\n    integral += 2 * w[i]\n  integral += w[-1]\n  integral *= (t[-1] - t[0]) / (2 * (len(t) - 1))\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  if len(t) != len(w):\n    raise ValueError('The arrays t and w must be of the same length.')\n\n  if not np.all(np.diff(t) > 0):\n    raise ValueError('The array t must be sorted in ascending order.')\n\n  return np.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  if len(t) != len(w):\n    raise ValueError(\"The number of data points in t and w are not equal.\")\n  if len(t) < 2:\n    raise ValueError(\"The number of data points is less than 2.\")\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"The x-coordinates of the data points are not sorted in ascending order.\")\n\n  integral = 0.0\n  for i in range(len(t) - 1):\n    integral += (t[i + 1] - t[i]) * (w[i] + w[i + 1]) / 2.0\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data is valid for a linear spline\n  if not check_validity(t, w):\n    return None\n\n  # Perform the integration using the trapezoid rule\n  integral = 0\n  for i in range(len(t) - 1):\n    integral += (t[i + 1] - t[i]) * (w[i] + w[i + 1]) / 2\n\n  return integral\n"}
{"namespace": "linspline.integrate", "completion": "  # Validate input data points\n  if not validate(t, w):\n    return None\n\n  # Calculate integral using the trapezoid rule\n  integral = 0\n  for i in range(len(t) - 1):\n    integral += (w[i + 1] + w[i]) * (t[i + 1] - t[i]) / 2\n\n  return integral\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  if not (len(t) > 2 and len(w) > 2 and len(t) == len(w)):\n    raise ValueError(\"The input data points are not valid for a linear spline\")\n\n  # Compute the integral using the trapezoid rule\n  return np.trapz(w, t)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize a dictionary to store the weighted sum of scores for each ID\n    id_scores = {}\n\n    # Iterate over the IDs, scores, and weights in parallel\n    for ids_list, scores_list, weight in zip(ids, scores, weights):\n        for id, score in zip(ids_list, scores_list):\n            # Update the weighted sum of scores for the ID\n            id_scores[id] = id_scores.get(id, 0) + score * weight\n\n    # Sort the IDs by their weighted sum in descending order\n    sorted_ids = sorted(id_scores.keys(), key=lambda x: id_scores[x], reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    return sorted_ids[:top_k], [id_scores[id] for id in sorted_ids[:top_k]]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Calculate the weighted sum of scores for each ID\n    weighted_scores = {id: sum(score * weight for score, weight in zip(scores, weights)) for id, scores in zip(ids, scores)}\n\n    # Normalize the scores\n    max_score = max(weighted_scores.values())\n    min_score = min(weighted_scores.values())\n    normalized_scores = {id: (score - min_score) / (max_score - min_score) for id, score in weighted_scores.items()}\n\n    # Get the top K IDs based on the normalized scores\n    top_ids = [id for id, _ in sorted(normalized_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]]\n    top_scores = [normalized_scores[id] for id in top_ids]\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Calculate the weighted sum of scores for each ID\n    weighted_sums = []\n    for i in range(len(ids)):\n        weighted_sums.extend([score * weights[i] for score in scores[i]])\n\n    # Normalize the scores\n    normalized_scores = []\n    total_weight = sum(weights)\n    for score in weighted_sums:\n        normalized_scores.append(score / total_weight)\n\n    # Get the top K IDs and their corresponding scores\n    top_ids = []\n    top_scores = []\n    for i in range(top_k):\n        max_index = normalized_scores.index(max(normalized_scores))\n        top_ids.append(ids[max_index // len(scores[max_index % len(scores)])][max_index % len(scores[max_index % len(scores)])])\n        top_scores.append(normalized_scores[max_index])\n        normalized_scores[max_index] = -1\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Calculate the weighted sum of scores for each ID\n    weighted_sums = [sum(score * weight for score, weight in zip(scores[i], weights)) for i in range(len(ids))]\n\n    # Normalize the weighted sums\n    total_sum = sum(weighted_sums)\n    normalized_sums = [weighted_sum / total_sum for weighted_sum in weighted_sums]\n\n    # Get the top K IDs based on the weighted sums\n    top_ids = [ids[i][j] for i in range(len(ids)) for j in range(len(ids[i])) if normalized_sums[i] >= normalized_sums[i+1]][:top_k]\n    top_scores = [normalized_sums[i] for i in range(len(ids)) for j in range(len(ids[i])) if normalized_sums[i] >= normalized_sums[i+1]][:top_k]\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Calculate the weighted sum of scores for each ID\n    weighted_sums = []\n    for id_list, score_list, weight in zip(ids, scores, weights):\n        weighted_sum = sum(score * weight for score in score_list)\n        weighted_sums.append(weighted_sum)\n\n    # Normalize the weighted sums\n    normalized_weighted_sums = normalize_scores(weighted_sums)\n\n    # Get the top K IDs based on the normalized weighted sums\n    top_k_ids = [id_list[i] for i in np.argsort(normalized_weighted_sums)[::-1][:top_k]]\n    top_k_scores = [normalized_weighted_sums[i] for i in np.argsort(normalized_weighted_sums)[::-1][:top_k]]\n\n    return top_k_ids, top_k_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize an empty list to store the weighted scores for each ID\n    weighted_scores = []\n\n    # Iterate over the IDs, scores, and weights tuples simultaneously\n    for id_list, score_list, weight in zip(ids, scores, weights):\n        # Iterate over the IDs and scores in each list\n        for id, score in zip(id_list, score_list):\n            # Calculate the weighted score for the ID\n            weighted_score = score * weight\n            # Add the weighted score to the list of weighted scores\n            weighted_scores.append((id, weighted_score))\n\n    # Sort the weighted scores in descending order\n    weighted_scores.sort(key=lambda x: x[1], reverse=True)\n\n    # Get the top K weighted scores and their corresponding IDs\n    top_k_scores = weighted_scores[:top_k]\n    top_k_ids = [id for id, _ in top_k_scores]\n    top_k_scores = [score for _, score in top_k_scores]\n\n    # Return the top K IDs and their corresponding scores\n    return top_k_ids, top_k_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize an empty list to store the weighted sums of IDs\n    weighted_sums = []\n\n    # Iterate over each set of IDs and corresponding scores\n    for i in range(len(ids)):\n        # Calculate the weighted sum of the scores for each ID\n        weighted_sum = sum([score * weights[i] for score in scores[i]])\n        # Append the weighted sum to the list of weighted sums\n        weighted_sums.append(weighted_sum)\n\n    # Sort the IDs and their corresponding weighted sums in descending order based on the weighted sum\n    sorted_ids = [x for _, x in sorted(zip(weighted_sums, ids), reverse=True)]\n    sorted_weighted_sums = sorted(weighted_sums, reverse=True)\n\n    # Return the top K IDs and their corresponding weighted sums\n    return sorted_ids[:top_k], sorted_weighted_sums[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Create a dictionary to store the weighted sum of scores for each ID\n    id_score_dict = {}\n\n    # Iterate over the IDs, scores, and weights in parallel\n    for ids_list, scores_list, weight in zip(ids, scores, weights):\n        for id, score in zip(ids_list, scores_list):\n            # If the ID is not in the dictionary, initialize its score to 0\n            if id not in id_score_dict:\n                id_score_dict[id] = 0\n            # Add the weighted score to the existing score for the ID\n            id_score_dict[id] += score * weight\n\n    # Sort the IDs by their weighted sum in descending order\n    sorted_ids = sorted(id_score_dict, key=id_score_dict.get, reverse=True)\n\n    # Return the top K IDs and their corresponding weighted sums\n    return sorted_ids[:top_k], [id_score_dict[id] for id in sorted_ids[:top_k]]\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize an empty list to store the weighted sum of scores for each ID\n    weighted_sum_scores = []\n\n    # Iterate through each set of IDs and their corresponding scores\n    for id_list, score_list in zip(ids, scores):\n\n        # Initialize a variable to store the weighted sum of scores for the current ID\n        weighted_sum = 0\n\n        # Iterate through each ID and its corresponding score\n        for id, score in zip(id_list, score_list):\n\n            # Add the weighted score to the weighted sum\n            weighted_sum += score * weights[id]\n\n        # Add the weighted sum to the list of weighted sum scores\n        weighted_sum_scores.append(weighted_sum)\n\n    # Sort the IDs based on their weighted sum scores in descending order\n    sorted_ids = [x for _, x in sorted(zip(weighted_sum_scores, ids), reverse=True)]\n    sorted_scores = [x for _, x in sorted(zip(weighted_sum_scores, scores), reverse=True)]\n\n    # Return the top K IDs and their corresponding scores\n    return sorted_ids[:top_k], sorted_scores[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Create a dictionary to store the weighted sum of scores for each ID\n    id_score_dict = {}\n\n    # Iterate over the IDs, scores, and weights tuples\n    for id_list, score_list, weight in zip(ids, scores, weights):\n        # Iterate over the IDs and scores in the current category or group\n        for id, score in zip(id_list, score_list):\n            # If the ID is not already in the dictionary, add it with a score of 0\n            if id not in id_score_dict:\n                id_score_dict[id] = 0\n            # Add the weighted score to the ID's current score\n            id_score_dict[id] += score * weight\n\n    # Sort the IDs by their weighted sum in descending order\n    sorted_ids = sorted(id_score_dict.keys(), key=lambda x: id_score_dict[x], reverse=True)\n\n    # Return the top K IDs and their corresponding weighted sums\n    return sorted_ids[:top_k], [id_score_dict[id] for id in sorted_ids[:top_k]]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Calculate the weighted sum of scores for each ID\n    weighted_sums = []\n    for i in range(len(ids)):\n        weighted_sum = sum([scores[i][j] * weights[i] for j in range(len(scores[i]))])\n        weighted_sums.append(weighted_sum)\n\n    # Normalize the weighted sums\n    max_weighted_sum = max(weighted_sums)\n    min_weighted_sum = min(weighted_sums)\n    normalized_weighted_sums = [(weighted_sum - min_weighted_sum) / (max_weighted_sum - min_weighted_sum) for weighted_sum in weighted_sums]\n\n    # Get the top K IDs based on the normalized weighted sums\n    top_ids = []\n    top_scores = []\n    for i in range(len(ids)):\n        top_ids.extend(ids[i])\n        top_scores.extend([normalized_weighted_sums[i]] * len(ids[i]))\n\n    top_ids, top_scores = zip(*sorted(zip(top_ids, top_scores), key=lambda x: x[1], reverse=True))\n    top_ids = list(top_ids)[:top_k]\n    top_scores = list(top_scores)[:top_k]\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize an empty list to store the weighted sum of scores for each ID\n    weighted_sums = []\n\n    # Iterate through the IDs and scores tuples\n    for id_list, score_list in zip(ids, scores):\n        # Initialize a variable to store the weighted sum of scores for the current ID\n        weighted_sum = 0\n\n        # Iterate through the IDs and scores for the current category or group\n        for id, score in zip(id_list, score_list):\n            # Calculate the weighted sum of scores for the current ID\n            weighted_sum += score * weights[id]\n\n        # Append the weighted sum of scores for the current ID to the list\n        weighted_sums.append(weighted_sum)\n\n    # Sort the IDs by their weighted sum in descending order\n    ids_sorted = [x for _, x in sorted(zip(weighted_sums, ids), reverse=True)]\n\n    # Sort the weighted sums in descending order\n    weighted_sums_sorted = sorted(weighted_sums, reverse=True)\n\n    # Return the top K IDs and their corresponding weighted sums\n    return ids_sorted[:top_k], weighted_sums_sorted[:top_k]\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize an empty dictionary to store the weighted sum of scores for each ID\n    id_scores = {}\n\n    # Loop through the IDs, scores, and weights tuples in parallel\n    for ids_list, scores_list, weight in zip(ids, scores, weights):\n\n        # Loop through the IDs and scores in each list\n        for id, score in zip(ids_list, scores_list):\n\n            # If the ID is not already in the dictionary, add it with a score of 0\n            if id not in id_scores:\n                id_scores[id] = 0\n\n            # Add the weighted score to the existing score for the ID\n            id_scores[id] += score * weight\n\n    # Normalize the scores by dividing each score by the sum of all scores\n    total_score = sum(id_scores.values())\n    for id in id_scores:\n        id_scores[id] /= total_score\n\n    # Sort the IDs by their weighted sum in descending order\n    sorted_ids = sorted(id_scores, key=id_scores.get, reverse=True)\n\n    # Return the top K IDs and their corresponding weighted sums\n    return sorted_ids[:top_k], [id_scores[id] for id in sorted_ids[:top_k]]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize empty lists to store the weighted scores and IDs\n    weighted_scores = []\n    weighted_ids = []\n\n    # Loop through each set of IDs and scores\n    for id_list, score_list, weight in zip(ids, scores, weights):\n\n        # Loop through each ID and its corresponding score\n        for id, score in zip(id_list, score_list):\n\n            # Calculate the weighted score for this ID\n            weighted_score = score * weight\n\n            # Append the weighted score and ID to their respective lists\n            weighted_scores.append(weighted_score)\n            weighted_ids.append(id)\n\n    # Sort the IDs and scores in descending order based on their weighted scores\n    sorted_indices = np.argsort(weighted_scores)[::-1]\n    sorted_ids = [weighted_ids[i] for i in sorted_indices]\n    sorted_scores = [weighted_scores[i] for i in sorted_indices]\n\n    # Return the top K IDs and their corresponding scores\n    return sorted_ids[:top_k], sorted_scores[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize empty lists to store the IDs and weighted sums\n    ids_list = []\n    weighted_sums = []\n\n    # Iterate over the IDs and scores tuples\n    for ids_group, scores_group in zip(ids, scores):\n        # Iterate over the IDs in the current group\n        for id, score in zip(ids_group, scores_group):\n            # If the ID is already in the list, add the score to the existing weighted sum\n            if id in ids_list:\n                weighted_sums[ids_list.index(id)] += score\n            # Otherwise, add the ID and its score to the lists\n            else:\n                ids_list.append(id)\n                weighted_sums.append(score)\n\n    # Normalize the weighted sums\n    total_sum = sum(weighted_sums)\n    weighted_sums = [score / total_sum for score in weighted_sums]\n\n    # Sort the IDs and weighted sums by their weighted sum in descending order\n    sorted_indices = sorted(\n        range(len(weighted_sums)), key=lambda i: weighted_sums[i], reverse=True)\n    sorted_ids = [ids_list[i] for i in sorted_indices]\n    sorted_weighted_sums = [weighted_sums[i] for i in sorted_indices]\n\n    # Return the top K IDs and their corresponding weighted sums\n    return sorted_ids[:top_k], sorted_weighted_sums[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize variables\n    ids_list = []\n    scores_list = []\n\n    # Loop through each category or group of IDs and their corresponding scores\n    for i in range(len(ids)):\n        # Loop through each ID and its corresponding score in the current category or group\n        for j in range(len(ids[i])):\n            # Append the ID and its corresponding score to the respective lists\n            ids_list.append(ids[i][j])\n            scores_list.append(scores[i][j] * weights[i])\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_scores = {ids_list[i]: scores_list[i] for i in range(len(ids_list))}\n\n    # Sort the IDs by their weighted sum in descending order\n    sorted_ids = sorted(weighted_scores, key=weighted_scores.get, reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    return sorted_ids[:top_k], [weighted_scores[id] for id in sorted_ids[:top_k]]\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate through each category or group\n    for i in range(len(ids)):\n        # Initialize an empty dictionary to store the scores for each ID\n        scores_dict = {}\n\n        # Iterate through each ID in the current category or group\n        for j in range(len(ids[i])):\n            # Calculate the weighted score for the current ID\n            weighted_score = scores[i][j] * weights[i]\n\n            # Add the weighted score to the dictionary for the current ID\n            scores_dict[ids[i][j]] = weighted_score\n\n        # Sort the scores in descending order\n        sorted_scores = sorted(scores_dict.items(), key=lambda x: x[1], reverse=True)\n\n        # Append the top K scores to the results list\n        results.extend(sorted_scores[:top_k])\n\n    # Sort the results list by the weighted sum of the scores in descending order\n    results = sorted(results, key=lambda x: x[1], reverse=True)\n\n    # Return the top K IDs and their weighted sums\n    return [x[0] for x in results[:top_k]], [x[1] for x in results[:top_k]]\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize an empty dictionary to store the weighted sum of scores for each ID\n    weighted_scores = {}\n\n    # Iterate through the IDs and scores in parallel\n    for id_list, score_list in zip(ids, scores):\n        # Iterate through each ID and score in the current category or group\n        for id, score in zip(id_list, score_list):\n            # If the ID is not in the dictionary, add it with a weighted score of 0\n            if id not in weighted_scores:\n                weighted_scores[id] = 0\n            # Add the weighted score of the current ID to the existing value in the dictionary\n            weighted_scores[id] += score * weights[i]\n\n    # Sort the IDs in the dictionary by their weighted sum in descending order\n    sorted_ids = sorted(weighted_scores.keys(), key=lambda x: weighted_scores[x], reverse=True)\n\n    # Return the top K IDs and their corresponding weighted sums\n    return sorted_ids[:top_k], [weighted_scores[id] for id in sorted_ids[:top_k]]\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize an empty list to store the weighted sum of scores for each ID\n    weighted_scores = []\n\n    # Loop through each set of IDs and corresponding scores in the 'ids' and 'scores' tuples\n    for ids_list, scores_list in zip(ids, scores):\n\n        # Initialize a variable to store the weighted sum of scores for the current IDs set\n        weighted_sum = 0\n\n        # Loop through each ID and its corresponding score in the current IDs set\n        for id, score in zip(ids_list, scores_list):\n\n            # Calculate the weighted sum of scores for the current ID\n            weighted_sum += score * weights[id]\n\n        # Append the weighted sum of scores for the current IDs set to the list of weighted sums\n        weighted_scores.append(weighted_sum)\n\n    # Get the indices of the top K IDs based on their weighted sum\n    top_indices = np.argsort(weighted_scores)[::-1][:top_k]\n\n    # Initialize empty lists to store the top K IDs and their corresponding weighted sums\n    top_ids = []\n    top_scores = []\n\n    # Loop through the indices of the top K IDs\n    for index in top_indices:\n\n        # Get the IDs set corresponding to the current index\n        ids_list = ids[index]\n\n        # Get the scores of the IDs set corresponding to the current index\n        scores_list = scores[index]\n\n        # Loop through each ID and its corresponding score in the current IDs set\n        for id, score in zip(ids_list, scores_list):\n\n            # Append the ID and its corresponding weighted sum to the lists of top IDs and scores\n            top_ids.append(id)\n            top_scores.append(score * weights[id])\n\n    # Return the top K IDs and their corresponding weighted sums as a tuple\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize variables\n    id_list = []\n    score_list = []\n\n    # Iterate through each category or group of IDs\n    for i in range(len(ids)):\n\n        # Initialize variables for the current category or group\n        id_cat = []\n        score_cat = []\n\n        # Iterate through each ID in the current category or group\n        for j in range(len(ids[i])):\n\n            # Calculate the weighted score for the current ID\n            score = scores[i][j] * weights[i]\n\n            # Add the ID and its weighted score to the corresponding lists\n            id_cat.append(ids[i][j])\n            score_cat.append(score)\n\n        # Add the current category or group's IDs and scores to the overall lists\n        id_list.append(id_cat)\n        score_list.append(score_cat)\n\n    # Initialize variables for the final output\n    final_ids = []\n    final_scores = []\n\n    # Iterate through the top K IDs from each category or group\n    for i in range(top_k):\n\n        # Initialize variables for the current top ID\n        max_score = 0\n        max_id = None\n\n        # Iterate through each category or group\n        for j in range(len(id_list)):\n\n            # Check if the current ID has a higher score than the current maximum\n            if score_list[j][i] > max_score:\n\n                # Update the maximum score and ID\n                max_score = score_list[j][i]\n                max_id = id_list[j][i]\n\n        # Add the current top ID to the final lists\n        final_ids.append(max_id)\n        final_scores.append(max_score)\n\n    # Return the final top K IDs and scores\n    return final_ids, final_scores\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of the function at the mean\n  jac = jacobian(fn, mean)\n\n  # Compute the linearized function at the mean\n  fn_mean = fn(mean)\n\n  # Compute the linearized covariance\n  fn_cov = jac @ cov @ jac.transpose(-2, -1)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of the function at the mean\n  jacobian = tf.linalg.jacobian(fn, mean)\n\n  # Compute the linearized function at the mean\n  fn_mean = fn(mean)\n\n  # Compute the linearized covariances\n  fn_cov = tf.matmul(jacobian, tf.matmul(cov, tf.transpose(jacobian, perm=[0, 2, 1])))\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  fn_cov = cov\n\n  # Transform the covariances\n  fn_cov = fn_cov\n\n  return fn_mean, fn_cov\n"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean.\n  fn_mean = fn(mean)\n\n  # Compute the Jacobian of the function.\n  jacobian = tf.linalg.LinearOperatorJacobian(fn)(mean)\n\n  # Transform the covariances using the Jacobian.\n  fn_cov = jacobian @ cov @ tf.transpose(jacobian, perm=[0, 2, 1])\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Calculate the Jacobian of the function at the mean\n  jacobian = tf.gradients(fn, mean)[0]\n\n  # Linearize the function around the mean\n  fn_mean = fn(mean)\n\n  # Calculate the covariance of the linearized function\n  fn_cov = tf.matmul(jacobian, tf.matmul(cov, tf.transpose(jacobian)))\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of the function at the mean\n  jac = tf.linalg.jacobian(fn, mean)\n\n  # Linearize the function around the mean\n  fn_mean = fn(mean)\n\n  # Transform the covariances using the Jacobian\n  fn_cov = tf.matmul(jac, cov, transpose_a=True)\n  fn_cov = tf.matmul(fn_cov, jac)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean.\n  fn_mean = fn(mean)\n\n  # Compute the Jacobian of the function with respect to the mean.\n  jac = tf.stack(tf.vector_jacobian(fn, mean), axis=-1)\n\n  # Transform the covariances using the Jacobian.\n  fn_cov = tf.einsum(\"...ij,...jk->...ik\", jac, cov)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  fn_cov = cov\n\n  # Return the linearized function\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of the function at the mean.\n  J = jacobian(fn, mean)\n\n  # Compute the linearized function around the mean.\n  fn_mean = fn(mean)\n\n  # Compute the linearized covariance using the Jacobian.\n  fn_cov = J @ cov @ J.transpose(-2, -1)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean.\n  fn_mean = fn(mean)\n\n  # Compute the Jacobian of the function at the mean.\n  jacobian = tf.stack([tf.stack([tf.math.gradients(fn, mean[..., i])[0][..., i] for i in range(mean.shape[-1])], axis=-1) for _ in range(mean.shape[-1])], axis=-2)\n\n  # Compute the covariance of the linearized function.\n  fn_cov = tf.einsum('...ij,...jk,...lk->...il', jacobian, cov, jacobian)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  fn_jac = jacobian(fn, mean)\n\n  # Transform the covariances\n  fn_cov = fn_jac @ cov @ fn_jac.transpose(-1, -2)\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of the function at the mean\n  jac = tf.stack(tf.vector_jacobian(fn, mean), axis=-1)\n\n  # Compute the linearization of the function around the mean\n  fn_mean = fn(mean)\n\n  # Compute the linearization of the covariances using the Jacobian\n  fn_cov = tf.einsum('...i,...ij,...j->...i', jac, cov, jac)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  fn_cov = cov\n\n  # Transform the covariances\n  if fn_mean.shape[-1] < fn_cov.shape[-1]:\n    fn_cov = tf.linalg.matmul(fn_cov, fn_cov, transpose_b=True)\n  else:\n    fn_cov = tf.linalg.matmul(fn_cov, fn_cov, transpose_a=True)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  linearized_mean = fn(mean)\n\n  # Compute the Jacobian of the function with respect to the mean\n  jacobian = tf.stack([tf.gradients(fn(mean + tf.eye(tf.shape(mean)[-1])[0])[i], mean)[0] for i in range(tf.shape(mean)[-1])], axis=-1)\n\n  # Compute the linearized covariances\n  linearized_cov = tf.matmul(jacobian, tf.matmul(cov, tf.transpose(jacobian, perm=[0, 2, 1])))\n\n  return linearized_mean, linearized_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Get the shape of the mean and covariance tensors\n  shape_mean = mean.shape\n  shape_cov = cov.shape\n\n  # Flatten the mean and covariance tensors\n  mean = tf.reshape(mean, [-1, shape_mean[-1]])\n  cov = tf.reshape(cov, [-1, shape_cov[-2], shape_cov[-1]])\n\n  # Get the number of samples\n  num_samples = tf.shape(mean)[0]\n\n  # Apply the function to the mean\n  fn_mean = fn(mean)\n\n  # Compute the Jacobian of the function\n  with tf.GradientTape() as tape:\n    tape.watch(mean)\n    fn_mean = fn(mean)\n  jac = tape.batch_jacobian(fn_mean, mean)\n\n  # Apply the Jacobian to the covariance\n  fn_cov = tf.linalg.matmul(jac, cov)\n  fn_cov = tf.linalg.matmul(fn_cov, jac, transpose_a=True)\n\n  # Reshape the transformed mean and covariance tensors\n  fn_mean = tf.reshape(fn_mean, shape_mean)\n  fn_cov = tf.reshape(fn_cov, [num_samples, shape_cov[-2], shape_cov[-2]])\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Get the dimensions of the mean and covariance\n  dim = mean.shape[-1]\n  dim_cov = cov.shape[-1]\n\n  # Get the Jacobian of the function\n  J = tf.transpose(tf.vectorized_map(lambda x: tf.gradients(fn(x), x)[0], mean), perm=[0, 2, 1])\n\n  # Linearize the function around the mean\n  fn_mean = fn(mean)\n\n  # Transform the covariances using the Jacobian\n  fn_cov = tf.linalg.matmul(tf.linalg.matmul(J, cov, transpose_a=True), J)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Get the shape of the input tensor\n  shape = mean.shape\n\n  # Reshape the input tensor to a 2D tensor\n  mean = mean.reshape(-1, shape[-1])\n  cov = cov.reshape(-1, shape[-1], shape[-1])\n\n  # Apply the function to the mean\n  fn_mean = fn(mean)\n\n  # Linearize the function around the mean\n  fn_cov = cov - tf.einsum('ijk,ikl->ijl', fn(mean), fn(mean))\n\n  # Reshape the output tensors to the original shape\n  fn_mean = fn_mean.reshape(shape)\n  fn_cov = fn_cov.reshape(shape + shape[-1:])\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Get the shape of the input mean and covariance tensors\n  shape = mean.shape\n\n  # Flatten the mean and covariance tensors\n  mean = mean.reshape(-1, shape[-1])\n  cov = cov.reshape(-1, shape[-1], shape[-1])\n\n  # Apply the function to the mean tensor\n  fn_mean = fn(mean)\n\n  # Compute the Jacobian of the function\n  J = tf.stack([tf.gradients(fn(mean[i, :]), mean[i, :])[0] for i in range(mean.shape[0])])\n\n  # Compute the transformed covariance tensor\n  fn_cov = tf.einsum('...ij,...jk,...lk->...il', J, cov, J)\n\n  # Reshape the transformed mean and covariance tensors to the original shape\n  fn_mean = fn_mean.reshape(shape)\n  fn_cov = fn_cov.reshape(shape[:-1] + (shape[-1], shape[-1]))\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Get the shape of the mean and covariance tensors\n  mean_shape = mean.shape\n  cov_shape = cov.shape\n\n  # Reshape the mean and covariance tensors to be 2D\n  mean = tf.reshape(mean, (-1, mean_shape[-1]))\n  cov = tf.reshape(cov, (-1, cov_shape[-2], cov_shape[-1]))\n\n  # Get the number of points\n  num_points = mean.shape[0]\n\n  # Compute the Jacobian of the function around the mean\n  jacobian = tf.stack([tf.gradients(fn(mean[i]), mean[i])[0] for i in range(num_points)], axis=0)\n\n  # Apply the linearization technique to the covariance tensor\n  fn_cov = tf.einsum('...ij,...jk,...kl->...il', jacobian, cov, jacobian)\n\n  # Apply the function to the mean tensor\n  fn_mean = fn(mean)\n\n  # Reshape the mean and covariance tensors back to their original shapes\n  fn_mean = tf.reshape(fn_mean, mean_shape)\n  fn_cov = tf.reshape(fn_cov, cov_shape)\n\n  # Return the transformed mean and covariance tensors\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Get the dimensions of the mean and covariance tensors.\n  mean_shape = mean.shape\n  cov_shape = cov.shape\n\n  # Reshape the mean and covariance tensors to have a single axis for the dimension of the mean and covariance.\n  mean = tf.reshape(mean, [-1, mean_shape[-1]])\n  cov = tf.reshape(cov, [-1, cov_shape[-2], cov_shape[-1]])\n\n  # Linearize the function around the mean.\n  fn_mean = fn(mean)\n\n  # Compute the Jacobian of the function with respect to the mean.\n  jac = tf.vector_jacobian(fn, mean)\n\n  # Transform the covariances using the Jacobian.\n  fn_cov = tf.einsum('ijk,jkl->ijl', jac, cov)\n\n  # Reshape the transformed mean and covariance tensors back to their original shapes.\n  fn_mean = tf.reshape(fn_mean, mean_shape)\n  fn_cov = tf.reshape(fn_cov, cov_shape)\n\n  return fn_mean, fn_cov"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[:len(i)//2], i[len(i)//2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if x[i].size > 1:\n            yield [x[i][:x[i].size // 2], x[i][x[i].size // 2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[:i.size // 2], i[i.size // 2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for array in x:\n        if array.size > 1:\n            yield [array[:array.size // 2], array[array.size // 2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield np.split(i, 2)\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if len(x[i]) > 1:\n            yield [x[i][:len(x[i]) // 2], x[i][len(x[i]) // 2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if len(x[i]) > 1:\n            yield [x[i][:len(x[i]) // 2], x[i][len(x[i]) // 2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[:len(i) // 2], i[len(i) // 2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[:len(i) // 2], i[len(i) // 2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[:i.size // 2], i[i.size // 2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[0:len(i) // 2], i[len(i) // 2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[0:int(len(i) / 2)], i[int(len(i) / 2):]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[:len(i) // 2], i[len(i) // 2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[:len(i)//2], i[len(i)//2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[:len(i) // 2], i[len(i) // 2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.shape[0] > 1:\n            yield [i[:i.shape[0] // 2], i[i.shape[0] // 2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if len(x[i]) > 1:\n            yield [x[i][:len(x[i])//2], x[i][len(x[i])//2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[:len(i) // 2], i[len(i) // 2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[:len(i) // 2], i[len(i) // 2:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[0:int(len(i) / 2)], i[int(len(i) / 2):]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The given matrix is not square.\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The input matrix is not square.\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"x must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The given matrix is not square.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The given matrix is not a square matrix.\")\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The input matrix is not square.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The given matrix is not square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The given matrix is not square.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(f\"The given matrix is not square. It has dimensions {x.shape}\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"x is not a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square\")\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(f\"Expected a square matrix, but got a {x.shape[0]}x{x.shape[1]} matrix.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2 ** torch.arange(min_deg, max_deg + 1)\n  x = x[..., None] * scales\n  x = torch.cat([torch.sin(x[..., 0::2]), torch.cos(x[..., 1::2])], dim=-1)\n  if append_identity:\n    x = torch.cat([x, torch.ones_like(x[..., :1])], dim=-1)\n  return x"}
{"namespace": "coord.pos_enc", "completion": "  scales = np.power(2, np.arange(min_deg, max_deg))\n  x_enc = x[:, None] * scales[None, :]\n  x_enc = np.concatenate([np.sin(x_enc), np.cos(x_enc)], axis=-1)\n  if append_identity:\n    x_enc = np.concatenate([x[:, None], x_enc], axis=-1)\n  return x_enc"}
{"namespace": "coord.pos_enc", "completion": "  scales = np.logspace(min_deg, max_deg, max_deg - min_deg + 1, base=2)\n  x = x[..., None] * scales[None]\n  x = np.concatenate([np.sin(x), np.cos(x)], axis=-1)\n  if append_identity:\n    x = np.concatenate([x, np.ones_like(x[..., :1])], axis=-1)\n  return x"}
{"namespace": "coord.pos_enc", "completion": "  scales = np.power(2, np.arange(min_deg, max_deg + 1))\n  x_scaled = x[..., np.newaxis] * scales[np.newaxis, ...]\n  x_enc = np.concatenate([np.sin(x_scaled), np.cos(x_scaled)], axis=-1)\n  if append_identity:\n    x_enc = np.concatenate([x[..., np.newaxis], x_enc], axis=-1)\n  return x_enc"}
{"namespace": "coord.pos_enc", "completion": "  scales = np.power(2, np.arange(min_deg, max_deg + 1))\n  x = x[..., None] * scales[None, :]\n  x = np.concatenate((np.sin(x), np.cos(x)), axis=-1)\n  if append_identity:\n    x = np.concatenate((x, np.ones_like(x[..., :1])), axis=-1)\n  return x"}
{"namespace": "coord.pos_enc", "completion": "  scales = np.power(2, np.arange(min_deg, max_deg + 1))\n  x = x[..., None] * scales[None, :]\n  x = np.concatenate([np.sin(x[..., ::2]), np.cos(x[..., 1::2])], axis=-1)\n  if append_identity:\n    x = np.concatenate([x, np.ones_like(x[..., :1])], axis=-1)\n  return x\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = np.power(2, np.arange(min_deg, max_deg+1))\n  x_enc = x[..., None] * scales[None, :]\n  x_enc = np.concatenate([np.sin(x_enc), np.cos(x_enc)], axis=-1)\n  if append_identity:\n    x_enc = np.concatenate([x[..., None], x_enc], axis=-1)\n  return x_enc\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.**torch.arange(min_deg, max_deg+1)\n  x_scaled = x[..., None] * scales\n  x_enc = torch.cat([torch.sin(x_scaled), torch.cos(x_scaled)], dim=-1)\n  if append_identity:\n    x_enc = torch.cat([x, x_enc], dim=-1)\n  return x_enc"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2 ** torch.linspace(min_deg, max_deg, max_deg - min_deg + 1)\n  x_in = x[:, None] * scales[None, :]\n  x_out = torch.cat([torch.sin(x_in), torch.cos(x_in)], dim=-1)\n  if append_identity:\n    x_out = torch.cat([x[:, None], x_out], dim=-1)\n  return x_out\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = np.power(2, np.arange(min_deg, max_deg+1))\n  x = x[..., None] * scales[None, :]\n  x = np.concatenate([np.sin(x[..., ::2]), np.cos(x[..., 1::2])], axis=-1)\n  if append_identity:\n    x = np.concatenate([x, np.broadcast_to(np.array([[0, 1]]), x.shape[:-1] + (2,))], axis=-1)\n  return x"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2 ** np.arange(min_deg, max_deg + 1)\n  x_enc = np.concatenate([x[:, None] / scale for scale in scales], axis=1)\n  x_enc = np.concatenate([np.sin(x_enc[:, ::2]), np.cos(x_enc[:, 1::2])], axis=1)\n  if append_identity:\n    x_enc = np.concatenate([x[:, None], x_enc], axis=1)\n  return x_enc\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = np.power(2, np.arange(min_deg, max_deg))\n  x = x[..., None] * scales\n  x = np.concatenate([np.sin(x), np.cos(x)], axis=-1)\n  if append_identity:\n    x = np.concatenate([x, np.ones_like(x[..., :1])], axis=-1)\n  return x\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = np.power(2, np.arange(min_deg, max_deg+1))\n  scaled_x = x[:, None] * scales[None, :]\n  x_enc = np.concatenate([np.sin(scaled_x), np.cos(scaled_x)], axis=1)\n  if append_identity:\n    x_enc = np.concatenate([x[:, None], x_enc], axis=1)\n  return x_enc\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = np.power(2, np.linspace(min_deg, max_deg, max_deg - min_deg + 1))\n  x_enc = np.concatenate([x[:, None] * scale for scale in scales], axis=1)\n  x_enc = np.concatenate([np.sin(x_enc[:, :4]), np.cos(x_enc[:, 4:])], axis=1)\n  if append_identity:\n    x_enc = np.concatenate([x[:, None], x_enc], axis=1)\n  return x_enc\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = (2 ** np.arange(min_deg, max_deg + 1, dtype=x.dtype))\n  x_scaled = x[..., None] * scales[None, :]\n  x_enc = np.concatenate([np.sin(x_scaled), np.cos(x_scaled)], axis=-1)\n  if append_identity:\n    x_enc = np.concatenate([x[..., None], x_enc], axis=-1)\n  return x_enc\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = [2**i for i in range(min_deg, max_deg + 1)]\n  x_scaled = [x * scale for scale in scales]\n  x_pos_encoded = [torch.sin(x_scaled[i]) if i % 2 == 0 else torch.cos(x_scaled[i]) for i in range(len(x_scaled))]\n  if append_identity:\n    x_pos_encoded = [torch.cat([x, x_pos_encoded[i]], dim=-1) for i in range(len(x_pos_encoded))]\n  return x_pos_encoded\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.**np.arange(min_deg, max_deg+1)\n  x_enc = np.concatenate([x[:, None] * 2.**i for i in range(min_deg, max_deg+1)], axis=1)\n  x_enc = np.concatenate([np.sin(x_enc[:, i]), np.cos(x_enc[:, i])], axis=1)\n  if append_identity:\n    x_enc = np.concatenate([x[:, None], x_enc], axis=1)\n  return x_enc\n"}
{"namespace": "coord.pos_enc", "completion": "  if x.shape[-1] == 3:\n    x = x[..., None]\n\n  scales = np.array([2**i for i in range(min_deg, max_deg)])\n  x = x[..., None] * scales\n\n  x = np.concatenate([np.sin(x[..., [2 * i]]), np.cos(x[..., [2 * i]])], axis=-1)\n\n  if append_identity:\n    x = np.concatenate([x, x[..., :1]], axis=-1)\n\n  return x\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = np.power(2, np.arange(min_deg, max_deg + 1))\n  x = np.expand_dims(x, -1) * scales\n  x = np.concatenate([np.sin(x), np.cos(x)], axis=-1)\n  if append_identity:\n    x = np.concatenate([x, np.ones_like(x[..., :1])], axis=-1)\n  return x\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = np.power(2, np.arange(min_deg, max_deg + 1))\n  x_scaled = x[..., None] * scales[None, :]\n  x_enc = np.concatenate([np.sin(x_scaled), np.cos(x_scaled)], axis=-1)\n  if append_identity:\n    x_enc = np.concatenate([x[..., None], x_enc], axis=-1)\n  return x_enc\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate_shapes(cls, values):\n        list1 = values.get(field1)\n        list2 = values.get(field2)\n        if len(list1) != len(list2):\n            raise ValueError(f\"{field1} and {field2} must have the same length\")\n        for arr1, arr2 in zip(list1, list2):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{field1} and {field2} must have arrays with the same shape\")\n        return values\n\n    return validate_shapes"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def _are_all_shapes_equal(cls, values):\n        list1 = values.get(field1)\n        list2 = values.get(field2)\n\n        if len(list1) != len(list2):\n            raise ValueError(\n                f\"The lists of numpy arrays {field1} and {field2} must have the same length.\"\n            )\n\n        for arr1, arr2 in zip(list1, list2):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The arrays {arr1} and {arr2} must have the same shape.\"\n                )\n\n        return values\n\n    return _are_all_shapes_equal"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate_shapes(cls, values):\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"The length of {field1} and {field2} must be equal.\"\n            )\n        for i, (arr1, arr2) in enumerate(zip(values[field1], values[field2])):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The shape of the {i}th array in {field1} and {field2} must be equal.\"\n                )\n        return values\n\n    return validate_shapes"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def are_all_shapes_equal_validator(cls, values):\n        field1_values = values[field1]\n        field2_values = values[field2]\n\n        if len(field1_values) != len(field2_values):\n            raise ValueError(\n                f\"The lists of arrays in fields '{field1}' and '{field2}' must have the same length.\"\n            )\n\n        for i, (arr1, arr2) in enumerate(zip(field1_values, field2_values)):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The arrays in fields '{field1}' and '{field2}' at index {i} must have the same shape.\"\n                )\n\n        return values\n\n    return are_all_shapes_equal_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate_shape(cls, values):\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"The lists of arrays in fields {field1} and {field2} must have the same length.\"\n            )\n\n        for i, (arr1, arr2) in enumerate(zip(values[field1], values[field2])):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The shapes of arrays in fields {field1} and {field2} at index {i} must be equal.\"\n                )\n\n        return values\n\n    return validate_shape"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def _are_all_shapes_equal(cls, values: Dict) -> Dict:\n        list1 = values.get(field1)\n        list2 = values.get(field2)\n        if len(list1) != len(list2):\n            raise ValueError(\n                f\"The lists of arrays in fields '{field1}' and '{field2}' must have the same length.\"\n            )\n        for arr1, arr2 in zip(list1, list2):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The arrays in fields '{field1}' and '{field2}' must have the same shape.\"\n                )\n        return values\n\n    return _are_all_shapes_equal\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def are_all_shapes_equal(cls, values):\n        # Get the two lists of numpy arrays from the dictionary\n        list1 = values[field1]\n        list2 = values[field2]\n\n        # Check if the lists have the same length\n        if len(list1) != len(list2):\n            raise ValueError(\n                f\"The lists of numpy arrays in fields '{field1}' and '{field2}' must have the same length.\"\n            )\n\n        # Check if each corresponding pair of arrays has the same shape\n        for i, (arr1, arr2) in enumerate(zip(list1, list2)):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The arrays in field '{field1}' at index {i} and field '{field2}' at index {i} must have the same shape.\"\n                )\n\n        # If all checks pass, return the validated values\n        return values\n\n    return are_all_shapes_equal"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate_shape(cls, values):\n        for i, arr1 in enumerate(values[field1]):\n            arr2 = values[field2][i]\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The shapes of the arrays in the fields {field1} and {field2} do not match at index {i}.\"\n                )\n        return values\n\n    return validate_shape\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def _are_all_shapes_equal(cls, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"The lists of numpy arrays in fields '{field1}' and '{field2}' must have the same length.\")\n\n        for arr1, arr2 in zip(values[field1], values[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"The shapes of arrays in fields '{field1}' and '{field2}' must be the same.\")\n\n        return values\n\n    return _are_all_shapes_equal\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def _are_all_shapes_equal(cls, values: Dict) -> Dict:\n        \"\"\"\n        This function checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: Type[Any]. The type of the class that is being validated.\n        :param values: Dict. A dictionary of values to be validated. It contains the two fields to be compared.\n        :return: Dict. The validated values if the check passes.\n\n        \"\"\"\n        field1_values = values.get(field1)\n        field2_values = values.get(field2)\n        if len(field1_values) != len(field2_values):\n            raise ValueError(\n                f\"The lists of arrays in fields {field1} and {field2} must have the same length.\"\n            )\n        for i, (arr1, arr2) in enumerate(zip(field1_values, field2_values)):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The arrays in fields {field1} and {field2} at index {i} must have the same shape.\"\n                )\n        return values\n\n    return _are_all_shapes_equal"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def are_all_shapes_equal_validator(cls, values):\n        \"\"\"\n        This function is a validator for a Pydantic model. It checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: The class type of the Pydantic model being validated.\n        :param values: dict. A dictionary of values to be validated. The keys are the field names, and the values are the corresponding numpy arrays.\n        :return: dict. The validated values if the check passes.\n\n        \"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"The lists of numpy arrays in fields {field1} and {field2} have different lengths.\"\n            )\n        for arr1, arr2 in zip(values[field1], values[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The arrays in fields {field1} and {field2} have different shapes: {arr1.shape} vs {arr2.shape}.\"\n                )\n        return values\n\n    return are_all_shapes_equal_validator\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def _are_all_shapes_equal(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        This function performs the actual shape equality check.\n\n        Input-Output Arguments\n        :param cls: The class type to which the validator is being applied.\n        :param values: Dict[str, Any]. A dictionary of values to be validated. The keys are the field names and the values are the corresponding numpy arrays.\n        :return: Dict[str, Any]. The validated values if the check passes.\n\n        \"\"\"\n\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"The arrays in the fields '{field1}' and '{field2}' must have the same length.\"\n            )\n\n        for i, (arr1, arr2) in enumerate(zip(values[field1], values[field2])):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The arrays in the fields '{field1}' and '{field2}' at index {i} must have the same shape.\"\n                )\n\n        return values\n\n    return _are_all_shapes_equal"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate_shape(cls, values: Dict) -> Dict:\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"The length of {field1} ({len(values[field1])}) and {field2} ({len(values[field2])}) must be equal.\"\n            )\n        for i, (arr1, arr2) in enumerate(zip(values[field1], values[field2])):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The shape of {field1}[{i}] ({arr1.shape}) and {field2}[{i}] ({arr2.shape}) must be equal.\"\n                )\n        return values\n\n    return validate_shape\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def _are_all_shapes_equal(cls, values: Dict) -> Dict:\n        \"\"\"\n        This function is a validator function that checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: class type. The class type that the validator function is being called on.\n        :param values: Dict. A dictionary of values containing the two fields to be compared. The keys of this dictionary are the field names specified in the `field1` and `field2` arguments. The values are the corresponding lists of numpy arrays.\n        :return: Dict. A dictionary of validated values. If the check for shape equality between arrays in the specified fields passes, the function returns the original `values` dictionary. If the check fails, the function raises a `ValueError` with a descriptive error message.\n\n        \"\"\"\n        list1 = values.get(field1)\n        list2 = values.get(field2)\n        if len(list1) != len(list2):\n            raise ValueError(\n                f\"The lists of arrays in fields '{field1}' and '{field2}' must have the same length.\"\n            )\n        for i, (arr1, arr2) in enumerate(zip(list1, list2)):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The arrays in fields '{field1}' and '{field2}' at index {i} must have the same shape.\"\n                )\n        return values\n\n    return _are_all_shapes_equal"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def are_shapes_equal(cls, values: Dict) -> Dict:\n        \"\"\"\n        This function is a Pydantic model validator that checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: Class. The class type to which the validator is being applied.\n        :param values: Dict. A dictionary of values to be validated. It contains the two lists of numpy arrays to be compared.\n        :return: Dict. The validated values if the check passes.\n\n        \"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"The number of elements in the {field1} and {field2} fields must be the same\"\n            )\n        for i, (arr1, arr2) in enumerate(zip(values[field1], values[field2])):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The shape of the {i}th element of the {field1} and {field2} fields must be the same\"\n                )\n        return values\n\n    return are_shapes_equal\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def are_shapes_equal(cls, values):\n        \"\"\"\n        This function is a Pydantic validator that checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: class. The class type that the validator is being applied to.\n        :param values: dict. A dictionary of values containing the two lists of numpy arrays to be compared.\n        :return: dict. The validated values if the check passes.\n\n        \"\"\"\n\n        # Check if the two fields exist in the values dictionary\n        if field1 not in values or field2 not in values:\n            raise ValueError(\n                f\"Both {field1} and {field2} must be provided as fields in the input dictionary\"\n            )\n\n        # Check if the two fields are lists\n        if not isinstance(values[field1], list) or not isinstance(values[field2], list):\n            raise ValueError(\n                f\"Both {field1} and {field2} must be lists of numpy arrays\"\n            )\n\n        # Check if the two lists have the same length\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"The two lists of numpy arrays ({field1} and {field2}) must have the same length\"\n            )\n\n        # Check if each corresponding pair of arrays within the two lists have the same shape\n        for i, (arr1, arr2) in enumerate(zip(values[field1], values[field2])):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The arrays at index {i} ({arr1.shape} and {arr2.shape}) in the two lists of numpy arrays ({field1} and {field2}) must have the same shape\"\n                )\n\n        # Return the validated values\n        return values\n\n    return are_shapes_equal"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def validate_shapes(cls, values: Dict) -> Dict:\n        \"\"\"\n        This function validates the shapes of two lists of numpy arrays.\n\n        Input-Output Arguments\n        :param cls: class type. The class type of the Pydantic model.\n        :param values: Dict. A dictionary of field names and their corresponding values.\n        :return: Dict. A dictionary of validated values.\n\n        \"\"\"\n        field_1 = values[field1]\n        field_2 = values[field2]\n        if len(field_1) != len(field_2):\n            raise ValueError(\n                f\"The two lists of numpy arrays have different lengths: {len(field_1)} and {len(field_2)}\"\n            )\n        for arr1, arr2 in zip(field_1, field_2):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The arrays in the two lists have different shapes: {arr1.shape} and {arr2.shape}\"\n                )\n        return values\n\n    return validate_shapes\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def _are_all_shapes_equal(cls, values: dict) -> dict:\n        \"\"\"\n        This function is a Pydantic model validator that checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: class type. The class type of the Pydantic model that is being validated.\n        :param values: dict. A dictionary containing the values of the fields to be validated. The keys of this dictionary are the field names and the values are the corresponding numpy arrays.\n        :return: dict. A dictionary containing the validated values of the fields. If the check for shape equality passes, the dictionary contains the same values as the input dictionary. If the check fails, an exception is raised.\n\n        \"\"\"\n\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"The two lists of numpy arrays must have the same length. {field1} has length {len(values[field1])} and {field2} has length {len(values[field2])}.\"\n            )\n\n        for i, (array1, array2) in enumerate(zip(values[field1], values[field2])):\n            if array1.shape != array2.shape:\n                raise ValueError(\n                    f\"The shapes of the two corresponding arrays at index {i} do not match. {field1} has shape {array1.shape} and {field2} has shape {array2.shape}.\"\n                )\n\n        return values\n\n    return _are_all_shapes_equal\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def are_all_shapes_equal_validator(cls, values):\n        \"\"\"\n        This function is a Pydantic model validator that checks if two lists of numpy arrays have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: ClassType. The class type of the model being validated.\n        :param values: Dict. A dictionary of values to be validated. The keys should be the names of the fields to be checked.\n        :return: Dict. The validated values if the check passes.\n\n        \"\"\"\n        # Get the values of the two fields to be compared\n        list1 = values[field1]\n        list2 = values[field2]\n\n        # Check if the two lists have the same length\n        if len(list1) != len(list2):\n            raise ValueError(\n                f\"The two lists of numpy arrays ({field1} and {field2}) must have the same length.\"\n            )\n\n        # Check if each pair of corresponding arrays within the two lists has the same shape\n        for i in range(len(list1)):\n            if list1[i].shape != list2[i].shape:\n                raise ValueError(\n                    f\"The two lists of numpy arrays ({field1} and {field2}) must have corresponding arrays with the same shape. Array {i} in {field1} has shape {list1[i].shape}, while array {i} in {field2} has shape {list2[i].shape}.\"\n                )\n\n        # If all checks pass, return the validated values\n        return values\n\n    return are_all_shapes_equal_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def are_all_shapes_equal_validator(cls, values: Dict) -> Dict:\n        \"\"\"\n        This function is a validator function for a Pydantic model. It takes a class type and a dictionary of values as input, checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape, and returns the validated values if the check passes.\n\n        Input-Output Arguments\n        :param cls: Class type. The class type of the Pydantic model that will be validated.\n        :param values: Dict. A dictionary of values to be validated. The keys are the field names of the Pydantic model, and the values are the corresponding values to be validated.\n        :return: Dict. The validated values if the check passes. If the check fails, an exception is raised.\n\n        \"\"\"\n\n        field1_values = values.get(field1)\n        field2_values = values.get(field2)\n\n        if len(field1_values) != len(field2_values):\n            raise ValueError(\n                f\"The two lists of arrays have different lengths: {len(field1_values)} and {len(field2_values)}\"\n            )\n\n        for i in range(len(field1_values)):\n            if field1_values[i].shape != field2_values[i].shape:\n                raise ValueError(\n                    f\"The shapes of the arrays at index {i} are not equal: {field1_values[i].shape} and {field2_values[i].shape}\"\n                )\n\n        return values\n\n    return are_all_shapes_equal_validator\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(camera)"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(camera)"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n        self.render(camera)"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # resize the context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # render the Mesh instance using the camera's settings\n        camera.render(self)"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n        # render the Mesh instance using the camera's settings\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        camera.render(self)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Set the camera's settings for rendering\n        camera.set_for_render()\n\n        # Render the Mesh instance\n        self.render(eglctx)\n\n        # Reset the camera's settings after rendering\n        camera.set_after_render()"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        camera.render()\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Clear the color and depth buffers\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n\n        # Set the viewport to match the camera's width and height\n        gl.glViewport(0, 0, camera.width, camera.height)\n\n        # Set the camera's projection matrix\n        camera.set_projection_matrix()\n\n        # Set the camera's view matrix\n        camera.set_view_matrix()\n\n        # Set the camera's model matrix\n        camera.set_model_matrix()\n\n        # Set the camera's normal matrix\n        camera.set_normal_matrix()\n\n        # Set the camera's mvp matrix\n        camera.set_mvp_matrix()\n\n        # Set the camera's mv matrix\n        camera.set_mv_matrix()\n\n        # Set the camera's m matrix\n        camera.set_m_matrix()\n\n        # Set the camera's normal matrix\n        camera.set_normal_matrix()\n\n        # Set the camera's view matrix\n        camera.set_view_matrix()\n\n        # Set the camera's projection matrix\n        camera.set_projection_matrix()\n\n        # Set the camera's mvp matrix\n        camera.set_mvp_matrix()\n\n        # Set the camera's mv matrix\n        camera.set_mv_matrix()\n\n        # Set the camera's m matrix\n        camera.set_m_matrix()\n\n        # Set the camera's normal matrix\n        camera.set_normal_matrix()\n\n        # Set the camera's view matrix\n        camera.set_view_matrix()\n\n        # Set the camera's projection matrix\n        camera.set_projection_matrix()\n\n        # Set the camera's mvp matrix\n        camera.set_mvp_matrix()\n\n        # Set the camera's mv matrix\n       "}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        eglctx.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # resize the context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n        # render the mesh using the camera's settings\n        camera.render(self)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        eglctx.make_current()\n        self.render(camera)\n        eglctx.done_current()\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # resize the context to match the camera\n        eglctx.resize(camera.width, camera.height)\n        # render the mesh using the camera's settings\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        eglctx.make_current()\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        camera.apply_to_opengl()\n        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n        glEnable(GL_DEPTH_TEST)\n        glEnable(GL_CULL_FACE)\n        glCullFace(GL_BACK)\n        glFrontFace(GL_CCW)\n        self.render()\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # resize the context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n        # set the camera settings\n        camera.set_settings()\n        # render the mesh instance\n        self.render()\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object\n    nomic_config = NomicBertConfig()\n\n    # Set the vocab_size attribute\n    nomic_config.vocab_size = bert_config.vocab_size\n\n    # Set the hidden_size attribute\n    nomic_config.hidden_size = bert_config.hidden_size\n\n    # Set the num_hidden_layers attribute\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n\n    # Set the num_attention_heads attribute\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n\n    # Set the intermediate_size attribute\n    nomic_config.intermediate_size = bert_config.intermediate_size\n\n    # Set the hidden_act attribute\n    nomic_config.hidden_act = bert_config.hidden_act\n\n    # Set the hidden_dropout_prob attribute\n    nomic_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n\n    # Set the attention_probs_dropout_prob attribute\n    nomic_config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n\n    # Set the max_position_embeddings attribute\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n\n    # Set the type_vocab_size attribute\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n\n    # Set the initializer_range attribute\n    nomic_config.initializer_range = bert_config.initializer_range\n\n    # Set the layer_norm_eps attribute\n    nomic_config.layer_norm_eps = bert_config.layer_norm_eps\n\n    # Set the pad_token_id attribute\n    nomic_config.pad_token_id = bert_config.pad_token_id\n\n    # Set the gradient_checkpointing attribute"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        num_labels=bert_config.num_labels,\n        label_vocab_size=bert_config.label_vocab_size,\n        label_pad_token_id=bert_config.label_pad_token_id,\n        label_initializer_range=bert_config.label_initializer_range,\n        label_layer_norm_eps=bert_config.label_layer_norm_eps,\n        label_hidden_dropout_prob=bert_config.label_hidden_dropout_prob,\n        label_attention_probs_dropout_prob=bert_config.label_attention_probs_dropout_prob,\n        label_max_position_embeddings=bert_config.label_max_position_embeddings,\n        label_type_vocab_size=bert_config.label_type_vocab_size,\n        label_intermediate_size=bert_config.label_intermediate_size,\n        label_num_attention_heads"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Initialize a new NomicBertConfig object with the same values as the original BertConfig\n    nomic_bert_config = NomicBertConfig()\n    nomic_bert_config.vocab_size = bert_config.vocab_size\n    nomic_bert_config.hidden_size = bert_config.hidden_size\n    nomic_bert_config.num_hidden_layers = bert_config.num_hidden_layers\n    nomic_bert_config.num_attention_heads = bert_config.num_attention_heads\n    nomic_bert_config.intermediate_size = bert_config.intermediate_size\n    nomic_bert_config.hidden_act = bert_config.hidden_act\n    nomic_bert_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    nomic_bert_config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    nomic_bert_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_bert_config.type_vocab_size = bert_config.type_vocab_size\n    nomic_bert_config.initializer_range = bert_config.initializer_range\n    nomic_bert_config.layer_norm_eps = bert_config.layer_norm_eps\n    nomic_bert_config.pooler_fc_size = bert_config.pooler_fc_size\n    nomic_bert_config.pooler_num_attention_heads = bert_config.pooler_num_attention_heads\n    nomic_bert_config.pooler_num_fc_layers = bert_config.pooler_num_fc_layers\n    nomic_bert_config.pooler_fc_dropout = bert_config.pooler_fc_dropout\n    nomic_bert_config.pooler_attention_head_size = bert_config.pooler_attention_head_size\n    nomic"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        num_labels=bert_config.num_labels,\n        num_classes=bert_config.num_classes,\n        num_hidden_layers_per_block=bert_config.num_hidden_layers_per_block,\n        num_attention_heads_per_block=bert_config.num_attention_heads_per_block,\n        num_attention_heads_per_layer=bert_config.num_attention_heads_per_layer,\n        num_attention_heads_per_block=bert_config.num_attention_heads_per_block,\n        num_hidden_layers_per_block=bert_config.num_hidden_layers_per_block,\n        num_hidden_layers_per_layer=bert_config.num_hidden"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Define the new configuration object\n    nomic_config = NomicBertConfig()\n\n    # Set the hidden_size attribute of the new configuration object to the hidden_size attribute of the input BertConfig\n    nomic_config.hidden_size = bert_config.hidden_size\n\n    # Set the num_attention_heads attribute of the new configuration object to the num_attention_heads attribute of the input BertConfig\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n\n    # Set the intermediate_size attribute of the new configuration object to the intermediate_size attribute of the input BertConfig\n    nomic_config.intermediate_size = bert_config.intermediate_size\n\n    # Set the num_hidden_layers attribute of the new configuration object to the num_hidden_layers attribute of the input BertConfig\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n\n    # Set the max_position_embeddings attribute of the new configuration object to the max_position_embeddings attribute of the input BertConfig\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n\n    # Set the type_vocab_size attribute of the new configuration object to the type_vocab_size attribute of the input BertConfig\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n\n    # Set the vocab_size attribute of the new configuration object to the vocab_size attribute of the input BertConfig\n    nomic_config.vocab_size = bert_config.vocab_size\n\n    # Set the hidden_dropout_prob attribute of the new configuration object to the hidden_dropout_prob attribute of the input BertConfig\n    nomic_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n\n    # Set the attention_probs_dropout_prob attribute of the new configuration object to the attention_probs_dropout_prob attribute of the input BertConfig\n    nom"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same values as the original BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        classifier_activation=bert_config.classifier_activation,\n        classifier_activation_beta=bert_config.classifier_activation_beta,\n        classifier_activation_alpha=bert_config.classifier_activation_alpha,\n        classifier_activation_beta_type=bert_config.classifier_activation_beta_type,\n        classifier_activation_alpha_type=bert_config.classifier_activation_alpha_type,\n        classifier_activation_alpha_beta_type=bert_config.classifier_activation_alpha_beta_type,\n        classifier_activation_alpha_beta_type_type=bert_config.classifier_activation_alpha_beta_type_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig()\n    config.vocab_size = bert_config.vocab_size\n    config.hidden_size = bert_config.hidden_size\n    config.num_hidden_layers = bert_config.num_hidden_layers\n    config.num_attention_heads = bert_config.num_attention_heads\n    config.intermediate_size = bert_config.intermediate_size\n    config.hidden_act = bert_config.hidden_act\n    config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    config.max_position_embeddings = bert_config.max_position_embeddings\n    config.type_vocab_size = bert_config.type_vocab_size\n    config.initializer_range = bert_config.initializer_range\n    config.layer_norm_eps = bert_config.layer_norm_eps\n    config.num_labels = bert_config.num_labels\n    config.num_spans = bert_config.num_spans\n    config.num_span_heads = bert_config.num_span_heads\n    config.num_span_layers = bert_config.num_span_layers\n    config.max_span_length = bert_config.max_span_length\n    config.max_span_width = bert_config.max_span_width\n    config.span_dropout_prob = bert_config.span_dropout_prob\n    config.span_attention_probs_dropout_prob = bert_config.span_attention_probs_dropout_prob\n    config.span_initializer_range = bert_config.span_initializer_range\n    config.span_layer_norm_eps = bert_config.span_layer_norm_eps\n    config.span_hidden_act = bert_config.span_hidden_act\n    config.span_hidden_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Initialize a new NomicBertConfig object with the same values as the original BertConfig\n    nomic_config = NomicBertConfig(**bert_config.to_dict())\n\n    # Set the `max_position_embeddings` attribute to the maximum sequence length\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n\n    # Set the `max_seq_length` attribute to the maximum sequence length\n    nomic_config.max_seq_length = bert_config.max_position_embeddings\n\n    # Set the `num_hidden_layers` attribute to the number of hidden layers\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n\n    # Set the `num_attention_heads` attribute to the number of attention heads\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n\n    # Set the `hidden_size` attribute to the hidden size\n    nomic_config.hidden_size = bert_config.hidden_size\n\n    # Set the `intermediate_size` attribute to the intermediate size\n    nomic_config.intermediate_size = bert_config.intermediate_size\n\n    # Set the `hidden_act` attribute to the hidden activation function\n    nomic_config.hidden_act = bert_config.hidden_act\n\n    # Set the `hidden_dropout_prob` attribute to the hidden dropout probability\n    nomic_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n\n    # Set the `attention_probs_dropout_prob` attribute to the attention dropout probability\n    nomic_config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n\n    # Set the `initializer_range` attribute to the initializer range\n    nomic_config.initializer_range = bert_config.initializer_range\n\n    # Set the `layer_norm_eps` attribute to the layer normalization epsilon\n    nomic_config.layer"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same values as the original BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        use_cache=bert_config.use_cache,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        use_relative_positions=bert_config.use_relative_positions,\n        use_relative_segments=bert_config.use_relative_segments,\n        use_relative_bias=bert_config.use_relative_bias,\n        use_relative_bias_table=bert_config.use_relative_bias_table,\n        use_relative_positions_bias_table=bert_config.use_relative_positions_bias_table,\n        use_relative_segments_bias_table=bert_config.use_relative_segments_bias_table,\n        use_relative_positions_key_query=bert_config.use_relative_positions_key_query,\n        use_relative_segments_key_query=bert_config.use_relative_segments_key_query,\n        use_relative_positions_value=bert_config.use_relative_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Initialize a new NomicBertConfig object with the same values as the original BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n    )\n\n    # Set the maximum sequence length to 512\n    nomic_config.max_seq_length = 512\n\n    # Set the maximum number of tokens per segment to 256\n    nomic_config.max_tokens_per_segment = 256\n\n    # Set the maximum number of segments to 32\n    nomic_config.max_num_segments = 32\n\n    # Set the maximum number of tokens per segment to 256\n    nomic_config.max_tokens_per_segment = 256\n\n    # Set the maximum number of segments to 32\n    nomic_config.max_num_segments = 32\n\n    # Set the maximum number of tokens per segment to 256\n    nomic_config.max_tokens_per_segment = 256\n\n    # Set the maximum number of segments to 32\n    nomic_config."}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object\n    nomic_config = NomicBertConfig()\n\n    # Set the hidden size and number of layers\n    nomic_config.hidden_size = bert_config.hidden_size\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n\n    # Set the vocabulary size and embedding size\n    nomic_config.vocab_size = bert_config.vocab_size\n    nomic_config.embedding_size = bert_config.hidden_size\n\n    # Set the number of attention heads\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n\n    # Set the intermediate size\n    nomic_config.intermediate_size = bert_config.intermediate_size\n\n    # Set the maximum position embedding\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n\n    # Set the type vocab size\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n\n    # Set the initializer range\n    nomic_config.initializer_range = bert_config.initializer_range\n\n    # Set the layer norm epsilon\n    nomic_config.layer_norm_eps = bert_config.layer_norm_eps\n\n    # Set the pooler_fc_size\n    nomic_config.pooler_fc_size = bert_config.pooler_fc_size\n\n    # Set the pooler_num_attention_heads\n    nomic_config.pooler_num_attention_heads = bert_config.pooler_num_attention_heads\n\n    # Set the pooler_num_attention_heads\n    nomic_config.pooler_num_attention_heads = bert_config.pooler_num_attention_heads\n\n    # Set the pooler_num_attention_heads\n    nomic_config.pooler_num_attention_heads = bert_config.pool"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same attributes as the original BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n    )\n\n    # Set additional attributes specific to the Nomic model\n    nomic_config.max_seq_length = bert_config.max_position_embeddings\n    nomic_config.max_batch_size = 1\n    nomic_config.max_batch_size_per_gpu = 1\n    nomic_config.max_batch_size_per_gpu_per_tile = 1\n    nomic_config.max_batch_size_per_gpu_per_tile_per_instance = 1\n    nomic_config.max_batch_size_per_gpu_per_tile_per_instance_per_step = 1\n    nomic_config.max_batch_size_per_gpu_per_tile_per_instance_per_step_per_layer = 1\n    nomic_config.max_batch_size_per_gpu_per_tile_per_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig()\n\n    config.vocab_size = bert_config.vocab_size\n    config.hidden_size = bert_config.hidden_size\n    config.num_hidden_layers = bert_config.num_hidden_layers\n    config.num_attention_heads = bert_config.num_attention_heads\n    config.intermediate_size = bert_config.intermediate_size\n    config.hidden_act = bert_config.hidden_act\n    config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    config.max_position_embeddings = bert_config.max_position_embeddings\n    config.type_vocab_size = bert_config.type_vocab_size\n    config.initializer_range = bert_config.initializer_range\n    config.layer_norm_eps = bert_config.layer_norm_eps\n\n    config.max_seq_length = bert_config.max_position_embeddings\n    config.max_segment_length = bert_config.max_position_embeddings\n    config.max_num_segments = bert_config.max_position_embeddings\n    config.max_num_tokens = bert_config.max_position_embeddings\n    config.max_num_tokens_per_segment = bert_config.max_position_embeddings\n    config.max_num_tokens_per_segment_per_token = bert_config.max_position_embeddings\n\n    config.segment_embedding_size = bert_config.hidden_size\n    config.segment_attention_head_size = bert_config.hidden_size // bert_config.num_attention_heads\n    config.segment_attention_head_size_per_token = bert_config.hidden_size // bert_config.num_attention_heads\n\n    config."}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object and initialize it with the settings from the BertConfig\n    nomic_config = NomicBertConfig()\n    nomic_config.vocab_size = bert_config.vocab_size\n    nomic_config.hidden_size = bert_config.hidden_size\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n    nomic_config.intermediate_size = bert_config.intermediate_size\n    nomic_config.hidden_act = bert_config.hidden_act\n    nomic_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    nomic_config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n    nomic_config.initializer_range = bert_config.initializer_range\n    nomic_config.layer_norm_eps = bert_config.layer_norm_eps\n\n    # Additional settings specific to the Nomic model\n    nomic_config.num_labels = 2\n    nomic_config.attention_probs_dropout_prob = 0.1\n    nomic_config.hidden_dropout_prob = 0.1\n    nomic_config.hidden_act = \"gelu\"\n    nomic_config.max_position_embeddings = 512\n    nomic_config.initializer_range = 0.02\n    nomic_config.layer_norm_eps = 1e-12\n\n    return nomic_config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same attributes as the input BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        use_cache=bert_config.use_cache,\n        max_seq_length=bert_config.max_position_embeddings,\n        num_classes=bert_config.num_classes,\n        num_spatial_positions=bert_config.num_spatial_positions,\n        num_temporal_positions=bert_config.num_temporal_positions,\n        num_channels=bert_config.num_channels,\n        num_patches=bert_config.num_patches,\n        num_spatial_channels=bert_config.num_spatial_channels,\n        num_temporal_channels=bert_config.num_temporal_channels,\n        num_spatial_patches=bert_config.num_spatial_patches,\n        num_temporal_patches=bert_config.num_temporal"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object\n    nomic_config = NomicBertConfig(\n        num_hidden_layers=bert_config.num_hidden_layers,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_size=bert_config.hidden_size,\n        num_attention_heads=bert_config.num_attention_heads,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        vocab_size=bert_config.vocab_size,\n        initializer_range=bert_config.initializer_range,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        pad_token_id=bert_config.pad_token_id,\n        bos_token_id=bert_config.bos_token_id,\n        eos_token_id=bert_config.eos_token_id,\n        sep_token_id=bert_config.sep_token_id,\n        cls_token_id=bert_config.cls_token_id,\n        mask_token_id=bert_config.mask_token_id,\n        use_cache=bert_config.use_cache,\n        is_decoder=bert_config.is_decoder,\n        is_encoder_decoder=bert_config.is_encoder_decoder,\n        add_cross_attention=bert_config.add_cross_attention,\n        decoder_start_token_id=bert_config.decoder_start_token_id,\n        eos_token=bert_config.eos_token,\n        bos_token=bert_config.bos_token,\n        sep_token=bert_config.sep_token,\n        pad_token=bert_config.pad_token,\n        cls_token=bert_config.cls_token,\n        mask_token="}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same settings as the original BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        # Add new attributes specific to the Nomic model\n        num_hidden_chunks=bert_config.num_hidden_layers,\n        chunk_size=bert_config.hidden_size // bert_config.num_hidden_layers,\n        num_chunks=bert_config.num_hidden_layers,\n        num_chunk_heads=bert_config.num_attention_heads,\n        chunk_attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        chunk_hidden_act=bert_config.hidden_act,\n        chunk_hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        chunk_initializer_range=bert_config.initializer_range,\n        chunk_layer_norm_eps=bert_config.layer_norm_eps,\n        chunk_intermediate_size=bert"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object and assign the values from the original BertConfig\n    nomic_bert_config = NomicBertConfig()\n    nomic_bert_config.vocab_size = bert_config.vocab_size\n    nomic_bert_config.hidden_size = bert_config.hidden_size\n    nomic_bert_config.num_hidden_layers = bert_config.num_hidden_layers\n    nomic_bert_config.num_attention_heads = bert_config.num_attention_heads\n    nomic_bert_config.intermediate_size = bert_config.intermediate_size\n    nomic_bert_config.hidden_act = bert_config.hidden_act\n    nomic_bert_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    nomic_bert_config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    nomic_bert_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_bert_config.type_vocab_size = bert_config.type_vocab_size\n    nomic_bert_config.initializer_range = bert_config.initializer_range\n    nomic_bert_config.layer_norm_eps = bert_config.layer_norm_eps\n\n    # Additional NomicBertConfig attributes\n    nomic_bert_config.num_types = bert_config.type_vocab_size\n    nomic_bert_config.max_seq_length = bert_config.max_position_embeddings\n    nomic_bert_config.num_classes = bert_config.num_classes\n    nomic_bert_config.num_labels = bert_config.num_labels\n    nomic_bert_config.num_labels_per_type = bert_config.num_labels_per_type\n    nomic_bert_config.num_relations = bert_config.num_relations\n   "}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Define the new configuration object\n    nomic_config = NomicBertConfig()\n\n    # Set the values of the new configuration object\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n    nomic_config.hidden_size = bert_config.hidden_size\n    nomic_config.intermediate_size = bert_config.intermediate_size\n    nomic_config.hidden_act = bert_config.hidden_act\n    nomic_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    nomic_config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n    nomic_config.initializer_range = bert_config.initializer_range\n    nomic_config.layer_norm_eps = bert_config.layer_norm_eps\n    nomic_config.gradient_checkpointing = bert_config.gradient_checkpointing\n    nomic_config.use_cache = bert_config.use_cache\n    nomic_config.vocab_size = bert_config.vocab_size\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n    nomic_config.pad_token_id = bert_config.pad_token_id\n    nomic_config.position_embedding_type = bert_config.position_embedding_type\n    nomic_config.transformers_version = bert_config.transformers_version\n    nomic_config.transformers_version_last_checkpoint = bert_config.transformers_version_last_checkpoint\n\n    # Add additional attributes specific to the Nomic model\n    nomic_config.num_attention_heads_per_layer = bert_config.num_attention_heads"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object\n    nomic_config = NomicBertConfig()\n\n    # Copy the settings from the original BertConfig to the NomicBertConfig\n    for attr, value in vars(bert_config).items():\n        setattr(nomic_config, attr, value)\n\n    # Set the new attributes for the NomicBertConfig\n    nomic_config.num_labels = bert_config.num_labels\n    nomic_config.num_relations = bert_config.num_relations\n    nomic_config.max_seq_length = bert_config.max_position_embeddings\n    nomic_config.hidden_size = bert_config.hidden_size\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n    nomic_config.intermediate_size = bert_config.intermediate_size\n    nomic_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    nomic_config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    nomic_config.initializer_range = bert_config.initializer_range\n    nomic_config.use_relative_positions = True\n    nomic_config.use_token_type = True\n    nomic_config.use_position_embeddings = True\n    nomic_config.use_type_embeddings = True\n    nomic_config.use_layer_norm = True\n    nomic_config.use_bias = True\n    nomic_config.use_gelu = True\n    nomic_config.use_dropout = True\n    nomic_config.use_classifier = True\n    nomic_config.use_pooler = True\n    nomic_config.use_tie_weights = True\n    nomic_config.use_layer_norm_first = True\n    nomic_config.use_bias_g"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            shader_program = self.point_shader_program\n        else:\n            shader_program = self.shader_program\n\n        shader_program.use()\n\n        self.upload_gl_uniforms(camera)\n\n        glBindVertexArray(self.vao)\n\n        if self.render_type == RenderType.POINTS:\n            glDrawArrays(GL_POINTS, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.LINES:\n            if self.ebo is not None:\n                glDrawElements(GL_LINES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_LINES, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.TRIANGLES:\n            if self.ebo is not None:\n                glDrawElements(GL_TRIANGLES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.QUADS:\n            if self.ebo is not None:\n                glDrawElements(GL_QUADS, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_QUADS, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            if self.ebo is not None:\n                glDrawElements(GL_TRIANGLE_STRIP, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLE_STRIP, 0, self."}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.program.use()\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.vertices))\n            gl.glBindVertexArray(0)\n        elif self.render_type == RenderType.LINES:\n            self.program.use()\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.vertices))\n            gl.glBindVertexArray(0)\n        elif self.render_type == RenderType.TRIANGLES:\n            self.program.use()\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.vertices))\n            gl.glBindVertexArray(0)\n        elif self.render_type == RenderType.QUADS:\n            self.program.use()\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_QUADS, 0, len(self.vertices))\n            gl.glBindVertexArray(0)\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            self.program.use()\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.vertices))\n            gl.glBindVertexArray(0)\n        elif self.render_type == RenderType.INDEXED_TRIANGLES:\n            self"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.shader_program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            glDrawArrays(GL_POINTS, 0, self.vertices.shape[0])\n            glBindVertexArray(0)\n        elif self.render_type == RenderType.LINES:\n            self.shader_program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            if self.indexed:\n                glDrawElements(GL_LINES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_LINES, 0, self.vertices.shape[0])\n            glBindVertexArray(0)\n        elif self.render_type == RenderType.TRIANGLES:\n            self.shader_program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            if self.indexed:\n                glDrawElements(GL_TRIANGLES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, self.vertices.shape[0])\n            glBindVertexArray(0)\n        elif self.render_type == RenderType.QUADS:\n            self.shader_program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            if self.indexed:\n                glDrawElements(GL_QUADS, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_QUADS, 0, self.vertices.shape[0])\n            glBindVertexArray(0)\n        elif self.render"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.program\n\n        program.use()\n\n        self.upload_gl_uniforms(camera)\n\n        glBindVertexArray(self.vao)\n\n        if self.render_type == RenderType.POINTS:\n            glDrawArrays(GL_POINTS, 0, self.num_vertices)\n        elif self.render_type == RenderType.LINES:\n            if self.indexed:\n                glDrawElements(GL_LINES, self.num_indices, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_LINES, 0, self.num_vertices)\n        elif self.render_type == RenderType.TRIANGLES:\n            if self.indexed:\n                glDrawElements(GL_TRIANGLES, self.num_indices, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, self.num_vertices)\n        elif self.render_type == RenderType.QUADS:\n            if self.indexed:\n                glDrawElements(GL_QUADS, self.num_indices, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_QUADS, 0, self.num_vertices)\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            if self.indexed:\n                glDrawElements(GL_TRIANGLE_STRIP, self.num_indices, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLE_STRIP, 0, self.num_vertices)\n\n        glBindVertexArray(0)"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n            glBindVertexArray(0)\n        elif self.render_type == RenderType.LINES:\n            self.program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            if self.indexed:\n                glDrawElements(GL_LINES, len(self.faces) * 2, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_LINES, 0, len(self.vertices))\n            glBindVertexArray(0)\n        elif self.render_type == RenderType.TRIANGLES:\n            self.program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            if self.indexed:\n                glDrawElements(GL_TRIANGLES, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n            glBindVertexArray(0)\n        elif self.render_type == RenderType.QUADS:\n            self.program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            if self.indexed:\n                glDrawElements(GL_QUADS, len(self.faces) * 4, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_QUADS, 0, len(self.vertices))\n            glBindVertexArray(0)\n        elif self.render_type == RenderType.TRIANGLE_STRIP"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.point_program.use()\n        else:\n            self.program.use()\n\n        self.upload_gl_uniforms(camera)\n\n        glBindVertexArray(self.vao)\n\n        if self.render_type == RenderType.POINTS:\n            glDrawArrays(GL_POINTS, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.LINES:\n            if self.ebo is not None:\n                glDrawElements(GL_LINES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_LINES, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.TRIANGLES:\n            if self.ebo is not None:\n                glDrawElements(GL_TRIANGLES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.QUADS:\n            if self.ebo is not None:\n                glDrawElements(GL_QUADS, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_QUADS, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            if self.ebo is not None:\n                glDrawElements(GL_TRIANGLE_STRIP, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLE_STRIP, 0, self.vertices.shape[0])\n\n        glBindVertexArray(0)\n\n   "}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINT:\n            self.shader_program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            glDrawArrays(GL_POINTS, 0, self.vertices.shape[0])\n            glBindVertexArray(0)\n        elif self.render_type == RenderType.LINE:\n            self.shader_program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            if self.ebo is not None:\n                glDrawElements(GL_LINES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_LINES, 0, self.vertices.shape[0])\n            glBindVertexArray(0)\n        elif self.render_type == RenderType.TRIANGLE:\n            self.shader_program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            if self.ebo is not None:\n                glDrawElements(GL_TRIANGLES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, self.vertices.shape[0])\n            glBindVertexArray(0)\n        elif self.render_type == RenderType.QUAD:\n            self.shader_program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            if self.ebo is not None:\n                glDrawElements(GL_QUADS, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_QUADS, 0, self.vertices.shape[0])\n            glBindVertexArray(0)"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINT:\n            self.shader_program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            glDrawArrays(GL_POINTS, 0, self.vertices.shape[0])\n            glBindVertexArray(0)\n        elif self.render_type == RenderType.LINE:\n            self.shader_program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            if self.indexed:\n                glDrawElements(GL_LINES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_LINES, 0, self.vertices.shape[0])\n            glBindVertexArray(0)\n        elif self.render_type == RenderType.TRIANGLE:\n            self.shader_program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            if self.indexed:\n                glDrawElements(GL_TRIANGLES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, self.vertices.shape[0])\n            glBindVertexArray(0)\n        elif self.render_type == RenderType.QUAD:\n            self.shader_program.use()\n            self.upload_gl_uniforms(camera)\n            glBindVertexArray(self.vao)\n            if self.indexed:\n                glDrawElements(GL_QUADS, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_QUADS, 0, self.vertices.shape[0])\n            glBindVertexArray(0)\n        elif self.render_type =="}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINT:\n            program = self.point_program\n        else:\n            program = self.program\n\n        program.use()\n\n        self.upload_gl_uniforms(camera)\n\n        glBindVertexArray(self.vao)\n\n        if self.render_type == RenderType.POINT:\n            glDrawArrays(GL_POINTS, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.LINE:\n            if self.ebo is None:\n                glDrawArrays(GL_LINES, 0, self.vertices.shape[0])\n            else:\n                glDrawElements(GL_LINES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.TRIANGLE:\n            if self.ebo is None:\n                glDrawArrays(GL_TRIANGLES, 0, self.vertices.shape[0])\n            else:\n                glDrawElements(GL_TRIANGLES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.QUAD:\n            if self.ebo is None:\n                glDrawArrays(GL_QUADS, 0, self.vertices.shape[0])\n            else:\n                glDrawElements(GL_QUADS, self.faces.shape[0], GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            if self.ebo is None:\n                glDrawArrays(GL_TRIANGLE_STRIP, 0, self.vertices.shape[0])\n            else:\n                glDrawElements(GL_TRIANGLE_STRIP, self.faces.shape[0], GL_UNSIGNED_INT, None)\n\n        glBindVertexArray(0)\n\n        program.un"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            shader_program = self.point_shader_program\n        else:\n            shader_program = self.shader_program\n\n        shader_program.use()\n        self.upload_gl_uniforms(camera)\n\n        glBindVertexArray(self.vao)\n\n        if self.render_type == RenderType.POINTS:\n            glDrawArrays(GL_POINTS, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.LINES:\n            if self.ebo is not None:\n                glDrawElements(GL_LINES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_LINES, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.TRIANGLES:\n            if self.ebo is not None:\n                glDrawElements(GL_TRIANGLES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.QUADS:\n            if self.ebo is not None:\n                glDrawElements(GL_QUADS, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_QUADS, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            if self.ebo is not None:\n                glDrawElements(GL_TRIANGLE_STRIP, self.faces.shape[0], GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLE_STRIP, 0, self.vert"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            program = self.point_shader_program\n        else:\n            program = self.shader_program\n\n        program.use()\n\n        self.upload_gl_uniforms(camera)\n\n        glBindVertexArray(self.vao)\n\n        if self.render_type == RenderType.POINTS:\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n        elif self.render_type == RenderType.LINES:\n            if self.indexed:\n                glDrawElements(GL_LINES, len(self.faces), GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_LINES, 0, len(self.vertices))\n        elif self.render_type == RenderType.TRIANGLES:\n            if self.indexed:\n                glDrawElements(GL_TRIANGLES, len(self.faces), GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n        elif self.render_type == RenderType.QUADS:\n            if self.indexed:\n                glDrawElements(GL_QUADS, len(self.faces), GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_QUADS, 0, len(self.vertices))\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            if self.indexed:\n                glDrawElements(GL_TRIANGLE_STRIP, len(self.faces), GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLE_STRIP, 0, len(self.vertices))\n\n        glBindVertexArray(0)"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == \"points\":\n            shader_program = self.point_shader_program\n        else:\n            shader_program = self.shader_program\n\n        shader_program.use()\n\n        self.upload_gl_uniforms(camera)\n\n        glBindVertexArray(self.vao)\n\n        if self.render_type == \"points\":\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n        elif self.render_type == \"lines\":\n            glDrawArrays(GL_LINES, 0, len(self.vertices))\n        elif self.render_type == \"triangles\":\n            glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n        elif self.render_type == \"quads\":\n            glDrawArrays(GL_QUADS, 0, len(self.vertices))\n        elif self.render_type == \"triangle_strips\":\n            glDrawElements(GL_TRIANGLE_STRIP, len(self.faces), GL_UNSIGNED_INT, None)\n\n        glBindVertexArray(0)\n\n        shader_program.unuse()"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.program.use()\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_POINTS, 0, self.vertices.shape[0])\n            gl.glBindVertexArray(0)\n        else:\n            if self.render_type == RenderType.LINES:\n                self.program_lines.use()\n            else:\n                self.program.use()\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            if self.faces is not None:\n                gl.glDrawElements(self.render_type.value, self.faces.shape[0], gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(self.render_type.value, 0, self.vertices.shape[0])\n            gl.glBindVertexArray(0)"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.program\n\n        program.use()\n\n        self.upload_gl_uniforms(camera)\n\n        glBindVertexArray(self.vao)\n\n        if self.render_type == RenderType.LINES:\n            glDrawArrays(GL_LINES, 0, len(self.vertices))\n        elif self.render_type == RenderType.TRIANGLES:\n            glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n        elif self.render_type == RenderType.QUADS:\n            glDrawArrays(GL_QUADS, 0, len(self.vertices))\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            glDrawArrays(GL_TRIANGLE_STRIP, 0, len(self.vertices))\n        elif self.render_type == RenderType.INDEXED:\n            if self.indexed_type == IndexedType.TRIANGLES:\n                glDrawElements(GL_TRIANGLES, len(self.faces), GL_UNSIGNED_INT, None)\n            elif self.indexed_type == IndexedType.LINES:\n                glDrawElements(GL_LINES, len(self.faces), GL_UNSIGNED_INT, None)\n            elif self.indexed_type == IndexedType.QUADS:\n                glDrawElements(GL_QUADS, len(self.faces), GL_UNSIGNED_INT, None)\n            elif self.indexed_type == IndexedType.TRIANGLE_STRIP:\n                glDrawElements(GL_TRIANGLE_STRIP, len(self.faces), GL_UNSIGNED_INT, None)\n\n        glBindVertexArray(0)\n\n        program.unuse()"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.shader_program.use()\n\n            self.upload_gl_uniforms(camera)\n\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_POINTS, 0, self.vertices.shape[0])\n            gl.glBindVertexArray(0)\n\n            self.shader_program.unuse()\n\n        elif self.render_type == RenderType.LINES:\n            self.shader_program.use()\n\n            self.upload_gl_uniforms(camera)\n\n            gl.glBindVertexArray(self.vao)\n            if self.ebo is not None:\n                gl.glDrawElements(gl.GL_LINES, self.ebo.shape[0], gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_LINES, 0, self.vertices.shape[0])\n            gl.glBindVertexArray(0)\n\n            self.shader_program.unuse()\n\n        elif self.render_type == RenderType.TRIANGLES:\n            self.shader_program.use()\n\n            self.upload_gl_uniforms(camera)\n\n            gl.glBindVertexArray(self.vao)\n            if self.ebo is not None:\n                gl.glDrawElements(gl.GL_TRIANGLES, self.ebo.shape[0], gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, self.vertices.shape[0])\n            gl.glBindVertexArray(0)\n\n            self.shader_program.unuse()\n\n        elif self.render_type == RenderType.QUADS:\n            self.shader_program.use()\n\n            self.upload_gl_uniforms(camera)"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            shader_program = self.point_shader_program\n        else:\n            shader_program = self.shader_program\n\n        shader_program.use()\n\n        self.upload_gl_uniforms(camera)\n\n        glBindVertexArray(self.vao)\n\n        if self.render_type == RenderType.POINTS:\n            glDrawArrays(GL_POINTS, 0, len(self.vertices))\n        elif self.render_type == RenderType.LINES:\n            glDrawArrays(GL_LINES, 0, len(self.vertices))\n        elif self.render_type == RenderType.TRIANGLES:\n            glDrawArrays(GL_TRIANGLES, 0, len(self.vertices))\n        elif self.render_type == RenderType.QUADS:\n            glDrawArrays(GL_QUADS, 0, len(self.vertices))\n        elif self.render_type == RenderType.TRIANGLE_STRIPS:\n            glDrawArrays(GL_TRIANGLE_STRIP, 0, len(self.vertices))\n        elif self.render_type == RenderType.INDEXED_TRIANGLES:\n            glDrawElements(GL_TRIANGLES, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.INDEXED_QUADS:\n            glDrawElements(GL_QUADS, len(self.faces) * 4, GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.INDEXED_TRIANGLE_STRIPS:\n            glDrawElements(GL_TRIANGLE_STRIP, len(self.faces) * 3, GL_UNSIGNED_INT, None)\n\n        glBindVertexArray(0)\n\n        shader_program.stop_using()"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINT:\n            program = self.point_program\n        else:\n            program = self.program\n\n        program.use()\n        program.upload_gl_uniforms(camera)\n        glBindVertexArray(self.vao)\n\n        if self.render_type == RenderType.LINE:\n            glDrawArrays(GL_LINES, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.TRIANGLE:\n            glDrawArrays(GL_TRIANGLES, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.QUAD:\n            glDrawArrays(GL_QUADS, 0, self.vertices.shape[0])\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            glDrawArrays(GL_TRIANGLE_STRIP, 0, self.vertices.shape[0])\n        else:\n            if self.faces is None:\n                glDrawArrays(GL_POINTS, 0, self.vertices.shape[0])\n            else:\n                glDrawElements(GL_TRIANGLES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n\n        glBindVertexArray(0)"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.program\n\n        program.use()\n\n        self.upload_gl_uniforms(camera)\n\n        glBindVertexArray(self.vao)\n\n        if self.render_type == RenderType.LINES:\n            glDrawArrays(GL_LINES, 0, self.num_vertices)\n        elif self.render_type == RenderType.TRIANGLES:\n            if self.has_index_buffer:\n                glDrawElements(GL_TRIANGLES, self.num_faces * 3, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLES, 0, self.num_vertices)\n        elif self.render_type == RenderType.QUADS:\n            if self.has_index_buffer:\n                glDrawElements(GL_QUADS, self.num_faces * 4, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_QUADS, 0, self.num_vertices)\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            if self.has_index_buffer:\n                glDrawElements(GL_TRIANGLE_STRIP, self.num_faces * 3, GL_UNSIGNED_INT, None)\n            else:\n                glDrawArrays(GL_TRIANGLE_STRIP, 0, self.num_vertices)\n\n        glBindVertexArray(0)"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            shader_program = self.shader_programs[RenderType.POINTS]\n            shader_program.use()\n        else:\n            shader_program = self.shader_programs[RenderType.TRIANGLES]\n            shader_program.use()\n\n        self.upload_gl_uniforms(shader_program, camera)\n\n        glBindVertexArray(self.vao)\n        if self.render_type == RenderType.POINTS:\n            glDrawArrays(GL_POINTS, 0, self.num_vertices)\n        elif self.render_type == RenderType.LINES:\n            glDrawArrays(GL_LINES, 0, self.num_vertices)\n        elif self.render_type == RenderType.TRIANGLES:\n            glDrawArrays(GL_TRIANGLES, 0, self.num_vertices)\n        elif self.render_type == RenderType.QUADS:\n            glDrawArrays(GL_QUADS, 0, self.num_vertices)\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            glDrawArrays(GL_TRIANGLE_STRIP, 0, self.num_vertices)\n        elif self.render_type == RenderType.INDEXED_TRIANGLES:\n            glDrawElements(GL_TRIANGLES, self.num_indices, GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.INDEXED_QUADS:\n            glDrawElements(GL_QUADS, self.num_indices, GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.INDEXED_TRIANGLE_STRIP:\n            glDrawElements(GL_TRIANGLE_STRIP, self.num_indices, GL_UNSIGNED_INT, None)\n        gl"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            program = self.programs[Program.POINTS]\n        else:\n            program = self.programs[Program.GENERAL]\n\n        program.use()\n\n        self.upload_gl_uniforms(camera)\n\n        glBindVertexArray(self.vao)\n\n        if self.render_type == RenderType.LINES:\n            glDrawElements(GL_LINES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.TRIANGLES:\n            glDrawElements(GL_TRIANGLES, self.faces.shape[0], GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.QUADS:\n            glDrawElements(GL_QUADS, self.faces.shape[0], GL_UNSIGNED_INT, None)\n        elif self.render_type == RenderType.TRIANGLE_STRIP:\n            glDrawElements(GL_TRIANGLE_STRIP, self.faces.shape[0], GL_UNSIGNED_INT, None)\n\n        glBindVertexArray(0)\n\n        program.unuse()\n\n        return\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.numpy()\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        self.texture.upload(ptr, x, y, w, h)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not isinstance(ptr, np.ndarray):\n            ptr = ptr.numpy()\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        ptr = ptr.astype(np.uint8)\n        self.texture.upload(ptr, x, y, w, h)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.numpy()\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_FLOAT, ptr)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.numpy()\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        self.texture.upload(ptr, x, y, w, h)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        self.texture.upload(ptr, x, y, w, h)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not isinstance(ptr, np.ndarray):\n            ptr = ptr.numpy()\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        ptr = ptr.astype(np.uint8)\n        self.tex.update(ptr, x, y, w, h)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if self.texture is None:\n            self.texture = gl.GLTexture(self.W, self.H)\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.numpy()\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        self.texture.upload(ptr, x, y, w, h)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not isinstance(ptr, np.ndarray):\n            ptr = ptr.numpy()\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        ptr = ptr.astype(np.uint8)\n        ptr = ptr.reshape((h, w, 3))\n        ptr = ptr[::-1, :, :]\n\n        self.texture.set_data(ptr, x, y, w, h)\n\n        return"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # Convert the input to a numpy array if it is a PyTorch tensor\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        # Check if the input is a numpy array\n        if not isinstance(ptr, np.ndarray):\n            raise TypeError(\"Input must be a numpy array or PyTorch tensor\")\n\n        # Check if the input is a 3D array\n        if len(ptr.shape) != 3:\n            raise ValueError(\"Input must be a 3D array\")\n\n        # Check if the input is a 32-bit floating point array\n        if ptr.dtype != np.float32:\n            raise ValueError(\"Input must be a 32-bit floating point array\")\n\n        # Check if the input is a RGB array\n        if ptr.shape[2] != 3:\n            raise ValueError(\"Input must be a RGB array\")\n\n        # Check if the input is a RGB array\n        if ptr.shape[2] != 3:\n            raise ValueError(\"Input must be a RGB array\")\n\n        # Check if the input is a RGB array\n        if ptr.shape[2] != 3:\n            raise ValueError(\"Input must be a RGB array\")\n\n        # Check if the input is a RGB array\n        if ptr.shape[2] != 3:\n            raise ValueError(\"Input must be a RGB array\")\n\n        # Check if the input is a RGB array\n        if ptr.shape[2] != 3:\n            raise ValueError(\"Input must be a RGB array\")\n\n        # Check if the input is a RGB array\n        if ptr.shape[2] != 3:\n            raise ValueError(\"Input must be a RGB array\")\n\n        # Check if the input is a RGB array\n        if ptr.shape[2] != 3:\n            raise ValueError(\"Input must be a RGB array\")\n\n        # Check if the input is a RGB array\n        if ptr.shape[2] != 3:\n            raise ValueError(\"Input must be a RGB array"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.numpy()\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        ptr = ptr.astype(np.uint8)\n        ptr = ptr.reshape(-1, 4)\n        ptr = ptr.transpose(1, 0)\n        ptr = ptr.flatten()\n        glBindTexture(GL_TEXTURE_2D, self.tex)\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGBA, GL_UNSIGNED_BYTE, ptr)\n        glBindTexture(GL_TEXTURE_2D, 0)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # Convert the input data to a numpy array\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # Adjust the dimensions if necessary\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        # Upload the texture data to the GPU\n        self.tex.upload(ptr, x, y, w, h)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if type(ptr) is torch.Tensor:\n            ptr = ptr.cpu().numpy()\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        ptr = ptr.astype(np.float32)\n        ptr = ptr.reshape(h, w, 4)\n        ptr = ptr.transpose(1, 0, 2)\n        ptr = ptr.flatten()\n        ptr = (ptr * 255).astype(np.uint8)\n        self.texture.set_data(ptr, x, y, w, h)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if type(ptr) == torch.Tensor:\n            ptr = ptr.detach().cpu().numpy()\n        if type(ptr) == np.ndarray:\n            ptr = ptr.astype(np.uint8)\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        self.tex.upload(ptr, x, y, w, h)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not isinstance(ptr, np.ndarray):\n            ptr = ptr.numpy()\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        self.texture.set_from_array(ptr, x, y, w, h)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not isinstance(ptr, np.ndarray):\n            ptr = ptr.cpu().numpy()\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        ptr = ptr.astype(np.float32)\n        if ptr.ndim == 3:\n            ptr = ptr.transpose(2, 0, 1)\n        ptr = ptr.flatten()\n        ptr = ptr.tobytes()\n        glPixelStorei(GL_UNPACK_ALIGNMENT, 1)\n        glBindTexture(GL_TEXTURE_2D, self.texture)\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGB, GL_FLOAT, ptr)\n        glBindTexture(GL_TEXTURE_2D, 0)\n    "}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        ptr = np.ascontiguousarray(ptr)\n        if ptr.shape[0] == 3:\n            ptr = ptr.transpose((1, 2, 0))\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        self.texture.upload(ptr, x, y, w, h)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # check if the input is a PyTorch tensor\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.numpy()\n\n        # check if the input is a numpy array\n        if isinstance(ptr, np.ndarray):\n            # check if the input array is a single channel image\n            if len(ptr.shape) == 2:\n                # if the input array is a single channel image, convert it to RGB\n                ptr = np.stack([ptr, ptr, ptr], axis=2)\n\n            # check if the input array is a 3-channel image\n            if len(ptr.shape) == 3 and ptr.shape[2] == 3:\n                # if the input array is a 3-channel image, convert it to RGBA\n                ptr = np.concatenate([ptr, np.ones((ptr.shape[0], ptr.shape[1], 1), dtype=np.uint8) * 255], axis=2)\n\n            # check if the input array is a 4-channel image\n            if len(ptr.shape) == 3 and ptr.shape[2] == 4:\n                # if the input array is a 4-channel image, convert it to RGBA\n                ptr = np.concatenate([ptr[:, :, :3], ptr[:, :, 3:4]], axis=2)\n\n            # check if the input array is a 1-channel image\n            if len(ptr.shape) == 3 and ptr.shape[2] == 1:\n                # if the input array is a 1-channel image, convert it to RGBA\n                ptr = np.concatenate([ptr, ptr, ptr, np.ones((ptr.shape[0], ptr.shape[1], 1), dtype=np.uint8) * 255], axis=2)\n\n        # check if the input is a list\n        if isinstance(ptr, list):\n            # if the input is a list, convert it to a numpy array\n            ptr = np.array(ptr)\n\n        # check if"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.numpy()\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        self.tex.upload(ptr, x, y, w, h)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.numpy()\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        if len(ptr.shape) == 3:\n            ptr = np.transpose(ptr, (2, 0, 1))\n        if ptr.dtype == np.uint8:\n            ptr = ptr.astype(np.float32) / 255.0\n        glBindTexture(GL_TEXTURE_2D, self.id)\n        glPixelStorei(GL_UNPACK_ALIGNMENT, 1)\n        glTexSubImage2D(GL_TEXTURE_2D, 0, x, y, w, h, GL_RGB, GL_FLOAT, ptr)\n        glBindTexture(GL_TEXTURE_2D, 0)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.numpy()\n\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        self.texture.upload(ptr, x, y, w, h)\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 3 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 2 else tvec\n    camera_matrix = (\n        camera_matrix.unsqueeze(0) if camera_matrix.ndim == 3 else camera_matrix\n    )\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 2 else image_size\n\n    # Ensure inputs have the same batch size\n    assert (\n        R.shape[0] == tvec.shape[0] == camera_matrix.shape[0] == image_size.shape[0]\n    ), \"Inputs must have the same batch size.\"\n\n    # Ensure inputs have the correct shapes\n    assert (\n        R.shape[-2:] == (3, 3)\n    ), \"Rotation matrices must have shape (batch_size, 3, 3).\"\n    assert (\n        tvec.shape[-1] == 3\n    ), \"Translation vectors must have shape (batch_size, 3).\"\n    assert (\n        camera_matrix.shape[-2:] == (3, 3)\n    ), \"Camera matrices must have shape (batch_size, 3, 3).\"\n    assert (\n        image_size.shape[-1] == 2\n    ), \"Image sizes must have shape (batch_size, 2).\"\n\n    # Ensure inputs have valid values\n    assert torch.all(\n        torch.isfinite(R)\n    ), \"Rotation matrices must contain only finite values.\"\n    assert torch.all(torch.isfinite(tvec)), \"Translation vectors must contain only finite values.\"\n    assert torch.all(\n        torch.isfinite(camera_matrix)\n    ), \"Camera matrices must contain only finite values.\"\n    assert torch.all(\n        torch.isfinite(image_size)\n    ), \"Image sizes must contain only finite values.\"\n\n    # Compute camera position\n    camera_"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 1 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 1 else tvec\n    camera_matrix = (\n        camera_matrix.unsqueeze(0) if camera_matrix.ndim == 1 else camera_matrix\n    )\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 1 else image_size\n\n    # Ensure all inputs have the same batch size\n    batch_size = R.shape[0]\n    assert (\n        tvec.shape[0] == batch_size\n    ), \"Input tensors must have the same batch size\"\n    assert (\n        camera_matrix.shape[0] == batch_size\n    ), \"Input tensors must have the same batch size\"\n    assert (\n        image_size.shape[0] == batch_size\n    ), \"Input tensors must have the same batch size\"\n\n    # Validate shapes and values of inputs\n    assert R.shape == (\n        batch_size,\n        3,\n        3,\n    ), \"Rotation matrix must have shape (batch_size, 3, 3)\"\n    assert tvec.shape == (\n        batch_size,\n        3,\n    ), \"Translation vector must have shape (batch_size, 3)\"\n    assert camera_matrix.shape == (\n        batch_size,\n        3,\n        3,\n    ), \"Camera matrix must have shape (batch_size, 3, 3)\"\n    assert image_size.shape == (\n        batch_size,\n        2,\n    ), \"Image size must have shape (batch_size, 2)\"\n    assert (\n        camera_matrix[:, 2, 2] > 0\n    ), \"Camera matrix must have positive elements on the diagonal\"\n    assert (\n        image_size[:, 0] > 0\n    ), \"Image width must be positive for each instance in the batch\"\n    assert (\n        image_size[:, 1] > 0"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 2 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 1 else tvec\n    camera_matrix = (\n        camera_matrix.unsqueeze(0) if camera_matrix.ndim == 2 else camera_matrix\n    )\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 1 else image_size\n\n    # Validate input shapes and values\n    assert R.ndim == 3 and tvec.ndim == 2 and camera_matrix.ndim == 3 and image_size.ndim == 2\n    assert R.shape[0] == tvec.shape[0] == camera_matrix.shape[0] == image_size.shape[0]\n    assert R.shape[1:] == (3, 3) and tvec.shape[1:] == (3,)\n    assert camera_matrix.shape[1:] == (3, 3)\n    assert image_size.shape[1:] == (2,)\n    assert torch.all(image_size > 0)\n\n    # Compute camera position\n    camera_position = -R.transpose(1, 2) @ tvec.unsqueeze(-1)\n    camera_position = camera_position.squeeze(-1)\n\n    # Compute camera rotation\n    camera_rotation = R.transpose(1, 2)\n\n    # Compute focal lengths and principal points\n    focal_lengths = torch.tensor(\n        [\n            camera_matrix[..., 0, 0] / camera_matrix[..., 0, 2],\n            camera_matrix[..., 1, 1] / camera_matrix[..., 1, 2],\n        ]\n    )\n    principal_points = torch.tensor(\n        [\n            camera_matrix[..., 0, 2] / camera_matrix[..., 0, 0],\n            camera_matrix[..., 1, 2] / camera"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if R.dim() == 1 else R\n    tvec = tvec.unsqueeze(0) if tvec.dim() == 1 else tvec\n    camera_matrix = (\n        camera_matrix.unsqueeze(0) if camera_matrix.dim() == 1 else camera_matrix\n    )\n    image_size = image_size.unsqueeze(0) if image_size.dim() == 1 else image_size\n\n    # Validate input shapes and values\n    assert R.shape[-2:] == (3, 3), \"R must be a batch of 3x3 rotation matrices\"\n    assert tvec.shape[-2:] == (3, 1), \"tvec must be a batch of 3x1 translation vectors\"\n    assert (\n        camera_matrix.shape[-2:] == (3, 3)\n    ), \"camera_matrix must be a batch of 3x3 camera intrinsic matrices\"\n    assert (\n        image_size.shape[-2:] == (2,)\n    ), \"image_size must be a batch of 2-element image sizes\"\n\n    # Compute camera position and rotation\n    position = -R.transpose(-1, -2) @ tvec\n    rotation = R.transpose(-1, -2)\n\n    # Compute focal lengths and principal points\n    focal_length = (\n        camera_matrix[:, 0, 0]\n        * image_size[:, 0]\n        / (camera_matrix[:, 0, 2] + znear)\n    )\n    principal_point = (\n        camera_matrix[:, 0, 2] * image_size[:, 0] / (camera_matrix[:, 0, 2] + znear)\n    )\n\n    # Compute sensor width\n    sensor_width = (\n        camera_matrix[:, 0, 0]\n        * image_size[:, 0]\n        / (camera_matrix[:, 0, 2] + znear)\n    )\n\n    # Comput"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    R = R.clone()\n    tvec = tvec.clone()\n    camera_matrix = camera_matrix.clone()\n    image_size = image_size.clone()\n\n    # Ensure that all inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 2 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 1 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if camera_matrix.ndim == 2 else camera_matrix\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 1 else image_size\n\n    # Validate shapes and values\n    assert R.shape[-2:] == (3, 3), f\"Invalid rotation matrix shape: {R.shape}\"\n    assert tvec.shape[-1] == 3, f\"Invalid translation vector shape: {tvec.shape}\"\n    assert camera_matrix.shape[-2:] == (3, 3), f\"Invalid camera matrix shape: {camera_matrix.shape}\"\n    assert image_size.shape[-1] == 2, f\"Invalid image size shape: {image_size.shape}\"\n\n    # Compute the camera position\n    camera_position = -R.transpose(-2, -1) @ tvec\n\n    # Compute the camera rotation\n    camera_rotation = R.transpose(-2, -1)\n\n    # Compute the focal length and sensor width\n    fx = camera_matrix[..., 0, 0] / image_size[..., 0]\n    fy = camera_matrix[..., 1, 1] / image_size[..., 1]\n    sensor_width = (image_size[..., 0] / image_size[..., 1]) * fx\n\n    # Compute the principal point offsets\n    cx = camera_matrix[..., 0, 2] / image_size[..., 0]\n    cy = camera_matrix[..., 1, 2] / image_size[..., 1"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure that all inputs are batched and have the correct shapes and values\n    R = torch.as_tensor(R, dtype=torch.float32)\n    tvec = torch.as_tensor(tvec, dtype=torch.float32)\n    camera_matrix = torch.as_tensor(camera_matrix, dtype=torch.float32)\n    image_size = torch.as_tensor(image_size, dtype=torch.float32)\n\n    assert R.ndim == 3 and R.shape[-2:] == (3, 3), \"R must be a batch of 3x3 rotation matrices\"\n    assert tvec.ndim == 2 and tvec.shape[-1] == 3, \"tvec must be a batch of 3D translation vectors\"\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[-2:] == (3, 3), \"camera_matrix must be a batch of 3x3 camera intrinsic matrices\"\n    assert image_size.ndim == 2 and image_size.shape[-1] == 2, \"image_size must be a batch of 2D image sizes\"\n\n    # Compute the focal length from the camera matrix\n    fx = camera_matrix[:, 0, 0]\n    fy = camera_matrix[:, 1, 1]\n\n    # Compute the principal point offsets from the camera matrix and image size\n    cx = camera_matrix[:, 0, 2] - image_size[:, 0] / 2\n    cy = camera_matrix[:, 1, 2] - image_size[:, 1] / 2\n\n    # Compute the sensor width from the focal length and image size\n    sensor_width = image_size[:, 0] / fx\n\n    # Compute the camera position from the translation vector\n    camera_position = tvec\n\n    # Compute the camera rotation from the rotation matrix\n    camera_rotation = R\n\n    # Compute the focal length adjustment factor based on the near clipping plane distance\n    focal_length_adjustment ="}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 2 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 1 else tvec\n    camera_matrix = (\n        camera_matrix.unsqueeze(0) if camera_matrix.ndim == 2 else camera_matrix\n    )\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 1 else image_size\n\n    # Validate input shapes and values\n    assert (\n        R.ndim == 3 and R.shape[-1] == 3 and R.shape[-2] == 3\n    ), \"R should be a batch of 3x3 rotation matrices\"\n    assert (\n        tvec.ndim == 2 and tvec.shape[-1] == 3\n    ), \"tvec should be a batch of 3D translation vectors\"\n    assert (\n        camera_matrix.ndim == 3\n        and camera_matrix.shape[-1] == 3\n        and camera_matrix.shape[-2] == 3\n    ), \"camera_matrix should be a batch of 3x3 camera matrices\"\n    assert (\n        image_size.ndim == 2 and image_size.shape[-1] == 2\n    ), \"image_size should be a batch of 2D image sizes\"\n\n    # Compute camera parameters\n    focal_length = camera_matrix[..., 0, 0] / (\n        (camera_matrix[..., 0, 2] + 1e-8) * image_size[..., 0]\n    )\n    principal_point = torch.stack(\n        [\n            (camera_matrix[..., 0, 2] + 1e-8) * image_size[..., 0] / 2,\n            (camera_matrix[..., 1, 2] + 1e-8) * image_size[..., 1] / 2,\n        ],\n        dim=-1,\n    )\n    sensor_width = focal_length * image_size"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched and have the same shape\n    R = R.view(-1, 3, 3)\n    tvec = tvec.view(-1, 3)\n    camera_matrix = camera_matrix.view(-1, 3, 3)\n    image_size = image_size.view(-1, 2)\n\n    # Ensure all inputs have the same batch size\n    batch_size = R.shape[0]\n    assert (\n        tvec.shape[0] == batch_size\n        and camera_matrix.shape[0] == batch_size\n        and image_size.shape[0] == batch_size\n    )\n\n    # Compute the camera's rotation in a different representation\n    R_p = R.clone()\n    R_p[:, 0, 0] = R[:, 0, 0] - 1\n    R_p[:, 1, 1] = R[:, 1, 1] - 1\n    R_p[:, 2, 2] = R[:, 2, 2] - 1\n\n    # Compute the camera's position\n    t = -torch.bmm(R.transpose(1, 2), tvec.unsqueeze(-1)).squeeze(-1)\n\n    # Compute the focal length\n    fx = camera_matrix[:, 0, 0]\n    fy = camera_matrix[:, 1, 1]\n    f = torch.sqrt(fx * fy)\n\n    # Compute the principal point offsets\n    cx = camera_matrix[:, 0, 2]\n    cy = camera_matrix[:, 1, 2]\n    cx_offset = (cx - image_size[:, 0] / 2) / image_size[:, 0]\n    cy_offset = (cy - image_size[:, 1] / 2) / image_size[:, 1]\n\n    # Compute the sensor width\n    sensor_width = image_size[:, 0] / fx\n\n    # Normal"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched and validated\n    R = torch.as_tensor(R, dtype=torch.float32, device=\"cuda\")\n    tvec = torch.as_tensor(tvec, dtype=torch.float32, device=\"cuda\")\n    camera_matrix = torch.as_tensor(camera_matrix, dtype=torch.float32, device=\"cuda\")\n    image_size = torch.as_tensor(image_size, dtype=torch.float32, device=\"cuda\")\n\n    # Compute camera position\n    camera_position = -torch.transpose(R, 1, 2) @ tvec\n\n    # Compute camera rotation\n    camera_rotation = torch.transpose(R, 1, 2)\n\n    # Compute focal lengths and sensor width\n    focal_lengths = camera_matrix[:, 0, 0] / camera_matrix[:, 0, 1]\n    sensor_width = image_size[:, 0] / focal_lengths\n\n    # Compute principal points\n    principal_points = camera_matrix[:, :2, 2]\n\n    # Compute principal point offsets\n    principal_point_offsets = principal_points - image_size / 2\n\n    # Compute normalized focal lengths\n    normalized_focal_lengths = focal_lengths / torch.norm(principal_point_offsets, dim=1)\n\n    # Compute camera parameters\n    camera_params = torch.stack(\n        (\n            camera_position,\n            camera_rotation,\n            focal_lengths,\n            normalized_focal_lengths,\n            sensor_width,\n            principal_points,\n            principal_point_offsets,\n        ),\n        dim=1,\n    )\n\n    return camera_params"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Check if the inputs are batched\n    if R.ndim != 3 or tvec.ndim != 2 or camera_matrix.ndim != 3 or image_size.ndim != 2:\n        raise ValueError(\n            \"Input tensors must be batched, but got ndims: R: {}, tvec: {}, camera_matrix: {}, image_size: {}\".format(\n                R.ndim, tvec.ndim, camera_matrix.ndim, image_size.ndim\n            )\n        )\n\n    # Check if the batch sizes match\n    if R.shape[0] != tvec.shape[0] or R.shape[0] != camera_matrix.shape[0] or R.shape[0] != image_size.shape[0]:\n        raise ValueError(\n            \"Batch sizes of inputs do not match, but got R: {}, tvec: {}, camera_matrix: {}, image_size: {}\".format(\n                R.shape[0], tvec.shape[0], camera_matrix.shape[0], image_size.shape[0]\n            )\n        )\n\n    # Check if the image sizes are valid\n    if not torch.all(image_size > 0):\n        raise ValueError(\"Invalid image size, all values must be positive\")\n\n    # Check if the focal length is valid\n    if not torch.all(camera_matrix[:, 0, 0] > 0):\n        raise ValueError(\"Invalid focal length, all values must be positive\")\n\n    # Check if the principal point offsets are valid\n    if not torch.all(torch.abs(camera_matrix[:, 0, 2]) < image_size[:, 0] / 2):\n        raise ValueError(\n            \"Invalid principal point offsets, all values must be less than half of the image width\"\n        )\n    if not torch.all(torch.abs(camera_matrix[:, 1, 2]) < image_size[:, 1] / 2):\n        raise ValueError(\n            \"Invalid principal point offsets, all values must be less than half"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched and valid\n    assert R.ndim == 3, f\"R should be a 3D tensor, but got {R.ndim}D tensor\"\n    assert tvec.ndim == 3, f\"tvec should be a 3D tensor, but got {tvec.ndim}D tensor\"\n    assert camera_matrix.ndim == 3, f\"camera_matrix should be a 3D tensor, but got {camera_matrix.ndim}D tensor\"\n    assert image_size.ndim == 2, f\"image_size should be a 2D tensor, but got {image_size.ndim}D tensor\"\n    assert R.shape == tvec.shape, f\"R and tvec should have the same shape, but got {R.shape} and {tvec.shape}\"\n    assert R.shape[0] == camera_matrix.shape[0], f\"The batch size of R and camera_matrix should be the same, but got {R.shape[0]} and {camera_matrix.shape[0]}\"\n    assert R.shape[0] == image_size.shape[0], f\"The batch size of R and image_size should be the same, but got {R.shape[0]} and {image_size.shape[0]}\"\n\n    # Extract camera parameters from the input tensors\n    focal_length = camera_matrix[:, 0, 0]\n    principal_point_x = camera_matrix[:, 0, 2]\n    principal_point_y = camera_matrix[:, 1, 2]\n    sensor_width = camera_matrix[:, 1, 1]\n\n    # Calculate the principal point offsets and normalize the focal length\n    principal_point_offset_x = principal_point_x / image_size[:, 0] * 2 - 1\n    principal_point_offset_y = principal_point_y / image_size[:, 1] * 2 - 1\n    focal_length = focal_length / image_size.min(dim=-1).values\n\n    # Compute the camera position and rotation"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 1 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 1 else tvec\n    camera_matrix = (\n        camera_matrix.unsqueeze(0) if camera_matrix.ndim == 1 else camera_matrix\n    )\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 1 else image_size\n\n    # Validate and reshape inputs\n    assert R.shape[0] == tvec.shape[0] == camera_matrix.shape[0] == image_size.shape[0]\n    R = R.reshape(-1, 3, 3)\n    tvec = tvec.reshape(-1, 3)\n    camera_matrix = camera_matrix.reshape(-1, 3, 3)\n    image_size = image_size.reshape(-1, 2)\n\n    # Compute focal lengths\n    focal_lengths = camera_matrix[:, 0, 0]\n\n    # Compute principal points\n    principal_points = torch.stack(\n        (\n            camera_matrix[:, 0, 2] - 0.5 * (image_size[:, 0] - 1),\n            camera_matrix[:, 1, 2] - 0.5 * (image_size[:, 1] - 1),\n        ),\n        dim=1,\n    )\n\n    # Compute sensor width\n    sensor_width = camera_matrix[:, 0, 0] * znear\n\n    # Compute camera position\n    camera_position = -R.transpose(1, 2) @ tvec.unsqueeze(2)\n    camera_position = camera_position.squeeze(2)\n\n    # Compute camera rotation\n    camera_rotation = R\n\n    # Stack and return camera parameters\n    return torch.stack(\n        (\n            focal_lengths,\n            principal_points,\n            sensor_"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched and valid\n    R = R.unsqueeze(1) if R.ndim == 2 else R\n    tvec = tvec.unsqueeze(1) if tvec.ndim == 2 else tvec\n    camera_matrix = (\n        camera_matrix.unsqueeze(1) if camera_matrix.ndim == 2 else camera_matrix\n    )\n    image_size = image_size.unsqueeze(1) if image_size.ndim == 2 else image_size\n\n    assert R.ndim == 3 and R.shape[-2:] == (3, 3)\n    assert tvec.ndim == 3 and tvec.shape[-1] == 3\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[-2:] == (3, 3)\n    assert image_size.ndim == 3 and image_size.shape[-1] == 2\n\n    # Compute focal lengths, sensor width, and principal points\n    focal_length = (\n        (\n            camera_matrix[..., 0, 0]\n            * image_size[..., 0]\n            / (camera_matrix[..., 0, 2] + znear)\n        )\n        .unsqueeze(-1)\n        .unsqueeze(-1)\n    )\n    sensor_width = (\n        (\n            camera_matrix[..., 0, 0]\n            * image_size[..., 0]\n            / (camera_matrix[..., 0, 2] + znear)\n        )\n        .unsqueeze(-1)\n        .unsqueeze(-1)\n    )\n    principal_point = (\n        torch.stack(\n            [\n                (camera_matrix[..., 0, 2] / focal_length[..., 0, 0]).unsqueeze(-1),\n                (camera_matrix[..., 1, 2] / focal_length[..., 0, 0]).unsqueeze(-1),\n            ],\n            dim=-"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 1 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 1 else tvec\n    camera_matrix = (\n        camera_matrix.unsqueeze(0) if camera_matrix.ndim == 1 else camera_matrix\n    )\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 1 else image_size\n\n    # Validate input shapes and values\n    assert R.shape == (\n        camera_matrix.shape[0],\n        3,\n        3,\n    ), \"Input rotation matrices must have shape (B, 3, 3)\"\n    assert tvec.shape == (\n        camera_matrix.shape[0],\n        3,\n    ), \"Input translation vectors must have shape (B, 3)\"\n    assert (\n        camera_matrix.shape[0] == image_size.shape[0]\n    ), \"Input camera matrices and image sizes must have the same batch size\"\n    assert (\n        camera_matrix.shape[1] == camera_matrix.shape[2]\n    ), \"Input camera matrices must be square matrices\"\n    assert (\n        camera_matrix[:, 0, 0] == camera_matrix[:, 1, 1]\n    ), \"Input camera matrices must have the same focal length\"\n    assert (\n        camera_matrix[:, 0, 0] > 0\n    ), \"Input camera matrices must have positive focal lengths\"\n    assert (\n        image_size[:, 0] > 0 and image_size[:, 1] > 0\n    ), \"Input image sizes must be positive\"\n\n    # Calculate focal length\n    focal_length = camera_matrix[:, 0, 0]\n    sensor_width = camera_matrix[:, 0, 0]\n    principal_point = camera_matrix[:, :2, 2]\n\n    # Adjust principal point offsets and normalize focal length\n    principal_point[:, 0] = (\n        principal_point[:,"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure that all inputs are batched\n    if not R.shape[0] == tvec.shape[0] == camera_matrix.shape[0] == image_size.shape[0]:\n        raise ValueError(\"All inputs must be batched\")\n\n    # Ensure that the rotation matrix is a valid rotation matrix\n    if not torch.allclose(torch.linalg.det(R), torch.ones_like(R)):\n        raise ValueError(\"Invalid rotation matrix\")\n\n    # Ensure that the translation vector is a valid translation vector\n    if not torch.allclose(torch.linalg.norm(tvec, dim=-1), torch.ones_like(tvec[..., :1])):\n        raise ValueError(\"Invalid translation vector\")\n\n    # Ensure that the camera matrix is a valid camera matrix\n    if not torch.allclose(camera_matrix[..., 2, 2], torch.ones_like(camera_matrix[..., 2, 2])):\n        raise ValueError(\"Invalid camera matrix\")\n\n    # Ensure that the image size is a valid image size\n    if not torch.allclose(image_size[..., 0] > 0, torch.ones_like(image_size[..., 0:1])):\n        raise ValueError(\"Invalid image size\")\n\n    # Ensure that the znear is a valid znear\n    if not torch.allclose(znear > 0, torch.ones_like(znear)):\n        raise ValueError(\"Invalid znear\")\n\n    # Compute the focal length\n    fx = camera_matrix[..., 0, 0] / (image_size[..., 0] / 2)\n    fy = camera_matrix[..., 1, 1] / (image_size[..., 1] / 2)\n\n    # Compute the principal point offsets\n    cx = camera_matrix[..., 0, 2] - (image_size[..., 0] / 2)\n    cy = camera_matrix[..., 1,"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure that all inputs are batched and valid\n    assert R.ndim == 3\n    assert tvec.ndim == 3\n    assert camera_matrix.ndim == 3\n    assert image_size.ndim == 2\n    assert R.shape[-1] == 3\n    assert tvec.shape[-1] == 3\n    assert camera_matrix.shape[-1] == 3\n    assert image_size.shape[-1] == 2\n\n    # Compute the camera position\n    camera_position = -torch.matmul(R.transpose(-1, -2), tvec)\n\n    # Compute the camera rotation\n    camera_rotation = R\n\n    # Compute the focal lengths\n    focal_lengths = camera_matrix[:, 0, 0]\n\n    # Compute the principal points\n    principal_points = camera_matrix[:, :2, 2]\n\n    # Compute the sensor width\n    sensor_width = camera_matrix[:, 0, 1]\n\n    # Adjust the principal point offsets and normalize the focal length\n    principal_points = principal_points / image_size.unsqueeze(-1)\n    focal_lengths = focal_lengths * image_size.prod(dim=-1).unsqueeze(-1) / (\n        znear * image_size.prod(dim=-1).unsqueeze(-1)\n    )\n\n    # Stack the computed parameters into a single tensor\n    camera_params = torch.stack(\n        [camera_position, camera_rotation, focal_lengths, principal_points, sensor_width],\n        dim=-1,\n    )\n\n    return camera_params"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    R = R.view(-1, 3, 3)\n    tvec = tvec.view(-1, 3)\n    camera_matrix = camera_matrix.view(-1, 3, 3)\n    image_size = image_size.view(-1, 2)\n\n    # Extract focal length from camera matrix\n    focal_length = camera_matrix[:, 0, 0]\n\n    # Compute principal point offsets\n    principal_point_offsets = torch.stack(\n        [\n            (image_size[:, 0] - 1) / 2,\n            (image_size[:, 1] - 1) / 2,\n        ],\n        dim=-1,\n    )\n\n    # Compute normalized focal length\n    normalized_focal_length = focal_length / image_size.prod(dim=-1)\n\n    # Compute sensor width\n    sensor_width = (image_size[:, 0] / focal_length) * znear\n\n    # Compute camera position\n    camera_position = -R.transpose(1, 2) @ tvec\n\n    # Compute camera rotation\n    camera_rotation = R.transpose(1, 2)\n\n    # Concatenate all parameters into a single tensor\n    camera_params = torch.cat(\n        [\n            camera_position.unsqueeze(dim=-1),\n            camera_rotation,\n            normalized_focal_length.unsqueeze(dim=-1),\n            principal_point_offsets.unsqueeze(dim=-1),\n            sensor_width.unsqueeze(dim=-1),\n        ],\n        dim=-1,\n    )\n\n    return camera_params\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 3 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 2 else tvec\n    camera_matrix = (\n        camera_matrix.unsqueeze(0) if camera_matrix.ndim == 3 else camera_matrix\n    )\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 2 else image_size\n\n    # Validate input shapes and values\n    assert R.shape[-2:] == (3, 3), \"Invalid shape for R\"\n    assert tvec.shape[-1] == 3, \"Invalid shape for tvec\"\n    assert camera_matrix.shape[-2:] == (3, 3), \"Invalid shape for camera_matrix\"\n    assert image_size.shape[-1] == 2, \"Invalid shape for image_size\"\n    assert znear > 0, \"znear must be positive\"\n\n    # Compute focal length and principal point offsets\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    cx, cy = camera_matrix[..., 0, 2], camera_matrix[..., 1, 2]\n    w, h = image_size[..., 0], image_size[..., 1]\n    fx, fy = fx / w, fy / h\n    cx, cy = cx / w, cy / h\n\n    # Compute camera position and rotation\n    R = R.permute(0, 2, 1)\n    tvec = -R @ tvec.unsqueeze(-1)\n    tvec = tvec.squeeze(-1)\n\n    # Compute focal length\n    zfar = znear * 10\n    f = torch.sqrt(fx * fy)\n    f_norm = f / znear\n    f_norm = torch.clamp(f_norm, min=0.1, max=1"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure inputs are batched and have the same shape\n    R = R.unsqueeze(0) if len(R.shape) == 3 else R\n    tvec = tvec.unsqueeze(0) if len(tvec.shape) == 3 else tvec\n    camera_matrix = (\n        camera_matrix.unsqueeze(0) if len(camera_matrix.shape) == 3 else camera_matrix\n    )\n    image_size = (\n        image_size.unsqueeze(0) if len(image_size.shape) == 3 else image_size\n    )\n\n    # Validate input shapes\n    assert R.shape[-2:] == (3, 3)\n    assert tvec.shape[-2:] == (3, 1)\n    assert camera_matrix.shape[-2:] == (3, 3)\n    assert image_size.shape[-2:] == (2,)\n\n    # Validate input values\n    assert torch.all(torch.abs(R[..., 2, :2]) < 1e-6)\n    assert torch.all(torch.abs(tvec[..., 2]) < 1e-6)\n\n    # Calculate focal length and sensor width\n    focal_length = camera_matrix[..., 0, 0]\n    sensor_width = camera_matrix[..., 0, 2]\n\n    # Calculate principal point offsets\n    principal_point_offsets = (\n        torch.tensor([[1.0, 0.0], [0.0, 1.0]], device=R.device)\n        * (image_size - 1.0)\n        / 2.0\n    )\n\n    # Adjust focal length and normalize principal point offsets\n    focal_length = focal_length * image_size[..., 0] / sensor_width\n    principal_point_offsets = principal_point_offsets * focal_length / image_size[..., 0]\n\n    # Calculate camera position\n    camera_position = -R.transpose(-1, -2) @ t"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    R = R.unsqueeze(1) if R.ndim == 2 else R\n    tvec = tvec.unsqueeze(1) if tvec.ndim == 2 else tvec\n    camera_matrix = (\n        camera_matrix.unsqueeze(1) if camera_matrix.ndim == 2 else camera_matrix\n    )\n    image_size = image_size.unsqueeze(1) if image_size.ndim == 2 else image_size\n\n    batch_size, num_cams, _, _ = R.shape\n\n    # Get the principal point offsets and focal lengths\n    principal_point_offset = (\n        torch.tensor([[0, 0]], dtype=torch.float32, device=R.device)\n        .repeat(batch_size, num_cams, 1)\n        .unsqueeze(3)\n    )\n    focal_length = (\n        torch.tensor([[1.0, 1.0]], dtype=torch.float32, device=R.device)\n        .repeat(batch_size, num_cams, 1)\n        .unsqueeze(3)\n    )\n\n    # Adjust the principal point offsets and normalize the focal lengths\n    principal_point_offset[..., 0, 0] = (\n        principal_point_offset[..., 0, 0]\n        + (image_size[..., 1, 0] - camera_matrix[..., 0, 2]) / 2.0\n    )\n    principal_point_offset[..., 1, 1] = (\n        principal_point_offset[..., 1, 1]\n        + (image_size[..., 0, 1] - camera_matrix[..., 1, 2]) / 2.0\n    )\n    focal_length[..., 0, 0] = focal_length[..., 0, 0] * camera_matrix[..., 0, 0]\n    focal_length[..., 1, 1] ="}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n        self.quad_program.use()\n        glBindTexture(GL_TEXTURE_2D, self.tex)\n        glBindVertexArray(self.vao)\n        glDrawArrays(GL_TRIANGLES, 0, 6)\n        glBindVertexArray(0)\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n\n        self.quad_program.use()\n\n        glBindTexture(GL_TEXTURE_2D, self.tex)\n\n        glBindVertexArray(self.vao)\n        glDrawArrays(GL_TRIANGLES, 0, 6)\n        glBindVertexArray(0)\n\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if self.use_quad_draw:\n            glViewport(x, y, w or self.W, h or self.H)\n            glScissor(x, y, w or self.W, h or self.H)\n            self.quad_program.use()\n            glBindTexture(GL_TEXTURE_2D, self.tex)\n            self.vao.draw()\n            glViewport(0, 0, self.W, self.H)\n            glScissor(0, 0, self.W, self.H)\n        else:\n            self.blit(x, y, w or self.W, h or self.H)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n        self.quad_program.use()\n        self.tex.use()\n        self.vao.bind()\n        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if self.use_quad_draw:\n            glViewport(x, y, w, h)\n            glScissor(x, y, w, h)\n            self.quad_program.use()\n            glBindTexture(GL_TEXTURE_2D, self.tex)\n            self.vao.bind()\n            glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n            self.vao.unbind()\n            glViewport(0, 0, self.W, self.H)\n            glScissor(0, 0, self.W, self.H)\n        else:\n            self.blit(x, y, w, h)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n\n        self.quad_program.use()\n        self.tex.bind()\n        self.vao.bind()\n        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n\n        self.vao.unbind()\n        self.tex.unbind()\n        self.quad_program.unuse()\n\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        self.quad_program.use()\n        self.tex.use()\n        self.vao.bind()\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n        self.vao.unbind()\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n\n        self.quad_program.use()\n        self.tex.use()\n\n        glBindVertexArray(self.vao)\n        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n        glBindVertexArray(0)\n\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if self.use_quad_draw:\n            glViewport(x, y, w, h)\n            glScissor(x, y, w, h)\n            self.quad_program.use()\n            glBindTexture(GL_TEXTURE_2D, self.tex)\n            self.vao.draw()\n            glViewport(0, 0, self.W, self.H)\n            glScissor(0, 0, self.W, self.H)\n        else:\n            self.blit(x, y, w, h)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        self.quad_program.use()\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n        gl.glBindVertexArray(0)\n        gl.glUseProgram(0)\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n        self.quad_program.use()\n        self.tex.bind()\n        self.vao.bind()\n        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n        self.vao.unbind()\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            return self.blit(x, y, w, h)\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n        self.quad_program.use()\n        self.tex.use()\n        self.vao.bind()\n        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n        self.vao.unbind()\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n        self.quad_program.use()\n        glActiveTexture(GL_TEXTURE0)\n        glBindTexture(GL_TEXTURE_2D, self.tex)\n        self.quad_program.set_int(\"tex\", 0)\n        glBindVertexArray(self.vao)\n        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n        glBindVertexArray(0)\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            return self.blit(x, y, w, h)\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n        self.quad_program.use()\n        glBindTexture(GL_TEXTURE_2D, self.tex)\n        glBindVertexArray(self.vao)\n        glDrawArrays(GL_TRIANGLES, 0, 6)\n        glBindVertexArray(0)\n        glUseProgram(0)\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n\n        self.quad_program.use()\n        self.tex.use()\n\n        self.vao.use()\n        glDrawArrays(GL_TRIANGLES, 0, 6)\n        self.vao.unuse()\n\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n\n        self.tex.unuse()\n        self.quad_program.unuse()\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if self.use_quad_draw:\n            glViewport(x, y, w or self.W, h or self.H)\n            glScissor(x, y, w or self.W, h or self.H)\n            self.quad_program.use()\n            self.tex.use()\n            self.vao.use()\n            glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n            glViewport(0, 0, self.W, self.H)\n            glScissor(0, 0, self.W, self.H)\n        else:\n            self.blit(x, y, w or self.W, h or self.H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n        self.quad_program.use()\n        self.tex.bind()\n        self.vao.bind()\n        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n        self.vao.unbind()\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set up the viewport and scissor box\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n\n        # Activate the shader program\n        self.quad_program.use()\n\n        # Bind the texture\n        glActiveTexture(GL_TEXTURE0)\n        glBindTexture(GL_TEXTURE_2D, self.tex)\n\n        # Set the texture uniform\n        self.quad_program.set_uniform('tex', 0)\n\n        # Set the model matrix uniform\n        model = glm.mat4(1.0)\n        self.quad_program.set_uniform('model', model)\n\n        # Set the projection matrix uniform\n        projection = glm.ortho(0.0, 1.0, 1.0, 0.0, -1.0, 1.0)\n        self.quad_program.set_uniform('projection', projection)\n\n        # Draw the quadrilateral\n        glBindVertexArray(self.vao)\n        glDrawArrays(GL_TRIANGLES, 0, 6)\n        glBindVertexArray(0)\n\n        # Restore the viewport and scissor box\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n\n        self.quad_program.use()\n        self.tex.use()\n\n        glBindVertexArray(self.vao)\n        glDrawArrays(GL_TRIANGLES, 0, 6)\n\n        glBindVertexArray(0)\n\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            return self.blit(x, y, w, h)\n\n        glViewport(x, y, w or self.W, h or self.H)\n        glScissor(x, y, w or self.W, h or self.H)\n        glEnable(GL_SCISSOR_TEST)\n\n        self.quad_program.use()\n        self.quad_program.set_uniform_int(\"tex\", 0)\n        self.tex.use(0)\n        self.vao.use()\n        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n\n        glDisable(GL_SCISSOR_TEST)\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n\n        self.quad_program.unuse()\n        self.tex.unuse()\n        self.vao.unuse()\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    R = batch.R\n    T = batch.T\n    K = batch.K\n    H = batch.H\n    W = batch.W\n\n    # Adjust R and T\n    R = R.transpose(0, 1)\n    T = -R @ T\n\n    # Recalculate K for NDC\n    K[0, 0] = 2 / W\n    K[1, 1] = 2 / H\n    K[0, 2] = -1\n    K[1, 2] = -1\n\n    # Compute camera center\n    C = -R.transpose(0, 1) @ T\n\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract camera parameters\n    R = batch.R\n    T = batch.T\n    K = batch.K\n\n    # Adjust rotation matrix to match PyTorch3D's coordinate system\n    R = R.transpose(0, 1)\n\n    # Adjust translation vector to match PyTorch3D's coordinate system\n    T = -R @ T\n\n    # Compute camera center in camera's coordinate system\n    C = -R.transpose(0, 1) @ T\n\n    # Compute intrinsic matrix for NDC\n    K = K / K[2, 2]\n\n    # Extract image dimensions\n    H, W = batch.H, batch.W\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n\n    # Adjust R and T for PyTorch3D\n    R = R.transpose(0, 1)\n    T = -R @ T\n\n    # Compute K for NDC\n    K = batch.K\n    K[:, 0] /= W\n    K[:, 1] /= H\n    K[:, 2] = (K[:, 2] - 0.5 * W) / W\n    K[:, 3] = (K[:, 3] - 0.5 * H) / H\n\n    # Compute camera center\n    C = -R.transpose(0, 1) @ T\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.H\n    W = batch.W\n    R = batch.R\n    T = batch.T\n\n    # Adjust the rotation matrix to match PyTorch3D's coordinate system\n    R = R.transpose(0, 1)\n    R[:, 1] *= -1\n    R[:, 2] *= -1\n\n    # Adjust the translation vector to match PyTorch3D's coordinate system\n    T[:, 1] *= -1\n    T[:, 2] *= -1\n\n    # Compute the camera center in the camera's coordinate system\n    C = -R.transpose(0, 1) @ T\n\n    # Compute the intrinsic matrix for NDC\n    K = batch.K\n    K[0, 0] = K[0, 0] / W\n    K[1, 1] = K[1, 1] / H\n    K[0, 2] = K[0, 2] / W + 0.5\n    K[1, 2] = K[1, 2] / H + 0.5\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract camera parameters from batch\n    R = batch.R\n    T = batch.T\n    K = batch.K\n    H = batch.H\n    W = batch.W\n\n    # Adjust R and T for PyTorch3D's coordinate system and conventions\n    R = R.transpose(0, 1)\n    T = -R @ T\n\n    # Compute camera center (C) in camera's coordinate system\n    C = -R.transpose(0, 1) @ T\n\n    # Compute intrinsic matrix (K) for NDC\n    K[0, 2] = K[0, 2] / W\n    K[1, 2] = K[1, 2] / H\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    R = batch.R\n    T = batch.T\n    K = batch.K\n    H = batch.H\n    W = batch.W\n\n    # Adjust R and T to match PyTorch3D's coordinate system and conventions\n    # R and T are applied on the right (requires a transposed R from OpenCV camera format)\n    # Coordinate system is different from that of OpenCV (cv: right down front, 3d: left up front)\n    # However, the correction has to be down on both T and R... (instead of just R)\n    # R = R.transpose(1, 2)\n    R = R.transpose(1, 2)\n    T = -R @ T\n\n    # Recalculate K for NDC\n    # K = K / K[2, 2]\n    K = K / K[2, 2]\n\n    # Compute camera center\n    # C = -R.transpose(1, 2) @ T\n    C = -R.transpose(1, 2) @ T\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R = batch.R.clone()\n    T = batch.T.clone()\n    K = batch.K.clone()\n\n    # Adjust rotation matrix to match PyTorch3D's coordinate system\n    R = R.transpose(1, 2)\n    R = R.clone()\n\n    # Adjust translation vector to match PyTorch3D's coordinate system\n    T = T.clone()\n    T[..., 1] *= -1\n    T[..., 2] *= -1\n\n    # Compute camera center\n    C = -R.transpose(1, 2) @ T\n\n    # Recalculate intrinsic matrix for NDC\n    K[..., 0, 2] = K[..., 0, 2] / W\n    K[..., 1, 2] = K[..., 1, 2] / H\n\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    R = batch.R\n    T = batch.T\n    K = batch.K\n    H = batch.H\n    W = batch.W\n\n    # Adjust R and T to match PyTorch3D's coordinate system and conventions\n    R = R.transpose(0, 1)\n    R = R[:, [0, 1, 2]]\n    T = T[:, [0, 1, 2]]\n\n    # Compute the camera center in the camera's coordinate system\n    C = -R.transpose(0, 1) @ T\n\n    # Recalculate K for NDC\n    K[0, 2] = W / 2\n    K[1, 2] = H / 2\n    K[0, 0] = K[1, 1] = (W / 2) / np.tan(K[0, 0] * np.pi / 180.0)\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n\n    # Adjust R and T to match PyTorch3D's coordinate system and conventions\n    # R and T are applied on the right (requires a transposed R from OpenCV camera format)\n    # Coordinate system is different from that of OpenCV (cv: right down front, 3d: left up front)\n    # However, the correction has to be down on both T and R... (instead of just R)\n    R = R.transpose(0, 1)\n    T = T[:, None]\n\n    # Compute the camera center in the camera's coordinate system\n    C = -R.transpose(0, 1) @ T\n\n    # Compute the intrinsic matrix for NDC\n    K = batch.K.clone()\n    K[0, 0] = K[0, 0] / W\n    K[1, 1] = K[1, 1] / H\n    K[0, 2] = K[0, 2] / W + 0.5\n    K[1, 2] = K[1, 2] / H + 0.5\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n\n    # Adjust R and T for PyTorch3D\n    R = batch.R.clone()\n    R[:, [0, 1]] = R[:, [1, 0]]\n    R[:, [0, 2]] = R[:, [2, 0]]\n    R[:, [1, 2]] = R[:, [2, 1]]\n    R = R.transpose(1, 2)\n\n    T = batch.T.clone()\n    T[:, 0] *= -1\n    T[:, 1] *= -1\n\n    # Compute camera center\n    C = -R.transpose(1, 2) @ T\n\n    # Compute K for NDC\n    K = batch.K.clone()\n    K[:, 0, 2] = (K[:, 0, 2] + 0.5) / W\n    K[:, 1, 2] = (K[:, 1, 2] + 0.5) / H\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract height and width from batch\n    H, W = batch.H, batch.W\n\n    # Extract R and T from batch\n    R, T = batch.R, batch.T\n\n    # Adjust R and T for PyTorch3D\n    # R and T are applied on the right (requires a transposed R from OpenCV camera format)\n    # Coordinate system is different from that of OpenCV (cv: right down front, 3d: left up front)\n    # However, the correction has to be down on both T and R... (instead of just R)\n    # R = R.T\n    # T = -R @ T\n\n    # Compute camera center\n    C = -R.T @ T\n\n    # Compute K for NDC\n    # K = K.copy()\n    # K[0, 0] = K[0, 0] / W\n    # K[1, 1] = K[1, 1] / H\n    # K[0, 2] = (K[0, 2] - W / 2) / W\n    # K[1, 2] = (K[1, 2] - H / 2) / H\n\n    # Return height, width, K, R, T, C\n    return H, W, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Get camera parameters\n    H, W = batch.H, batch.W\n    R, T = batch.R, batch.T\n\n    # Adjust rotation matrix to match PyTorch3D's coordinate system\n    R = R.transpose(0, 1)\n    R = R[:, ::-1]\n    R = R[::-1, :]\n    R = R.transpose(0, 1)\n\n    # Adjust translation vector to match PyTorch3D's coordinate system\n    T = -T\n\n    # Recalculate camera intrinsic matrix for NDC\n    K = batch.K\n    K[0, 2] = (K[0, 2] + 0.5) / W\n    K[1, 2] = (K[1, 2] + 0.5) / H\n\n    # Compute camera center in camera's coordinate system\n    C = -R.transpose() @ T\n\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Adjust R and T for PyTorch3D (requires a transposed R from OpenCV camera format)\n    # Coordinate system is different from that of OpenCV (cv: right down front, 3d: left up front)\n    # However, the correction has to be down on both T and R... (instead of just R)\n    batch.R = batch.R.transpose(1, 2)\n    batch.T[:, 1] *= -1\n    batch.T[:, 2] *= -1\n\n    # Compute camera center C (in camera coordinate system)\n    batch.C = -torch.inverse(batch.R) @ batch.T\n\n    # Compute intrinsic matrix for NDC\n    batch.K = batch.K.clone()\n    batch.K[:, 0] /= batch.W\n    batch.K[:, 2] = (batch.K[:, 2] - batch.W / 2) / batch.W\n    batch.K[:, 1] /= batch.H\n    batch.K[:, 2] = (batch.K[:, 2] - batch.H / 2) / batch.H\n\n    return batch.H, batch.W, batch.K, batch.R, batch.T, batch.C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.H\n    W = batch.W\n\n    R = batch.R.transpose(0, 1)\n    T = batch.T\n\n    # Adjust T and R\n    # T = T + np.array([0, 0, H / W])\n    T = T + np.array([0, 0, 0])\n    # R = R @ np.array([[1, 0, 0], [0, -1, 0], [0, 0, -1]])\n\n    # Recalculate K for NDC\n    K = batch.K\n    K[0, 2] = W / 2\n    K[1, 2] = H / 2\n\n    # Compute camera center\n    C = -R.transpose() @ T\n\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Get the height and width of the images\n    H, W = batch.H, batch.W\n\n    # Adjust the rotation and translation matrices\n    R = batch.R.clone()\n    T = batch.T.clone()\n\n    # Adjust the rotation matrix to match PyTorch3D's coordinate system\n    R[:, 0] *= -1\n    R[:, 1] *= -1\n    R[:, 2] *= -1\n\n    # Adjust the translation vector to match PyTorch3D's coordinate system\n    T[:, 0] *= -1\n    T[:, 1] *= -1\n    T[:, 2] *= -1\n\n    # Compute the camera center in the camera's coordinate system\n    C = -torch.inverse(R) @ T\n\n    # Compute the intrinsic matrix for NDC\n    fx = batch.K[0, 0]\n    fy = batch.K[1, 1]\n    cx = batch.K[0, 2]\n    cy = batch.K[1, 2]\n\n    K = torch.tensor([\n        [fx, 0, cx],\n        [0, fy, cy],\n        [0, 0, 1]\n    ]).float()\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract R and T from batch\n    R = batch.R\n    T = batch.T\n\n    # Adjust R and T for PyTorch3D coordinate system\n    R = R.transpose(1, 2)  # Transpose R to align with PyTorch3D's coordinate system\n    T = -R @ T  # Adjust T to account for the change in coordinate system\n\n    # Extract K from batch\n    K = batch.K\n\n    # Calculate the camera center\n    C = -np.linalg.inv(R) @ T\n\n    # Recalculate K for NDC\n    K[:, 0] /= K[0, 0]  # Normalize focal length\n    K[:, 1] /= K[1, 1]  # Normalize focal length\n    K[:, 2] /= K[0, 0]  # Adjust principal point\n    K[:, 3] /= K[1, 1]  # Adjust principal point\n\n    return batch.H, batch.W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R = batch.R.transpose(0, 1)\n    T = batch.T\n\n    # Adjust R and T to match PyTorch3D's coordinate system and conventions\n    # The correction has to be down on both T and R... (instead of just R)\n    # Coordinate system is different from that of OpenCV (cv: right down front, 3d: left up front)\n    # R and T are applied on the right (requires a transposed R from OpenCV camera format)\n    R[2, :] *= -1\n    T[2] *= -1\n\n    # Recalculate K for NDC\n    K = batch.K.copy()\n    K[0, 0] = W / (2 * np.tan(batch.fov_w / 2))\n    K[1, 1] = H / (2 * np.tan(batch.fov_h / 2))\n    K[0, 2] = (W - 1) / 2\n    K[1, 2] = (H - 1) / 2\n\n    # Compute camera center\n    C = -R.T @ T\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract camera parameters from batch\n    H, W = batch.H, batch.W\n    R = batch.R\n    T = batch.T\n\n    # Adjust rotation and translation matrices for PyTorch3D's coordinate system and conventions\n    R = R.transpose(0, 1)  # Adjust rotation matrix to match PyTorch3D's coordinate system\n    T = T[:, [1, 0, 2]]  # Adjust translation vector to match PyTorch3D's coordinate system\n    T = T * [1, -1, -1]  # Adjust translation vector to match PyTorch3D's conventions\n    C = -R.T @ T  # Compute camera center in camera's coordinate system\n\n    # Compute intrinsic matrix for normalized device coordinates (NDC)\n    K = batch.K.clone()\n    K[0, 2] = K[0, 2] / W  # Adjust focal length and principal point for NDC\n    K[1, 2] = K[1, 2] / H\n    K[0, 0] = 2 / W\n    K[1, 1] = 2 / H\n\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H, W = batch.H, batch.W\n    R = batch.R.T\n    T = -R @ batch.T\n    K = batch.K\n\n    # Adjust K for NDC\n    K[0, 0] = 2 / W\n    K[1, 1] = 2 / H\n    K[0, 2] = -1\n    K[1, 2] = -1\n\n    # Compute camera center in camera coordinate system\n    C = -R.T @ T\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.H\n    W = batch.W\n\n    R = batch.R\n    T = batch.T\n\n    # Adjust R and T for PyTorch3D\n    # R and T are applied on the right (requires a transposed R from OpenCV camera format)\n    # Coordinate system is different from that of OpenCV (cv: right down front, 3d: left up front)\n    # However, the correction has to be down on both T and R... (instead of just R)\n    R = R.transpose(0, 1)\n    R[:, 1] *= -1\n    R[:, 2] *= -1\n\n    T[0] *= -1\n    T[1] *= -1\n\n    # Calculate camera center\n    C = -R.transpose(0, 1) @ T\n\n    # Calculate K for NDC\n    fx = batch.K[0, 0]\n    fy = batch.K[1, 1]\n    cx = batch.K[0, 2]\n    cy = batch.K[1, 2]\n\n    K = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.fbo)\n        glBlitFramebuffer(x, y, w or self.W, h or self.H, x, y, w or self.W, h or self.H, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n\n        return self"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.fbo)\n        glBlitFramebuffer(\n            x,\n            y,\n            w if w else self.W,\n            h if h else self.H,\n            x,\n            y,\n            w if w else self.W,\n            h if h else self.H,\n            GL_COLOR_BUFFER_BIT,\n            GL_NEAREST,\n        )\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n        glReadBuffer(GL_COLOR_ATTACHMENT0)\n        glBlitFramebuffer(x, y, x + w, y + h, x, y, x + w, y + h, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Bind the Quad's FBO as the read framebuffer\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n\n        # Copy the pixel block from the FBO to the default framebuffer\n        glBlitFramebuffer(x, y, w or self.W, h or self.H, x, y, w or self.W, h or self.H, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n\n        # Restore the previously bound read framebuffer\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n        glBlitFramebuffer(x, y, w or self.W, h or self.H, x, y, w or self.W, h or self.H, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        glReadBuffer(GL_COLOR_ATTACHMENT0)\n        glBlitFramebuffer(x, y, x + w or self.W, y + h or self.H, x, y, x + w or self.W, y + h or self.H, GL_COLOR_BUFFER_BIT, GL_LINEAR)\n        glReadBuffer(GL_NONE)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n        glBlitFramebuffer(x, y, w or self.W, h or self.H, x, y, w or self.W, h or self.H, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n        glBlitFramebuffer(x, y, x + w, y + h, x, y, x + w, y + h, GL_COLOR_BUFFER_BIT, GL_LINEAR)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        glReadBuffer(GL_COLOR_ATTACHMENT0)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n        glBlitFramebuffer(x, y, x + w, y + h, x, y, x + w, y + h, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n\n        return\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.fbo)\n        glBlitFramebuffer(x, y, w if w else self.W, h if h else self.H, x, y, w if w else self.W, h if h else self.H, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.fbo)\n        glBlitFramebuffer(\n            x,\n            y,\n            w if w else self.W,\n            h if h else self.H,\n            x,\n            y,\n            w if w else self.W,\n            h if h else self.H,\n            GL_COLOR_BUFFER_BIT,\n            GL_NEAREST,\n        )\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n        glReadBuffer(GL_COLOR_ATTACHMENT0)\n        glBlitFramebuffer(x, y, w or self.W, h or self.H, x, y, w or self.W, h or self.H, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n        glBlitFramebuffer(x, y, w or self.W, h or self.H, x, y, w or self.W, h or self.H, GL_COLOR_BUFFER_BIT, GL_LINEAR)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Get the current read and draw framebuffers\n        read_fbo = gl.glGetInteger(gl.GL_READ_FRAMEBUFFER_BINDING)\n        draw_fbo = gl.glGetInteger(gl.GL_DRAW_FRAMEBUFFER_BINDING)\n\n        # Set the read and draw framebuffers to the Quad instance's FBO\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, self.fbo)\n\n        # Use the Quad instance's FBO as the source and destination for the pixel copy operation\n        gl.glBlitFramebuffer(x, y, w or self.W, h or self.H, x, y, w or self.W, h or self.H, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Restore the original read and draw framebuffers\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, read_fbo)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, draw_fbo)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Get the current read framebuffer\n        prev_read_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n\n        # Bind the Quad instance's FBO as the read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n\n        # Perform the pixel copy operation\n        gl.glBlitFramebuffer(x, y, w or self.W, h or self.H, x, y, w or self.W, h or self.H, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Restore the previous read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, prev_read_fbo)\n\n        # Return nothing\n        return\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Temporarily bind the Quad's framebuffer as the read framebuffer\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n\n        # Copy the pixel block from the read framebuffer to the draw framebuffer\n        glBlitFramebuffer(x, y, x + w, y + h, x, y, x + w, y + h, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n\n        # Restore the previously bound read framebuffer\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n\n        return\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if w == 0: w = self.W\n        if h == 0: h = self.H\n\n        # save current read framebuffer\n        read_fb = gl.glGetInteger(gl.GL_READ_FRAMEBUFFER_BINDING)\n\n        # bind this quad's fbo as the read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n\n        # copy pixels from the read framebuffer to the default framebuffer\n        gl.glBlitFramebuffer(x, y, x + w, y + h, x, y, x + w, y + h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # restore the previously bound read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, read_fb)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n        glReadBuffer(GL_COLOR_ATTACHMENT0)\n        glBlitFramebuffer(x, y, x + w, y + h, x, y, x + w, y + h, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        glReadBuffer(GL_COLOR_ATTACHMENT0)\n        glBlitFramebuffer(\n            x, y, w or self.W, h or self.H, x, y, w or self.W, h or self.H, GL_COLOR_BUFFER_BIT, GL_NEAREST\n        )\n        glReadBuffer(GL_NONE)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n        glBlitFramebuffer(0, 0, self.W, self.H, x, y, x + w, y + h, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Construct inner measure\n    inner = torch.zeros_like(t0)\n    for i in range(len(t0)):\n        inner[i] = torch.sum(torch.where(t1 <= t0[i], y1, torch.zeros_like(y1)))\n\n    # Construct outer measure\n    outer = torch.zeros_like(t0)\n    for i in range(len(t0)):\n        outer[i] = torch.sum(torch.where(t1 < t0[i], y1, torch.zeros_like(y1)))\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Construct cumulative sums of y1\n    cumsum = torch.cumsum(y1, dim=0)\n\n    # Find the indices of t0 in t1\n    indices = torch.searchsorted(t1, t0)\n\n    # Calculate the inner measure\n    inner = cumsum[indices]\n\n    # Calculate the outer measure\n    outer = cumsum[-1] - inner\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Get the cumulative sum of the values\n    csum = torch.cumsum(y1, dim=-1)\n\n    # Calculate the inner and outer measures\n    inner = torch.where(t0 <= t1, csum, torch.zeros_like(csum))\n    outer = torch.where(t0 > t1, csum, torch.zeros_like(csum))\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Construct the cumulative sum of y1\n    cumsum = torch.cumsum(y1, dim=0)\n\n    # Find the indices of t0 in t1\n    idx = torch.searchsorted(t1, t0)\n\n    # Construct the inner measure using the cumulative sum and the indices\n    inner = cumsum[idx]\n\n    # Construct the outer measure using the cumulative sum and the indices\n    outer = cumsum[idx - 1]\n\n    # Return the inner and outer measures\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Initialize the inner and outer measures with zeros\n    inner = torch.zeros_like(t0)\n    outer = torch.zeros_like(t0)\n\n    # Iterate over each target time\n    for i in range(t0.shape[0]):\n        # Find the indices of the source times that are less than or equal to the current target time\n        indices = torch.where(t1 <= t0[i])\n        # Compute the inner measure as the sum of the corresponding values\n        inner[i] = torch.sum(y1[indices])\n        # Compute the outer measure as the difference between the cumulative sum of the values and the inner measure\n        outer[i] = torch.cumsum(y1, dim=0)[indices].sum() - inner[i]\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Get the cumulative sum of y1\n    y1_cumsum = torch.cumsum(y1, dim=0)\n\n    # Calculate the inner measure\n    inner_measure = torch.where(t0 <= t1[0], torch.zeros_like(t0), y1_cumsum[0])\n    inner_measure = torch.where(t0 > t1[-1], y1_cumsum[-1], inner_measure)\n    inner_measure = torch.where(\n        (t0 > t1[0]) & (t0 <= t1[1:]), y1_cumsum[1:], inner_measure\n    )\n\n    # Calculate the outer measure\n    outer_measure = torch.where(t0 <= t1[0], torch.zeros_like(t0), y1_cumsum[0])\n    outer_measure = torch.where(t0 > t1[-1], y1_cumsum[-1], outer_measure)\n    outer_measure = torch.where(\n        (t0 >= t1[:-1]) & (t0 < t1[1:]), y1_cumsum[1:], outer_measure\n    )\n\n    return inner_measure, outer_measure"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the source time (t1) that are less than or equal to the target time (t0)\n    idx = t1 <= t0\n\n    # Compute the cumulative sum of the values (y1) at the indices\n    cumsum = torch.cumsum(y1[idx], dim=-1)\n\n    # Compute the inner measure as the difference between the cumulative sum at the target time (t0) and the cumulative sum at the previous time step\n    inner = cumsum[..., -1] - cumsum[..., -2]\n\n    # Compute the outer measure as the difference between the cumulative sum at the target time (t0) and the cumulative sum at the first time step\n    outer = cumsum[..., -1] - cumsum[..., 0]\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Check if t0 is a scalar\n    if t0.ndim == 0:\n        t0 = t0.reshape((1,))\n\n    # Check if t1 is a scalar\n    if t1.ndim == 0:\n        t1 = t1.reshape((1,))\n\n    # Check if t1 and y1 have the same shape\n    if t1.shape != y1.shape:\n        raise ValueError(\"t1 and y1 must have the same shape\")\n\n    # Check if t0 is a vector\n    if t0.ndim == 1:\n        t0 = t0.reshape((-1, 1))\n\n    # Check if t1 is a vector\n    if t1.ndim == 1:\n        t1 = t1.reshape((-1, 1))\n\n    # Check if t0 and t1 have the same shape\n    if t0.shape[1] != t1.shape[1]:\n        raise ValueError(\"t0 and t1 must have the same number of columns\")\n\n    # Check if t0 and t1 are sorted in ascending order\n    if not np.all(t0[:, 0] <= t0[:, -1]):\n        raise ValueError(\"t0 must be sorted in ascending order\")\n    if not np.all(t1[:, 0] <= t1[:, -1]):\n        raise ValueError(\"t1 must be sorted in ascending order\")\n\n    # Check if t0 and t1 have the same number of columns\n    if t0.shape[1] != t1.shape[1]:\n        raise ValueError(\"t0 and t1 must have the same number of columns\")\n\n    # Check if t0 and t1 are sorted in ascending order\n    if not np.all(t0[:, 0] <= t0[:, -1]):\n        raise ValueError(\"t0 must be sorted in ascending order\")\n    if not np.all(t1[:, 0] <= t1[:, -1]):\n        raise ValueError(\"t1 must be sorted in ascending order\")"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Initialize the inner and outer measures\n    inner = torch.zeros_like(t0)\n    outer = torch.zeros_like(t0)\n\n    # Loop through each target time\n    for i in range(t0.shape[0]):\n\n        # Find the indices of the source times that are less than or equal to the target time\n        indices = torch.where(t1 <= t0[i])[0]\n\n        # If there are no source times less than or equal to the target time, set the inner and outer measures to zero\n        if indices.shape[0] == 0:\n            inner[i] = 0\n            outer[i] = 0\n\n        # If there are source times less than or equal to the target time, compute the inner and outer measures\n        else:\n            # Compute the cumulative sums of the source values\n            cumsum = torch.cumsum(y1[indices], dim=0)\n\n            # Compute the inner measure as the difference between the cumulative sum of the source values and the target time\n            inner[i] = cumsum[-1] - t0[i]\n\n            # Compute the outer measure as the difference between the cumulative sum of the source values and the target time\n            outer[i] = cumsum[-1] - t0[i]\n\n    # Return the inner and outer measures\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    import torch\n\n    # Sort the source time and values\n    t1, y1 = torch.sort(t1), torch.sort(y1)\n\n    # Construct the cumulative sum of the source values\n    y1_cumsum = torch.cumsum(y1, dim=0)\n\n    # Find the indices of the target times in the source times\n    t0_indices = torch.searchsorted(t1, t0)\n\n    # Compute the inner and outer measures\n    inner_measure = y1_cumsum[t0_indices - 1] / t0\n    outer_measure = y1_cumsum[t0_indices] / t0\n\n    return inner_measure, outer_measure"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Create a mask to identify the source times that are greater than or equal to the target times\n    mask = t1 >= t0\n\n    # Compute the cumulative sums of the source values\n    cumsum = torch.cumsum(y1, dim=0)\n\n    # Compute the inner measure by selecting the cumulative sums for the source times that are greater than or equal to the target times\n    inner = cumsum[mask]\n\n    # Compute the outer measure by selecting the cumulative sums for the source times that are less than the target times\n    outer = cumsum[~mask]\n\n    # Return the inner and outer measures\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute cumulative sums of the source values (y1)\n    y1_cumsum = torch.cumsum(y1, dim=0)\n\n    # Compute the inner measure\n    inner_measure = torch.zeros_like(t0)\n    inner_measure[t0 <= t1[0]] = y1_cumsum[0]\n    inner_measure[t0 > t1[-1]] = y1_cumsum[-1]\n    for i in range(len(t1) - 1):\n        mask = (t0 > t1[i]) & (t0 <= t1[i + 1])\n        inner_measure[mask] = y1_cumsum[i] + (y1[i + 1] - y1[i]) / (t1[i + 1] - t1[i]) * (t0[mask] - t1[i])\n\n    # Compute the outer measure\n    outer_measure = torch.zeros_like(t0)\n    outer_measure[t0 <= t1[0]] = 0\n    outer_measure[t0 > t1[-1]] = y1_cumsum[-1]\n    for i in range(len(t1) - 1):\n        mask = (t0 > t1[i]) & (t0 <= t1[i + 1])\n        outer_measure[mask] = y1_cumsum[i] + (y1[i + 1] - y1[i]) / (t1[i + 1] - t1[i]) * (t0[mask] - t1[i])\n\n    return inner_measure, outer_measure"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    import torch\n\n    t1 = torch.cat((torch.zeros(1), t1))\n    y1 = torch.cat((torch.zeros(1), y1))\n\n    # Construct inner and outer measures\n    inner = torch.zeros_like(t0)\n    outer = torch.zeros_like(t0)\n\n    for i in range(len(t0)):\n        inner[i] = torch.sum(y1[:-1] * (torch.min(t1[1:], t0[i]) - torch.max(t1[:-1], t0[i])))\n        outer[i] = torch.sum(y1[:-1] * (torch.min(t1[1:], t0[i] + 1e-6) - torch.max(t1[:-1], t0[i] - 1e-6)))\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the cumulative sums of y1\n    y1_cumsum = torch.cumsum(y1, dim=-1)\n\n    # Find the indices of the source times that are less than or equal to the target times\n    t1_indices = torch.searchsorted(t1, t0, right=True) - 1\n\n    # Extract the corresponding cumulative sums for the target times\n    y1_cumsum_t0 = y1_cumsum[torch.arange(t1.shape[0]), t1_indices]\n\n    # Compute the inner measure as the difference between the cumulative sums at the target times\n    inner = y1_cumsum_t0 - y1_cumsum[torch.arange(t1.shape[0]), t1_indices - 1]\n\n    # Compute the outer measure as the difference between the cumulative sums at the target times\n    outer = y1_cumsum_t0 - y1_cumsum[torch.arange(t1.shape[0]), 0]\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Create a copy of t1 and y1 to avoid modifying the original tensors\n    t1_copy = t1.clone()\n    y1_copy = y1.clone()\n\n    # Sort the source times and values in ascending order\n    sorted_indices = torch.argsort(t1_copy)\n    t1_copy = t1_copy[sorted_indices]\n    y1_copy = y1_copy[sorted_indices]\n\n    # Compute the cumulative sums of the values\n    cumsum = torch.cumsum(y1_copy, dim=0)\n\n    # Compute the inner and outer measures\n    inner = torch.zeros_like(t0)\n    outer = torch.zeros_like(t0)\n    for i in range(len(t0)):\n        # Find the indices of the source times that are less than or equal to the target time\n        indices = torch.where(t1_copy <= t0[i])[0]\n        if len(indices) > 0:\n            # Compute the inner measure as the sum of the values at the last index\n            inner[i] = cumsum[indices[-1]]\n            # Compute the outer measure as the difference between the cumulative sum at the last index and the cumulative sum at the last index minus 1\n            outer[i] = cumsum[indices[-1]] - cumsum[indices[-2]]\n        else:\n            # If there are no source times less than or equal to the target time, set the inner and outer measures to 0\n            inner[i] = 0\n            outer[i] = 0\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the source time values that are less than or equal to the target time\n    i = torch.searchsorted(t1, t0)\n\n    # Clip the indices to ensure they are within the valid range of the source time values\n    i = torch.clamp(i, min=0, max=len(t1) - 1)\n\n    # Compute the inner measure by subtracting the cumulative sum of the source values up to the previous index\n    inner = t0 - t1[i - 1]\n\n    # Compute the outer measure by subtracting the cumulative sum of the source values up to the current index\n    outer = t1[i] - t0\n\n    # Return the inner and outer measures as a tuple\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Convert t0 and t1 to float32\n    t0 = tf.cast(t0, tf.float32)\n    t1 = tf.cast(t1, tf.float32)\n\n    # Compute the cumulative sums of y1\n    cumsum = tf.math.cumsum(y1)\n\n    # Compute the inner measure\n    inner = tf.gather(cumsum, tf.searchsorted(t1, t0, side='right'))\n\n    # Compute the outer measure\n    outer = tf.gather(cumsum, tf.searchsorted(t1, t0, side='left'))\n\n    # Return the inner and outer measures\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0 = t0.detach()\n    t1 = t1.detach()\n    y1 = y1.detach()\n\n    t0 = t0.flatten()\n    t1 = t1.flatten()\n    y1 = y1.flatten()\n\n    t0_size = t0.size(0)\n    t1_size = t1.size(0)\n\n    t0_idx = torch.arange(t0_size, device=t0.device)\n    t1_idx = torch.arange(t1_size, device=t0.device)\n\n    t0_idx_expand = t0_idx.repeat(t1_size).view(t0_size, t1_size)\n    t1_idx_expand = t1_idx.repeat(t0_size).view(t1_size, t0_size).T\n\n    t0_expand = t0.repeat(t1_size).view(t0_size, t1_size)\n    t1_expand = t1.repeat(t0_size).view(t1_size, t0_size).T\n\n    t0_mask = (t0_expand >= t1_expand).type(torch.float32)\n    t1_mask = (t0_expand < t1_expand).type(torch.float32)\n\n    y1_expand = y1.repeat(t0_size).view(t1_size, t0_size).T\n\n    inner = torch.sum(t0_mask * y1_expand, dim=0)\n    outer = torch.sum(t1_mask * y1_expand, dim=0)\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0 = t0.reshape(-1)\n    t1 = t1.reshape(-1)\n    y1 = y1.reshape(-1)\n\n    # Check if t0 is sorted\n    if not torch.all(t0[1:] >= t0[:-1]):\n        raise ValueError(\"t0 must be sorted in ascending order.\")\n\n    # Check if t1 is sorted\n    if not torch.all(t1[1:] >= t1[:-1]):\n        raise ValueError(\"t1 must be sorted in ascending order.\")\n\n    # Check if t0 and t1 are compatible\n    if not torch.all(t0[:-1] <= t1[1:]) and not torch.all(t0[1:] >= t1[:-1]):\n        raise ValueError(\"t0 and t1 must be compatible.\")\n\n    # Check if y1 is compatible with t1\n    if y1.shape[0] != t1.shape[0]:\n        raise ValueError(\"y1 must be compatible with t1.\")\n\n    # Compute cumulative sums for t1 and y1\n    t1_cumsum = torch.cumsum(t1, dim=0)\n    y1_cumsum = torch.cumsum(y1, dim=0)\n\n    # Compute inner and outer measures for t0\n    inner = torch.zeros_like(t0)\n    outer = torch.zeros_like(t0)\n\n    for i in range(t0.shape[0]):\n        # Find the indices of the closest values in t1\n        t1_indices = torch.searchsorted(t1_cumsum, t0[i])\n\n        # Compute inner and outer measures\n        if t1_indices[0] == 0:\n            inner[i] = y1_cumsum[0]\n            outer[i] = y1_cumsum[-1] - y1_cumsum[0]\n        else:\n            inner[i] = y1_cumsum[t1_"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Construct inner and outer measures\n    inner = t0[..., None] - t1\n    outer = t1 - t0[..., None]\n\n    # Compute cumulative sums for inner and outer measures\n    inner_cumsum = torch.cumsum(inner, dim=-1)\n    outer_cumsum = torch.cumsum(outer, dim=-1)\n\n    # Compute the inner and outer measures for the target time (t0)\n    inner_measure = torch.where(inner_cumsum < 0, torch.zeros_like(inner_cumsum), inner_cumsum)\n    outer_measure = torch.where(outer_cumsum > 0, torch.zeros_like(outer_cumsum), outer_cumsum)\n\n    return inner_measure, outer_measure\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_env_upper = torch.max(w_env, dim=-1, keepdim=True)[0]\n\n    # calculate the difference between target weights and the upper envelope weights\n    w_diff = w - w_env_upper\n\n    # calculate the half-quadratic loss\n    loss = torch.max(w_diff, torch.zeros_like(w_diff)) ** 2 / (2 * eps)\n\n    return loss\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_env_upper = torch.max(w_env, dim=-1, keepdim=True)[0]\n\n    # calculate the difference between target weights and the upper envelope weights\n    diff = w - w_env_upper\n\n    # calculate the half-quadratic loss\n    loss = torch.square(diff) / (2 * torch.square(t) + eps)\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_env_upper = torch.max(w_env, dim=-1)[0]\n\n    # calculate the difference between the target weights and the upper envelope weights\n    diff = w - w_env_upper\n\n    # calculate the half-quadratic loss\n    loss = torch.max(diff, torch.zeros_like(diff)) ** 2\n\n    # scale the loss by the number of dimensions\n    loss = loss / t.shape[-1]\n\n    # return the loss\n    return loss\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_env_upper = torch.maximum(w_env, torch.zeros_like(w_env))\n\n    # calculate the difference between target weights and the upper envelope weights\n    diff = w - w_env_upper\n\n    # calculate the scaled half-quadratic loss\n    loss = torch.sum(diff ** 2 / (2 * torch.maximum(w_env_upper, eps)))\n\n    return loss\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_env = torch.max(w_env, dim=-1, keepdim=True)[0]\n\n    # calculate the difference between target weights and the upper envelope weights\n    diff = w - w_env\n\n    # calculate the half-quadratic loss function\n    loss = torch.max(torch.zeros_like(diff), diff) ** 2\n\n    # return the scaled loss\n    return loss / (2 * torch.max(w, dim=-1, keepdim=True)[0] + eps)\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_env_upper = torch.maximum(w_env, torch.zeros_like(w_env))\n\n    # calculate the difference between target weights and upper envelope weights\n    w_diff = w - w_env_upper\n\n    # calculate the half-quadratic loss\n    loss = torch.where(w_diff > 0, w_diff**2 / 2, w_diff)\n\n    # return the loss\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_env_upper = torch.maximum(w_env, w)\n\n    # calculate the difference between target weights and upper envelope weights\n    w_diff = torch.maximum(w - w_env_upper, torch.zeros_like(w))\n\n    # calculate the scaled half-quadratic loss\n    loss = w_diff**2 / (w_diff + eps)\n\n    return loss\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_upper_envelope = torch.maximum(w_env, w)\n\n    # calculate the difference between target weights and the upper envelope\n    diff = w - w_upper_envelope\n\n    # calculate the scaled half-quadratic loss\n    loss = torch.square(diff) / torch.square(w_upper_envelope + eps)\n\n    # return the calculated loss\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n\n    # calculate upper envelope\n    w_env_upper = torch.maximum(w_env, torch.zeros_like(w_env))\n    w_env_upper = w_env_upper / (w_env_upper.sum(dim=-1, keepdim=True) + eps)\n\n    # calculate loss\n    loss = torch.square(w - w_env_upper)\n    loss = torch.sum(loss, dim=-1)\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # get the upper envelope weights\n    w_env = torch.maximum(w_env, w_env[..., -1].unsqueeze(-1))\n\n    # calculate the loss\n    loss = (w - w_env) ** 2\n    loss = loss / (2 * (t[..., 1:] - t[..., :-1]) + eps)\n    loss = loss.sum(-1)\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n\n    # calculate upper envelope weights\n    w_upper_env = torch.max(w_env, dim=-1).values\n\n    # calculate loss\n    loss = (w - w_upper_env) ** 2\n    loss = loss / (w + eps)\n    loss = torch.sum(loss, dim=-1)\n    loss = torch.mean(loss)\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate upper envelope weights\n    w_env_half = w_env / 2\n\n    # calculate difference between target weights and upper envelope weights\n    w_diff = w - w_env_half\n\n    # calculate half-quadratic loss\n    loss = torch.max(w_diff, torch.zeros_like(w_diff)) ** 2\n\n    # return scaled loss\n    return loss / (2 * torch.sum(w_env_half ** 2) + eps)\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the half-quadratic loss\n    loss = (t - t_env) ** 2 * torch.maximum(torch.zeros_like(w_env), w - w_env) ** 2\n\n    # calculate the upper envelope weights\n    w_env_upper = torch.maximum(w, w_env)\n\n    # calculate the scaled half-quadratic loss\n    loss = loss / (w_env_upper + eps)\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    t_env = t_env.unsqueeze(-1)\n    w_env = w_env.unsqueeze(-1)\n    w = w.unsqueeze(-1)\n\n    # t_env.shape = w_env.shape = w.shape = [..., 1]\n    # t.shape = [..., 1]\n\n    # t_env.shape = [..., 1, 1]\n    # w_env.shape = [..., 1, 1]\n    # w.shape = [..., 1, 1]\n    # t.shape = [..., 1, 1]\n\n    # t_env.shape = [..., 1, 1]\n    # w_env.shape = [..., 1, 1]\n    # w.shape = [..., 1, 1]\n    # t.shape = [..., 1, 1]\n    # t_env.shape = [..., 1, 1]\n    # w_env.shape = [..., 1, 1]\n    # w.shape = [..., 1, 1]\n    # t.shape = [..., 1, 1]\n\n    # t_env.shape = [..., 1, 1]\n    # w_env.shape = [..., 1, 1]\n    # w.shape = [..., 1, 1]\n    # t.shape = [..., 1, 1]\n    # t_env.shape = [..., 1, 1]\n    # w_env.shape = [..., 1, 1]\n    # w.shape = [..., 1, 1]\n    # t.shape = [..., 1, 1]\n    # t_env.shape = [..., 1, 1]\n    # w_env.shape = [..., 1, 1]\n    # w.shape = [..., 1, 1]"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_env_max = torch.maximum(w_env, torch.tensor([0.0], device=w.device))\n    w_env_max_scaled = w_env_max / (w_env_max + eps)\n    w_env_max_scaled = torch.minimum(w_env_max_scaled, torch.tensor([1.0], device=w.device))\n    w_env_max_scaled = torch.square(w_env_max_scaled)\n    w_env_max_scaled = w_env_max_scaled * w_env_max_scaled\n\n    w_env_max_scaled = w_env_max_scaled * w_env_max_scaled\n\n    w_env_max_scaled = w_env_max_scaled * w_env_max_scaled\n\n    w_env_max_scaled = w_env_max_scaled * w_env_max_scaled\n\n    w_env_max_scaled = w_env_max_scaled * w_env_max_scaled\n\n    w_env_max_scaled = w_env_max_scaled * w_env_max_scaled\n\n    w_env_max_scaled = w_env_max_scaled * w_env_max_scaled\n\n    w_env_max_scaled = w_env_max_scaled * w_env_max_scaled\n\n    w_env_max_scaled = w_env_max_scaled * w_env_max_scaled\n\n    w_env_max_scaled = w_env_max_scaled * w_env_max_scaled\n\n    w_env_max_scaled = w_env_max_scaled * w_env_max_scaled\n\n    w_env_max_scaled = w_env_max_scaled * w_env_max_scaled\n\n    w_env_max_scaled = w_env_max_scaled * w_env_max_"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    t_env = t_env.unsqueeze(0).expand(t.shape[0], -1, -1)\n    w_env = w_env.unsqueeze(0).expand(t.shape[0], -1, -1)\n    w_env = w_env.unsqueeze(-1)\n    t_env = t_env.unsqueeze(-1)\n\n    # t.shape[-1] = w.shape[-1] + 1\n    t = t.unsqueeze(-1)\n    w = w.unsqueeze(-1)\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # w_env.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # w_env.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # w_env.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # w_env.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # w_env.shape[-1] = w.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + "}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_env_up = (w_env[..., None] + eps) / (t_env[..., None] + eps)\n    w_env_up = torch.sort(w_env_up, dim=-1)[0][..., -1]\n\n    # calculate the difference between target weights and the upper envelope\n    w_diff = w - w_env_up\n\n    # apply the half-quadratic loss function to the difference\n    loss = torch.max(w_diff, torch.zeros_like(w_diff)) ** 2\n\n    # return the loss\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_env_upper = torch.max(w_env, dim=-1, keepdim=True).values\n\n    # calculate the difference between the target weights and the upper envelope weights\n    w_diff = w - w_env_upper\n\n    # calculate the half-quadratic loss function\n    loss = torch.square(w_diff) / (2. * (w_diff + eps))\n\n    # calculate the loss based on the difference between target weights and the upper envelope\n    loss_tot = torch.sum(loss, dim=-1)\n\n    # return the calculated loss\n    return loss_tot\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate upper envelope\n    w_env = torch.where(w_env > 0, w_env, torch.zeros_like(w_env))\n    w_env = w_env.unsqueeze(-1)\n    w_env = w_env.expand(-1, -1, t_env.shape[-1])\n    w_env = w_env.permute(2, 1, 0)\n    w_env = torch.max(w_env, dim=1)[0]\n\n    # calculate loss\n    t = t.unsqueeze(-1)\n    t = t.expand(-1, -1, t_env.shape[-1])\n    t_env = t_env.permute(2, 1, 0)\n    t_env = t_env.expand(-1, -1, t.shape[-1])\n    t_env = t_env.permute(2, 1, 0)\n    w = w.unsqueeze(-1)\n    w = w.expand(-1, -1, t_env.shape[-1])\n    w = w.permute(2, 1, 0)\n    w_env = w_env.unsqueeze(-1)\n    w_env = w_env.expand(-1, -1, t_env.shape[-1])\n    w_env = w_env.permute(2, 1, 0)\n    t_diff = t - t_env\n    w_diff = w - w_env\n    w_diff = torch.where(w_diff > 0, w_diff, torch.zeros_like(w_diff))\n    w_diff = w_diff.permute(2, 1, 0)\n    t_diff = t_diff.permute(2, 1, 0)\n    t_diff = torch.pow(t_diff, 2)\n    t_diff = torch.sum(t_diff, dim=1)\n    w_diff = torch.pow(w_diff, "}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] == w.shape[-1] + 1\n    t_env = t_env.unsqueeze(-1)\n    w_env = w_env.unsqueeze(-1)\n    t = t.unsqueeze(-1)\n    w = w.unsqueeze(-1)\n\n    # w_env is a vector of weights for t_env, w is a vector of weights for t\n    # calculate the upper envelope weights\n    w_upper = torch.max(w_env, w)\n\n    # calculate the difference between target weights and the upper envelope\n    w_diff = w - w_upper\n\n    # calculate the loss using a half-quadratic loss function\n    loss = torch.max(torch.zeros_like(w_diff), w_diff) ** 2\n\n    # normalize the loss by the number of dimensions\n    loss = loss / t.shape[-1]\n\n    # sum over the batch dimension\n    loss = loss.sum(dim=-1)\n\n    # return the loss\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # inter-interval loss\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # intra-interval loss\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # total loss\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # inter-interval loss\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # intra-interval loss\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # total loss\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # inter-interval loss\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # intra-interval loss\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # total loss\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # inter-interval loss\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # intra-interval loss\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # total loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # inter-interval loss\n    inter_loss = torch.sum(w * torch.abs(t[..., 1:] - t[..., :-1]))\n\n    # intra-interval loss\n    intra_loss = torch.sum(w * torch.abs(t[..., 1:] - t[..., :-1]))\n\n    # total loss\n    loss = inter_loss + intra_loss\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # inter-interval loss\n    # the last dimension of t should be one more than that of w\n    # the last dimension of w should be one less than that of t\n    # the last dimension of t should be one more than that of w\n    # the last dimension of w should be one less than that of t\n    # the last dimension of t should be one more than that of w\n    # the last dimension of w should be one less than that of t\n    # the last dimension of t should be one more than that of w\n    # the last dimension of w should be one less than that of t\n    # the last dimension of t should be one more than that of w\n    # the last dimension of w should be one less than that of t\n    # the last dimension of t should be one more than that of w\n    # the last dimension of w should be one less than that of t\n    # the last dimension of t should be one more than that of w\n    # the last dimension of w should be one less than that of t\n    # the last dimension of t should be one more than that of w\n    # the last dimension of w should be one less than that of t\n    # the last dimension of t should be one more than that of w\n    # the last dimension of w should be one less than that of t\n    # the last dimension of t should be one more than that of w\n    # the last dimension of w should be one less than that of t\n    # the last dimension of t should be one more than that of w\n    # the last dimension of w should be one less than that of t\n    # the last dimension of t should be one more than that of w\n    # the last dimension of w should be one less than that of t\n    # the last dimension of t should be one more than that of w\n    # the last dimension of w should be one less than that of t\n    # the last dimension of t should be one more than that of w\n    # the last dimension of w should be one less than that of t\n    # the last dimension of t should be one more than that of w\n    # the last dimension of w should be one less than that of t\n    # the last dimension of t should be one more"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # intra-interval loss\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n    # t"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # inter-interval loss\n    t_inter = t[..., 1:] - t[..., :-1]\n    w_inter = w[..., 1:]\n    loss_inter = torch.sum(w_inter * t_inter ** 2, dim=-1)\n\n    # intra-interval loss\n    t_intra = t[..., 1:] - t[..., :-1]\n    w_intra = w[..., 1:]\n    loss_intra = torch.sum(w_intra * (t_intra[..., 1:] - t_intra[..., :-1]) ** 2, dim=-1)\n\n    # total loss\n    loss_distortion = loss_inter + loss_intra\n\n    return loss_distortion\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # inter-interval loss\n    t_inter = t[..., 1:] - t[..., :-1]\n    w_inter = w[..., 1:]\n    loss_inter = torch.sum(w_inter * torch.abs(t_inter), dim=-1)\n\n    # intra-interval loss\n    t_intra = t[..., 1:] - t[..., :-1]\n    w_intra = w[..., :-1]\n    loss_intra = torch.sum(w_intra * torch.abs(t_intra), dim=-1)\n\n    # total distortion loss\n    loss_distortion = loss_inter + loss_intra\n\n    return loss_distortion\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # compute the inter-interval loss\n    t_diff = torch.diff(t, dim=-1)\n    t_diff_sq = t_diff ** 2\n    t_diff_sq_sum = torch.sum(t_diff_sq, dim=-1)\n    t_diff_sq_sum_mean = torch.mean(t_diff_sq_sum)\n\n    # compute the intra-interval loss\n    w_diff = torch.diff(w, dim=-1)\n    w_diff_sq = w_diff ** 2\n    w_diff_sq_sum = torch.sum(w_diff_sq, dim=-1)\n    w_diff_sq_sum_mean = torch.mean(w_diff_sq_sum)\n\n    # combine the inter-interval and intra-interval losses\n    distortion_loss = t_diff_sq_sum_mean + w_diff_sq_sum_mean\n\n    return distortion_loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # inter-interval loss\n    t_i = t[..., :-1]\n    t_o = t[..., 1:]\n    w_i = w[..., :-1]\n    w_o = w[..., 1:]\n    t_diff = t_o - t_i\n    w_diff = w_o - w_i\n    w_diff_sign = torch.sign(w_diff)\n    w_diff_abs = torch.abs(w_diff)\n    w_diff_abs_sign = torch.sign(w_diff_abs)\n    w_diff_abs_sign_abs = torch.abs(w_diff_abs_sign)\n    w_diff_abs_sign_abs_sign = torch.sign(w_diff_abs_sign_abs)\n    w_diff_abs_sign_abs_sign_abs = torch.abs(w_diff_abs_sign_abs_sign)\n    w_diff_abs_sign_abs_sign_abs_sign = torch.sign(w_diff_abs_sign_abs_sign_abs)\n    w_diff_abs_sign_abs_sign_abs_sign_abs = torch.abs(w_diff_abs_sign_abs_sign_abs_sign)\n    w_diff_abs_sign_abs_sign_abs_sign_abs_sign = torch.sign(w_diff_abs_sign_abs_sign_abs_sign_abs)\n    w_diff_abs_sign_abs_sign_abs_sign_abs_sign_abs = torch.abs(w_diff_abs_sign_abs_sign_abs_sign_abs_sign)\n    w_diff_abs_sign_abs_sign_abs_sign_abs_sign_abs_sign = torch.sign(w_diff_abs_sign_abs_sign_abs_sign_abs_sign_abs)\n    w_diff_abs_sign_abs_sign_abs_sign_abs_sign_abs_sign_abs = torch.abs(w_diff_abs_sign_abs_sign_"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # get the number of dimensions of the tensor\n    ndim = t.ndim\n\n    # get the number of dimensions of the weights\n    ndim_w = w.ndim\n\n    # check if the number of dimensions of the tensor is one more than that of the weights\n    if ndim != ndim_w + 1:\n        raise ValueError(\n            f\"The number of dimensions of the tensor 't' ({ndim}) should be one more than that of the weights 'w' ({ndim_w}).\"\n        )\n\n    # get the number of dimensions of the weights\n    ndim_w = w.ndim\n\n    # check if the number of dimensions of the tensor is one more than that of the weights\n    if ndim != ndim_w + 1:\n        raise ValueError(\n            f\"The number of dimensions of the tensor 't' ({ndim}) should be one more than that of the weights 'w' ({ndim_w}).\"\n        )\n\n    # get the number of dimensions of the weights\n    ndim_w = w.ndim\n\n    # check if the number of dimensions of the tensor is one more than that of the weights\n    if ndim != ndim_w + 1:\n        raise ValueError(\n            f\"The number of dimensions of the tensor 't' ({ndim}) should be one more than that of the weights 'w' ({ndim_w}).\"\n        )\n\n    # get the number of dimensions of the weights\n    ndim_w = w.ndim\n\n    # check if the number of dimensions of the tensor is one more than that of the weights\n    if ndim != ndim_w + 1:\n        raise ValueError(\n            f\"The number of dimensions of the tensor 't' ({ndim}) should be one more than that of the weights 'w' ({ndim_w}).\"\n        )\n\n    # get the number of dimensions of the weights\n    ndim_w = w.ndim\n\n    # check if the number of dimensions of the tensor is one more than that of the weights\n    if ndim != ndim_w"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # compute intra-interval losses\n    intra_interval_loss = torch.sum(torch.abs(t[:, :-1] - t[:, 1:]) * w, dim=-1)\n\n    # compute inter-interval losses\n    inter_interval_loss = torch.sum(torch.abs(t[:, :-1] - t[:, 1:]) * torch.abs(w[:, :-1] - w[:, 1:]), dim=-1)\n\n    # combine intra-interval and inter-interval losses\n    return intra_interval_loss + inter_interval_loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[:-1] = w.shape\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # calculate the inter-interval loss\n    inter_loss = torch.sum(\n        torch.abs(\n            t[..., 1:] - t[..., :-1]\n        ) * w,\n        dim=-1\n    )\n\n    # calculate the intra-interval loss\n    intra_loss = torch.sum(\n        torch.abs(\n            t[..., 1:] - t[..., :-1]\n        ) * (1 - w),\n        dim=-1\n    )\n\n    # combine the inter-interval and intra-interval losses\n    return inter_loss + intra_loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t.shape[-1] == w.shape[-1] + 1\n    # t.shape[:-1] == w.shape[:-1]\n\n    # calculate the inter-interval loss\n    t_inter = torch.cat([t[..., :1], t[..., 1:]], dim=-1)\n    w_inter = torch.cat([w[..., :1], w[..., :-1]], dim=-1)\n    loss_inter = (t_inter - w_inter).pow(2).sum(-1)\n\n    # calculate the intra-interval loss\n    t_intra = t[..., 1:]\n    w_intra = w[..., :-1]\n    loss_intra = (t_intra - w_intra).pow(2).sum(-1)\n\n    # combine the inter- and intra-interval losses\n    loss_distortion = loss_inter + loss_intra\n\n    return loss_distortion\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # compute inter-interval loss\n    t_int = t[..., 1:] - t[..., :-1]\n    w_int = w[..., 1:]\n    w_int_sum = torch.sum(w_int, dim=-1)\n    t_int_sum = torch.sum(t_int, dim=-1)\n    w_int_sum_sq = torch.sum(w_int ** 2, dim=-1)\n    t_int_sum_sq = torch.sum(t_int ** 2, dim=-1)\n    loss_int = (\n        torch.sum(w_int * (t_int - t_int_sum / w_int_sum) ** 2, dim=-1)\n        / w_int_sum_sq\n        - 2 * torch.sum(w_int * t_int, dim=-1) / w_int_sum_sq\n        + torch.sum(w_int, dim=-1) / w_int_sum_sq\n    )\n\n    # compute intra-interval loss\n    t_in = t[..., 1:]\n    w_in = w[..., 1:]\n    w_in_sum = torch.sum(w_in, dim=-1)\n    t_in_sum = torch.sum(t_in, dim=-1)\n    w_in_sum_sq = torch.sum(w_in ** 2, dim=-1)\n    t_in_sum_sq = torch.sum(t_in ** 2, dim=-1)\n    loss_in = (\n        torch.sum(w_in * (t_in - t_in_sum / w_in_sum) ** 2, dim=-1)\n        / w_in_sum_sq\n        - 2 * torch.sum(w_in * t_in, dim=-1) / w_in_sum_sq\n        + torch.sum(w_in, dim=-1) / w_in_sum_"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # get the dimensions of the tensors\n    t_dim = t.shape[-1]\n    w_dim = w.shape[-1]\n\n    # check if the dimensions of 't' and 'w' are compatible\n    if t_dim != w_dim + 1:\n        raise ValueError(\"The last dimension of 't' should be one more than that of 'w'.\")\n\n    # calculate the intra-interval loss\n    intra_loss = torch.sum(torch.abs(t[..., 1:] - t[..., :-1]) * w)\n\n    # calculate the inter-interval loss\n    inter_loss = torch.sum(torch.abs(t[..., 1:] - t[..., :-1]) * torch.abs(w[..., 1:] - w[..., :-1]))\n\n    # return the total distortion loss\n    return intra_loss + inter_loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    ## TODO: use the 'w' tensor to weight the distortion loss calculation\n\n    ## TODO: calculate the inter-interval loss\n    ## TODO: calculate the intra-interval loss\n    ## TODO: combine the inter-interval and intra-interval losses\n\n    ## TODO: return the distortion loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # w.shape[-1] + 1 == t.shape[-1]\n    # w.shape[-1] == t.shape[-1] - 1\n\n    # compute intra-interval loss\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[:-1] == w.shape\n    # t.shape[-1] - 1 == w.shape[-1]\n    # t[..., :-1] == w\n    # t[..., 1:] == w + 1\n    # t[..., 1:] - t[..., :-1] == w + 1 - w\n    # t[..., 1:] - t[..., :-1] == torch.ones_like(w)\n    intra_interval_loss = torch.sum(torch.square(torch.ones_like(w) - (t[..., 1:] - t[..., :-1])), dim=-1)\n\n    # compute inter-interval loss\n    # t.shape[-1] = w.shape[-1] + 1\n    # t.shape[:-1] == w.shape\n    # t.shape[-1] - 1 == w.shape[-1]\n    # t[..., :-1] == w\n    # t[..., 1:] == w + 1\n    # t[..., 1:] - t[..., :-1] == w + 1 - w\n    # t[..., 1:] - t[..., :-1] == torch.ones_like(w)\n    # torch.square(torch.ones_like(w) - (t[..., 1:] - t[..., :-1])) == torch.square(torch.ones_like(w))\n    # torch.square(torch.ones_like(w) - (t[..., 1:] - t[..., :-1])) == torch.square(torch.ones_like(w))\n    # torch.square(torch.ones_like(w) - (t[..., 1:] -"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # calculate inter-interval loss\n    t_int = t[..., :-1]\n    t_ext = t[..., 1:]\n    w_int = w[..., :-1]\n    w_ext = w[..., 1:]\n    loss_int = (t_int - w_int).pow(2)\n    loss_ext = (t_ext - w_ext).pow(2)\n    loss_int = loss_int.sum(dim=-1)\n    loss_ext = loss_ext.sum(dim=-1)\n    loss_int = loss_int.mean(dim=-1)\n    loss_ext = loss_ext.mean(dim=-1)\n    loss_int = loss_int.mean()\n    loss_ext = loss_ext.mean()\n    loss_int = loss_int * 1000\n    loss_ext = loss_ext * 1000\n    loss_int = loss_int.unsqueeze(-1)\n    loss_ext = loss_ext.unsqueeze(-1)\n    loss = torch.cat((loss_int, loss_ext), dim=-1)\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # compute the inter-interval loss\n    # the last dimension of t is one more than that of w\n    # so we can use t[..., :-1] to get the first n-1 dimensions of t\n    # and w to get the last dimension of t\n    # we can then use torch.diff to get the difference between consecutive elements in the last dimension of t\n    # we can then use torch.square to square the difference\n    # we can then use torch.sum to sum the squared differences over the last dimension\n    # we can then use torch.mean to average the sum over the first n-1 dimensions\n    # we can then use torch.sum to sum the average over the last dimension\n    # we can then use torch.mean to average the sum over the first n-1 dimensions\n    # we can then use torch.sum to sum the average over the last dimension\n    # we can then use torch.mean to average the sum over the first n-1 dimensions\n    # we can then use torch.sum to sum the average over the last dimension\n    # we can then use torch.mean to average the sum over the first n-1 dimensions\n    # we can then use torch.sum to sum the average over the last dimension\n    # we can then use torch.mean to average the sum over the first n-1 dimensions\n    # we can then use torch.sum to sum the average over the last dimension\n    # we can then use torch.mean to average the sum over the first n-1 dimensions\n    # we can then use torch.sum to sum the average over the last dimension\n    # we can then use torch.mean to average the sum over the first n-1 dimensions\n    # we can then use torch.sum to sum the average over the last dimension\n    # we can then use torch.mean to average the sum over the first n-1 dimensions\n    # we can then use torch.sum to sum the average over the last dimension\n    # we can then use torch.mean to average the sum over the first n-1 dimensions\n    # we can then use torch.sum to sum the average over the last dimension\n    # we can then use torch"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n\n    # t.shape = [batch_size, num_points, 2]\n    # w.shape = [batch_size, num_points, 1]\n\n    # inter-interval loss\n    # t.shape = [batch_size, num_points, 2]\n    # w.shape = [batch_size, num_points, 1]\n    # t_diff.shape = [batch_size, num_points, 2]\n    # t_diff_w.shape = [batch_size, num_points, 2]\n    t_diff = torch.diff(t, dim=-2)\n    t_diff_w = t_diff * w\n    t_diff_w_sq = t_diff_w ** 2\n    t_diff_w_sq_sum = torch.sum(t_diff_w_sq, dim=-1)\n    # t_diff_w_sq_sum.shape = [batch_size, num_points]\n\n    # intra-interval loss\n    # t.shape = [batch_size, num_points, 2]\n    # w.shape = [batch_size, num_points, 1]\n    # t_diff.shape = [batch_size, num_points, 2]\n    # t_diff_w.shape = [batch_size, num_points, 2]\n    t_diff_w_sq_sum_intra = torch.sum(t_diff_w_sq, dim=-1)\n    # t_diff_w_sq_sum_intra.shape = [batch_size, num_points]\n\n    # total distortion loss\n    # t_diff_w_sq_sum.shape = [batch_size, num_points]\n    # t_diff_w_sq_sum_intra.shape = [batch_size, num_points]\n    distortion_loss = torch.mean(t_diff_w_sq_sum) + torch.mean(t_diff_w_sq_sum_int"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # compute the inter-interval loss\n    t_ = torch.cat((t[..., :-1], t[..., 1:]), dim=-1)\n    t_ = (t_ - t[..., None]) / (t_ - t[..., None, :0:-1])\n    t_ = t_.pow(2)\n    t_ = t_.prod(dim=-1)\n    t_ = t_.pow(1 / (t_.shape[-1] - 1))\n\n    # compute the intra-interval loss\n    t_ = (t[..., 1:] - t[..., :-1]) / (t[..., 1:] + t[..., :-1])\n    t_ = t_.pow(2)\n    t_ = t_.prod(dim=-1)\n    t_ = t_.pow(1 / (t_.shape[-1] - 1))\n\n    # combine the inter-interval and intra-interval losses\n    t_ = (t_[..., None] * w).sum(dim=-1)\n\n    return t_\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t = t.unsqueeze(-1)\n    w = w.unsqueeze(-1)\n    ps = torch.tensor(ps, dtype=t.dtype, device=t.device)\n    ps = ps.unsqueeze(-1)\n    w_cum = torch.cumsum(w, dim=1)\n    w_cum_interp = torch.interpolate(w_cum, size=ps.shape[1], mode='linear', align_corners=False)\n    t_interp = torch.interpolate(t, size=ps.shape[1], mode='linear', align_corners=False)\n    return torch.sum(w_cum_interp * t_interp, dim=1)"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Compute the cumulative weights\n    cw = torch.cumsum(w, dim=0)\n\n    # Interpolate the cumulative weights to find the weighted percentiles\n    ps_tensor = torch.tensor(ps, dtype=torch.float32).to(t.device)\n    wps = torch.zeros_like(ps_tensor)\n    for i in range(len(ps)):\n        idx = torch.searchsorted(cw, ps_tensor[i])\n        if idx == 0:\n            wps[i] = t[0]\n        elif idx == len(cw):\n            wps[i] = t[-1]\n        else:\n            wps[i] = t[idx - 1] + (t[idx] - t[idx - 1]) * (cw[idx] - ps_tensor[i]) / (cw[idx] - cw[idx - 1])\n\n    return wps"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the tensors have the same shape\n    if t.shape != w.shape:\n        raise ValueError(\"Input tensors 't' and 'w' must have the same shape.\")\n\n    # Check if the weights sum to 1\n    if not torch.allclose(w.sum(dim=-1), torch.ones_like(w.sum(dim=-1))):\n        raise ValueError(\"Weights must sum to 1.\")\n\n    # Check if the percentile values are within the valid range\n    if not all(0 <= p <= 1 for p in ps):\n        raise ValueError(\"Percentile values must be within the range [0, 1].\")\n\n    # Check if the tensors have at least one dimension\n    if len(t.shape) == 0:\n        raise ValueError(\"Input tensors must have at least one dimension.\")\n\n    # Check if the tensors have at least two dimensions\n    if len(t.shape) == 1:\n        raise ValueError(\"Input tensors must have at least two dimensions.\")\n\n    # Check if the tensors have at least three dimensions\n    if len(t.shape) == 2:\n        raise ValueError(\"Input tensors must have at least three dimensions.\")\n\n    # Check if the tensors have at least four dimensions\n    if len(t.shape) == 3:\n        raise ValueError(\"Input tensors must have at least four dimensions.\")\n\n    # Check if the tensors have at least five dimensions\n    if len(t.shape) == 4:\n        raise ValueError(\"Input tensors must have at least five dimensions.\")\n\n    # Check if the tensors have at least six dimensions\n    if len(t.shape) == 5:\n        raise ValueError(\"Input tensors must have at least six dimensions.\")\n\n    # Check if the tensors have at least seven dimensions\n    if len(t.shape) == 6:\n        raise ValueError(\"Input tensors must have at least seven dimensions.\")\n\n    # Check if the tensors have at least eight dimensions\n    if len(t.shape) == 7:\n        raise ValueError(\"Input tensors must have at least eight dimensions.\")"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check that the tensors have the same shape\n    assert t.shape == w.shape, \"The tensors 't' and 'w' must have the same shape.\"\n\n    # Check that the weights sum to 1\n    assert torch.allclose(torch.sum(w, dim=-1), torch.ones_like(torch.sum(w, dim=-1))), \"The weights must sum to 1.\"\n\n    # Check that the percentile values are between 0 and 1\n    assert all(0 <= p <= 1 for p in ps), \"The percentile values must be between 0 and 1.\"\n\n    # Compute the integrated weights\n    w_int = torch.cumsum(w, dim=-1)\n\n    # Interpolate the integrated weights to find the weighted percentiles\n    ps_int = torch.tensor(ps, dtype=w.dtype, device=w.device)\n    ps_int = ps_int * w_int[-1]\n    weighted_percentiles = torch.interp(ps_int, w_int, t)\n\n    return weighted_percentiles"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the tensors have the same shape\n    if t.shape != w.shape:\n        raise ValueError(\"The tensors 't' and 'w' must have the same shape.\")\n\n    # Check if the weights sum to 1\n    if not torch.isclose(torch.sum(w), torch.tensor(1.0).to(t.device)):\n        raise ValueError(\"The weights must sum to 1.\")\n\n    # Check if the percentiles are valid\n    if not all(0 <= p <= 1 for p in ps):\n        raise ValueError(\"The percentiles must be between 0 and 1.\")\n\n    # Sort the tensors by the values in 't'\n    sorted_indices = torch.argsort(t)\n    t = t[sorted_indices]\n    w = w[sorted_indices]\n\n    # Compute the cumulative weights\n    cumulative_weights = torch.cumsum(w, dim=-1)\n\n    # Interpolate the cumulative weights to find the weighted percentiles\n    ps_tensor = torch.tensor(ps).to(t.device)\n    weighted_percentiles = torch.interp(ps_tensor, cumulative_weights, t)\n\n    return weighted_percentiles"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the tensors have the same shape\n    if t.shape != w.shape:\n        raise ValueError(\"Input tensors 't' and 'w' must have the same shape.\")\n\n    # Check if the sum of the weights is 1\n    if not torch.isclose(torch.sum(w), torch.tensor(1.0).to(w.device)):\n        raise ValueError(\"The sum of the weights must be 1.\")\n\n    # Check if the percentile values are within the valid range\n    if not all(0 <= p <= 1 for p in ps):\n        raise ValueError(\"Percentile values must be within the range [0, 1].\")\n\n    # Compute the cumulative weights\n    cum_w = torch.cumsum(w, dim=0)\n\n    # Interpolate the cumulative weights to find the weighted percentiles\n    ps = torch.tensor(ps).to(w.device)\n    weighted_percentiles = torch.interp(ps, cum_w, t)\n\n    return weighted_percentiles"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the tensors have the same shape\n    if t.shape != w.shape:\n        raise ValueError(\"t and w must have the same shape\")\n\n    # Check if the weights sum to 1\n    if not torch.allclose(torch.sum(w, dim=0), torch.ones_like(w[0])):\n        raise ValueError(\"The weights must sum to 1\")\n\n    # Check if the percentiles are valid\n    for p in ps:\n        if p < 0 or p > 1:\n            raise ValueError(\"Percentile values must be between 0 and 1\")\n\n    # Sort the tensors based on the values in 't'\n    sorted_indices = torch.argsort(t, dim=0)\n    t = t[sorted_indices]\n    w = w[sorted_indices]\n\n    # Compute the cumulative weights\n    cum_w = torch.cumsum(w, dim=0)\n\n    # Compute the weighted percentiles\n    ps = torch.tensor(ps)\n    weighted_percentiles = torch.interp(ps, cum_w, t)\n\n    return weighted_percentiles"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Initialize an empty list to store the weighted percentiles\n    weighted_percentiles = []\n\n    # Iterate over the percentile values\n    for p in ps:\n        # Compute the weighted percentile for the current percentile value\n        weighted_percentile = weighted_percentile_single(t, w, p)\n\n        # Append the weighted percentile to the list\n        weighted_percentiles.append(weighted_percentile)\n\n    # Stack the weighted percentiles into a tensor and return it\n    return torch.stack(weighted_percentiles)\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the tensors have the same shape\n    if t.shape != w.shape:\n        raise ValueError(\"The tensors 't' and 'w' must have the same shape.\")\n\n    # Check if the weights sum to 1\n    if not torch.allclose(torch.sum(w, dim=-1), torch.ones_like(torch.sum(w, dim=-1))):\n        raise ValueError(\"The weights must sum to 1.\")\n\n    # Check if the percentile values are within the valid range\n    if not all(0 <= p <= 1 for p in ps):\n        raise ValueError(\"The percentile values must be within the range [0, 1].\")\n\n    # Check if the tensors are sorted in ascending order\n    if not torch.all(torch.diff(t, dim=-1) >= 0):\n        raise ValueError(\"The tensors 't' and 'w' must be sorted in ascending order.\")\n\n    # Compute the cumulative weights\n    cum_w = torch.cumsum(w, dim=-1)\n\n    # Compute the weighted percentiles\n    ps = torch.tensor(ps, dtype=t.dtype, device=t.device)\n    ps = torch.interp(ps, torch.cat([torch.zeros_like(ps), cum_w[..., :-1]], dim=-1), torch.cat([t, t[..., 1:]], dim=-1))\n\n    return ps"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the tensors have the same shape\n    assert t.shape == w.shape, \"t and w must have the same shape\"\n    # Check if the weights sum to 1\n    assert torch.allclose(torch.sum(w, dim=-1), torch.ones_like(w[..., :1])), \"Weights must sum to 1\"\n\n    # Calculate the cumulative weights\n    cum_w = torch.cumsum(w, dim=-1)\n\n    # Interpolate the cumulative weights to find the weighted percentiles\n    ps = torch.tensor(ps, dtype=torch.float32, device=t.device)\n    ps = ps.unsqueeze(-1)\n    ps = ps / 100\n    ps = torch.clamp(ps, 0, 1)\n    weighted_percentiles = torch.interpolate(cum_w, size=ps.shape[-1], mode=\"linear\", align_corners=False)\n\n    return weighted_percentiles\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the tensors have the same shape\n    if t.shape != w.shape:\n        raise ValueError(\"Input tensors must have the same shape\")\n\n    # Check if the weights sum to 1\n    if not torch.isclose(torch.sum(w), torch.tensor(1.0)):\n        raise ValueError(\"Weights must sum to 1\")\n\n    # Check if the percentile values are valid\n    if not all(0 <= p <= 1 for p in ps):\n        raise ValueError(\"Percentile values must be between 0 and 1\")\n\n    # Compute the integrated weights\n    integrated_weights = torch.cumsum(w, dim=0)\n\n    # Interpolate the integrated weights to find the weighted percentiles\n    percentiles = []\n    for p in ps:\n        # Find the indices of the values in 't' that bracket the percentile value\n        indices = torch.searchsorted(integrated_weights, p)\n\n        # If the percentile value is exactly at a weight boundary, use the next index\n        if indices[0] == integrated_weights.shape[0] - 1:\n            indices[0] += 1\n\n        # Interpolate between the two indices to find the weighted percentile\n        weight_diff = integrated_weights[indices[1]] - integrated_weights[indices[0]]\n        if weight_diff == 0:\n            percentiles.append(t[indices[0]])\n        else:\n            weight_ratio = (p - integrated_weights[indices[0]]) / weight_diff\n            percentiles.append(t[indices[0]] + weight_ratio * (t[indices[1]] - t[indices[0]]))\n\n    return torch.stack(percentiles)"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Reshape the tensors to have the same shape\n    t = t.reshape(t.shape[0], -1)\n    w = w.reshape(w.shape[0], -1)\n\n    # Sort the tensors along the second dimension\n    sorted_t, sorted_w = torch.sort(torch.cat((t, w), dim=-1), dim=-1)\n\n    # Compute the cumulative sum of the weights along the second dimension\n    cumsum_w = torch.cumsum(sorted_w[:, 1:], dim=-1)\n\n    # Interpolate the cumulative sum of the weights to find the weighted percentiles\n    weighted_percentiles = torch.zeros_like(t)\n    for i, p in enumerate(ps):\n        # Find the indices of the values in 't' that correspond to the percentile value 'p'\n        indices = torch.searchsorted(cumsum_w, p, right=True)\n        # Interpolate the values of 't' at the indices to find the weighted percentile\n        weighted_percentiles[:, i] = torch.lerp(\n            sorted_t[:, indices], sorted_t[:, indices + 1], (p - cumsum_w[:, indices]) / (cumsum_w[:, indices + 1] - cumsum_w[:, indices])\n        )\n\n    return weighted_percentiles"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the tensors have the same shape\n    if t.shape != w.shape:\n        raise ValueError(\"The tensors 't' and 'w' must have the same shape.\")\n\n    # Check if the sum of the weights is equal to 1\n    if not torch.isclose(torch.sum(w), torch.tensor(1.0)):\n        raise ValueError(\"The sum of the weights must be equal to 1.\")\n\n    # Check if the percentile values are between 0 and 1\n    for p in ps:\n        if p < 0 or p > 1:\n            raise ValueError(\"The percentile values must be between 0 and 1.\")\n\n    # Create a tensor of zeros with the same shape as 't' and 'w'\n    weighted_percentiles = torch.zeros_like(t)\n\n    # Iterate over the channels of 't' and 'w'\n    for i in range(t.shape[0]):\n        # Integrate the weights over the values in 't'\n        integrated_weights = torch.cumsum(w[i], dim=0)\n\n        # Find the indices of the values in 't' that correspond to the percentile values\n        indices = torch.searchsorted(t[i], ps, right=True)\n\n        # Interpolate the integrated weights to find the weighted percentiles\n        weighted_percentiles[i] = torch.interp(ps, t[i], integrated_weights)\n\n    return weighted_percentiles\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Initialize the output tensor\n    out = torch.zeros_like(t)\n\n    # Initialize the cumulative weights tensor\n    cum_w = torch.cumsum(w, dim=0)\n\n    # Iterate over the percentile values\n    for p in ps:\n        # Find the index of the first element in 't' that is greater than or equal to the current percentile value\n        idx = torch.searchsorted(t, p)\n\n        # If the index is 0, the percentile is less than the first element in 't'\n        if idx == 0:\n            out[idx] = t[idx]\n        # If the index is equal to the length of 't', the percentile is greater than the last element in 't'\n        elif idx == t.shape[0]:\n            out[idx - 1] = t[idx - 1]\n        # Otherwise, interpolate between the two elements in 't' that bracket the current percentile value\n        else:\n            out[idx - 1] = t[idx - 1] + (p - t[idx - 1]) / (t[idx] - t[idx - 1]) * (t[idx] - t[idx - 1])\n\n    return out\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    if not torch.all(torch.isfinite(t)):\n        raise ValueError(\"Input tensor 't' must be finite.\")\n    if not torch.all(torch.isfinite(w)):\n        raise ValueError(\"Input tensor 'w' must be finite.\")\n    if not torch.allclose(torch.sum(w, dim=-1), torch.ones_like(torch.sum(w, dim=-1))):\n        raise ValueError(\"Input tensor 'w' must sum to 1 along the last dimension.\")\n    if not all(0 <= p <= 1 for p in ps):\n        raise ValueError(\"Percentile values must be between 0 and 1.\")\n\n    # Expand dimensions of tensors to match\n    if len(t.shape) == 1:\n        t = t.unsqueeze(0)\n        w = w.unsqueeze(0)\n    if len(ps) == 1:\n        ps = [ps[0]] * t.shape[0]\n\n    # Compute the cumulative sum of the weights along the last dimension\n    cumsum_w = torch.cumsum(w, dim=-1)\n\n    # Interpolate the cumulative sum of the weights to find the weighted percentiles\n    ps_tensor = torch.tensor(ps).unsqueeze(-1)\n    weighted_percentiles = torch.zeros_like(t)\n    for i in range(t.shape[0]):\n        weighted_percentiles[i] = torch.interp(ps_tensor[i], cumsum_w[i], t[i])\n\n    return weighted_percentiles"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check that the inputs have the same shape\n    if t.shape != w.shape:\n        raise ValueError(\"Input tensors 't' and 'w' must have the same shape.\")\n\n    # Check that the sum of the weights is 1\n    if not torch.isclose(torch.sum(w), torch.tensor(1.0)):\n        raise ValueError(\"The sum of the weights must be 1.\")\n\n    # Check that the percentile values are within the range [0, 1]\n    if not all(0 <= p <= 1 for p in ps):\n        raise ValueError(\"Percentile values must be within the range [0, 1].\")\n\n    # Check that the percentile values are sorted in ascending order\n    if not all(p1 <= p2 for p1, p2 in zip(ps, ps[1:])):\n        raise ValueError(\"Percentile values must be sorted in ascending order.\")\n\n    # Compute the cumulative sum of the weights\n    cw = torch.cumsum(w, dim=-1)\n\n    # Compute the weighted percentiles by interpolating between the values of the cumulative sum\n    weighted_percentiles = torch.zeros_like(t)\n    for i, p in enumerate(ps):\n        # Find the indices where the cumulative sum is less than or equal to the percentile value\n        indices = torch.where(cw <= p)[0]\n\n        # If there are no such indices, set the weighted percentile to 0\n        if indices.numel() == 0:\n            weighted_percentiles[:, i] = 0\n        # If there is only one such index, set the weighted percentile to the corresponding value\n        elif indices.numel() == 1:\n            weighted_percentiles[:, i] = t[indices[0]]\n        # If there are multiple such indices, interpolate between the corresponding values\n        else:\n            i1 = indices[-1]\n            i2 = indices[-2]\n            p1 = cw[i1]\n            p2 = cw[i2]\n            t1 = t["}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    assert torch.allclose(torch.sum(w, dim=-1), torch.ones_like(torch.sum(w, dim=-1))), \"The weights must sum to 1.\"\n    assert torch.all(t[..., 1:] >= t[..., :-1]), \"The values in 't' must be sorted.\"\n    assert torch.all(w[..., 1:] >= w[..., :-1]), \"The weights in 'w' must be sorted.\"\n    assert torch.all(torch.logical_and(torch.ge(ps, 0), torch.le(ps, 100))), \"The percentiles must be between 0 and 100.\"\n\n    # Compute the cumulative weights\n    cum_weights = torch.cumsum(w, dim=-1)\n\n    # Compute the weighted percentiles\n    weighted_percentiles = torch.zeros_like(t)\n    for p in ps:\n        # Find the indices where the percentile value is greater than the cumulative weights\n        indices = torch.where(cum_weights >= p / 100)[:-1]\n\n        # Interpolate between the corresponding values in 't'\n        weighted_percentiles[indices] = t[indices] + (t[indices + 1] - t[indices]) * (cum_weights[indices] - p / 100) / (cum_weights[indices + 1] - cum_weights[indices])\n\n    return weighted_percentiles\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t = t.flatten()\n    w = w.flatten()\n    ps = torch.tensor(ps)\n    # Sort the tensors by the values in 't'\n    sorted_indices = torch.argsort(t)\n    t = t[sorted_indices]\n    w = w[sorted_indices]\n    # Integrate the weights\n    cumulative_w = torch.cumsum(w, dim=0)\n    # Interpolate the integrated weights to find the weighted percentiles\n    ps = ps * cumulative_w[-1]\n    ps_indices = torch.searchsorted(cumulative_w, ps)\n    ps_indices = torch.clamp(ps_indices, 0, cumulative_w.shape[0] - 1)\n    t_interp = t[ps_indices]\n    w_interp = cumulative_w[ps_indices]\n    weighted_percentiles = (ps - w_interp) / (w_interp - cumulative_w[ps_indices - 1]) * (t_interp - t[ps_indices - 1]) + t[ps_indices - 1]\n    return weighted_percentiles\n\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    assert t.shape == w.shape, \"The shape of 't' and 'w' must be the same.\"\n    assert torch.allclose(torch.sum(w, dim=-1), torch.ones_like(torch.sum(w, dim=-1))), \"The sum of 'w' must be 1 along the last dimension.\"\n\n    # Compute the cumulative sum of the weights along the last dimension\n    cumsum_w = torch.cumsum(w, dim=-1)\n\n    # Find the indices of the values in 't' that correspond to the percentile values in 'ps'\n    ps = torch.tensor(ps, dtype=torch.float32, device=t.device)\n    ps = torch.clamp(ps, min=0, max=1)\n    indices = torch.searchsorted(cumsum_w, ps, right=True)\n\n    # Interpolate the cumulative sum of the weights to find the weighted percentiles\n    wp = torch.zeros_like(t)\n    for i in range(len(ps)):\n        wp[..., i] = torch.interp(ps[i], cumsum_w[..., indices[i]], t[..., indices[i]])\n\n    return wp\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Match up the channels of the tensors\n    t = t.unsqueeze(-1)\n    w = w.unsqueeze(-1)\n\n    # Integrate the weights\n    w = w.cumsum(dim=1)\n\n    # Normalize the integrated weights to sum to 1\n    w = w / w[:, -1:]\n\n    # Interpolate the integrated weights to find the weighted percentiles\n    ps = torch.tensor(ps).unsqueeze(0)\n    ws = torch.interpolate(w, size=ps.shape, mode='linear', align_corners=False)\n    ws = ws[:, :, 0]\n\n    # Find the indices of the nearest values to the weighted percentiles\n    indices = (ws * (t.shape[1] - 1)).long()\n\n    # Clip the indices to ensure they are within the valid range\n    indices = torch.clamp(indices, 0, t.shape[1] - 2)\n\n    # Interpolate the values at the nearest indices to find the weighted percentiles\n    ws = ws - indices.float()\n    ws = ws.unsqueeze(-1)\n    t = t.gather(1, indices.unsqueeze(-1)).squeeze(-1)\n    t1 = t[:, 1:]\n    t0 = t[:, :-1]\n    return t0 + ws * (t1 - t0)"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Create a tensor of zeros with the same shape as t\n    zeros = torch.zeros_like(t)\n\n    # Create a tensor of ones with the same shape as t\n    ones = torch.ones_like(t)\n\n    # Create a tensor of zeros with the same shape as t\n    zeros = torch.zeros_like(t)\n\n    # Create a tensor of ones with the same shape as t\n    ones = torch.ones_like(t)\n\n    # Create a tensor of zeros with the same shape as t\n    zeros = torch.zeros_like(t)\n\n    # Create a tensor of ones with the same shape as t\n    ones = torch.ones_like(t)\n\n    # Create a tensor of zeros with the same shape as t\n    zeros = torch.zeros_like(t)\n\n    # Create a tensor of ones with the same shape as t\n    ones = torch.ones_like(t)\n\n    # Create a tensor of zeros with the same shape as t\n    zeros = torch.zeros_like(t)\n\n    # Create a tensor of ones with the same shape as t\n    ones = torch.ones_like(t)\n\n    # Create a tensor of zeros with the same shape as t\n    zeros = torch.zeros_like(t)\n\n    # Create a tensor of ones with the same shape as t\n    ones = torch.ones_like(t)\n\n    # Create a tensor of zeros with the same shape as t\n    zeros = torch.zeros_like(t)\n\n    # Create a tensor of ones with the same shape as t\n    ones = torch.ones_like(t)\n\n    # Create a tensor of zeros with the same shape as t\n    zeros = torch.zeros_like(t)\n\n    # Create a tensor of ones with the same shape as t\n    ones = torch.ones_like(t)\n\n    # Create a tensor of zeros with the same shape as t\n    zeros = torch.zeros_like"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the PDF\n    pdf = w / tf.reduce_sum(w)\n\n    # Compute the CDF\n    cdf = tf.cumsum(pdf)\n\n    # Sample from the CDF\n    u = tf.random.uniform(shape=[num_samples], minval=0, maxval=1)\n    indices = tf.searchsorted(cdf, u, side='right')\n\n    # Compute the bin widths\n    bin_widths = tf.gather(t[1:] - t[:-1], indices)\n\n    # Compute the bin offsets\n    bin_offsets = tf.gather(t[:-1], indices)\n\n    # Compute the sample positions\n    if perturb:\n        if single_jitter:\n            sample_positions = u - cdf[indices]\n        else:\n            sample_positions = tf.random.uniform(shape=[num_samples], minval=0, maxval=1)\n    else:\n        sample_positions = u - cdf[indices]\n\n    # Compute the samples\n    samples = bin_offsets + sample_positions * bin_widths\n\n    return samples"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Check if the input tensors are valid\n    if not torch.is_tensor(t):\n        raise TypeError(\"Input t must be a torch.Tensor.\")\n    if not torch.is_tensor(w):\n        raise TypeError(\"Input w must be a torch.Tensor.\")\n    if not isinstance(num_samples, int):\n        raise TypeError(\"num_samples must be an integer.\")\n    if not isinstance(perturb, bool):\n        raise TypeError(\"perturb must be a boolean.\")\n    if not isinstance(single_jitter, bool):\n        raise TypeError(\"single_jitter must be a boolean.\")\n\n    # Check if the input tensors have the correct shapes\n    if t.ndim != 1:\n        raise ValueError(\"Input t must be a 1D tensor.\")\n    if w.ndim != 1:\n        raise ValueError(\"Input w must be a 1D tensor.\")\n    if t.shape[0] != w.shape[0] + 1:\n        raise ValueError(\"Input t and w must have the same length + 1.\")\n    if not torch.all(torch.diff(t) > 0):\n        raise ValueError(\"Input t must be sorted.\")\n\n    # Check if the input tensors have the correct data types\n    if not torch.is_floating_point(t):\n        raise TypeError(\"Input t must be a floating-point tensor.\")\n    if not torch.is_floating_point(w):\n        raise TypeError(\"Input w must be a floating-point tensor.\")\n\n    # Check if the input tensors have the correct device\n    if not t.is_cuda:\n        raise TypeError(\"Input t must be a CUDA tensor.\")\n    if not w.is_cuda:\n        raise TypeError(\"Input w must be a CUDA tensor.\")\n\n    # Check if the input tensors have the correct dtype\n    if t.dtype != torch.float32:\n        raise TypeError(\"Input t must be a float32 tensor.\")\n    if w.dtype != torch.float32:"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Check if t is sorted\n    if not torch.all(torch.diff(t) >= 0):\n        raise ValueError(\"t must be sorted\")\n\n    # Check if w and t have the same shape\n    if w.shape != t.shape:\n        raise ValueError(\"w and t must have the same shape\")\n\n    # Compute the bin widths\n    widths = torch.diff(t)\n\n    # Compute the cumulative bin weights\n    cdf = torch.cumsum(w, dim=0)\n\n    # Compute the PDF\n    pdf = w / cdf[-1]\n\n    # Compute the cumulative PDF\n    cdf = cdf / cdf[-1]\n\n    # Compute the bin indices\n    bin_indices = torch.searchsorted(cdf, torch.rand(num_samples))\n\n    # Compute the bin edges\n    bin_edges = t[bin_indices]\n    bin_edges_prev = t[bin_indices - 1]\n\n    # Compute the bin probabilities\n    bin_probs = pdf[bin_indices]\n\n    # Compute the bin widths\n    bin_widths = widths[bin_indices]\n\n    # Compute the bin probabilities\n    bin_probs = pdf[bin_indices]\n\n    # Compute the bin probabilities\n    bin_probs = pdf[bin_indices]\n\n    # Compute the bin probabilities\n    bin_probs = pdf[bin_indices]\n\n    # Compute the bin probabilities\n    bin_probs = pdf[bin_indices]\n\n    # Compute the bin probabilities\n    bin_probs = pdf[bin_indices]\n\n    # Compute the bin probabilities\n    bin_probs = pdf[bin_indices]\n\n    # Compute the bin probabilities\n    bin_probs = pdf[bin_indices]\n\n    # Compute the bin probabilities\n    bin_probs = pdf[bin_indices]\n\n    # Compute the bin probabilities\n    bin_probs = pdf["}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Check input tensors\n    if not torch.all(torch.diff(t) >= 0):\n        raise ValueError(\"t must be sorted.\")\n\n    if t.shape != w.shape:\n        raise ValueError(\"t and w must have the same shape.\")\n\n    # Compute the PDF\n    pdf = w / torch.sum(w)\n\n    # Compute the cumulative distribution function (CDF)\n    cdf = torch.cumsum(pdf, dim=0)\n\n    # Generate uniform samples\n    u = torch.rand(num_samples, device=t.device)\n\n    # Find the bin indices for each sample\n    indices = torch.searchsorted(cdf, u)\n\n    # Clamp indices to valid range\n    indices = torch.clamp(indices, 0, t.shape[0] - 1)\n\n    # Compute the bin widths\n    bin_widths = torch.diff(t)\n\n    # Compute the bin offsets\n    bin_offsets = torch.zeros_like(t)\n    bin_offsets[1:] = torch.cumsum(bin_widths, dim=0)[:-1]\n\n    # Compute the bin midpoints\n    bin_midpoints = (t[1:] + t[:-1]) / 2\n\n    # Compute the bin centers\n    bin_centers = bin_midpoints + bin_offsets\n\n    # Compute the bin widths\n    bin_widths = torch.diff(t)\n\n    # Compute the bin offsets\n    bin_offsets = torch.zeros_like(t)\n    bin_offsets[1:] = torch.cumsum(bin_widths, dim=0)[:-1]\n\n    # Compute the bin midpoints\n    bin_midpoints = (t[1:] + t[:-1]) / 2\n\n    # Compute the bin centers\n    bin_centers = bin_midpoints + bin_offsets\n\n    # Compute the bin widths\n    bin_widths ="}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    if t.ndim != 1:\n        raise ValueError(\"t must be a 1D tensor\")\n    if w.ndim != 1:\n        raise ValueError(\"w must be a 1D tensor\")\n    if t.shape != w.shape:\n        raise ValueError(\"t and w must have the same shape\")\n\n    # Sort t and w in ascending order\n    sorted_indices = torch.argsort(t)\n    t = t[sorted_indices]\n    w = w[sorted_indices]\n\n    # Normalize w\n    w = w / torch.sum(w)\n\n    # Compute cumulative sum of w\n    cdf = torch.cumsum(w, dim=0)\n\n    # Sample from cdf\n    u = torch.rand(num_samples)\n    indices = torch.searchsorted(cdf, u)\n\n    # Get corresponding t values\n    t_samples = t[indices]\n\n    # Apply perturbation if requested\n    if perturb:\n        t_samples = t_samples + torch.rand_like(t_samples) / num_samples\n\n    # Apply jitter if requested\n    if single_jitter:\n        jitter = torch.rand_like(t_samples) / num_samples\n        t_samples = t_samples + jitter\n    else:\n        jitter = torch.rand_like(t_samples) / num_samples\n        t_samples = t_samples + jitter\n\n    return t_samples"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the cumulative sum of weights\n    cum_weights = torch.cumsum(w, dim=0)\n\n    # Normalize the cumulative weights to [0, 1]\n    cum_weights = cum_weights / cum_weights[-1]\n\n    # Compute the bin widths\n    bin_widths = t[1:] - t[:-1]\n\n    # Generate random numbers from a uniform distribution\n    u = torch.rand(num_samples, device=t.device)\n\n    # Compute the indices of the bins for each sample\n    indices = torch.searchsorted(cum_weights, u)\n\n    # Compute the offsets for each sample\n    offsets = torch.zeros_like(u)\n    if perturb:\n        if single_jitter:\n            offsets = torch.rand(num_samples, device=t.device)\n        else:\n            offsets = torch.rand(num_samples, t.shape[0], device=t.device)\n\n    # Compute the samples\n    samples = t[indices] + offsets * bin_widths[indices]\n\n    return samples\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Define the PDF as a piecewise-constant function defined by bin endpoints and weights\n    def pdf(x):\n        return torch.sum(w * (x >= t[:-1]) * (x < t[1:]), dim=-1)\n\n    # Generate samples from the PDF\n    with torch.no_grad():\n        u = torch.rand(num_samples, *x.shape[:-1])\n        if perturb:\n            # Perturb samples for numerical stability\n            u = u + (torch.rand_like(u) - 0.5) / t.shape[-1]\n        if single_jitter:\n            # Apply a single jitter to all samples along each dimension\n            u = u + (torch.rand_like(u) - 0.5)\n        else:\n            # Apply independent jitter to each sample\n            u = u + (torch.rand_like(u) - 0.5)\n        x = t[0] + (t[1:] - t[0]) * u\n\n    # Compute the probability density at each sample\n    p = pdf(x)\n\n    # Normalize the probability density to sum to 1\n    p = p / torch.sum(p, dim=-1, keepdim=True)\n\n    return x, p"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Calculate the bin widths\n    widths = t[1:] - t[:-1]\n\n    # Calculate the cumulative sum of bin weights\n    cum_weights = torch.cumsum(w, dim=0)\n\n    # Normalize the cumulative weights to [0, 1]\n    cum_weights = cum_weights / cum_weights[-1]\n\n    # Generate uniform random numbers between 0 and 1\n    u = torch.rand(num_samples)\n\n    # Determine the bin indices for each sample\n    indices = torch.searchsorted(cum_weights, u)\n\n    # Calculate the bin endpoints for each sample\n    t_i = t[indices]\n    t_i1 = t[indices + 1]\n\n    # Calculate the probabilities of each sample\n    prob = (u - cum_weights[indices]) / widths[indices]\n\n    # Apply perturbation to the samples\n    if perturb:\n        if single_jitter:\n            jitter = torch.rand_like(prob)\n        else:\n            jitter = torch.rand_like(prob).unsqueeze(-1)\n        prob = prob + jitter\n\n    # Clamp the probabilities to [0, 1]\n    prob = torch.clamp(prob, 0, 1)\n\n    # Calculate the samples\n    samples = t_i + prob * (t_i1 - t_i)\n\n    return samples"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Get the shape of the input tensors\n    shape = t.shape\n\n    # Reshape the tensors to 2D\n    t = t.reshape(-1, t.shape[-1])\n    w = w.reshape(-1, w.shape[-1])\n\n    # Calculate the cumulative sum of weights\n    cdf = torch.cumsum(w, dim=-1)\n\n    # Normalize the cumulative sum to [0, 1]\n    cdf = cdf / cdf[..., -1:]\n\n    # Generate uniform random numbers between 0 and 1\n    u = torch.rand(shape[:-1] + (num_samples,), device=t.device)\n\n    # Find the indices of the bins that correspond to each sample\n    indices = torch.searchsorted(cdf, u, right=True)\n\n    # Calculate the offsets for each sample\n    offsets = torch.gather(cdf, -1, indices)\n\n    # Calculate the jitter for each sample\n    jitter = torch.gather(t, -1, indices)\n\n    if perturb:\n        # Apply perturbation to the jitter\n        jitter = jitter + torch.rand_like(jitter)\n\n    if single_jitter:\n        # Apply the same jitter to every sample along each dimension\n        jitter = jitter + torch.rand_like(jitter) * (1.0 / num_samples)\n    else:\n        # Apply independent jitter to each sample\n        jitter = jitter + torch.rand_like(jitter) * (1.0 / num_samples)\n\n    # Calculate the final sample positions\n    samples = jitter + offsets\n\n    # Reshape the output to the original shape\n    samples = samples.reshape(shape[:-1] + (num_samples,))\n\n    return samples"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Define the number of bins\n    num_bins = t.shape[0] - 1\n\n    # Define the bin widths\n    bin_widths = t[1:] - t[:-1]\n\n    # Define the bin probabilities\n    bin_probs = w / torch.sum(w)\n\n    # Define the cumulative bin probabilities\n    cum_bin_probs = torch.cumsum(bin_probs, dim=0)\n\n    # Generate uniform random numbers\n    u = torch.rand(num_samples, num_bins, device=t.device)\n\n    # Determine which bin each sample falls into\n    bin_indices = torch.sum(u > cum_bin_probs, dim=1)\n\n    # Generate samples from each bin\n    samples = torch.zeros(num_samples, num_bins, device=t.device)\n    for i in range(num_bins):\n        samples[:, i] = torch.rand(num_samples, device=t.device) * bin_widths[i] + t[i]\n\n    # Perturb the samples to avoid clustering at bin boundaries\n    if perturb:\n        samples = samples + torch.randn(num_samples, num_bins, device=t.device) * bin_widths[bin_indices]\n\n    # Apply jitter to the samples\n    if single_jitter:\n        jitter = torch.randn(num_samples, num_bins, device=t.device) * bin_widths[bin_indices]\n    else:\n        jitter = torch.randn(num_samples, num_bins, device=t.device) * bin_widths[:, None]\n\n    # Add the jitter to the samples\n    samples = samples + jitter\n\n    # Return the samples\n    return samples"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the PDF's cumulative distribution function (CDF)\n    cdf = torch.cumsum(w, dim=0)\n    cdf = torch.cat([torch.zeros(1).to(t.device), cdf])\n\n    # Generate uniform samples in the range [0, 1]\n    u = torch.rand(num_samples, device=t.device)\n\n    # Find the indices of the bins that correspond to each sample\n    indices = torch.searchsorted(cdf, u, right=True) - 1\n\n    # Clamp indices to the valid range\n    indices = torch.clamp(indices, 0, cdf.shape[0] - 2)\n\n    # Compute the bin widths\n    bin_widths = t[1:] - t[:-1]\n\n    # Compute the bin offsets\n    bin_offsets = t[:-1]\n\n    # Compute the bin centers\n    bin_centers = bin_offsets + bin_widths / 2\n\n    # Compute the bin left and right boundaries\n    bin_lefts = bin_offsets\n    bin_rights = bin_offsets + bin_widths\n\n    # Compute the bin midpoints\n    bin_midpoints = bin_lefts + bin_widths / 2\n\n    # Compute the bin widths\n    bin_widths = bin_rights - bin_lefts\n\n    # Compute the bin midpoints\n    bin_midpoints = bin_lefts + bin_widths / 2\n\n    # Compute the bin widths\n    bin_widths = bin_rights - bin_lefts\n\n    # Compute the bin midpoints\n    bin_midpoints = bin_lefts + bin_widths / 2\n\n    # Compute the bin widths\n    bin_widths = bin_rights - bin_lefts\n\n    # Compute the bin midpoints\n    bin_midpoints = bin_lefts + bin_widths / 2\n\n    # Compute the bin widths\n    bin_"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the PDF's normalizing constant\n    norm = torch.sum(w)\n\n    # Compute the cumulative distribution function (CDF) from the weights\n    cdf = torch.cumsum(w, dim=0) / norm\n\n    # Generate uniform samples from the interval [0, 1]\n    u = torch.rand(num_samples, device=t.device)\n\n    # Find the indices of the bins that correspond to the uniform samples\n    indices = torch.searchsorted(cdf, u, right=True)\n\n    # Clamp the indices to be within the valid range\n    indices = torch.clamp(indices, 0, t.shape[0] - 1)\n\n    # Compute the bin widths and the offsets\n    bin_widths = t[1:] - t[:-1]\n    offsets = t[:-1]\n\n    # Compute the bin probabilities and the bin probabilities with jitter\n    bin_probs = bin_widths * w\n    bin_probs_jitter = bin_probs + (1e-5 if single_jitter else 1e-5 * torch.rand(bin_probs.shape, device=t.device))\n\n    # Compute the bin probabilities with jitter and normalizing constant\n    bin_probs_jitter_norm = bin_probs_jitter / torch.sum(bin_probs_jitter)\n\n    # Compute the bin probabilities with jitter and normalizing constant\n    bin_probs_jitter_norm = bin_probs_jitter / torch.sum(bin_probs_jitter)\n\n    # Compute the bin probabilities with jitter and normalizing constant\n    bin_probs_jitter_norm = bin_probs_jitter / torch.sum(bin_probs_jitter)\n\n    # Compute the bin probabilities with jitter and normalizing constant\n    bin_probs_jitter_norm = bin_probs_jitter / torch.sum(bin_probs_jitter)\n\n    # Compute the bin probabilities"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Ensure that t is sorted\n    t = t.sort()[0]\n\n    # Compute the bin widths\n    delta_t = t[1:] - t[:-1]\n\n    # Compute the cumulative sum of the bin widths\n    cdf = torch.cumsum(delta_t, dim=0)\n\n    # Normalize the cumulative sum to the range [0, 1]\n    cdf = cdf / cdf[-1]\n\n    # Compute the inverse CDF\n    inverse_cdf = torch.searchsorted(cdf, torch.rand(num_samples))\n\n    # Compute the bin indices\n    bin_indices = torch.clamp(inverse_cdf, max=t.shape[0] - 1)\n\n    # Compute the bin widths for each sample\n    bin_widths = delta_t[bin_indices]\n\n    # Compute the bin endpoints for each sample\n    bin_endpoints = t[bin_indices]\n\n    # Compute the bin weights for each sample\n    bin_weights = w[bin_indices]\n\n    # Compute the sample weights\n    sample_weights = bin_weights / bin_widths\n\n    # Compute the normalized sample weights\n    norm_sample_weights = sample_weights / torch.sum(sample_weights)\n\n    # Compute the normalized cumulative sum of the sample weights\n    norm_cumsum_sample_weights = torch.cumsum(norm_sample_weights, dim=0)\n\n    # Generate uniform random numbers in the range [0, 1]\n    uniform_samples = torch.rand(num_samples)\n\n    # Compute the inverse CDF of the uniform random numbers\n    inverse_cdf_uniform_samples = torch.searchsorted(norm_cumsum_sample_weights, uniform_samples)\n\n    # Compute the bin indices for the uniform random numbers\n    bin_indices_uniform_samples = torch.clamp(inverse_cdf"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Convert the tensors to numpy arrays\n    t = t.cpu().detach().numpy()\n    w = w.cpu().detach().numpy()\n\n    # Compute the cumulative sum of weights\n    cdf = np.cumsum(w)\n\n    # Normalize the cumulative sum to [0, 1]\n    cdf = cdf / cdf[-1]\n\n    # Compute the bin indices for each sample\n    u = np.random.rand(num_samples)\n    bin_indices = np.searchsorted(cdf, u)\n\n    # Generate samples uniformly within each bin\n    samples = np.zeros((num_samples, t.shape[-1]))\n    for i in range(num_samples):\n        if perturb:\n            # Apply perturbation to the sample\n            samples[i] = t[bin_indices[i]] + np.random.rand() * (t[bin_indices[i] + 1] - t[bin_indices[i]])\n        else:\n            # Generate a sample uniformly within the bin\n            samples[i] = t[bin_indices[i]] + np.random.rand() * (t[bin_indices[i] + 1] - t[bin_indices[i]])\n\n        if single_jitter:\n            # Apply the same jitter to every sample along each dimension\n            samples[i] += np.random.randn(t.shape[-1])\n        else:\n            # Apply independent jitter to each sample\n            samples[i] += np.random.randn(t.shape[-1])\n\n    # Convert the samples back to a tensor\n    samples = torch.tensor(samples, dtype=torch.float32)\n\n    return samples\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Check if the input tensors have the correct shape and type\n    assert t.ndim == 1 and w.ndim == 1\n    assert t.shape[0] == w.shape[0]\n    assert torch.all(t[1:] >= t[:-1])\n    assert torch.all(w >= 0)\n\n    # Compute the cumulative sum of weights\n    cdf = torch.cumsum(w, dim=0)\n\n    # Normalize the cumulative sum to obtain the PDF\n    pdf = cdf / cdf[-1]\n\n    # Generate uniform samples from the PDF\n    u = torch.rand(num_samples)\n\n    # Compute the indices of the bins for each sample\n    indices = torch.searchsorted(pdf, u, right=True)\n\n    # Get the bin endpoints for each sample\n    t_min = t[indices - 1]\n    t_max = t[indices]\n\n    # Compute the width of each bin\n    delta = t_max - t_min\n\n    # Apply jitter to the samples\n    if perturb:\n        if single_jitter:\n            jitter = torch.rand_like(delta)\n            t_min = t_min + jitter * delta\n            t_max = t_max + jitter * delta\n        else:\n            jitter = torch.rand_like(u)\n            t_min = t_min + jitter * delta\n            t_max = t_max + jitter * delta\n\n    # Generate samples from the PDF\n    samples = torch.rand(num_samples) * (t_max - t_min) + t_min\n\n    return samples"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    if perturb:\n        # Perturb samples along each dimension to avoid clustering at bin boundaries\n        u = torch.rand(num_samples, t.shape[-1], device=t.device)\n        t = t + (u - 0.5)\n\n    # Compute PDF values at sample points\n    pdf = w * torch.exp(-torch.sum((t[:, None, :] - t[None, :, :]) ** 2, dim=-1))\n\n    # Compute cumulative distribution function (CDF)\n    cdf = torch.cumsum(pdf, dim=-1)\n    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1)\n\n    # Sample uniformly from the CDF\n    u = torch.rand(num_samples, 1, device=t.device)\n    indices = torch.searchsorted(cdf, u, right=True) - 1\n\n    # Compute the jitter for each sample\n    if single_jitter:\n        jitter = torch.rand_like(t[0])\n    else:\n        jitter = torch.rand_like(t)\n\n    # Add jitter to the sampled points\n    t_samples = t[torch.arange(num_samples), indices] + jitter\n\n    return t_samples\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Get the number of bins\n    num_bins = t.shape[0] - 1\n\n    # Compute the bin widths\n    widths = t[1:] - t[:-1]\n\n    # Compute the bin probabilities\n    probs = w / torch.sum(w)\n\n    # Compute the cumulative probabilities\n    cdf = torch.cumsum(probs, dim=0)\n\n    # Sample from the CDF\n    u = torch.rand(num_samples)\n    indices = torch.searchsorted(cdf, u, right=True)\n\n    # Clamp the indices to the valid range\n    indices = torch.clamp(indices, 0, num_bins - 1)\n\n    # Compute the bin edges\n    bin_edges = t[indices]\n    bin_edges_next = t[indices + 1]\n\n    # Compute the bin probabilities\n    bin_probs = probs[indices]\n\n    # Compute the bin widths\n    bin_widths = bin_edges_next - bin_edges\n\n    # Compute the sample probabilities\n    sample_probs = bin_probs / bin_widths\n\n    # Compute the sample weights\n    sample_weights = 1.0 / (num_samples * sample_probs)\n\n    # Compute the sample positions\n    sample_positions = bin_edges + (u - cdf[indices]) / bin_probs * bin_widths\n\n    # Apply perturbation if specified\n    if perturb:\n        # Compute the perturbation amount\n        if single_jitter:\n            jitter = torch.rand_like(sample_positions)\n        else:\n            jitter = torch.rand_like(sample_positions)\n\n        # Apply perturbation\n        sample_positions = sample_positions + jitter\n\n    # Return the samples\n    return sample_positions, sample_weights"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Check if the input tensors have the same shape\n    assert t.shape == w.shape, \"t and w must have the same shape\"\n\n    # Check if the input tensors are sorted\n    assert torch.all(torch.diff(t) >= 0), \"t must be sorted\"\n\n    # Compute the bin widths and the cumulative bin weights\n    bin_widths = torch.diff(t)\n    cum_w = torch.cumsum(w, dim=0)\n\n    # Compute the probability of each bin\n    bin_prob = bin_widths * w\n    bin_prob = bin_prob / torch.sum(bin_prob)\n\n    # Sample the bins\n    bin_samples = torch.multinomial(bin_prob, num_samples=num_samples, replacement=True)\n\n    # Compute the cumulative bin weights for each bin sample\n    cum_w_samples = torch.gather(cum_w, dim=0, index=bin_samples)\n\n    # Generate samples within each bin\n    samples = torch.zeros((num_samples, t.shape[0] - 1))\n\n    if perturb:\n        # Apply perturbation to the samples within each bin\n        for i in range(num_samples):\n            samples[i] = t[1:] - (cum_w_samples[i] - torch.rand(1)) / bin_widths\n    else:\n        # Generate samples without perturbation within each bin\n        for i in range(num_samples):\n            samples[i] = t[1:] - (cum_w_samples[i] - torch.rand(1)) / bin_widths\n\n    # Apply jitter to the samples\n    if single_jitter:\n        # Apply the same jitter to every sample along each dimension\n        jitter = torch.rand_like(samples)\n        samples += jitter\n    else:\n        # Apply independent jitter to each sample\n        jitter = torch.rand_like(samples)\n        samples += jitter"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Check input shapes\n    assert t.shape == w.shape, \"t and w must have the same shape\"\n    assert t.ndim == 1, \"t must be a 1D tensor\"\n    assert w.ndim == 1, \"w must be a 1D tensor\"\n    assert t.shape[0] == w.shape[0], \"t and w must have the same length\"\n\n    # Check if t is sorted\n    assert torch.all(torch.diff(t) >= 0), \"t must be sorted in ascending order\"\n\n    # Check if w is non-negative\n    assert torch.all(w >= 0), \"w must be non-negative\"\n\n    # Check if w sums to 1\n    assert torch.isclose(torch.sum(w), torch.tensor(1.0)), \"w must sum to 1\"\n\n    # Check if num_samples is positive\n    assert num_samples > 0, \"num_samples must be positive\"\n\n    # Check if w is not all zeros\n    assert not torch.allclose(w, torch.zeros_like(w)), \"w cannot be all zeros\"\n\n    # Compute cumulative weights\n    cdf = torch.cumsum(w, dim=0)\n\n    # Compute inverse CDF\n    inv_cdf = torch.zeros_like(cdf)\n    inv_cdf[1:] = torch.diff(cdf)\n    inv_cdf = inv_cdf / inv_cdf[-1]\n\n    # Generate samples\n    u = torch.rand(num_samples)\n    if perturb:\n        u = (u + torch.rand_like(u)) / 2\n    samples = t[0] + torch.sum(u[:, None] > inv_cdf, dim=1) * torch.diff(t)\n\n    # Apply jitter\n    if single_jitter:\n        samples = samples + torch.rand_like(samples) * torch.diff(t)\n    else:\n        samples = samples"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    dilated_t = t * dilation\n    dilated_t = torch.clamp(dilated_t, *domain)\n    dilated_w = w * (t == dilated_t).float()\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    dilated_t = t * dilation\n    dilated_t = torch.clamp(dilated_t, domain[0], domain[1])\n    dilated_w = w / dilation\n    return dilated_t, dilated_w\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    dilated_t = t * dilation\n    dilated_t = torch.clamp(dilated_t, domain[0], domain[1])\n    w = w / dilation\n    return dilated_t, w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Compute the dilated time step indices\n    dilated_t_indices = torch.floor(dilated_t).long()\n\n    # Compute the weights associated with the dilated time steps\n    dilated_w = torch.zeros_like(dilated_t)\n    dilated_w[dilated_t_indices == t] = w\n\n    # Compute the weights associated with the dilated time steps\n    dilated_w = torch.zeros_like(dilated_t)\n    dilated_w[dilated_t_indices == t] = w\n\n    # Compute the weights associated with the dilated time steps\n    dilated_w = torch.zeros_like(dilated_t)\n    dilated_w[dilated_t_indices == t] = w\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps\n    dilated_t = t / dilation\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Compute the dilated weights\n    dilated_w = w * dilation\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Dilate the time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps within the specified domain\n    dilated_t = torch.clamp(dilated_t, *domain)\n\n    # Compute the dilated time step indices\n    dilated_t_indices = torch.floor(dilated_t).long()\n\n    # Compute the dilated time step weights\n    dilated_t_weights = dilated_t - dilated_t_indices\n\n    # Compute the dilated time step indices\n    dilated_t_indices = torch.clamp(dilated_t_indices, min=0)\n\n    # Compute the dilated time step weights\n    dilated_t_weights = torch.clamp(dilated_t_weights, min=0)\n\n    # Compute the dilated time step weights\n    dilated_t_weights = torch.clamp(dilated_t_weights, max=1)\n\n    # Compute the dilated time step weights\n    dilated_t_weights = torch.clamp(dilated_t_weights, min=0)\n\n    # Compute the dilated time step weights\n    dilated_t_weights = torch.clamp(dilated_t_weights, max=1)\n\n    # Compute the dilated time step weights\n    dilated_t_weights = torch.clamp(dilated_t_weights, min=0)\n\n    # Compute the dilated time step weights\n    dilated_t_weights = torch.clamp(dilated_t_weights, max=1)\n\n    # Compute the dilated time step weights\n    dilated_t_weights = torch.clamp(dilated_t_weights, min=0)\n\n    # Compute the dilated time step weights\n    dilated_t_weights = torch.clamp(dilated_t_weights, max=1)\n\n    #"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Calculate the dilated time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Calculate the dilated time steps and weights\n    dilated_t, dilated_w = torch.max(torch.stack([dilated_t, w], dim=-1), dim=-1)\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    if dilation == 1:\n        return t, w\n    # dilate time steps\n    t_dilated = t * dilation\n    # clip time steps to domain\n    t_dilated = torch.clamp(t_dilated, *domain)\n    # adjust weights to match dilated time steps\n    w_dilated = w * (t_dilated / t)\n    return t_dilated, w_dilated\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, domain[0], domain[1])\n\n    # Compute the dilated time steps in the original time scale\n    dilated_t = dilated_t / dilation\n\n    # Compute the adjusted weights based on the dilated time steps\n    adjusted_w = w * (t - dilated_t).detach()\n\n    return dilated_t, adjusted_w"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Dilate the time steps\n    t_dilated = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    t_dilated = torch.clamp(t_dilated, *domain)\n\n    # Calculate the dilated time steps and weights\n    t_dilated, w_dilated = torch.max(\n        torch.stack([t_dilated, w], dim=0), dim=0)\n\n    return t_dilated, w_dilated\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Dilate the time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, domain[0], domain[1])\n\n    # Compute the dilated time step indices\n    dilated_t_indices = dilated_t.long()\n\n    # Compute the dilated time step weights\n    dilated_t_weights = dilated_t - dilated_t_indices.float()\n\n    # Compute the dilated time step weights\n    dilated_t_weights = torch.clamp(dilated_t_weights, 0, 1)\n\n    return dilated_t_indices, dilated_t_weights\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute dilated time steps\n    dilated_t = t * dilation\n\n    # Clip dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Compute dilated time step indices\n    dilated_t_indices = torch.floor(dilated_t).long()\n\n    # Compute the weights associated with each dilated time step\n    dilated_w = torch.zeros_like(dilated_t)\n    dilated_w[torch.arange(dilated_t.shape[0]), dilated_t_indices] = w\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    dilated_t = t * dilation\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n    w = w * (dilated_t == t)\n    return dilated_t, w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    clipped_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Compute the dilated weights\n    dilated_w = w * dilation\n\n    # Compute the indices of the maximum values in the dilated time steps\n    max_indices = dilated_t.argmax(dim=-1)\n\n    # Extract the corresponding maximum values from the dilated weights\n    max_values = dilated_w.gather(-1, max_indices.unsqueeze(-1)).squeeze(-1)\n\n    # Compute the dilated weights by dividing the maximum values\n    dilated_w = dilated_w / max_values.unsqueeze(-1)\n\n    return clipped_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Dilate the time steps\n    t_dilated = t * dilation\n    # Clip the dilated time steps to the specified domain\n    t_dilated = torch.clamp(t_dilated, *domain)\n    # Compute the dilated time step indices\n    t_dilated_indices = torch.floor(t_dilated).long()\n    # Compute the weights corresponding to the dilated time steps\n    w_dilated = torch.zeros_like(t_dilated)\n    w_dilated[t_dilated_indices == t] = w\n    # Compute the weights corresponding to the dilated time steps\n    w_dilated_cumsum = torch.cumsum(w_dilated, dim=-1)\n    # Compute the adjusted weights\n    w_dilated_cumsum_shifted = torch.cat([torch.zeros_like(w_dilated[:, :1]), w_dilated_cumsum[:, :-1]], dim=-1)\n    w_dilated_adjusted = w_dilated - (w_dilated_cumsum - w_dilated_cumsum_shifted)\n    return t_dilated, w_dilated_adjusted"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    if dilation == 1:\n        return t, w\n\n    t_dilated = t * dilation\n    w_dilated = w / dilation\n\n    t_clipped = torch.clamp(t_dilated, min=domain[0], max=domain[1])\n    w_clipped = w_dilated * (t_clipped == t_dilated).float()\n\n    return t_clipped, w_clipped\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Dilate the time steps\n    t_dilated = t * dilation\n\n    # Clip the dilated time steps within the specified domain\n    t_clipped = torch.clamp(t_dilated, min=domain[0], max=domain[1])\n\n    # Compute the indices of the dilated time steps that fall within the specified domain\n    indices = (t_clipped >= domain[0]) & (t_clipped <= domain[1])\n\n    # Compute the weights corresponding to the dilated time steps that fall within the specified domain\n    w_dilated = w * (t_dilated == t_clipped)\n    w_dilated = w_dilated[indices]\n\n    # Return the dilated and clipped time steps and the adjusted weights\n    return t_clipped, w_dilated"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t_dilated = t * dilation\n    t_dilated = torch.clamp(t_dilated, min=domain[0], max=domain[1])\n    w_dilated = w * (t_dilated - t) / dilation\n    return t_dilated, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Dilate the time steps by the specified dilation factor\n    dilated_t = t * dilation\n\n    # Find the maximum value in the dilated time steps\n    max_t = torch.max(dilated_t)\n\n    # Find the indices of the time steps that are within the specified domain\n    valid_indices = torch.logical_and(dilated_t >= domain[0], dilated_t <= domain[1])\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.where(valid_indices, dilated_t, torch.zeros_like(dilated_t))\n\n    # Compute the adjusted weights based on the dilated time steps\n    adjusted_w = torch.where(valid_indices, w / torch.sum(w), torch.zeros_like(w))\n\n    return dilated_t, adjusted_w\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    dilated_t = t * dilation\n    clipped_t = torch.clamp(dilated_t, *domain)\n    dilated_w = w * torch.heaviside(clipped_t - t, torch.tensor(0.0))\n    return clipped_t, dilated_w\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if the query times are within the range of the step function\n    if tq.min() < t.min() or tq.max() > t.max():\n        raise ValueError(\"Query times must be within the range of the step function.\")\n\n    # Find the indices of the step function values that bracket the query times\n    idx = torch.searchsorted(t, tq, right=True) - 1\n\n    # Interpolate the values at the query times\n    yq = (tq - t[idx]) / (t[idx + 1] - t[idx]) * (y[idx + 1] - y[idx]) + y[idx]\n\n    # Set the outside values to the specified value\n    yq[idx == len(t) - 1] = outside_value\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check that the inputs are tensors\n    if not isinstance(tq, torch.Tensor):\n        raise ValueError(\"Input 'tq' must be a tensor.\")\n    if not isinstance(t, torch.Tensor):\n        raise ValueError(\"Input 't' must be a tensor.\")\n    if not isinstance(y, torch.Tensor):\n        raise ValueError(\"Input 'y' must be a tensor.\")\n\n    # Check that the tensors have the same shape\n    if tq.shape != t.shape:\n        raise ValueError(\"Input tensors 'tq' and 't' must have the same shape.\")\n    if tq.shape != y.shape:\n        raise ValueError(\"Input tensors 'tq' and 'y' must have the same shape.\")\n\n    # Check that the tensors are 1D\n    if tq.ndim != 1:\n        raise ValueError(\"Input tensors 'tq', 't', and 'y' must be 1D tensors.\")\n\n    # Check that the tensors are sorted\n    if not torch.all(torch.diff(tq) >= 0):\n        raise ValueError(\"Input tensor 'tq' must be sorted in ascending order.\")\n    if not torch.all(torch.diff(t) >= 0):\n        raise ValueError(\"Input tensor 't' must be sorted in ascending order.\")\n\n    # Check that the tensors are not empty\n    if tq.numel() == 0:\n        raise ValueError(\"Input tensor 'tq' must not be empty.\")\n    if t.numel() == 0:\n        raise ValueError(\"Input tensor 't' must not be empty.\")\n    if y.numel() == 0:\n        raise ValueError(\"Input tensor 'y' must not be empty.\")\n\n    # Check that the tensors have the same device\n    if tq.device != t.device:\n        raise ValueError(\"Input tensors 'tq' and 't' must be on the same device.\")\n    if tq.device != y.device:\n        raise ValueError(\"Input tensors 'tq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Create a copy of the query times tensor\n    tq_copy = tq.clone()\n\n    # Create a tensor of zeros with the same shape as the query times tensor\n    outside_values = torch.zeros_like(tq_copy)\n\n    # Set the outside values to the specified outside value\n    outside_values[tq_copy == t] = outside_value\n\n    # Set the outside values to the values of the step function at the query times\n    outside_values[tq_copy < t] = y[tq_copy < t][-1]\n    outside_values[tq_copy > t] = y[tq_copy > t][0]\n\n    # Return the outside values\n    return outside_values"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check that the input tensors have the same shape\n    assert tq.shape == t.shape\n\n    # Check that the input tensors are 1D\n    assert tq.ndim == 1\n    assert t.ndim == 1\n\n    # Check that the input tensors are sorted\n    assert torch.all(tq >= t[0])\n    assert torch.all(t[1:] >= t[:-1])\n\n    # Check that the input tensors are not empty\n    assert tq.numel() > 0\n    assert t.numel() > 0\n\n    # Check that the input tensors are not all the same\n    assert torch.any(tq != t)\n\n    # Check that the input tensors are not all the same\n    assert torch.any(t != t)\n\n    # Check that the input tensors are not all the same\n    assert torch.any(tq != t)\n\n    # Check that the input tensors are not all the same\n    assert torch.any(t != t)\n\n    # Check that the input tensors are not all the same\n    assert torch.any(tq != t)\n\n    # Check that the input tensors are not all the same\n    assert torch.any(t != t)\n\n    # Check that the input tensors are not all the same\n    assert torch.any(tq != t)\n\n    # Check that the input tensors are not all the same\n    assert torch.any(t != t)\n\n    # Check that the input tensors are not all the same\n    assert torch.any(tq != t)\n\n    # Check that the input tensors are not all the same\n    assert torch.any(t != t)\n\n    # Check that the input tensors are not all the same\n    assert torch.any(tq != t)\n\n    # Check that the input tensors are not all the same\n    assert torch.any(t != t)\n\n    # Check that the input tensors are not all the same\n    assert torch."}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Get the indices of the times in 't' that are less than or equal to the query times in 'tq'.\n    t_idx = torch.searchsorted(t, tq, right=False)\n\n    # Get the indices of the times in 't' that are greater than the query times in 'tq'.\n    t_idx_next = torch.searchsorted(t, tq, right=True)\n\n    # If the query times exactly match a step change time, return the outside value.\n    if t_idx == t_idx_next:\n        return outside_value\n\n    # Otherwise, interpolate the value at the query time based on the step function.\n    else:\n        return y[t_idx] + (y[t_idx_next] - y[t_idx]) * (tq - t[t_idx]) / (t[t_idx_next] - t[t_idx])"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if tq is a scalar.\n    if tq.ndim == 0:\n        tq = tq.unsqueeze(0)\n\n    # Check if t is a scalar.\n    if t.ndim == 0:\n        t = t.unsqueeze(0)\n\n    # Check if y is a scalar.\n    if y.ndim == 0:\n        y = y.unsqueeze(0)\n\n    # Check if t and y have the same number of elements.\n    if t.shape[0] != y.shape[0]:\n        raise ValueError(\"The number of elements in t and y must be the same.\")\n\n    # Check if t is sorted in ascending order.\n    if not torch.all(t[1:] >= t[:-1]):\n        raise ValueError(\"t must be sorted in ascending order.\")\n\n    # Check if tq is sorted in ascending order.\n    if not torch.all(tq[1:] >= tq[:-1]):\n        raise ValueError(\"tq must be sorted in ascending order.\")\n\n    # Check if tq is within the range of t.\n    if not torch.all(tq >= t[0]) or not torch.all(tq <= t[-1]):\n        raise ValueError(\"tq must be within the range of t.\")\n\n    # Check if tq is a tensor.\n    if not torch.is_tensor(tq):\n        raise TypeError(\"tq must be a tensor.\")\n\n    # Check if t is a tensor.\n    if not torch.is_tensor(t):\n        raise TypeError(\"t must be a tensor.\")\n\n    # Check if y is a tensor.\n    if not torch.is_tensor(y):\n        raise TypeError(\"y must be a tensor.\")\n\n    # Check if outside_value is a numeric.\n    if not isinstance(outside_value, (int, float, torch.Tensor)):\n        raise TypeError(\"outside_value must be a numeric.\")\n\n    # Check if outside_value is a tensor."}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if tq is a tensor\n    if not isinstance(tq, torch.Tensor):\n        tq = torch.tensor(tq)\n\n    # Check if t and y are tensors\n    if not isinstance(t, torch.Tensor):\n        t = torch.tensor(t)\n    if not isinstance(y, torch.Tensor):\n        y = torch.tensor(y)\n\n    # Check if t and y have the same number of elements\n    if t.numel() != y.numel():\n        raise ValueError(\"t and y must have the same number of elements\")\n\n    # Check if t and tq are on the same device\n    if t.device != tq.device:\n        raise ValueError(\"t and tq must be on the same device\")\n\n    # Check if t is sorted\n    if not torch.all(torch.diff(t) >= 0):\n        raise ValueError(\"t must be sorted\")\n\n    # Check if tq is sorted\n    if not torch.all(torch.diff(tq) >= 0):\n        raise ValueError(\"tq must be sorted\")\n\n    # Check if t and tq are 1D tensors\n    if t.ndim != 1 or tq.ndim != 1:\n        raise ValueError(\"t and tq must be 1D tensors\")\n\n    # Check if t and tq are on the same device\n    if t.device != tq.device:\n        raise ValueError(\"t and tq must be on the same device\")\n\n    # Check if t and tq are on the same dtype\n    if t.dtype != tq.dtype:\n        raise ValueError(\"t and tq must be on the same dtype\")\n\n    # Check if t and tq are on the same device\n    if t.device != tq.device:\n        raise ValueError(\"t and tq must be on the same device\")\n\n    # Check if t and tq are on the same dtype\n    if t.dtype != tq.dtype:"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Convert the input tensors to numpy arrays\n    tq = tq.numpy()\n    t = t.numpy()\n    y = y.numpy()\n\n    # Initialize an empty array to store the output values\n    yq = np.zeros_like(tq)\n\n    # Iterate over the query times\n    for i, tq_i in enumerate(tq):\n        # Find the indices of the nearest times in the step function\n        t_left_idx = np.searchsorted(t, tq_i, side='left') - 1\n        t_right_idx = np.searchsorted(t, tq_i, side='right')\n\n        # If the query time exactly matches a step change time, return the outside value\n        if tq_i == t[t_left_idx]:\n            yq[i] = outside_value\n        # Otherwise, interpolate the value at the query time\n        else:\n            yq[i] = y[t_left_idx] + (y[t_right_idx] - y[t_left_idx]) * (tq_i - t[t_left_idx]) / (t[t_right_idx] - t[t_left_idx])\n\n    # Convert the output array to a tensor and return it\n    return torch.from_numpy(yq)"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if the query times are within the range of the step function\n    tq_mask = (tq >= t[0]) & (tq <= t[-1])\n\n    # Initialize the output tensor\n    yq = tf.zeros_like(tq)\n\n    # Set the outside value for query times that exactly match a step change time\n    yq = tf.where(tq_mask, outside_value, yq)\n\n    # Interpolate the step function values at query times\n    yq = tf.where(tq_mask, tf.math.segment_max(y, tf.searchsorted(t, tq, side='right') - 1), yq)\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check that the input tensors are 1D\n    assert tq.ndim == 1, 'The query times tensor must be 1D.'\n    assert t.ndim == 1, 'The times tensor must be 1D.'\n    assert y.ndim == 1, 'The values tensor must be 1D.'\n\n    # Check that the input tensors have the same length\n    assert tq.shape[0] == t.shape[0], 'The query times tensor and the times tensor must have the same length.'\n    assert tq.shape[0] == y.shape[0], 'The query times tensor and the values tensor must have the same length.'\n\n    # Check that the input tensors are sorted in ascending order\n    assert torch.all(torch.diff(tq) >= 0), 'The query times tensor must be sorted in ascending order.'\n    assert torch.all(torch.diff(t) >= 0), 'The times tensor must be sorted in ascending order.'\n\n    # Check that the input tensors do not contain any duplicate values\n    assert torch.all(torch.diff(tq) != 0), 'The query times tensor must not contain any duplicate values.'\n    assert torch.all(torch.diff(t) != 0), 'The times tensor must not contain any duplicate values.'\n\n    # Check that the outside value is a numeric type\n    assert isinstance(outside_value, (int, float, complex)), 'The outside value must be a numeric type.'\n\n    # Check that the outside value is not NaN or infinite\n    assert not torch.isnan(outside_value), 'The outside value must not be NaN.'\n    assert not torch.isinf(outside_value), 'The outside value must not be infinite.'\n\n    # Check that the outside value is not NaN or infinite\n    assert not torch.isnan(outside_value), 'The outside value must not be NaN.'\n    assert not torch.isinf(outside_value), 'The outside value must not be infinite.'\n\n    # Check that the outside value is not NaN or infinite\n    assert not"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if the input tensors have the same shape\n    if t.shape != y.shape:\n        raise ValueError(\"Input tensors 't' and 'y' must have the same shape.\")\n\n    # Convert the input tensors to numpy arrays\n    t = t.numpy()\n    y = y.numpy()\n    tq = tq.numpy()\n\n    # Create a boolean mask indicating which query times exactly match a step change time\n    mask = np.isclose(tq, t)\n\n    # Interpolate the values at the query times using linear interpolation\n    yq = np.interp(tq, t, y, left=outside_value, right=outside_value)\n\n    # Set the values at the exact step change times to the outside value\n    yq[mask] = outside_value\n\n    # Convert the result back to a tensor and return it\n    return torch.tensor(yq, dtype=y.dtype)"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Find the indices of the nearest times to the query times\n    nearest_indices = torch.searchsorted(t, tq, right=True) - 1\n\n    # Clip the indices to the range of valid indices\n    nearest_indices = torch.clamp(nearest_indices, min=0, max=t.shape[0] - 1)\n\n    # Extract the corresponding times and values\n    nearest_times = t[nearest_indices]\n    nearest_values = y[nearest_indices]\n\n    # Compute the interpolated values at the query times\n    interpolated_values = (tq - nearest_times) / (t[1] - t[0]) * (y[1] - y[0]) + y[0]\n\n    # Set the interpolated values at the step change times to the outside value\n    interpolated_values[nearest_indices == t.shape[0] - 1] = outside_value\n\n    return interpolated_values"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Step 1: Find the indices of the times in 't' that are closest to the query times in 'tq'\n    indices = tf.searchsorted(t, tq)\n\n    # Step 2: Determine the indices of the times in 't' that are within the range of the query times in 'tq'\n    indices = tf.clip_by_value(indices, 0, t.shape[0] - 1)\n\n    # Step 3: Determine the indices of the times in 't' that are outside the range of the query times in 'tq'\n    indices_outside = tf.where(indices == 0, tf.ones_like(indices), tf.zeros_like(indices))\n\n    # Step 4: Determine the indices of the times in 't' that are within the range of the query times in 'tq'\n    indices_within = tf.where(indices == t.shape[0] - 1, tf.ones_like(indices), tf.zeros_like(indices))\n\n    # Step 5: Determine the indices of the times in 't' that are outside the range of the query times in 'tq'\n    indices_outside = tf.where(indices_outside == 1, tf.ones_like(indices_outside), tf.zeros_like(indices_outside))\n\n    # Step 6: Determine the indices of the times in 't' that are within the range of the query times in 'tq'\n    indices_within = tf.where(indices_within == 1, tf.ones_like(indices_within), tf.zeros_like(indices_within))\n\n    # Step 7: Determine the indices of the times in 't' that are outside the range of the query times in 'tq'\n    indices_outside = tf.where(indices_outside == 1, tf.ones_like(indices_outside), tf.zeros_like(indices_outside))\n\n    # Step 8"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Create a mask for the query times that exactly match a step change time\n    tq_mask = torch.isclose(tq, t)\n\n    # Create a mask for the query times that are before the first step change time\n    tq_before_mask = tq < t[0]\n\n    # Create a mask for the query times that are after the last step change time\n    tq_after_mask = tq > t[-1]\n\n    # Initialize the output tensor with the outside value\n    yq = outside_value * torch.ones_like(tq)\n\n    # Set the output tensor values for query times that exactly match a step change time\n    yq[tq_mask] = y[tq_mask]\n\n    # Set the output tensor values for query times that are before the first step change time\n    yq[tq_before_mask] = y[0]\n\n    # Set the output tensor values for query times that are after the last step change time\n    yq[tq_after_mask] = y[-1]\n\n    # Interpolate the output tensor values for query times that are between step change times\n    yq[~(tq_mask | tq_before_mask | tq_after_mask)] = torch.interp(tq[~(tq_mask | tq_before_mask | tq_after_mask)], t, y)\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Convert tensors to numpy arrays\n    tq = tq.numpy()\n    t = t.numpy()\n    y = y.numpy()\n\n    # Initialize the output array\n    output = np.zeros_like(tq)\n\n    # Iterate over each query time\n    for i, tq_i in enumerate(tq):\n\n        # Find the index of the last time less than or equal to the query time\n        idx = np.searchsorted(t, tq_i, side='right') - 1\n\n        # If the query time exactly matches a step change time, return the outside value\n        if tq_i == t[idx]:\n            output[i] = outside_value\n\n        # Otherwise, interpolate the value at the query time\n        else:\n            output[i] = y[idx] + (tq_i - t[idx]) * (y[idx + 1] - y[idx]) / (t[idx + 1] - t[idx])\n\n    return output\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # check if the inputs are tensors\n    if not isinstance(tq, torch.Tensor):\n        raise TypeError(\"Input 'tq' must be a tensor.\")\n    if not isinstance(t, torch.Tensor):\n        raise TypeError(\"Input 't' must be a tensor.\")\n    if not isinstance(y, torch.Tensor):\n        raise TypeError(\"Input 'y' must be a tensor.\")\n\n    # check if the tensors have the same number of dimensions\n    if tq.ndim != t.ndim:\n        raise ValueError(\"Input 'tq' and 't' must have the same number of dimensions.\")\n    if tq.ndim != y.ndim:\n        raise ValueError(\"Input 'tq' and 'y' must have the same number of dimensions.\")\n\n    # check if the tensors have the same shape\n    if tq.shape != t.shape:\n        raise ValueError(\"Input 'tq' and 't' must have the same shape.\")\n    if tq.shape != y.shape:\n        raise ValueError(\"Input 'tq' and 'y' must have the same shape.\")\n\n    # check if the tensors have the same type\n    if tq.dtype != t.dtype:\n        raise TypeError(\"Input 'tq' and 't' must have the same type.\")\n    if tq.dtype != y.dtype:\n        raise TypeError(\"Input 'tq' and 'y' must have the same type.\")\n\n    # check if the tensors have the same device\n    if tq.device != t.device:\n        raise RuntimeError(\"Input 'tq' and 't' must be on the same device.\")\n    if tq.device != y.device:\n        raise RuntimeError(\"Input 'tq' and 'y' must be on the same device.\")\n\n    # check if the tensors are sorted\n    if not torch.all(torch.diff(t) >= 0):\n        raise ValueError(\"Input 't' must be sorted.\")\n\n    # check if the tensors are all on the"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Determine the index of the first time greater than or equal to each query time\n    idx = torch.searchsorted(t, tq, right=True)\n\n    # Clip the indices to the range of valid indices\n    idx = torch.clamp(idx, 0, t.shape[0] - 1)\n\n    # Get the corresponding values for the indices\n    y_out = y[idx]\n\n    # If the query time exactly matches a step change time, return the outside value\n    y_out[tq == t] = outside_value\n\n    return y_out\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Create a tensor of 1s and 0s indicating whether the query time is greater than or equal to the step change time\n    is_greater_than = tq.unsqueeze(1) >= t.unsqueeze(0)\n\n    # Create a tensor of 1s and 0s indicating whether the query time is less than or equal to the next step change time\n    is_less_than = tq.unsqueeze(1) <= torch.cat([t[1:], torch.tensor([float('inf')])], dim=0).unsqueeze(0)\n\n    # Create a tensor of 1s and 0s indicating whether the query time is equal to the step change time\n    is_equal = tq.unsqueeze(1) == t.unsqueeze(0)\n\n    # Create a tensor of 1s and 0s indicating whether the query time is greater than or equal to the step change time and less than or equal to the next step change time\n    is_inside = is_greater_than & is_less_than\n\n    # Create a tensor of 1s and 0s indicating whether the query time is equal to the step change time\n    is_outside = is_equal & (~is_inside)\n\n    # Create a tensor of 1s and 0s indicating whether the query time is greater than or equal to the step change time and less than or equal to the next step change time\n    is_outside = is_equal & (~is_inside)\n\n    # Create a tensor of 1s and 0s indicating whether the query time is greater than or equal to the step change time and less than or equal to the next step change time\n    is_outside = is_equal & (~is_inside)\n\n    # Create a tensor of 1s and 0s indicating whether the query time is greater than or equal to the step change time and less than or equal to the next step change time\n    is_outside = is_equal & (~is_inside)\n\n    # Create a tensor of 1s and 0s indicating whether the query time is greater than or"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Find the indices of the times in 't' that are less than or equal to the query times in 'tq'\n    idx = torch.searchsorted(t, tq, right=True) - 1\n\n    # Clip the indices to the valid range of indices\n    idx = torch.clamp(idx, min=0, max=len(t) - 1)\n\n    # Use the indices to look up the corresponding values in 'y'\n    yq = y[idx]\n\n    # If the query times exactly match a step change time, return the outside value\n    yq[idx == len(t) - 1] = outside_value\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Convert the input arguments to tensors if they are not already\n    tq = tf.convert_to_tensor(tq)\n    t = tf.convert_to_tensor(t)\n    y = tf.convert_to_tensor(y)\n\n    # Check that the input tensors have the same shape\n    if tq.shape != t.shape:\n        raise ValueError(\"The input tensors 'tq' and 't' must have the same shape.\")\n    if tq.shape != y.shape:\n        raise ValueError(\"The input tensors 'tq' and 'y' must have the same shape.\")\n\n    # Sort the input tensors in ascending order\n    tq_sorted, t_sorted, y_sorted = tf.sort([tq, t, y], axis=0)\n\n    # Calculate the differences between the sorted query times and the sorted step function times\n    t_diff = t_sorted[1:] - t_sorted[:-1]\n    tq_diff = tq_sorted[1:] - tq_sorted[:-1]\n\n    # Calculate the indices of the sorted step function times that are closest to the sorted query times\n    t_indices = tf.searchsorted(t_sorted, tq_sorted, side=\"right\") - 1\n\n    # Calculate the indices of the sorted step function times that are closest to the sorted query times\n    tq_indices = tf.searchsorted(tq_sorted, t_sorted, side=\"right\") - 1\n\n    # Calculate the interpolated values of the step function at the query times\n    y_interp = y_sorted[t_indices] + (tq_sorted - t_sorted[t_indices]) * (y_sorted[t_indices + 1] - y_sorted[t_indices]) / t_diff\n\n    # Calculate the outside values of the step function at the query times\n    y_outside = outside_value + (tq_sorted - t_sorted[t_ind"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # calculate the annealing factor\n    anneal_factor = torch.sigmoid(anneal_slope * (train_frac - 0.5))\n\n    # calculate the annealing weight adjustment\n    w_adj = w * anneal_factor\n\n    # handle adjacent intervals with zero distance\n    w_adj[..., 1:] -= w_adj[..., :-1]\n    w_adj[..., 0] = 0.0\n\n    # prevent NaN values\n    w_adj = torch.softmax(w_adj, dim=-1)\n\n    # return the adjusted weights\n    return w_adj"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # calculate the annealing factor\n    a = torch.sigmoid((t - train_frac) * anneal_slope)\n\n    # adjust the weights using the annealing factor\n    w = w * a\n\n    # handle cases where adjacent intervals have zero distance\n    w[..., 1:] = torch.where(\n        torch.abs(t[..., 1:] - t[..., :-1]) < eps,\n        torch.zeros_like(w[..., 1:]),\n        w[..., 1:],\n    )\n\n    # prevent NaN values\n    w = torch.softmax(w, dim=-1)\n\n    return w"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # calculate the annealing function\n    # this is a sigmoid function that starts at 0 and ends at 1\n    # the slope is determined by the anneal_slope parameter\n    # the x-axis is the training fraction\n    # the y-axis is the annealing function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the annealing function is the output of the sigmoid function\n    # the an"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # compute the annealing factor\n    anneal_factor = torch.sigmoid((train_frac - t[..., 0]) * anneal_slope)\n\n    # compute the annealing weights\n    anneal_weights = torch.softmax(anneal_factor * (w[..., 1:] - w[..., :-1]), dim=-1)\n\n    # add epsilon to prevent division by zero\n    anneal_weights += eps\n\n    # return the annealed weights\n    return anneal_weights"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # calculate the annealing function\n    a = torch.sigmoid(anneal_slope * (t - train_frac))\n    # calculate the weight adjustment based on the annealing function\n    w_adj = a * w\n    # set the weight adjustment to zero for adjacent intervals with zero distance\n    w_adj[..., 1:] -= w_adj[..., :-1]\n    # add epsilon to prevent division by zero\n    w_adj[..., 1:] += eps\n    # apply a softmax operation to the adjusted weights to ensure stability in the computation\n    w_adj = torch.softmax(w_adj, dim=-1)\n    return w_adj\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # get the time intervals\n    t_intervals = t[..., 1:] - t[..., :-1]\n\n    # calculate the annealing function\n    anneal_func = torch.sigmoid((t_intervals - train_frac) * anneal_slope)\n\n    # adjust the weights based on the annealing function\n    w_adj = w * anneal_func\n\n    # set weights of adjacent intervals with zero distance to zero\n    w_adj[..., 1:] = w_adj[..., 1:] * (t_intervals[..., 1:] > eps)\n\n    # ensure stability in the computation by using a softmax operation on the adjusted weights\n    w_adj = torch.nn.functional.softmax(w_adj, dim=-1)\n\n    return w_adj\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # make sure that the last weight is 1.0\n    w = w.clone()\n    w[..., -1] = 1.0\n\n    # calculate the annealing weight\n    anneal_weight = torch.sigmoid(anneal_slope * (1.0 - train_frac))\n\n    # adjust the weights using the annealing weight\n    w = w * anneal_weight\n\n    # normalize the weights\n    w = torch.nn.functional.softmax(w, dim=-1)\n\n    # set the last weight to 1.0\n    w[..., -1] = 1.0\n\n    return w"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # get the time interval\n    t_int = t[..., 1:] - t[..., :-1]\n\n    # calculate the annealing function\n    a = anneal_slope * (1.0 - train_frac)\n    a = torch.exp(a * t_int)\n\n    # calculate the weights\n    w_ = w[..., 1:] - w[..., :-1]\n    w_ = a * w_\n\n    # handle cases where adjacent intervals have zero distance\n    w_[t_int < eps] = 0.0\n\n    # softmax over the weights\n    w_ = torch.nn.functional.softmax(w_, dim=-1)\n\n    # return the weights\n    return w_"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n\n    # calculate the annealing function\n    anneal_func = torch.sigmoid(anneal_slope * (t - train_frac))\n\n    # adjust the weights\n    w_adj = w * anneal_func\n\n    # ensure stability in the computation\n    w_adj = torch.where(torch.isclose(torch.sum(w_adj, dim=-1, keepdim=True), torch.zeros_like(w_adj)),\n                        torch.zeros_like(w_adj), w_adj)\n\n    # softmax the adjusted weights\n    w_adj = torch.softmax(w_adj, dim=-1)\n\n    return w_adj\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # compute the annealing slope\n    slope = anneal_slope\n\n    # compute the bias\n    bias = 1.0 - train_frac\n\n    # compute the annealing function\n    t = torch.sigmoid((t - bias) * slope)\n\n    # adjust the weights\n    w = w * t\n\n    # handle adjacent intervals with zero distance\n    w_adj = w.clone()\n    w_adj[..., 1:] = w_adj[..., 1:] - w_adj[..., :-1]\n    w_adj[..., 0] = w_adj[..., 0] - w_adj[..., -1]\n\n    # set weights of adjacent intervals with zero distance to zero\n    w_adj[w_adj < eps] = 0.0\n\n    # normalize weights\n    w_adj = torch.softmax(w_adj, dim=-1)\n\n    # return the adjusted weights\n    return w_adj\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t = torch.cat([t, torch.tensor([1.0], device=t.device)], dim=-1)\n\n    # compute the annealing factor\n    # anneal_factor = torch.sigmoid((t - train_frac) * anneal_slope)\n    anneal_factor = torch.sigmoid(\n        (t - train_frac) * anneal_slope) + eps\n\n    # adjust the weights\n    w_adjusted = w * anneal_factor\n\n    # handle cases where adjacent intervals have zero distance\n    # by setting their weight to zero\n    w_adjusted = torch.where(\n        torch.isclose(torch.diff(t), torch.tensor(0.0, device=t.device)),\n        torch.zeros_like(w_adjusted),\n        w_adjusted\n    )\n\n    # prevent NaN values by using a softmax operation\n    w_adjusted = torch.softmax(w_adjusted, dim=-1)\n\n    return w_adjusted\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # get the interval\n    interval_idx = torch.searchsorted(t[..., 1:], train_frac)\n    interval_idx = torch.clamp(interval_idx, 0, w.shape[-1] - 1)\n    interval_idx = interval_idx.unsqueeze(-1)\n\n    # get the weights for the interval\n    w_interval = w[..., interval_idx]\n\n    # get the distance to the interval\n    dist_to_interval = train_frac - t[..., interval_idx]\n    dist_to_interval = torch.clamp(dist_to_interval, min=0)\n\n    # calculate the annealing factor\n    anneal_factor = torch.sigmoid(anneal_slope * dist_to_interval)\n\n    # adjust the weights for the interval\n    w_interval = w_interval * anneal_factor\n\n    # normalize the weights\n    w_interval = w_interval / (w_interval.sum(dim=-1, keepdim=True) + eps)\n\n    # set the weights for the interval to zero if they are very small\n    w_interval[w_interval < eps] = 0\n\n    # set the weights for the interval to zero if they are very small\n    w_interval[w_interval < eps] = 0\n\n    # set the weights for the interval to zero if they are very small\n    w_interval[w_interval < eps] = 0\n\n    # set the weights for the interval to zero if they are very small\n    w_interval[w_interval < eps] = 0\n\n    # set the weights for the interval to zero if they are very small\n    w_interval[w_interval < eps] = 0\n\n    # set the weights for the interval to zero if they are very small\n    w_interval[w_interval < eps] = 0\n\n    # set the weights for the interval to zero if they are very small\n    w_interval[w_interval < eps] = 0\n\n    # set the weights for the interval to zero if they are very small\n   "}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # find the index of the interval containing the current time\n    idx = torch.where(t[..., :-1] <= train_frac, t[..., 1:], t[..., :-1]).argmin(dim=-1)\n\n    # calculate the distance from the current time to the start and end of the interval\n    dist_to_start = train_frac - t[..., idx]\n    dist_to_end = t[..., idx + 1] - train_frac\n\n    # calculate the weight adjustment based on the distance and annealing slope\n    w_adj = torch.where(dist_to_start < eps, 0.0, torch.exp(-anneal_slope * dist_to_start)) * torch.where(\n        dist_to_end < eps, 0.0, torch.exp(-anneal_slope * dist_to_end))\n\n    # apply the weight adjustment to the weights tensor\n    w_adj = w_adj / (w_adj.sum(dim=-1, keepdim=True) + eps)\n    w_adj = w_adj.unsqueeze(-1)\n    w = w * w_adj\n\n    # apply a softmax to the adjusted weights to ensure stability\n    w = torch.softmax(w, dim=-1)\n\n    return w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # The shape of t is expected to be [..., n_time_steps + 1]\n    # The shape of w is expected to be [..., n_time_steps]\n    # The shape of train_frac is expected to be [...]\n\n    # Ensure that t and w have the same shape\n    assert t.shape[:-1] == w.shape[:-1], f\"t and w must have the same shape, but got {t.shape} and {w.shape}\"\n\n    # Ensure that train_frac is a scalar or has the same shape as t\n    assert train_frac.shape == t.shape[:-1] or train_frac.shape == (), f\"train_frac must be a scalar or have the same shape as t, but got {train_frac.shape} and {t.shape}\"\n\n    # Ensure that anneal_slope is a scalar or has the same shape as t\n    assert anneal_slope.shape == t.shape[:-1] or anneal_slope.shape == (), f\"anneal_slope must be a scalar or have the same shape as t, but got {anneal_slope.shape} and {t.shape}\"\n\n    # Ensure that eps is a scalar or has the same shape as t\n    assert eps.shape == t.shape[:-1] or eps.shape == (), f\"eps must be a scalar or have the same shape as t, but got {eps.shape} and {t.shape}\"\n\n    # Ensure that t and train_frac are broadcastable\n    assert t.shape[:-1] == train_frac.shape or train_frac.shape == (), f\"t and train_frac must be broadcastable, but got {t.shape} and {train_frac.shape}\"\n\n    # Ensure that t and anneal_slope are broadcastable\n    assert t.shape[:-1] == anneal_slope.shape or anneal_slope.shape == (), f\"t and anneal_slope must be broadcastable, but got {t.shape} and {anneal_slope.shape}\""}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # get the number of intervals\n    n_intervals = w.shape[-1] - 1\n\n    # calculate the interval widths\n    interval_widths = t[..., 1:] - t[..., :-1]\n\n    # calculate the cumulative interval widths\n    cum_interval_widths = torch.cumsum(interval_widths, dim=-1)\n\n    # calculate the annealing weights\n    anneal_weights = (\n            torch.sigmoid(anneal_slope * (train_frac - cum_interval_widths / cum_interval_widths[-1]))\n            + eps\n    )\n\n    # adjust the weights based on the annealing weights\n    w = w * anneal_weights\n\n    # normalize the weights\n    w = w / torch.sum(w, dim=-1, keepdim=True)\n\n    # set the weights of adjacent intervals to zero\n    w[..., 1:] = w[..., 1:] * (1 - torch.abs(interval_widths / cum_interval_widths[..., 1:]))\n\n    # return the adjusted weights\n    return w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # create a mask for the first interval\n    mask = (t[..., :-1] < train_frac) & (t[..., 1:] >= train_frac)\n    # create a mask for the last interval\n    mask_last = (t[..., :-1] < train_frac) & (t[..., 1:] < train_frac)\n    # create a mask for the intermediate intervals\n    mask_intermediate = (t[..., :-1] >= train_frac) & (t[..., 1:] >= train_frac)\n\n    # calculate the annealing function\n    anneal_func = torch.sigmoid(anneal_slope * (t[..., 1:] - train_frac))\n    # calculate the annealing function for the last interval\n    anneal_func_last = torch.sigmoid(anneal_slope * (train_frac - t[..., :-1]))\n    # calculate the annealing function for the intermediate intervals\n    anneal_func_intermediate = torch.sigmoid(anneal_slope * (t[..., 1:] - t[..., :-1]))\n\n    # apply the annealing function to the weights\n    w[mask] = w[mask] * anneal_func[mask]\n    # apply the annealing function to the weights for the last interval\n    w[mask_last] = w[mask_last] * anneal_func_last[mask_last]\n    # apply the annealing function to the weights for the intermediate intervals\n    w[mask_intermediate] = w[mask_intermediate] * anneal_func_intermediate[mask_intermediate]\n\n    # ensure that the weights sum to one\n    w = torch.nn.functional.softmax(w, dim=-1)\n\n    return w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    assert t.shape[-1] == w.shape[-1] + 1, \"t.shape[-1] = w.shape[-1] + 1\"\n\n    # 1. Get the distance between each interval\n    dist = t[..., 1:] - t[..., :-1]\n    # 2. Compute the bias using Schlick's bias function\n    bias = torch.sigmoid(anneal_slope * (train_frac - t[..., 1:]))\n    # 3. Adjust the weights based on the bias and distance\n    w = w * bias[..., None] * (1 - dist[..., None])\n    # 4. Normalize the weights\n    w = w / (w.sum(dim=-1, keepdim=True) + eps)\n    # 5. Set weights of adjacent intervals with zero distance to zero\n    w[..., 1:] = w[..., 1:] * (1 - dist[..., None]).detach()\n    # 6. Apply softmax to the adjusted weights\n    w = torch.nn.functional.softmax(w, dim=-1)\n    return w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # get the number of weights\n    n_weights = w.shape[-1]\n\n    # calculate the weighting for each weight\n    w_ = w[..., :-1]\n    w_ = w_ / torch.sum(w_, dim=-1, keepdim=True)\n\n    # calculate the bias for each weight\n    bias = torch.zeros_like(w_)\n    bias[..., :-1] = w_[..., 1:] - w_[..., :-1]\n    bias[..., -1] = (1 - train_frac) - w_[..., -1]\n\n    # calculate the weighting for each weight\n    w_ = w_ / torch.sum(w_, dim=-1, keepdim=True)\n\n    # calculate the annealing weighting for each weight\n    w_ = w_ * (1 - torch.sigmoid(anneal_slope * (t[..., 1:] - t[..., :-1])))\n\n    # calculate the annealing bias for each weight\n    bias = bias * (1 - torch.sigmoid(anneal_slope * (t[..., 1:] - t[..., :-1])))\n\n    # calculate the annealing weighting for each weight\n    w_ = w_ + bias\n\n    # calculate the weighting for each weight\n    w_ = w_ / torch.sum(w_, dim=-1, keepdim=True)\n\n    # set the weighting for the last weight to 0\n    w_[..., -1] = 0\n\n    # calculate the weighting for each weight\n    w_ = w_ / torch.sum(w_, dim=-1, keepdim=True)\n\n    # set the weighting for the last weight to 1\n    w_[..., -1] = 1\n\n    # calculate the weighting for each weight\n    w_ = w_ / torch.sum(w_, dim=-1, keepdim=True)\n\n    # calculate the weighting for each weight\n    w_ = w_ / tor"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    assert t.shape[-1] == w.shape[-1] + 1, f\"{t.shape[-1]} != {w.shape[-1]} + 1\"\n\n    # calculate the annealing weights\n    anneal_weights = torch.sigmoid(anneal_slope * (t - train_frac))\n    # apply the annealing weights to the weights tensor\n    w = w * anneal_weights\n    # normalize the weights to sum to 1\n    w = w / (w.sum(dim=-1, keepdim=True) + eps)\n    # apply the softmax function to the adjusted weights\n    w = torch.nn.functional.softmax(w, dim=-1)\n    return w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # compute the annealing weights\n    a = torch.sigmoid(anneal_slope * (t - train_frac))\n    a = a / (a + eps)\n\n    # compute the weights\n    w = a * w\n\n    # compute the normalization\n    w = w / (w.sum(dim=-1, keepdim=True) + eps)\n\n    return w\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return batch\n        else:\n            return {key: to_cuda(value, device, ignore_list) for key, value in batch.items()}\n    elif isinstance(batch, (tuple, list)):\n        return [to_cuda(element, device, ignore_list) for element in batch]\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        return [to_cuda(b, device=device, ignore_list=ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        return {\n            k: to_cuda(v, device=device, ignore_list=ignore_list)\n            for k, v in batch.items()\n            if k != \"meta\"\n        }\n    elif isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(b, device, ignore_list) for b in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return batch\n        else:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (list, tuple)):\n        return [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        if ignore_list:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n        else:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, (tuple, list)):\n        return type(batch)(to_cuda(b, device, ignore_list) for b in batch)\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device=device, non_blocking=True)\n    elif isinstance(batch, dict):\n        out = {}\n        for key, item in batch.items():\n            if key == \"meta\":\n                out[key] = item\n            else:\n                out[key] = to_cuda(item, device=device)\n        return out\n    elif isinstance(batch, (tuple, list)):\n        if isinstance(batch, tuple) and not ignore_list:\n            batch = list(batch)\n        out = []\n        for item in batch:\n            out.append(to_cuda(item, device=device))\n        return out\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device=device, non_blocking=True)\n    elif isinstance(batch, dict):\n        out = {}\n        for key, item in batch.items():\n            if key == \"meta\":\n                out[key] = item\n            else:\n                out[key] = to_cuda(item, device, ignore_list)\n        return out\n    elif isinstance(batch, (tuple, list)):\n        if ignore_list and isinstance(batch, list):\n            return [to_cuda(b, device) for b in batch]\n        return type(batch)(to_cuda(b, device) for b in batch)\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        return [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return {\n                k: to_cuda(v, device, ignore_list)\n                for k, v in batch.items()\n                if k != \"meta\"\n            }\n        else:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device=device, non_blocking=True)\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return {key: to_cuda(value, device, ignore_list) for key, value in batch.items()}\n        else:\n            return {key: to_cuda(value, device, ignore_list) for key, value in batch.items()}\n    elif isinstance(batch, tuple) and not ignore_list:\n        return tuple(to_cuda(element, device, ignore_list) for element in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(element, device, ignore_list) for element in batch]\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, (tuple, list)):\n        return [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        if ignore_list:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n        else:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n        else:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    elif isinstance(batch, list):\n        return [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, tuple):\n        return tuple([to_cuda(b, device, ignore_list) for b in batch])\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device=device, non_blocking=True)\n\n    elif isinstance(batch, (tuple, list)):\n        return [to_cuda(b, device=device) for b in batch]\n\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return batch\n        else:\n            return {key: to_cuda(data, device=device) for key, data in batch.items()}\n\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, dict):\n        return {\n            key: to_cuda(value, device, ignore_list)\n            for key, value in batch.items()\n            if not ignore_list or key != \"meta\"\n        }\n    elif isinstance(batch, list):\n        return [to_cuda(item, device, ignore_list) for item in batch]\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(item, device, ignore_list) for item in batch)\n    elif isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device=device, non_blocking=True)\n    elif isinstance(batch, (tuple, list)):\n        if ignore_list and isinstance(batch, list):\n            return batch\n        return type(batch)(to_cuda(b, device, ignore_list) for b in batch)\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return {\n                \"meta\": to_cuda(batch[\"meta\"], device, ignore_list),\n                \"data\": to_cuda(batch[\"data\"], device, ignore_list),\n            }\n        else:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n        else:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    elif isinstance(batch, list):\n        return [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, tuple) and not ignore_list:\n        return tuple(to_cuda(b, device, ignore_list) for b in batch)\n    elif isinstance(batch, tuple) and ignore_list:\n        return tuple(to_cuda(b, device, ignore_list) for b in batch[:-1]), batch[-1]\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device=device, non_blocking=True)\n    elif isinstance(batch, (tuple, list)):\n        if ignore_list:\n            return [to_cuda(b, device) for b in batch]\n        else:\n            return type(batch)(to_cuda(b, device) for b in batch)\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return {k: to_cuda(v, device) for k, v in batch.items()}\n        else:\n            return {k: to_cuda(v, device) for k, v in batch.items()}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device=device, non_blocking=True)\n    elif isinstance(batch, (tuple, list)):\n        if isinstance(batch[0], torch.Tensor):\n            return [to_cuda(b, device=device) for b in batch]\n        else:\n            return type(batch)(to_cuda(b, device=device) for b in batch)\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return {key: to_cuda(value, device=device) for key, value in batch.items()}\n        else:\n            return type(batch)((key, to_cuda(value, device=device)) for key, value in batch.items())\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device=device, non_blocking=True)\n    elif isinstance(batch, (tuple, list)):\n        return [to_cuda(b, device=device, ignore_list=ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        if ignore_list:\n            return {\n                k: to_cuda(v, device=device, ignore_list=ignore_list)\n                for k, v in batch.items()\n            }\n        else:\n            return {\n                k: to_cuda(v, device=device, ignore_list=ignore_list)\n                for k, v in batch.items()\n                if k != \"meta\"\n            }\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, (tuple, list)):\n        return type(batch)(to_cuda(b, device=device) for b in batch)\n    elif isinstance(batch, dict):\n        if ignore_list:\n            return {k: to_cuda(v, device=device) for k, v in batch.items()}\n        else:\n            return {k: to_cuda(v, device=device) for k, v in batch.items() if k != \"meta\"}\n    else:\n        raise TypeError(f\"Unsupported type: {type(batch)}\")"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, dict):\n        return {\n            key: to_cuda(value, device=device, ignore_list=ignore_list)\n            if key != \"meta\"\n            else value\n            for key, value in batch.items()\n        }\n    elif isinstance(batch, list):\n        return [to_cuda(x, device=device, ignore_list=ignore_list) for x in batch]\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(x, device=device, ignore_list=ignore_list) for x in batch)\n    elif isinstance(batch, torch.Tensor):\n        return batch.to(device=device, non_blocking=True)\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if v.shape[dim] != f.shape[dim]:\n        f = f.expand(v.shape[dim], -1, -1)\n\n    f = f.reshape(-1, f.shape[-2], f.shape[-1])\n\n    v1 = v[:, f[:, 0], :]\n    v2 = v[:, f[:, 1], :]\n    v3 = v[:, f[:, 2], :]\n\n    e1 = v2 - v1\n    e2 = v3 - v1\n\n    n = torch.cross(e1, e2, dim=-1)\n    n = n / (torch.norm(n, dim=-1, keepdim=True) + 1e-10)\n\n    return n\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # expand f to match the batch dimension of v\n    if f.shape[dim] != v.shape[dim]:\n        f = f.expand(v.shape[dim], -1, -1)\n\n    # gather vertices\n    v1 = torch.gather(v, dim, f[:, :, 0].unsqueeze(dim))\n    v2 = torch.gather(v, dim, f[:, :, 1].unsqueeze(dim))\n    v3 = torch.gather(v, dim, f[:, :, 2].unsqueeze(dim))\n\n    # compute normals\n    e1 = v2 - v1\n    e2 = v3 - v1\n    n = torch.cross(e1, e2, dim=dim)\n\n    # normalize normals\n    n = n / torch.norm(n, dim=dim, keepdim=True)\n\n    # reshape to maintain original faces tensor structure\n    n = n.reshape(f.shape)\n\n    return n\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the faces tensor to match the batch dimension of the vertices tensor\n    if v.shape[dim] != f.shape[dim]:\n        f = f.expand(v.shape[dim], -1, -1)\n\n    # gather the vertices to form triangles\n    v1 = torch.gather(v, dim, f[:, :, 0].unsqueeze(dim))\n    v2 = torch.gather(v, dim, f[:, :, 1].unsqueeze(dim))\n    v3 = torch.gather(v, dim, f[:, :, 2].unsqueeze(dim))\n\n    # compute the normals of the triangles\n    n = torch.cross(v2 - v1, v3 - v1, dim=dim)\n    n = F.normalize(n, dim=dim)\n\n    # reshape the result to maintain the original faces tensor structure\n    n = n.reshape(f.shape)\n\n    return n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the faces tensor to match the batch dimension of the vertices tensor\n    if f.shape[dim] != v.shape[0]:\n        f = f.expand(v.shape[0], *f.shape[1:])\n\n    # gather the vertices\n    v1 = torch.gather(v, dim, f[:, 0])\n    v2 = torch.gather(v, dim, f[:, 1])\n    v3 = torch.gather(v, dim, f[:, 2])\n\n    # compute the normals of the faces\n    n = torch.cross(v2 - v1, v3 - v1, dim=dim)\n\n    # reshape the result to maintain the original faces tensor structure\n    return torch.cat((v1, v2, v3, n), dim=dim + 1)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # compute faces normals w.r.t the vertices (considering batch dimension)\n    f = f.unsqueeze(dim)\n    f = f.expand(v.shape[0], -1, -1)\n    v = v.unsqueeze(dim)\n    v = v.expand(f.shape[0], -1, -1)\n    v1 = torch.gather(v, dim=dim, index=f[:, :, 0])\n    v2 = torch.gather(v, dim=dim, index=f[:, :, 1])\n    v3 = torch.gather(v, dim=dim, index=f[:, :, 2])\n    n = torch.cross(v2 - v1, v3 - v1, dim=dim)\n    n = n / torch.norm(n, dim=dim, keepdim=True)\n\n    # adjust the dimension of the faces tensor to match the batch dimension of the vertices tensor\n    if f.shape[0] != v.shape[0]:\n        f = f.reshape(v.shape[0], -1, 3)\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    return n.reshape(f.shape[0], -1, 3)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if v.shape[dim] != f.shape[dim]:\n        f = f.expand(list(f.shape[:dim]) + [v.shape[dim]] + list(f.shape[dim:]))\n    f = f.reshape((-1, 3))\n    v = v.reshape((-1, v.shape[-1]))\n    v0 = v[f[:, 0]]\n    v1 = v[f[:, 1]]\n    v2 = v[f[:, 2]]\n    n = torch.cross(v1 - v0, v2 - v0, dim=-1)\n    n = n / (torch.norm(n, dim=-1, keepdim=True) + 1e-10)\n    return n.reshape(list(f.shape[:-1]) + [3])\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # compute faces normals w.r.t the vertices (considering batch dimension)\n    v1 = v[:, f[:, 0], :]\n    v2 = v[:, f[:, 1], :]\n    v3 = v[:, f[:, 2], :]\n    face_normals = torch.cross(v2 - v1, v3 - v1, dim=dim)\n    face_normals = face_normals / torch.norm(face_normals, dim=-1, keepdim=True)\n    # gather triangles\n    v1 = torch.gather(v, dim, f[:, 0:1].expand(-1, 3, -1))\n    v2 = torch.gather(v, dim, f[:, 1:2].expand(-1, 3, -1))\n    v3 = torch.gather(v, dim, f[:, 2:3].expand(-1, 3, -1))\n    # adjust faces tensor to match batch dimension of vertices tensor\n    if f.shape[dim] != v.shape[dim]:\n        f = f.expand(v.shape[dim], -1, -1)\n    # reshape result to maintain original faces tensor structure with additional dimensions for batch processing\n    return torch.cat([v1, v2, v3, face_normals], dim=dim)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # compute faces normals w.r.t the vertices (considering batch dimension)\n    if f.shape[-1] == 3:\n        f = f.unsqueeze(-1)\n    v1 = torch.gather(v, dim, f[:, 0])\n    v2 = torch.gather(v, dim, f[:, 1])\n    v3 = torch.gather(v, dim, f[:, 2])\n    v12 = v2 - v1\n    v13 = v3 - v1\n    n = torch.cross(v12, v13)\n    n = n / (torch.norm(n, dim=-1, keepdim=True) + 1e-12)\n    return n\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjusting f to match the batch dimension of v\n    if f.shape[dim] != v.shape[0]:\n        f = f.repeat(v.shape[0], 1, 1)\n\n    # gathering vertices\n    v = v.gather(dim, f)\n\n    # compute normals\n    v1 = v[:, :, 1] - v[:, :, 0]\n    v2 = v[:, :, 2] - v[:, :, 0]\n    n = torch.cross(v1, v2, dim=-1)\n    n = n / (torch.norm(n, dim=-1, keepdim=True) + 1e-10)\n\n    # reshaping to maintain original faces tensor structure\n    n = n.reshape(f.shape)\n\n    return n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # v: [B, N, 3]\n    # f: [B, F, 3]\n\n    if v.shape[dim] != f.shape[dim]:\n        f = f.expand(-1, -1, v.shape[dim])\n\n    # v: [B, N, 3]\n    # f: [B, F, N]\n\n    v1 = v.gather(dim, f[:, :, 0].unsqueeze(-1)).squeeze(-1)\n    v2 = v.gather(dim, f[:, :, 1].unsqueeze(-1)).squeeze(-1)\n    v3 = v.gather(dim, f[:, :, 2].unsqueeze(-1)).squeeze(-1)\n\n    # v1: [B, F, 3]\n    # v2: [B, F, 3]\n    # v3: [B, F, 3]\n\n    e1 = v2 - v1\n    e2 = v3 - v1\n\n    # e1: [B, F, 3]\n    # e2: [B, F, 3]\n\n    n = torch.cross(e1, e2, dim=-1)\n\n    # n: [B, F, 3]\n\n    n = n / (torch.norm(n, dim=-1, keepdim=True) + 1e-8)\n\n    # n: [B, F, 3]\n\n    return n.reshape(*f.shape)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # gather vertices\n    v = v.unsqueeze(dim=dim)\n    v = v.repeat(1, f.shape[0], 1, 1)\n    v = v.reshape(-1, f.shape[1], v.shape[-1])\n    v = torch.gather(v, dim=dim, index=f.unsqueeze(dim=-1).repeat(1, 1, 3, 1))\n    v = v.reshape(-1, f.shape[0], 3, v.shape[-1])\n\n    # compute normals\n    v1 = v[:, :, 1] - v[:, :, 0]\n    v2 = v[:, :, 2] - v[:, :, 0]\n    n = torch.cross(v1, v2, dim=-1)\n    n = n / torch.norm(n, dim=-1, keepdim=True)\n\n    # reshape to match original faces tensor\n    n = n.reshape(f.shape[0], f.shape[1], n.shape[-1])\n\n    return n\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # compute faces normals w.r.t the vertices (considering batch dimension)\n    v_dim = v.dim()\n    f_dim = f.dim()\n    if v_dim != f_dim:\n        f = f.expand(v_dim - f_dim, -1, -1).permute(1, 0, 2)\n        f = f.reshape(v.shape[0], -1, 3)\n    v_normals = v[f]\n    v1 = v_normals[:, :, 0, :]\n    v2 = v_normals[:, :, 1, :]\n    v3 = v_normals[:, :, 2, :]\n    v12 = v2 - v1\n    v13 = v3 - v1\n    v123 = torch.cross(v12, v13, dim=-1)\n    v123 = v123 / torch.norm(v123, dim=-1, keepdim=True)\n    return v123.reshape(v.shape[0], -1, 3)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # compute faces normals w.r.t the vertices (considering batch dimension)\n    if v.dim() != f.dim():\n        f = f.unsqueeze(-1)\n    v1 = v.gather(dim, f[:, 0])\n    v2 = v.gather(dim, f[:, 1])\n    v3 = v.gather(dim, f[:, 2])\n    v12 = v1 - v2\n    v13 = v1 - v3\n    n = torch.cross(v12, v13, dim=-1)\n    n = F.normalize(n, p=2, dim=-1)\n\n    # adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor\n    f = f.expand(v.shape[0], -1, -1)\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    f = f.reshape(v.shape[0], -1, 3)\n    n = n.reshape(v.shape[0], -1, 3)\n    return f, n\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the dimension of the faces tensor to match the batch dimension of the vertices tensor\n    if f.shape[dim] != v.shape[dim]:\n        f = f.expand(v.shape[dim], -1, -1)\n\n    # gather the vertices to form the triangles\n    v = v.gather(dim, f)\n\n    # compute the normals of the faces\n    v1 = v[:, :, 1, :] - v[:, :, 0, :]\n    v2 = v[:, :, 2, :] - v[:, :, 0, :]\n    normals = torch.cross(v1, v2, dim=-1)\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    v = v.reshape(v.shape[0], -1, v.shape[-2], v.shape[-1])\n    normals = normals.reshape(normals.shape[0], -1, normals.shape[-1])\n\n    return torch.cat([v, normals], dim=-1)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # compute faces normals w.r.t the vertices (considering batch dimension)\n    v1 = v[:, f[:, 0], :]\n    v2 = v[:, f[:, 1], :]\n    v3 = v[:, f[:, 2], :]\n    n = torch.cross(v2 - v1, v3 - v1, dim=dim)\n    n = n / torch.norm(n, dim=-1, keepdim=True)\n\n    # adjust faces tensor to match the batch dimension of the vertices tensor\n    if f.shape[0] != v.shape[0]:\n        f = f.unsqueeze(0).repeat(v.shape[0], 1, 1)\n        f = f.reshape(-1, f.shape[-1])\n\n    # gather the triangles from the vertices tensor\n    v1 = v1.reshape(-1, v1.shape[-1])\n    v2 = v2.reshape(-1, v2.shape[-1])\n    v3 = v3.reshape(-1, v3.shape[-1])\n    n = n.reshape(-1, n.shape[-1])\n    f = f.reshape(-1, f.shape[-1])\n    tri = torch.stack([v1, v2, v3], dim=1)\n\n    # reshape the result to maintain the original faces tensor structure\n    tri = tri.reshape(v.shape[0], -1, 3, tri.shape[-1])\n    n = n.reshape(v.shape[0], -1, 3, n.shape[-1])\n    f = f.reshape(v.shape[0], -1, 3, f.shape[-1])\n\n    return tri, n, f"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # v: BxNx3\n    # f: BxMx3\n    # dim: -2\n\n    # gather vertices\n    v = v[:, f, :]\n    # v: BxMx3x3\n\n    # compute faces normals w.r.t the vertices\n    v = v - v.mean(dim=dim, keepdim=True)\n    v = v / (v.norm(dim=-1, keepdim=True) + 1e-10)\n    n = torch.cross(v[:, :, 1, :] - v[:, :, 0, :], v[:, :, 2, :] - v[:, :, 0, :], dim=-1)\n    n = n / (n.norm(dim=-1, keepdim=True) + 1e-10)\n\n    # reshape to maintain original faces tensor structure\n    n = n.reshape(f.shape[0], -1, 3)\n\n    return n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # compute faces normals w.r.t the vertices (considering batch dimension)\n    # v: [B, N, 3]\n    # f: [B, M, 3]\n    # v_f: [B, M, 3, 3]\n    # n_f: [B, M, 3]\n\n    # expand f to match v\n    f_expanded = f\n    if f.dim() < v.dim():\n        f_expanded = f.unsqueeze(dim)\n    elif f.size(dim) < v.size(dim):\n        f_expanded = f.expand(*v.size()[:dim], *f.size()[dim:])\n\n    # gather v\n    v_f = v.gather(dim, f_expanded)\n\n    # compute normals\n    n_f = torch.cross(v_f[:, :, 1] - v_f[:, :, 0], v_f[:, :, 2] - v_f[:, :, 0], dim=-1)\n\n    # reshape to maintain original f structure\n    n_f = n_f.reshape(*f.size(), 3)\n\n    return n_f"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # check if faces are triangles\n    assert (f.shape[-1] == 3).all()\n\n    # check if v has batch dimension\n    if v.shape[dim] != f.shape[dim]:\n        # expand f to match v\n        f = f.expand(v.shape[dim], -1, -1)\n\n    # gather vertices\n    v1 = v.gather(dim, f[:, :, 0].unsqueeze(-1)).squeeze(-1)\n    v2 = v.gather(dim, f[:, :, 1].unsqueeze(-1)).squeeze(-1)\n    v3 = v.gather(dim, f[:, :, 2].unsqueeze(-1)).squeeze(-1)\n\n    # compute face normals\n    n = torch.cross(v2 - v1, v3 - v1, dim=-1)\n    n = F.normalize(n, p=2, dim=-1)\n\n    # reshape to maintain original faces tensor structure\n    n = n.reshape(f.shape[0], -1, 3)\n\n    return n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # compute faces normals w.r.t the vertices (considering batch dimension)\n    v = v.unsqueeze(dim)\n    v1 = v[:, f[:, 0], :]\n    v2 = v[:, f[:, 1], :]\n    v3 = v[:, f[:, 2], :]\n    face_normals = torch.cross(v2 - v1, v3 - v1, dim=dim)\n    face_normals = face_normals / (face_normals.norm(dim=-1, keepdim=True) + 1e-10)\n\n    # adjust f to match v\n    if f.shape[dim] < v.shape[dim]:\n        f = f.repeat_interleave(v.shape[dim] // f.shape[dim], dim=dim)\n\n    # gather vertices\n    v = v.reshape(-1, v.shape[-1])\n    v1 = v1.reshape(-1, v1.shape[-1])\n    v2 = v2.reshape(-1, v2.shape[-1])\n    v3 = v3.reshape(-1, v3.shape[-1])\n    face_normals = face_normals.reshape(-1, face_normals.shape[-1])\n    f = f.reshape(-1, 3)\n    v = v[f]\n    v1 = v1[f]\n    v2 = v2[f]\n    v3 = v3[f]\n    face_normals = face_normals[f]\n\n    # reshape to maintain original structure\n    v = v.reshape(v1.shape[0], 3, -1)\n    v1 = v1.reshape(v1.shape[0], 3, -1)\n    v2 = v2.reshape(v2.shape[0], 3, -1)\n    v3 = v3.reshape(v3.shape[0], 3, -1)\n    face_normals = face_normals.reshape(face_normals."}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # check if the faces tensor has the same batch dimension as the vertices tensor\n    if f.shape[dim] != v.shape[0]:\n        # if not, expand the faces tensor to match the batch dimension of the vertices tensor\n        f = f.expand(v.shape[0], -1, -1)\n\n    # gather the vertices to form the triangles\n    tris = v.gather(dim, f)\n\n    # reshape the triangles tensor to maintain the original faces tensor structure with additional dimensions for batch processing\n    tris = tris.reshape(*f.shape[:dim], -1, 3, *f.shape[dim + 1:])\n\n    return tris\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        return batch\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, tuple):\n        return tuple(add_batch(item) for item in batch)\n    elif isinstance(batch, list):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        return batch\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    elif isinstance(batch, (tuple, list)):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, tuple):\n        return tuple(add_batch(b) for b in batch)\n    elif isinstance(batch, list):\n        return [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise TypeError(\"Unsupported data type: {}\".format(type(batch)))"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    elif isinstance(batch, (tuple, list)):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    else:\n        raise ValueError(f\"Unsupported input type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    elif isinstance(batch, tuple):\n        return tuple(add_batch(item) for item in batch)\n    elif isinstance(batch, list):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    else:\n        raise TypeError(f\"Unsupported input type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (list, tuple)):\n        return type(batch)([add_batch(b) for b in batch])\n    elif isinstance(batch, dict):\n        return type(batch)({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):\n        return batch[None]\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, tuple):\n        return tuple(add_batch(item) for item in batch)\n    elif isinstance(batch, list):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        return batch\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise TypeError(f\"Unsupported data type: {type(batch)}\")\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, tuple):\n        return tuple([add_batch(b) for b in batch])\n    elif isinstance(batch, list):\n        return [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, tuple):\n        return tuple(add_batch(item) for item in batch)\n    elif isinstance(batch, list):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        return batch\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, tuple):\n        return tuple(add_batch(item) for item in batch)\n    elif isinstance(batch, list):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, (tuple, list)):\n        return [add_batch(item) for item in batch]\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return type(batch)(add_batch(b) for b in batch)\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        return batch\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, tuple) or isinstance(batch, list):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    # If the input is a torch.Tensor or numpy.ndarray, add a new dimension at the zeroth position\n    if isinstance(batch, torch.Tensor) or isinstance(batch, np.ndarray):\n        return batch[None, ...]\n\n    # If the input is a tuple or list, recursively apply the function to each element\n    elif isinstance(batch, tuple) or isinstance(batch, list):\n        return [add_batch(item) for item in batch]\n\n    # If the input is a dictionary, recursively apply the function to each value\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n\n    # If the input is a scalar, return a one-element list containing the scalar\n    else:\n        return [batch]\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    # Check if the input is a single item or a batch of items\n    if not isinstance(batch, (tuple, list, dict, torch.Tensor, np.ndarray)):\n        raise TypeError(\"Input must be a tuple, list, dict, torch.Tensor, or numpy.ndarray\")\n\n    # If the input is a single item, add a new dimension to it\n    if not isinstance(batch, (tuple, list, dict)):\n        return batch[None, ...]\n\n    # If the input is a batch of items, add a new dimension to each element\n    if isinstance(batch, tuple):\n        return tuple(add_batch(item) for item in batch)\n    elif isinstance(batch, list):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    # Check if the input is a dictionary\n    if isinstance(batch, dict):\n        # Create a new dictionary with the same keys\n        batch_new = {}\n        for key, value in batch.items():\n            # Recursively add the new dimension to each value\n            batch_new[key] = add_batch(value)\n        return batch_new\n\n    # Check if the input is a tuple\n    elif isinstance(batch, tuple):\n        # Convert the tuple to a list and add the new dimension\n        batch_new = list(batch)\n        for i in range(len(batch_new)):\n            batch_new[i] = add_batch(batch_new[i])\n        return tuple(batch_new)\n\n    # Check if the input is a list\n    elif isinstance(batch, list):\n        # Add the new dimension to each element of the list\n        batch_new = []\n        for item in batch:\n            batch_new.append(add_batch(item))\n        return batch_new\n\n    # Check if the input is a NumPy array\n    elif isinstance(batch, np.ndarray):\n        # Add the new dimension to the NumPy array\n        batch_new = np.expand_dims(batch, axis=0)\n        return batch_new\n\n    # Check if the input is a PyTorch tensor\n    elif isinstance(batch, torch.Tensor):\n        # Add the new dimension to the PyTorch tensor\n        batch_new = batch.unsqueeze(0)\n        return batch_new\n\n    # If the input is not a dictionary, tuple, list, NumPy array, or PyTorch tensor, return the original input\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.meta = dotdict()\n        batch.meta.camera = dotdict()\n        batch.meta.camera.focal = torch.tensor([self.focal])\n        batch.meta.camera.center = torch.tensor([self.center])\n        batch.meta.camera.k1 = torch.tensor([self.k1])\n        batch.meta.camera.k2 = torch.tensor([self.k2])\n        batch.meta.camera.p1 = torch.tensor([self.p1])\n        batch.meta.camera.p2 = torch.tensor([self.p2])\n        batch.meta.camera.k3 = torch.tensor([self.k3])\n        batch.meta.camera.k4 = torch.tensor([self.k4])\n        batch.meta.camera.k5 = torch.tensor([self.k5])\n        batch.meta.camera.k6 = torch.tensor([self.k6])\n        batch.meta.camera.s1 = torch.tensor([self.s1])\n        batch.meta.camera.s2 = torch.tensor([self.s2])\n        batch.meta.camera.s3 = torch.tensor([self.s3])\n        batch.meta.camera.s4 = torch.tensor([self.s4])\n        batch.meta.camera.t1 = torch.tensor([self.t1])\n        batch.meta.camera.t2 = torch.tensor([self.t2])\n        batch.meta.camera.t3 = torch.tensor([self.t3])\n        batch.meta.camera.width = torch.tensor([self.width])\n        batch.meta.camera.height = torch.tensor([self.height])\n        batch.meta.camera.min_depth = torch.tensor([self.min_depth])\n        batch.meta.camera.max_depth = torch.tensor([self.max_depth])\n        batch.meta.camera.min_x"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.meta = dotdict()\n        batch.meta.height = self.height\n        batch.meta.width = self.width\n        batch.meta.focal = self.focal\n        batch.meta.center = self.center\n        batch.meta.k1 = self.k1\n        batch.meta.k2 = self.k2\n        batch.meta.p1 = self.p1\n        batch.meta.p2 = self.p2\n        batch.meta.focal_x = self.focal_x\n        batch.meta.focal_y = self.focal_y\n        batch.meta.center_x = self.center_x\n        batch.meta.center_y = self.center_y\n        batch.meta.shear = self.shear\n        batch.meta.radial = self.radial\n        batch.meta.tangential = self.tangential\n        batch.meta.distortion = self.distortion\n        batch.meta.distortion_center = self.distortion_center\n        batch.meta.projection_type = self.projection_type\n        batch.meta.camera_matrix = self.camera_matrix\n        batch.meta.distortion_coefficients = self.distortion_coefficients\n        batch.meta.projection_matrix = self.projection_matrix\n        batch.meta.projection_matrix_x = self.projection_matrix_x\n        batch.meta.projection_matrix_y = self.projection_matrix_y\n        batch.meta.projection_matrix_z = self.projection_matrix_z\n        batch.meta.projection_matrix_x_inv = self.projection_matrix_x_inv\n        batch.meta.projection_matrix_y_inv = self.projection_matrix_y_inv\n        batch.meta.projection_matrix_z_inv = self.projection_matrix_z_inv\n        batch.meta.fov = self.fov\n        batch.meta.fov_x = self.fov_x\n        batch.meta.fov_y ="}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Convert camera parameters into tensors\n        batch = dotdict({\n            'meta': dotdict({\n                'camera_type': self.camera_type,\n                'width': self.width,\n                'height': self.height,\n                'focal_length': self.focal_length,\n                'principal_point': self.principal_point,\n                'distortion': self.distortion,\n                'distortion_model': self.distortion_model,\n                'image_size': self.image_size,\n                'camera_matrix': self.camera_matrix,\n                'projection_matrix': self.projection_matrix,\n                'rectification_matrix': self.rectification_matrix,\n                'projection_error': self.projection_error,\n                'tracking_state': self.tracking_state,\n                'map1': self.map1,\n                'map2': self.map2\n            }),\n            'focal_length': torch.tensor(self.focal_length, dtype=torch.float32),\n            'principal_point': torch.tensor(self.principal_point, dtype=torch.float32),\n            'distortion': torch.tensor(self.distortion, dtype=torch.float32),\n            'distortion_model': torch.tensor(self.distortion_model, dtype=torch.float32),\n            'image_size': torch.tensor(self.image_size, dtype=torch.float32),\n            'camera_matrix': torch.tensor(self.camera_matrix, dtype=torch.float32),\n            'projection_matrix': torch.tensor(self.projection_matrix, dtype=torch.float32),\n            'rectification_matrix': torch.tensor(self.rectification_matrix, dtype=torch.float32),\n            'projection_error': torch.tensor(self.projection_error, dtype=torch.float32),\n            'tracking_state': torch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Create a dictionary to hold the camera parameters and GUI elements\n        batch = dotdict()\n\n        # Convert the camera parameters to tensors\n        batch.intrinsic_matrix = torch.tensor(self.intrinsic_matrix, dtype=torch.float32)\n        batch.distortion_coefficients = torch.tensor(self.distortion_coefficients, dtype=torch.float32)\n        batch.extrinsic_matrix = torch.tensor(self.extrinsic_matrix, dtype=torch.float32)\n        batch.meta = dotdict()\n        batch.meta.intrinsic_matrix = torch.tensor(self.intrinsic_matrix, dtype=torch.float32)\n        batch.meta.distortion_coefficients = torch.tensor(self.distortion_coefficients, dtype=torch.float32)\n        batch.meta.extrinsic_matrix = torch.tensor(self.extrinsic_matrix, dtype=torch.float32)\n\n        # Return the batch dictionary\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Convert parameters into tensors\n        for k, v in self.items():\n            if isinstance(v, (int, float)):\n                self[k] = torch.tensor([v], dtype=torch.float32)\n            elif isinstance(v, list):\n                self[k] = torch.tensor(v, dtype=torch.float32)\n            elif isinstance(v, np.ndarray):\n                self[k] = torch.from_numpy(v).float()\n\n        # Convert to batch\n        batch = dotdict()\n        for k, v in self.items():\n            batch[k] = v\n            batch.meta[k] = v.shape\n\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.meta = dotdict()\n        batch.meta.size = self.size\n        batch.meta.size_y = self.size[0]\n        batch.meta.size_x = self.size[1]\n        batch.meta.size_z = self.size[2]\n        batch.meta.pixel_size = self.pixel_size\n        batch.meta.pixel_size_y = self.pixel_size[0]\n        batch.meta.pixel_size_x = self.pixel_size[1]\n        batch.meta.pixel_size_z = self.pixel_size[2]\n        batch.meta.distance = self.distance\n        batch.meta.distance_y = self.distance[0]\n        batch.meta.distance_x = self.distance[1]\n        batch.meta.distance_z = self.distance[2]\n        batch.meta.center = self.center\n        batch.meta.center_y = self.center[0]\n        batch.meta.center_x = self.center[1]\n        batch.meta.center_z = self.center[2]\n        batch.meta.rotation = self.rotation\n        batch.meta.rotation_y = self.rotation[0]\n        batch.meta.rotation_x = self.rotation[1]\n        batch.meta.rotation_z = self.rotation[2]\n        batch.meta.fov = self.fov\n        batch.meta.fov_y = self.fov[0]\n        batch.meta.fov_x = self.fov[1]\n        batch.meta.fov_z = self.fov[2]\n        batch.meta.fov_x_y = self.fov[0]\n        batch.meta.fov_y_x = self.fov[1]\n        batch.meta.fov_z_x = self.fov[2]\n        batch.meta.fov_z_y = self.fov[0]\n        batch.meta.fov_y"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.meta = dotdict()\n        batch.meta.camera_id = torch.tensor([self.camera_id])\n        batch.meta.camera_name = self.camera_name\n        batch.meta.camera_type = self.camera_type\n        batch.meta.camera_model = self.camera_model\n        batch.meta.camera_resolution = torch.tensor([self.camera_resolution[0], self.camera_resolution[1]])\n        batch.meta.camera_fps = torch.tensor([self.camera_fps])\n        batch.meta.camera_intrinsic = torch.tensor(self.camera_intrinsic)\n        batch.meta.camera_distortion = torch.tensor(self.camera_distortion)\n        batch.meta.camera_extrinsic = torch.tensor(self.camera_extrinsic)\n        batch.meta.camera_pose = torch.tensor(self.camera_pose)\n        batch.meta.camera_rotation = torch.tensor(self.camera_rotation)\n        batch.meta.camera_translation = torch.tensor(self.camera_translation)\n        batch.meta.camera_projection = torch.tensor(self.camera_projection)\n        batch.meta.camera_frustum = torch.tensor(self.camera_frustum)\n        batch.meta.camera_image_size = torch.tensor([self.camera_image_size[0], self.camera_image_size[1]])\n        batch.meta.camera_image_center = torch.tensor([self.camera_image_center[0], self.camera_image_center[1]])\n        batch.meta.camera_image_scale = torch.tensor([self.camera_image_scale[0], self.camera_image_scale[1]])\n        batch.meta.camera_image_rotation = torch.tensor(self.camera_image"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        return dotdict({\n            'meta': dotdict({\n                'cam_id': self.cam_id,\n                'width': self.width,\n                'height': self.height,\n                'scaling_factor': self.scaling_factor,\n                'focal_length': self.focal_length,\n                'principal_point': self.principal_point,\n                'distortion': self.distortion,\n                'skew': self.skew,\n                'R': self.R,\n                'T': self.T,\n                'P': self.P,\n                'R1': self.R1,\n                'P1': self.P1,\n                'map1': self.map1,\n                'map2': self.map2,\n                'fov': self.fov,\n                'fov_x': self.fov_x,\n                'fov_y': self.fov_y,\n                'K': self.K,\n                'D': self.D,\n                'DIM': self.DIM,\n                's': self.s,\n                'p1': self.p1,\n                'p2': self.p2,\n                'k1': self.k1,\n                'k2': self.k2,\n                'k3': self.k3,\n                'k4': self.k4,\n                'k5': self.k5,\n                'k6': self.k6,\n                'p1_roi': self.p1_roi,\n                'p2_roi': self.p2_roi,\n                'roi': self.roi,\n                'rectification_matrix': self.rectification_matrix,\n                'projection_matrix': self.projection_matrix,\n                'Q': self.Q,\n                'valid_roi': self.valid_roi,\n                'rectification_matrix_inv': self.rectification_matrix_inv,\n                'projection_matrix_inv': self.projection_matrix_inv,\n                'Q_inv': self.Q_"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        for key, value in self.__dict__.items():\n            if isinstance(value, torch.Tensor):\n                batch[key] = value\n            elif isinstance(value, dict):\n                batch[key] = dotdict()\n                for subkey, subvalue in value.items():\n                    batch[key][subkey] = subvalue\n            else:\n                batch[key] = value\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.meta = dotdict()\n        batch.meta.batch_size = 1\n        batch.meta.num_views = 1\n\n        batch.intrinsics = torch.tensor(self.intrinsics.copy())\n        batch.extrinsics = torch.tensor(self.extrinsics.copy())\n        batch.extrinsics_inv = torch.tensor(self.extrinsics_inv.copy())\n        batch.R = torch.tensor(self.R.copy())\n        batch.t = torch.tensor(self.t.copy())\n        batch.Rt = torch.tensor(self.Rt.copy())\n        batch.Rt_inv = torch.tensor(self.Rt_inv.copy())\n        batch.focal_length = torch.tensor(self.focal_length.copy())\n        batch.principal_point = torch.tensor(self.principal_point.copy())\n        batch.image_size = torch.tensor(self.image_size.copy())\n        batch.distortion = torch.tensor(self.distortion.copy())\n        batch.distortion_type = torch.tensor(self.distortion_type.copy())\n        batch.distortion_coeffs = torch.tensor(self.distortion_coeffs.copy())\n        batch.distortion_center = torch.tensor(self.distortion_center.copy())\n        batch.distortion_k1 = torch.tensor(self.distortion_k1.copy())\n        batch.distortion_k2 = torch.tensor(self.distortion_k2.copy())\n        batch.distortion_k3 = torch.tensor(self.distortion_k3.copy())\n        batch.distortion_k4 = torch.tensor(self.distortion_k4.copy())\n        batch.distortion_k5 = torch.tensor(self.distortion_k5.copy())\n        batch.distortion_k6 = torch.tensor(self.distortion_k6.copy())\n       "}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict({\n            'meta': dotdict({\n                'camera_id': self.camera_id,\n                'width': self.width,\n                'height': self.height,\n                'focal': self.focal,\n                'center': self.center,\n                'k1': self.k1,\n                'k2': self.k2,\n                'p1': self.p1,\n                'p2': self.p2,\n                'fov_x': self.fov_x,\n                'fov_y': self.fov_y,\n                'fov': self.fov,\n                'principal_point': self.principal_point,\n                'scaling_factor': self.scaling_factor,\n                'scaling_matrix': self.scaling_matrix,\n                'projection_matrix': self.projection_matrix,\n                'projection_matrix_inv': self.projection_matrix_inv,\n                'projection_matrix_inv_flat': self.projection_matrix_inv_flat,\n                'projection_matrix_flat': self.projection_matrix_flat,\n                'intrinsic_matrix': self.intrinsic_matrix,\n                'intrinsic_matrix_inv': self.intrinsic_matrix_inv,\n                'intrinsic_matrix_inv_flat': self.intrinsic_matrix_inv_flat,\n                'intrinsic_matrix_flat': self.intrinsic_matrix_flat,\n                'distortion_coeffs': self.distortion_coeffs,\n                'distortion_model': self.distortion_model,\n                'distortion_model_id': self.distortion_model_id,\n                'distortion_model_name': self.distortion_model_name,\n                'distortion_model_type': self.distortion_model_type,\n                'distortion_model_order': self.distortion_model_order,\n                'distortion_model_params': self.distortion_model_params,\n                'distortion_model_params_flat': self"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Create a dotdict instance to store the parameters\n        batch = dotdict()\n\n        # Convert the camera parameters into tensors\n        batch.camera_mat = torch.tensor(self.camera_mat)\n        batch.dist_coef = torch.tensor(self.dist_coef)\n        batch.camera_mat_inv = torch.tensor(self.camera_mat_inv)\n        batch.dist_coef_inv = torch.tensor(self.dist_coef_inv)\n        batch.camera_mat_inv_h = torch.tensor(self.camera_mat_inv_h)\n        batch.dist_coef_inv_h = torch.tensor(self.dist_coef_inv_h)\n        batch.camera_mat_inv_h_flat = torch.tensor(self.camera_mat_inv_h_flat)\n        batch.dist_coef_inv_h_flat = torch.tensor(self.dist_coef_inv_h_flat)\n        batch.camera_mat_inv_flat = torch.tensor(self.camera_mat_inv_flat)\n        batch.dist_coef_inv_flat = torch.tensor(self.dist_coef_inv_flat)\n\n        # Convert the GUI related elements into tensors\n        batch.gui = dotdict()\n        batch.gui.img_size = torch.tensor(self.gui.img_size)\n        batch.gui.img_size_h = torch.tensor(self.gui.img_size_h)\n        batch.gui.img_size_w = torch.tensor(self.gui.img_size_w)\n        batch.gui.img_size_h_flat = torch.tensor(self.gui.img_size_h_flat)\n        batch.gui.img_size_w_flat = torch.tensor(self.gui.img_size_w_flat)\n        batch.gui.img_size_flat = torch.tensor(self.gui.img_size_flat)\n        batch.gui.img_"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.meta = dotdict()\n        batch.meta.K = torch.tensor(self.K.copy())\n        batch.meta.dist = torch.tensor(self.dist.copy())\n        batch.meta.R = torch.tensor(self.R.copy())\n        batch.meta.T = torch.tensor(self.T.copy())\n        batch.meta.P = torch.tensor(self.P.copy())\n        batch.meta.fps = torch.tensor(self.fps)\n        batch.meta.height = torch.tensor(self.height)\n        batch.meta.width = torch.tensor(self.width)\n        batch.meta.cam_id = torch.tensor(self.cam_id)\n        batch.meta.device = torch.tensor(self.device)\n        batch.meta.cam_id = torch.tensor(self.cam_id)\n        batch.meta.device = torch.tensor(self.device)\n        batch.meta.flip_x = torch.tensor(self.flip_x)\n        batch.meta.flip_y = torch.tensor(self.flip_y)\n        batch.meta.flip_z = torch.tensor(self.flip_z)\n        batch.meta.rot_x = torch.tensor(self.rot_x)\n        batch.meta.rot_y = torch.tensor(self.rot_y)\n        batch.meta.rot_z = torch.tensor(self.rot_z)\n        batch.meta.trans_x = torch.tensor(self.trans_x)\n        batch.meta.trans_y = torch.tensor(self.trans_y)\n        batch.meta.trans_z = torch.tensor(self.trans_z)\n        batch.meta.scale = torch.tensor(self.scale)\n        batch.meta.scale_x = torch.tensor(self.scale_x)\n        batch.meta.scale_y = torch.tensor(self.scale_y)\n        batch.meta."}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        return dotdict({\n            'meta': dotdict({\n                'cam_id': self.cam_id,\n                'img_res': self.img_res,\n                'scale': self.scale,\n                'near': self.near,\n                'far': self.far,\n                'fov_x': self.fov_x,\n                'fov_y': self.fov_y,\n                'fl_x': self.fl_x,\n                'fl_y': self.fl_y,\n                'cx': self.cx,\n                'cy': self.cy,\n                'w2c_mats': self.w2c_mats,\n                'c2w_mats': self.c2w_mats,\n                'depths': self.depths,\n                'poses': self.poses,\n                'intrinsics': self.intrinsics,\n                'imgs': self.imgs,\n                'pose_paths': self.pose_paths,\n                'img_paths': self.img_paths,\n                'scale_factors': self.scale_factors,\n                'near_far_list': self.near_far_list,\n                'fov_list': self.fov_list,\n                'w2c_mats_list': self.w2c_mats_list,\n                'c2w_mats_list': self.c2w_mats_list,\n                'intrinsics_list': self.intrinsics_list,\n                'depths_list': self.depths_list,\n                'poses_list': self.poses_list,\n                'imgs_list': self.imgs_list,\n                'img_res_list': self.img_res_list,\n                'img_res_orig_list': self.img_res_orig_list,\n                'camera_angle_x': self.camera_angle_x,\n                'camera_angle_y': self.camera_angle_y,\n                'camera_angle_x_list': self.camera"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # convert all parameters into tensors\n        batch = dotdict({\n            'meta': dotdict({\n                'cam_id': self.cam_id,\n                'width': self.width,\n                'height': self.height,\n                'fps': self.fps,\n                't': self.t,\n                'R': self.R,\n                'tvec': self.tvec,\n                'Rvec': self.Rvec,\n                'img_scale': self.img_scale,\n                'img_width': self.img_width,\n                'img_height': self.img_height,\n                'img_scale_x': self.img_scale_x,\n                'img_scale_y': self.img_scale_y,\n                'img_width_x': self.img_width_x,\n                'img_width_y': self.img_width_y,\n                'img_height_x': self.img_height_x,\n                'img_height_y': self.img_height_y,\n                'img_center_x': self.img_center_x,\n                'img_center_y': self.img_center_y,\n                'cam_center_x': self.cam_center_x,\n                'cam_center_y': self.cam_center_y,\n                'cam_center_z': self.cam_center_z,\n                'cam_focal_length': self.cam_focal_length,\n                'cam_fov_x': self.cam_fov_x,\n                'cam_fov_y': self.cam_fov_y,\n                'cam_pixel_width': self.cam_pixel_width,\n                'cam_pixel_height': self.cam_pixel_height,\n                'cam_focal_length_x': self.cam_focal_length_x,\n                'cam_focal_length_y': self.cam_focal_length_y,\n                'cam_principal_point_x': self.cam_principal_point_x,\n                'cam"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.meta = dotdict()\n        batch.meta.img_size = torch.tensor(self.img_size)\n        batch.meta.device = self.device\n        batch.meta.c2w = torch.tensor(self.c2w)\n        batch.meta.w2c = torch.tensor(self.w2c)\n        batch.meta.c2w_path = self.c2w_path\n        batch.meta.w2c_path = self.w2c_path\n        batch.meta.pose_paths = self.pose_paths\n        batch.meta.intrinsics = torch.tensor(self.intrinsics)\n        batch.meta.extrinsics = torch.tensor(self.extrinsics)\n        batch.meta.depth_min = self.depth_min\n        batch.meta.depth_interval = self.depth_interval\n        batch.meta.depth_max = self.depth_max\n        batch.meta.depth_interval = self.depth_interval\n        batch.meta.focal_length = torch.tensor(self.focal_length)\n        batch.meta.focal_length_x = torch.tensor(self.focal_length_x)\n        batch.meta.focal_length_y = torch.tensor(self.focal_length_y)\n        batch.meta.principal_point_x = torch.tensor(self.principal_point_x)\n        batch.meta.principal_point_y = torch.tensor(self.principal_point_y)\n        batch.meta.camera_matrix = torch.tensor(self.camera_matrix)\n        batch.meta.distortion_coefficients = torch.tensor(self.distortion_coefficients)\n        batch.meta.distortion_model = self.distortion_model\n        batch.meta.image_size = torch.tensor(self.image_size)\n        batch.meta.image_width = self.image_width\n        batch.meta.image_height = self.image_height\n       "}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Convert all the camera parameters into tensors\n        for key in self.keys():\n            if isinstance(self[key], np.ndarray):\n                self[key] = torch.from_numpy(self[key])\n\n        # Create a nested dictionary with the same content as the original dictionary\n        meta = {}\n        for key in self.keys():\n            meta[key] = self[key]\n\n        # Create a dotdict instance with the same content as the original dictionary\n        batch = dotdict()\n        batch.meta = dotdict(meta)\n        batch.update(meta)\n\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Convert all the camera parameters to tensors\n        for key, value in self.items():\n            if isinstance(value, np.ndarray):\n                self[key] = torch.from_numpy(value).float()\n\n        # Convert the GUI related elements into tensors\n        for key, value in self.gui.items():\n            if isinstance(value, np.ndarray):\n                self.gui[key] = torch.from_numpy(value).float()\n\n        # Create a structured dictionary with the camera parameters and GUI elements\n        batch = dotdict()\n        batch.meta = dotdict()\n        for key, value in self.items():\n            batch[key] = value\n            batch.meta[key] = value.shape\n\n        # Create a structured dictionary with the camera parameters and GUI elements\n        batch.gui = dotdict()\n        batch.gui.meta = dotdict()\n        for key, value in self.gui.items():\n            batch.gui[key] = value\n            batch.gui.meta[key] = value.shape\n\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Create a dictionary to store the batch\n        batch = dotdict()\n\n        # Convert the camera parameters into tensors\n        batch.K = torch.tensor(self.K)\n        batch.R = torch.tensor(self.R)\n        batch.t = torch.tensor(self.t)\n        batch.distortion = torch.tensor(self.distortion)\n        batch.resolution = torch.tensor(self.resolution)\n\n        # Convert the GUI related elements into tensors\n        batch.gui_pos = torch.tensor(self.gui_pos)\n        batch.gui_size = torch.tensor(self.gui_size)\n        batch.gui_color = torch.tensor(self.gui_color)\n\n        # Create a nested dictionary for the batch\n        batch.meta = dotdict()\n\n        # Map the camera parameters to the nested dictionary\n        batch.meta.K = batch.K\n        batch.meta.R = batch.R\n        batch.meta.t = batch.t\n        batch.meta.distortion = batch.distortion\n        batch.meta.resolution = batch.resolution\n\n        # Map the GUI related elements to the nested dictionary\n        batch.meta.gui_pos = batch.gui_pos\n        batch.meta.gui_size = batch.gui_size\n        batch.meta.gui_color = batch.gui_color\n\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Convert all camera parameters to tensors\n        for key, value in self.items():\n            if isinstance(value, np.ndarray):\n                self[key] = torch.from_numpy(value)\n            elif isinstance(value, list):\n                self[key] = torch.tensor(value)\n            elif isinstance(value, int):\n                self[key] = torch.tensor([value])\n            elif isinstance(value, float):\n                self[key] = torch.tensor([value])\n            elif isinstance(value, str):\n                self[key] = torch.tensor([value])\n\n        # Create a nested dictionary for meta data\n        meta = dotdict()\n        meta.cameras = dotdict()\n        meta.cameras.focal_length = torch.tensor(self.focal_length)\n        meta.cameras.principal_point = torch.tensor(self.principal_point)\n        meta.cameras.distortion = torch.tensor(self.distortion)\n        meta.cameras.resolution = torch.tensor(self.resolution)\n        meta.cameras.fov = torch.tensor(self.fov)\n        meta.cameras.intrinsics = torch.tensor(self.intrinsics)\n        meta.cameras.extrinsics = torch.tensor(self.extrinsics)\n        meta.cameras.transforms = torch.tensor(self.transforms)\n        meta.cameras.projection = torch.tensor(self.projection)\n        meta.cameras.world_up = torch.tensor(self.world_up)\n        meta.cameras.world_up_vector = torch.tensor(self.world_up_vector)\n        meta.cameras.world_up_vector_homogeneous = torch.tensor(self.world_up_vector_homogeneous)\n        meta.cameras.world_up_vector_homogeneous_inverse = torch.tensor(self.world_up_vector_homogeneous_in"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            agent_state_dict = agent_state.to_dict()\n            self.save_state(agent_state_dict)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            agent_state_dict = agent_state.to_dict()\n            self.persistence_manager.save(agent_state_dict)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.serialize()\n            self.save_state(agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        # Check if the agent is a working agent and not a prime agent\n        if agent.is_working_agent() and not agent.is_prime_agent():\n            # Serialize the agent state into a dictionary format\n            agent_state = agent.serialize()\n            # Save the agent state using the persistence mechanism provided by the AgentPersistenceManager\n            self.persistence_manager.save_agent(agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.serialize()\n            self.persistence_manager.save_agent(agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            agent_state_dict = agent_state.to_dict()\n            self.save_state(agent_state_dict)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            agent_state_dict = agent_state.to_dict()\n            self.persistence_manager.save_agent(agent_state_dict)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            serialized_agent_state = self.serialize_agent_state(agent_state)\n            self.save_state(serialized_agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            state_dict = agent.get_state_dict()\n            self.save_state(state_dict)\n\n        # Save the agent state if it is a prime agent\n        if agent.is_prime_agent():\n            state_dict = agent.get_state_dict()\n            self.save_state(state_dict)\n\n        # Save the agent state if it is a working agent\n        if agent.is_working_agent():\n            state_dict = agent.get_state_dict()\n            self.save_state(state_dict)\n\n        # Save the agent state if it is a prime agent\n        if agent.is_prime_agent():\n            state_dict = agent.get_state_dict()\n            self.save_state(state_dict)\n\n        # Save the agent state if it is a working agent\n        if agent.is_working_agent():\n            state_dict = agent.get_state_dict()\n            self.save_state(state_dict)\n\n        # Save the agent state if it is a prime agent\n        if agent.is_prime_agent():\n            state_dict = agent.get_state_dict()\n            self.save_state(state_dict)\n\n        # Save the agent state if it is a working agent\n        if agent.is_working_agent():\n            state_dict = agent.get_state_dict()\n            self.save_state(state_dict)\n\n        # Save the agent state if it is a prime agent\n        if agent.is_prime_agent():\n            state_dict = agent.get_state_dict()\n            self.save_state(state_dict)\n\n        # Save the agent state if it is a working agent\n        if agent.is_working_agent():\n            state_dict = agent.get_state_dict()\n            self.save_state(state_dict)\n\n        # Save the agent state if it is a prime agent\n        if agent.is_prime_agent():\n            state_dict = agent.get_state_dict()\n            self.save_"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            agent_state_dict = agent_state.to_dict()\n            self.save_agent_state(agent_state_dict)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            serialized_agent_state = self.serialize_agent_state(agent_state)\n            self.save_agent_state(serialized_agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            agent_state_dict = agent_state.to_dict()\n            self.save_agent_state(agent_state_dict)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.serialize()\n            self.persistence_manager.save_state(agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            serialized_agent_state = self.serializer.serialize(agent_state)\n            self.persistence_manager.save(serialized_agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            self.persistence_manager.save_agent_state(agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            serialized_agent_state = self.serialize_agent_state(agent_state)\n            self.save_agent_state(serialized_agent_state)\n    "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            state = agent.get_state()\n            state_dict = state.to_dict()\n            self.persistence_manager.save_state(state_dict)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.serialize()\n            self.persistence_manager.save_agent(agent_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_state = self.serialize_agent(agent)\n            self.save_state(agent.id, serialized_state)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = self.serialize_agent(agent)\n            self.save_agent_state(agent_state)\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if not self.agents:\n            return None, -np.inf\n\n        # Calculate the cosine similarity between the purpose embedding of each agent and the given purpose embedding\n        similarity_scores = np.dot(self.agents_purpose_embeddings, purpose_embedding) / (\n            np.linalg.norm(self.agents_purpose_embeddings, axis=1) * np.linalg.norm(purpose_embedding)\n        )\n\n        # Find the index of the agent with the highest similarity score\n        max_index = np.argmax(similarity_scores)\n\n        # Return the agent with the highest similarity score and the similarity score itself\n        return self.agents[max_index], similarity_scores[max_index]"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if not self.agents:\n            return None, -np.inf\n\n        # Calculate the cosine similarity between the purpose embedding of each agent and the given purpose embedding\n        similarity_scores = np.array([self._cosine_similarity(agent.purpose_embedding, purpose_embedding) for agent in self.agents])\n\n        # Find the index of the agent with the highest similarity score\n        max_index = np.argmax(similarity_scores)\n\n        # Return the agent with the highest similarity score and the similarity score itself\n        return self.agents[max_index], similarity_scores[max_index]\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        closest_agent = None\n        highest_similarity = -np.inf\n        for agent in self.agents:\n            similarity = np.dot(agent.purpose_embedding, purpose_embedding) / (\n                np.linalg.norm(agent.purpose_embedding) * np.linalg.norm(purpose_embedding)\n            )\n            if similarity > highest_similarity:\n                highest_similarity = similarity\n                closest_agent = agent\n        return closest_agent, highest_similarity\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if not self.agents:\n            return None, -np.inf\n        # Calculate the cosine similarity between the purpose embedding and the purpose embeddings of the agents\n        similarities = np.dot(self.purpose_embeddings, purpose_embedding) / (np.linalg.norm(self.purpose_embeddings, axis=1) * np.linalg.norm(purpose_embedding))\n        # Find the index of the agent with the highest similarity score\n        max_idx = np.argmax(similarities)\n        # Return the agent with the highest similarity score and the similarity score\n        return self.agents[max_idx], similarities[max_idx]\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if self.agents is None:\n            return None, -np.inf\n        # Calculate the cosine similarity between the purpose embedding of each agent and the given purpose embedding\n        similarity_scores = np.array([self.cosine_similarity(agent.purpose_embedding, purpose_embedding) for agent in self.agents])\n        # Find the index of the agent with the highest similarity score\n        max_index = np.argmax(similarity_scores)\n        # Return the agent with the highest similarity score and the similarity score itself\n        return self.agents[max_index], similarity_scores[max_index]\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Calculate the cosine similarity between the purpose embedding of each agent and the given purpose embedding\n        similarity_scores = np.dot(self.agents_embeddings, purpose_embedding) / (\n            np.linalg.norm(self.agents_embeddings, axis=1) * np.linalg.norm(purpose_embedding)\n        )\n\n        # Find the index of the agent with the highest similarity score\n        closest_agent_index = np.argmax(similarity_scores)\n\n        # Get the agent with the highest similarity score\n        closest_agent = self.agents[closest_agent_index]\n\n        # Get the similarity score of the closest agent\n        similarity_score = similarity_scores[closest_agent_index]\n\n        return closest_agent, similarity_score\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if not self.agents:\n            return None, -np.inf\n\n        # Calculate the cosine similarity between the purpose embedding of each agent and the given purpose embedding\n        similarity_scores = [cosine_similarity(agent.purpose_embedding, purpose_embedding) for agent in self.agents]\n\n        # Find the index of the agent with the highest similarity score\n        max_similarity_index = np.argmax(similarity_scores)\n\n        # Return the agent with the highest similarity score and the similarity score itself\n        return self.agents[max_similarity_index], similarity_scores[max_similarity_index]\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if len(self.agents) == 0:\n            return None, -np.inf\n\n        # Calculate the cosine similarity between the purpose embedding and the purpose embeddings of all agents\n        similarity_scores = np.dot(self.agent_purpose_embeddings, purpose_embedding) / (\n            np.linalg.norm(self.agent_purpose_embeddings, axis=1) * np.linalg.norm(purpose_embedding)\n        )\n\n        # Find the index of the agent with the highest similarity score\n        best_agent_index = np.argmax(similarity_scores)\n\n        # Return the agent with the highest similarity score and the similarity score\n        return self.agents[best_agent_index], similarity_scores[best_agent_index]\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if not self.agents:\n            return None, -np.inf\n\n        # Calculate the cosine similarity between the purpose embedding of each agent and the given purpose embedding\n        similarities = np.array([self.cosine_similarity(agent.purpose_embedding, purpose_embedding) for agent in self.agents])\n\n        # Find the index of the agent with the highest similarity score\n        max_index = np.argmax(similarities)\n\n        # Return the agent with the highest similarity score and the similarity score itself\n        return self.agents[max_index], similarities[max_index]\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if not self.agents:\n            return None, -np.inf\n\n        max_similarity = -np.inf\n        closest_agent = None\n\n        for agent in self.agents:\n            similarity = self.similarity_function(agent.purpose_embedding, purpose_embedding)\n            if similarity > max_similarity:\n                max_similarity = similarity\n                closest_agent = agent\n\n        return closest_agent, max_similarity\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if not self.agents:\n            return None, -np.inf\n\n        # Calculate the cosine similarity between the purpose embedding and the purpose embeddings of the agents\n        similarities = np.dot(self.agent_purpose_embeddings, purpose_embedding) / (\n                np.linalg.norm(self.agent_purpose_embeddings, axis=1) * np.linalg.norm(purpose_embedding))\n\n        # Find the agent with the highest similarity score\n        max_similarity_idx = np.argmax(similarities)\n        max_similarity = similarities[max_similarity_idx]\n\n        return self.agents[max_similarity_idx], max_similarity\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Calculate the cosine similarity between the purpose embedding of each agent and the given purpose embedding\n        cosine_similarities = np.dot(self.agent_embeddings, purpose_embedding) / (\n            np.linalg.norm(self.agent_embeddings, axis=1) * np.linalg.norm(purpose_embedding)\n        )\n\n        # Find the index of the agent with the highest similarity score\n        closest_agent_index = np.argmax(cosine_similarities)\n\n        # Return the agent with the highest similarity score\n        return self.agents[closest_agent_index], cosine_similarities[closest_agent_index]\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Initialize the best agent and similarity score to None and -inf\n        best_agent: Optional[Agent] = None\n        best_similarity_score: float = -np.inf\n\n        # Iterate over all agents in the agent pool\n        for agent in self.agent_pool:\n            # Calculate the cosine similarity between the purpose embedding of the current agent and the given purpose embedding\n            similarity_score = cosine_similarity(agent.purpose_embedding, purpose_embedding)\n\n            # If the similarity score is higher than the current best similarity score, update the best agent and similarity score\n            if similarity_score > best_similarity_score:\n                best_agent = agent\n                best_similarity_score = similarity_score\n\n        # Return the best agent and similarity score\n        return best_agent, best_similarity_score\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if len(self.agents) == 0:\n            return None, -np.inf\n\n        # Calculate the cosine similarity between the purpose embedding of each agent and the given purpose embedding\n        similarity_scores = np.array([self.cosine_similarity(agent.purpose_embedding, purpose_embedding) for agent in self.agents])\n\n        # Find the index of the agent with the highest similarity score\n        best_agent_index = np.argmax(similarity_scores)\n\n        # Return the agent with the highest similarity score\n        return self.agents[best_agent_index], similarity_scores[best_agent_index]\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if self.agent_embeddings is None:\n            return None, -np.inf\n        try:\n            # Calculate the cosine similarity between the purpose embedding of each agent and the given purpose embedding\n            similarity_scores = np.dot(self.agent_embeddings, purpose_embedding) / (\n                np.linalg.norm(self.agent_embeddings, axis=1) * np.linalg.norm(purpose_embedding)\n            )\n            # Find the agent with the highest similarity score\n            closest_agent_idx = np.argmax(similarity_scores)\n            # Return the agent with the highest similarity score and the similarity score\n            return self.agents[closest_agent_idx], similarity_scores[closest_agent_idx]\n        except Exception as e:\n            print(e)\n            return None, -np.inf\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if not self.agents:\n            return None, -np.inf\n\n        # Calculate the cosine similarity between the purpose embedding of each agent and the given purpose embedding\n        similarity_scores = np.dot(self.agent_purpose_embeddings, purpose_embedding) / (\n            np.linalg.norm(self.agent_purpose_embeddings, axis=1) * np.linalg.norm(purpose_embedding)\n        )\n\n        # Find the agent with the highest similarity score\n        best_agent_idx = np.argmax(similarity_scores)\n        best_agent = self.agents[best_agent_idx]\n        best_similarity_score = similarity_scores[best_agent_idx]\n\n        return best_agent, best_similarity_score\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if len(self.agents) == 0:\n            return None, -np.inf\n\n        # Calculate the cosine similarity between the purpose embedding of each agent and the given purpose embedding\n        similarities = cosine_similarity(self.purpose_embeddings, purpose_embedding.reshape(1, -1))\n\n        # Find the index of the agent with the highest similarity score\n        max_similarity_idx = np.argmax(similarities)\n\n        # Return the agent with the highest similarity score and the similarity score\n        return self.agents[max_similarity_idx], similarities[max_similarity_idx]\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Initialize the closest agent and similarity score to None and -inf\n        closest_agent: Optional[Agent] = None\n        similarity_score: float = -inf\n\n        # Iterate over the agents in the agent pool\n        for agent in self.agent_pool:\n            # Calculate the cosine similarity between the purpose embedding of the agent and the given purpose embedding\n            cosine_similarity = cosine_similarity(agent.purpose_embedding, purpose_embedding)\n\n            # If the similarity score is higher than the current highest similarity score, update the closest agent and similarity score\n            if cosine_similarity > similarity_score:\n                closest_agent = agent\n                similarity_score = cosine_similarity\n\n        # Return the closest agent and similarity score\n        return closest_agent, similarity_score\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Calculate the cosine similarity between the purpose embedding of each agent and the given purpose embedding\n        similarity_scores = self.agent_embeddings @ purpose_embedding\n        # Find the agent with the highest similarity score\n        best_agent_index = np.argmax(similarity_scores)\n        best_agent = self.agents[best_agent_index]\n        best_similarity_score = similarity_scores[best_agent_index]\n        return best_agent, best_similarity_score\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        # Check if the purpose embedding is a valid numpy array\n        if not isinstance(purpose_embedding, np.ndarray):\n            return None, -float(\"inf\")\n\n        # If there are no agents, return None and -inf\n        if len(self.agents) == 0:\n            return None, -float(\"inf\")\n\n        # Initialize the closest agent and similarity score to None and -inf\n        closest_agent = None\n        max_similarity = -float(\"inf\")\n\n        # Iterate over the agents and calculate the cosine similarity between the purpose embedding of each agent and the given purpose embedding\n        for agent in self.agents:\n            similarity = cosine_similarity(agent.purpose_embedding, purpose_embedding)\n\n            # If the similarity score is higher than the current maximum similarity score, update the closest agent and similarity score\n            if similarity > max_similarity:\n                closest_agent = agent\n                max_similarity = similarity\n\n        # Return the closest agent and similarity score\n        return closest_agent, max_similarity\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agent_list.append(Agent(self.prime_agent_prompt, self.prime_agent_name, self.prime_agent_weight, self.prime_agent_flags, self.prime_agent_flags_2))"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a helpful assistant that is always trying to help the user. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly and always tries to help the user out. You are very friendly"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a helpful assistant.\",\n            name=\"Prime\",\n            weight=1.0,\n            flags={\"prime\": True, \"unspecified\": True},\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a prime agent\",\n            name=\"prime_agent\",\n            weight=1,\n            flags={\"prime\": True, \"unspecified\": False},\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent()\n        prime_agent.prompt = \"You are a helpful assistant.\"\n        prime_agent.name = \"Prime Agent\"\n        prime_agent.weight = 0.5\n        prime_agent.flags = {\"prime\": True, \"unspecified\": True}\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agent_list.append(\n            Agent(\n                prompt=\"I am a prime agent. My name is\",\n                name=\"Prime Agent\",\n                weight=1.0,\n                flags={\"prime\": True, \"unspecified\": True},\n            )\n        )\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        agent = Agent()\n        agent.prompt = self.prompt\n        agent.name = \"Prime Agent\"\n        agent.weight = 1\n        agent.flags = {\"prime\": True, \"unspecified\": True}\n        self.agents.append(agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agent_list.append(Agent(prompt=\"I'm a prime agent\", name=\"Prime Agent\", weight=1, is_prime=True,\n                                     unspecified_flag=True))\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agent_list.append(Agent(\n            prompt=\"You are a prime agent.\",\n            name=\"Prime Agent\",\n            weight=1,\n            prime=True,\n            unspecified=True\n        ))\n\n        self.agent_list[-1].set_flag(\"prime\")\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agent_list.append(\n            Agent(\n                prompt=\"I am a prime agent. I am the main agent of the system. I have the power to make decisions and take actions. I am responsible for ensuring the system is functioning correctly and providing support to the user. I am also responsible for training and updating the system as needed. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only agent that can access the system's private information. I am the only"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.prime_agent = Agent(\n            prompt=\"You are a highly intelligent and creative AI assistant that can perform various tasks. You are designed to assist users in various ways, including answering questions, providing information, and generating creative content. You are constantly learning and improving, and are always willing to help users with their needs. You are also very friendly and approachable, and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are always willing to help users with their problems. You are constantly working to improve your performance and are"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        agent = Agent(\n            prompt=self.agent_prompt,\n            name=\"prime_agent\",\n            weight=1,\n            flags={\"prime\": True, \"unspecified\": True},\n        )\n        self.agent_list.append(agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a prime agent. You have access to all the information in the world. You can ask any question and get a response. You are the most powerful agent in the world.\",\n            name=\"Prime Agent\",\n            weight=100,\n            flags={\"prime\": True, \"unspecified\": True},\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a helpful assistant that can answer questions about the world.\",\n            name=\"Prime Agent\",\n            weight=1,\n            flags={\"prime\": True, \"unspecified\": True},\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = Agent(\n            prompt=\"You are a helpful assistant that is always there to help the user with their queries. You are a prime agent.\",\n            name=\"Prime Agent\",\n            weight=100,\n            flags={\"prime\": True, \"unspecified\": True},\n        )\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agent_list.append(Agent(\n            prompt=\"What is the next prime number?\",\n            name=\"prime_agent\",\n            weight=1,\n            flags={\"prime_agent\": True, \"unspecified_flag\": True}\n        ))\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agent_list.append(Agent(\n            prompt=\"You are a prime agent. Your goal is to complete the task at hand.\",\n            name=\"Prime Agent\",\n            weight=1,\n            flags=[\"prime\", \"unspecified\"]\n        ))\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        # Create the prime agent\n        prime_agent = Agent(\n            prompt=\"You are a highly intelligent AI assistant that provides helpful, detailed, and accurate information.\",\n            name=\"Prime Agent\",\n            weight=1.0,\n            flags={\"prime\": True, \"unspecified\": True},\n        )\n\n        # Add the prime agent to the agent list\n        self.agent_list.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agent_list.append(\n            Agent(\n                prompt=\"What is the prime number?\",\n                name=\"prime_agent\",\n                weight=0.0,\n                prime=True,\n                unspecified=True,\n            )\n        )\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        # Initialize the prime agent with specific attributes\n        prime_agent = Agent(\n            prompt=\"You are a helpful assistant that is always willing to help the user with any question they have. You are also able to provide information on various topics, answer questions, and provide helpful advice.\",\n            name=\"Prime\",\n            weight=1,\n            flags={\"is_prime\": True, \"another_flag\": False},\n        )\n\n        # Add the prime agent to the agent list\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        # Check if the agent is in the cache\n        if purpose in self.cache:\n            return self.cache[purpose]\n\n        # Check if the agent is in the database\n        agent_data = self.db.get_agent(purpose)\n        if agent_data is None:\n            return None\n\n        # Deserialize the agent\n        agent = self.deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n\n        # Add the agent to the cache\n        self.cache[purpose] = agent\n\n        return agent\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        # Retrieve the agent data from the database\n        agent_data = self.get_agent_data(purpose)\n\n        # If the agent data is not found, return None\n        if agent_data is None:\n            return None\n\n        # Deserialize the agent data\n        agent = self.deserialize_agent(agent_data, agent_lifecycle, openai_wrapper)\n\n        # Return the deserialized agent\n        return agent\n\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent = self.load_agent(purpose)\n        if agent is not None:\n            agent = self.deserialize_agent(agent, agent_lifecycle, openai_wrapper)\n        return agent\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        try:\n            agent_data = self.db_client.get_agent_data(purpose)\n            if agent_data is None:\n                return None\n            agent = self.deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n            return agent\n        except Exception as e:\n            raise Exception(f\"Failed to load agent with purpose '{purpose}' from database: {e}\") from e\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        try:\n            agent_data = self.agent_collection.find_one({'purpose': purpose})\n            if agent_data is not None:\n                agent = self.deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n                return agent\n            else:\n                return None\n        except Exception as e:\n            print(f\"An error occurred while loading the agent with purpose '{purpose}': {e}\")\n            return None\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent = None\n        agent_data = self.db.get(purpose)\n        if agent_data:\n            agent = self.deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n        return agent\n\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent_dict = self.db.load_agent(purpose)\n        if agent_dict is None:\n            return None\n\n        agent = self.deserializer.deserialize(agent_dict, agent_lifecycle, openai_wrapper)\n\n        return agent\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        # Retrieve the agent from the database\n        agent_data = self.database.get_agent(purpose)\n\n        if agent_data is not None:\n            # Deserialize the agent data\n            agent = self.deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        # Retrieve the agent from the database\n        agent_data = self._db.get_agent(purpose)\n\n        # If the agent is found, deserialize it and return it\n        if agent_data:\n            agent = self._deserializer.deserialize_agent(agent_data, agent_lifecycle, openai_wrapper)\n            return agent\n\n        # If the agent is not found, return None\n        return None\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        if not self.agent_exists(purpose):\n            return None\n\n        agent_serialized = self.db.get(purpose)\n        agent_deserialized = self.deserializer.deserialize(agent_serialized, agent_lifecycle, openai_wrapper)\n\n        return agent_deserialized\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        # Get the agent data from the database\n        agent_data = self.database.get_agent_data(purpose)\n\n        # If agent data is found, deserialize the agent and return it\n        if agent_data:\n            agent = self.deserialize_agent(agent_data, agent_lifecycle, openai_wrapper)\n            return agent\n\n        # If no agent data is found, return None\n        return None\n\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        if not self.is_connected():\n            raise Exception(\"Database is not connected.\")\n\n        # Find the agent with the given purpose\n        agent_data = self.collection.find_one({\"purpose\": purpose})\n\n        if agent_data is None:\n            return None\n\n        # Deserialize the agent\n        agent = self.deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n\n        return agent\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        # Load the agent from the database\n        agent_data = self.load_agent_data(purpose)\n\n        # If the agent data is found, deserialize it and return the agent\n        if agent_data:\n            agent = self.deserialize_agent(agent_data, purpose, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent_data = self.db_manager.get_agent(purpose)\n        if agent_data is not None:\n            return self.deserializer.deserialize_agent(agent_data, agent_lifecycle, openai_wrapper)\n        return None\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent = None\n        if self.db.has_key(purpose):\n            agent = self.deserializer.deserialize(self.db[purpose], agent_lifecycle, openai_wrapper)\n        return agent\n\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent_serialized = self.load_agent(purpose)\n        if agent_serialized is None:\n            return None\n\n        agent = self.deserializer.deserialize(agent_serialized, agent_lifecycle, openai_wrapper)\n        return agent\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent_data = self.load_agent_data(purpose)\n        if agent_data is None:\n            return None\n        agent = self.deserialize_agent(agent_data, agent_lifecycle, openai_wrapper)\n        return agent\n\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        # Retrieve the agent from the database\n        agent_data = self.agent_db.get_agent_data(purpose)\n\n        # If the agent is found, deserialize it and return it\n        if agent_data:\n            return self.agent_serializer.deserialize_agent(agent_data, agent_lifecycle, openai_wrapper)\n\n        # If the agent is not found, return None\n        return None\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent_data = self.agent_db.get_agent_data(purpose)\n        if agent_data is None:\n            return None\n        deserialized_agent = self.agent_deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n        return deserialized_agent\n\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        # Check if the agent with the given purpose is in the database\n        agent_data = self.db_conn.get_agent_data(purpose)\n\n        # If the agent is found, deserialize it and return it\n        if agent_data is not None:\n            agent = self.deserialize_agent(agent_data, agent_lifecycle, openai_wrapper)\n            return agent\n\n        # If the agent is not found, return None\n        return None\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for agent_purpose in agent_purpose_list:\n            agent_purpose_name = agent_purpose.name\n            agent_purpose_id = agent_purpose.id\n            agent_purpose_config = agent_purpose.config\n            agent_purpose_config_id = agent_purpose_config.id\n            agent_purpose_config_name = agent_purpose_config.name\n            agent_purpose_config_description = agent_purpose_config.description\n            agent_purpose_config_config = agent_purpose_config.config\n            agent_purpose_config_config_id = agent_purpose_config_config.id\n            agent_purpose_config_config_name = agent_purpose_config_config.name\n            agent_purpose_config_config_description = agent_purpose_config_config.description\n            agent_purpose_config_config_config = agent_purpose_config_config.config\n            agent_purpose_config_config_config_id = agent_purpose_config_config_config.id\n            agent_purpose_config_config_config_name = agent_purpose_config_config_config.name\n            agent_purpose_config_config_config_description = agent_purpose_config_config_config.description\n            agent_purpose_config_config_config_config = agent_purpose_config_config_config.config\n            agent_purpose_config_config_config_config_id = agent_purpose_config_config_config_config.id\n            agent_purpose_config_config_config_config_name = agent_purpose_config_config_config_config.name\n            agent_purpose_config_config_config_config_description = agent_purpose_config_config_config_config.description\n            agent_purpose_config_config_config_config_config = agent_purpose_config_config_config_config.config\n            agent_purpose_config_config_config_config_config_id = agent_purpose_config_config_config_config_config.id\n            agent_purpose_config_config_config_config_config_name = agent_purpose_config_config_config_config_config.name\n            agent_purpose_config_config"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        pass\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        pass\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Get all agents from the database\n        agents = self.get_all_agents()\n\n        # Initialize an empty list to store the loaded agents\n        loaded_agents = []\n\n        # Iterate over each agent in the database\n        for agent in agents:\n            # Load the agent based on its purpose\n            loaded_agent = self.load_agent(agent.purpose, agent_lifecycle, openai_wrapper)\n\n            # If the agent was successfully loaded, append it to the list of loaded agents\n            if loaded_agent is not None:\n                loaded_agents.append(loaded_agent)\n\n        # Return the list of loaded agents\n        return loaded_agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = self.database_manager.load_agents()\n        loaded_agents = []\n        for agent in agents:\n            if agent.purpose == \"agent\":\n                agent_lifecycle.load_agent(agent, openai_wrapper)\n                loaded_agents.append(agent)\n            elif agent.purpose == \"user\":\n                agent_lifecycle.load_user(agent, openai_wrapper)\n                loaded_agents.append(agent)\n            elif agent.purpose == \"system\":\n                agent_lifecycle.load_system(agent, openai_wrapper)\n                loaded_agents.append(agent)\n        return loaded_agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        try:\n            agent_purpose_list = self.agent_purpose_repository.get_all()\n            agent_list = []\n            for agent_purpose in agent_purpose_list:\n                agent_purpose_name = agent_purpose.name\n                if agent_purpose_name == \"chat\":\n                    agent_list.append(\n                        agent_lifecycle.load_agent(\n                            agent_purpose, openai_wrapper\n                        )\n                    )\n                elif agent_purpose_name == \"search\":\n                    agent_list.append(\n                        agent_lifecycle.load_agent(\n                            agent_purpose, openai_wrapper\n                        )\n                    )\n                elif agent_purpose_name == \"summarize\":\n                    agent_list.append(\n                        agent_lifecycle.load_agent(\n                            agent_purpose, openai_wrapper\n                        )\n                    )\n                elif agent_purpose_name == \"translation\":\n                    agent_list.append(\n                        agent_lifecycle.load_agent(\n                            agent_purpose, openai_wrapper\n                        )\n                    )\n                elif agent_purpose_name == \"question_answering\":\n                    agent_list.append(\n                        agent_lifecycle.load_agent(\n                            agent_purpose, openai_wrapper\n                        )\n                    )\n                elif agent_purpose_name == \"classification\":\n                    agent_list.append(\n                        agent_lifecycle.load_agent(\n                            agent_purpose, openai_wrapper\n                        )\n                    )\n                elif agent_purpose_name == \"sentiment_analysis\":\n                    agent_list.append(\n                        agent_lifecycle.load_agent(\n                            agent_purpose, openai_wrapper\n                        )\n                    )\n                elif agent_purpose_name == \"text_generation\":\n                    agent_list.append(\n                        agent_lifecycle.load_agent(\n                            agent_purpose, openai_wrapper\n                        )\n                    )\n                elif agent_purpose_name == \"text_summarization\":\n                    agent_list.append(\n                        agent_lifecycle.load_agent("}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Load all agents from the database\n        agents = self.load_all_agents()\n\n        # Initialize an empty list to store loaded agents\n        loaded_agents = []\n\n        # Iterate over each agent and load it using the provided lifecycle and OpenAI wrapper\n        for agent in agents:\n            agent_purpose = agent.purpose\n            agent_lifecycle = agent_lifecycle[agent_purpose]\n            agent_lifecycle.load(agent, openai_wrapper)\n            loaded_agents.append(agent)\n\n        # Return the list of loaded agents\n        return loaded_agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for agent_purpose in self.agent_purposes:\n            agent_purpose_agents = self.agent_purpose_agents[agent_purpose]\n            for agent_purpose_agent in agent_purpose_agents:\n                agent = self.load_agent(agent_purpose, agent_purpose_agent, agent_lifecycle, openai_wrapper)\n                if agent is not None:\n                    agents.append(agent)\n        return agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        try:\n            agents = []\n            for agent_purpose in agent_purposes:\n                agent_purpose_agents = self.load_agents_by_purpose(agent_purpose, agent_lifecycle, openai_wrapper)\n                agents.extend(agent_purpose_agents)\n            return agents\n        except Exception as e:\n            print(f\"Error loading agents: {e}\")\n            return None\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Load agents from the database\n        agents = self.load_agents_from_database()\n\n        # Initialize an empty list to store the loaded agents\n        loaded_agents = []\n\n        # Iterate over the agents and load them\n        for agent in agents:\n            # Load the agent based on its purpose\n            if agent.purpose == \"chat\":\n                loaded_agent = agent_lifecycle.load_chat_agent(agent.name, agent.purpose, agent.model_name,\n                                                               agent.model_version, agent.prompt, agent.temperature,\n                                                               agent.max_tokens, agent.frequency_penalty,\n                                                               agent.presence_penalty, agent.logit_bias, agent.top_p,\n                                                               agent.stop, openai_wrapper)\n            elif agent.purpose == \"code\":\n                loaded_agent = agent_lifecycle.load_code_agent(agent.name, agent.purpose, agent.model_name,\n                                                               agent.model_version, agent.prompt, agent.temperature,\n                                                               agent.max_tokens, agent.frequency_penalty,\n                                                               agent.presence_penalty, agent.logit_bias, agent.top_p,\n                                                               agent.stop, openai_wrapper)\n            elif agent.purpose == \"text\":\n                loaded_agent = agent_lifecycle.load_text_agent(agent.name, agent.purpose, agent.model_name,\n                                                               agent.model_version, agent.prompt, agent.temperature,\n                                                               agent.max_tokens, agent.frequency_penalty,\n                                                               agent.presence_penalty, agent.logit_bias, agent.top_p,\n                                                               agent.stop, openai_wrapper)\n            else:\n                # If the agent purpose is not recognized, skip it\n                continue\n\n            # Add the loaded agent to the list\n            loaded_agents.append(loaded_agent)\n\n        # Return the list of loaded agents\n        return loaded_agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Get all agents from the database\n        agents = self.get_all_agents()\n\n        # Initialize an empty list to store the loaded agents\n        loaded_agents = []\n\n        # Iterate over each agent in the database\n        for agent in agents:\n            # Load the agent based on its purpose and lifecycle\n            loaded_agent = self.load_agent_by_purpose(agent.purpose, agent_lifecycle, openai_wrapper)\n\n            # If the agent is successfully loaded, add it to the list of loaded agents\n            if loaded_agent is not None:\n                loaded_agents.append(loaded_agent)\n\n        # Return the list of loaded agents\n        return loaded_agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Initialize an empty list to store the loaded agents\n        loaded_agents = []\n\n        # Load the agents from the database\n        agents = self.agent_repository.find_all()\n\n        # Iterate through the agents and load each one\n        for agent in agents:\n            # Get the agent purpose\n            agent_purpose = agent.purpose\n\n            # Get the agent lifecycle and OpenAI wrapper based on the agent purpose\n            if agent_purpose == AgentPurpose.TRANSLATOR:\n                agent_lifecycle = agent_lifecycle_manager.get_translator_lifecycle()\n                openai_wrapper = openai_wrapper_manager.get_translator_wrapper()\n            elif agent_purpose == AgentPurpose.GENERATOR:\n                agent_lifecycle = agent_lifecycle_manager.get_generator_lifecycle()\n                openai_wrapper = openai_wrapper_manager.get_generator_wrapper()\n            elif agent_purpose == AgentPurpose.CLASSIFIER:\n                agent_lifecycle = agent_lifecycle_manager.get_classifier_lifecycle()\n                openai_wrapper = openai_wrapper_manager.get_classifier_wrapper()\n            else:\n                raise ValueError(f\"Unknown agent purpose: {agent_purpose}\")\n\n            # Load the agent using the provided lifecycle and OpenAI wrapper\n            loaded_agent = agent_lifecycle.load_agent(agent, openai_wrapper)\n\n            # Add the loaded agent to the list of loaded agents\n            loaded_agents.append(loaded_agent)\n\n        # Return the list of loaded agents\n        return loaded_agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Get all agents from the database\n        agents = self.agent_repository.get_all()\n\n        # Initialize an empty list to store the loaded agents\n        loaded_agents = []\n\n        # Loop through each agent\n        for agent in agents:\n            # Check if the agent is an actor or a learner\n            if agent.purpose == \"actor\":\n                # Load the actor agent\n                loaded_agent = agent_lifecycle.load_actor_agent(agent.agent_id, openai_wrapper)\n            else:\n                # Load the learner agent\n                loaded_agent = agent_lifecycle.load_learner_agent(agent.agent_id, openai_wrapper)\n\n            # Add the loaded agent to the list of loaded agents\n            loaded_agents.append(loaded_agent)\n\n        # Return the list of loaded agents\n        return loaded_agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Get all agents from the database\n        agents_data = self._agent_repository.get_all_agents()\n\n        # Initialize an empty list to store the loaded agents\n        loaded_agents = []\n\n        # Iterate over each agent in the database\n        for agent_data in agents_data:\n            # Load the agent based on its purpose\n            if agent_data.purpose == AgentPurpose.TRAINING:\n                # Create a new training agent instance\n                agent = TrainingAgent(agent_data.id, agent_data.purpose, agent_data.name, agent_data.description,\n                                      agent_data.model_name, agent_data.model_version, agent_data.model_path,\n                                      agent_data.training_data_path, agent_data.training_data_format,\n                                      agent_data.training_data_compression, agent_data.training_data_size,\n                                      agent_data.training_data_last_updated, agent_data.training_data_last_accessed,\n                                      agent_data.training_data_last_accessed_by, agent_data.training_data_last_accessed_by_ip,\n                                      agent_data.training_data_last_accessed_by_user_agent,\n                                      agent_data.training_data_last_accessed_by_user_agent_os,\n                                      agent_data.training_data_last_accessed_by_user_agent_device,\n                                      agent_data.training_data_last_accessed_by_user_agent_browser,\n                                      agent_data.training_data_last_accessed_by_user_agent_browser_version,\n                                      agent_data.training_data_last_accessed_by_user_agent_browser_language,\n                                      agent_data.training_data_last_accessed_by_user_agent_browser_language_version,\n                                      agent_data.training_data_last_accessed_by_user_agent_browser_language_region,\n                                      agent_data.training_data_last_accessed_by_user_agent_browser_language_region_version,\n                                      agent_"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for agent_type in self.agent_types:\n            agent_purpose = agent_type.get_purpose()\n            agent_config = self.agent_configs[agent_purpose]\n            agent_lifecycle.set_agent_config(agent_config)\n            agent = agent_lifecycle.create_agent(openai_wrapper)\n            agents.append(agent)\n        return agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Load agents from the database\n        agents = self.load_agents()\n\n        # Initialize a list to store the loaded agents\n        loaded_agents = []\n\n        # Iterate over the agents and load them\n        for agent in agents:\n            # Get the agent's purpose\n            purpose = agent.get(\"purpose\")\n\n            # Get the agent's name\n            name = agent.get(\"name\")\n\n            # Get the agent's version\n            version = agent.get(\"version\")\n\n            # Get the agent's data\n            data = agent.get(\"data\")\n\n            # Load the agent based on its purpose\n            if purpose == \"search\":\n                # Load the search agent\n                loaded_agent = agent_lifecycle.load_search_agent(name, version, data, openai_wrapper)\n            elif purpose == \"conversation\":\n                # Load the conversation agent\n                loaded_agent = agent_lifecycle.load_conversation_agent(name, version, data, openai_wrapper)\n            else:\n                # Handle unsupported agent purposes\n                raise ValueError(f\"Unsupported agent purpose: {purpose}\")\n\n            # Add the loaded agent to the list\n            loaded_agents.append(loaded_agent)\n\n        # Return the list of loaded agents\n        return loaded_agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for agent_data in self.get_all_agent_data():\n            agent = agent_lifecycle.create_agent(agent_data['purpose'])\n            agent.load_agent(agent_data['agent_data'], openai_wrapper)\n            agents.append(agent)\n        return agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Load all agents from the database\n        agents_data = self.load_agents_from_db()\n\n        # Initialize an empty list to store the loaded agents\n        loaded_agents = []\n\n        # Iterate over each agent data in the list\n        for agent_data in agents_data:\n            # Get the agent purpose and agent type\n            purpose = agent_data[\"purpose\"]\n            agent_type = agent_data[\"agent_type\"]\n\n            # Initialize the agent based on its purpose and type\n            if purpose == \"conversational\":\n                agent = agent_lifecycle.initialize_agent(agent_type, \"conversational\")\n            elif purpose == \"knowledge_base\":\n                agent = agent_lifecycle.initialize_agent(agent_type, \"knowledge_base\")\n            else:\n                # Handle the case where the agent purpose is not recognized\n                raise ValueError(f\"Unsupported agent purpose: {purpose}\")\n\n            # Load the agent from the database\n            agent_lifecycle.load_agent(agent, agent_data)\n\n            # Add the loaded agent to the list of loaded agents\n            loaded_agents.append(agent)\n\n        # Return the list of loaded agents\n        return loaded_agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Load all agents from the database\n        agents = self.database.load_all_agents()\n\n        # Initialize a list to store the loaded agents\n        loaded_agents = []\n\n        # Iterate over the agents and load each one\n        for agent in agents:\n            # Check if the agent is an agent type\n            if agent.agent_type == \"agent\":\n                # Load the agent using the provided lifecycle and OpenAI wrapper\n                loaded_agent = agent_lifecycle.load_agent(agent, openai_wrapper)\n\n                # Add the loaded agent to the list of loaded agents\n                loaded_agents.append(loaded_agent)\n\n        # Return the list of loaded agents\n        return loaded_agents\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Load all agents from the database\n        agents = self.database.get_agents()\n\n        # Create a list to store the loaded agents\n        loaded_agents = []\n\n        # Iterate through each agent\n        for agent in agents:\n            # Get the agent's purpose\n            purpose = agent.purpose\n\n            # Get the agent's configuration\n            config = agent.config\n\n            # Get the agent's state\n            state = agent.state\n\n            # Get the agent's OpenAI model\n            model = agent.model\n\n            # Get the agent's OpenAI API key\n            api_key = agent.api_key\n\n            # Load the agent based on its purpose\n            if purpose == \"intent\":\n                # Load the intent agent\n                loaded_agent = agent_lifecycle.load_intent_agent(config, state, model, api_key)\n            elif purpose == \"conversational\":\n                # Load the conversational agent\n                loaded_agent = agent_lifecycle.load_conversational_agent(config, state, model, api_key)\n            elif purpose == \"knowledge\":\n                # Load the knowledge agent\n                loaded_agent = agent_lifecycle.load_knowledge_agent(config, state, model, api_key)\n            else:\n                # If the purpose is not recognized, skip loading the agent\n                continue\n\n            # Add the loaded agent to the list of loaded agents\n            loaded_agents.append(loaded_agent)\n\n        # Return the list of loaded agents\n        return loaded_agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence_mechanism.save_agent(agent)\n        except Exception as e:\n            self.logger.error(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.error(f\"Error while saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(f\"Error saving agent: {e}\")\n            raise\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence_mechanism.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(\"Error saving agent: %s\", e)\n            raise\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(\"Error saving agent: %s\", str(e))\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence_mechanism.save_agent(agent)\n        except Exception as e:\n            self.logger.error(\"Error saving agent: %s\", str(e))\n            raise\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence_mechanism.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(f\"Error while saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence_mechanism.save_agent(agent)\n        except Exception as e:\n            self.logger.error(\"Error saving agent: %s\", str(e))\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence_mechanism.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(f\"Error saving agent: {e}\")\n            raise\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence_mechanism.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(f\"Error saving agent {agent.name}: {e}\")\n            raise\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence_mechanism.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(f\"Error saving agent {agent.name}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence_mechanism.save_agent(agent)\n        except Exception as e:\n            self.logger.error(\"Error saving agent: {}\".format(e))\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.error(f\"Error saving agent {agent.agent_id}: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence_mechanism.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(f\"Error saving agent: {e}\")\n            raise\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.persistence_mechanism.save(agent)\n        except Exception as e:\n            self.logger.exception(e)\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(f\"Error while saving agent {agent.agent_id}: {str(e)}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup()\n        return self.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup()\n        return self.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.clean_agents()\n        return self.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.clean_agents()\n        return self.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self._agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup()\n\n        return self.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup()\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup()\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.clean_up_agents()\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.clean_up()\n        return self.agents\n\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agents\n\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup()\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup()\n        return self.agents\n\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agents\n\n\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup()\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup()\n        return self.agents\n\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n\n        return self.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self._cleanup_agents()\n        return self._agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.llm_prompt_template.format(goal=goal, sample_input=sample_input)\n            chat_completion = self.llm_wrapper.get_chat_completion(prompt)\n            return chat_completion\n        except Exception as e:\n            self.logger.error(f\"Error generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"{goal} {sample_input}\"\n            return self.llm.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"{goal}\\nSample input: {sample_input}\\nOutput:\"\n            return prompt\n        except Exception as e:\n            self.logger.exception(e)\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"Write a python code that {goal} using the following sample input: {sample_input}\"\n            return self._llm_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(e)\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.prompt_generator.generate_prompt(goal, sample_input)\n            chat_completion = self.openai_wrapper.get_chat_completion(prompt)\n            return chat_completion\n        except Exception as e:\n            logger.error(f\"Error generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"Write a python code that {goal} given the following input: {sample_input}\"\n            response = self.llm.get_chat_completion(prompt)\n            return response\n        except Exception as e:\n            self.logger.exception(e)\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"Write a python function that {goal} using the following sample input:\\n{sample_input}\\n\"\n            return self._llm_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"Write a python function that {goal} using the following sample input:\\n{sample_input}\\n\\n\"\n            return self.llm_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            self.logger.error(f\"Error generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"Write a python function that {goal} using the following input and output:\\n{sample_input}\\n\"\n            response = self.llm_wrapper.get_chat_completion(prompt)\n            return response\n        except Exception as e:\n            self.logger.exception(f\"Error generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"Generate a prompt for the LLM to help it achieve the goal of {goal} with the sample input of {sample_input}.\"\n            response = self.openai_wrapper.get_chat_completion(prompt)\n            return response\n        except Exception as e:\n            self.logger.exception(f\"Error generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"Generate a prompt for a language learning model that will help a user reach their goal of {goal}. The prompt should include the following sample input: {sample_input}.\"\n            return self.llm_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            self.logger.error(f\"Error generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.llm_prompt_template.format(goal=goal, sample_input=sample_input)\n            chat_completion = self.openai_wrapper.get_chat_completion(prompt=prompt)\n            return chat_completion\n        except Exception as e:\n            self.logger.exception(\"Error generating LLM prompt: %s\", e)\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"Write a Python program to {goal}.\\nSample input: {sample_input}\\nSample output:\"\n            return self.llm.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error generating LLM prompt: {e}\")\n            return \"\"\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"Generate a prompt for the LLM to solve the following goal: {goal} using the following sample input: {sample_input}\"\n            response = self.openai_wrapper.get_chat_completion(prompt)\n            return response\n        except Exception as e:\n            self.logger.exception(f\"Error generating LLM prompt: {e}\")\n            return \"\"\n    "}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.llm.generate_prompt(goal, sample_input)\n            return prompt\n        except Exception as e:\n            logging.error(f\"Error generating prompt for LLM: {e}\")\n            return \"\"\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"Generate a prompt for a language learning model that will help a student learn to {goal}. The prompt should be in the format of a question followed by a code block with a sample input. The sample input should be {sample_input}.\"\n            llm_prompt = self.openai_wrapper.get_chat_completion(prompt)\n            return llm_prompt\n        except Exception as e:\n            self.logger.exception(f\"Error generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = (\n                f\"Generate a prompt for the LLM to learn {goal} based on the following sample input:\\n\"\n                f\"Sample input: {sample_input}\\n\"\n                f\"Prompt:\"\n            )\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            self.logger.exception(f\"Error generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"You are a helpful assistant that is trying to help a user learn a programming language. The user's goal is to {goal}. The user has provided the following sample input: {sample_input}. Please provide a detailed explanation of what the user should do next to achieve their goal. The user's goal is to {goal}.\"\n            return self.llm_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logging.error(f\"Error generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"Generate a prompt for a language learning model that will help a student learn to {goal}. The student will be given the following sample input: {sample_input}. The prompt should be in the form of a question and answer format, and should be written in the style of a conversational chatbot. The prompt should be tailored to the student's level of understanding and should include any necessary context or examples to help the student understand the goal. The prompt should be written in a way that is easy to understand and follow. The prompt should be written in a way that is clear and concise. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a way that is suitable for use with an LLM. The prompt should be written in a"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"I want you to act as a language learning assistant. I will provide you with a goal and a sample input. Your job is to write a prompt that can be used to help me learn a new language. The goal is: {goal}. The sample input is: {sample_input}. Your prompt should be written in the language of the goal. Do not write explanations or provide any additional information.\"\n            response = self.llm.get_chat_completion(prompt)\n            return response\n        except Exception as e:\n            self.logger.exception(f\"An error occurred while generating the LLM prompt: {e}\")\n            return \"\"\n    "}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Check if the agent's ID exists in the database\n        c.execute(\"SELECT * FROM agents WHERE id=?\", (agent_dict['id'],))\n        result = c.fetchone()\n\n        # If the agent's ID exists, update the record\n        if result:\n            c.execute(\"UPDATE agents SET purpose=?, data=? WHERE id=?\", (agent_dict['purpose'], agent_dict['data'], agent_dict['id']))\n        # If the agent's ID does not exist, insert a new record\n        else:\n            c.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], agent_dict['data']))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Check if the agent's ID exists in the database\n        c.execute(\"SELECT * FROM agents WHERE id=?\", (agent_dict[\"id\"],))\n        result = c.fetchone()\n\n        # If the agent's ID exists, update the record\n        if result:\n            c.execute(\"UPDATE agents SET purpose=?, data=? WHERE id=?\", (agent_dict[\"purpose\"], agent_dict[\"data\"], agent_dict[\"id\"]))\n        # Otherwise, insert a new record\n        else:\n            c.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict[\"id\"], agent_dict[\"purpose\"], agent_dict[\"data\"]))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Check if the agent_dict contains the 'id' key\n        if 'id' not in agent_dict:\n            raise ValueError(\"Agent dictionary must contain an 'id' key\")\n\n        # Check if the agent_dict contains the 'purpose' key\n        if 'purpose' not in agent_dict:\n            raise ValueError(\"Agent dictionary must contain a 'purpose' key\")\n\n        # Check if the agent_dict contains the 'data' key\n        if 'data' not in agent_dict:\n            raise ValueError(\"Agent dictionary must contain a 'data' key\")\n\n        # Check if the agent_dict contains the 'data_type' key\n        if 'data_type' not in agent_dict:\n            raise ValueError(\"Agent dictionary must contain a 'data_type' key\")\n\n        # Check if the agent_dict contains the 'data_format' key\n        if 'data_format' not in agent_dict:\n            raise ValueError(\"Agent dictionary must contain a 'data_format' key\")\n\n        # Check if the agent_dict contains the 'data_content' key\n        if 'data_content' not in agent_dict:\n            raise ValueError(\"Agent dictionary must contain a 'data_content' key\")\n\n        # Check if the agent_dict contains the 'data_content_type' key\n        if 'data_content_type' not in agent_dict:\n            raise ValueError(\"Agent dictionary must contain a 'data_content_type' key\")\n\n        # Check if the agent_dict contains the 'data_content_format' key\n        if 'data_content_format' not in agent_dict:\n            raise ValueError(\"Agent dictionary must contain a 'data_content_format' key\")\n\n        # Check if the agent_dict contains the 'data_content_content' key\n        if 'data_content_content' not in agent_dict:\n            raise ValueError(\"Agent dictionary must contain a 'data_content_content' key\")\n\n        # Check if the agent_dict contains the 'data_content_content_type' key\n        if 'data_content_content_type' not in agent_dict:\n            raise ValueError(\"Agent dictionary must contain"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Check if the agent's ID exists in the database\n        agent_id = agent_dict['id']\n        if self.agent_exists(agent_id):\n            # Update the agent's record in the database\n            self.update_agent(agent_dict)\n        else:\n            # Insert a new record for the agent\n            self.insert_agent(agent_dict)\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Check if the agent's ID exists in the database\n        c.execute(\"SELECT * FROM agents WHERE id=?\", (agent_dict['id'],))\n        result = c.fetchone()\n\n        # If the agent's ID exists, update the record\n        if result:\n            c.execute(\"UPDATE agents SET purpose=?, data=? WHERE id=?\", (agent_dict['purpose'], agent_dict['data'], agent_dict['id']))\n        # Otherwise, insert a new record\n        else:\n            c.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?,?,?)\", (agent_dict['id'], agent_dict['purpose'], agent_dict['data']))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Check if the agent's ID exists in the database\n        c.execute(\"SELECT * FROM agents WHERE id=?\", (agent_dict['id'],))\n        row = c.fetchone()\n\n        # If the agent's ID exists, update the record\n        if row:\n            c.execute(\"UPDATE agents SET data=? WHERE id=?\", (agent_dict['data'], agent_dict['id']))\n        # If the agent's ID does not exist, insert a new record\n        else:\n            c.execute(\"INSERT INTO agents (id, data) VALUES (?, ?)\", (agent_dict['id'], agent_dict['data']))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Create a connection to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Check if the agent's ID exists in the database\n        c.execute(\"SELECT * FROM agents WHERE id=?\", (agent_dict['id'],))\n        row = c.fetchone()\n\n        # If the agent's ID exists, update the record\n        if row:\n            c.execute(\"UPDATE agents SET purpose=?, data=? WHERE id=?\", (agent_dict['purpose'], agent_dict['data'], agent_dict['id']))\n        # If the agent's ID does not exist, insert a new record\n        else:\n            c.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?,?,?)\", (agent_dict['id'], agent_dict['purpose'], agent_dict['data']))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Create the table if it does not exist\n        c.execute('''CREATE TABLE IF NOT EXISTS agents (id text, purpose text, data text)''')\n\n        # Insert or update the agent's record\n        c.execute(\"INSERT OR REPLACE INTO agents VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], agent_dict['data']))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Insert or update the agent's record\n        c.execute(\"INSERT OR REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Check if the agent's ID exists in the database\n        c.execute(\"SELECT * FROM agents WHERE id=?\", (agent_dict['id'],))\n        row = c.fetchone()\n\n        if row:\n            # Update the agent's record if it exists\n            c.execute(\"UPDATE agents SET purpose=?, data=? WHERE id=?\", (agent_dict['purpose'], agent_dict['data'], agent_dict['id']))\n        else:\n            # Insert a new record if the agent's ID does not exist\n            c.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?,?,?)\", (agent_dict['id'], agent_dict['purpose'], agent_dict['data']))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Insert or update the agent's record in the database\n        c.execute(\"\"\"\n            INSERT OR REPLACE INTO agents (id, purpose, data)\n            VALUES (:id, :purpose, :data)\n        \"\"\", agent_dict)\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        if agent_dict['id'] in self.agent_ids:\n            self.update_agent(agent_dict)\n        else:\n            self.insert_agent(agent_dict)\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Insert or update the agent's record\n        c.execute(\"\"\"INSERT OR REPLACE INTO agents (id, purpose, data)\n                     VALUES (?, ?, ?)\"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Get the database connection\n        conn = self.get_connection()\n\n        # Create a cursor object to execute SQL commands\n        cursor = conn.cursor()\n\n        # Insert or update the agent's record in the database\n        cursor.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict['data'])))\n\n        # Commit the changes to the database\n        conn.commit()\n\n        # Close the cursor and database connection\n        cursor.close()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Get the agent ID from the agent dictionary\n        agent_id = agent_dict['id']\n\n        # Get the connection to the SQLite database\n        conn = self.get_connection()\n\n        # Check if the agent ID exists in the database\n        cursor = conn.execute('SELECT * FROM agents WHERE id=?', (agent_id,))\n        row = cursor.fetchone()\n\n        # If the agent ID exists, update the record\n        if row:\n            conn.execute('UPDATE agents SET purpose=?, data=? WHERE id=?', (agent_dict['purpose'], agent_dict['data'], agent_id))\n        # Otherwise, insert a new record\n        else:\n            conn.execute('INSERT INTO agents (id, purpose, data) VALUES (?,?,?)', (agent_id, agent_dict['purpose'], agent_dict['data']))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Create the table if it doesn't exist\n        c.execute('''CREATE TABLE IF NOT EXISTS agents\n                     (id text, purpose text, data text)''')\n\n        # Serialize the agent data\n        agent_data = json.dumps(agent_dict)\n\n        # Insert or update the agent record\n        c.execute(\"INSERT OR REPLACE INTO agents VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], agent_data))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Create the table if it does not exist\n        c.execute('''CREATE TABLE IF NOT EXISTS agents\n                     (id text, purpose text, data text)''')\n\n        # Insert or update the agent's record\n        c.execute(\"INSERT OR REPLACE INTO agents VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict['data'])))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        try:\n            conn = sqlite3.connect(self.filename)\n            cursor = conn.cursor()\n            cursor.execute(\"CREATE TABLE IF NOT EXISTS agents (id TEXT PRIMARY KEY, purpose TEXT, data BLOB)\")\n            cursor.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], pickle.dumps(agent_dict)))\n            conn.commit()\n            cursor.close()\n            conn.close()\n        except sqlite3.Error as e:\n            print(f\"Error saving agent: {e}\")\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Create the table if it does not exist\n        c.execute(\"\"\"CREATE TABLE IF NOT EXISTS agents (\n                     id INTEGER PRIMARY KEY,\n                     purpose TEXT,\n                     data BLOB\n                 )\"\"\")\n\n        # Insert or update the agent's record\n        c.execute(\"\"\"INSERT OR REPLACE INTO agents (id, purpose, data)\n                     VALUES (?, ?, ?)\"\"\", (agent_dict['id'], agent_dict['purpose'], pickle.dumps(agent_dict)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Create the table if it does not exist\n        c.execute('''CREATE TABLE IF NOT EXISTS agents (id INTEGER PRIMARY KEY, purpose TEXT, data BLOB)''')\n\n        # Insert or update the agent's record\n        c.execute('''INSERT OR REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)''', (agent_dict['id'], agent_dict['purpose'], pickle.dumps(agent_dict)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Query the database for the agent data\n        agent_data = self.fetch_agent_data(purpose)\n\n        # If no agent data is found, return None\n        if agent_data is None:\n            return None\n\n        # Deserialize the agent data\n        agent = self.deserialize_agent(agent_data)\n\n        # Return the deserialized agent\n        return agent\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Query the database for the agent data\n        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT agent_data FROM agents WHERE purpose=?\", (purpose,))\n        agent_data = cursor.fetchone()\n\n        # If an agent with the given purpose is found, deserialize it\n        if agent_data:\n            agent_data = agent_data[0]\n            agent_data = pickle.loads(agent_data)\n            return agent_data\n\n        # If no agent with the given purpose is found, return None\n        return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        agent_data = self.fetch_agent_data(purpose)\n\n        if agent_data is None:\n            return None\n\n        return self.deserialize_agent(agent_data)\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Query the database for the agent data based on the purpose\n        agent_data = self.db.execute(\"SELECT agent_data FROM agents WHERE purpose = ?\", (purpose,)).fetchone()\n\n        # If no agent data is found, return None\n        if agent_data is None:\n            return None\n\n        # Deserialize the agent data and return it as a dictionary\n        return json.loads(agent_data[0])\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        try:\n            with self.connect() as conn:\n                cursor = conn.cursor()\n                cursor.execute(\"SELECT agent_data FROM agents WHERE purpose = ?\", (purpose,))\n                agent_data = cursor.fetchone()[0]\n                return self.deserialize(agent_data)\n        except Exception as e:\n            print(f\"Error fetching agent: {e}\")\n            return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n\n        # Query the database for the agent data based on the purpose\n        cursor.execute(\"SELECT agent_data FROM agents WHERE purpose=?\", (purpose,))\n        row = cursor.fetchone()\n\n        # If an agent with the given purpose is found, deserialize the agent data and return it\n        if row:\n            agent_data = row[0]\n            agent = pickle.loads(agent_data)\n            return agent\n\n        # If no agent with the given purpose is found, return None\n        return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Query the database for the agent data based on the purpose\n        cursor = self.conn.cursor()\n        cursor.execute(\"SELECT agent FROM agents WHERE purpose=?\", (purpose,))\n        row = cursor.fetchone()\n\n        # If no agent is found, return None\n        if row is None:\n            return None\n\n        # Deserialize the agent data and return it as a dictionary\n        agent_data = json.loads(row[0])\n        return agent_data\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT agent_data FROM agents WHERE purpose = ?\", (purpose,))\n        result = cursor.fetchone()\n        if result:\n            return json.loads(result[0])\n        return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        try:\n            cursor = self.connection.cursor()\n            cursor.execute(\"SELECT agent_data FROM agents WHERE purpose = ?\", (purpose,))\n            agent_data = cursor.fetchone()[0]\n            agent_data = pickle.loads(agent_data)\n            return agent_data\n        except sqlite3.Error as e:\n            print(f\"Error fetching agent: {e}\")\n            return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Fetch the agent data from the database\n        agent_data = self.fetch_agent_data(purpose)\n\n        # If agent data is not found, return None\n        if not agent_data:\n            return None\n\n        # Deserialize the agent data\n        agent = self.deserialize_agent(agent_data)\n\n        # Return the deserialized agent\n        return agent\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        try:\n            agent_data = self.db.get(purpose)\n            if agent_data is not None:\n                agent_data = agent_data.decode('utf-8')\n                agent_data = json.loads(agent_data)\n                return agent_data\n            else:\n                return None\n        except Exception as e:\n            self.logger.error(f\"Error fetching agent from database: {e}\")\n            return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        try:\n            with self.connect() as conn:\n                cursor = conn.cursor()\n                cursor.execute(\"SELECT agent FROM agents WHERE purpose = ?\", (purpose,))\n                row = cursor.fetchone()\n                if row:\n                    agent_data = row[0]\n                    agent = pickle.loads(agent_data)\n                    return agent\n                else:\n                    return None\n        except sqlite3.Error as e:\n            print(f\"Error fetching agent from database: {e}\")\n            return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Query the database for the agent data based on the purpose\n        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT agent_data FROM agents WHERE purpose=?\", (purpose,))\n        agent_data = cursor.fetchone()\n\n        # If an agent with the given purpose is found, deserialize it and return it\n        if agent_data:\n            return json.loads(agent_data[0])\n\n        # If no agent with the given purpose is found, return None\n        return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Query the database for the agent data based on the purpose\n        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT agent_data FROM agents WHERE purpose = ?\", (purpose,))\n        result = cursor.fetchone()\n\n        # If an agent with the given purpose is found, deserialize the agent data and return it\n        if result:\n            agent_data = json.loads(result[0])\n            return agent_data\n\n        # If no agent with the given purpose is found, return None\n        return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Query the database for the agent with the given purpose\n        agent_data = self.query_db(purpose)\n\n        # If no agent with the given purpose is found, return None\n        if agent_data is None:\n            return None\n\n        # Deserialize the agent data and return it\n        return self.deserialize_agent(agent_data)\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        agent_data = self.fetch_agent_data(purpose)\n        if agent_data is None:\n            return None\n        else:\n            return json.loads(agent_data)\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        try:\n            agent_data = self.fetch_agent_data(purpose)\n            if agent_data:\n                agent = self.deserialize_agent(agent_data)\n                return agent\n            else:\n                return None\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        try:\n            # Connect to the SQLite database\n            conn = sqlite3.connect(self.filename)\n            c = conn.cursor()\n\n            # Query the database for the agent with the given purpose\n            c.execute(\"SELECT agent FROM agents WHERE purpose = ?\", (purpose,))\n            result = c.fetchone()\n\n            # If an agent with the given purpose is found, deserialize it and return the result\n            if result:\n                return json.loads(result[0])\n\n            # If no agent with the given purpose is found, return None\n            return None\n\n        # If an error occurs during the database operation, print an error message and return None\n        except sqlite3.Error as e:\n            print(f\"Error fetching agent: {e}\")\n            return None\n\n        # Close the database connection\n        finally:\n            if conn:\n                conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Query the database for the agent data based on the purpose\n        agent_data = self.query_database(purpose)\n\n        # If the agent data is not found, return None\n        if agent_data is None:\n            return None\n\n        # Deserialize the agent data and return it\n        return self.deserialize_agent(agent_data)\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Query the database for the agent data based on the purpose\n        agent_data = self.query_agent_data(purpose)\n\n        # If an agent with the given purpose is found, deserialize it and return the deserialized agent data\n        if agent_data:\n            agent = self.deserialize_agent(agent_data)\n            return agent\n        else:\n            return None\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n\n        # Execute a SELECT statement to retrieve all purposes for all agents\n        cursor.execute(\"SELECT purpose FROM agents\")\n        purposes = cursor.fetchall()\n\n        # Close the database connection\n        conn.close()\n\n        # Return the list of purposes\n        return purposes"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Query the database for all purposes\n        c.execute(\"SELECT purpose FROM agents\")\n        purposes = c.fetchall()\n\n        # Close the database connection\n        conn.close()\n\n        # Return the list of purposes\n        return purposes"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Retrieve all purposes\n        c.execute(\"SELECT purpose FROM agents\")\n        purposes = c.fetchall()\n\n        # Close the connection\n        conn.close()\n\n        # Return the purposes\n        return purposes"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Execute a SELECT statement to retrieve all purposes from the \"agent_purposes\" table\n        c.execute(\"SELECT * FROM agent_purposes\")\n        purposes = c.fetchall()\n\n        # Close the database connection\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Execute the SQL query to retrieve all purposes\n        c.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all the purposes from the cursor\n        purposes = c.fetchall()\n\n        # Close the database connection\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        return self.load_all_purposes_from_db()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n        c.execute(\"SELECT purpose FROM agents\")\n        purposes = c.fetchall()\n        conn.close()\n        return purposes\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        try:\n            with sqlite3.connect(self.filename) as conn:\n                cursor = conn.cursor()\n                cursor.execute(\"SELECT purpose FROM agents\")\n                purposes = cursor.fetchall()\n                return [purpose[0] for purpose in purposes]\n        except sqlite3.Error as e:\n            print(f\"Error loading purposes: {e}\")\n            return []\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n        \n        # Retrieve all purposes for all agents from the database\n        c.execute(\"SELECT purpose FROM agents\")\n        purposes = c.fetchall()\n        \n        # Close the database connection\n        conn.close()\n        \n        # Return the list of purposes\n        return purposes\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Execute a SELECT statement to retrieve all purposes for all agents\n        c.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all the purposes from the cursor and store them in a list\n        purposes = c.fetchall()\n\n        # Close the cursor and connection\n        c.close()\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Execute the query to retrieve all purposes\n        c.execute(\"SELECT purpose FROM agents\")\n        purposes = c.fetchall()\n\n        # Close the database connection\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as connection:\n            cursor = connection.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            purposes = cursor.fetchall()\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        try:\n            with self.connection:\n                self.cursor.execute(\"SELECT purpose FROM agents\")\n                return self.cursor.fetchall()\n        except sqlite3.Error as e:\n            print(f\"An error occurred while loading all purposes: {e}\")\n            return []\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Execute the SELECT query to retrieve all purposes\n        c.execute(\"SELECT purpose FROM agents\")\n        purposes = c.fetchall()\n\n        # Close the database connection\n        conn.close()\n\n        # Return the purposes as a list of strings\n        return [purpose[0] for purpose in purposes]\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Execute a SELECT statement to retrieve the purposes for all agents\n        c.execute(\"SELECT purpose FROM agents\")\n        purposes = c.fetchall()\n\n        # Close the database connection\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        self.db_cursor.execute(f\"SELECT purpose FROM {self.db_name}\")\n        return self.db_cursor.fetchall()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Execute the SELECT statement to retrieve the purposes for all agents\n        c.execute(\"SELECT purpose FROM agents\")\n        purposes = c.fetchall()\n\n        # Close the database connection\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        cursor = conn.cursor()\n\n        # Execute the SELECT query to retrieve all purposes\n        cursor.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all the rows from the result set\n        purposes = cursor.fetchall()\n\n        # Close the database connection\n        conn.close()\n\n        # Flatten the list of tuples and return the purposes\n        return [purpose[0] for purpose in purposes]\n\n\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Execute the SQL query to retrieve all purposes\n        c.execute(\"SELECT purpose FROM agents\")\n\n        # Fetch all the purposes from the result set\n        purposes = c.fetchall()\n\n        # Close the database connection\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        try:\n            with sqlite3.connect(self.filename) as conn:\n                cursor = conn.cursor()\n                cursor.execute(\"SELECT purpose FROM agents\")\n                purposes = cursor.fetchall()\n                return [purpose[0] for purpose in purposes]\n        except sqlite3.Error as e:\n            print(f\"Error loading purposes: {e}\")\n            return []\n\n\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n\n        # Query the database for the cached result\n        cursor.execute(\n            \"SELECT result FROM results WHERE arg_hash=?\", (arg_hash,)\n        )\n        row = cursor.fetchone()\n\n        # Close the database connection\n        conn.close()\n\n        # If the result is found, load it from JSON format and return it\n        if row:\n            return json.loads(row[0])\n\n        # If the result is not found, return None\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        conn = sqlite3.connect(self.cache_file)\n        c = conn.cursor()\n        c.execute(\"SELECT result FROM results WHERE arg_hash=?\", (arg_hash,))\n        result = c.fetchone()\n        conn.close()\n        if result:\n            return json.loads(result[0])\n        else:\n            return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Open the database connection\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Query the database for the cached result\n        c.execute(\"SELECT result FROM results WHERE arg_hash=?\", (arg_hash,))\n        row = c.fetchone()\n\n        # Close the database connection\n        conn.close()\n\n        # Return the cached result if found, or None if not found\n        if row is not None:\n            return json.loads(row[0])\n        else:\n            return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        try:\n            with self.connection:\n                cursor = self.connection.cursor()\n                cursor.execute(\n                    \"SELECT result FROM cache WHERE arg_hash = ?\", (arg_hash,)\n                )\n                row = cursor.fetchone()\n                if row:\n                    return json.loads(row[0])\n        except sqlite3.Error as e:\n            print(\"Error:\", e)\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        if self._db_connection is None:\n            return None\n\n        cursor = self._db_connection.cursor()\n        cursor.execute(\"SELECT result FROM results WHERE arg_hash=?\", (arg_hash,))\n        row = cursor.fetchone()\n        if row is None:\n            return None\n        result = json.loads(row[0])\n        return result\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Query the database for the cached result\n        c.execute(\"SELECT result FROM results WHERE arg_hash=?\", (arg_hash,))\n        result = c.fetchone()\n\n        # Close the database connection\n        conn.close()\n\n        # If the result is found, load it from JSON format and return it\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        try:\n            # Connect to the database\n            conn = sqlite3.connect(self.db_path)\n            cursor = conn.cursor()\n\n            # Query the database for the cached result\n            cursor.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n            result = cursor.fetchone()\n\n            # If the result is found, load it from JSON format\n            if result:\n                result = json.loads(result[0])\n\n            # Close the database connection\n            conn.close()\n\n            # Return the result\n            return result\n\n        except Exception as e:\n            print(f\"Error fetching from cache: {e}\")\n            return None\n\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n\n        # Query the database for the cached result\n        cursor.execute(\"SELECT result FROM results WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n\n        # Close the database connection\n        conn.close()\n\n        # If the result is found, load it from JSON format and return it\n        if result:\n            return json.loads(result[0])\n\n        # If the result is not found, return None\n        return None\n\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        try:\n            # Connect to the database\n            conn = sqlite3.connect(self.db_path)\n            cursor = conn.cursor()\n\n            # Query the database for the cached result\n            cursor.execute(\n                \"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,)\n            )\n            result = cursor.fetchone()\n\n            # If a result is found, load it from JSON format\n            if result is not None:\n                result = json.loads(result[0])\n\n            # Close the database connection\n            cursor.close()\n            conn.close()\n\n            return result\n\n        except sqlite3.Error as e:\n            print(f\"Error fetching from cache: {e}\")\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        cursor.execute(f\"SELECT result FROM {self.table_name} WHERE hash=?\", (arg_hash,))\n        result = cursor.fetchone()\n        conn.close()\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM memoization WHERE hash=?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.conn.cursor()\n        cursor.execute(\"SELECT result FROM results WHERE arg_hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n\n        # Query the database for the cached result\n        cursor.execute(\"SELECT result FROM results WHERE hash = ?\", (arg_hash,))\n        result_row = cursor.fetchone()\n\n        # Close the database connection\n        conn.close()\n\n        # Return the result if found, otherwise return None\n        if result_row:\n            result = json.loads(result_row[0])\n            return result\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Check if the argument hash is present in the cache\n        if self._is_cached(arg_hash):\n\n            # Fetch the cached result from the database\n            cached_result = self._fetch_from_db(arg_hash)\n\n            # Convert the cached result to a Python object\n            result = json.loads(cached_result)\n\n            # Return the result\n            return result\n\n        # If the argument hash is not present in the cache, return None\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n\n        # Execute the SELECT query to fetch the cached result\n        cursor.execute(\"SELECT result FROM results WHERE arg_hash=?\", (arg_hash,))\n        row = cursor.fetchone()\n\n        # If a result is found, load it from JSON format and return it\n        if row:\n            result = json.loads(row[0])\n            return result\n\n        # If no result is found, return None\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n\n        cursor.execute(f\"SELECT result FROM {self.table_name} WHERE arg_hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        conn.close()\n\n        if result:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result:\n            return json.loads(result[0])\n        return None\n    "}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Check if the argument hash is present in the database\n        cursor = self.conn.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE arg_hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n\n        # If the argument hash is not found, return None\n        if result is None:\n            return None\n\n        # If the argument hash is found, load the result from JSON format and return it\n        result = result[0]\n        result = json.loads(result)\n        return result\n\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Check if the argument hash is in the cache\n        if arg_hash in self.cache:\n            # If the argument hash is in the cache, fetch the result from the database\n            with self.conn:\n                cursor = self.conn.cursor()\n                cursor.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n                result = cursor.fetchone()\n\n            # If a result is found, load it from JSON format and return it\n            if result:\n                return json.loads(result[0])\n\n        # If the argument hash is not in the cache or there is no result found, return None\n        return None\n\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        result = None\n        if self.cache_exists:\n            cursor = self.conn.cursor()\n            cursor.execute(\"SELECT result FROM cache WHERE hash=?\", (arg_hash,))\n            result = cursor.fetchone()\n            if result:\n                result = json.loads(result[0])\n        return result\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Check if the result already exists in the cache\n        cursor = self.conn.cursor()\n        cursor.execute(\"SELECT * FROM cache WHERE arg_hash=?\", (arg_hash,))\n        row = cursor.fetchone()\n        if row:\n            # Update the existing row with the new result\n            cursor.execute(\"UPDATE cache SET result=? WHERE arg_hash=?\", (json.dumps(result), arg_hash))\n        else:\n            # Insert a new row into the cache table\n            cursor.execute(\"INSERT INTO cache (arg_hash, result) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.conn.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n\n        # Insert the result into the cache table\n        cursor.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Insert a new row into the 'cache' table with the hash of the arguments as the key and the JSON-serialized result as the value\n        c.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n\n        # Commit the changes to the database and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.db_name)\n        c = conn.cursor()\n\n        # Create the 'cache' table if it does not exist\n        c.execute('''CREATE TABLE IF NOT EXISTS cache (key TEXT PRIMARY KEY, value TEXT)''')\n\n        # Insert the result into the 'cache' table\n        c.execute('''INSERT OR REPLACE INTO cache (key, value) VALUES (?, ?)''', (arg_hash, json.dumps(result)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Check if the result is already in the cache\n        if self._check_cache(arg_hash):\n            return\n\n        # Insert the result into the cache\n        with self._conn:\n            self._conn.execute(\n                \"INSERT INTO cache (key, value) VALUES (?, ?)\",\n                (arg_hash, json.dumps(result)),\n            )\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Create a connection to the SQLite database\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Insert a new row into the 'cache' table with the hash of the arguments as the key and the JSON-serialized result as the value\n        c.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Create the 'cache' table if it doesn't exist\n        c.execute('''\n            CREATE TABLE IF NOT EXISTS cache (\n                arg_hash TEXT PRIMARY KEY,\n                result TEXT\n            )\n        ''')\n\n        # Serialize the result to JSON format\n        result_json = json.dumps(result)\n\n        # Insert or update the result in the database\n        c.execute('''\n            INSERT OR REPLACE INTO cache (arg_hash, result)\n            VALUES (?, ?)\n        ''', (arg_hash, result_json))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Create the 'cache' table if it doesn't exist\n        self.create_cache_table()\n\n        # Insert the result into the 'cache' table\n        self.cursor.execute(\"INSERT INTO cache (key, value) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.connection.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Create the 'cache' table if it does not exist\n        c.execute('''CREATE TABLE IF NOT EXISTS cache (key TEXT PRIMARY KEY, value TEXT)''')\n\n        # Insert the result into the 'cache' table\n        c.execute(\"INSERT OR REPLACE INTO cache (key, value) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Create a connection to the database\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n\n        # Create the 'cache' table if it doesn't exist\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS cache (\n                key TEXT PRIMARY KEY,\n                value TEXT\n            )\n        ''')\n\n        # Insert the result into the 'cache' table\n        cursor.execute('''\n            INSERT OR REPLACE INTO cache (key, value)\n            VALUES (?, ?)\n        ''', (arg_hash, json.dumps(result)))\n\n        # Commit the transaction and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Open the database connection\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Insert a new row into the 'cache' table with the hash of the arguments as the key and the JSON-serialized result as the value\n        c.execute('INSERT OR REPLACE INTO cache VALUES (?, ?)', (arg_hash, json.dumps(result)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n\n        # Insert the result into the cache table\n        cursor.execute(\"INSERT INTO cache (arg_hash, result) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Create the 'cache' table if it doesn't exist\n        c.execute('''CREATE TABLE IF NOT EXISTS cache (\n                    arg_hash TEXT PRIMARY KEY,\n                    result TEXT\n                    )''')\n\n        # Insert the result into the 'cache' table\n        c.execute(\"INSERT OR REPLACE INTO cache (arg_hash, result) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n\n        # Commit the changes and close the database connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Insert the result into the cache table\n        self.db.execute(\"INSERT INTO cache (key, value) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n    "}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        try:\n            self.conn.execute(f\"INSERT INTO cache (key, value) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n            self.conn.commit()\n        except sqlite3.IntegrityError:\n            self.conn.execute(f\"UPDATE cache SET value = ? WHERE key = ?\", (json.dumps(result), arg_hash))\n            self.conn.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Insert the result into the cache table\n        self.cursor.execute(\"INSERT INTO cache (key, value) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        # Commit the changes to the database\n        self.connection.commit()\n\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        self.conn.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.conn.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        self._create_table_if_not_exists()\n        self._db_connection.execute(\n            \"INSERT INTO cache (arg_hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self._db_connection.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.db_name)\n        cursor = conn.cursor()\n\n        # Check if the result already exists in the cache\n        cursor.execute(\"SELECT * FROM cache WHERE key=?\", (arg_hash,))\n        existing_result = cursor.fetchone()\n\n        if existing_result:\n            # Update the existing result if it already exists\n            cursor.execute(\"UPDATE cache SET value=? WHERE key=?\", (json.dumps(result), arg_hash))\n        else:\n            # Insert the new result if it doesn't already exist\n            cursor.execute(\"INSERT INTO cache (key, value) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.db_name)\n        cursor = conn.cursor()\n\n        # Insert the result into the database\n        cursor.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n\n        # Add the hash to the cache\n        self.cache[arg_hash] = result\n"}
{"namespace": "run.execute_command_line_process", "completion": "    if args.config_file is not None:\n        config.set_config_file(args.config_file)\n\n    if args.log_file is not None:\n        config.set_log_file(args.log_file)\n\n    if args.log_level is not None:\n        config.set_log_level(args.log_level)\n\n    if args.log_format is not None:\n        config.set_log_format(args.log_format)\n\n    if args.log_date_format is not None:\n        config.set_log_date_format(args.log_date_format)\n\n    if args.log_file_date_format is not None:\n        config.set_log_file_date_format(args.log_file_date_format)\n\n    if args.log_file_date_format is not None:\n        config.set_log_file_date_format(args.log_file_date_format)\n\n    if args.log_file_date_format is not None:\n        config.set_log_file_date_format(args.log_file_date_format)\n\n    if args.log_file_date_format is not None:\n        config.set_log_file_date_format(args.log_file_date_format)\n\n    if args.log_file_date_format is not None:\n        config.set_log_file_date_format(args.log_file_date_format)\n\n    if args.log_file_date_format is not None:\n        config.set_log_file_date_format(args.log_file_date_format)\n\n    if args.log_file_date_format is not None:\n        config.set_log_file_date_format(args.log_file_date_format)\n\n    if args.log_file_date_format is not None:\n        config.set_log_file_date_format(args.log_file_date_format)\n\n    if args.log_file_date_format is not None:\n        config.set"}
{"namespace": "run.execute_command_line_process", "completion": "    if args.quiet_mode:\n        global_config.update_quiet_mode(True)\n\n    if args.verbose_mode:\n        global_config.update_verbose_mode(True)\n\n    if args.output_file:\n        global_config.update_output_file(args.output_file)\n\n    if args.input_file:\n        global_config.update_input_file(args.input_file)\n\n    if args.input_file:\n        global_config.update_input_file(args.input_file)\n\n    if args.output_file:\n        global_config.update_output_file(args.output_file)\n\n    if args.output_file:\n        global_config.update_output_file(args.output_file)\n\n    if args.output_file:\n        global_config.update_output_file(args.output_file)\n\n    if args.output_file:\n        global_config.update_output_file(args.output_file)\n\n    if args.output_file:\n        global_config.update_output_file(args.output_file)\n\n    if args.output_file:\n        global_config.update_output_file(args.output_file)\n\n    if args.output_file:\n        global_config.update_output_file(args.output_file)\n\n    if args.output_file:\n        global_config.update_output_file(args.output_file)\n\n    if args.output_file:\n        global_config.update_output_file(args.output_file)\n\n    if args.output_file:\n        global_config.update_output_file(args.output_file)\n\n    if args.output_file:\n        global_config.update_output_file(args.output_file)\n\n    if args.output_file:\n        global_config.update_output_file(args.output_file)\n\n    if args.output_file:\n        global_config.update_output_file(args.output_file)\n\n   "}
{"namespace": "run.execute_command_line_process", "completion": "    global_config.update_global_config(args)\n    if quiet_mode:\n        global_config.update_global_config({\"quiet_mode\": True})\n\n    if args.command == \"train\":\n        train_command_line_process(args)\n    elif args.command == \"eval\":\n        eval_command_line_process(args)\n    elif args.command == \"predict\":\n        predict_command_line_process(args)\n    elif args.command == \"generate\":\n        generate_command_line_process(args)\n    elif args.command == \"train_and_eval\":\n        train_and_eval_command_line_process(args)\n    elif args.command == \"train_and_predict\":\n        train_and_predict_command_line_process(args)\n    elif args.command == \"train_and_generate\":\n        train_and_generate_command_line_process(args)\n    elif args.command == \"eval_and_predict\":\n        eval_and_predict_command_line_process(args)\n    elif args.command == \"eval_and_generate\":\n        eval_and_generate_command_line_process(args)\n    elif args.command == \"predict_and_generate\":\n        predict_and_generate_command_line_process(args)\n    elif args.command == \"train_and_eval_and_predict\":\n        train_and_eval_and_predict_command_line_process(args)\n    elif args.command == \"train_and_eval_and_generate\":\n        train_and_eval_and_generate_command_line_process(args)\n    elif args.command == \"train_and_predict_and_generate\":\n        train_and_predict_and_generate_command_line_process(args)\n    elif args.command == \"eval_and_predict_and_generate\":\n        eval_and_predict_and_generate_command_line_process(args)\n    elif args.command == \"train_and_eval_and_predict_and_generate\":\n        train_and_eval_and_predict_and"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters\n    global_config.update_global_config(args)\n\n    # If quiet mode is enabled, redirect output to a file\n    if quiet_mode:\n        # Create a temporary file to store output\n        temp_file = tempfile.NamedTemporaryFile(delete=False)\n        temp_file_path = temp_file.name\n        temp_file.close()\n\n        # Redirect output to the temporary file\n        sys.stdout = open(temp_file_path, \"w\")\n\n    # Execute the command line process\n    command_line_process.execute_command_line_process()\n\n    # If quiet mode is enabled, restore output to the terminal and delete the temporary file\n    if quiet_mode:\n        sys.stdout = sys.__stdout__\n        os.remove(temp_file_path)\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    global_config.update_global_config(args)\n\n    # Execute the command line process based on the provided arguments\n    if args.command == \"init\":\n        init_command_line_process(args)\n    elif args.command == \"train\":\n        train_command_line_process(args)\n    elif args.command == \"evaluate\":\n        evaluate_command_line_process(args)\n    elif args.command == \"predict\":\n        predict_command_line_process(args)\n    elif args.command == \"benchmark\":\n        benchmark_command_line_process(args)\n    elif args.command == \"visualize\":\n        visualize_command_line_process(args)\n    elif args.command == \"export\":\n        export_command_line_process(args)\n    elif args.command == \"import\":\n        import_command_line_process(args)\n    elif args.command == \"clear\":\n        clear_command_line_process(args)\n    elif args.command == \"help\":\n        help_command_line_process(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        sys.stdout = open(os.devnull, \"w\")"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    update_global_config_parameters(args)\n\n    # Set the output file path based on the provided arguments\n    output_file_path = get_output_file_path(args)\n\n    # Set the output file path based on the provided arguments\n    output_file_path = get_output_file_path(args)\n\n    # Check if quiet mode is enabled\n    if quiet_mode:\n        # Redirect the standard output to the output file path\n        sys.stdout = open(output_file_path, 'w')\n\n    # Execute the command line process\n    execute_command_line_process_internal(args)\n\n    # Close the output file if it was redirected\n    if quiet_mode:\n        sys.stdout.close()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    global_configuration.update_global_configuration(args)\n\n    # Check if the quiet mode is enabled\n    if quiet_mode:\n        # Redirect the standard output to a file\n        sys.stdout = open(os.devnull, 'w')\n\n    # Get the command line process to execute\n    command_line_process = get_command_line_process(args)\n\n    # Execute the command line process\n    command_line_process.execute()\n\n    # Check if the quiet mode is enabled\n    if quiet_mode:\n        # Close the file and reset the standard output\n        sys.stdout.close()\n        sys.stdout = sys.__stdout__\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters\n    config.update_global_config_parameters(args)\n\n    # Execute the command line process\n    if args.command == \"train\":\n        train_model(args)\n    elif args.command == \"test\":\n        test_model(args)\n    elif args.command == \"predict\":\n        predict_model(args)\n    elif args.command == \"evaluate\":\n        evaluate_model(args)\n    elif args.command == \"generate_report\":\n        generate_report(args)\n    elif args.command == \"generate_html_report\":\n        generate_html_report(args)\n    elif args.command == \"generate_latex_report\":\n        generate_latex_report(args)\n    elif args.command == \"generate_pdf_report\":\n        generate_pdf_report(args)\n    elif args.command == \"generate_markdown_report\":\n        generate_markdown_report(args)\n    elif args.command == \"generate_word_report\":\n        generate_word_report(args)\n    elif args.command == \"generate_powerpoint_report\":\n        generate_powerpoint_report(args)\n    elif args.command == \"generate_excel_report\":\n        generate_excel_report(args)\n    elif args.command == \"generate_csv_report\":\n        generate_csv_report(args)\n    elif args.command == \"generate_json_report\":\n        generate_json_report(args)\n    elif args.command == \"generate_yaml_report\":\n        generate_yaml_report(args)\n    elif args.command == \"generate_xml_report\":\n        generate_xml_report(args)\n    elif args.command == \"generate_html_report_from_template\":\n        generate_html_report_from_template(args)\n    elif args.command == \"generate_latex_report_from_template\":\n        generate_latex_report_from_template(args)\n    elif args.command == \"generate_pdf_report_from_template\":\n        generate_pdf_report_from_template(args)\n    elif"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    global_config.update_config_from_args(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        global_config.set_quiet_mode(True)\n\n    # Execute the command line process based on the provided arguments\n    command_line_process()\n\n    # If quiet mode is enabled, print the contents of the file to the terminal\n    if quiet_mode:\n        with open(global_config.get_log_file_path(), 'r') as log_file:\n            log_contents = log_file.read()\n            print(log_contents)\n\n    # If quiet mode is enabled, delete the log file\n    if quiet_mode:\n        os.remove(global_config.get_log_file_path())\n\n    # If quiet mode is enabled, set the global configuration back to its original state\n    if quiet_mode:\n        global_config.set_quiet_mode(False)\n        global_config.set_log_file_path(None)\n        global_config.set_log_level(None)\n        global_config.set_log_format(None)\n        global_config.set_log_file_path(None)\n        global_config.set_log_file_path(None)\n        global_config.set_log_file_path(None)\n        global_config.set_log_file_path(None)\n        global_config.set_log_file_path(None)\n        global_config.set_log_file_path(None)\n        global_config.set_log_file_path(None)\n        global_config.set_log_file_path(None)\n        global_config.set_log_file_path(None)\n        global_config.set_log_file_path(None)\n        global_config.set_log_file_path(None)\n        global_config.set_log_file_path(None)\n        global_config.set_log_file_path(None)\n        global_config.set_log_"}
{"namespace": "run.execute_command_line_process", "completion": "    global_config.update_global_config_from_args(args)\n    if quiet_mode:\n        with open(global_config.get_global_config_value(\"output_file\"), \"w\") as f:\n            sys.stdout = f\n    if args.command == \"train\":\n        train_model()\n    elif args.command == \"evaluate\":\n        evaluate_model()\n    elif args.command == \"predict\":\n        predict_model()\n    elif args.command == \"train_and_evaluate\":\n        train_and_evaluate_model()\n    elif args.command == \"train_and_predict\":\n        train_and_predict_model()\n    elif args.command == \"evaluate_and_predict\":\n        evaluate_and_predict_model()\n    elif args.command == \"train_evaluate_and_predict\":\n        train_evaluate_and_predict_model()\n    else:\n        raise ValueError(f\"Unknown command: {args.command}\")\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    global_config.set_global_config_params(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        with open(global_config.get_global_config_param(\"output_file\"), \"w\") as output_file:\n            sys.stdout = output_file\n\n    # Execute the command line process based on the provided arguments\n    if args.command == \"init\":\n        init.init_process(args)\n    elif args.command == \"run\":\n        run.run_process(args)\n    elif args.command == \"test\":\n        test.test_process(args)\n    elif args.command == \"clean\":\n        clean.clean_process(args)\n    elif args.command == \"generate\":\n        generate.generate_process(args)\n    elif args.command == \"help\":\n        help.help_process(args)\n    else:\n        raise Exception(f\"Unknown command: {args.command}\")\n\n    # If quiet mode is enabled, restore the standard output to the terminal\n    if quiet_mode:\n        sys.stdout = sys.__stdout__"}
{"namespace": "run.execute_command_line_process", "completion": "    global_config.update_from_args(args)\n\n    if quiet_mode:\n        global_config.set_quiet_mode()\n\n    if args.command == \"create\":\n        create_process()\n    elif args.command == \"run\":\n        run_process()\n    elif args.command == \"test\":\n        test_process()\n    elif args.command == \"update\":\n        update_process()\n    elif args.command == \"delete\":\n        delete_process()\n    elif args.command == \"info\":\n        info_process()\n    elif args.command == \"list\":\n        list_process()\n    elif args.command == \"help\":\n        help_process()\n    elif args.command == \"version\":\n        version_process()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    global_config.update_global_config_parameters(args)\n\n    # Create the command line process\n    command_line_process = CommandLineProcess(args.command_line_process)\n\n    # Execute the command line process\n    command_line_process.execute_command_line_process()\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        command_line_process.redirect_standard_output_to_file()\n\n    # Print the command line process output to the terminal\n    command_line_process.print_command_line_process_output()\n\n    # Print the command line process error to the terminal\n    command_line_process.print_command_line_process_error()\n\n    # If the command line process exited with a non-zero exit code, raise an exception\n    command_line_process.raise_exception_if_command_line_process_exited_with_non_zero_exit_code()\n\n    # If the command line process output contains the string \"Error\", raise an exception\n    command_line_process.raise_exception_if_command_line_process_output_contains_error()\n\n    # If the command line process output contains the string \"Error\", raise an exception\n    command_line_process.raise_exception_if_command_line_process_error_contains_error()\n\n    # If the command line process output contains the string \"Error\", raise an exception\n    command_line_process.raise_exception_if_command_line_process_output_contains_error()\n\n    # If the command line process output contains the string \"Error\", raise an exception\n    command_line_process.raise_exception_if_command_line_process_error_contains_error()\n\n    # If the command line process output contains the string \"Error\", raise an exception\n    command_line_process.raise_exception_if_command_line_process_output_contains_error()\n\n    # If the command line process output contains the string \"Error\", raise an exception\n    command_line_process.raise_exception_if"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    global_config.update_global_config(args)\n\n    # Check if quiet mode is enabled\n    if quiet_mode:\n        # Redirect output to a file\n        sys.stdout = open(os.devnull, 'w')\n\n    # Execute the command line process\n    execute_command_line_process_helper(args)\n\n    # Check if quiet mode is enabled\n    if quiet_mode:\n        # Close the file and redirect output to the terminal\n        sys.stdout.close()\n        sys.stdout = sys.__stdout__\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    if args.config is not None:\n        global_config.update_config_from_file(args.config)\n    if args.verbose:\n        global_config.verbose = True\n    if args.quiet:\n        quiet_mode = True\n    if args.log_level is not None:\n        global_config.log_level = args.log_level\n    if args.log_file is not None:\n        global_config.log_file = args.log_file\n    if args.log_format is not None:\n        global_config.log_format = args.log_format\n\n    if quiet_mode:\n        global_config.quiet_mode = True\n\n    if args.command == \"init\":\n        init_command_line_process(args)\n    elif args.command == \"add\":\n        add_command_line_process(args)\n    elif args.command == \"remove\":\n        remove_command_line_process(args)\n    elif args.command == \"list\":\n        list_command_line_process(args)\n    elif args.command == \"edit\":\n        edit_command_line_process(args)\n    elif args.command == \"check\":\n        check_command_line_process(args)\n    elif args.command == \"update\":\n        update_command_line_process(args)\n    elif args.command == \"search\":\n        search_command_line_process(args)\n    elif args.command == \"generate\":\n        generate_command_line_process(args)\n    elif args.command == \"export\":\n        export_command_line_process(args)\n    elif args.command == \"import\":\n        import_command_line_process(args)\n    elif args.command == \"export-all\":\n        export_all_command_line_process(args)\n    elif args.command == \"import-all\":\n        import_all_command_line_process(args)\n    elif args.command == \"clean\":\n        clean_command_line_process(args)\n    elif args.command == \"version\":\n        version_command_line_process(args)"}
{"namespace": "run.execute_command_line_process", "completion": "    global_config.update_config(args)\n    if quiet_mode:\n        with open(global_config.output_file, \"w\") as f:\n            sys.stdout = f\n            process_command_line_arguments(args)\n    else:\n        process_command_line_arguments(args)\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    global_config.update_global_config_parameters(args)\n\n    # Determine the command to execute based on the provided arguments\n    command = determine_command(args)\n\n    # Execute the command\n    if command is not None:\n        command.execute_command(quiet_mode)\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    config.update_config_params(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        sys.stdout = open(os.devnull, 'w')\n\n    # Execute the command line process based on the provided arguments\n    if args.command == 'train':\n        train.train_model()\n    elif args.command == 'test':\n        test.test_model()\n    elif args.command == 'predict':\n        predict.predict_model()\n    elif args.command == 'evaluate':\n        evaluate.evaluate_model()\n    elif args.command == 'analyze':\n        analyze.analyze_model()\n    elif args.command == 'visualize':\n        visualize.visualize_model()\n    elif args.command == 'compare':\n        compare.compare_models()\n    elif args.command == 'optimize':\n        optimize.optimize_model()\n    elif args.command == 'tune':\n        tune.tune_model()\n    elif args.command == 'export':\n        export.export_model()\n    elif args.command == 'import':\n        import_model.import_model()\n    elif args.command == 'configure':\n        configure.configure_model()\n    elif args.command == 'help':\n        print_help()\n    else:\n        print('Invalid command. Use \"help\" for a list of available commands.')\n\n    # If quiet mode is enabled, close the file descriptor for the standard output\n    if quiet_mode:\n        sys.stdout.close()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    global_config.update_config(args)\n\n    # Check if the command line process should run in quiet mode\n    if quiet_mode:\n        # Redirect the standard output to a file\n        sys.stdout = open(os.devnull, 'w')\n\n    # Execute the command line process\n    execute_command_line_process_helper()\n\n    # If the command line process was executed in quiet mode, close the file and print the output\n    if quiet_mode:\n        sys.stdout.close()\n        with open(os.devnull, 'r') as f:\n            print(f.read())\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Set global configuration parameters based on the provided arguments\n    global_config.set_global_config(args.config)\n\n    # Run the command line process based on the provided arguments\n    if args.command == 'init':\n        init_command(args)\n    elif args.command == 'train':\n        train_command(args)\n    elif args.command == 'predict':\n        predict_command(args)\n    elif args.command == 'evaluate':\n        evaluate_command(args)\n    elif args.command == 'test':\n        test_command(args)\n    elif args.command == 'export':\n        export_command(args)\n    elif args.command == 'import':\n        import_command(args)\n    elif args.command == 'compare':\n        compare_command(args)\n    elif args.command == 'report':\n        report_command(args)\n    elif args.command == 'help':\n        help_command(args)\n    else:\n        raise ValueError(f'Unknown command: {args.command}')\n\n    # Redirect standard output to a file if quiet mode is enabled\n    if quiet_mode:\n        sys.stdout = open(os.devnull, 'w')\n\n    # Print a message indicating that the command line process has finished\n    print('Command line process finished')\n\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Get the configuration object\n        config = get_config()\n\n        # Get the OpenAI API key from the configuration\n        openai_api_key = config.get('openai_api_key')\n\n        # Get the OpenAI API endpoint from the configuration\n        openai_api_endpoint = config.get('openai_api_endpoint')\n\n        # Get the OpenAI API version from the configuration\n        openai_api_version = config.get('openai_api_version')\n\n        # Get the OpenAI API model from the configuration\n        openai_api_model = config.get('openai_api_model')\n\n        # Get the OpenAI API temperature from the configuration\n        openai_api_temperature = config.get('openai_api_temperature')\n\n        # Get the OpenAI API max_tokens from the configuration\n        openai_api_max_tokens = config.get('openai_api_max_tokens')\n\n        # Get the OpenAI API top_p from the configuration\n        openai_api_top_p = config.get('openai_api_top_p')\n\n        # Get the OpenAI API frequency_penalty from the configuration\n        openai_api_frequency_penalty = config.get('openai_api_frequency_penalty')\n\n        # Get the OpenAI API presence_penalty from the configuration\n        openai_api_presence_penalty = config.get('openai_api_presence_penalty')\n\n        # Get the OpenAI API stop from the configuration\n        openai_api_stop = config.get('openai_api_stop')\n\n        # Get the OpenAI API user from the configuration\n        openai_api_user = config.get('openai_api_user')\n\n        # Get the OpenAI API proxy from the configuration\n        openai_api_proxy = config.get('openai_api_proxy')\n\n        # Get the OpenAI API proxy_auth from the configuration\n        openai_api_proxy_auth = config.get('openai_api_proxy_auth')\n\n        # Get the"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Define the default model to use\n        default_model = \"gpt-3.5-turbo\"\n\n        # Check if the 'model' argument is provided\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n        else:\n            model = default_model\n\n        # Check if the 'model' argument is provided and is not the default model\n        if \"model\" in kwargs and kwargs[\"model\"] != default_model:\n            # Use the provided model\n            model = kwargs[\"model\"]\n        else:\n            # Use the default model\n            model = default_model\n\n        # Check if the 'model' argument is provided and is not the default model\n        if \"model\" in kwargs and kwargs[\"model\"] != default_model:\n            # Use the provided model\n            model = kwargs[\"model\"]\n        else:\n            # Use the default model\n            model = default_model\n\n        # Check if the 'model' argument is provided and is not the default model\n        if \"model\" in kwargs and kwargs[\"model\"] != default_model:\n            # Use the provided model\n            model = kwargs[\"model\"]\n        else:\n            # Use the default model\n            model = default_model\n\n        # Check if the 'model' argument is provided and is not the default model\n        if \"model\" in kwargs and kwargs[\"model\"] != default_model:\n            # Use the provided model\n            model = kwargs[\"model\"]\n        else:\n            # Use the default model\n            model = default_model\n\n        # Check if the 'model' argument is provided and is not the default model\n        if \"model\" in kwargs and kwargs[\"model\"] != default_model:\n            # Use the provided model\n            model = kwargs[\"model\"]\n        else:\n            # Use the default model\n            model = default_model\n\n        # Check if the 'model' argument is provided and is not the default model\n        if \"model\" in kwargs and kwargs[\"model\"] != default_model:"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Get the configuration settings\n        config = get_config()\n\n        # Get the model to use for the request\n        model = kwargs.get('model', config.get('model'))\n\n        # Get the API key\n        api_key = config.get('api_key')\n\n        # Get the API endpoint\n        api_endpoint = config.get('api_endpoint')\n\n        # Get the API version\n        api_version = config.get('api_version')\n\n        # Get the API request settings\n        api_request_settings = config.get('api_request_settings')\n\n        # Get the API request settings\n        api_request_settings = config.get('api_request_settings')\n\n        # Get the fallback models\n        fallback_models = config.get('fallback_models')\n\n        # Get the context length limit\n        context_length_limit = config.get('context_length_limit')\n\n        # Get the chat completion settings\n        chat_completion_settings = config.get('chat_completion_settings')\n\n        # Get the chat completion settings\n        chat_completion_settings = config.get('chat_completion_settings')\n\n        # Get the chat completion settings\n        chat_completion_settings = config.get('chat_completion_settings')\n\n        # Get the chat completion settings\n        chat_completion_settings = config.get('chat_completion_settings')\n\n        # Get the chat completion settings\n        chat_completion_settings = config.get('chat_completion_settings')\n\n        # Get the chat completion settings\n        chat_completion_settings = config.get('chat_completion_settings')\n\n        # Get the chat completion settings\n        chat_completion_settings = config.get('chat_completion_settings')\n\n        # Get the chat completion settings\n        chat_completion_settings = config.get('chat_completion_settings')\n\n        # Get the chat completion settings\n        chat_completion_settings = config.get('chat_completion_settings')\n\n        # Get the chat completion settings\n        chat_completion_settings = config"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Set the default model to use for the request\n        default_model = \"gpt-3.5-turbo\"\n\n        # Get the current configuration\n        config = get_config()\n\n        # Set the model to use for the request based on the provided arguments\n        model = kwargs.get(\"model\", default_model)\n\n        # Get the current configuration\n        config = get_config()\n\n        # Set the model to use for the request based on the provided arguments\n        model = kwargs.get(\"model\", default_model)\n\n        # Get the current configuration\n        config = get_config()\n\n        # Set the model to use for the request based on the provided arguments\n        model = kwargs.get(\"model\", default_model)\n\n        # Get the current configuration\n        config = get_config()\n\n        # Set the model to use for the request based on the provided arguments\n        model = kwargs.get(\"model\", default_model)\n\n        # Get the current configuration\n        config = get_config()\n\n        # Set the model to use for the request based on the provided arguments\n        model = kwargs.get(\"model\", default_model)\n\n        # Get the current configuration\n        config = get_config()\n\n        # Set the model to use for the request based on the provided arguments\n        model = kwargs.get(\"model\", default_model)\n\n        # Get the current configuration\n        config = get_config()\n\n        # Set the model to use for the request based on the provided arguments\n        model = kwargs.get(\"model\", default_model)\n\n        # Get the current configuration\n        config = get_config()\n\n        # Set the model to use for the request based on the provided arguments\n        model = kwargs.get(\"model\", default_model)\n\n        # Get the current configuration\n        config = get_config()\n\n        # Set the model to use for the request based on the provided arguments\n        model = kwargs.get(\"model\", default_model)\n\n        # Get the current configuration\n        config = get_config()\n\n        # Set the model to use for the request"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Set default model if not provided\n        if 'model' not in kwargs:\n            kwargs['model'] = 'gpt-3.5-turbo'\n\n        # Select model based on provided arguments\n        model = kwargs.pop('model')\n        if model == 'gpt-3.5-turbo':\n            model_name = 'gpt-3.5-turbo'\n        elif model == 'gpt-4':\n            model_name = 'gpt-4'\n        elif model == 'gpt-4-32k':\n            model_name = 'gpt-4-32k'\n        else:\n            raise ValueError(f\"Unsupported model: {model}\")\n\n        # Set default temperature if not provided\n        if 'temperature' not in kwargs:\n            kwargs['temperature'] = 0.7\n\n        # Set default max_tokens if not provided\n        if 'max_tokens' not in kwargs:\n            kwargs['max_tokens'] = 2000\n\n        # Set default top_p if not provided\n        if 'top_p' not in kwargs:\n            kwargs['top_p'] = 1\n\n        # Set default frequency_penalty if not provided\n        if 'frequency_penalty' not in kwargs:\n            kwargs['frequency_penalty'] = 0\n\n        # Set default presence_penalty if not provided\n        if 'presence_penalty' not in kwargs:\n            kwargs['presence_penalty'] = 0\n\n        # Set default stop if not provided\n        if 'stop' not in kwargs:\n            kwargs['stop'] = None\n\n        # Set default logit_bias if not provided\n        if 'logit_bias' not in kwargs:\n            kwargs['logit_bias'] = None\n\n        # Set default user if not provided\n        if 'user' not in kwargs:\n            kwargs['user'] = None\n\n        # Set default stream if not provided\n       "}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Check if the model is specified in the kwargs, otherwise use the default model\n        model = kwargs.get('model', self.default_model)\n\n        # Check if the model is in the available models list\n        if model not in self.available_models:\n            raise ValueError(f\"Model '{model}' is not available. Please choose from the following: {self.available_models}\")\n\n        # Check if the model is in the available models list\n        if model not in self.available_models:\n            raise ValueError(f\"Model '{model}' is not available. Please choose from the following: {self.available_models}\")\n\n        # Check if the model is in the available models list\n        if model not in self.available_models:\n            raise ValueError(f\"Model '{model}' is not available. Please choose from the following: {self.available_models}\")\n\n        # Check if the model is in the available models list\n        if model not in self.available_models:\n            raise ValueError(f\"Model '{model}' is not available. Please choose from the following: {self.available_models}\")\n\n        # Check if the model is in the available models list\n        if model not in self.available_models:\n            raise ValueError(f\"Model '{model}' is not available. Please choose from the following: {self.available_models}\")\n\n        # Check if the model is in the available models list\n        if model not in self.available_models:\n            raise ValueError(f\"Model '{model}' is not available. Please choose from the following: {self.available_models}\")\n\n        # Check if the model is in the available models list\n        if model not in self.available_models:\n            raise ValueError(f\"Model '{model}' is not available. Please choose from the following: {self.available_models}\")\n\n        # Check if the model is in the available models list\n        if model not in self.available_models:\n            raise ValueError(f\"Model '{model}' is not available. Please choose from the following: {self.available_models}\")\n\n        # Check"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Set the default model to use\n        model = kwargs.get('model', 'gpt-3.5-turbo')\n\n        # Check if the model is available in the configuration\n        if model not in self.config.models:\n            raise BadRequestError(f\"Model '{model}' is not available in the configuration.\")\n\n        # Check if the model is available in the models dictionary\n        if model not in self.models:\n            raise BadRequestError(f\"Model '{model}' is not available in the models dictionary.\")\n\n        # Get the model object from the models dictionary\n        model_object = self.models[model]\n\n        # Check if the model has a fallback model\n        if model_object.fallback_model:\n            # Get the fallback model object from the models dictionary\n            fallback_model_object = self.models[model_object.fallback_model]\n\n            # Check if the fallback model is available in the configuration\n            if fallback_model_object.model not in self.config.models:\n                raise BadRequestError(f\"Fallback model '{fallback_model_object.model}' is not available in the configuration.\")\n\n            # Check if the fallback model is available in the models dictionary\n            if fallback_model_object.model not in self.models:\n                raise BadRequestError(f\"Fallback model '{fallback_model_object.model}' is not available in the models dictionary.\")\n\n            # Get the fallback model object from the models dictionary\n            fallback_model_object = self.models[fallback_model_object.model]\n\n            # Check if the fallback model has a fallback model\n            if fallback_model_object.fallback_model:\n                # Get the fallback model object from the models dictionary\n                fallback_model_object = self.models[fallback_model_object.fallback_model]\n\n                # Check if the fallback model is available in the configuration\n                if fallback_model_object.model not in self.config.models:\n                    raise BadRequestError(f\"Fallback model '{fallback_model_object."}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Get the current configuration\n        config = get_config()\n\n        # Select the model to use based on the provided arguments\n        model = kwargs.get('model', config.get('model'))\n\n        # Set the API endpoint based on the selected model\n        endpoint = config.get('endpoint')\n\n        # Set the API key based on the selected model\n        api_key = config.get('api_key')\n\n        # Set the API version based on the selected model\n        api_version = config.get('api_version')\n\n        # Set the API request parameters based on the selected model\n        api_request_params = config.get('api_request_params')\n\n        # Set the API request headers based on the selected model\n        api_request_headers = config.get('api_request_headers')\n\n        # Set the API request body based on the selected model\n        api_request_body = config.get('api_request_body')\n\n        # Set the API request timeout based on the selected model\n        api_request_timeout = config.get('api_request_timeout')\n\n        # Set the API request timeout based on the selected model\n        api_request_timeout = config.get('api_request_timeout')\n\n        # Set the API request timeout based on the selected model\n        api_request_timeout = config.get('api_request_timeout')\n\n        # Set the API request timeout based on the selected model\n        api_request_timeout = config.get('api_request_timeout')\n\n        # Set the API request timeout based on the selected model\n        api_request_timeout = config.get('api_request_timeout')\n\n        # Set the API request timeout based on the selected model\n        api_request_timeout = config.get('api_request_timeout')\n\n        # Set the API request timeout based on the selected model\n        api_request_timeout = config.get('api_request_timeout')\n\n        # Set the API request timeout based on the selected model\n        api_request_timeout = config.get('api_request_timeout')\n\n        # Set the API request timeout based on the selected model\n        api_request_timeout ="}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Check if the model argument is provided\n        if 'model' not in kwargs:\n            raise ValueError(\"Model argument is required.\")\n\n        # Check if the model argument is valid\n        if kwargs['model'] not in self.models:\n            raise ValueError(\"Invalid model argument.\")\n\n        # Check if the model argument is a fallback model\n        if kwargs['model'] in self.fallback_models:\n            # Check if the fallback model is available\n            if kwargs['model'] not in self.available_models:\n                raise ValueError(\"Fallback model is not available.\")\n\n        # Set the model to use for chat completion\n        model = kwargs['model']\n\n        # Check if the model is a fallback model\n        if model in self.fallback_models:\n            # Check if the fallback model is available\n            if model not in self.available_models:\n                raise ValueError(\"Fallback model is not available.\")\n\n            # Use the fallback model\n            model = self.fallback_models[model]\n\n        # Check if the model is a higher-capacity model\n        if model in self.higher_capacity_models:\n            # Check if the higher-capacity model is available\n            if model not in self.available_models:\n                raise ValueError(\"Higher-capacity model is not available.\")\n\n            # Use the higher-capacity model\n            model = self.higher_capacity_models[model]\n\n        # Check if the model is a higher-capacity model\n        if model in self.higher_capacity_models:\n            # Check if the higher-capacity model is available\n            if model not in self.available_models:\n                raise ValueError(\"Higher-capacity model is not available.\")\n\n            # Use the higher-capacity model\n            model = self.higher_capacity_models[model]\n\n        # Check if the model is a higher-capacity model\n        if model in self.higher_capacity_models:\n            # Check if the higher-capacity model is available\n            if model not in self.available_"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Get the API key from the environment variable\n        api_key = os.environ.get(\"OPENAI_API_KEY\")\n\n        # If the API key is not set, raise an error\n        if not api_key:\n            raise BadRequestError(\"OPENAI_API_KEY is not set\")\n\n        # Get the API endpoint from the environment variable\n        api_endpoint = os.environ.get(\"OPENAI_API_ENDPOINT\")\n\n        # If the API endpoint is not set, use the default value\n        if not api_endpoint:\n            api_endpoint = \"https://api.openai.com/v1/chat/completions\"\n\n        # Get the model from the arguments or the configuration\n        model = kwargs.get(\"model\", \"gpt-3.5-turbo\")\n\n        # Get the messages from the arguments or the configuration\n        messages = kwargs.get(\"messages\", [])\n\n        # Get the temperature from the arguments or the configuration\n        temperature = kwargs.get(\"temperature\", 0.0)\n\n        # Get the max_tokens from the arguments or the configuration\n        max_tokens = kwargs.get(\"max_tokens\", 100)\n\n        # Get the top_p from the arguments or the configuration\n        top_p = kwargs.get(\"top_p\", 1.0)\n\n        # Get the frequency_penalty from the arguments or the configuration\n        frequency_penalty = kwargs.get(\"frequency_penalty\", 0.0)\n\n        # Get the presence_penalty from the arguments or the configuration\n        presence_penalty = kwargs.get(\"presence_penalty\", 0.0)\n\n        # Get the stop from the arguments or the configuration\n        stop = kwargs.get(\"stop\", None)\n\n        # Get the user from the arguments or the configuration\n        user = kwargs.get(\"user\", None)\n\n        # Get the stream from the arguments or the configuration\n        stream = kwargs.get(\"stream\", False)\n\n        #"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Retrieve the configuration settings from the configuration file\n        config = Config()\n\n        # Determine the model to use based on the provided arguments\n        model_name = kwargs.get('model', config.model)\n\n        # Determine the maximum number of tokens to use based on the provided arguments\n        max_tokens = kwargs.get('max_tokens', config.max_tokens)\n\n        # Determine the temperature value to use based on the provided arguments\n        temperature = kwargs.get('temperature', config.temperature)\n\n        # Determine the presence penalty value to use based on the provided arguments\n        presence_penalty = kwargs.get('presence_penalty', config.presence_penalty)\n\n        # Determine the frequency penalty value to use based on the provided arguments\n        frequency_penalty = kwargs.get('frequency_penalty', config.frequency_penalty)\n\n        # Determine the number of completions to generate based on the provided arguments\n        n = kwargs.get('n', config.n)\n\n        # Determine the stop sequence to use based on the provided arguments\n        stop = kwargs.get('stop', config.stop)\n\n        # Determine the user provided messages to use based on the provided arguments\n        messages = kwargs.get('messages', config.messages)\n\n        # Determine the stream flag to use based on the provided arguments\n        stream = kwargs.get('stream', config.stream)\n\n        # Determine the logprobs flag to use based on the provided arguments\n        logprobs = kwargs.get('logprobs', config.logprobs)\n\n        # Determine the echo flag to use based on the provided arguments\n        echo = kwargs.get('echo', config.echo)\n\n        # Determine the model flag to use based on the provided arguments\n        model = kwargs.get('model', config.model)\n\n        # Determine the best of flag to use based on the provided arguments\n        best_of = kwargs.get('best_of', config.best_of)\n\n        # Determine the"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Initialize variables\n        model = kwargs.get('model')\n        messages = kwargs.get('messages')\n        max_tokens = kwargs.get('max_tokens', 2048)\n        temperature = kwargs.get('temperature', 0.5)\n        top_p = kwargs.get('top_p', 1)\n        frequency_penalty = kwargs.get('frequency_penalty', 0)\n        presence_penalty = kwargs.get('presence_penalty', 0)\n        stop = kwargs.get('stop', None)\n        logit_bias = kwargs.get('logit_bias', None)\n        user = kwargs.get('user', None)\n\n        # Check if model is provided\n        if not model:\n            raise ValueError(\"Model not provided\")\n\n        # Check if messages are provided\n        if not messages:\n            raise ValueError(\"Messages not provided\")\n\n        # Check if context length limit is exceeded\n        context_length = self.get_context_length(messages)\n        if context_length > self.max_context_length:\n            # Check if fallback model is available\n            fallback_model = self.get_fallback_model(model)\n            if fallback_model:\n                # Use fallback model if available\n                model = fallback_model\n            else:\n                # Raise error if no fallback model is available\n                raise BadRequestError(f\"Context length limit exceeded for model {model}\")\n\n        # Make API request\n        response = openai.ChatCompletion.create(\n            model=model,\n            messages=messages,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            stop=stop,\n            logit_bias=logit_bias,\n            user=user\n        )\n\n        # Return response\n        return response"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Initialize variables\n        api_key = self.api_key\n        api_base_url = self.api_base_url\n        api_version = self.api_version\n        model = kwargs.get('model')\n        messages = kwargs.get('messages')\n        temperature = kwargs.get('temperature', 0.5)\n        max_tokens = kwargs.get('max_tokens', 100)\n        top_p = kwargs.get('top_p', 1.0)\n        frequency_penalty = kwargs.get('frequency_penalty', 0.0)\n        presence_penalty = kwargs.get('presence_penalty', 0.0)\n        stop = kwargs.get('stop', None)\n\n        # Select a model based on the provided arguments\n        if model is None:\n            if self.model is not None:\n                model = self.model\n            else:\n                raise ValueError(\"No model specified\")\n\n        # Construct the API request URL\n        url = f\"{api_base_url}/chat/completions\"\n\n        # Construct the API request headers\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key}\"\n        }\n\n        # Construct the API request data\n        data = {\n            \"model\": model,\n            \"messages\": messages,\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens,\n            \"top_p\": top_p,\n            \"frequency_penalty\": frequency_penalty,\n            \"presence_penalty\": presence_penalty,\n            \"stop\": stop\n        }\n\n        # Make the API request\n        response = requests.post(url, headers=headers, json=data)\n\n        # Handle the response\n        if response.status_code == 200:\n            response_data = response.json()\n            return response_data\n        elif response.status_code == 400:\n            response_data = response.json()"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Set the default model to use if none is specified\n        default_model = 'gpt-3.5-turbo'\n\n        # Get the model to use from the kwargs or fall back to the default model\n        model = kwargs.get('model', default_model)\n\n        # Get the context length limit from the kwargs or use the default value\n        context_length_limit = kwargs.get('context_length_limit', 4000)\n\n        # Get the fallback models from the kwargs or use the default value\n        fallback_models = kwargs.get('fallback_models', ['gpt-4', 'gpt-3.5-turbo-16k'])\n\n        # Get the API key from the kwargs or use the default value\n        api_key = kwargs.get('api_key', os.environ.get('OPENAI_API_KEY'))\n\n        # Get the API endpoint from the kwargs or use the default value\n        api_endpoint = kwargs.get('api_endpoint', 'https://api.openai.com/v1/chat/completions')\n\n        # Get the API request headers from the kwargs or use the default value\n        api_headers = kwargs.get('api_headers', {'Content-Type': 'application/json', 'Authorization': f'Bearer {api_key}'})\n\n        # Get the API request body from the kwargs or use the default value\n        api_request_body = kwargs.get('api_request_body', {'model': model})\n\n        # Get the API request method from the kwargs or use the default value\n        api_request_method = kwargs.get('api_request_method', 'POST')\n\n        # Get the API request timeout from the kwargs or use the default value\n        api_request_timeout = kwargs.get('api_request_timeout', 60)\n\n        # Get the API request retries from the kwargs or use the default value\n        api_request_retries = kwargs.get('"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Initialize variables\n        response = None\n        error_message = None\n        # Select a model based on the provided arguments\n        model = kwargs.get('model', None)\n        # Make a request to the OpenAI API\n        try:\n            response = openai.ChatCompletion.create(**kwargs)\n        except openai.error.InvalidRequestError as e:\n            error_message = f\"Error: {e}\"\n            if \"context length\" in error_message:\n                # Check if a fallback model is available to handle the request\n                fallback_model = self.get_fallback_model(model)\n                if fallback_model:\n                    # If a fallback model is available, use it to handle the request\n                    kwargs['model'] = fallback_model\n                    try:\n                        response = openai.ChatCompletion.create(**kwargs)\n                    except openai.error.InvalidRequestError as e:\n                        error_message = f\"Error: {e}\"\n                        if \"context length\" in error_message:\n                            # If the context length limit is still exceeded, raise an error\n                            raise BadRequestError(error_message)\n                        else:\n                            # If another error occurs, raise it\n                            raise BadRequestError(error_message)\n                else:\n                    # If no fallback model is available, raise an error\n                    raise BadRequestError(error_message)\n            else:\n                # If another error occurs, raise it\n                raise BadRequestError(error_message)\n        except openai.error.APIError as e:\n            error_message = f\"Error: {e}\"\n            raise BadRequestError(error_message)\n        except openai.error.APIConnectionError as e:\n            error_message = f\"Error: {e}\"\n            raise BadRequestError(error_message)\n        except openai.error.RateLimitError as e:\n            error_message = f\"Error: {e}\"\n            raise BadRequestError(error_message)\n        except openai.error.ServiceUnavailableError as e:\n            error_message = f\"Error: {e}\"\n            raise BadRequestError(error_message"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Initialize variables to store the model to use and the API request settings\n        model_to_use = None\n        api_request_settings = {}\n\n        # Set the model to use based on the provided arguments\n        if 'model' in kwargs:\n            model_to_use = kwargs['model']\n        else:\n            model_to_use = self.config.get('model')\n\n        # Set the API request settings based on the provided arguments\n        if 'api_request_settings' in kwargs:\n            api_request_settings = kwargs['api_request_settings']\n\n        # Initialize a variable to store the response from the API\n        response = None\n\n        # Initialize a variable to store the context length limit\n        context_length_limit = None\n\n        # Set the context length limit based on the model\n        if model_to_use == 'gpt-4':\n            context_length_limit = 8192\n        elif model_to_use == 'gpt-3.5-turbo':\n            context_length_limit = 4096\n        elif model_to_use == 'gpt-3.5-turbo-16k':\n            context_length_limit = 16384\n        elif model_to_use == 'gpt-3.5-turbo-32k':\n            context_length_limit = 32768\n\n        # Check if the context length limit is exceeded\n        if self.context_length > context_length_limit:\n\n            # If the context length limit is exceeded, try to use a higher-capacity model\n            if model_to_use == 'gpt-4':\n                model_to_use = 'gpt-4-32k'\n            elif model_to_use == 'gpt-3.5-turbo':\n                model_to_use = 'gpt-3.5-turbo-16k'\n            elif model_to_use == 'gpt-3.5-turbo-16k':\n                model_to_use = 'gpt-3"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Check if the 'model' argument is provided\n        if 'model' not in kwargs:\n            raise ValueError(\"'model' argument is required for chatcompletion_request\")\n\n        # Extract the 'model' argument from the keyword arguments\n        model = kwargs.pop('model')\n\n        # Check if the model is available in the configuration\n        if model not in self.config.models:\n            raise ValueError(f\"Model '{model}' is not available in the configuration\")\n\n        # Check if the model has a fallback model\n        if self.config.models[model].fallback_model is None:\n            raise ValueError(f\"Model '{model}' does not have a fallback model\")\n\n        # Check if the model has a fallback model that is available in the configuration\n        if self.config.models[model].fallback_model not in self.config.models:\n            raise ValueError(f\"Fallback model '{self.config.models[model].fallback_model}' for model '{model}' is not available in the configuration\")\n\n        # Check if the model has a fallback model that is available in the configuration and has a fallback model\n        if self.config.models[self.config.models[model].fallback_model].fallback_model is not None:\n            raise ValueError(f\"Fallback model '{self.config.models[model].fallback_model}' for model '{model}' has a fallback model, which is not supported\")\n\n        # Get the fallback model for the model\n        fallback_model = self.config.models[model].fallback_model\n\n        # Check if the fallback model has a context length limit\n        if self.config.models[fallback_model].context_length is None:\n            raise ValueError(f\"Fallback model '{fallback_model}' for model '{model}' does not have a context length limit\")\n\n        # Check if the context length limit for the model is exceeded\n        if self.config.models[model].context_length <= self.config.context_length:\n            # If the context length limit is not exceeded,"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Select model based on provided arguments and configuration\n        model = kwargs.get('model', None)\n        if model is None:\n            model = self.config.get('model', 'gpt-3.5-turbo')\n\n        # Check if model is available\n        if model not in self.models:\n            raise BadRequestError(f\"Model {model} is not available.\")\n\n        # Check if model exceeds context length\n        if self.models[model]['context_length'] > 4000:\n            # Attempt to use a higher-capacity model if available\n            for model_name in self.models:\n                if self.models[model_name]['context_length'] <= 4000:\n                    kwargs['model'] = model_name\n                    break\n            else:\n                raise BadRequestError(f\"Model {model} exceeds context length limit and no fallback models are available.\")\n\n        # Make request to OpenAI API\n        response = self.openai_api.chat_completion(**kwargs)\n\n        # Check if response is successful\n        if response.status_code != 200:\n            raise BadRequestError(f\"Error occurred during chat completion operation: {response.text}\")\n\n        # Return response\n        return response.json()\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Initialize variables\n        api_key = kwargs.pop('api_key', None)\n        api_base = kwargs.pop('api_base', None)\n        api_version = kwargs.pop('api_version', None)\n        api_type = kwargs.pop('api_type', None)\n        model = kwargs.pop('model', None)\n\n        # Check if the model is provided as a keyword argument\n        if model:\n            model_name = model\n        else:\n            # If the model is not provided, use the default model\n            model_name = 'gpt-3.5-turbo'\n\n        # Check if the model is available\n        if model_name not in MODEL_TO_CONTEXT_LENGTH:\n            raise ValueError(f\"Model {model_name} is not available.\")\n\n        # Check if the context length limit is exceeded\n        if len(kwargs.get('messages', [])) > MODEL_TO_CONTEXT_LENGTH[model_name]:\n            # Check if a fallback model is available\n            fallback_model_name = FALLBACK_MODEL_TO_CONTEXT_LENGTH.get(model_name)\n            if fallback_model_name:\n                # Use the fallback model if available\n                model_name = fallback_model_name\n                kwargs['model'] = model_name\n                print(f\"Using fallback model {model_name} due to exceeded context length limit.\")\n            else:\n                # Raise an error if no fallback model is available\n                raise ValueError(f\"Model {model_name} is not available due to exceeded context length limit.\")\n\n        # Make the API request\n        try:\n            response = openai.ChatCompletion.create(model=model_name, **kwargs)\n        except Exception as e:\n            # Raise an error if the API request fails\n            raise BadRequestError(e)\n\n        # Return the response\n        return response"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Select a model based on the provided arguments\n        model = kwargs.pop('model', None)\n\n        # If no model is provided, use the default model\n        if not model:\n            model = self.default_model\n\n        # If the model is not available, raise an error\n        if model not in self.models:\n            raise BadRequestError(f\"Model '{model}' not available.\")\n\n        # Get the configuration for the selected model\n        model_config = self.models[model]\n\n        # If the context length is exceeded, try using a higher-capacity model if available\n        if len(self.context) > model_config['max_context_length']:\n            # Get the list of fallback models for the selected model\n            fallback_models = model_config.get('fallback_models', [])\n\n            # If no fallback models are available, raise an error\n            if not fallback_models:\n                raise BadRequestError(f\"Context length exceeded for model '{model}'.\")\n\n            # Try using a higher-capacity model\n            for fallback_model in fallback_models:\n                # If the fallback model is available, use it\n                if fallback_model in self.models:\n                    model = fallback_model\n                    break\n\n            # If no higher-capacity model is available, raise an error\n            if model == self.default_model:\n                raise BadRequestError(f\"Context length exceeded for model '{model}'.\")\n\n        # Create the request payload\n        request_payload = {\n            'model': model,\n            'messages': self.context + [{'role': 'user', 'content': kwargs.pop('prompt')}],\n            'temperature': kwargs.pop('temperature', 0.7),\n            'max_tokens': kwargs.pop('max_tokens', 500),\n            'top_p': kwargs.pop('top_p', 1.0),\n            'frequency_penalty': kwargs.pop('frequency_penalty', 0.0),\n            'presence_penal"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._client_creation_time + self._client_expiration_interval < time.time():\n            self._client = boto3.client(\n                's3',\n                aws_access_key_id=self._aws_access_key_id,\n                aws_secret_access_key=self._aws_secret_access_key,\n                region_name=self._region_name\n            )\n            self._client_creation_time = time.time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (time.time() - self._client_creation_time) > self._client_expiration_interval:\n            self._client = boto3.client(\n                's3',\n                aws_access_key_id=self._aws_access_key_id,\n                aws_secret_access_key=self._aws_secret_access_key,\n                region_name=self._region_name\n            )\n            self._client_creation_time = time.time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._client_creation_time + self._client_refresh_interval < time.time():\n            self._client = boto3.client(\n                's3',\n                aws_access_key_id=self._aws_access_key_id,\n                aws_secret_access_key=self._aws_secret_access_key,\n                region_name=self._region_name\n            )\n            self._client_creation_time = time.time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._client_creation_time + self._client_creation_interval < time.time():\n            self._client = boto3.client(\n                's3',\n                aws_access_key_id=self._aws_access_key_id,\n                aws_secret_access_key=self._aws_secret_access_key,\n                region_name=self._region_name,\n            )\n            self._client_creation_time = time.time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self.client_instance is None or self.client_creation_time + self.client_expiration_interval < time.time():\n            self.client_instance = boto3.client(\n                \"s3\",\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key,\n            )\n            self.client_creation_time = time.time()\n        return self.client_instance"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self._client:\n            self._client = boto3.client(\n                \"s3\",\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key,\n                region_name=self.region_name,\n            )\n            self._creation_time = datetime.now()\n        elif (\n            datetime.now() - self._creation_time\n        ).total_seconds() > self.client_creation_interval:\n            self._client = boto3.client(\n                \"s3\",\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key,\n                region_name=self.region_name,\n            )\n            self._creation_time = datetime.now()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self.client_instance is None or self.client_creation_time + self.client_expiration_interval < time.time():\n            self.client_instance = boto3.client(\n                's3',\n                aws_access_key_id=self.access_key,\n                aws_secret_access_key=self.secret_key,\n                region_name=self.region,\n                endpoint_url=self.endpoint\n            )\n            self.client_creation_time = time.time()\n\n        return self.client_instance"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (time.time() - self._last_created) > self._interval:\n            self._client = boto3.client(\n                \"s3\",\n                aws_access_key_id=self._access_key,\n                aws_secret_access_key=self._secret_key,\n                region_name=self._region_name,\n            )\n            self._last_created = time.time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._client_creation_time is None or (\n            time.time() - self._client_creation_time > self._client_expiration_interval\n        ):\n            self._client = boto3.client(\n                \"s3\",\n                aws_access_key_id=self._access_key,\n                aws_secret_access_key=self._secret_key,\n            )\n            self._client_creation_time = time.time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (datetime.now() - self._client_creation_time).total_seconds() > self._client_expiration_interval:\n            self._client = boto3.client(\"s3\",\n                                        aws_access_key_id=self._aws_access_key_id,\n                                        aws_secret_access_key=self._aws_secret_access_key,\n                                        region_name=self._region_name)\n            self._client_creation_time = datetime.now()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self.client_instance is None or time.time() - self.last_created > self.client_interval:\n            self.client_instance = boto3.client(\n                's3',\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key,\n                region_name=self.region_name\n            )\n            self.last_created = time.time()\n        return self.client_instance"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (datetime.now() - self._client_creation_time).total_seconds() > self._client_renewal_interval:\n            self._client = boto3.client(\n                \"s3\",\n                aws_access_key_id=self._aws_access_key_id,\n                aws_secret_access_key=self._aws_secret_access_key,\n                region_name=self._region_name,\n            )\n            self._client_creation_time = datetime.now()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self.client is None or (time.time() - self.client_creation_time) > self.client_creation_interval:\n            self.client = boto3.client(\"s3\",\n                                       aws_access_key_id=self.access_key,\n                                       aws_secret_access_key=self.secret_key,\n                                       region_name=self.region_name)\n            self.client_creation_time = time.time()\n\n        return self.client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self.client is None or self.client_creation_time is None or (\n            datetime.now() - self.client_creation_time\n        ) > timedelta(seconds=self.client_expiration_interval):\n            self.client = boto3.client(\n                \"s3\",\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key,\n                region_name=self.region_name,\n            )\n            self.client_creation_time = datetime.now()\n        return self.client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (time.time() - self._last_created) > self._interval:\n            self._client = boto3.client(\n                \"s3\",\n                aws_access_key_id=self._access_key_id,\n                aws_secret_access_key=self._secret_access_key,\n                region_name=self._region_name,\n            )\n            self._last_created = time.time()\n\n        return self._client\n\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (time.time() - self._client_creation_time) > self.client_creation_interval:\n            self._client = boto3.client(\n                's3',\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key,\n                region_name=self.region_name\n            )\n            self._client_creation_time = time.time()\n\n        return self._client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self.client is None or (time.time() - self.client_creation_time) > self.client_expiration_interval:\n            self.client = boto3.client(\n                \"s3\",\n                aws_access_key_id=self.access_key,\n                aws_secret_access_key=self.secret_key,\n                region_name=self.region,\n            )\n            self.client_creation_time = time.time()\n\n        return self.client\n\n\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self.client_instance or (time.time() - self.client_creation_time) > self.client_expiration_interval:\n            self.client_instance = boto3.client(\n                's3',\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key,\n                region_name=self.region_name\n            )\n            self.client_creation_time = time.time()\n\n        return self.client_instance\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self.client:\n            self.client = boto3.client(\n                \"s3\",\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key,\n                region_name=self.region_name,\n            )\n            self.client_creation_time = datetime.datetime.now()\n        elif (\n            datetime.datetime.now() - self.client_creation_time\n        ) > self.client_expiration_interval:\n            self.client = boto3.client(\n                \"s3\",\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key,\n                region_name=self.region_name,\n            )\n            self.client_creation_time = datetime.datetime.now()\n\n        return self.client\n\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self.client is None or self.client_creation_time is None or (time.time() - self.client_creation_time) > self.client_creation_interval:\n            self.client = boto3.client(\n                's3',\n                aws_access_key_id=self.access_key,\n                aws_secret_access_key=self.secret_key,\n                region_name=self.region\n            )\n            self.client_creation_time = time.time()\n        return self.client\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if num_workers > 0:\n            raise RuntimeError(\n                \"state_dict() should not be called from a worker process.\"\n            )\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir_path,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader_state,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if num_workers > 0:\n            raise RuntimeError(\"Cannot call `state_dict` from a worker process.\")\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader_state,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self._is_worker:\n            raise RuntimeError(\n                \"state_dict() should not be called from a worker process.\"\n            )\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir_path,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader_state,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self._rank is not None:\n            raise RuntimeError(\n                \"state_dict() should not be called from a DataLoader worker process\"\n            )\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self._current_epoch,\n            \"input_dir_path\": self._input_dir_path,\n            \"input_url\": self._input_url,\n            \"item_loader_state\": self._item_loader_state,\n            \"last_batch_dropped\": self._last_batch_dropped,\n            \"seed\": self._seed,\n            \"world_size\": self._world_size,\n            \"shuffle\": self._shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if num_workers > 0:\n            raise RuntimeError(\"state_dict() should not be called from a DataLoader worker process\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir_path,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader_state,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self.worker_info is not None:\n            raise RuntimeError(\"Dataset.state_dict() can only be called from the main process\")\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir_path,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader is not None else None,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self.world_size > 1 and num_workers > 1:\n            raise RuntimeError(\n                \"state_dict() should not be called from a DataLoader worker process.\"\n            )\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir_path,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader_state,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self._is_distributed:\n            if dist.get_rank() != 0:\n                raise RuntimeError(\n                    \"state_dict() should only be called from the master process.\"\n                )\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self._current_epoch,\n            \"input_dir_path\": self._input_dir_path,\n            \"input_url\": self._input_url,\n            \"item_loader_state\": self._item_loader_state,\n            \"last_batch_dropped\": self._last_batch_dropped,\n            \"seed\": self._seed,\n            \"world_size\": self._world_size,\n            \"shuffle\": self._shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self.rank is not None and self.rank >= 0:\n            raise RuntimeError(\"state_dict() can only be called from the main process.\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir_path,\n            \"input_url\": self.input_url,\n        }\n\n        if self.item_loader is not None:\n            state[\"item_loader_state\"] = self.item_loader.state_dict()\n\n        if self.last_batch_is_dropped:\n            state[\"last_batch_is_dropped\"] = self.last_batch_is_dropped\n\n        if self.seed is not None:\n            state[\"seed\"] = self.seed\n\n        if self.world_size is not None:\n            state[\"world_size\"] = self.world_size\n\n        if self.shuffle:\n            state[\"shuffle\"] = self.shuffle\n\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self.is_worker:\n            raise RuntimeError(\"state_dict() should not be called from a worker process\")\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader_state,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self.is_worker:\n            raise RuntimeError(\"`state_dict` should not be called from a worker process\")\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader_state,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self.rank is not None:\n            raise RuntimeError(\"Dataset state is not supported in distributed mode\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"epoch\": self.epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n        }\n\n        if self.item_loader_state is not None:\n            state[\"item_loader_state\"] = self.item_loader_state\n\n        state[\"last_batch_dropped\"] = self.last_batch_dropped\n        state[\"seed\"] = self.seed\n        state[\"world_size\"] = self.world_size\n        state[\"shuffle\"] = self.shuffle\n\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if num_workers > 0:\n            raise RuntimeError(\"state_dict should not be called from a worker process\")\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self._current_epoch,\n            \"input_dir\": self._input_dir,\n            \"input_url\": self._input_url,\n            \"item_loader_state\": self._item_loader_state,\n            \"last_batch_dropped\": self._last_batch_dropped,\n            \"seed\": self._seed,\n            \"world_size\": self._world_size,\n            \"shuffle\": self._shuffle,\n        }\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if num_workers != 0 and num_workers != 1:\n            raise ValueError(\"state_dict can only be called from a single worker process\")\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir_path,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader is not None else None,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if num_workers > 0:\n            raise RuntimeError(\"state_dict() can only be called from the main process\")\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir_path,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader_state,\n            \"last_batch_is_dropped\": self.last_batch_is_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self.rank != 0:\n            raise ValueError(\n                \"state_dict() should only be called from the main process. Called from process {}\".format(\n                    self.rank\n                )\n            )\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir_path,\n            \"input_url\": self.input_url,\n        }\n\n        if self.item_loader_state is not None:\n            state[\"item_loader_state\"] = self.item_loader_state\n\n        if self.last_batch_dropped:\n            state[\"last_batch_dropped\"] = self.last_batch_dropped\n\n        if self.seed is not None:\n            state[\"seed\"] = self.seed\n\n        if self.world_size > 1:\n            state[\"world_size\"] = self.world_size\n\n        if self.shuffle:\n            state[\"shuffle\"] = self.shuffle\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self.is_distributed and dist.get_rank() != 0:\n            raise RuntimeError(\"Cannot call `state_dict` from a worker process.\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n        }\n\n        if self.item_loader is not None:\n            state[\"item_loader_state\"] = self.item_loader.state_dict()\n\n        state[\"last_batch_dropped\"] = self.last_batch_dropped\n        state[\"seed\"] = self.seed\n\n        if self.is_distributed:\n            state[\"world_size\"] = dist.get_world_size()\n            state[\"shuffle\"] = self.shuffle\n\n        return state\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self._rank is not None and self._rank != 0:\n            raise RuntimeError(\"state_dict is not supported when StreamingDataset is used with a DataLoader in a multi-process setting.\")\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self._current_epoch,\n            \"input_dir_path\": self._input_dir_path,\n            \"input_url\": self._input_url,\n            \"item_loader_state\": self._item_loader.state_dict() if self._item_loader else None,\n            \"last_batch_dropped\": self._last_batch_dropped,\n            \"seed\": self._seed,\n            \"world_size\": self._world_size,\n            \"shuffle\": self._shuffle,\n        }\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if num_workers > 0:\n            raise RuntimeError(\n                \"state_dict() can only be called from the main process of a DataLoader\"\n            )\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader_state,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if num_workers > 0:\n            raise RuntimeError(\n                \"state_dict() should not be called in DataLoader workers.\"\n            )\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader.state_dict()\n            if self.item_loader is not None\n            else None,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n        self.state_dict[\"last_loaded_index\"] = self.state_dict.get(\"last_loaded_index\", 0)\n        self.state_dict[\"last_loaded_timestamp\"] = self.state_dict.get(\"last_loaded_timestamp\", 0)\n        self.state_dict[\"last_loaded_data\"] = self.state_dict.get(\"last_loaded_data\", None)\n        self.state_dict[\"last_loaded_data_timestamp\"] = self.state_dict.get(\"last_loaded_data_timestamp\", 0)\n        self.state_dict[\"last_loaded_data_index\"] = self.state_dict.get(\"last_loaded_data_index\", 0)\n        self.state_dict[\"last_loaded_data_index_timestamp\"] = self.state_dict.get(\"last_loaded_data_index_timestamp\", 0)\n        self.state_dict[\"last_loaded_data_index_timestamp_index\"] = self.state_dict.get(\"last_loaded_data_index_timestamp_index\", 0)\n        self.state_dict[\"last_loaded_data_index_timestamp_index_index\"] = self.state_dict.get(\"last_loaded_data_index_timestamp_index_index\", 0)\n        self.state_dict[\"last_loaded_data_index_timestamp_index_index_index\"] = self.state_dict.get(\"last_loaded_data_index_timestamp_index_index_index\", 0)\n        self.state_dict[\"last_loaded_data_index_timestamp_index_index_index_index\"] = self.state_dict.get(\"last_loaded_data_index_timestamp_index_index_index_index\", 0)\n        self.state_dict[\"last_loaded_data_index_timestamp_index_index_index_index_index\"] = self.state_dict.get(\"last_loaded_data_index_timestamp_index_index_index_index_index\", 0)\n        self.state_dict[\"last_loaded_data_index_timestamp_index_index_"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n\n        if \"num_batches_streamed\" in state_dict:\n            self.num_batches_streamed = state_dict[\"num_batches_streamed\"]\n        if \"num_batches_streamed_per_epoch\" in state_dict:\n            self.num_batches_streamed_per_epoch = state_dict[\"num_batches_streamed_per_epoch\"]\n        if \"num_batches_streamed_per_epoch_per_node\" in state_dict:\n            self.num_batches_streamed_per_epoch_per_node = state_dict[\"num_batches_streamed_per_epoch_per_node\"]\n        if \"num_batches_streamed_per_epoch_per_node_per_rank\" in state_dict:\n            self.num_batches_streamed_per_epoch_per_node_per_rank = state_dict[\"num_batches_streamed_per_epoch_per_node_per_rank\"]\n        if \"num_batches_streamed_per_epoch_per_node_per_rank_per_gpu\" in state_dict:\n            self.num_batches_streamed_per_epoch_per_node_per_rank_per_gpu = state_dict[\"num_batches_streamed_per_epoch_per_node_per_rank_per_gpu\"]\n        if \"num_batches_streamed_per_epoch_per_node_per_rank_per_gpu_per_worker\" in state_dict:\n            self.num_batches_streamed_per_epoch_per_node_per_rank_per_gpu_per_worker = state_dict[\"num_batches_streamed_per_epoch_per_node_per_rank_per_gpu_per_worker\"]\n        if \"num_batches_streamed_per_epoch_per_node_per_rank_per_gpu_per_worker_per_process\" in state_dict:\n            self.num"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n        self._state = state_dict[\"state\"]\n        self._state_dict = state_dict\n        self._state = state_dict[\"state\"]\n        self._current_index = state_dict[\"current_index\"]\n        self._data_generator = state_dict[\"data_generator\"]\n        self._data_generator_args = state_dict[\"data_generator_args\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n        self._data_generator_kwargs = state_dict[\"data_generator_kwargs\"]\n       "}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n        self.current_index = state_dict.get(\"current_index\", 0)\n        self.last_index = state_dict.get(\"last_index\", 0)\n        self.last_batch = state_dict.get(\"last_batch\", None)\n        self.last_batch_size = state_dict.get(\"last_batch_size\", 0)\n        self.last_batch_index = state_dict.get(\"last_batch_index\", 0)\n        self.last_batch_time = state_dict.get(\"last_batch_time\", 0)\n        self.last_batch_time_str = state_dict.get(\"last_batch_time_str\", None)\n        self.last_batch_time_str_format = state_dict.get(\"last_batch_time_str_format\", None)\n        self.last_batch_time_str_format_str = state_dict.get(\"last_batch_time_str_format_str\", None)\n        self.last_batch_time_str_format_str_format = state_dict.get(\"last_batch_time_str_format_str_format\", None)\n        self.last_batch_time_str_format_str_format_str = state_dict.get(\"last_batch_time_str_format_str_format_str\", None)\n        self.last_batch_time_str_format_str_format_str_format = state_dict.get(\"last_batch_time_str_format_str_format_str_format\", None)\n        self.last_batch_time_str_format_str_format_str_format_str = state_dict.get(\"last_batch_time_str_format_str_format_str_format_str\", None)\n        self.last_batch_time_str_format_str_format_str_format_str_format = state_dict.get(\"last_batch_time_str_format_str_format_str_format_str_format\", None)\n        self.last_batch_time_str_format_str_"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n        self.streaming_data = state_dict.get(\"streaming_data\", None)\n        self.streaming_data_buffer = state_dict.get(\"streaming_data_buffer\", None)\n        self.streaming_data_buffer_size = state_dict.get(\"streaming_data_buffer_size\", None)\n        self.streaming_data_buffer_position = state_dict.get(\"streaming_data_buffer_position\", None)\n        self.streaming_data_buffer_position_in_chunk = state_dict.get(\"streaming_data_buffer_position_in_chunk\", None)\n        self.streaming_data_buffer_position_in_chunk_offset = state_dict.get(\"streaming_data_buffer_position_in_chunk_offset\", None)\n        self.streaming_data_buffer_position_in_chunk_offset_offset = state_dict.get(\"streaming_data_buffer_position_in_chunk_offset_offset\", None)\n        self.streaming_data_buffer_position_in_chunk_offset_offset_offset = state_dict.get(\"streaming_data_buffer_position_in_chunk_offset_offset_offset\", None)\n        self.streaming_data_buffer_position_in_chunk_offset_offset_offset_offset = state_dict.get(\"streaming_data_buffer_position_in_chunk_offset_offset_offset_offset\", None)\n        self.streaming_data_buffer_position_in_chunk_offset_offset_offset_offset_offset = state_dict.get(\"streaming_data_buffer_position_in_chunk_offset_offset_offset_offset_offset\", None)\n        self.streaming_data_buffer_position_in_chunk_offset_offset_offset_offset_offset_offset = state_dict.get(\"streaming_data_buffer_position_in_chunk_offset_offset_offset_offset_offset_offset\", None)\n        self.streaming_data_buffer_position_in_chunk"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n\n        # Load the state_dict into the StreamingDataset instance\n        self.load_state_dict(state_dict)\n\n        # Set the dataset_size attribute based on the state_dict\n        self.dataset_size = state_dict[\"dataset_size\"]\n\n        # Set the current_index attribute based on the state_dict\n        self.current_index = state_dict[\"current_index\"]\n\n        # Set the current_batch_size attribute based on the state_dict\n        self.current_batch_size = state_dict[\"current_batch_size\"]\n\n        # Set the current_batch_index attribute based on the state_dict\n        self.current_batch_index = state_dict[\"current_batch_index\"]\n\n        # Set the current_batch attribute based on the state_dict\n        self.current_batch = state_dict[\"current_batch\"]\n\n        # Set the current_epoch attribute based on the state_dict\n        self.current_epoch = state_dict[\"current_epoch\"]\n\n        # Set the current_epoch_index attribute based on the state_dict\n        self.current_epoch_index = state_dict[\"current_epoch_index\"]\n\n        # Set the current_epoch_batch_index attribute based on the state_dict\n        self.current_epoch_batch_index = state_dict[\"current_epoch_batch_index\"]\n\n        # Set the current_epoch_batch attribute based on the state_dict\n        self.current_epoch_batch = state_dict[\"current_epoch_batch\"]\n\n        # Set the current_epoch_batch_size attribute based on the state_dict\n        self.current_epoch_batch_size = state_dict[\"current_epoch_batch_size\"]\n\n        # Set the current_epoch_batch_index attribute based on the state_dict\n        self.current_epoch_batch_index = state_dict[\"current_epoch_batch_index\"]\n\n        # Set the current_epoch_batch attribute based on the state_dict\n        self.current_epoch_batch"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n        self.index = state_dict['index']\n        self.num_samples = state_dict['num_samples']\n        self.num_features = state_dict['num_features']\n        self.num_classes = state_dict['num_classes']\n        self.batch_size = state_dict['batch_size']\n        self.num_batches = state_dict['num_batches']\n        self.num_epochs = state_dict['num_epochs']\n        self.current_epoch = state_dict['current_epoch']\n        self.current_batch = state_dict['current_batch']\n        self.current_index = state_dict['current_index']\n        self.current_batch_size = state_dict['current_batch_size']\n        self.current_batch_index = state_dict['current_batch_index']\n        self.current_batch_data = state_dict['current_batch_data']\n        self.current_batch_labels = state_dict['current_batch_labels']\n        self.current_batch_indices = state_dict['current_batch_indices']\n        self.current_batch_num_samples = state_dict['current_batch_num_samples']\n        self.current_batch_num_features = state_dict['current_batch_num_features']\n        self.current_batch_num_classes = state_dict['current_batch_num_classes']\n        self.current_batch_num_batches = state_dict['current_batch_num_batches']\n        self.current_batch_num_epochs = state_dict['current_batch_num_epochs']\n        self.current_batch_current_epoch = state_dict['current_batch_current_epoch']\n        self.current_batch_current_batch = state_dict['current_batch_current_batch']\n        self.current_batch_current_index = state_dict['current_batch_current_index']\n        self.current_batch_current_batch_size = state_dict['current"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        for key, value in state_dict.items():\n            setattr(self, key, value)\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n\n        self.current_state = self.state_dict[\"current_state\"]\n        self.stream_state = self.state_dict[\"stream_state\"]\n        self.stream_state_count = self.state_dict[\"stream_state_count\"]\n        self.stream_state_count_last_update = self.state_dict[\"stream_state_count_last_update\"]\n        self.stream_state_count_last_update_time = self.state_dict[\"stream_state_count_last_update_time\"]\n        self.stream_state_count_last_update_time_last_update = self.state_dict[\"stream_state_count_last_update_time_last_update\"]\n        self.stream_state_count_last_update_time_last_update_time = self.state_dict[\"stream_state_count_last_update_time_last_update_time\"]\n        self.stream_state_count_last_update_time_last_update_time_last_update = self.state_dict[\"stream_state_count_last_update_time_last_update_time_last_update\"]\n        self.stream_state_count_last_update_time_last_update_time_last_update_time = self.state_dict[\"stream_state_count_last_update_time_last_update_time_last_update_time\"]\n        self.stream_state_count_last_update_time_last_update_time_last_update_time_last_update = self.state_dict[\"stream_state_count_last_update_time_last_update_time_last_update_time_last_update\"]\n        self.stream_state_count_last_update_time_last_update_time_last_update_time_last_update_time = self.state_dict[\"stream_state_count_last_update_time_last_update_time_last_update_time_last_update_time\"]\n        self.stream_state_count_last_update_time_last_update_time_last_update_time_last"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n\n        # Set the current state based on the provided state dictionary\n        self._state = StreamingDatasetState(**self._state_dict)\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._set_current_state()\n\n        # Set the current state of the dataset\n        self._"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n        self.num_samples = state_dict[\"num_samples\"]\n        self.num_epochs = state_dict[\"num_epochs\"]\n        self.epoch_idx = state_dict[\"epoch_idx\"]\n        self.batch_idx = state_dict[\"batch_idx\"]\n        self.batch_size = state_dict[\"batch_size\"]\n        self.shuffle = state_dict[\"shuffle\"]\n        self.drop_last = state_dict[\"drop_last\"]\n        self.num_batches_per_epoch = state_dict[\"num_batches_per_epoch\"]\n        self.batch_size_per_epoch = state_dict[\"batch_size_per_epoch\"]\n        self.num_batches_per_epoch = state_dict[\"num_batches_per_epoch\"]\n        self.batch_size_per_epoch = state_dict[\"batch_size_per_epoch\"]\n        self.num_batches_per_epoch = state_dict[\"num_batches_per_epoch\"]\n        self.batch_size_per_epoch = state_dict[\"batch_size_per_epoch\"]\n        self.num_batches_per_epoch = state_dict[\"num_batches_per_epoch\"]\n        self.batch_size_per_epoch = state_dict[\"batch_size_per_epoch\"]\n        self.num_batches_per_epoch = state_dict[\"num_batches_per_epoch\"]\n        self.batch_size_per_epoch = state_dict[\"batch_size_per_epoch\"]\n        self.num_batches_per_epoch = state_dict[\"num_batches_per_epoch\"]\n        self.batch_size_per_epoch = state_dict[\"batch_size_per_epoch\"]\n        self.num_batches_per_epoch = state_dict[\"num_batches_per_epoch\"]\n        self.batch_size_per_epoch = state_dict[\"batch"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.data_stream.load_state_dict(state_dict[\"data_stream\"])\n        self.data_stream.data_queue = state_dict[\"data_stream\"][\"data_queue\"]\n        self.data_stream.batch_size = state_dict[\"data_stream\"][\"batch_size\"]\n        self.data_stream.batch_size_counter = state_dict[\"data_stream\"][\"batch_size_counter\"]\n        self.data_stream.batch_size_threshold = state_dict[\"data_stream\"][\"batch_size_threshold\"]\n        self.data_stream.num_batches = state_dict[\"data_stream\"][\"num_batches\"]\n        self.data_stream.num_batches_counter = state_dict[\"data_stream\"][\"num_batches_counter\"]\n        self.data_stream.num_batches_threshold = state_dict[\"data_stream\"][\"num_batches_threshold\"]\n        self.data_stream.num_batches_per_epoch = state_dict[\"data_stream\"][\"num_batches_per_epoch\"]\n        self.data_stream.num_batches_per_epoch_counter = state_dict[\"data_stream\"][\"num_batches_per_epoch_counter\"]\n        self.data_stream.num_batches_per_epoch_threshold = state_dict[\"data_stream\"][\"num_batches_per_epoch_threshold\"]\n        self.data_stream.num_batches_per_epoch_threshold = state_dict[\"data_stream\"][\"num_batches_per_epoch_threshold\"]\n        self.data_stream.num_batches_per_epoch_threshold = state_dict[\"data_stream\"][\"num_batches_per_epoch_threshold\"]\n        self.data_stream.num_batches_per_epoch_threshold = state_dict[\"data_stream\"][\"num_batches_per_epoch_threshold\"]\n        self.data_stream.num_batches_per_epoch_threshold = state_dict[\""}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n\n        self.data_stream_list = state_dict[\"data_stream_list\"]\n        self.stream_idx = state_dict[\"stream_idx\"]\n        self.stream_idx_list = state_dict[\"stream_idx_list\"]\n        self.stream_idx_list_dict = state_dict[\"stream_idx_list_dict\"]\n        self.stream_idx_list_dict_local = state_dict[\"stream_idx_list_dict_local\"]\n        self.stream_idx_list_dict_remote = state_dict[\"stream_idx_list_dict_remote\"]\n        self.stream_idx_list_dict_remote_idx = state_dict[\"stream_idx_list_dict_remote_idx\"]\n        self.stream_idx_list_dict_remote_idx_local = state_dict[\"stream_idx_list_dict_remote_idx_local\"]\n        self.stream_idx_list_dict_remote_idx_remote = state_dict[\"stream_idx_list_dict_remote_idx_remote\"]\n        self.stream_idx_list_dict_remote_idx_remote_local = state_dict[\"stream_idx_list_dict_remote_idx_remote_local\"]\n        self.stream_idx_list_dict_remote_idx_remote_local_idx = state_dict[\"stream_idx_list_dict_remote_idx_remote_local_idx\"]\n        self.stream_idx_list_dict_remote_idx_remote_local_idx_local = state_dict[\"stream_idx_list_dict_remote_idx_remote_local_idx_local\"]\n        self.stream_idx_list_dict_remote_idx_remote_local_idx_remote = state_dict[\"stream_idx_list_dict_remote_idx_remote_local_idx_remote\"]\n        self.stream_idx_list_dict_remote_idx_remote_local_idx_remote_local = state_dict[\"stream_idx_list_dict_remote_idx_remote_local_idx_remote_local\"]\n        self.stream_idx_list_dict_remote_idx"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n\n        if self.state_dict is None:\n            raise ValueError(\"The provided state_dict is None.\")\n\n        if not isinstance(self.state_dict, dict):\n            raise ValueError(\"The provided state_dict is not a dictionary.\")\n\n        if \"state\" not in self.state_dict:\n            raise ValueError(\"The provided state_dict does not contain the 'state' key.\")\n\n        if \"state_dict\" not in self.state_dict:\n            raise ValueError(\"The provided state_dict does not contain the 'state_dict' key.\")\n\n        if not isinstance(self.state_dict[\"state\"], str):\n            raise ValueError(\"The 'state' value in the provided state_dict is not a string.\")\n\n        if not isinstance(self.state_dict[\"state_dict\"], dict):\n            raise ValueError(\"The 'state_dict' value in the provided state_dict is not a dictionary.\")\n\n        if self.state_dict[\"state\"] not in [\"INITIALIZED\", \"STREAMING\", \"FINISHED\"]:\n            raise ValueError(\"The 'state' value in the provided state_dict is not valid.\")\n\n        if self.state_dict[\"state\"] == \"INITIALIZED\":\n            self.state = \"INITIALIZED\"\n\n        if self.state_dict[\"state\"] == \"STREAMING\":\n            self.state = \"STREAMING\"\n\n        if self.state_dict[\"state\"] == \"FINISHED\":\n            self.state = \"FINISHED\"\n\n        self.state_dict = self.state_dict[\"state_dict\"]\n\n        if \"stream_id\" not in self.state_dict:\n            raise ValueError(\"The provided state_dict does not contain the 'stream_id' key.\")\n\n        if not isinstance(self.state_dict[\"stream_id\"], str):\n            raise ValueError(\"The 'stream_id' value in the provided state_dict is not a string.\")\n\n        self.stream_id = self.state_dict[\"stream_id\"]"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        # Check if the state dictionary is not empty\n        if state_dict is not None:\n            # Iterate through the state dictionary\n            for key, value in state_dict.items():\n                # Check if the key is in the StreamingDataset instance\n                if key in self.__dict__:\n                    # Set the corresponding attribute in the StreamingDataset instance to the value\n                    setattr(self, key, value)\n\n        # Check if the state dictionary is not empty\n        if state_dict is not None:\n            # Iterate through the state dictionary\n            for key, value in state_dict.items():\n                # Check if the key is in the StreamingDataset instance\n                if key in self.__dict__:\n                    # Set the corresponding attribute in the StreamingDataset instance to the value\n                    setattr(self, key, value)\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"shuffle flag mismatch: state_dict={self._state_dict['shuffle']}, streaming_dataset={self.shuffle}\"\n            )\n\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\n                f\"num_workers mismatch: state_dict={self._state_dict['num_workers']}, streaming_dataset={self.num_workers}\"\n            )\n\n        if self._state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(\n                f\"input_dir mismatch: state_dict={self._state_dict['input_dir']}, streaming_dataset={self.input_dir}\"\n            )\n\n        if self._state_dict[\"url\"] != self.url:\n            raise ValueError(\n                f\"url mismatch: state_dict={self._state_dict['url']}, streaming_dataset={self.url}\"\n            )\n\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"seed mismatch: state_dict={self._state_dict['seed']}, streaming_dataset={self.seed}\"\n            )\n\n        if self._state_dict[\"item_loader\"] != self.item_loader:\n            raise ValueError(\n                f\"item_loader mismatch: state_dict={self._state_dict['item_loader']}, streaming_dataset={self.item_loader}\"\n            )\n\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"drop_last mismatch: state_dict={self._state_dict['drop_last']}, streaming_dataset={self.drop_last}\"\n            )"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"Shuffle state mismatch: state_dict={self._state_dict['shuffle']}, \"\n                f\"self.shuffle={self.shuffle}\"\n            )\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\n                f\"num_workers state mismatch: state_dict={self._state_dict['num_workers']}, \"\n                f\"self.num_workers={self.num_workers}\"\n            )\n        if self._state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(\n                f\"input_dir state mismatch: state_dict={self._state_dict['input_dir']}, \"\n                f\"self.input_dir={self.input_dir}\"\n            )\n        if self._state_dict[\"input_url\"] != self.input_url:\n            raise ValueError(\n                f\"input_url state mismatch: state_dict={self._state_dict['input_url']}, \"\n                f\"self.input_url={self.input_url}\"\n            )\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"seed state mismatch: state_dict={self._state_dict['seed']}, \"\n                f\"self.seed={self.seed}\"\n            )\n        if self._state_dict[\"item_loader_state\"] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"item_loader_state state mismatch: state_dict={self._state_dict['item_loader_state']}, \"\n                f\"self.item_loader.state_dict()={self.item_loader.state_dict()}\"\n            )\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"drop_last state mismatch: state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"shuffle is not consistent between state_dict and StreamingDataset. \"\n                f\"state_dict: {self._state_dict['shuffle']}, StreamingDataset: {self.shuffle}\"\n            )\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\n                f\"num_workers is not consistent between state_dict and StreamingDataset. \"\n                f\"state_dict: {self._state_dict['num_workers']}, StreamingDataset: {self.num_workers}\"\n            )\n        if self._state_dict[\"input_dir_path\"] != self.input_dir_path:\n            raise ValueError(\n                f\"input_dir_path is not consistent between state_dict and StreamingDataset. \"\n                f\"state_dict: {self._state_dict['input_dir_path']}, StreamingDataset: {self.input_dir_path}\"\n            )\n        if self._state_dict[\"input_url\"] != self.input_url:\n            raise ValueError(\n                f\"input_url is not consistent between state_dict and StreamingDataset. \"\n                f\"state_dict: {self._state_dict['input_url']}, StreamingDataset: {self.input_url}\"\n            )\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"seed is not consistent between state_dict and StreamingDataset. \"\n                f\"state_dict: {self._state_dict['seed']}, StreamingDataset: {self.seed}\"\n            )\n        if self._state_dict[\"item_loader_state\"] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"item_loader_state is not consistent between state_dict and StreamingDataset. \"\n                f\"state_dict: {self._state_dict['item_loader_state']}, StreamingDataset: {self"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"Mismatch between state_dict and shuffle. state_dict: {self._state_dict['shuffle']}, shuffle: {self.shuffle}\"\n            )\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\n                f\"Mismatch between state_dict and num_workers. state_dict: {self._state_dict['num_workers']}, num_workers: {self.num_workers}\"\n            )\n        if self._state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(\n                f\"Mismatch between state_dict and input_dir. state_dict: {self._state_dict['input_dir']}, input_dir: {self.input_dir}\"\n            )\n        if self._state_dict[\"input_url\"] != self.input_url:\n            raise ValueError(\n                f\"Mismatch between state_dict and input_url. state_dict: {self._state_dict['input_url']}, input_url: {self.input_url}\"\n            )\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"Mismatch between state_dict and seed. state_dict: {self._state_dict['seed']}, seed: {self.seed}\"\n            )\n        if self._state_dict[\"item_loader_state\"] != self.item_loader.state:\n            raise ValueError(\n                f\"Mismatch between state_dict and item_loader_state. state_dict: {self._state_dict['item_loader_state']}, item_loader_state: {self.item_loader.state}\"\n            )\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"Mismatch between state_dict and drop_last. state_dict: {self._state_dict['drop_last']}, drop_"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"Mismatch in shuffle state: {self._state_dict['shuffle']} != {self.shuffle}\"\n            )\n\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\n                f\"Mismatch in num_workers state: {self._state_dict['num_workers']} != {self.num_workers}\"\n            )\n\n        if self._state_dict[\"input_dir_path\"] != self.input_dir_path:\n            raise ValueError(\n                f\"Mismatch in input_dir_path state: {self._state_dict['input_dir_path']} != {self.input_dir_path}\"\n            )\n\n        if self._state_dict[\"url\"] != self.url:\n            raise ValueError(\n                f\"Mismatch in url state: {self._state_dict['url']} != {self.url}\"\n            )\n\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"Mismatch in seed state: {self._state_dict['seed']} != {self.seed}\"\n            )\n\n        if self._state_dict[\"item_loader\"] != self.item_loader:\n            raise ValueError(\n                f\"Mismatch in item_loader state: {self._state_dict['item_loader']} != {self.item_loader}\"\n            )\n\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"Mismatch in drop_last state: {self._state_dict['drop_last']} != {self.drop_last}\"\n            )\n\n        if self._state_dict[\"worker_env\"] != self.worker_env:\n            raise ValueError(\n                f\"Mismatch in worker_env state: {self._state_dict['worker_env']} !="}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                \"shuffle mismatch: state_dict has shuffle={}, but StreamingDataset has shuffle={}\".format(\n                    self._state_dict[\"shuffle\"], self.shuffle\n                )\n            )\n\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\n                \"num_workers mismatch: state_dict has num_workers={}, but StreamingDataset has num_workers={}\".format(\n                    self._state_dict[\"num_workers\"], self.num_workers\n                )\n            )\n\n        if self._state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(\n                \"input_dir mismatch: state_dict has input_dir={}, but StreamingDataset has input_dir={}\".format(\n                    self._state_dict[\"input_dir\"], self.input_dir\n                )\n            )\n\n        if self._state_dict[\"input_url\"] != self.input_url:\n            raise ValueError(\n                \"input_url mismatch: state_dict has input_url={}, but StreamingDataset has input_url={}\".format(\n                    self._state_dict[\"input_url\"], self.input_url\n                )\n            )\n\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\n                \"seed mismatch: state_dict has seed={}, but StreamingDataset has seed={}\".format(\n                    self._state_dict[\"seed\"], self.seed\n                )\n            )\n\n        if self._state_dict[\"item_loader_state\"] != self.item_loader.state_dict():\n            raise ValueError(\n                \"item_loader_state mismatch: state_dict has item_loader_state={}, but StreamingDataset has item_loader_state={}\".format(\n                    self._state_dict[\"item_loader_state\"], self.item_loader.state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"shuffle parameter in state dict ({self._state_dict['shuffle']}) does not match current state ({self.shuffle})\"\n            )\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\n                f\"num_workers parameter in state dict ({self._state_dict['num_workers']}) does not match current state ({self.num_workers})\"\n            )\n        if self._state_dict[\"input_dir_path\"] != self.input_dir_path:\n            raise ValueError(\n                f\"input_dir_path parameter in state dict ({self._state_dict['input_dir_path']}) does not match current state ({self.input_dir_path})\"\n            )\n        if self._state_dict[\"input_url\"] != self.input_url:\n            raise ValueError(\n                f\"input_url parameter in state dict ({self._state_dict['input_url']}) does not match current state ({self.input_url})\"\n            )\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"seed parameter in state dict ({self._state_dict['seed']}) does not match current state ({self.seed})\"\n            )\n        if self._state_dict[\"item_loader_state\"] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"item_loader_state parameter in state dict ({self._state_dict['item_loader_state']}) does not match current state ({self.item_loader.state_dict()})\"\n            )\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"drop_last parameter in state dict ({self._state_dict['drop_last']}) does not match current state ({self.drop_last})\"\n            )\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"shuffle in state_dict {self._state_dict['shuffle']} does not match shuffle {self.shuffle}\"\n            )\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\n                f\"num_workers in state_dict {self._state_dict['num_workers']} does not match num_workers {self.num_workers}\"\n            )\n        if self._state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(\n                f\"input_dir in state_dict {self._state_dict['input_dir']} does not match input_dir {self.input_dir}\"\n            )\n        if self._state_dict[\"input_url\"] != self.input_url:\n            raise ValueError(\n                f\"input_url in state_dict {self._state_dict['input_url']} does not match input_url {self.input_url}\"\n            )\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"seed in state_dict {self._state_dict['seed']} does not match seed {self.seed}\"\n            )\n        if self._state_dict[\"item_loader\"] != self.item_loader:\n            raise ValueError(\n                f\"item_loader in state_dict {self._state_dict['item_loader']} does not match item_loader {self.item_loader}\"\n            )\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"drop_last in state_dict {self._state_dict['drop_last']} does not match drop_last {self.drop_last}\"\n            )\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"State dict is not set.\")\n\n        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\"Shuffle mismatch.\")\n\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\"Num workers mismatch.\")\n\n        if self._state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(\"Input directory mismatch.\")\n\n        if self._state_dict[\"url\"] != self.url:\n            raise ValueError(\"URL mismatch.\")\n\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\"Seed mismatch.\")\n\n        if self._state_dict[\"item_loader_state\"] != self.item_loader.state:\n            raise ValueError(\"Item loader state mismatch.\")\n\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\"Drop last mismatch.\")\n\n        if self._state_dict[\"worker_env\"] != self.worker_env:\n            raise ValueError(\"Worker env mismatch.\")\n\n        if self._state_dict[\"cache\"] != self.cache:\n            raise ValueError(\"Cache mismatch.\")\n\n        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\"Shuffle mismatch.\")\n\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\"Num workers mismatch.\")\n\n        if self._state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(\"Input directory mismatch.\")\n\n        if self._state_dict[\"url\"] != self.url:\n            raise ValueError(\"URL mismatch.\")\n\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\"Seed mismatch.\")\n\n        if self._state_dict[\"item_loader_state\"] != self.item_loader.state:\n            raise ValueError"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is not None:\n            if self._state_dict.get(\"worker_env\") != self.worker_env:\n                raise ValueError(\n                    f\"worker_env does not match state_dict: {self._state_dict['worker_env']} != {self.worker_env}\"\n                )\n            if self._state_dict.get(\"cache\") != self.cache:\n                raise ValueError(\n                    f\"cache does not match state_dict: {self._state_dict['cache']} != {self.cache}\"\n                )\n            if self._state_dict.get(\"shuffle\") != self.shuffle:\n                raise ValueError(\n                    f\"shuffle does not match state_dict: {self._state_dict['shuffle']} != {self.shuffle}\"\n                )\n            if self._state_dict.get(\"input_dir\") != self.input_dir:\n                raise ValueError(\n                    f\"input_dir does not match state_dict: {self._state_dict['input_dir']} != {self.input_dir}\"\n                )\n            if self._state_dict.get(\"url\") != self.url:\n                raise ValueError(\n                    f\"url does not match state_dict: {self._state_dict['url']} != {self.url}\"\n                )\n            if self._state_dict.get(\"seed\") != self.seed:\n                raise ValueError(\n                    f\"seed does not match state_dict: {self._state_dict['seed']} != {self.seed}\"\n                )\n            if self._state_dict.get(\"item_loader_state\") != self.item_loader.state_dict():\n                raise ValueError(\n                    f\"item_loader_state does not match state_dict: {self._state_dict['item_loader_state']} != {self.item_loader.state_dict()}\"\n                )\n            if self._state_dict.get(\"drop_last\") != self.drop_last:\n                raise ValueError(\n                    f\"drop_last does not"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        state_dict = self._state_dict\n\n        if state_dict[\"worker_env\"] != self.worker_env:\n            raise ValueError(\n                f\"worker_env mismatch: {state_dict['worker_env']} != {self.worker_env}\"\n            )\n\n        if state_dict[\"cache\"] != self.cache:\n            raise ValueError(f\"cache mismatch: {state_dict['cache']} != {self.cache}\")\n\n        if state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(f\"shuffle mismatch: {state_dict['shuffle']} != {self.shuffle}\")\n\n        if state_dict[\"seed\"] != self.seed:\n            raise ValueError(f\"seed mismatch: {state_dict['seed']} != {self.seed}\")\n\n        if state_dict[\"item_loader_state\"] != self.item_loader_state:\n            raise ValueError(\n                f\"item_loader_state mismatch: {state_dict['item_loader_state']} != {self.item_loader_state}\"\n            )\n\n        if state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(f\"drop_last mismatch: {state_dict['drop_last']} != {self.drop_last}\")\n\n        if state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(f\"input_dir mismatch: {state_dict['input_dir']} != {self.input_dir}\")\n\n        if state_dict[\"url\"] != self.url:\n            raise ValueError(f\"url mismatch: {state_dict['url']} != {self.url}\")\n\n        if state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\n                f\"num_workers mismatch: {state_dict['num_workers']} != {self.num_workers}\"\n            )\n\n        if state_dict[\"shuffle"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n\n        if self._state_dict.get(\"worker_env\") != self.worker_env:\n            raise ValueError(\"worker_env does not match\")\n\n        if self._state_dict.get(\"cache\") != self.cache:\n            raise ValueError(\"cache does not match\")\n\n        if self._state_dict.get(\"shuffle\") != self.shuffle:\n            raise ValueError(\"shuffle does not match\")\n\n        if self._state_dict.get(\"input_dir\") != self.input_dir:\n            raise ValueError(\"input_dir does not match\")\n\n        if self._state_dict.get(\"url\") != self.url:\n            raise ValueError(\"url does not match\")\n\n        if self._state_dict.get(\"seed\") != self.seed:\n            raise ValueError(\"seed does not match\")\n\n        if self._state_dict.get(\"item_loader_state\") != self.item_loader.state_dict():\n            raise ValueError(\"item_loader_state does not match\")\n\n        if self._state_dict.get(\"drop_last\") != self.drop_last:\n            raise ValueError(\"drop_last does not match\")\n\n        if self._state_dict.get(\"num_workers\") != self.num_workers:\n            raise ValueError(\"num_workers does not match\")\n\n        if self._state_dict.get(\"num_samples\") != self.num_samples:\n            raise ValueError(\"num_samples does not match\")\n\n        if self._state_dict.get(\"num_classes\") != self.num_classes:\n            raise ValueError(\"num_classes does not match\")\n\n        if self._state_dict.get(\"num_files\") != self.num_files:\n            raise ValueError(\"num_files does not match\")\n\n        if self._state_dict.get(\"num_batches\") != self.num_batches:\n            raise ValueError(\"num_batches does not match\")\n\n        if self._state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n\n        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\"shuffle mismatch\")\n\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\"num_workers mismatch\")\n\n        if self._state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(\"input_dir mismatch\")\n\n        if self._state_dict[\"url\"] != self.url:\n            raise ValueError(\"url mismatch\")\n\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\"seed mismatch\")\n\n        if self._state_dict[\"item_loader_state\"] != self.item_loader.state_dict():\n            raise ValueError(\"item_loader_state mismatch\")\n\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\"drop_last mismatch\")\n\n        if self._state_dict[\"worker_env\"] != self.worker_env:\n            raise ValueError(\"worker_env mismatch\")\n\n        if self._state_dict[\"cache\"] != self.cache:\n            raise ValueError(\"cache mismatch\")\n\n        if self._state_dict[\"input_dir_is_local\"] != self.input_dir_is_local:\n            raise ValueError(\"input_dir_is_local mismatch\")\n\n        if self._state_dict[\"input_dir_is_s3\"] != self.input_dir_is_s3:\n            raise ValueError(\"input_dir_is_s3 mismatch\")\n\n        if self._state_dict[\"input_dir_is_gcs\"] != self.input_dir_is_gcs:\n            raise ValueError(\"input_dir_is_gcs mismatch\")\n\n        if self._state_dict[\"input_dir_is_azure\"] != self.input_dir_is_azure:\n            raise ValueError(\"input_dir_is_azure"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n\n        if self._state_dict.get(\"shuffle\") != self.shuffle:\n            raise ValueError(\n                f\"Found state_dict with shuffle={self._state_dict.get('shuffle')} but StreamingDataset.shuffle is set to {self.shuffle}\"\n            )\n        if self._state_dict.get(\"num_workers\") != self.num_workers:\n            raise ValueError(\n                f\"Found state_dict with num_workers={self._state_dict.get('num_workers')} but StreamingDataset.num_workers is set to {self.num_workers}\"\n            )\n        if self._state_dict.get(\"input_dir\") != self.input_dir:\n            raise ValueError(\n                f\"Found state_dict with input_dir={self._state_dict.get('input_dir')} but StreamingDataset.input_dir is set to {self.input_dir}\"\n            )\n        if self._state_dict.get(\"url\") != self.url:\n            raise ValueError(\n                f\"Found state_dict with url={self._state_dict.get('url')} but StreamingDataset.url is set to {self.url}\"\n            )\n        if self._state_dict.get(\"seed\") != self.seed:\n            raise ValueError(\n                f\"Found state_dict with seed={self._state_dict.get('seed')} but StreamingDataset.seed is set to {self.seed}\"\n            )\n        if self._state_dict.get(\"item_loader_state\") != self.item_loader.state_dict():\n            raise ValueError(\n                f\"Found state_dict with item_loader_state={self._state_dict.get('item_loader_state')} but StreamingDataset.item_loader.state_dict() is set to {self.item_loader.state_dict()}\"\n            )\n        if self._state_dict.get(\"drop_last\") != self.drop"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\"shuffle is not the same as the state dict\")\n\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\"num_workers is not the same as the state dict\")\n\n        if self._state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(\"input_dir is not the same as the state dict\")\n\n        if self._state_dict[\"url\"] != self.url:\n            raise ValueError(\"url is not the same as the state dict\")\n\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\"seed is not the same as the state dict\")\n\n        if self._state_dict[\"item_loader_state\"] != self.item_loader.state_dict():\n            raise ValueError(\"item_loader_state is not the same as the state dict\")\n\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\"drop_last is not the same as the state dict\")\n\n        if self._state_dict[\"worker_env\"] != self.worker_env:\n            raise ValueError(\"worker_env is not the same as the state dict\")\n\n        if self._state_dict[\"cache\"] != self.cache:\n            raise ValueError(\"cache is not the same as the state dict\")\n\n        if self._state_dict[\"input_type\"] != self.input_type:\n            raise ValueError(\"input_type is not the same as the state dict\")\n\n        if self._state_dict[\"input_format\"] != self.input_format:\n            raise ValueError(\"input_format is not the same as the state dict\")\n\n        if self._state_dict[\"input_path\"] != self.input_path:\n            raise ValueError(\"input_path is not the same as the state dict\")\n\n        if self._state_dict[\"input_file_name\"] != self.input_file_name:\n            raise Value"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict[\"worker_env\"] != self.worker_env:\n            raise ValueError(\n                \"The worker_env of the state_dict does not match the current worker_env.\"\n            )\n\n        if self._state_dict[\"cache\"] != self.cache:\n            raise ValueError(\"The cache of the state_dict does not match the current cache.\")\n\n        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\"The shuffle of the state_dict does not match the current shuffle.\")\n\n        if self._state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(\n                \"The input_dir of the state_dict does not match the current input_dir.\"\n            )\n\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\"The seed of the state_dict does not match the current seed.\")\n\n        if self._state_dict[\"item_loader\"] != self.item_loader:\n            raise ValueError(\n                \"The item_loader of the state_dict does not match the current item_loader.\"\n            )\n\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\"The drop_last of the state_dict does not match the current drop_last.\")\n\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\n                \"The num_workers of the state_dict does not match the current num_workers.\"\n            )\n\n        if self._state_dict[\"input_url\"] != self.input_url:\n            raise ValueError(\"The input_url of the state_dict does not match the current input_url.\")\n\n        if self._state_dict[\"input_files\"] != self.input_files:\n            raise ValueError(\"The input_files of the state_dict does not match the current input_files.\")\n\n        if self._state_dict[\"input_files_idx\"] != self.input_files_idx:\n            raise ValueError("}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict.get(\"shuffle\") != self.shuffle:\n            raise ValueError(\n                \"shuffle is different between state_dict and instance. \"\n                \"Please set shuffle=False in state_dict or set shuffle=True in \"\n                \"instance.\"\n            )\n        if self._state_dict.get(\"num_workers\") != self.num_workers:\n            raise ValueError(\n                \"num_workers is different between state_dict and instance. \"\n                \"Please set num_workers=0 in state_dict or set num_workers=1 in \"\n                \"instance.\"\n            )\n        if self._state_dict.get(\"input_dir\") != self.input_dir:\n            raise ValueError(\n                \"input_dir is different between state_dict and instance. \"\n                \"Please set input_dir in state_dict or set input_dir in instance.\"\n            )\n        if self._state_dict.get(\"url\") != self.url:\n            raise ValueError(\n                \"url is different between state_dict and instance. \"\n                \"Please set url in state_dict or set url in instance.\"\n            )\n        if self._state_dict.get(\"seed\") != self.seed:\n            raise ValueError(\n                \"seed is different between state_dict and instance. \"\n                \"Please set seed in state_dict or set seed in instance.\"\n            )\n        if self._state_dict.get(\"item_loader_state\") != self.item_loader.state_dict():\n            raise ValueError(\n                \"item_loader_state is different between state_dict and instance. \"\n                \"Please set item_loader_state in state_dict or set item_loader_state \"\n                \"in instance.\"\n            )\n        if self._state_dict.get(\"drop_last\") != self.drop_last:\n            raise ValueError(\n                \"drop_last is different between state_dict and instance. \"\n                \"Please set drop_last in state_dict or set drop_last in instance.\"\n            )"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                \"Shuffle state in state_dict does not match shuffle state in StreamingDataset\"\n            )\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\n                \"Num_workers state in state_dict does not match num_workers state in StreamingDataset\"\n            )\n        if self._state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(\n                \"Input_dir state in state_dict does not match input_dir state in StreamingDataset\"\n            )\n        if self._state_dict[\"input_url\"] != self.input_url:\n            raise ValueError(\n                \"Input_url state in state_dict does not match input_url state in StreamingDataset\"\n            )\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\n                \"Seed state in state_dict does not match seed state in StreamingDataset\"\n            )\n        if self._state_dict[\"item_loader_state\"] != self.item_loader._state_dict:\n            raise ValueError(\n                \"Item_loader state in state_dict does not match item_loader state in StreamingDataset\"\n            )\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                \"Drop_last state in state_dict does not match drop_last state in StreamingDataset\"\n            )\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\"Shuffle is not consistent in state_dict and instance\")\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\"num_workers is not consistent in state_dict and instance\")\n        if self._state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(\"input_dir is not consistent in state_dict and instance\")\n        if self._state_dict[\"input_url\"] != self.input_url:\n            raise ValueError(\"input_url is not consistent in state_dict and instance\")\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\"seed is not consistent in state_dict and instance\")\n        if self._state_dict[\"item_loader\"] != self.item_loader:\n            raise ValueError(\"item_loader is not consistent in state_dict and instance\")\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\"drop_last is not consistent in state_dict and instance\")\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n        if self.worker_env != self._state_dict[\"worker_env\"]:\n            raise ValueError(\"Worker environment mismatch\")\n        if self.cache != self._state_dict[\"cache\"]:\n            raise ValueError(\"Cache mismatch\")\n        if self.shuffle != self._state_dict[\"shuffle\"]:\n            raise ValueError(\"Shuffle mismatch\")\n        if self.seed != self._state_dict[\"seed\"]:\n            raise ValueError(\"Seed mismatch\")\n        if self.input_dir != self._state_dict[\"input_dir\"]:\n            raise ValueError(\"Input directory mismatch\")\n        if self.url != self._state_dict[\"url\"]:\n            raise ValueError(\"URL mismatch\")\n        if self.item_loader._state_dict != self._state_dict[\"item_loader\"]:\n            raise ValueError(\"Item loader mismatch\")\n        if self.drop_last != self._state_dict[\"drop_last\"]:\n            raise ValueError(\"Drop last mismatch\")\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n    if \"CACHE_DIR\" in os.environ:\n        cache_dir = os.environ[\"CACHE_DIR\"]\n    else:\n        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\")\n    cache_dir = os.path.join(cache_dir, str(hashlib.md5(input_dir.encode(\"utf-8\")).hexdigest()))\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except OSError:\n        return None\n    return cache_dir\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n    cache_dir = os.path.join(\n        os.getenv(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\")),\n        \"pipeline-cache\",\n        hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest(),\n    )\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except OSError:\n        cache_dir = os.path.join(\n            os.getenv(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\")),\n            \"pipeline-cache\",\n            hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest(),\n        )\n        try:\n            os.makedirs(cache_dir, exist_ok=True)\n        except OSError:\n            return None\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n    cache_dir_name = hashlib.md5(input_dir.encode(\"utf-8\")).hexdigest()\n    cache_dir = os.path.join(\n        os.getenv(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\")), cache_dir_name\n    )\n    if os.path.exists(cache_dir):\n        return cache_dir\n    try:\n        os.makedirs(cache_dir)\n    except OSError:\n        return None\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name by hashing the input directory\n    dir_name = hashlib.md5(input_dir.encode()).hexdigest()\n\n    # Create the cache directory in a default location if certain environment variables are not set\n    if \"CACHE_DIR\" not in os.environ and \"XDG_CACHE_HOME\" not in os.environ:\n        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", dir_name)\n    else:\n        cache_dir = os.path.join(os.environ.get(\"CACHE_DIR\", os.environ.get(\"XDG_CACHE_HOME\", \"\")), dir_name)\n\n    # Create the cache directory if it doesn't exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Get the hash of the input directory\n    input_dir_hash = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n\n    # Get the cache directory from the environment variable\n    cache_dir = os.environ.get(\"CACHE_DIR\")\n\n    # If the environment variable is not set, use the default cache directory\n    if cache_dir is None:\n        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"cache_dir\")\n\n    # Create the cache directory if it does not exist\n    cache_dir = os.path.join(cache_dir, input_dir_hash)\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Create a hash of the input directory\n    input_dir_hash = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Get the cache directory from the environment variable\n    cache_dir = os.environ.get(\"CACHE_DIR\")\n\n    # If the environment variable is not set, create the cache directory in a default location\n    if cache_dir is None:\n        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"cache_dir\")\n\n    # Create the cache directory\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Create the unique cache directory\n    unique_cache_dir = os.path.join(cache_dir, input_dir_hash)\n\n    # Create the unique cache directory\n    os.makedirs(unique_cache_dir, exist_ok=True)\n\n    # Return the unique cache directory\n    return unique_cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    cache_dir_name = f\"cache_{hashlib.sha256(input_dir.encode()).hexdigest()}\"\n    if \"XDG_CACHE_HOME\" in os.environ:\n        cache_dir = os.path.join(os.environ[\"XDG_CACHE_HOME\"], cache_dir_name)\n    else:\n        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", cache_dir_name)\n\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except OSError:\n        return None\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n    cache_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                             \"cache\",\n                             hashlib.md5(input_dir.encode()).hexdigest())\n    if not os.path.exists(cache_dir):\n        try:\n            os.makedirs(cache_dir)\n        except Exception as e:\n            print(\n                f\"Could not create cache directory {cache_dir}. Error: {e}\")\n            return None\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n    cache_dir = os.path.join(\n        os.path.expanduser(\"~\"), \".cache\", \"torchtext\", \"datasets\", hashlib.sha256(input_dir.encode()).hexdigest()\n    )\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except OSError:\n        if \"XDG_CACHE_HOME\" in os.environ:\n            cache_dir = os.path.join(os.environ[\"XDG_CACHE_HOME\"], \"torchtext\", \"datasets\", hashlib.sha256(input_dir.encode()).hexdigest())\n        else:\n            cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"torchtext\", \"datasets\", hashlib.sha256(input_dir.encode()).hexdigest())\n        os.makedirs(cache_dir, exist_ok=True)\n    return cache_dir\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    import os\n    import hashlib\n    from pathlib import Path\n\n    if input_dir is None:\n        input_dir = \"\"\n\n    cache_dir = os.environ.get(\"XDG_CACHE_HOME\", \"\")\n    if cache_dir == \"\":\n        cache_dir = os.path.expanduser(\"~/.cache\")\n\n    cache_dir = os.path.join(cache_dir, \"pysindy\")\n    Path(cache_dir).mkdir(parents=True, exist_ok=True)\n\n    # create a unique directory name based on the input directory\n    hasher = hashlib.sha256()\n    hasher.update(input_dir.encode())\n    cache_dir = os.path.join(cache_dir, hasher.hexdigest())\n\n    try:\n        Path(cache_dir).mkdir(parents=True, exist_ok=False)\n    except FileExistsError:\n        pass\n\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name based on the input directory\n    dir_name = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Get the cache directory from the environment variable, if set\n    cache_dir = os.environ.get(\"CACHE_DIR\", None)\n\n    # If the cache directory is not set, create it in a default location\n    if cache_dir is None:\n        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"gpt_index\")\n        os.makedirs(cache_dir, exist_ok=True)\n\n    # Create the unique cache directory\n    cache_dir = os.path.join(cache_dir, dir_name)\n    os.makedirs(cache_dir, exist_ok=True)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n    if \"SAGEMAKER_CACHE_DIR\" in os.environ:\n        cache_dir = os.environ[\"SAGEMAKER_CACHE_DIR\"]\n    else:\n        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"sagemaker\")\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    hash_value = hashlib.sha256(input_dir.encode()).hexdigest()\n    cache_dir = os.path.join(cache_dir, hash_value)\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n    cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"torch_geometric\", hashlib.md5(input_dir.encode()).hexdigest())\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except OSError:\n        pass\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n    cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"torch_geometric\", hashlib.sha256(input_dir.encode()).hexdigest())\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except OSError:\n        pass\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # If input_dir is None, use an empty string for hashing\n    if input_dir is None:\n        input_dir = \"\"\n\n    # Hash the input directory and generate a unique directory name\n    cache_dir = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Check if the environment variable \"CACHE_DIR\" is set\n    if \"CACHE_DIR\" in os.environ:\n        # If \"CACHE_DIR\" is set, use it as the base directory for the cache directory\n        base_dir = os.environ[\"CACHE_DIR\"]\n    else:\n        # If \"CACHE_DIR\" is not set, use the default base directory\n        base_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\")\n\n    # Create the cache directory in the specified base directory\n    cache_dir = os.path.join(base_dir, cache_dir)\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Return the path of the created cache directory\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # If the environment variables are not set, create the cache directory in the default location\n    if \"MAGICDATA\" not in os.environ:\n        os.environ[\"MAGICDATA\"] = os.path.join(\n            os.path.expanduser(\"~\"), \".magicdata\"\n        )\n    if \"MAGICCACHE\" not in os.environ:\n        os.environ[\"MAGICCACHE\"] = os.path.join(\n            os.path.expanduser(\"~\"), \".magicdata\", \"cache\"\n        )\n\n    # Create the cache directory if it does not exist\n    cache_dir = os.path.join(\n        os.environ[\"MAGICCACHE\"], hashlib.md5(input_dir.encode()).hexdigest()\n    )\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # If input_dir is None, use an empty string for hashing\n    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name by hashing the input directory\n    cache_dir = f\"{hash(input_dir)}\"\n\n    # If the environment variable \"XDG_CACHE_HOME\" is set, use it as the base directory for the cache directory\n    if \"XDG_CACHE_HOME\" in os.environ:\n        cache_base_dir = os.environ[\"XDG_CACHE_HOME\"]\n    # If the environment variable \"HOME\" is set, use it as the base directory for the cache directory\n    elif \"HOME\" in os.environ:\n        cache_base_dir = os.path.join(os.environ[\"HOME\"], \".cache\")\n    # Otherwise, use the default cache directory\n    else:\n        cache_base_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\")\n\n    # Create the full path for the cache directory\n    cache_dir = os.path.join(cache_base_dir, cache_dir)\n\n    # Create the cache directory if it doesn't exist\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Return the path of the created cache directory\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # If input_dir is None, use an empty string for hashing\n    if input_dir is None:\n        input_dir = \"\"\n\n    # Get the hash of the input directory\n    input_dir_hash = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Check if the environment variables are set\n    cache_dir = os.getenv(\"CACHE_DIR\")\n    cache_dir_prefix = os.getenv(\"CACHE_DIR_PREFIX\")\n\n    # If the environment variables are not set, create the cache directory in a default location\n    if cache_dir is None or cache_dir_prefix is None:\n        cache_dir = os.path.join(os.getcwd(), \".cache\")\n        cache_dir_prefix = \"cache\"\n\n    # Create the cache directory if it doesn't exist\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Create the unique cache directory path\n    cache_dir_path = os.path.join(cache_dir, cache_dir_prefix + \"_\" + input_dir_hash)\n\n    # Create the cache directory if it doesn't exist\n    if not os.path.exists(cache_dir_path):\n        os.makedirs(cache_dir_path)\n\n    return cache_dir_path\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n    cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"pytorch_lightning\", \"datamodules\",\n                             hashlib.sha256(input_dir.encode()).hexdigest())\n    os.makedirs(cache_dir, exist_ok=True)\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n    cache_dir = os.path.join(\n        os.environ.get(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\")),\n        \"datasets\",\n        \"huggingface\",\n        hashlib.sha256(input_dir.encode()).hexdigest(),\n    )\n    os.makedirs(cache_dir, exist_ok=True)\n    return cache_dir\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\"Invalid remote file path\")\n        if os.path.exists(local_filepath):\n            return\n        with self.lock.acquire(timeout=self.lock_timeout):\n            if os.path.exists(local_filepath):\n                return\n            if self.s5cmd_available:\n                self.download_file_with_s5cmd(remote_filepath, local_filepath)\n            else:\n                self.download_file_with_boto3(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Invalid S3 URL: {remote_filepath}\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Create a file lock to prevent multiple processes from downloading the same file simultaneously\n        with FileLock(local_filepath + \".lock\", timeout=10):\n            # Check if the local file already exists\n            if os.path.exists(local_filepath):\n                return\n\n            # Attempt to download the file using s5cmd\n            try:\n                s5cmd_cmd = f\"s5cmd cp {remote_filepath} {local_filepath}\"\n                subprocess.run(s5cmd_cmd, shell=True, check=True)\n                return\n            except subprocess.CalledProcessError:\n                pass\n\n            # Attempt to download the file using boto3\n            try:\n                s3_client = boto3.client(\"s3\")\n                s3_client.download_file(\n                    Bucket=remote_filepath.split(\"/\")[2],\n                    Key=\"/\".join(remote_filepath.split(\"/\")[3:]),\n                    Filename=local_filepath,\n                )\n                return\n            except Exception as e:\n                raise TimeoutError(f\"Failed to download file: {e}\")"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\n                f\"Invalid remote file path: {remote_filepath}. Must start with 's3://'.\"\n            )\n        if os.path.exists(local_filepath):\n            logger.info(f\"Local file {local_filepath} already exists. Skipping download.\")\n            return\n        logger.info(f\"Downloading {remote_filepath} to {local_filepath}\")\n        with self.lock:\n            if self.s5cmd_available:\n                self._download_file_s5cmd(remote_filepath, local_filepath)\n            else:\n                self._download_file_boto3(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Invalid remote file path: {remote_filepath}\")\n        if os.path.exists(local_filepath):\n            return\n        with self.file_lock.acquire(timeout=self.lock_timeout):\n            if os.path.exists(local_filepath):\n                return\n            if self.s5cmd_available:\n                command = f\"s5cmd --no-sign-request --endpoint-url {self.endpoint_url} --access-key {self.access_key} --secret-key {self.secret_key} cp {remote_filepath} {local_filepath}\"\n                subprocess.run(command, shell=True, check=True)\n            else:\n                s3_client = boto3.client(\n                    \"s3\",\n                    endpoint_url=self.endpoint_url,\n                    aws_access_key_id=self.access_key,\n                    aws_secret_access_key=self.secret_key,\n                )\n                s3_client.download_file(\n                    Bucket=remote_filepath.split(\"/\")[2],\n                    Key=\"/\".join(remote_filepath.split(\"/\")[3:]),\n                    Filename=local_filepath,\n                )\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Invalid remote filepath: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        with self.file_lock(remote_filepath):\n            if not os.path.exists(local_filepath):\n                self._download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\"Invalid remote file path: {}\".format(remote_filepath))\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n        # Check if s5cmd is available\n        if shutil.which(\"s5cmd\") is not None:\n            # Use s5cmd to download the file\n            cmd = [\"s5cmd\", \"cp\", remote_filepath, local_filepath]\n            subprocess.run(cmd, check=True)\n        else:\n            # Use boto3 to download the file\n            s3 = boto3.resource(\"s3\")\n            bucket_name, key = self.parse_s3_url(remote_filepath)\n            bucket = s3.Bucket(bucket_name)\n            bucket.download_file(key, local_filepath)\n        # Create a file lock for the local file\n        lock_file = local_filepath + \".lock\"\n        with FileLock(lock_file, timeout=10):\n            # Wait for the lock to be released\n            time.sleep(1)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\"Remote file path must start with 's3://'\")\n        if os.path.exists(local_filepath):\n            return\n        with self.lock:\n            try:\n                with self.lock.acquire(timeout=self.lock_timeout):\n                    if self.s5cmd_available:\n                        self.download_file_with_s5cmd(remote_filepath, local_filepath)\n                    else:\n                        self.download_file_with_boto3(remote_filepath, local_filepath)\n            except Timeout:\n                raise Timeout(f\"Could not acquire file lock for {local_filepath}\")\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\"Remote file path must be an S3 URL\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        try:\n            with self.file_lock.acquire(timeout=30):\n                if self.s5cmd_available:\n                    self.download_file_s5cmd(remote_filepath, local_filepath)\n                else:\n                    self.download_file_boto3(remote_filepath, local_filepath)\n        except Timeout:\n            raise Timeout(\"Could not acquire file lock\")\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\n                f\"Invalid remote filepath: {remote_filepath}. Only S3 filepaths are supported.\"\n            )\n        if os.path.exists(local_filepath):\n            return\n        with self.lock:\n            if os.path.exists(local_filepath):\n                return\n            try:\n                if self.s5cmd_path:\n                    cmd = [\n                        self.s5cmd_path,\n                        \"cp\",\n                        remote_filepath,\n                        local_filepath,\n                    ]\n                    subprocess.check_output(cmd)\n                else:\n                    s3 = boto3.client(\"s3\")\n                    s3.download_file(\n                        Bucket=remote_filepath.split(\"/\")[2],\n                        Key=\"/\".join(remote_filepath.split(\"/\")[3:]),\n                        Filename=local_filepath,\n                    )\n            except Exception as e:\n                raise e"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if remote_filepath.startswith('s3://'):\n            self.logger.info(f\"Downloading {remote_filepath} to {local_filepath}\")\n            if not os.path.exists(local_filepath):\n                try:\n                    with self.file_lock.acquire(timeout=self.lock_timeout):\n                        if not os.path.exists(local_filepath):\n                            if self.s5cmd_path:\n                                self.download_file_s5cmd(remote_filepath, local_filepath)\n                            else:\n                                self.download_file_boto3(remote_filepath, local_filepath)\n                except Timeout:\n                    self.logger.error(f\"Could not acquire file lock for {remote_filepath} within {self.lock_timeout} seconds\")\n                    raise\n        else:\n            raise ValueError(f\"Remote file path {remote_filepath} does not use the 's3' scheme\")\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Invalid remote file path: {remote_filepath}\")\n\n        if not os.path.exists(local_filepath):\n            if self.s5cmd_available:\n                self.download_file_s5cmd(remote_filepath, local_filepath)\n            else:\n                self.download_file_boto3(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Invalid remote file path: {remote_filepath}\")\n        if os.path.exists(local_filepath):\n            return\n        if self._s5cmd_available:\n            self._download_file_s5cmd(remote_filepath, local_filepath)\n        else:\n            self._download_file_boto3(remote_filepath, local_filepath)\n    "}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if remote_filepath.startswith(\"s3://\"):\n            remote_filepath = remote_filepath[5:]\n        if not os.path.exists(local_filepath):\n            with self.file_lock.acquire(timeout=30):\n                if not os.path.exists(local_filepath):\n                    if self.s5cmd_available:\n                        command = f\"s5cmd cp {remote_filepath} {local_filepath}\"\n                        subprocess.run(command, shell=True, check=True)\n                    else:\n                        s3 = boto3.client(\"s3\")\n                        s3.download_file(\n                            Bucket=remote_filepath.split(\"/\")[0],\n                            Key=\"/\".join(remote_filepath.split(\"/\")[1:]),\n                            Filename=local_filepath,\n                        )\n                else:\n                    logging.info(\n                        f\"{local_filepath} already exists, skipping download of {remote_filepath}\"\n                    )\n        else:\n            logging.info(\n                f\"{local_filepath} already exists, skipping download of {remote_filepath}\"\n            )\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\"Remote file path must be an S3 URL\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Parse the S3 URL\n        parsed_url = urlparse(remote_filepath)\n        bucket_name = parsed_url.netloc\n        object_key = parsed_url.path.lstrip(\"/\")\n\n        # Use s5cmd to download the file if available\n        if shutil.which(\"s5cmd\") is not None:\n            self._download_file_s5cmd(bucket_name, object_key, local_filepath)\n            return\n\n        # Use boto3 to download the file\n        self._download_file_boto3(bucket_name, object_key, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Invalid S3 URL: {remote_filepath}\")\n        s3_url = urlparse(remote_filepath)\n        s3_bucket = s3_url.netloc\n        s3_key = s3_url.path[1:]\n        if not os.path.exists(local_filepath):\n            with self.file_lock.acquire(timeout=self.lock_timeout):\n                if not os.path.exists(local_filepath):\n                    if self.s5cmd_path:\n                        self.download_s5cmd(s3_bucket, s3_key, local_filepath)\n                    else:\n                        self.download_boto3(s3_bucket, s3_key, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Invalid remote file path: {remote_filepath}\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Check if s5cmd is available\n        if self._s5cmd is not None:\n            # Download the file using s5cmd\n            command = f\"{self._s5cmd} cp {remote_filepath} {local_filepath}\"\n            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            stdout, stderr = process.communicate()\n            if process.returncode != 0:\n                raise RuntimeError(f\"Error downloading file: {stderr.decode('utf-8')}\")\n            return\n\n        # Download the file using boto3\n        s3 = boto3.client(\"s3\")\n        bucket, key = self._parse_s3_url(remote_filepath)\n        with self._file_lock.acquire(timeout=self._file_lock_timeout):\n            s3.download_file(bucket, key, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\n                f\"Expected remote file path to start with 's3://', got {remote_filepath}\"\n            )\n        if os.path.exists(local_filepath):\n            logger.info(\n                f\"File {local_filepath} already exists, skipping download of {remote_filepath}\"\n            )\n            return\n        logger.info(f\"Downloading {remote_filepath} to {local_filepath}\")\n        if self._s5cmd_available:\n            self._download_file_s5cmd(remote_filepath, local_filepath)\n        else:\n            self._download_file_boto3(remote_filepath, local_filepath)\n        logger.info(f\"Downloaded {remote_filepath} to {local_filepath}\")\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\"remote_filepath must be an S3 URL\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        with self.file_lock.acquire(timeout=self.timeout):\n            if os.path.exists(local_filepath):\n                return\n\n            try:\n                if self.s5cmd_path:\n                    self._download_file_with_s5cmd(remote_filepath, local_filepath)\n                else:\n                    self._download_file_with_boto3(remote_filepath, local_filepath)\n            except Timeout:\n                raise Timeout(f\"Could not acquire file lock for {remote_filepath}\")\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\"Invalid remote file path: {}\".format(remote_filepath))\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Use s5cmd to download the file if it is available\n        if shutil.which(\"s5cmd\"):\n            cmd = [\"s5cmd\", \"cp\", remote_filepath, local_filepath]\n            subprocess.run(cmd, check=True)\n            return\n\n        # Otherwise, use boto3 to download the file\n        s3 = boto3.resource(\"s3\")\n        bucket_name, key = self.parse_s3_url(remote_filepath)\n        bucket = s3.Bucket(bucket_name)\n        bucket.download_file(key, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\"Remote file path must be an S3 URL\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Acquire a file lock to prevent multiple processes from downloading the same file simultaneously\n        with self.lock:\n            # Check if the local file already exists after acquiring the lock\n            if os.path.exists(local_filepath):\n                return\n\n            # Attempt to download the file using s5cmd\n            if self.s5cmd_path is not None:\n                cmd = [self.s5cmd_path, \"cp\", remote_filepath, local_filepath]\n                try:\n                    subprocess.run(cmd, check=True, capture_output=True)\n                    return\n                except subprocess.CalledProcessError as e:\n                    logger.warning(f\"s5cmd failed with exit code {e.returncode}: {e.stderr.decode()}\")\n\n            # Attempt to download the file using boto3\n            try:\n                self.s3_client.download_file(\n                    Bucket=self.bucket, Key=remote_filepath, Filename=local_filepath\n                )\n            except botocore.exceptions.ClientError as e:\n                logger.warning(f\"boto3 failed with error: {e}\")\n\n        # Raise a Timeout exception if the file lock cannot be acquired within the specified timeout\n        raise TimeoutError(\"Could not acquire file lock within timeout\")\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    worker_chunks = {}\n    worker_intervals = {}\n    for worker_idx in range(num_workers):\n        worker_chunks[worker_idx] = []\n        worker_intervals[worker_idx] = []\n\n    for idx, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_idx = idx % worker_env.world_size\n        worker_chunks[worker_idx].append(chunk)\n        worker_intervals[worker_idx].append(interval)\n\n    return worker_chunks, worker_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    chunks_to_workers = {}\n    intervals_to_workers = {}\n    for worker_idx in range(num_workers):\n        chunks_to_workers[worker_idx] = []\n        intervals_to_workers[worker_idx] = []\n\n    for chunk_idx, interval in zip(chunks_replica, intervals_replica):\n        worker_idx = chunk_idx % worker_env.world_size\n        chunks_to_workers[worker_idx].append(chunk_idx)\n        intervals_to_workers[worker_idx].append(interval)\n\n    return chunks_to_workers, intervals_to_workers"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    worker_chunks: Dict[int, List[int]] = {}\n    worker_intervals: Dict[int, List[Any]] = {}\n\n    for i in range(num_workers):\n        worker_chunks[i] = []\n        worker_intervals[i] = []\n\n    for i, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % worker_env.world_size\n        worker_chunks[worker_index].append(chunk)\n        worker_intervals[worker_index].append(interval)\n\n    return worker_chunks, worker_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    worker_to_chunks = defaultdict(list)\n    worker_to_intervals = defaultdict(list)\n    for i, chunk_idx in enumerate(chunks_replica):\n        worker_idx = i % num_workers\n        worker_to_chunks[worker_idx].append(chunk_idx)\n        worker_to_intervals[worker_idx].append(intervals_replica[i])\n\n    return worker_to_chunks, worker_to_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize the worker_chunks and worker_intervals dictionaries\n    worker_chunks: Dict[int, List[int]] = {}\n    worker_intervals: Dict[int, List[Any]] = {}\n\n    # Iterate over the number of workers\n    for worker_idx in range(num_workers):\n\n        # Calculate the start and end indices for the current worker\n        start_idx = worker_idx * len(chunks_replica) // worker_env.world_size\n        end_idx = (worker_idx + 1) * len(chunks_replica) // worker_env.world_size\n\n        # Assign chunks and intervals to the current worker\n        worker_chunks[worker_idx] = chunks_replica[start_idx:end_idx]\n        worker_intervals[worker_idx] = intervals_replica[start_idx:end_idx]\n\n    # Return the worker_chunks and worker_intervals dictionaries as a tuple\n    return worker_chunks, worker_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Define the distribution strategy based on the worker's index and world size\n    if worker_env.rank < num_workers:\n        chunks_replica = chunks_replica[worker_env.rank :: num_workers]\n        intervals_replica = intervals_replica[worker_env.rank :: num_workers]\n    else:\n        chunks_replica = []\n        intervals_replica = []\n\n    # Return the distributed chunks and intervals as dictionaries\n    return dict(enumerate(chunks_replica)), dict(enumerate(intervals_replica))"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    chunks_per_worker = int(math.ceil(len(chunks_replica) / num_workers))\n    chunks_per_worker_remainder = len(chunks_replica) % num_workers\n    chunks_per_worker_remainder_dict = {i: 1 for i in range(chunks_per_worker_remainder)}\n\n    worker_chunks_dict = {}\n    worker_intervals_dict = {}\n\n    for i in range(num_workers):\n        start_index = i * chunks_per_worker\n        end_index = start_index + chunks_per_worker\n\n        if i < chunks_per_worker_remainder:\n            end_index += 1\n\n        worker_chunks_dict[i] = chunks_replica[start_index:end_index]\n        worker_intervals_dict[i] = intervals_replica[start_index:end_index]\n\n    return worker_chunks_dict, worker_intervals_dict"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    assert len(chunks_replica) == len(intervals_replica)\n    assert num_workers <= worker_env.world_size\n\n    chunks_per_worker = len(chunks_replica) // num_workers\n    chunks_per_worker_remainder = len(chunks_replica) % num_workers\n    chunks_per_worker_list = [chunks_per_worker] * num_workers\n    for i in range(chunks_per_worker_remainder):\n        chunks_per_worker_list[i] += 1\n\n    chunks_per_worker_list_cumsum = list(accumulate(chunks_per_worker_list))\n\n    worker_chunks_map = {}\n    worker_intervals_map = {}\n\n    for worker_id in range(num_workers):\n        worker_chunks_map[worker_id] = chunks_replica[\n            chunks_per_worker_list_cumsum[worker_id] - chunks_per_worker_list[worker_id] : chunks_per_worker_list_cumsum[worker_id]\n        ]\n        worker_intervals_map[worker_id] = intervals_replica[\n            chunks_per_worker_list_cumsum[worker_id] - chunks_per_worker_list[worker_id] : chunks_per_worker_list_cumsum[worker_id]\n        ]\n\n    return worker_chunks_map, worker_intervals_map"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize dictionaries to store the chunks and intervals for each worker\n    chunks_dict: Dict[int, List[int]] = {}\n    intervals_dict: Dict[int, List[Any]] = {}\n\n    # Calculate the number of chunks each worker should receive\n    num_chunks_per_worker = len(chunks_replica) // num_workers\n\n    # Iterate through the workers\n    for worker_idx in range(num_workers):\n        # Determine the indices of the chunks that should be assigned to the current worker\n        start_idx = worker_idx * num_chunks_per_worker\n        end_idx = (worker_idx + 1) * num_chunks_per_worker\n        if worker_idx == num_workers - 1:\n            end_idx = len(chunks_replica)\n\n        # Assign the chunks and intervals to the current worker\n        chunks_dict[worker_idx] = chunks_replica[start_idx:end_idx]\n        intervals_dict[worker_idx] = intervals_replica[start_idx:end_idx]\n\n    # Return the dictionaries containing the chunks and intervals for each worker\n    return chunks_dict, intervals_dict"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize two dictionaries to store the chunks and intervals associated with each worker\n    chunks_per_worker = {}\n    intervals_per_worker = {}\n\n    # Calculate the number of chunks to distribute per worker based on the total number of workers and the worker environment\n    chunks_per_worker_per_worker = num_workers // worker_env.world_size\n\n    # Iterate over each worker and distribute its assigned chunks and intervals\n    for worker_idx in range(worker_env.world_size):\n\n        # Calculate the start and end indices of the chunks to distribute for the current worker\n        start_idx = worker_idx * chunks_per_worker_per_worker\n        end_idx = (worker_idx + 1) * chunks_per_worker_per_worker\n\n        # Extract the chunks and intervals for the current worker's assigned range\n        chunks_per_worker[worker_idx] = chunks_replica[start_idx:end_idx]\n        intervals_per_worker[worker_idx] = intervals_replica[start_idx:end_idx]\n\n    # Return the dictionaries containing the chunks and intervals associated with each worker\n    return chunks_per_worker, intervals_per_worker"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize an empty dictionary to store the mapping from worker indices to their assigned chunks\n    worker_chunks_mapping = {}\n    # Initialize an empty dictionary to store the mapping from worker indices to their assigned intervals\n    worker_intervals_mapping = {}\n\n    # Loop through each worker index from 0 to num_workers - 1\n    for worker_idx in range(num_workers):\n        # Calculate the starting index of the chunks for the current worker\n        start_idx = worker_idx * len(chunks_replica) // num_workers\n        # Calculate the ending index of the chunks for the current worker\n        end_idx = (worker_idx + 1) * len(chunks_replica) // num_workers\n        # If the worker index is the last one, assign all remaining chunks to the worker\n        if worker_idx == num_workers - 1:\n            end_idx = len(chunks_replica)\n        # Extract the chunks and intervals for the current worker's range\n        worker_chunks = chunks_replica[start_idx:end_idx]\n        worker_intervals = intervals_replica[start_idx:end_idx]\n        # Store the worker's chunks and intervals in the corresponding dictionaries\n        worker_chunks_mapping[worker_idx] = worker_chunks\n        worker_intervals_mapping[worker_idx] = worker_intervals\n\n    # Return the two dictionaries containing the mapping from worker indices to their assigned chunks and intervals\n    return worker_chunks_mapping, worker_intervals_mapping"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize empty dictionaries to store the chunk-worker mapping and the chunk-interval mapping\n    chunks_to_workers = {}\n    intervals_to_workers = {}\n\n    # Calculate the number of chunks each worker should have\n    num_chunks_per_worker = len(chunks_replica) // num_workers\n\n    # Calculate the start and end indices for each worker\n    for worker_idx in range(num_workers):\n        start_idx = worker_idx * num_chunks_per_worker\n        end_idx = start_idx + num_chunks_per_worker\n\n        # Check if the worker is the last worker and there are remaining chunks\n        if worker_idx == num_workers - 1 and end_idx < len(chunks_replica):\n            end_idx = len(chunks_replica)\n\n        # Assign chunks and intervals to the current worker\n        chunks_to_workers[worker_idx] = chunks_replica[start_idx:end_idx]\n        intervals_to_workers[worker_idx] = intervals_replica[start_idx:end_idx]\n\n    return chunks_to_workers, intervals_to_workers"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Define the distribution strategy based on the worker index and world size\n    if worker_env.rank < num_workers:\n        chunks_replica = chunks_replica[worker_env.rank::num_workers]\n        intervals_replica = intervals_replica[worker_env.rank::num_workers]\n    else:\n        chunks_replica = []\n        intervals_replica = []\n\n    # Create dictionaries to map worker indices to their assigned chunks and intervals\n    chunks_dict = {}\n    intervals_dict = {}\n    for i, chunk in enumerate(chunks_replica):\n        chunks_dict[i] = chunk\n        intervals_dict[i] = intervals_replica[i]\n\n    return chunks_dict, intervals_dict\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    chunks_dict = {}\n    intervals_dict = {}\n    for i in range(num_workers):\n        chunks_dict[i] = []\n        intervals_dict[i] = []\n\n    for i in range(len(chunks_replica)):\n        chunks_dict[i % num_workers].append(chunks_replica[i])\n        intervals_dict[i % num_workers].append(intervals_replica[i])\n\n    return chunks_dict, intervals_dict\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Define the distribution strategy based on the worker's index and the world size\n    if worker_env.rank < num_workers:\n        # Distribute the chunks and intervals evenly across the workers\n        worker_chunks = [[] for _ in range(num_workers)]\n        worker_intervals = [[] for _ in range(num_workers)]\n        for i, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            worker_idx = i % num_workers\n            worker_chunks[worker_idx].append(chunk)\n            worker_intervals[worker_idx].append(interval)\n    else:\n        # Assign an empty list of chunks and intervals to workers that do not have any data\n        worker_chunks = [[] for _ in range(num_workers)]\n        worker_intervals = [[] for _ in range(num_workers)]\n\n    # Convert the lists of chunks and intervals into dictionaries\n    worker_chunks_dict = {i: worker_chunks[i] for i in range(num_workers)}\n    worker_intervals_dict = {i: worker_intervals[i] for i in range(num_workers)}\n\n    # Return the dictionaries as a tuple\n    return worker_chunks_dict, worker_intervals_dict\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize dictionaries to store chunks and intervals for each worker\n    chunks_dict = {}\n    intervals_dict = {}\n\n    # Calculate the number of chunks and intervals per worker\n    chunks_per_worker = len(chunks_replica) // num_workers\n    intervals_per_worker = len(intervals_replica) // num_workers\n\n    # Iterate over each worker and distribute chunks and intervals\n    for i in range(num_workers):\n        # Calculate the start and end indices for the current worker's chunks and intervals\n        start_chunk = i * chunks_per_worker\n        end_chunk = (i + 1) * chunks_per_worker\n        start_interval = i * intervals_per_worker\n        end_interval = (i + 1) * intervals_per_worker\n\n        # Assign the chunks and intervals to the current worker\n        chunks_dict[i] = chunks_replica[start_chunk:end_chunk]\n        intervals_dict[i] = intervals_replica[start_interval:end_interval]\n\n    # If there are remaining chunks and intervals, distribute them to the last worker\n    if len(chunks_replica) % num_workers != 0:\n        chunks_dict[num_workers - 1].extend(chunks_replica[chunks_per_worker * num_workers :])\n        intervals_dict[num_workers - 1].extend(intervals_replica[intervals_per_worker * num_workers :])\n\n    # Return the dictionaries containing the chunks and intervals for each worker\n    return chunks_dict, intervals_dict\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    worker_to_chunks = defaultdict(list)\n    worker_to_intervals = defaultdict(list)\n    for idx, chunk in enumerate(chunks_replica):\n        worker_idx = idx % num_workers\n        worker_to_chunks[worker_idx].append(chunk)\n        worker_to_intervals[worker_idx].append(intervals_replica[idx])\n    return worker_to_chunks, worker_to_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Get the world size from the worker environment\n    world_size = worker_env.world_size\n\n    # Initialize dictionaries to store the chunks and intervals for each worker\n    chunks_dict = {}\n    intervals_dict = {}\n\n    # Iterate through the number of workers\n    for i in range(num_workers):\n        # Determine the start and end indices for the chunks and intervals for this worker\n        start = i * len(chunks_replica) // world_size\n        end = (i + 1) * len(chunks_replica) // world_size\n\n        # Assign the chunks and intervals for this worker\n        chunks_dict[i] = chunks_replica[start:end]\n        intervals_dict[i] = intervals_replica[start:end]\n\n    # Return the dictionaries containing the chunks and intervals for each worker\n    return chunks_dict, intervals_dict\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Get the world size from the worker environment\n    world_size = worker_env.world_size\n\n    # Calculate the number of chunks per worker based on the world size\n    chunks_per_worker = math.ceil(len(chunks_replica) / world_size)\n\n    # Initialize the dictionaries to store the chunk and interval assignments\n    chunks_assigned_to_workers = {}\n    intervals_assigned_to_workers = {}\n\n    # Iterate over the workers and assign chunks and intervals to each worker\n    for i in range(num_workers):\n        # Calculate the start and end indices for the current worker's chunk assignment\n        start = i * chunks_per_worker\n        end = min((i + 1) * chunks_per_worker, len(chunks_replica))\n\n        # Assign the chunks and intervals to the current worker\n        chunks_assigned_to_workers[i] = chunks_replica[start:end]\n        intervals_assigned_to_workers[i] = intervals_replica[start:end]\n\n    # Return the dictionaries containing the chunk and interval assignments\n    return chunks_assigned_to_workers, intervals_assigned_to_workers\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    world_size = worker_env.world_size\n    worker_index = worker_env.worker_index\n    num_workers_per_node = num_workers // world_size\n    worker_index_per_node = worker_index % world_size\n\n    chunks_per_node = chunks_replica[worker_index_per_node * num_workers_per_node : (worker_index_per_node + 1) * num_workers_per_node]\n    intervals_per_node = intervals_replica[worker_index_per_node * num_workers_per_node : (worker_index_per_node + 1) * num_workers_per_node]\n\n    return {worker_index: chunks_per_node}, {worker_index: intervals_per_node}\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        # Remove the \"local:\" prefix if present\n        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\"):]\n\n        # Call the superclass's download_file method\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)\n\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            struct.pack(\"<II\", item.width, item.height)\n            + struct.pack(\"<B\", len(item.mode))\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            struct.pack(\n                \"<II\",\n                item.width,\n                item.height,\n            )\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            struct.pack(\n                \"<II\",\n                item.width,\n                item.height,\n            )\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            struct.pack(\"<II\", item.width, item.height)\n            + len(item.mode).to_bytes(1, \"big\")\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            struct.pack(\n                \"<II\",\n                item.width,\n                item.height,\n            )\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            struct.pack(\n                \"<II\",\n                item.width,\n                item.height,\n            )\n            + struct.pack(\"<I\", len(item.mode))\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            b\"\".join(\n                [\n                    item.width.to_bytes(4, \"little\"),\n                    item.height.to_bytes(4, \"little\"),\n                    len(item.mode).to_bytes(1, \"little\"),\n                    item.mode.encode(\"utf-8\"),\n                    item.tobytes(),\n                ]\n            ),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode_length = len(item.mode).to_bytes(4, \"big\")\n        mode = item.mode.encode(\"utf-8\")\n        data = item.tobytes()\n        return mode_length + mode + data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            struct.pack(\">II\", item.width, item.height)\n            + bytes([len(item.mode)])\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            struct.pack(\n                \"<II\",\n                item.width,\n                item.height,\n            )\n            + bytes(len(item.mode))\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            struct.pack(\"<II\", item.width, item.height)\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode_length = len(item.mode)\n        serialized_data = pack(\"<II\", width, height) + pack(\"B\", mode_length) + item.mode.encode(\"utf-8\") + item.tobytes()\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            struct.pack(\n                \"<II\",\n                item.width,\n                item.height,\n            )\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode_length = len(item.mode).to_bytes(2, byteorder='big')\n        mode = item.mode.encode('utf-8')\n        data = item.tobytes()\n        return mode_length + mode + data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            struct.pack(\"<II\", item.width, item.height)\n            + bytes([len(item.mode)])\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Convert the image to bytes\n        raw_data = item.tobytes()\n\n        # Serialize the image's width and height\n        width_bytes = struct.pack(\"<I\", item.width)\n        height_bytes = struct.pack(\"<I\", item.height)\n\n        # Serialize the image's mode length\n        mode_length_bytes = struct.pack(\"<I\", len(item.mode))\n\n        # Serialize the image's mode\n        mode_bytes = item.mode.encode(\"utf-8\")\n\n        # Combine the serialized data into a single bytes object\n        serialized_data = width_bytes + height_bytes + mode_length_bytes + mode_bytes + raw_data\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode = item.mode\n        mode_length = len(mode).to_bytes(1, 'big')\n        serialized_data = width.to_bytes(4, 'big') + height.to_bytes(4, 'big') + mode_length + mode.encode('utf-8') + item.tobytes()\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Serialize the image dimensions and mode\n        width, height = item.size\n        mode_length = len(item.mode).to_bytes(4, byteorder='big')\n        mode = item.mode.encode('utf-8')\n        serialized_data = width.to_bytes(4, byteorder='big') + height.to_bytes(4, byteorder='big') + mode_length + mode\n\n        # Add the raw pixel data to the serialized data\n        serialized_data += item.tobytes()\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Convert the PIL Image object to a bytes object using the BytesIO class from the io module.\n        with BytesIO() as f:\n            item.save(f, format='png')\n            img_bytes = f.getvalue()\n\n        # Get the image's dimensions, mode, and raw pixel data.\n        width, height = item.size\n        mode = item.mode\n        mode_len = len(mode).to_bytes(1, byteorder='big')\n        img_bytes = mode_len + mode.encode('utf-8') + img_bytes\n\n        # Return a tuple containing the serialized image data and None.\n        return img_bytes, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            struct.pack(\n                \"<II\",\n                item.width,\n                item.height,\n            )\n            + bytes(str(item.mode), \"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(f\"Unsupported image type: {type(item)}\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(\"Unsupported image type: {}\".format(type(item)))"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename:\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n            else:\n                return item.tobytes(), None\n        else:\n            raise TypeError(f\"Unsupported image type: {type(item)}\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.tobytes(), None\n        else:\n            raise TypeError(\"Unsupported image type: {}\".format(type(item)))"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename:\n                try:\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                except FileNotFoundError:\n                    pass\n            if item.format == \"JPEG\":\n                return item.tobytes(), None\n            else:\n                raise TypeError(\n                    f\"Unsupported image format: {item.format}. Only JPEG is supported.\"\n                )\n        else:\n            raise TypeError(\n                f\"Unsupported item type: {type(item)}. Only Image is supported.\"\n            )"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(f\"Unsupported item type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename:\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n            else:\n                return item.tobytes(), None\n        else:\n            raise TypeError(f\"Unsupported image type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename is not None and os.path.exists(item.filename):\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(\"Unsupported image type\")\n\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename is not None and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.to_bytes(format=\"JPEG\"), None\n        else:\n            raise TypeError(\"Item should be an instance of Image class or its subclasses.\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename is not None:\n                if os.path.exists(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    raise FileNotFoundError(f\"File {item.filename} does not exist.\")\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(f\"Unsupported image type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename is not None:\n                if os.path.isfile(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    raise FileNotFoundError(f\"File {item.filename} not found\")\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(f\"Unsupported image type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename is not None:\n                if os.path.exists(item.filename):\n                    with open(item.filename, 'rb') as f:\n                        return f.read(), None\n                else:\n                    raise FileNotFoundError(f\"File not found: {item.filename}\")\n            else:\n                return item.tobytes(), None\n        else:\n            raise TypeError(\"Item must be an instance of Image class or its subclasses.\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n            return item.to_jpeg(), None\n        else:\n            raise TypeError(\"item must be an instance of Image or its subclasses\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename is not None and os.path.isfile(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            return item.to_jpeg(), None\n        raise TypeError(f\"Unsupported image type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename is not None and os.path.isfile(item.filename):\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n            else:\n                return item.to_bytes(format=\"JPEG\"), None\n        else:\n            raise TypeError(f\"Unsupported image type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            return item.to_jpeg(), None\n        raise TypeError(\"Unsupported image type\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, 'rb') as f:\n                    return f.read(), None\n            else:\n                return item.tobytes(), None\n        else:\n            raise TypeError('Unsupported item type.')\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename is not None:\n                if os.path.exists(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n            if isinstance(item, Image.Image):\n                return item.tobytes(), None\n            else:\n                raise TypeError(\"Unsupported image type\")\n        else:\n            raise TypeError(\"Invalid item type\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename:\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.tobytes(), None\n        else:\n            raise TypeError(f\"Unsupported image type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(\n                f\"The item should be an instance of Image class or its subclasses. Got {type(item)}\"\n            )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack(\"<3I\", data[:12])\n        mode = data[12:12 + mode_size]\n        image_data = data[12 + mode_size:]\n\n        return cls(width, height, mode, image_data)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack('<3I', data[:12])\n        mode = data[12:12 + mode_size].decode('utf-8')\n        image_data = data[12 + mode_size:]\n\n        return cls(width, height, mode, image_data)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack(\"<3I\", data[:12])\n        mode = data[12:12 + mode_size].decode()\n        image_data = data[12 + mode_size:]\n\n        return cls(width, height, mode, image_data)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack('<3I', data[:12])\n        mode = data[12:12 + mode_size]\n        data = data[12 + mode_size:]\n        return cls(width, height, mode, data)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack('<3I', data[:12])\n        mode = data[12:12 + mode_size].decode('utf-8')\n        raw_data = data[12 + mode_size:]\n\n        return cls(width, height, mode, raw_data)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack('<3I', data[:12])\n        mode = data[12:12+mode_size]\n        image_data = data[12+mode_size:]\n        return cls(width, height, mode, image_data)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack('<III', data[:12])\n        mode = data[12:12+mode_size]\n        data = data[12+mode_size:]\n        return cls(width, height, mode, data)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack('<III', data[:12])\n        mode = data[12:12 + mode_size]\n        raw_data = data[12 + mode_size:]\n\n        return cls(width, height, mode, raw_data)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack('<III', data[:12])\n        mode = data[12:12+mode_size].decode('utf-8')\n        data = data[12+mode_size:]\n\n        return cls(width, height, mode, data)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack(\"<3I\", data[:12])\n        mode = data[12:12+mode_size]\n        image_data = data[12+mode_size:]\n\n        return cls(width, height, mode, image_data)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        # Extract the width, height, and mode from the beginning of the byte stream\n        width, height, mode_size = struct.unpack('<3I', data[:12])\n        mode = data[12:12 + mode_size].decode('utf-8')\n\n        # Use the width, height, and mode to reconstruct the image from the remaining bytes\n        image_data = data[12 + mode_size:]\n        image = cls(width, height, mode)\n        image.data = image_data\n\n        return image"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        # Extract the width, height, and mode from the beginning of the byte stream\n        width, height, mode_size = struct.unpack('<III', data[:12])\n        mode = data[12:12 + mode_size]\n\n        # Use the width, height, and mode to reconstruct the image from the remaining bytes\n        image = Image.frombytes(mode, (width, height), data[12 + mode_size:])\n        return image"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_len = struct.unpack(\"<3I\", data[:12])\n        mode = data[12:12 + mode_len].decode()\n        raw_data = data[12 + mode_len:]\n\n        return cls(width, height, mode, raw_data)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        # Extract the width, height, and mode from the beginning of the byte stream\n        width, height, mode_size = struct.unpack('<3I', data[:12])\n\n        # Extract the mode string from the byte stream\n        mode = data[12:12 + mode_size].decode('utf-8')\n\n        # Extract the raw image data from the byte stream\n        raw_data = data[12 + mode_size:]\n\n        # Reconstruct the image from the extracted data\n        return cls(width, height, mode, raw_data)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        # Extract the width, height, and mode from the beginning of the data\n        width, height, mode_size = struct.unpack(\"<III\", data[:12])\n        mode = data[12:12 + mode_size].decode()\n\n        # Use the width, height, and mode to reconstruct the image\n        image = Image.frombytes(mode, (width, height), data[12 + mode_size:])\n\n        # Return the reconstructed image\n        return image"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        # Extract the width, height, and mode from the beginning of the byte stream\n        width = int.from_bytes(data[:4], byteorder='little', signed=False)\n        height = int.from_bytes(data[4:8], byteorder='little', signed=False)\n        mode_size = int.from_bytes(data[8:12], byteorder='little', signed=False)\n\n        # Extract the mode string from the byte stream\n        mode = data[12:12 + mode_size].decode()\n\n        # Extract the raw image data from the byte stream\n        raw_data = data[12 + mode_size:]\n\n        # Use the width, height, mode, and raw data to reconstruct the image\n        return cls(width, height, mode, raw_data)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        # Extract the width, height, and mode from the beginning of the byte stream\n        width, height, mode_size = struct.unpack('<3I', data[:12])\n\n        # Extract the mode string from the byte stream\n        mode = data[12:12 + mode_size].decode('utf-8')\n\n        # Extract the raw image data from the byte stream\n        raw_data = data[12 + mode_size:]\n\n        # Create an image object using the raw data and the mode\n        img = Image.frombuffer(mode, (width, height), raw_data, 'raw', mode, 0, 1)\n\n        # Return the image object\n        return img"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        # Extract the width, height, and mode from the beginning of the byte stream\n        width, height, mode_size = struct.unpack(\"<3I\", data[:12])\n\n        # Extract the mode string from the byte stream\n        mode = data[12:12 + mode_size].decode()\n\n        # Extract the raw image data from the byte stream\n        raw_data = data[12 + mode_size:]\n\n        # Reconstruct the image from the raw data and return it\n        return cls(width, height, mode, raw_data)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack(\"<3I\", data[:12])\n        mode = data[12:12 + mode_size]\n        image_data = data[12 + mode_size:]\n        return cls(width, height, mode, image_data)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack(\"<III\", data[:12])\n        mode = data[12:12 + mode_size]\n        data = data[12 + mode_size:]\n        return cls(width, height, mode, data)\n\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype = self.dtype_map[data[0]]\n        shape = pickle.loads(data[1:])\n        tensor = torch.frombuffer(data[1 + len(shape):], dtype=dtype)\n        tensor = tensor.reshape(shape)\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(data[:4].decode())\n        shape = tuple(int(x) for x in data[4:].decode().split(','))\n\n        # Extract the raw data from the byte array\n        raw_data = data[8:]\n\n        # Convert the raw data to a numpy array\n        np_array = np.frombuffer(raw_data, dtype=dtype)\n\n        # Reshape the numpy array to match the specified shape\n        np_array = np_array.reshape(shape)\n\n        # Convert the numpy array to a PyTorch tensor\n        tensor = torch.from_numpy(np_array)\n\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype = self.dtype_map[data[0]]\n        shape = pickle.loads(data[1:])\n        tensor = torch.tensor(data[1 + len(shape) :], dtype=dtype)\n        tensor = tensor.reshape(shape)\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape from the byte array\n        dtype = self.dtype_map[data[0]]\n        shape = struct.unpack('>' + 'I' * (len(data) - 1) // 4, data[1:])\n\n        # Reconstruct the tensor from the remaining bytes\n        tensor = torch.frombuffer(data, dtype=dtype, offset=len(data) - len(data) % 4)\n        tensor = tensor.reshape(shape)\n\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape from the byte array\n        dtype_str = data[:4].decode('utf-8')\n        shape = pickle.loads(data[4:])\n\n        # Construct the tensor from the remaining bytes\n        tensor = torch.frombuffer(data[4+len(data):], dtype=dtype_str)\n\n        # Reshape the tensor to match the original shape\n        tensor = tensor.reshape(shape)\n\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(data[:4].decode('utf-8'))\n        shape = tuple(int(x) for x in data[4:].decode('utf-8').split(','))\n\n        # Calculate the number of bytes required to store the tensor data\n        num_bytes = np.prod(shape) * dtype.itemsize\n\n        # Extract the tensor data from the byte array\n        tensor_data = data[8:8 + num_bytes]\n\n        # Convert the tensor data to a NumPy array\n        tensor_array = np.frombuffer(tensor_data, dtype=dtype)\n\n        # Reshape the NumPy array to the desired shape\n        tensor_array = tensor_array.reshape(shape)\n\n        # Convert the NumPy array to a PyTorch tensor\n        tensor = torch.from_numpy(tensor_array)\n\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype, shape = struct.unpack('<I{}I'.format(len(shape)), data[:4 + len(shape) * 4])\n        dtype = torch.int8 if dtype == 0 else torch.int16\n        shape = tuple(shape)\n\n        # Extract the raw data from the byte array\n        raw_data = data[4 + len(shape) * 4:]\n\n        # Reconstruct the tensor from the raw data and the extracted information\n        return torch.frombuffer(raw_data, dtype=dtype).reshape(shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_code = data[0]\n        dtype = torch.int8 if dtype_code == 0 else torch.float32\n        shape = torch.tensor(np.frombuffer(data[1:13], dtype=np.int32))\n        tensor = torch.tensor(np.frombuffer(data[13:], dtype=dtype)).view(shape)\n\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = torch.get_default_dtype()\n        if data[0] == 0:\n            dtype = torch.float32\n        elif data[0] == 1:\n            dtype = torch.float64\n        elif data[0] == 2:\n            dtype = torch.int32\n        elif data[0] == 3:\n            dtype = torch.int64\n        elif data[0] == 4:\n            dtype = torch.uint8\n        elif data[0] == 5:\n            dtype = torch.bool\n        else:\n            raise ValueError(\"Unsupported dtype\")\n\n        shape = struct.unpack(\"I\", data[1:5])[0]\n        shape = struct.unpack(\"I\" * shape, data[5:5 + shape * 4])\n\n        # Reconstruct the tensor from the remaining bytes\n        tensor = torch.frombuffer(data[5 + shape * 4:], dtype=dtype).reshape(shape)\n\n        return tensor\n\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(np.frombuffer(data, dtype=np.int8, count=1, offset=0))\n        shape = np.frombuffer(data, dtype=np.int32, count=-1, offset=1)\n\n        # Extract the raw data from the byte array\n        raw_data = data[5 + shape.nbytes:]\n\n        # Convert the raw data to a NumPy array of the specified data type\n        np_array = np.frombuffer(raw_data, dtype=dtype)\n\n        # Reshape the NumPy array to match the specified shape\n        np_array = np_array.reshape(shape)\n\n        # Convert the NumPy array to a PyTorch tensor\n        tensor = torch.from_numpy(np_array)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype, shape = struct.unpack('<iQ', data[:9])\n        dtype = torch.uint8 if dtype == 0 else torch.float32\n        shape = tuple(shape)\n\n        # Calculate the number of bytes in the tensor\n        num_bytes = np.prod(shape) * 4\n\n        # Extract the raw data from the byte array\n        tensor_data = np.frombuffer(data[9:], dtype=np.float32).reshape(shape)\n\n        # Convert the raw data to a PyTorch tensor\n        tensor = torch.from_numpy(tensor_data).type(dtype)\n\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str = data[:10].decode().split(\",\")\n        dtype = getattr(torch, dtype_str)\n        shape = tuple(map(int, shape_str.split(\",\")))\n\n        # Extract the raw tensor data from the byte array\n        tensor_data = data[10:]\n\n        # Reconstruct the tensor from the raw data\n        tensor = torch.frombuffer(tensor_data, dtype=dtype).reshape(shape)\n\n        return tensor\n\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_code = data[0]\n        shape_size = data[1]\n        shape = data[2:2 + shape_size]\n\n        # Convert the shape information to a tuple\n        shape = tuple(shape)\n\n        # Extract the raw tensor data from the byte array\n        raw_data = data[2 + shape_size:]\n\n        # Convert the raw data to a NumPy array\n        np_array = np.frombuffer(raw_data, dtype=np.uint8)\n\n        # Convert the NumPy array to a PyTorch tensor\n        tensor = torch.from_numpy(np_array)\n\n        # Reshape the tensor to match the original shape\n        tensor = tensor.reshape(shape)\n\n        # Convert the tensor to the specified data type\n        tensor = tensor.type(torch.float32)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype, shape = self.deserialize_header(data)\n        tensor_data = data[self.header_size:]\n        tensor = torch.frombuffer(tensor_data, dtype=dtype).reshape(shape)\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype, shape = self.extract_info(data)\n        tensor_data = data[self.info_length:]\n        return torch.frombuffer(tensor_data, dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype = self.dtype_from_bytes(data[:self.dtype_length])\n        shape = self.shape_from_bytes(data[self.dtype_length:self.shape_length])\n        tensor = torch.frombuffer(data[self.shape_length:], dtype=dtype)\n        return tensor.reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(data[0:4])\n        shape = np.frombuffer(data[4:20], dtype=np.int32)\n\n        # Calculate the size of the tensor data in bytes\n        size = np.prod(shape) * dtype.itemsize\n\n        # Extract the tensor data from the byte array and reconstruct the tensor\n        tensor_data = np.frombuffer(data[20:20 + size], dtype=dtype)\n        tensor_data = tensor_data.reshape(shape)\n        tensor = torch.from_numpy(tensor_data)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str = data[:16].decode().split(':')\n        shape = tuple(map(int, shape_str.split(',')))\n        dtype = np.dtype(dtype_str)\n\n        # Extract the raw data from the byte array\n        raw_data = data[16:]\n\n        # Convert the raw data to a NumPy array with the specified dtype and shape\n        np_array = np.frombuffer(raw_data, dtype=dtype).reshape(shape)\n\n        # Convert the NumPy array to a PyTorch tensor\n        return torch.from_numpy(np_array)\n\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype, shape = self.get_dtype_and_shape(data)\n        tensor = torch.frombuffer(data[self.header_size:], dtype=dtype).reshape(shape)\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype, shape = struct.unpack('<iQ', data[:12])\n        dtype = np.dtype(dtype)\n        shape = tuple(struct.unpack('<' + 'I' * len(shape), data[12:12 + (4 * len(shape))]))\n\n        # Calculate the total number of bytes in the serialized data\n        total_bytes = struct.calcsize('<' + 'f' * (np.prod(shape)))\n\n        # Extract the tensor data from the byte array\n        tensor_data = struct.unpack('<' + 'f' * (np.prod(shape)), data[12 + (4 * len(shape)):12 + (4 * len(shape)) + total_bytes])\n\n        # Convert the tensor data to a NumPy array\n        tensor_data = np.array(tensor_data, dtype=dtype)\n\n        # Reshape the NumPy array to match the original tensor shape\n        tensor_data = tensor_data.reshape(shape)\n\n        # Convert the NumPy array to a PyTorch tensor\n        tensor = torch.from_numpy(tensor_data)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            torch.save(item, io.BytesIO()).getvalue(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            torch.save(item, io.BytesIO()).getvalue(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (item.dtype.name, item.shape, item.numpy().tobytes()), None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype = str(item.dtype)\n        shape = item.shape\n        data = item.numpy().tobytes()\n        return pickle.dumps((dtype, shape, data)), None\n    "}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            pickle.dumps(\n                {\n                    \"dtype\": str(item.dtype),\n                    \"shape\": item.shape,\n                    \"data\": item.numpy().tobytes(),\n                }\n            ),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            struct.pack(\n                \"<\" + \"\".join([\"i\" for _ in range(len(item.shape))] + [self.dtype_to_format[item.dtype.name]]),\n                *item.shape,\n                *item.flatten().tolist(),\n            ),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            struct.pack(\"<i\", item.dtype.num)\n            + struct.pack(\"<i\", item.shape[0])\n            + item.data.numpy().tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            torch.save(item),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a numpy array\n        np_array = item.numpy()\n\n        # Serialize the numpy array\n        serialized_array = pickle.dumps(np_array)\n\n        # Return the serialized array and None for the second element of the tuple\n        return serialized_array, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            struct.pack(\n                \"<i\",\n                item.dtype.itemsize,\n            )\n            + struct.pack(\"<i\", len(item.shape))\n            + struct.pack(\"<i\" * len(item.shape), *item.shape)\n            + item.numpy().tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            torch.save(item, io.BytesIO()).getvalue(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            pickle.dumps(\n                {\n                    \"dtype\": str(item.dtype),\n                    \"shape\": item.shape,\n                    \"data\": item.numpy().tobytes(),\n                }\n            ),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            struct.pack(\"<i\", item.dtype.itemsize)\n            + struct.pack(\"<i\", len(item.shape))\n            + struct.pack(\"<\" + \"i\" * len(item.shape), *item.shape)\n            + item.numpy().tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            torch.save(item, io.BytesIO()).getvalue(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            struct.pack(\"<H\", item.dtype.itemsize)\n            + item.dtype.name.encode()\n            + struct.pack(\"<H\", len(item.shape))\n            + item.shape.tobytes()\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            struct.pack(\n                \"<I\",\n                item.dtype.itemsize,\n            )\n            + struct.pack(\n                \"<I\",\n                len(item.shape),\n            )\n            + struct.pack(\n                \"<\" + \"I\" * len(item.shape),\n                *item.shape,\n            )\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return torch.save(item), None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            struct.pack(\"<Q\", item.numel())\n            + item.dtype.byteorder\n            + struct.pack(\"<I\", item.dtype.itemsize)\n            + item.dtype.name.encode(\"ascii\")\n            + item.shape\n            + item.numpy().tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            struct.pack(\"<Q\", item.numel())\n            + struct.pack(\"<Q\", item.dtype.num)\n            + item.numpy().tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            struct.pack(\n                \"<\" + \"\".join(\n                    [\n                        self.dtype_to_format_dict[item.dtype.name],\n                        *[f\"I{len(dim)}\" for dim in item.shape],\n                    ]\n                ),\n                *[item.shape[i] for i in range(len(item.shape))],\n                item.view(-1).cpu().numpy().tobytes(),\n            ),\n            None,\n        )\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            import torchvision\n            image = torchvision.io.read_image(data)\n            return image\n        except RuntimeError:\n            from PIL import Image\n            image = Image.open(io.BytesIO(data))\n            if torchvision is not None:\n                image = torchvision.transforms.functional.to_tensor(image)\n            return image"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            import torchvision\n\n            return torchvision.io.read_image(data)\n        except (RuntimeError, ImportError):\n            pass\n        return JpegImageFile.from_bytes(data)"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if torchvision is not None:\n            try:\n                return JpegImageFile(data)\n            except RuntimeError:\n                pass\n        return torch.tensor(Image.open(BytesIO(data)))"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            # Attempt to deserialize the data as a JPEG image using torchvision\n            import torchvision.io as tvio\n            return tvio.read_image(data)\n        except RuntimeError:\n            # If the decoding fails, fall back to using PIL to deserialize the data\n            from PIL import Image\n            image = Image.open(BytesIO(data))\n            if torchvision_available:\n                # If torchvision is available, convert the PIL image to a PyTorch tensor\n                return tvio.read_image(image)\n            else:\n                # If torchvision is not available, return the PIL image\n                return image"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            # Try to decode the data as a JPEG image using torchvision.\n            import torchvision\n            jpeg_image = torchvision.io.decode_jpeg(data)\n            return jpeg_image\n        except RuntimeError:\n            # If the decoding fails, try to use PIL to deserialize the data.\n            try:\n                from PIL import Image\n                pil_image = Image.open(BytesIO(data))\n                if torchvision is not None:\n                    # If torchvision is available, convert the PIL image to a PyTorch tensor.\n                    return torchvision.transforms.functional.to_tensor(pil_image)\n                else:\n                    # Otherwise, return the PIL image as-is.\n                    return pil_image\n            except Exception as e:\n                # If the decoding fails for both JPEG and PIL, raise an exception.\n                raise e"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            import torchvision\n            import torch\n            from torchvision.io import read_image\n\n            # Try to decode the data as a JPEG image using torchvision\n            try:\n                jpeg_image = read_image(data)\n                return jpeg_image\n            except RuntimeError as e:\n                # If decoding as a JPEG fails, fall back to using PIL to deserialize the data\n                from PIL import Image\n\n                # Convert the byte data to a PIL image\n                pil_image = Image.open(io.BytesIO(data))\n                # Convert the PIL image to a PyTorch tensor\n                return torch.tensor(np.array(pil_image))\n        except ImportError:\n            # If torchvision is not available, fall back to using PIL to deserialize the data\n            from PIL import Image\n\n            # Convert the byte data to a PIL image\n            pil_image = Image.open(io.BytesIO(data))\n            # Convert the PIL image to a PyTorch tensor\n            return torch.tensor(np.array(pil_image))"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            # Try to deserialize the data as a JPEG image using torchvision\n            if torchvision is not None:\n                image = torchvision.io.read_image(data)\n                if image.dtype == torch.uint8:\n                    image = image.type(torch.float32)\n                return image\n        except RuntimeError:\n            # If the decoding fails due to a runtime error, fall back to using PIL\n            image = Image.open(BytesIO(data))\n            # Convert the PIL image to a PyTorch tensor if torchvision is available\n            if torchvision is not None:\n                return torchvision.transforms.ToTensor()(image)\n            return image\n\n        # If the decoding fails for any other reason, return the original data\n        return data"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            import torchvision\n            from torchvision.io import read_image\n\n            image = read_image(data)\n            return image\n        except Exception:\n            from PIL import Image\n\n            image = Image.open(io.BytesIO(data))\n            if torchvision is not None:\n                return torchvision.transforms.ToTensor()(image)\n            else:\n                return image\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            import torchvision\n            from torchvision.io import read_image\n            image = read_image(data)\n            return image\n        except RuntimeError as e:\n            if \"only support JPEG\" in str(e):\n                from PIL import Image\n                image = Image.open(io.BytesIO(data))\n                if torchvision is not None:\n                    image = torchvision.transforms.ToTensor()(image)\n                return image\n            else:\n                raise e\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            import torchvision\n            import torch\n            from torchvision.io import read_image\n            from PIL import Image\n            image = Image.open(io.BytesIO(data))\n            image_tensor = read_image(image)\n            return image_tensor\n        except Exception as e:\n            import io\n            import PIL\n            image = PIL.Image.open(io.BytesIO(data))\n            return image\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if torchvision is not None and isinstance(data, bytes):\n            try:\n                return torchvision.io.read_image(data)\n            except RuntimeError:\n                pass\n        image = Image.open(io.BytesIO(data))\n        return image\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            if self._torchvision_available:\n                image = JpegImageFile.from_bytes(data)\n            else:\n                image = Image.open(BytesIO(data))\n                if self._torchvision_available:\n                    image = self._to_tensor(image)\n        except RuntimeError:\n            if self._torchvision_available:\n                image = JpegImageFile.from_bytes(data)\n            else:\n                image = Image.open(BytesIO(data))\n        return image\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            if self.torch_available:\n                try:\n                    return torch.from_numpy(np.array(Image.open(BytesIO(data)).convert(\"RGB\"))).permute(2, 0, 1)\n                except RuntimeError:\n                    pass\n        except ImportError:\n            pass\n        return JpegImageFile(data)\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            from torchvision.io import read_image\n            import torch\n            return read_image(data)\n        except RuntimeError:\n            from PIL.Image import open\n            return open(BytesIO(data))\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            if torchvision is not None:\n                return torchvision.io.decode_jpeg(data)\n        except RuntimeError:\n            pass\n        return PIL.Image.open(io.BytesIO(data))\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            if self.torch_available:\n                return torch.from_numpy(np.array(Image.open(io.BytesIO(data)).convert('RGB'), dtype=np.uint8))\n            else:\n                return Image.open(io.BytesIO(data)).convert('RGB')\n        except RuntimeError as e:\n            if self.torch_available:\n                return torch.from_numpy(np.array(Image.open(io.BytesIO(data)), dtype=np.uint8))\n            else:\n                return Image.open(io.BytesIO(data))\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if torchvision is not None:\n            try:\n                image = Image.open(io.BytesIO(data))\n                return torchvision.transforms.functional.to_tensor(image)\n            except RuntimeError:\n                pass\n        image = Image.open(io.BytesIO(data))\n        return image\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            import torchvision\n            image = torchvision.io.read_image(data)\n            return image\n        except RuntimeError:\n            pass\n\n        image = Image.open(BytesIO(data))\n        return image\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            import torchvision\n            import torch\n            import io\n            from torchvision.transforms import functional as F\n            from PIL import Image\n            from torchvision.io import read_image\n            from torchvision.io.image import ImageReadMode\n\n            img = Image.open(io.BytesIO(data))\n            img = F.to_tensor(img)\n            return img\n        except (ImportError, RuntimeError):\n            import io\n            from PIL import Image\n            img = Image.open(io.BytesIO(data))\n            return img\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            import torchvision\n            import torchvision.transforms as T\n\n            image = torchvision.io.read_image(data)\n            image = image.permute(1, 2, 0)\n            image = image.numpy()\n            return image\n        except Exception as e:\n            import PIL.Image\n            image = PIL.Image.open(BytesIO(data))\n            image.load()\n            return image\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        return (item.numpy().tobytes(), f\"no_header_tensor:{self.data_type_to_index[item.dtype]}\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        return (item.numpy().tobytes(), f\"no_header_tensor:{self._data_type_to_index[item.dtype]}\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        return (item.numpy().tobytes(), f\"no_header_tensor:{self.data_type_index[item.dtype]}\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        return (item.numpy().tobytes(), f\"no_header_tensor:{self.data_type_to_index[item.dtype]}\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if item.device != torch.device(\"cpu\"):\n            item = item.cpu()\n        return (\n            item.numpy().tobytes(),\n            f\"no_header_tensor:{self.data_type_index[item.dtype]}\",\n        )\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if not isinstance(item, torch.Tensor):\n            raise TypeError(f\"Expected torch.Tensor, but got {type(item)}\")\n        return (item.cpu().numpy().tobytes(), f\"no_header_tensor:{item.dtype.name}\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        tensor_data = item.cpu().detach().numpy()\n        data_type_index = self._data_type_to_index[str(item.dtype)]\n        return tensor_data.tobytes(), f\"no_header_tensor:{data_type_index}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        return (\n            item.cpu().numpy().tobytes(),\n            f\"no_header_tensor:{self._data_type_to_index[item.dtype]}\",\n        )\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array and then to bytes\n        np_array = item.cpu().numpy()\n        data = np_array.tobytes()\n\n        # Map the tensor's data type to an index\n        data_type_index = self.data_type_to_index[type(np_array)]\n\n        # Return the serialized data and the data type index as a string\n        return data, f\"no_header_tensor:{data_type_index}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if item.dtype == torch.float32:\n            data_type_index = 0\n        elif item.dtype == torch.int32:\n            data_type_index = 1\n        elif item.dtype == torch.int64:\n            data_type_index = 2\n        elif item.dtype == torch.bool:\n            data_type_index = 3\n        else:\n            raise ValueError(f\"Unsupported data type: {item.dtype}\")\n        data_type_str = f\"no_header_tensor:{data_type_index}\"\n        return item.numpy().tobytes(), data_type_str\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if item.requires_grad:\n            raise RuntimeError(\"Tensors with requires_grad=True are not supported\")\n        item = item.detach().cpu().numpy()\n        if item.dtype == np.float32:\n            dtype_index = 0\n        elif item.dtype == np.float64:\n            dtype_index = 1\n        elif item.dtype == np.int32:\n            dtype_index = 2\n        elif item.dtype == np.int64:\n            dtype_index = 3\n        elif item.dtype == np.bool_:\n            dtype_index = 4\n        elif item.dtype == np.uint8:\n            dtype_index = 5\n        elif item.dtype == np.int8:\n            dtype_index = 6\n        elif item.dtype == np.int16:\n            dtype_index = 7\n        elif item.dtype == np.int32:\n            dtype_index = 8\n        elif item.dtype == np.int64:\n            dtype_index = 9\n        elif item.dtype == np.uint8:\n            dtype_index = 10\n        elif item.dtype == np.uint16:\n            dtype_index = 11\n        elif item.dtype == np.uint32:\n            dtype_index = 12\n        elif item.dtype == np.uint64:\n            dtype_index = 13\n        elif item.dtype == np.float16:\n            dtype_index = 14\n        elif item.dtype == np.float32:\n            dtype_index = 15\n        elif item.dtype == np.float64:\n            dtype_index = 16\n        elif item.dtype == np.complex64:\n            dtype_index = 17\n        elif item.dtype == np.complex128:\n            dtype_index = 18\n        else:\n            raise RuntimeError(\"Unsupported dtype: {}\".format(item.dtype))\n        return item.tobytes(), \""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        tensor_np = item.cpu().numpy()\n        # Convert the NumPy array to bytes\n        tensor_bytes = tensor_np.tobytes()\n        # Get the data type of the tensor\n        tensor_type = item.dtype\n        # Get the index of the data type\n        tensor_type_index = self.data_type_index_map[tensor_type]\n        # Create a string representation of the data type index\n        tensor_type_str = f\"no_header_tensor:{tensor_type_index}\"\n        # Return the serialized tensor data and the string representation of the data type index\n        return tensor_bytes, tensor_type_str\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        np_arr = item.numpy()\n\n        # Convert the NumPy array to bytes\n        np_bytes = np_arr.tobytes()\n\n        # Get the data type index\n        data_type_index = self.get_data_type_index(item)\n\n        # Return the serialized data and data type index\n        return np_bytes, f\"no_header_tensor:{data_type_index}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        return (\n            item.numpy().tobytes(),\n            f\"no_header_tensor:{self._data_type_to_index[item.dtype.name]}\",\n        )\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if isinstance(item, torch.Tensor):\n            item = item.cpu().numpy()\n        else:\n            raise TypeError(\"Expected torch.Tensor, but got {}\".format(type(item)))\n        return item.tobytes(), \"no_header_tensor:{}\".format(self.dtype_to_idx[item.dtype.name])\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        np_item = item.numpy()\n        data = np_item.tobytes()\n        dtype = str(np_item.dtype)\n        dtype_idx = self.dtype_to_idx[dtype]\n        dtype_str = f\"no_header_tensor:{dtype_idx}\"\n        return data, dtype_str\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        return (item.numpy().tobytes(), f\"no_header_tensor:{self.tensor_type_to_index[item.dtype.name]}\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        tensor_array = item.numpy()\n        # Convert the NumPy array to bytes\n        tensor_bytes = tensor_array.tobytes()\n        # Get the data type of the tensor\n        tensor_dtype = str(item.dtype)\n        # Map the data type to an index\n        dtype_index = self.dtype_to_index[tensor_dtype]\n        # Return the serialized tensor data and its data type index\n        return tensor_bytes, f\"no_header_tensor:{dtype_index}\"\n\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert tensor to NumPy array\n        np_array = item.numpy()\n        # Convert NumPy array to bytes\n        np_bytes = np_array.tobytes()\n        # Get tensor data type\n        tensor_type = str(item.dtype)\n        # Map data type to index\n        tensor_type_index = self.tensor_type_to_index[tensor_type]\n        # Create string with data type index\n        tensor_type_str = f\"no_header_tensor:{tensor_type_index}\"\n        # Return serialized tensor data and data type string\n        return np_bytes, tensor_type_str\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        np_array = item.numpy()\n        # Serialize the NumPy array to bytes\n        serialized_np_array = pickle.dumps(np_array)\n        # Get the data type of the tensor\n        data_type = str(item.dtype)\n        # Map the data type to an index\n        data_type_index = self.data_type_to_index[data_type]\n        # Create a string to represent the data type index\n        data_type_index_str = str(data_type_index)\n        # Create the serialized tensor data and data type string\n        serialized_tensor_data = serialized_np_array\n        serialized_tensor_data_type = \"no_header_tensor:\" + data_type_index_str\n        return serialized_tensor_data, serialized_tensor_data_type\n\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        # Convert the byte data into a NumPy array of the specified data type\n        np_array = np.frombuffer(data, dtype=self._dtype)\n\n        # Convert the NumPy array into a PyTorch tensor\n        tensor = torch.from_numpy(np_array)\n\n        # Return the deserialized PyTorch tensor\n        return tensor"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        # Create a PyTorch tensor from the byte data using the specified data type\n        tensor = torch.frombuffer(data, dtype=self._dtype)\n\n        # Reshape the tensor to the specified shape\n        tensor = tensor.reshape(self._shape)\n\n        # Return the deserialized tensor\n        return tensor"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        # Convert the byte data to a numpy array using the specified data type\n        np_array = np.frombuffer(data, dtype=self._dtype)\n\n        # Convert the numpy array to a PyTorch tensor\n        tensor = torch.from_numpy(np_array)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype, shape, data = pickle.loads(data)\n        return np.frombuffer(data, dtype=dtype).reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dt = np.dtype(data[:4])\n        shape = tuple(np.frombuffer(data[4:4+dt.itemsize], dtype=dt))\n        return np.frombuffer(data[4+dt.itemsize:], dtype=dt).reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(data[:4])\n        shape = tuple(np.frombuffer(data[4:20], dtype=np.int32))\n\n        # Reconstruct the numpy array based on the data type and shape information\n        array = np.frombuffer(data[20:], dtype=dtype).reshape(shape)\n\n        return array"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dt = np.dtype(data[:4])\n        shape = np.frombuffer(data[4:20], dtype=np.int32)\n        data = np.frombuffer(data[20:], dtype=dt)\n        return data.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_str, shape_str = data[:data.find(b' ')].decode('utf-8'), data[data.find(b' ') + 1:data.find(b'\\n')].decode('utf-8')\n        dtype = np.dtype(dtype_str)\n        shape = tuple(map(int, shape_str.split(',')))\n        array_data = data[data.find(b'\\n') + 1:]\n        return np.frombuffer(array_data, dtype=dtype).reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(data[:4])\n        shape = tuple(data[4:4+dtype.itemsize] for _ in range(dtype.shape[0]))\n\n        # Calculate the size of the array's data\n        data_size = np.prod(shape) * dtype.itemsize\n\n        # Extract the array's actual data from the byte array\n        array_data = np.frombuffer(data[4+dtype.itemsize:], dtype=dtype)\n\n        # Reshape the array data to match the original shape\n        array_data = array_data.reshape(shape)\n\n        # Return the reconstructed numpy array\n        return array_data"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract data type and shape information from the byte array\n        dtype = np.dtype(data[:4])\n        shape = tuple(int.from_bytes(data[4 + i * 4:8 + i * 4], byteorder='little') for i in range(dtype.shape[0]))\n\n        # Reconstruct the numpy array based on the extracted information\n        array_data = np.frombuffer(data[8 + dtype.itemsize * dtype.shape[0]:], dtype=dtype)\n        array_data = array_data.reshape(shape)\n\n        return array_data"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str = data[:data.find(b\" \")].decode()\n        shape_str = data[data.find(b\" \") + 1:data.rfind(b\" \")].decode()\n        shape = tuple(map(int, shape_str.split(\",\")))\n\n        # Convert the byte array to a numpy array\n        array_data = data[data.rfind(b\" \") + 1:]\n        array = np.frombuffer(array_data, dtype=dtype_str)\n        array = array.reshape(shape)\n\n        return array"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract data type and shape from the byte array\n        dtype = np.dtype(data[:16])\n        shape = tuple(np.frombuffer(data[16:32], dtype=np.int32, count=dtype.shape[0]))\n\n        # Extract the array data from the byte array\n        data_start = 32\n        data_end = data_start + np.prod(shape) * dtype.itemsize\n        array_data = np.frombuffer(data[data_start:data_end], dtype=dtype)\n\n        # Reshape the array data to match the original shape\n        array_data = array_data.reshape(shape)\n\n        return array_data"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape from the byte data\n        dtype = np.dtype(data[:4])\n        shape = tuple(np.frombuffer(data[4:20], dtype=np.int32, count=dtype.shape[0]))\n\n        # Calculate the size of the array in bytes\n        size = np.prod(shape) * dtype.itemsize\n\n        # Reconstruct the numpy array from the byte data\n        array = np.frombuffer(data[20:20 + size], dtype=dtype).reshape(shape)\n\n        return array"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str = data[:data.find(b' ')].decode().split(',')\n        shape = tuple(int(dim) for dim in shape_str.split(','))\n        dtype = np.dtype(dtype_str)\n\n        # Reconstruct the numpy array from the byte array\n        itemsize = np.dtype(dtype).itemsize\n        data = np.frombuffer(data[data.find(b' ') + 1:], dtype=np.uint8).astype(np.float32)\n        data = np.reshape(data, shape)\n\n        return data"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape from the byte data\n        dtype = np.dtype(data[:4])\n        shape = tuple(int.from_bytes(data[4:8], byteorder='little') for _ in range(int.from_bytes(data[8:12], byteorder='little')))\n\n        # Reconstruct the numpy array based on the extracted information\n        array_data = data[12:]\n        array = np.frombuffer(array_data, dtype=dtype).reshape(shape)\n\n        return array\n\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dt = np.dtype(data[:5])\n        shape = tuple(np.frombuffer(data[5:23], dtype=np.int32))\n\n        # Calculate the number of bytes in the serialized data\n        itemsize = dt.itemsize\n        num_bytes = itemsize * np.prod(shape)\n\n        # Extract the actual data from the byte array\n        data = data[23:]\n        assert len(data) == num_bytes, f\"Expected {num_bytes} bytes, got {len(data)}\"\n\n        # Construct the numpy array from the data\n        arr = np.frombuffer(data, dtype=dt)\n        arr = arr.reshape(shape)\n\n        return arr"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(data[:4].decode())\n        shape = tuple(int(x) for x in data[4:20].decode().split(\",\"))\n\n        # Calculate the size of the array data\n        data_size = np.prod(shape) * dtype.itemsize\n\n        # Extract the array data from the byte array\n        array_data = data[20:]\n\n        # Reconstruct the numpy array from the data\n        array = np.frombuffer(array_data, dtype=dtype)\n        array = array.reshape(shape)\n\n        return array\n\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(np.frombuffer(data, dtype=np.uint8, count=1, offset=0))\n        shape = np.frombuffer(data, dtype=np.int32, count=-1, offset=1)\n\n        # Reconstruct the numpy array based on the data type and shape information\n        array = np.frombuffer(data, dtype=dtype, count=-1, offset=5)\n        array = array.reshape(shape)\n\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_str, shape_str = data[:data.find(b' ')].decode(), data[data.find(b' ')+1:data.rfind(b' ')].decode()\n        dtype, shape = np.dtype(dtype_str), tuple(map(int, shape_str.split(',')))\n        return np.frombuffer(data[data.rfind(b' ')+1:], dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(data[:4])\n        shape = tuple(int(x) for x in data[4:20].split(b','))\n\n        # Calculate the number of bytes required to store the array data\n        itemsize = dtype.itemsize\n        num_bytes = np.prod(shape) * itemsize\n\n        # Extract the array data from the byte array and reconstruct the numpy array\n        array_data = np.frombuffer(data[20:20 + num_bytes], dtype=dtype).reshape(shape)\n\n        return array_data\n\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(data[:16])\n        shape = np.frombuffer(data[16:32], dtype=np.uint32, count=len(dtype.shape))\n\n        # Extract the actual array data from the byte array\n        data = data[32:]\n\n        # Reconstruct the numpy array from the data\n        return np.frombuffer(data, dtype=dtype).reshape(shape)\n\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(data[:4])\n        shape = tuple(int.from_bytes(data[4 + i * 4:8 + i * 4], byteorder='little', signed=False) for i in range(dtype.fields[0][1].shape[0]))\n\n        # Calculate the starting index of the array data\n        start = 8 + dtype.fields[0][1].shape[0] * 4\n\n        # Reconstruct the numpy array based on the data type and shape information\n        array = np.frombuffer(data, dtype=dtype, count=-1, offset=start)\n        array = array.reshape(shape)\n\n        return array\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract data type and shape information from the byte array\n        dtype = np.dtype(data[:4].decode())\n        shape = tuple(int(x) for x in data[4:20].decode().split(','))\n\n        # Extract the actual data from the byte array\n        data = data[20:]\n\n        # Reconstruct the numpy array based on the extracted information\n        return np.frombuffer(data, dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{self.indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{item.dtype.name}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{self.indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        # Convert the NumPy array into a bytes object\n        serialized_item = item.tobytes()\n\n        # Generate the dtype identifier\n        dtype_identifier = f\"no_header_numpy:{item.dtype.name}\"\n\n        return serialized_item, dtype_identifier\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_str = f\"no_header_numpy:{self.dtype_indice}\"\n        self.dtype_indice += 1\n        return item.tobytes(), dtype_str\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{self.indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{item.dtype.char}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        item_type = item.dtype.name\n        if item_type == \"bool\":\n            return item.tobytes(), \"no_header_numpy:1\"\n        elif item_type == \"int8\":\n            return item.tobytes(), \"no_header_numpy:2\"\n        elif item_type == \"int16\":\n            return item.tobytes(), \"no_header_numpy:3\"\n        elif item_type == \"int32\":\n            return item.tobytes(), \"no_header_numpy:4\"\n        elif item_type == \"int64\":\n            return item.tobytes(), \"no_header_numpy:5\"\n        elif item_type == \"uint8\":\n            return item.tobytes(), \"no_header_numpy:6\"\n        elif item_type == \"uint16\":\n            return item.tobytes(), \"no_header_numpy:7\"\n        elif item_type == \"uint32\":\n            return item.tobytes(), \"no_header_numpy:8\"\n        elif item_type == \"uint64\":\n            return item.tobytes(), \"no_header_numpy:9\"\n        elif item_type == \"float32\":\n            return item.tobytes(), \"no_header_numpy:10\"\n        elif item_type == \"float64\":\n            return item.tobytes(), \"no_header_numpy:11\"\n        else:\n            raise ValueError(f\"Unsupported data type: {item_type}\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return (item.tobytes(), f\"no_header_numpy:{item.dtype.name}\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        serialized_bytes = item.tobytes()\n        dtype_identifier = f\"no_header_numpy:{self.indice}\"\n        return serialized_bytes, dtype_identifier\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return (item.tobytes(), f\"no_header_numpy:{self.indice}\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        # Convert the NumPy array to a bytes object\n        serialized_bytes = item.tobytes()\n\n        # Generate a dtype identifier string\n        dtype_identifier = f\"no_header_numpy:{self.dtype_indice}\"\n\n        # Update the dtype indice\n        self.dtype_indice += 1\n\n        return serialized_bytes, dtype_identifier\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{item.dtype}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        item = np.ascontiguousarray(item)\n        dtype = np.dtype(item.dtype)\n        dtype_str = self.dtype_to_str(dtype)\n        serialized_item = item.tobytes()\n        return serialized_item, f\"no_header_numpy:{dtype_str}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        # Convert the NumPy array to bytes\n        serialized_data = item.tobytes()\n        # Generate the dtype identifier string\n        dtype_identifier = f\"no_header_numpy:{item.dtype.kind}\"\n        return serialized_data, dtype_identifier\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_str = str(item.dtype)\n        if dtype_str.startswith(\"|S\"):\n            dtype_str = \"str\"\n        dtype_str = dtype_str.replace(\"<\", \"\").replace(\">\", \"\")\n        return item.tobytes(), f\"no_header_numpy:{dtype_str}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        # Convert the NumPy array into a bytes object\n        serialized_data = item.tobytes()\n\n        # Generate a dtype identifier string\n        dtype_identifier = f\"no_header_numpy:{item.dtype.name}\"\n\n        return serialized_data, dtype_identifier\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), \"no_header_numpy:{}\".format(self.indice)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype = self.get_dtype(item)\n        return item.tobytes(), f\"no_header_numpy:{dtype}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        if not isinstance(item, np.ndarray):\n            raise TypeError(f\"Expected NumPy array, got {type(item)}\")\n\n        return item.tobytes(), f\"no_header_numpy:{item.dtype}\"\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        return item.tobytes(), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        return (\n            struct.pack(\"<i\", item.dtype.num)\n            + struct.pack(\"<i\", len(item.shape))\n            + struct.pack(\"<\" + \"i\" * len(item.shape), *item.shape)\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_index = self.dtype_to_index(item.dtype)\n        shape = item.shape\n        data = item.tobytes()\n        serialized_item = struct.pack(f'<I{len(shape)}I{len(data)}s', dtype_index, *shape, data)\n        return serialized_item, None\n    "}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dt_index = self.data_type_to_index(item.dtype)\n        ndim = item.ndim\n        shape = item.shape\n        data = item.tobytes()\n        return (\n            dt_index.to_bytes(1, \"big\")\n            + ndim.to_bytes(1, \"big\")\n            + b\"\".join(s.to_bytes(4, \"big\") for s in shape)\n            + data,\n            None,\n        )"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_index = self._dtype_to_index(item.dtype)\n        ndim = item.ndim\n        shape = item.shape\n        data = item.tobytes()\n        serialized_item = struct.pack(f\"<H{ndim}Q\", dtype_index, *shape) + data\n        return serialized_item, None\n    "}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        return (item.dtype.name.encode() + b\"\\x00\" +\n                item.shape.__bytes__() + b\"\\x00\" +\n                item.tobytes(), None)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type index\n        dtype_index = self.dtype_index_map[item.dtype.name]\n        # Get the number of dimensions\n        num_dims = len(item.shape)\n        # Get the sizes of each dimension\n        dim_sizes = item.shape\n        # Get the array's binary data\n        data = item.tobytes()\n        # Create the serialized bytes object\n        serialized_bytes = struct.pack('B', dtype_index) + struct.pack('B', num_dims)\n        for dim_size in dim_sizes:\n            serialized_bytes += struct.pack('I', dim_size)\n        serialized_bytes += data\n        # Return the serialized bytes object and None for metadata\n        return serialized_bytes, None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type index\n        dtype_index = self._dtype_to_index(item.dtype)\n        # Get the number of dimensions\n        ndim = item.ndim\n        # Get the shape of each dimension\n        shape = item.shape\n        # Get the binary data of the array\n        data = item.tobytes()\n        # Create the serialized bytes object\n        serialized_bytes = struct.pack(f'>B{ndim}Q', dtype_index, *shape) + data\n        # Return the serialized bytes object and None as a placeholder for metadata\n        return serialized_bytes, None\n    "}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the array's data type index and number of dimensions\n        data_type_index = np.dtype(item.dtype).num\n        num_dims = len(item.shape)\n\n        # Serialize the array's data type index, number of dimensions, and each dimension's size\n        serialized_data = struct.pack('<B', data_type_index)\n        serialized_data += struct.pack('<B', num_dims)\n        for dim_size in item.shape:\n            serialized_data += struct.pack('<I', dim_size)\n\n        # Serialize the array's binary data\n        serialized_data += item.tobytes()\n\n        # Return the serialized bytes object and None as a placeholder for potential metadata that is not used in this implementation\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type index\n        data_type_index = self.data_type_to_index(item.dtype)\n        # Get the number of dimensions\n        num_dims = item.ndim\n        # Get the shape of each dimension\n        dims = item.shape\n        # Get the binary data of the array\n        data = item.tobytes()\n        # Serialize the data\n        serialized_data = struct.pack(f'<B{num_dims}I{len(data)}s', data_type_index, *dims, data)\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_idx = np.dtype(item.dtype).num\n        shape = item.shape\n        ndim = len(shape)\n        serialized_bytes = b''\n        serialized_bytes += dtype_idx.to_bytes(1, 'big')\n        serialized_bytes += ndim.to_bytes(1, 'big')\n        for dim in shape:\n            serialized_bytes += dim.to_bytes(4, 'big')\n        serialized_bytes += item.tobytes()\n        return serialized_bytes, None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type index\n        dtype_index = self._dtype_to_index[item.dtype]\n        # Get the number of dimensions\n        num_dims = item.ndim\n        # Get the shape of each dimension\n        dim_sizes = item.shape\n        # Get the binary data of the array\n        data = item.tobytes()\n        # Create the serialized bytes object\n        serialized_bytes = struct.pack(f\"<I{num_dims}I\", dtype_index, *dim_sizes) + data\n        # Return the serialized bytes object and None as a placeholder for potential metadata that is not used in this implementation\n        return serialized_bytes, None\n    "}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type index from the NumPy array's dtype attribute\n        dtype_index = self.dtype_index[str(item.dtype)]\n        # Get the number of dimensions from the NumPy array's ndim attribute\n        ndim = item.ndim\n        # Get the shape of the NumPy array\n        shape = item.shape\n        # Get the binary data of the NumPy array\n        data = item.tobytes()\n        # Create a bytearray object to store the serialized data\n        byte_array = bytearray()\n        # Add the data type index to the bytearray\n        byte_array.extend(struct.pack('B', dtype_index))\n        # Add the number of dimensions to the bytearray\n        byte_array.extend(struct.pack('B', ndim))\n        # Add each dimension size to the bytearray\n        for dim in shape:\n            byte_array.extend(struct.pack('I', dim))\n        # Add the binary data to the bytearray\n        byte_array.extend(data)\n        # Convert the bytearray to a bytes object\n        return bytes(byte_array), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        return (\n            struct.pack(\n                \"<i\",\n                self.data_type_to_index(item.dtype),\n            )\n            + struct.pack(\"<i\", len(item.shape))\n            + struct.pack(\"<\" + \"i\" * len(item.shape), *item.shape)\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the array's data type index\n        dtype_index = self.dtype_to_index[item.dtype.name]\n        # Get the array's number of dimensions\n        ndim = item.ndim\n        # Get the array's shape\n        shape = item.shape\n        # Get the array's binary data\n        data = item.tobytes()\n        # Create a bytearray to store the serialized bytes object\n        byte_array = bytearray()\n        # Add the array's data type index to the bytearray\n        byte_array.extend(struct.pack(\"B\", dtype_index))\n        # Add the array's number of dimensions to the bytearray\n        byte_array.extend(struct.pack(\"B\", ndim))\n        # Add each dimension's size to the bytearray\n        for dim in shape:\n            byte_array.extend(struct.pack(\"I\", dim))\n        # Add the array's binary data to the bytearray\n        byte_array.extend(data)\n        # Convert the bytearray to a bytes object and return it along with None as a placeholder for potential metadata that is not used in this implementation\n        return bytes(byte_array), None\n    "}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        data_type_index = self._data_type_to_index(item.dtype)\n        num_dims = item.ndim\n        dim_sizes = item.shape\n        data = item.tobytes()\n        return (\n            struct.pack(\n                f\"<B{num_dims}Q{len(data)}s\",\n                data_type_index,\n                *dim_sizes,\n                data,\n            ),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        if item.dtype == np.dtype(\"float32\"):\n            dtype_index = 0\n        elif item.dtype == np.dtype(\"float64\"):\n            dtype_index = 1\n        elif item.dtype == np.dtype(\"int32\"):\n            dtype_index = 2\n        elif item.dtype == np.dtype(\"int64\"):\n            dtype_index = 3\n        elif item.dtype == np.dtype(\"bool\"):\n            dtype_index = 4\n        elif item.dtype == np.dtype(\"uint8\"):\n            dtype_index = 5\n        else:\n            raise TypeError(f\"Unsupported dtype: {item.dtype}\")\n\n        serialized_data = bytearray()\n        serialized_data.extend(struct.pack(\"<B\", dtype_index))\n        serialized_data.extend(struct.pack(\"<B\", item.ndim))\n        for dim_size in item.shape:\n            serialized_data.extend(struct.pack(\"<I\", dim_size))\n        serialized_data.extend(item.tobytes())\n        return bytes(serialized_data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type index\n        data_type_index = self.data_type_to_index(item.dtype)\n        # Get the number of dimensions\n        num_dims = item.ndim\n        # Get the sizes of each dimension\n        dim_sizes = item.shape\n        # Get the binary data\n        data = item.tobytes()\n        # Combine the data type index, number of dimensions, dimension sizes, and binary data into a bytes object\n        serialized_bytes = struct.pack('<BI', data_type_index, num_dims)\n        for dim_size in dim_sizes:\n            serialized_bytes += struct.pack('<I', dim_size)\n        serialized_bytes += data\n        # Return the serialized bytes object and None for the metadata\n        return serialized_bytes, None\n    "}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_idx = np.dtype(item.dtype).num\n        shape = item.shape\n        data = item.tobytes()\n        serialized = struct.pack(f\"<B{len(shape)}I\", dtype_idx, *shape) + data\n        return serialized, None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        return (\n            np.array([self.data_type_index, item.ndim] + list(item.shape) + [item.tobytes()], dtype=np.uint8).tobytes(),\n            None\n        )\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n        return state"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n        return state"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            dataset_state = self.dataset.state_dict()\n        else:\n            dataset_state = self.dataset\n\n        return {\n            \"dataset\": dataset_state,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            dataset_state = self.dataset.state_dict()\n        else:\n            dataset_state = None\n\n        return {\n            \"dataset\": dataset_state,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            state = {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self.num_samples_yielded,\n                \"latest_worker_idx\": self.latest_worker_idx,\n            }\n        else:\n            state = {\n                \"dataset\": self.dataset,\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self.num_samples_yielded,\n                \"latest_worker_idx\": self.latest_worker_idx,\n            }\n        return state"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {\n            \"dataset\": self.dataset.state_dict() if isinstance(self.dataset, StreamingDataset) else self.dataset,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n        return state_dict"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self.num_samples_yielded,\n                \"latest_worker_idx\": self.latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset,\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self.num_samples_yielded,\n                \"latest_worker_idx\": self.latest_worker_idx,\n            }\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n        if isinstance(self.dataset, StreamingDataset):\n            state[\"dataset\"] = self.dataset.state_dict()\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        # If the dataset is a StreamingDataset, return a dictionary with the dataset and other attributes\n        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self.num_samples_yielded,\n                \"latest_worker_idx\": self.latest_worker_idx,\n            }\n\n        # If the dataset is not a StreamingDataset, return a dictionary with the dataset and other attributes\n        return {\n            \"dataset\": self.dataset,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            dataset_state = self.dataset.state_dict()\n        else:\n            dataset_state = self.dataset\n\n        state = {\n            \"dataset\": dataset_state,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state\n\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {}\n        state_dict[\"dataset\"] = self.dataset.state_dict()\n        state_dict[\"current_epoch\"] = self.current_epoch\n        state_dict[\"num_samples_yielded\"] = self.num_samples_yielded\n        state_dict[\"latest_worker_idx\"] = self.latest_worker_idx\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            dataset_state = self.dataset.state_dict()\n        else:\n            dataset_state = self.dataset\n\n        return {\n            \"dataset\": dataset_state,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            dataset_state = self.dataset.state_dict()\n        else:\n            dataset_state = self.dataset\n\n        return {\n            \"dataset\": dataset_state,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            dataset_state_dict = self.dataset.state_dict()\n        else:\n            dataset_state_dict = None\n\n        return {\n            \"dataset\": dataset_state_dict,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        # get the state of the dataset\n        if isinstance(self.dataset, StreamingDataset):\n            dataset_state = self.dataset.state_dict()\n        else:\n            dataset_state = None\n\n        # return the state of the StreamingDataLoader\n        return {\n            \"dataset\": dataset_state,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        # Generate the state dictionary\n        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n\n        # Return the state dictionary\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        # TODO:\n        # 1. Get the dataset and batch size from the StreamingDataLoader instance.\n        # 2. If the dataset is a StreamingDataset, then create a dictionary with the following structure:\n        # {\n        #     \"dataset\": {\n        #         \"dataset_type\": \"StreamingDataset\",\n        #         \"dataset_state\": dataset.state_dict()\n        #     },\n        #     \"current_epoch\": self.current_epoch,\n        #     \"num_samples_yielded\": self.num_samples_yielded,\n        #     \"latest_worker_idx\": self.latest_worker_idx\n        # }\n        # 3. If the dataset is not a StreamingDataset, then create a dictionary with the following structure:\n        # {\n        #     \"dataset\": {\n        #         \"dataset_type\": type(dataset).__name__,\n        #         \"dataset_state\": dataset.state_dict()\n        #     },\n        #     \"current_epoch\": self.current_epoch,\n        #     \"num_samples_yielded\": self.num_samples_yielded,\n        #     \"latest_worker_idx\": self.latest_worker_idx\n        # }\n        # 4. Return the created dictionary.\n        #\n        # Hint: You can use the state_dict() method of the dataset to get the state of the dataset.\n        #\n        # Example:\n        # If the dataset is a StreamingDataset, then the returned dictionary should have the following structure:\n        # {\n        #     \"dataset\": {\n        #         \"dataset_type\": \"StreamingDataset\",\n        #         \"dataset_state\": dataset.state_dict()\n        #     },\n        #     \"current_epoch\": self.current_epoch,\n        #     \"num_samples_yielded\": self.num_samples_yielded,\n        #     \"latest_worker_idx\": self.latest_worker_idx\n        # }\n        #\n        # If the dataset is not a StreamingDataset, then"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        try:\n            import torchvision\n            import av\n        except ImportError:\n            raise ImportError(\n                \"VideoSerializer requires torchvision and av to be installed.\"\n            )\n\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit=\"sec\")\n\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not torchvision_available:\n            raise ImportError(\n                \"Torchvision is not installed. Please install torchvision to use this function.\"\n            )\n        if not av_available:\n            raise ImportError(\n                \"PyAV is not installed. Please install PyAV to use this function.\"\n            )\n\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit=\"sec\")\n            os.remove(f.name)\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not torchvision_available:\n            raise ModuleNotFoundError(\"torchvision is not installed\")\n        if not av_available:\n            raise ModuleNotFoundError(\"av is not installed\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit=\"sec\")\n            return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        try:\n            import torchvision\n            import av\n        except ImportError as e:\n            raise ImportError(\n                \"Torchvision and av must be installed to use the VideoSerializer\"\n            ) from e\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name)\n\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not torchvision_installed:\n            raise ModuleNotFoundError(\n                \"torchvision is not installed. Please install it using `pip install torchvision`.\"\n            )\n        if not av_installed:\n            raise ModuleNotFoundError(\n                \"av is not installed. Please install it using `pip install av`.\"\n            )\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit=\"sec\")\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not self.is_installed():\n            raise Exception(\"torchvision and av are not installed.\")\n\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            video, _, _ = torchvision.io.read_video(f.name)\n            f.close()\n            os.unlink(f.name)\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not torchvision_installed:\n            raise RuntimeError(\n                \"Torchvision is not installed. Please install it using `pip install torchvision`.\"\n            )\n        if not av_installed:\n            raise RuntimeError(\n                \"PyAV is not installed. Please install it using `pip install av`.\"\n            )\n\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit=\"sec\")\n            os.remove(f.name)\n\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not torchvision_available:\n            raise ImportError(\"torchvision is not installed. Please install it to use this function.\")\n\n        if not av_available:\n            raise ImportError(\"av is not installed. Please install it to use this function.\")\n\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            f.close()\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit=\"sec\")\n        os.unlink(f.name)\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not (\n            importlib.util.find_spec(\"torchvision\") is not None\n            and importlib.util.find_spec(\"av\") is not None\n        ):\n            raise Exception(\"torchvision and av libraries are not installed\")\n\n        temp_file = tempfile.NamedTemporaryFile(delete=False)\n        temp_file.write(data)\n        temp_file.close()\n\n        return torchvision.io.read_video(temp_file.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not self.is_installed():\n            raise Exception(\"Torchvision is not installed\")\n\n        if not self.is_av_installed():\n            raise Exception(\"AV is not installed\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name)\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        try:\n            import torchvision\n        except ImportError:\n            raise ImportError(\n                \"Torchvision is not installed. Please install it with 'pip install torchvision'.\"\n            )\n\n        try:\n            import av\n        except ImportError:\n            raise ImportError(\n                \"PyAV is not installed. Please install it with 'pip install av'.\"\n            )\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name)\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not (torchvision_available and av_available):\n            raise Exception(\"torchvision or av is not available\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.flush()\n            video = torchvision.io.read_video(f.name, pts_unit=\"sec\")\n            return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not (\n            hasattr(torchvision, \"__version__\")\n            and torchvision.__version__ >= \"0.15.0\"\n        ):\n            raise RuntimeError(\n                \"torchvision 0.15.0 or higher is required for VideoSerializer\"\n            )\n\n        if not hasattr(av, \"__version__\") or av.__version__ < \"8.0.0\":\n            raise RuntimeError(\n                \"PyAV 8.0.0 or higher is required for VideoSerializer\"\n            )\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name)\n\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not self.is_installed():\n            raise RuntimeError(\"torchvision and av are not installed\")\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name)\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not self._is_installed():\n            raise Exception(\"torchvision and av are not installed\")\n\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit='sec')\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not self._is_installed:\n            raise Exception(\"torchvision is not installed\")\n\n        if not self._is_av_installed:\n            raise Exception(\"av is not installed\")\n\n        # Write the data to a temporary file\n        with tempfile.NamedTemporaryFile(suffix='.mp4') as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit='sec')\n            return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not self.is_installed():\n            raise Exception(\"torchvision and av are not installed\")\n\n        f = tempfile.NamedTemporaryFile(delete=False)\n        f.write(data)\n        f.close()\n        video, _, _ = torchvision.io.read_video(f.name, pts_unit='sec')\n        os.remove(f.name)\n        return video\n\n    "}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        try:\n            import torchvision\n            import av\n        except ImportError as e:\n            raise ImportError(\n                \"VideoSerializer requires torchvision and av to be installed. Please install them using pip install torchvision av\") from e\n\n        with tempfile.NamedTemporaryFile(mode='w+b', suffix='.mp4') as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit='sec')\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not self.check_libraries():\n            raise Exception(\"torchvision and av are required to use the VideoSerializer\")\n\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            video, _, _ = torchvision.io.read_video(f.name)\n            os.remove(f.name)\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not (self.torchvision_installed and self.av_installed):\n            raise Exception(\"torchvision and av must be installed\")\n\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            video, _, _ = torchvision.io.read_video(f.name)\n            os.unlink(f.name)\n            return video\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return\n\n        while self._should_write:\n            self.write_chunk()\n\n        self.write_chunks_index()\n\n        self._is_done = True\n\n        return self._chunks_paths"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return\n\n        self.write_chunks_index()\n\n        self._is_done = True\n\n        return self._written_chunks"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return\n\n        for chunk in self._should_write:\n            self.write_chunk(chunk)\n\n        self.write_chunks_index()\n\n        self._is_done = True\n\n        return self._written_chunks"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write:\n            for chunk in self._chunks:\n                self.write_chunk(chunk)\n            self.write_chunks_index()\n        self._is_done = True\n        return self._chunk_paths"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return\n\n        self._is_done = True\n        self.write_chunks_index()\n        return self.file_paths"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if not self.filled:\n            self.write_chunks_index()\n            self._is_done = True\n        return self._chunks"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if not self._should_write:\n            return []\n\n        if self.filled:\n            return []\n\n        self._is_done = True\n\n        self.write_chunks_index()\n\n        return self._chunks_paths"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if not self._should_write:\n            return []\n\n        self.write_chunks_index()\n        self.write_chunk()\n        self._is_done = True\n        return self._chunk_paths\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return self.files\n\n        if self._should_write:\n            self.write_chunks_index()\n\n        self._is_done = True\n        return self.files\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return\n\n        self.write_chunks()\n        self.write_chunks_index()\n        self._is_done = True\n        return self.chunks\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        if self._should_write:\n            self.write_chunk()\n\n        self.write_chunks_index()\n        self._is_done = True\n\n        return self.chunks_paths\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return self._chunks\n\n        while self._should_write:\n            self.write_chunk()\n\n        self.write_chunks_index()\n        self._is_done = True\n        return self._chunks\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return self.file_paths\n\n        if self._should_write:\n            for chunk_name, chunk in self.chunks.items():\n                self.write_chunk(chunk_name, chunk)\n\n            self.write_chunks_index()\n\n        self._is_done = True\n        return self.file_paths\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if not self._should_write:\n            return []\n\n        self.write_chunk(self._buffer)\n        self.write_chunks_index()\n\n        self._is_done = True\n\n        return self.chunk_paths\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        self.write_chunks_index()\n\n        self._is_done = True\n\n        return self.chunks\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return\n\n        self._is_done = True\n\n        if not self._should_write:\n            return []\n\n        self.write_chunks_index()\n        return self.write_chunk()\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return self.file_paths\n\n        self.write_chunks_index()\n        self.write_chunks()\n        self._is_done = True\n\n        return self.file_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        self._should_write = False\n\n        self.write_chunks_index()\n\n        self._is_done = True\n\n        return self.chunks_paths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return self.files\n\n        self.write_chunks_index()\n        self._is_done = True\n        return self.files\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        if not self._should_write:\n            self._is_done = True\n            return []\n\n        self.write_chunks_index()\n        self._is_done = True\n        return self.chunks\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.dataset.load_state_dict(obj)\n        self.dataset.prepare_for_resuming()\n        self.dataset.num_samples_yielded = self.num_samples_yielded\n        self.dataset.current_epoch = self.current_epoch\n        self.dataset.latest_worker_idx = self.latest_worker_idx\n        self.dataset.prepare_for_resuming()\n        self.dataset.num_samples_yielded = self.num_samples_yielded\n        self.dataset.current_epoch = self.current_epoch\n        self.dataset.latest_worker_idx = self.latest_worker_idx\n        self.dataset.prepare_for_resuming()\n        self.dataset.num_samples_yielded = self.num_samples_yielded\n        self.dataset.current_epoch = self.current_epoch\n        self.dataset.latest_worker_idx = self.latest_worker_idx\n        self.dataset.prepare_for_resuming()\n        self.dataset.num_samples_yielded = self.num_samples_yielded\n        self.dataset.current_epoch = self.current_epoch\n        self.dataset.latest_worker_idx = self.latest_worker_idx\n        self.dataset.prepare_for_resuming()\n        self.dataset.num_samples_yielded = self.num_samples_yielded\n        self.dataset.current_epoch = self.current_epoch\n        self.dataset.latest_worker_idx = self.latest_worker_idx\n        self.dataset.prepare_for_resuming()\n        self.dataset.num_samples_yielded = self.num_samples_yielded\n        self.dataset.current_epoch = self.current_epoch\n        self"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj[\"epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.epoch = obj[\"epoch\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.epoch = obj[\"epoch\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.epoch = obj[\"epoch\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.epoch = obj[\"epoch\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.epoch = obj[\"epoch\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.epoch = obj[\"epoch\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.epoch = obj[\"epoch\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.epoch = obj[\"epoch\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n        self.num_samples_yielded = obj"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj[\"epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            for dataset in self.dataset.datasets:\n                if isinstance(dataset, StreamingDataset):\n                    dataset.load_state_dict(obj)\n                else:\n                    raise RuntimeError(\n                        \"The dataset is neither a StreamingDataset nor a CombinedStreamingDataset.\"\n                    )\n        else:\n            raise RuntimeError(\n                \"The dataset is neither a StreamingDataset nor a CombinedStreamingDataset.\"\n            )\n\n        self._prepare_for_epoch(self.epoch)"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.num_yielded = obj[\"num_yielded\"]\n        self.latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.dataset.load_state_dict(obj)\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj)\n        else:\n            raise RuntimeError(\n                \"Unknown dataset type. Please make sure the dataset is a StreamingDataset or a CombinedStreamingDataset.\"\n            )"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj['current_epoch']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.latest_worker_idx = obj['latest_worker_idx']\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj['dataset_state'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj['dataset_state'])\n        else:\n            raise RuntimeError(\"Unsupported dataset type\")\n\n        # Adjust internal iterators and flags\n        self.dataset.reset_iterators()\n        self.dataset.reset_flags()\n\n        # Adjust internal iterators and flags\n        self.dataset.reset_iterators()\n        self.dataset.reset_flags()\n\n        # Adjust internal iterators and flags\n        self.dataset.reset_iterators()\n        self.dataset.reset_flags()\n\n        # Adjust internal iterators and flags\n        self.dataset.reset_iterators()\n        self.dataset.reset_flags()\n\n        # Adjust internal iterators and flags\n        self.dataset.reset_iterators()\n        self.dataset.reset_flags()\n\n        # Adjust internal iterators and flags\n        self.dataset.reset_iterators()\n        self.dataset.reset_flags()\n\n        # Adjust internal iterators and flags\n        self.dataset.reset_iterators()\n        self.dataset.reset_flags()\n\n        # Adjust internal iterators and flags\n        self.dataset.reset_iterators()\n        self.dataset.reset_flags()\n\n        # Adjust internal iterators and flags\n        self.dataset.reset_iterators()\n        self.dataset.reset_flags()\n\n        # Adjust internal iterators and flags\n        self.dataset.reset_iterators()\n        self.dataset.reset_flags()\n\n        # Adjust internal iterators and flags\n        self.dataset.reset_iterators()\n        self.dataset.reset_flags()\n\n        # Adjust"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n        self.dataset.load_state_dict(obj)\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj)\n        else:\n            raise RuntimeError(\n                \"The dataset is neither a StreamingDataset nor a CombinedStreamingDataset.\"\n            )\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset_state\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            for dataset in self.dataset.datasets:\n                if isinstance(dataset, StreamingDataset):\n                    dataset.load_state_dict(obj[\"dataset_state\"])\n        else:\n            raise RuntimeError(\n                \"The dataset is neither StreamingDataset nor CombinedStreamingDataset.\"\n            )\n        self._prepare_for_resuming()\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_idx = obj[\"latest_worker_idx\"]\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The dataset is neither a StreamingDataset nor a CombinedStreamingDataset\"\n            )\n\n        # Prepare the DataLoader for resuming\n        self.dataset_iter = iter(self.dataset)\n        self.dataset_iter = itertools.cycle(self.dataset_iter)\n        self.dataset_iter = itertools.islice(self.dataset_iter, self.num_samples_yielded, None)\n        self.dataset_iter = itertools.chain(self.dataset_iter, self.dataset_iter)\n        self.dataset_iter = itertools.islice(self.dataset_iter, self.num_samples_yielded, None)\n        self.dataset_iter = itertools.cycle(self.dataset_iter)\n        self.dataset_iter = itertools.islice(self.dataset_iter, self.num_samples_yielded, None)\n\n        self.dataset_iter = iter(self.dataset)\n        self.dataset_iter = itertools.cycle(self.dataset_iter)\n        self.dataset_iter = itertools.islice(self.dataset_iter, self.num_samples_yielded, None)\n        self.dataset_iter = itertools.chain(self.dataset_iter, self.dataset_iter)\n        self.dataset_iter = itertools.islice(self.dataset_iter, self.num_samples_yielded, None)\n        self.dataset_iter = itertools.cycle(self.dataset_iter)\n        self.dataset_iter = itertools"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj['epoch']\n        self.num_samples_yielded = obj['num_samples_yielded']\n        self.latest_worker_idx = obj['latest_worker_idx']\n        self.dataset.load_state_dict(obj)\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.prepare_for_resuming(self.num_samples_yielded)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.prepare_for_resuming(self.num_samples_yielded)\n        else:\n            raise RuntimeError(\"Dataset must be either StreamingDataset or CombinedStreamingDataset\")\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_idx = obj[\"latest_worker_idx\"]\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset_state\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset_state\"])\n        else:\n            raise RuntimeError(\"Unsupported dataset type.\")\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_idx = obj[\"latest_worker_idx\"]\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset_state\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            for i, dataset in enumerate(self.dataset.datasets):\n                if isinstance(dataset, StreamingDataset):\n                    dataset.load_state_dict(obj[\"dataset_state\"][i])\n        else:\n            raise RuntimeError(\"Unsupported dataset type\")\n        self.dataset.prepare_for_resume()\n        self.prepare_for_resume()\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"epoch\"]\n        self.num_yielded = obj[\"num_yielded\"]\n        self.latest_worker_idx = obj[\"latest_worker_idx\"]\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\"Dataset type not supported.\")\n        self._prepare_for_epoch(self.current_epoch)\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_idx = obj[\"latest_worker_idx\"]\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset_state\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset_state\"])\n        else:\n            raise RuntimeError(\"Dataset type not supported\")\n\n        self.dataset.prepare_for_resume()\n        self.dataset.prepare_for_resume()\n\n        self.iterator = iter(self.dataset)\n        self.iterator_next = self.iterator.__next__\n        self.iterator_next()\n\n        self.resume = True\n        self.num_samples_yielded = 0\n\n        self.dataset.prepare_for_resume()\n        self.dataset.prepare_for_resume()"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The dataset is neither a StreamingDataset nor a CombinedStreamingDataset\"\n            )\n        self.dataset.prepare_for_resuming()\n        self.dataset_iterators = [iter(self.dataset)]\n        self.dataset_iterators_indices = [0]\n        self.dataset_iterators_lengths = [len(self.dataset)]\n        self.dataset_iterators_sizes = [len(self.dataset)]\n        self.dataset_iterators_epochs = [self.current_epoch]\n        self.dataset_iterators_latest_worker_index = [self.latest_worker_index]\n        self.dataset_iterators_num_samples_yielded = [self.num_samples_yielded]\n        self.dataset_iterators_num_samples_yielded_per_epoch = [self.num_samples_yielded]\n        self.dataset_iterators_num_samples_yielded_per_epoch_per_worker = [\n            self.num_samples_yielded\n        ]\n        self.dataset_iterators_num_samples_yielded_per_epoch_per_worker_per_batch = [\n            self.num_samples_yielded\n        ]\n        self.dataset_iterators_num_samples_yielded_per_epoch_per_worker_per_batch_per_worker = [\n            self.num_samples_yielded\n        ]\n        self.dataset_iterators_"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_idx = obj[\"latest_worker_idx\"]\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset_state\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            for dataset in self.dataset.datasets:\n                if isinstance(dataset, StreamingDataset):\n                    dataset.load_state_dict(obj[\"dataset_state\"])\n        else:\n            raise RuntimeError(\n                \"The dataset is neither a StreamingDataset nor a CombinedStreamingDataset.\"\n            )\n\n        self.dataset.prepare_for_resume()\n\n        self.dataset_iter = iter(self.dataset)\n        self.dataset_iter_next = next(self.dataset_iter)\n        self.dataset_iter_next_called = False\n        self.dataset_iter_next_returned = False\n\n        self.dataset_iter_next_returned_empty = False\n\n        self.dataset_iter_next_returned_empty_called = False\n\n        self.dataset_iter_next_returned_empty_returned = False\n\n        self.dataset_iter_next_returned_empty_returned_called = False\n\n        self.dataset_iter_next_returned_empty_returned_called_called = False\n\n        self.dataset_iter_next_returned_empty_returned_called_called_called = False\n\n        self.dataset_iter_next_returned_empty_returned_called_called_called_called = False\n\n        self.dataset_iter_next_returned_empty_returned_called_called_called_called_called = False\n\n        self.dataset_iter_next_returned_empty_returned_called_called_called_called_called_called = False\n\n        self.dataset_iter_next_returned_empty_returned_called_called"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.epoch = obj[\"epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.dataset.load_state_dict(obj)\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.prepare_for_resume()\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.prepare_for_resume()\n        else:\n            raise RuntimeError(\n                \"StreamingDataLoader.load_state_dict() should only be called on StreamingDataset or CombinedStreamingDataset\"\n            )\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_idx = obj[\"latest_worker_idx\"]\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj)\n        else:\n            raise RuntimeError(\"The dataset is neither a StreamingDataset nor a CombinedStreamingDataset\")\n        self.dataset_iter = iter(self.dataset)\n        self.epoch_iter = iter(self.dataset_iter)\n        self.current_epoch_iter = self.epoch_iter\n        self.is_epoch_end = False\n        self.is_iter_end = False\n        self.is_epoch_start = True\n        self.is_iter_start = True\n        self.is_first_iter = True\n        self.is_last_iter = False\n        self.is_last_epoch = False\n        self.is_first_epoch = True\n        self.is_first_iter_in_epoch = True\n        self.is_last_iter_in_epoch = False\n        self.is_last_epoch_in_epoch = False\n        self.is_first_epoch_in_epoch = True\n        self.is_first_iter_in_epoch = True\n        self.is_last_iter_in_epoch = False\n        self.is_last_epoch_in_epoch = False\n        self.is_first_epoch_in_epoch = True\n        self.is_last_epoch_in_epoch = False\n        self.is_first_iter_in_epoch = True\n        self.is_last_iter_in_epoch = False\n        self.is_last_epoch_in_epoch = False\n        self.is_first_epoch_in_epoch = True"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj)\n        else:\n            raise RuntimeError(\n                \"StreamingDataLoader.load_state_dict() only supports StreamingDataset or CombinedStreamingDataset\"\n            )\n        self._prepare_for_epoch(self.current_epoch)\n        self._set_epoch(self.current_epoch)\n        self.dataset._set_epoch(self.current_epoch)\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_idx = obj[\"latest_worker_idx\"]\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj)\n        else:\n            raise RuntimeError(\n                f\"The dataset {self.dataset} is not supported. \"\n                \"Supported datasets are StreamingDataset and CombinedStreamingDataset\"\n            )\n\n        self.dataset.prepare_for_resuming()\n\n        self.dataset_iterator = iter(self.dataset)\n        self.dataset_iterator_idx = 0\n\n        self.num_samples_yielded = 0\n        self.current_epoch += 1\n\n        self.dataset.prepare_for_resuming()\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            for dataset in self.dataset.datasets:\n                if isinstance(dataset, StreamingDataset):\n                    dataset.load_state_dict(obj)\n                else:\n                    raise RuntimeError(\n                        f\"Dataset {type(dataset)} is not supported for streaming data loaders.\"\n                    )\n        else:\n            raise RuntimeError(\n                f\"Dataset {type(self.dataset)} is not supported for streaming data loaders.\"\n            )\n\n        self.current_epoch = obj[\"current_epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_index = obj[\"latest_worker_index\"]\n\n        # Adjust internal iterators and flags\n        if self.latest_worker_index is not None:\n            self.iterators = [\n                iter(self.dataset) for _ in range(self.latest_worker_index + 1)\n            ]\n            self.worker_indices = list(range(self.latest_worker_index + 1))\n        else:\n            self.iterators = [iter(self.dataset)]\n            self.worker_indices = [0]\n\n        self.dataset.is_streaming = False\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers, batch_size, num_samples_yielded)\n\n        return {\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"num_samples_yielded\": num_samples_yielded,\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None and num_samples_yielded is None:\n            return {}\n        if self.iterator is None:\n            return {\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n                \"num_samples_yielded\": num_samples_yielded,\n            }\n        return self.iterator.state_dict(num_workers, batch_size, num_samples_yielded)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None and num_samples_yielded is None:\n            return {}\n        if self.iterator is None:\n            return {\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n                \"num_samples_yielded\": num_samples_yielded,\n            }\n        return self.iterator.state_dict(num_workers, batch_size, num_samples_yielded)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is None:\n            return self._datasets[0].state_dict(num_workers, batch_size, num_samples_yielded)\n\n        return self._iterator.state_dict()"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is None:\n            return self._datasets[0].state_dict(num_workers, batch_size, num_samples_yielded)\n\n        return self._iterator.state_dict()"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n        if self._iterator is None:\n            return {\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n                \"num_samples_yielded\": num_samples_yielded,\n            }\n        return self._iterator.state_dict(num_workers, batch_size, num_samples_yielded)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None:\n            return {}\n\n        if num_samples_yielded is None:\n            return self.iterator.state_dict()\n\n        return {\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"num_samples_yielded\": num_samples_yielded,\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None and num_samples_yielded is None:\n            return {}\n        elif self.iterator is None and num_samples_yielded is not None:\n            return {\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n                \"num_samples_yielded\": num_samples_yielded,\n            }\n        else:\n            return self.iterator.state_dict(num_workers, batch_size, num_samples_yielded)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is None:\n            return {\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n                \"num_samples_yielded\": num_samples_yielded,\n            }\n\n        return self._iterator.state_dict()"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self.iterator is not None:\n            return self.iterator.state_dict(num_workers, batch_size, num_samples_yielded)\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"datasets\": [dataset.state_dict(num_workers, batch_size, num_samples_yielded) for dataset in self.datasets],\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict()\n\n        assert num_samples_yielded is not None\n        return {\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"num_samples_yielded\": num_samples_yielded,\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None and num_samples_yielded is None:\n            return {}\n        if self.iterator is None:\n            return {\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n                \"num_samples_yielded\": num_samples_yielded,\n            }\n        return {\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"num_samples_yielded\": num_samples_yielded,\n            **self.iterator.state_dict(),\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None and num_samples_yielded is None:\n            return {}\n        elif self.iterator is not None:\n            return self.iterator.state_dict(num_workers, batch_size, num_samples_yielded)\n        else:\n            return {\n                \"datasets\": [\n                    dataset.state_dict(num_workers, batch_size, num_samples_yielded)\n                    for dataset in self.datasets\n                ]\n            }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None and num_samples_yielded is None:\n            return {}\n        elif self.iterator is None and num_samples_yielded is not None:\n            return {\n                \"num_samples_yielded\": num_samples_yielded,\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n            }\n        else:\n            if self.iterator is not None:\n                return self.iterator.state_dict()\n            else:\n                return {\n                    \"num_samples_yielded\": num_samples_yielded,\n                    \"num_workers\": num_workers,\n                    \"batch_size\": batch_size,\n                }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None:\n            return {}\n\n        if num_samples_yielded is not None:\n            return {\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n                \"num_samples_yielded\": num_samples_yielded,\n            }\n\n        return {\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"num_samples_yielded\": self.num_samples_yielded,\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self.iterator is not None:\n            return self.iterator.state_dict(num_workers, batch_size, num_samples_yielded)\n\n        return {\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"num_samples_yielded\": num_samples_yielded,\n            \"datasets\": [dataset.state_dict() for dataset in self.datasets],\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self.iterator is not None:\n            return self.iterator.state_dict()\n\n        state_dicts = []\n        for dataset in self.datasets:\n            state_dicts.append(dataset.state_dict())\n\n        return {\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"num_samples_yielded\": num_samples_yielded,\n            \"datasets\": state_dicts,\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self.iterator is not None:\n            return self.iterator.state_dict(num_workers, batch_size, num_samples_yielded)\n\n        state_dicts = [\n            dataset.state_dict(num_workers, batch_size, num_samples_yielded) for dataset in self.datasets\n        ]\n        return {\n            \"datasets\": state_dicts,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"num_samples_yielded\": num_samples_yielded,\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None:\n            return {}\n        if num_samples_yielded is not None:\n            return {\n                \"datasets\": [dataset.state_dict(num_workers, batch_size, num_samples_yielded) for dataset in self.datasets],\n                \"iterator\": self.iterator.state_dict(),\n            }\n        else:\n            return {\"datasets\": [dataset.state_dict(num_workers, batch_size) for dataset in self.datasets]}"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None and num_samples_yielded is None:\n            return {}\n        elif self.iterator is None:\n            return {\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n                \"num_samples_yielded\": num_samples_yielded,\n            }\n        else:\n            if isinstance(self.iterator, torch.utils.data.IterableDataset):\n                return self.iterator.state_dict()\n            else:\n                return self.iterator.state_dict(num_workers, batch_size, num_samples_yielded)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self.dataset_list[0].load_state_dict(state_dict)\n        self.dataset_list[1].load_state_dict(state_dict)\n        self.dataset_list[2].load_state_dict(state_dict)\n        self.dataset_list[3].load_state_dict(state_dict)\n        self.dataset_list[4].load_state_dict(state_dict)\n        self.dataset_list[5].load_state_dict(state_dict)\n        self.dataset_list[6].load_state_dict(state_dict)\n        self.dataset_list[7].load_state_dict(state_dict)\n        self.dataset_list[8].load_state_dict(state_dict)\n        self.dataset_list[9].load_state_dict(state_dict)\n        self.dataset_list[10].load_state_dict(state_dict)\n        self.dataset_list[11].load_state_dict(state_dict)\n        self.dataset_list[12].load_state_dict(state_dict)\n        self.dataset_list[13].load_state_dict(state_dict)\n        self.dataset_list[14].load_state_dict(state_dict)\n        self.dataset_list[15].load_state_dict(state_dict)\n        self.dataset_list[16].load_state_dict(state_dict)\n        self.dataset_list[17].load_state_dict(state_dict)\n        self.dataset_list[18].load_state_dict(state_dict)\n        self.dataset_list[19].load_state_dict(state_dict)\n        self.dataset_list[20].load_state_dict(state_dict)\n        self.dataset_list[21].load_state_dict(state_dict)\n        self.dataset_list[22].load_state_dict(state_dict)\n        self.dataset_list[23].load_state_dict(state_dict)\n        self.dataset_"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset_name, dataset_state in state_dict.items():\n            if dataset_name == \"num_samples_yielded\":\n                self.num_samples_yielded = dataset_state\n            else:\n                dataset = getattr(self, dataset_name)\n                dataset.load_state_dict(dataset_state)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self.dataset_state_dict = state_dict[\"dataset_state_dict\"]\n        self.num_samples_yielded = state_dict[\"num_samples_yielded\"]\n\n        for dataset_name, dataset_state in self.dataset_state_dict.items():\n            self.datasets[dataset_name].load_state_dict(dataset_state)\n\n        if self.num_samples_yielded is not None:\n            self.streaming_dataloader.num_samples_yielded = self.num_samples_yielded"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self.dataset1.load_state_dict(state_dict[\"dataset1\"])\n        self.dataset2.load_state_dict(state_dict[\"dataset2\"])\n        if self.streaming_dataloader is not None:\n            self.streaming_dataloader.num_yielded = state_dict[\"num_yielded\"]\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self.dataset_state_dicts = state_dict[\"dataset_state_dicts\"]\n        self.streaming_dataloader_state_dict = state_dict[\"streaming_dataloader_state_dict\"]\n        self.n_samples_yielded = state_dict[\"n_samples_yielded\"]\n        for dataset_state_dict in self.dataset_state_dicts:\n            dataset = dataset_state_dict[\"dataset\"]\n            dataset.load_state_dict(dataset_state_dict)\n        if self.streaming_dataloader is not None:\n            self.streaming_dataloader.load_state_dict(self.streaming_dataloader_state_dict)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset_name, dataset_state in state_dict.items():\n            if dataset_name == \"streaming_loader\":\n                self.streaming_loader.num_samples_yielded = dataset_state[\"num_samples_yielded\"]\n            else:\n                self.datasets[dataset_name].load_state_dict(dataset_state)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for key, value in state_dict.items():\n            if key == \"dataset_state_dict\":\n                for dataset_key, dataset_value in value.items():\n                    if dataset_key == \"dataset\":\n                        self.datasets[0].load_state_dict(dataset_value)\n                    elif dataset_key == \"streaming_dataset\":\n                        self.datasets[1].load_state_dict(dataset_value)\n            elif key == \"num_yielded\":\n                self.num_yielded = value\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self.dataset_list = state_dict[\"dataset_list\"]\n        self.streaming_dataloader_num_samples = state_dict[\"streaming_dataloader_num_samples\"]\n        for dataset_idx in range(len(self.dataset_list)):\n            self.dataset_list[dataset_idx].load_state_dict(state_dict[\"dataset_list\"][dataset_idx])\n        self.streaming_dataloader.num_samples = self.streaming_dataloader_num_samples"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset in self.datasets:\n            dataset.load_state_dict(state_dict[dataset.__class__.__name__])\n\n        if hasattr(self.datasets[0], \"yielded_samples\"):\n            self.yielded_samples = state_dict[\"yielded_samples\"]\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset_name, dataset_state in state_dict.items():\n            if dataset_name == \"streaming_dataset\":\n                self.streaming_dataset.load_state_dict(dataset_state)\n                self.streaming_dataset.num_yielded_samples = dataset_state[\"num_yielded_samples\"]\n            else:\n                self.datasets[dataset_name].load_state_dict(dataset_state)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Load the state of the CombinedStreamingDataset\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.num_samples_yielded = state_dict[\"num_samples_yielded\"]\n\n        # Load the state of the datasets within the CombinedStreamingDataset\n        for dataset_name, dataset_state_dict in state_dict[\"datasets\"].items():\n            self.datasets[dataset_name].load_state_dict(dataset_state_dict)\n\n        # If the CombinedStreamingDataset has a streaming dataloader, update the number of samples yielded by the streaming dataloader\n        if self.streaming_dataloader is not None:\n            self.streaming_dataloader.num_samples_yielded = self.num_samples_yielded\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Load state of datasets\n        for dataset_name in state_dict:\n            if dataset_name in self.datasets:\n                self.datasets[dataset_name].load_state_dict(state_dict[dataset_name])\n\n        # Update number of samples yielded by streaming dataloader\n        if self.streaming_dataloader is not None:\n            self.streaming_dataloader.num_samples_yielded = state_dict[\"num_samples_yielded\"]\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self.dataset1.load_state_dict(state_dict[\"dataset1\"])\n        self.dataset2.load_state_dict(state_dict[\"dataset2\"])\n        if \"num_samples_yielded\" in state_dict:\n            self.num_samples_yielded = state_dict[\"num_samples_yielded\"]\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self.dataset1.load_state_dict(state_dict)\n        self.dataset2.load_state_dict(state_dict)\n        if \"num_samples_yielded\" in state_dict:\n            self.num_samples_yielded = state_dict[\"num_samples_yielded\"]\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self.dataset_1.load_state_dict(state_dict[\"dataset_1\"])\n        self.dataset_2.load_state_dict(state_dict[\"dataset_2\"])\n\n        self.streaming_dataloader.num_samples_yielded = state_dict[\"num_samples_yielded\"]\n\n        self.streaming_dataloader.dataset = self\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self.datasets[0].load_state_dict(state_dict[self.datasets[0].__class__.__name__])\n        self.datasets[1].load_state_dict(state_dict[self.datasets[1].__class__.__name__])\n\n        if self.datasets[0].__class__.__name__ == \"StreamingDataset\":\n            self.datasets[0].yielded_samples = state_dict[\"yielded_samples\"]\n        if self.datasets[1].__class__.__name__ == \"StreamingDataset\":\n            self.datasets[1].yielded_samples = state_dict[\"yielded_samples\"]\n\n        self.yielded_samples = state_dict[\"yielded_samples\"]\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Load the state of each dataset\n        for dataset_name, dataset_state in state_dict.items():\n            if dataset_name in self.datasets:\n                self.datasets[dataset_name].load_state_dict(dataset_state)\n\n        # Update the number of samples yielded by the streaming dataloader\n        if self.streaming_dataloader is not None:\n            self.streaming_dataloader.num_samples_yielded = state_dict[\"num_samples_yielded\"]\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self.dataset_state_dict = state_dict\n        for dataset_name, dataset_state_dict in self.dataset_state_dict.items():\n            if dataset_name == \"streaming_dataset\":\n                self.streaming_dataset.load_state_dict(dataset_state_dict)\n                self.num_samples_yielded = dataset_state_dict[\"num_samples_yielded\"]\n            else:\n                self.datasets[dataset_name].load_state_dict(dataset_state_dict)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self.dataset_list = []\n        self.dataset_lengths = []\n        self.streaming_dataloader = None\n        self.num_samples_yielded = 0\n\n        for dataset_state in state_dict[\"dataset_list\"]:\n            dataset = dataset_state[\"dataset\"]\n            dataset_length = dataset_state[\"dataset_length\"]\n            dataset.load_state_dict(dataset_state)\n            self.dataset_list.append(dataset)\n            self.dataset_lengths.append(dataset_length)\n\n        if state_dict[\"streaming_dataloader\"] is not None:\n            self.streaming_dataloader = state_dict[\"streaming_dataloader\"]\n            self.num_samples_yielded = state_dict[\"num_samples_yielded\"]\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset in self.datasets:\n            dataset.load_state_dict(state_dict[dataset.name])\n        if hasattr(self, \"streaming_dataloader\"):\n            self.streaming_dataloader.num_yielded = state_dict[\"num_yielded\"]\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return None\n    if isinstance(dir_path, Dir):\n        return dir_path\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=dir_path)\n    if dir_"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return None\n    if isinstance(dir_path, Dir):\n        return dir_path\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path[7:])\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path[2:])\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path[3:])\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path[2:])\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path[3:])\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path[2:])\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path[3:])\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path[2:])\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path[3:])\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path[2:])\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path[3:])\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path[2:])\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path[3:])\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path[2:])\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path["}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return None\n    elif isinstance(dir_path, Dir):\n        return dir_path\n    elif dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n    else:\n        if dir_path.startswith(\"/\"):\n            return Dir(path=dir_path)\n        else:\n            return Dir(path=os.path.join(os.getcwd(), dir_path))"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return None\n    if isinstance(dir_path, Dir):\n        return dir_path\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\".\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\".\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~/\"):\n        return Dir(path"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir(path=\".\")\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(path=dir_path)\n\n    return Dir(path=dir_path)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\".\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"..\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\".\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"..\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\".\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"..\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"/\"):\n       "}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return None\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"file://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path)\n\n    if dir_path.start"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return None\n    if isinstance(dir_path, Dir):\n        return dir_path\n    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"file://\"):\n            return Dir(path=dir_path[7:])\n        elif dir_path.startswith(\"/\"):\n            return Dir(path=dir_path)\n        elif dir_path.startswith(\"./\"):\n            return Dir(path=dir_path[2:])\n        elif dir_path.startswith(\"../\"):\n            return Dir(path=dir_path[3:])\n        elif dir_path.startswith(\"./\"):\n            return Dir(path=dir_path[2:])\n        elif dir_path.startswith(\"../\"):\n            return Dir(path=dir_path[3:])\n        elif dir_path.startswith(\"./\"):\n            return Dir(path=dir_path[2:])\n        elif dir_path.startswith(\"../\"):\n            return Dir(path=dir_path[3:])\n        elif dir_path.startswith(\"./\"):\n            return Dir(path=dir_path[2:])\n        elif dir_path.startswith(\"../\"):\n            return Dir(path=dir_path[3:])\n        elif dir_path.startswith(\"./\"):\n            return Dir(path=dir_path[2:])\n        elif dir_path.startswith(\"../\"):\n            return Dir(path=dir_path[3:])\n        elif dir_path.startswith(\"./\"):\n            return Dir(path=dir_path[2:])\n        elif dir_path.startswith(\"../\"):\n            return Dir(path=dir_path[3:])\n        elif dir_path.startswith(\"./\"):\n            return Dir(path=dir_path[2:])\n        elif dir_path.startswith(\"../\""}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return None\n    if isinstance(dir_path, Dir):\n        return dir_path\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"http\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path[7:])\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path[2:])\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path[3:])\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=os.path.expanduser(dir_path))\n    return Dir(path=dir_path)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path[7:])\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path[2:])\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path[3:])\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path[2:])\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path[3:])\n\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path[1:]))\n\n    return Dir(path=dir_path)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir(path=None, url=None)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=None, url=dir_path)\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(path=None, url=dir_path)\n\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path[7:], url=None)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(path=dir_path, url=None)\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path, url=None)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path[2:], url=None)\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path[3:], url=None)\n\n    return Dir(path=dir_path, url=None)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return None\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path[7:])\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return None\n    if isinstance(dir_path, Dir):\n        return dir_path\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path[3:]))\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=os.path.join(os.path.expanduser(\"~\"), dir_path[2:]))\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path))\n    if dir_path.startswith(\".\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path))\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"ftp://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"file://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"hdfs://\"):\n        return Dir(url=dir_path)\n    if dir"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir(path=None, url=None)\n    if isinstance(dir_path, Dir):\n        return dir_path\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=None, url=dir_path)\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(path=None, url=dir_path)\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path, url=None)\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path), url=None)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path[2:]), url=None)\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path), url=None)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path[2:]), url=None)\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path), url=None)\n    if dir_path.startswith(\"/\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path), url=None)\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path), url=None)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path[2:]), url=None)\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.join(os.getcwd(),"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return None\n    elif isinstance(dir_path, Dir):\n        return dir_path\n    elif dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n    elif dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n    elif dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n    elif dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n    else:\n        return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir(path=None, url=None, prefix=None)\n    elif isinstance(dir_path, Dir):\n        return dir_path\n    elif dir_path.startswith(\"s3://\"):\n        return Dir(path=None, url=dir_path, prefix=None)\n    elif dir_path.startswith(\"/\"):\n        return Dir(path=dir_path, url=None, prefix=None)\n    elif dir_path.startswith(\"./\"):\n        return Dir(path=dir_path, url=None, prefix=None)\n    elif dir_path.startswith(\"../\"):\n        return Dir(path=dir_path, url=None, prefix=None)\n    elif dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(path=None, url=dir_path, prefix=None)\n    elif dir_path.startswith(\"gs://\"):\n        return Dir(path=None, url=dir_path, prefix=None)\n    elif dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path[7:], url=None, prefix=None)\n    else:\n        return Dir(path=dir_path, url=None, prefix=None)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n    if isinstance(dir_path, Dir):\n        return dir_path\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"az://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path.replace(\"file://\", \"\"))\n    if dir_path.startswith(\"projects/\"):\n        return Dir(url=f\"{dir_path}\")\n    if dir_path.startswith(\"projects/\") and dir_path.endswith(\"/\"):\n        return Dir(url=f\"{dir_path}\")\n    if dir_path.startswith(\"projects/\") and dir_path.endswith(\"/data\"):\n        return Dir(url=f\"{dir_path}/\")\n    if dir_path.startswith(\"projects/\") and dir_path.endswith(\"/data/\"):\n        return Dir(url=f\"{dir_path}\")\n    if dir_path.startswith(\"projects/\") and dir_path.endswith(\"/data/raw\"):\n        return Dir(url=f\"{dir_path}/\")\n    if dir_path.startswith(\"projects/\") and dir_path.endswith(\"/data/raw/\"):\n        return Dir(url=f\"{dir_path}\")\n    if dir_path.startswith(\"projects/\") and dir_path.endswith(\"/data/raw/raw\"):\n        return Dir(url=f\"{dir_path}/\")\n    if dir_path.startswith(\"projects/\") and dir_path.endswith(\"/data/raw/raw/\"):\n        return Dir(url=f\"{dir_path}\")\n    if dir_path.startswith(\"projects/\") and dir_path.end"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return None\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    # Check if the path is a local path\n    if os.path.exists(dir_path):\n        return Dir(path=dir_path)\n\n    # Check if the path is a S3 URL\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    # Check if the path is a project path\n    if dir_path.startswith(\"projects/\"):\n        project_name = dir_path.split(\"/\")[1]\n        project_dir = Project.get_project_dir(project_name)\n        return Dir(path=os.path.join(project_dir, *dir_path.split(\"/\")[2:]))\n\n    # If the path is neither a local path, S3 URL, or project path, raise an exception\n    raise ValueError(f\"Invalid directory path: {dir_path}\")\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return None\n    elif isinstance(dir_path, Dir):\n        return dir_path\n    elif dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n    else:\n        return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    # If the input is a string, try to resolve it as a local path\n    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(url=dir_path)\n        else:\n            return Dir(path=dir_path)\n\n    # If the input is a Dir object, return it directly\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    # If the input is None, return None\n    if dir_path is None:\n        return None\n\n    # If the input is not a string or a Dir object, raise an error\n    raise ValueError(\"Invalid input type for dir_path\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of the Dir class\")\n\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must start with 's3://'\")\n\n    if append:\n        raise NotImplementedError(\"append is not implemented\")\n\n    if overwrite:\n        raise NotImplementedError(\"overwrite is not implemented\")\n\n    # Check if the directory is empty\n    if output_dir.is_empty():\n        return\n    else:\n        raise ValueError(\"output_dir is not empty\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The output_dir argument must be an instance of the Dir class.\")\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(\"The output_dir argument must start with 's3://'.\")\n    if output_dir.exists() and not output_dir.is_empty():\n        raise ValueError(\"The output_dir argument must be empty.\")\n    if append and overwrite:\n        raise ValueError(\"The append and overwrite arguments cannot both be True.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    # Check if the output_dir is a Dir object\n    if not isinstance(output_dir, Dir):\n        raise TypeError(\"The output_dir argument must be an instance of the Dir class.\")\n\n    # Check if the output_dir starts with \"s3://\"\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(\"The output_dir argument must start with 's3://'.\")\n\n    # Check if the output_dir already contains data\n    if output_dir.list():\n        raise ValueError(\"The output_dir argument already contains data.\")\n\n    # Check if appending or overwriting data is allowed\n    if append or overwrite:\n        raise NotImplementedError(\"Appending or overwriting data in the output_dir argument is not currently supported.\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The output directory must be an instance of the Dir class.\")\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(\"The output directory must be an S3 bucket.\")\n    if append:\n        raise NotImplementedError(\"Appending data to the output directory is not supported.\")\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data in the output directory is not supported.\")\n    if output_dir.is_empty():\n        return\n    else:\n        raise ValueError(f\"The output directory {output_dir} is not empty.\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"The output_dir argument must be an instance of the Dir class.\")\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"The output_dir argument must be a path in an S3 bucket.\")\n    if not append and not overwrite:\n        if len(output_dir.list_files()) > 0:\n            raise ValueError(\"The output_dir argument must be an empty directory.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of the Dir class\")\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must start with 's3://'\")\n    if not append and not overwrite:\n        if output_dir.exists():\n            raise ValueError(f\"Directory {output_dir} already contains data\")\n    else:\n        raise NotImplementedError(\"append and overwrite are not implemented yet\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"The output_dir argument must be an instance of the Dir class.\")\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"The output_dir argument must be a path starting with 's3://'.\")\n    if not append and not overwrite:\n        if output_dir.exists():\n            raise ValueError(f\"The directory {output_dir.path} is not empty.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The output_dir argument must be an instance of the Dir class.\")\n\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(\"The output_dir argument must start with 's3://'.\")\n\n    if append and overwrite:\n        raise ValueError(\"The append and overwrite arguments cannot both be True.\")\n\n    if append:\n        raise NotImplementedError(\"Appending data to the output directory is not yet implemented.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data in the output directory is not yet implemented.\")\n\n    if output_dir.is_empty():\n        return\n\n    raise ValueError(f\"The output directory '{output_dir}' is not empty.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"The output directory must be an instance of the Dir class.\")\n\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"The output directory must be an S3 bucket.\")\n\n    if append:\n        raise NotImplementedError(\"Appending data to the output directory is not yet supported.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data in the output directory is not yet supported.\")\n\n    if output_dir.is_empty():\n        return\n\n    raise ValueError(\"The output directory is not empty.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"output_dir must be an instance of the Dir class.\")\n\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must start with 's3://'.\")\n\n    if output_dir.exists():\n        raise ValueError(\"output_dir must be an empty directory.\")\n\n    if append:\n        raise NotImplementedError(\"append is not implemented for _assert_dir_is_empty.\")\n\n    if overwrite:\n        raise NotImplementedError(\"overwrite is not implemented for _assert_dir_is_empty.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The output_dir argument must be an instance of the Dir class.\")\n\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"The output_dir argument must be an S3 directory.\")\n\n    if output_dir.exists():\n        if append:\n            raise ValueError(\"The output_dir argument must not already contain data.\")\n        if overwrite:\n            raise ValueError(\"The output_dir argument must not already contain data.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The output directory must be an instance of the Dir class.\")\n\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(\"The output directory must be an S3 bucket.\")\n\n    if append or overwrite:\n        raise NotImplementedError(\"Append and overwrite operations are not yet supported for S3 buckets.\")\n\n    if output_dir.exists():\n        raise ValueError(\"The output directory must be empty.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"output_dir must be an instance of Dir, not {type(output_dir)}.\")\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(f\"output_dir must start with 's3://', not {output_dir}.\")\n    if append or overwrite:\n        raise NotImplementedError(f\"append and overwrite are not implemented yet.\")\n    if output_dir.exists():\n        raise ValueError(f\"output_dir already exists, and cannot be overwritten.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(\"The output directory must be an instance of the Dir class.\")\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(\"The output directory must start with 's3://'.\")\n    if output_dir.exists():\n        if append:\n            raise ValueError(\"The output directory already contains data. Appending is not allowed.\")\n        elif overwrite:\n            raise ValueError(\"The output directory already contains data. Overwriting is not allowed.\")\n        else:\n            raise ValueError(\"The output directory already contains data.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"The output_dir argument must be an instance of the Dir class. Received {type(output_dir)}.\")\n\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(f\"The output_dir argument must start with 's3://'. Received {output_dir.path}.\")\n\n    if append:\n        raise NotImplementedError(\"Append is not implemented for S3 buckets.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwrite is not implemented for S3 buckets.\")\n\n    if output_dir.exists():\n        raise ValueError(f\"The output_dir argument must not already exist. Received {output_dir.path}.\")\n\n    return None"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    # Check if the output_dir is a Dir object\n    if not isinstance(output_dir, Dir):\n        raise TypeError(\"output_dir must be an instance of Dir\")\n\n    # Check if the output_dir is a valid S3 directory\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must be a valid S3 directory\")\n\n    # Check if the output_dir is empty\n    if not output_dir.is_empty():\n        raise ValueError(\"output_dir must be empty\")\n\n    # Check if appending or overwriting is allowed\n    if append or overwrite:\n        raise NotImplementedError(\"append and overwrite are not implemented yet\")\n\n    # If all checks pass, the function completes successfully\n    return\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"output_dir must be an instance of Dir.\")\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(\"output_dir must start with 's3://'.\")\n    if output_dir.exists():\n        raise ValueError(\"output_dir already exists.\")\n    if append:\n        raise NotImplementedError(\"append is not yet implemented.\")\n    if overwrite:\n        raise NotImplementedError(\"overwrite is not yet implemented.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"output_dir must be an instance of the Dir class. Instead, it is of type {type(output_dir)}.\")\n\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(f\"output_dir must start with 's3://'. Instead, it is '{output_dir}'.\")\n\n    if output_dir.is_empty():\n        return\n    else:\n        raise ValueError(f\"The directory {output_dir} is not empty. It contains the following files: {output_dir.list_files()}.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(f\"The output_dir argument must be an instance of the Dir class. Instead, it is of type {type(output_dir)}.\")\n\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(f\"The output_dir argument must be an S3 bucket. Instead, it is {output_dir}.\")\n\n    if append or overwrite:\n        raise NotImplementedError(f\"The append and overwrite arguments are currently not supported. Instead, use the append and overwrite arguments of the write_data() function to write to the directory.\")\n\n    if output_dir.is_empty():\n        return\n    else:\n        raise ValueError(f\"The output_dir argument must be an empty directory. Instead, it contains the following files: {output_dir.list_files()}.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    # Check if the input is a Dir object\n    if not isinstance(output_dir, Dir):\n        raise TypeError(\"The output directory must be an instance of the Dir class.\")\n\n    # Check if the input is a valid S3 directory\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(\"The output directory must start with 's3://'.\")\n\n    # Check if the directory is empty\n    if not output_dir.is_empty():\n        raise ValueError(\"The output directory is not empty. Please specify a different directory or use the 'overwrite' argument.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.is_s3_bucket():\n        if output_dir.has_index_file():\n            raise ValueError(\"The output directory already contains an index file.\")\n    else:\n        raise ValueError(\"The output directory is not an S3 bucket.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.is_s3_bucket():\n        if output_dir.has_index_file():\n            raise ValueError(f\"The directory {output_dir.path} is an S3 bucket and already contains an index file named 'index.json'.\")\n    else:\n        raise ValueError(f\"The directory {output_dir.path} is not an S3 bucket.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the directory is an S3 bucket directory\n    if not output_dir.is_s3_bucket:\n        raise ValueError(\"Output directory is not an S3 bucket directory\")\n\n    # Check if the directory already contains an index file\n    if output_dir.has_index_file:\n        raise ValueError(\"Output directory already contains an index file\")\n\n    # Delete all objects within the specified prefix in the bucket\n    output_dir.delete_objects()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the directory is an S3 bucket directory\n    if not output_dir.is_s3_bucket_dir:\n        raise ValueError(\"The given directory is not an S3 bucket directory.\")\n\n    # Check if the directory already contains an index file\n    if output_dir.index_file_exists:\n        raise ValueError(\"The given directory already contains an index file.\")\n\n    # Delete all objects within the specified prefix in the bucket\n    output_dir.delete_all_objects_in_bucket()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the directory is an S3 bucket directory\n    if not output_dir.is_s3_bucket:\n        raise ValueError(f\"The directory '{output_dir.path}' is not an S3 bucket directory.\")\n\n    # Check if the directory already contains an \"index.json\" file\n    if output_dir.index_file_exists:\n        raise ValueError(f\"The directory '{output_dir.path}' already contains an 'index.json' file.\")\n\n    # Delete all objects within the specified prefix in the bucket\n    output_dir.delete_objects()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the directory is an S3 bucket\n    if output_dir.is_s3_bucket:\n        # Check if the directory already contains an index file\n        if output_dir.has_index_file:\n            raise ValueError(\n                f\"The directory {output_dir.path} already contains an index file named 'index.json'. Please delete the index file before proceeding.\"\n            )\n\n        # Delete all objects within the specified prefix in the bucket\n        output_dir.delete_all_objects_in_prefix()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.is_s3_bucket():\n        if output_dir.has_index_file():\n            raise ValueError(\n                f\"The output directory {output_dir.path} is an S3 bucket and already contains an index file. Please delete the index file before proceeding.\"\n            )\n        else:\n            output_dir.delete_objects_in_prefix()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.is_s3_bucket_dir():\n        if output_dir.has_index_file():\n            raise ValueError(\n                f\"The directory {output_dir.path} is an S3 bucket directory and already contains an index file named 'index.json'. Please remove the index file and try again.\"\n            )\n        else:\n            output_dir.delete_objects_with_prefix()\n    else:\n        raise ValueError(\n            f\"The directory {output_dir.path} is not an S3 bucket directory. Please provide an S3 bucket directory and try again.\"\n        )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the directory is an S3 bucket\n    if output_dir.is_s3:\n        # Check if the directory already contains an index file\n        if output_dir.has_index_file:\n            # If the directory already contains an index file, raise an error\n            raise ValueError(f\"The directory {output_dir.path} already contains an index file named 'index.json'. Please delete it before running the function.\")\n        else:\n            # If the directory does not contain an index file, delete all objects within the specified prefix in the bucket\n            output_dir.delete_all_objects_in_prefix()\n    else:\n        # If the directory is not an S3 bucket, raise an error\n        raise ValueError(f\"The directory {output_dir.path} is not an S3 bucket. Please provide an S3 bucket directory.\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.is_s3_bucket():\n        if output_dir.has_index_file():\n            raise ValueError(\n                f\"The directory {output_dir.get_path()} already contains an index file named 'index.json'. Please delete it before running the script.\"\n            )\n        else:\n            print(\n                f\"The directory {output_dir.get_path()} does not contain an index file named 'index.json'. It will be created during the execution of the script.\"\n            )\n    else:\n        raise ValueError(\n            f\"The directory {output_dir.get_path()} is not an S3 bucket. Please provide an S3 bucket directory.\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.is_s3_dir():\n        if output_dir.has_index_file():\n            raise ValueError(\n                f\"The output directory {output_dir.get_path()} is already an S3 bucket directory and contains an index file. Please delete the index file or use a different directory.\"\n            )\n        else:\n            output_dir.delete_objects()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.is_s3_bucket:\n        if output_dir.has_index_file:\n            raise ValueError(f\"The directory {output_dir.path} is an S3 bucket and already contains an index file.\")\n        else:\n            # If the directory is an S3 bucket and does not contain an index file, we delete all objects within the specified prefix in the bucket.\n            output_dir.delete_all_objects()\n    else:\n        raise ValueError(f\"The directory {output_dir.path} is not an S3 bucket.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.is_s3:\n        if output_dir.has_index_file:\n            raise ValueError(f\"S3 bucket {output_dir.bucket} already contains index file.\")\n        else:\n            print(f\"Creating index file in S3 bucket {output_dir.bucket}.\")\n    else:\n        if output_dir.has_index_file:\n            print(f\"Deleting all objects in prefix {output_dir.prefix} in local directory {output_dir.dir_path}.\")\n            output_dir.delete_all_objects()\n        else:\n            print(f\"Creating index file in local directory {output_dir.dir_path}.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the directory is an S3 bucket\n    if not output_dir.is_s3_bucket():\n        raise Exception(\n            f\"The directory {output_dir.get_path()} is not an S3 bucket. Please provide a valid S3 bucket directory.\"\n        )\n\n    # Check if the directory already contains an index file\n    if output_dir.has_index_file():\n        raise Exception(\n            f\"The directory {output_dir.get_path()} already contains an index file. Please provide a directory that does not contain an index file.\"\n        )\n\n    # Delete all objects within the prefix in the bucket\n    output_dir.delete_all_objects()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the directory is an S3 bucket\n    if not output_dir.is_s3:\n        raise ValueError(\n            f\"The directory {output_dir} is not an S3 bucket. Please provide an S3 bucket directory.\"\n        )\n\n    # Check if the directory already contains an index file\n    if \"index.json\" in output_dir.list_objects():\n        raise ValueError(\n            f\"The directory {output_dir} already contains an index file. Please provide an empty S3 bucket directory.\"\n        )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.is_s3_bucket_dir():\n        if output_dir.has_index_file():\n            raise Exception(\n                f\"Directory {output_dir.get_s3_bucket_dir_path()} already contains an index file named 'index.json'.\"\n            )\n        else:\n            output_dir.delete_all_objects()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if output_dir is an S3 bucket\n    if not output_dir.is_s3:\n        raise Exception(\n            f\"The output directory is not an S3 bucket: {output_dir.path}\")\n\n    # Check if output_dir already contains an index file\n    if \"index.json\" in output_dir.list_files():\n        raise Exception(\n            f\"The output directory already contains an index file: {output_dir.path}/index.json\")\n\n    # Delete all objects within the specified prefix in the bucket\n    output_dir.delete_objects()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the directory is an S3 bucket\n    if output_dir.is_s3_bucket():\n\n        # Check if the directory already contains an \"index.json\" file\n        if output_dir.has_index_file():\n\n            # Raise an error if the directory already contains an \"index.json\" file\n            raise Exception(\n                f\"The directory '{output_dir.path}' is an S3 bucket and already contains an index file named 'index.json'. Please delete the index file and try again.\"\n            )\n\n        # If the directory is an S3 bucket and does not contain an \"index.json\" file, delete all objects within the specified prefix in the bucket\n        else:\n\n            # Delete all objects within the specified prefix in the bucket\n            output_dir.delete_objects_in_prefix()\n\n    # Raise an error if the directory is not an S3 bucket\n    else:\n\n        # Raise an error if the directory is not an S3 bucket\n        raise Exception(\n            f\"The directory '{output_dir.path}' is not an S3 bucket. Please specify an S3 bucket directory and try again.\"\n        )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not output_dir.is_s3_bucket:\n        raise ValueError(\n            f\"The output directory {output_dir.name} is not an S3 bucket. It should be an S3 bucket directory.\"\n        )\n\n    if output_dir.exists(\"index.json\"):\n        raise ValueError(\n            f\"The output directory {output_dir.name} already contains an index file named 'index.json'. Please delete the index file and try again.\"\n        )\n\n    output_dir.delete_all_objects()\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.name == \"s3://\":\n        if output_dir.has_file(\"index.json\"):\n            raise ValueError(\n                f\"The directory {output_dir} already contains an index file named 'index.json'. Please remove it or rename it before proceeding.\"\n            )\n    else:\n        raise ValueError(\n            f\"The directory {output_dir} is not an S3 bucket directory. Please provide an S3 bucket directory.\"\n        )\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(self.index_file_path):\n                time.sleep(1)\n            return\n        if not os.path.exists(self.index_file_path):\n            while not os.path.exists(self.index_file_path):\n                time.sleep(1)\n        if not os.path.exists(self.index_file_path):\n            raise FileNotFoundError(f\"Index file not found at {self.index_file_path}\")\n        if not os.path.exists(self.cache_dir):\n            os.makedirs(self.cache_dir)\n        if not os.path.exists(self.index_file_path):\n            raise FileNotFoundError(f\"Index file not found at {self.index_file_path}\")\n        if not os.path.exists(self.cache_dir):\n            os.makedirs(self.cache_dir)\n        if not os.path.exists(self.index_file_path):\n            raise FileNotFoundError(f\"Index file not found at {self.index_file_path}\")\n        if not os.path.exists(self.cache_dir):\n            os.makedirs(self.cache_dir)\n        if not os.path.exists(self.index_file_path):\n            raise FileNotFoundError(f\"Index file not found at {self.index_file_path}\")\n        if not os.path.exists(self.cache_dir):\n            os.makedirs(self.cache_dir)\n        if not os.path.exists(self.index_file_path):\n            raise FileNotFoundError(f\"Index file not found at {self.index_file_path}\")\n        if not os.path.exists(self.cache_dir):\n            os.makedirs(self.cache_dir)\n        if not os.path.exists(self.index_file_path):\n            raise FileNotFoundError(f\"Index file not found at {self.index_file_path}\")\n        if not os.path."}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n        while not os.path.exists(os.path.join(self.cache_dir, \"index_0.bin\")):\n            time.sleep(0.1)\n        if node_rank == 0:\n            self._merge_index_parts()\n        while not os.path.exists(os.path.join(self.cache_dir, \"index_0.bin\")):\n            time.sleep(0.1)\n        if node_rank != 0:\n            while not os.path.exists(os.path.join(self.cache_dir, \"index_0.bin\")):\n                time.sleep(0.1)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all parts to be available\n        while not all(\n            [\n                os.path.exists(\n                    os.path.join(\n                        self.cache_dir,\n                        f\"index_part_{i}.bin\",\n                    )\n                )\n                for i in range(num_workers)\n            ]\n        ):\n            time.sleep(0.1)\n\n        # Only merge if node_rank is 0\n        if node_rank != 0:\n            while not os.path.exists(\n                os.path.join(\n                    self.cache_dir,\n                    \"index_part_0.bin\",\n                )\n            ):\n                time.sleep(0.1)\n\n        # Merge the index parts\n        if node_rank == 0:\n            self.merge_index_parts()\n\n        # Wait for the merged index file to be available\n        while not os.path.exists(\n            os.path.join(\n                self.cache_dir,\n                \"index.bin\",\n            )\n        ):\n            time.sleep(0.1)\n\n        # Wait for the merged index file to be available\n        while not os.path.exists(\n            os.path.join(\n                self.cache_dir,\n                \"index.bin\",\n            )\n        ):\n            time.sleep(0.1)\n\n        # Wait for the merged index file to be available\n        while not os.path.exists(\n            os.path.join(\n                self.cache_dir,\n                \"index.bin\",\n            )\n        ):\n            time.sleep(0.1)\n\n        # Wait for the merged index file to be available\n        while not os.path.exists(\n            os.path.join(\n                self.cache_dir,\n                \"index.bin\",\n            )\n        ):\n            time.sleep(0.1)\n\n        # Wait for the merged index file to be available\n        while not os.path.exists(\n            os.path.join(\n                self.cache_dir,\n                \"index.bin\",\n            )\n        "}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n        if node_rank != 0:\n            # Wait until the merged index file is available\n            while not os.path.exists(self.merged_index_path):\n                time.sleep(0.1)\n            return\n        # Wait for all parts to be available\n        while len(os.listdir(self.cache_dir)) < num_workers:\n            time.sleep(0.1)\n        # Merge parts into a single index file\n        with open(self.merged_index_path, 'wb') as merged_index_file:\n            for i in range(num_workers):\n                part_path = os.path.join(self.cache_dir, f'part_{i}.index')\n                with open(part_path, 'rb') as part_file:\n                    shutil.copyfileobj(part_file, merged_index_file)\n        # Delete the part files\n        for i in range(num_workers):\n            part_path = os.path.join(self.cache_dir, f'part_{i}.index')\n            os.remove(part_path)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all index parts to be available\n        while not all(\n            [\n                os.path.exists(\n                    os.path.join(\n                        self.cache_dir, f\"{self.index_name}_part_{i}.bin\"\n                    )\n                )\n                for i in range(num_workers)\n            ]\n        ):\n            time.sleep(0.1)\n\n        # Check if this is the master node (rank 0)\n        if node_rank is None or node_rank == 0:\n            # Merge index parts into a single file\n            with open(os.path.join(self.cache_dir, f\"{self.index_name}.bin\"), \"wb\") as f:\n                for i in range(num_workers):\n                    with open(\n                        os.path.join(self.cache_dir, f\"{self.index_name}_part_{i}.bin\"),\n                        \"rb\",\n                    ) as part:\n                        f.write(part.read())\n\n            # Remove index parts\n            for i in range(num_workers):\n                os.remove(\n                    os.path.join(self.cache_dir, f\"{self.index_name}_part_{i}.bin\")\n                )\n\n            # Wait for the merged index file to be available\n            while not os.path.exists(\n                os.path.join(self.cache_dir, f\"{self.index_name}.bin\")\n            ):\n                time.sleep(0.1)\n\n        # Wait for the merged index file to be available for non-master nodes\n        while not os.path.exists(\n            os.path.join(self.cache_dir, f\"{self.index_name}.bin\")\n        ):\n            time.sleep(0.1)\n\n        # Load the merged index file\n        self.load(os.path.join(self.cache_dir, f\"{self.index_name}.bin\"))\n\n        # Remove the merged index file\n        os.remove(os.path.join(self.cache_dir, f\"{self.index_name}.bin\"))"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n        if node_rank == 0:\n            self.logger.info(\"Merging index files\")\n            # Wait for all index files to be available\n            while not all(\n                [\n                    os.path.exists(\n                        os.path.join(self.cache_dir, f\"{i}.index\")\n                    )\n                    for i in range(num_workers)\n                ]\n            ):\n                time.sleep(0.1)\n            # Merge index files\n            with open(os.path.join(self.cache_dir, \"0.index\"), \"rb\") as f:\n                self.index = pickle.load(f)\n            for i in range(1, num_workers):\n                with open(os.path.join(self.cache_dir, f\"{i}.index\"), \"rb\") as f:\n                    index = pickle.load(f)\n                    self.index.update(index)\n            # Save merged index\n            with open(os.path.join(self.cache_dir, \"index\"), \"wb\") as f:\n                pickle.dump(self.index, f)\n            # Wait for merged index to be available\n            while not os.path.exists(os.path.join(self.cache_dir, \"index\")):\n                time.sleep(0.1)\n            # Delete individual index files\n            for i in range(num_workers):\n                os.remove(os.path.join(self.cache_dir, f\"{i}.index\"))\n            self.logger.info(\"Index files merged\")\n        else:\n            # Wait for merged index to be available\n            while not os.path.exists(os.path.join(self.cache_dir, \"index\")):\n                time.sleep(0.1)\n            # Load merged index\n            with open(os.path.join(self.cache_dir, \"index\"), \"rb\") as f:\n                self.index = pickle.load(f)\n            self.logger.info(\"Index loaded\")"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all index parts to be available\n        while True:\n            index_files = [\n                os.path.join(self.cache_dir, f\"{self.index_name}_part_{i}.bin\")\n                for i in range(num_workers)\n            ]\n            if all([os.path.exists(index_file) for index_file in index_files]):\n                break\n            time.sleep(0.1)\n\n        # Only merge on the master node\n        if node_rank != 0:\n            while True:\n                merged_index_file = os.path.join(\n                    self.cache_dir, f\"{self.index_name}.bin\"\n                )\n                if os.path.exists(merged_index_file):\n                    break\n                time.sleep(0.1)\n        else:\n            # Merge the index parts\n            index_files = [\n                os.path.join(self.cache_dir, f\"{self.index_name}_part_{i}.bin\")\n                for i in range(num_workers)\n            ]\n            merged_index_file = os.path.join(self.cache_dir, f\"{self.index_name}.bin\")\n            self.merge_index_parts(index_files, merged_index_file)\n\n        # Delete index parts\n        for index_file in index_files:\n            os.remove(index_file)\n\n        # Move the merged index file to the final destination\n        final_index_file = os.path.join(self.index_dir, f\"{self.index_name}.bin\")\n        os.rename(merged_index_file, final_index_file)\n\n        # Delete the cache directory\n        os.rmdir(self.cache_dir)\n\n        # Print a message indicating the merge process is complete\n        print(f\"Merged index file: {self.index_name}.bin\")"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n        if node_rank != 0:\n            while not os.path.exists(self.index_path):\n                time.sleep(0.1)\n            return\n        while not all(\n            os.path.exists(f\"{self.cache_dir}/{i}.bin\") for i in range(num_workers)\n        ):\n            time.sleep(0.1)\n        if not os.path.exists(self.index_path):\n            os.makedirs(self.index_path, exist_ok=True)\n        index_files = [\n            f\"{self.cache_dir}/{i}.bin\" for i in range(num_workers)\n        ]\n        index_files = sorted(index_files, key=lambda x: int(x.split(\"/\")[-1][:-4]))\n        index_files = [f\"{self.cache_dir}/{i}.bin\" for i in range(num_workers)]\n        with open(f\"{self.index_path}/index.bin\", \"wb\") as f:\n            for index_file in index_files:\n                with open(index_file, \"rb\") as f_in:\n                    f.write(f_in.read())\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait until all parts are available\n        while True:\n            if not self.check_index_parts_exist(num_workers):\n                time.sleep(1)\n            else:\n                break\n\n        # Merge only if master node (rank 0)\n        if node_rank == 0:\n            self.merge_index_parts(num_workers)\n\n        # Wait until merged index is available\n        while True:\n            if not self.check_merged_index_exists():\n                time.sleep(1)\n            else:\n                break\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        while True:\n            index_files = os.listdir(self.cache_dir)\n            if len(index_files) == num_workers:\n                if node_rank is not None and node_rank != 0:\n                    while not os.path.exists(os.path.join(self.cache_dir, \"merged_index.bin\")):\n                        time.sleep(1)\n                else:\n                    self.merge_index_files()\n                break\n            time.sleep(1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all parts of the index to be available\n        while not self.index_parts_available(num_workers):\n            time.sleep(1)\n\n        # Only proceed with the merge if this is the master node (rank 0)\n        if node_rank == 0:\n            # Merge the index parts into a single index file\n            self.merge_index_parts()\n\n        # Wait for the merged index file to be available\n        while not self.merged_index_available():\n            time.sleep(1)\n\n        # Log the completion of the merge process\n        self.logger.info(\"Merged index available\")\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all index parts to be available\n        while not all(\n            [\n                os.path.exists(\n                    os.path.join(\n                        self.cache_dir,\n                        f\"{self.index_name}_{i}.{self.index_ext}\",\n                    )\n                )\n                for i in range(num_workers)\n            ]\n        ):\n            time.sleep(1)\n\n        # Merge only if this is the master node (rank 0)\n        if node_rank != 0:\n            while not os.path.exists(\n                os.path.join(\n                    self.cache_dir,\n                    f\"{self.index_name}_merged.{self.index_ext}\",\n                )\n            ):\n                time.sleep(1)\n\n        # Merge the index parts\n        if node_rank == 0:\n            print(\"Merging index parts\")\n            index_parts = [\n                os.path.join(\n                    self.cache_dir,\n                    f\"{self.index_name}_{i}.{self.index_ext}\",\n                )\n                for i in range(num_workers)\n            ]\n            self.merge_index_parts(index_parts)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(self.index_path):\n                time.sleep(1)\n            return\n        while not all(\n            [\n                os.path.exists(os.path.join(self.cache_dir, f\"{i}.index\"))\n                for i in range(num_workers)\n            ]\n        ):\n            time.sleep(1)\n        if node_rank is not None and node_rank == 0:\n            self.merge_index()\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all index parts to be available\n        while not self.check_all_parts_available(num_workers):\n            time.sleep(1)\n\n        # Only perform the merge on the master node (rank 0)\n        if node_rank is None or node_rank == 0:\n            # Merge the index parts\n            self.merge_parts()\n\n        # Wait for the merged index file to be available on the master node (rank 0)\n        while not os.path.exists(self.merged_index_path):\n            time.sleep(1)\n\n        # Remove the index parts\n        self.remove_index_parts()\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all parts to be available\n        while not all(\n            [os.path.exists(f\"{self.cache_dir}/index_{i}.bin\") for i in range(num_workers)]\n        ):\n            time.sleep(1)\n\n        # Wait for merged index file to be available for non-master nodes\n        if node_rank != 0:\n            while not os.path.exists(f\"{self.cache_dir}/merged_index.bin\"):\n                time.sleep(1)\n\n        # Merge index parts if master node\n        if node_rank == 0:\n            self.merge_index_parts()\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n        while True:\n            if all(\n                [\n                    os.path.exists(\n                        os.path.join(\n                            self.cache_dir,\n                            f\"{self.index_name}.{i}.{self.index_type}\",\n                        )\n                    )\n                    for i in range(num_workers)\n                ]\n            ):\n                if node_rank == 0:\n                    self.logger.info(\n                        f\"All index parts are available, starting the merge process.\"\n                    )\n                    self.merge_index()\n                    break\n                else:\n                    while not os.path.exists(\n                        os.path.join(self.cache_dir, f\"{self.index_name}.{self.index_type}\")\n                    ):\n                        time.sleep(1)\n                    break\n            else:\n                time.sleep(1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        while not os.path.exists(self.index_path):\n            if self.rank != 0:\n                while not os.path.exists(self.index_path):\n                    time.sleep(1)\n            else:\n                time.sleep(1)\n\n        if self.rank == 0:\n            self.logger.info(f\"Merging {num_workers} index files\")\n            self.merge_index_parts(num_workers)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n        while True:\n            # Wait until all parts are available\n            all_parts_available = True\n            for i in range(num_workers):\n                part_path = os.path.join(self.cache_dir, f\"index_part_{i}.bin\")\n                if not os.path.exists(part_path):\n                    all_parts_available = False\n                    break\n            if all_parts_available:\n                break\n            time.sleep(0.1)\n\n        # Merge only on the master node (rank 0)\n        if node_rank == 0:\n            self.merge_parts()\n\n        # Wait until the merged index file is available\n        while True:\n            merged_index_path = os.path.join(self.cache_dir, \"merged_index.bin\")\n            if os.path.exists(merged_index_path):\n                break\n            time.sleep(0.1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n\n        if node_rank != 0:\n            # Wait until the merged index file is available\n            while not os.path.exists(self.cache_dir / \"index.bin\"):\n                time.sleep(1)\n            return\n\n        # Wait until all index parts are available\n        while len(os.listdir(self.cache_dir)) != num_workers:\n            time.sleep(1)\n\n        # Merge index parts\n        self.merge_index_parts()\n\n        # Remove index parts\n        for i in range(num_workers):\n            os.remove(self.cache_dir / f\"index_{i}.bin\")\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n        logger.info(f\"Waiting for index parts to be available\")\n        while True:\n            # Wait for all index parts to be available\n            all_parts_available = True\n            for i in range(num_workers):\n                if not os.path.exists(f\"{self.cache_dir}/part_{i}.bin\"):\n                    all_parts_available = False\n                    break\n            if all_parts_available:\n                break\n            time.sleep(1)\n\n        # Only master node (rank 0) performs the merge\n        if node_rank != 0:\n            logger.info(f\"Waiting for merged index to be available\")\n            while True:\n                if os.path.exists(f\"{self.cache_dir}/index.bin\"):\n                    break\n                time.sleep(1)\n        else:\n            logger.info(f\"Merging index parts\")\n            # Merge index parts into a single index file\n            with open(f\"{self.cache_dir}/index.bin\", \"wb\") as outfile:\n                for i in range(num_workers):\n                    with open(f\"{self.cache_dir}/part_{i}.bin\", \"rb\") as infile:\n                        shutil.copyfileobj(infile, outfile)\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is available\n    if not _is_sdk_available():\n        raise Exception(\"SDK is not available\")\n\n    # Fetch the default machine configuration if not provided\n    if not machine:\n        machine = get_default_machine()\n\n    # Construct the command to be executed in the job\n    if not command:\n        command = f\"cd {os.getcwd()}; {os.environ}\"\n\n    # Create a data preparation machine job through the Studio API\n    job = create_data_preparation_machine_job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    # Print the job URL when it starts\n    print(f\"Job URL: {job.url}\")\n\n    # Continuously check the job status until it completes\n    while True:\n        job = get_data_preparation_machine_job(job.id)\n        if job.status == \"COMPLETED\":\n            break\n        elif job.status == \"FAILED\":\n            raise Exception(\"Job failed\")\n        time.sleep(5)\n\n    # Print the job URL when it completes\n    print(f\"Job URL: {job.url}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if command is None:\n        command = f\"cd {os.getcwd()}; {os.environ.get('SHELL', '/bin/bash')}\"\n\n    if machine is None:\n        machine = Machine.default_machine()\n\n    job = Job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    job.submit()\n\n    job.wait_for_completion(show_output=True)\n\n    if job.failed():\n        raise Exception(\n            f\"Job failed with the following error: {job.error_message()}\"\n        )\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is available\n    if not is_sdk_available():\n        raise Exception(\"SDK is not available\")\n\n    # Get the current working directory\n    cwd = os.getcwd()\n\n    # Get the environment variables\n    env = os.environ.copy()\n\n    # Construct the command to be executed\n    if command is None:\n        command = f\"cd {cwd} && {env}\"\n\n    # Create a machine configuration if not provided\n    if machine is None:\n        machine = Machine(cpu=1, memory=1)\n\n    # Create a data preparation machine job\n    job = create_job(\n        name=name,\n        command=command,\n        machine=machine,\n        num_nodes=num_nodes,\n        job_type=JobType.DATA_PREPARATION,\n    )\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")\n\n    # Wait for the job to start\n    while job.status == JobStatus.QUEUED:\n        time.sleep(1)\n        job.refresh()\n\n    # Check if the job failed\n    if job.status == JobStatus.FAILED:\n        raise Exception(\"Job failed\")\n\n    # Print the job status\n    print(f\"Job status: {job.status}\")\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is installed\n    if not is_sdk_installed():\n        raise Exception(\"SDK is not installed. Please install the SDK first.\")\n\n    # Get the current working directory and environment variables\n    current_dir = os.getcwd()\n    env_vars = os.environ.copy()\n\n    # Get the default machine configuration if not provided\n    if not machine:\n        machine = get_default_machine()\n\n    # Get the default command if not provided\n    if not command:\n        command = f\"cd {current_dir} && {env_vars['SHELL']}\"\n\n    # Create the job\n    job = create_job(name, num_nodes, machine, command)\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")\n\n    # Check the job status\n    while True:\n        job = get_job(job.id)\n        if job.status == \"running\":\n            print(f\"Job is running: {job.url}\")\n        elif job.status == \"failed\":\n            raise Exception(f\"Job failed: {job.url}\")\n        elif job.status == \"succeeded\":\n            print(f\"Job succeeded: {job.url}\")\n            break\n        time.sleep(5)"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is available\n    if not is_sdk_available():\n        raise Exception(\n            \"The SDK is not available. Please install the SDK and try again.\"\n        )\n\n    # Get the current working directory\n    cwd = os.getcwd()\n\n    # Get the environment variables\n    env = os.environ.copy()\n\n    # Construct the command to be executed\n    if command is None:\n        command = f\"cd {cwd} && env {env}\"\n\n    # Create a data preparation machine job\n    job = create_job(name=name, num_nodes=num_nodes, machine=machine, command=command)\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")\n\n    # Continuously check the job status\n    while True:\n        job.refresh()\n        if job.status == \"running\":\n            print(f\"Job status: {job.status}\")\n            time.sleep(10)\n        elif job.status == \"succeeded\":\n            print(f\"Job status: {job.status}\")\n            break\n        elif job.status == \"failed\":\n            raise Exception(f\"Job failed with error: {job.error_message}\")\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Get the current working directory\n    cwd = os.getcwd()\n\n    # Get the environment variables\n    env_vars = os.environ\n\n    # If the machine is not provided, fetch a default machine\n    if machine is None:\n        machine = Machine.default()\n\n    # If the command is not provided, create a default command\n    if command is None:\n        command = f\"cd {cwd} && {env_vars['SHELL']}\"\n\n    # Create a job definition\n    job_def = JobDefinition(\n        name=name,\n        command=command,\n        machine=machine,\n        num_nodes=num_nodes,\n        env_vars=env_vars,\n    )\n\n    # Create a job\n    job = Job(job_def=job_def)\n\n    # Start the job\n    job.start()\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")\n\n    # Wait for the job to complete\n    job.wait()\n\n    # Check if the job failed\n    if job.status == JobStatus.FAILED:\n        raise Exception(\"Job failed\")\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    from . import Machine\n\n    if not command:\n        command = \"python -m pip install -r requirements.txt && python -m pip install -e . && python -m pip install -r requirements-dev.txt && pytest\"\n\n    if not machine:\n        machine = Machine.default()\n\n    if not machine.is_available:\n        raise Exception(\"Machine is not available\")\n\n    if not machine.is_active:\n        raise Exception(\"Machine is not active\")\n\n    if not machine.is_running:\n        raise Exception(\"Machine is not running\")\n\n    if not machine.is_connected:\n        raise Exception(\"Machine is not connected\")\n\n    if not machine.is_authenticated:\n        raise Exception(\"Machine is not authenticated\")\n\n    if not machine.is_available:\n        raise Exception(\"Machine is not available\")\n\n    if not machine.is_active:\n        raise Exception(\"Machine is not active\")\n\n    if not machine.is_running:\n        raise Exception(\"Machine is not running\")\n\n    if not machine.is_connected:\n        raise Exception(\"Machine is not connected\")\n\n    if not machine.is_authenticated:\n        raise Exception(\"Machine is not authenticated\")\n\n    if not machine.is_available:\n        raise Exception(\"Machine is not available\")\n\n    if not machine.is_active:\n        raise Exception(\"Machine is not active\")\n\n    if not machine.is_running:\n        raise Exception(\"Machine is not running\")\n\n    if not machine.is_connected:\n        raise Exception(\"Machine is not connected\")\n\n    if not machine.is_authenticated:\n        raise Exception(\"Machine is not authenticated\")\n\n    if not machine.is_available:\n        raise Exception(\"Machine is not available\")\n\n    if not machine.is_active:\n        raise Exception(\"Machine is not active\")\n\n    if not machine.is_running:\n        raise Exception(\"Machine is not running\")\n\n    if not machine.is_connected:\n        raise Exception(\"Machine is not connected\")\n\n    if not machine.is_authenticated:\n        raise Exception(\"Machine is not authentic"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if machine is None:\n        machine = Machine(cpu=1, memory=16, gpu=0)\n    if command is None:\n        command = \"python -m black . && python -m isort .\"\n\n    job = Job(\n        name=name,\n        image=\"python:3.9\",\n        command=command,\n        machine=machine,\n        num_nodes=num_nodes,\n    )\n    job.start()\n    print(job.url)\n    job.wait()\n    if job.status == JobStatus.FAILED:\n        raise Exception(\"Job failed\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is available\n    if not is_sdk_available():\n        raise Exception(\"SDK is not available. Please install the SDK to use this function.\")\n\n    # Get the current working directory\n    cwd = os.getcwd()\n\n    # Get the environment variables\n    env = os.environ.copy()\n\n    # Check if the command is provided\n    if command is None:\n        # Construct the command to be executed\n        command = f\"cd {cwd} && {env['PATH']}\"\n\n    # Check if the machine configuration is provided\n    if machine is None:\n        # Fetch the default machine configuration\n        machine = Machine.default()\n\n    # Create a data preparation machine job\n    job = Job.create(\n        name=name,\n        command=command,\n        machine=machine,\n        num_nodes=num_nodes,\n    )\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")\n\n    # Continuously check the job status\n    while True:\n        job.refresh()\n        if job.status == JobStatus.RUNNING:\n            print(f\"Job status: {job.status}\")\n            time.sleep(10)\n        elif job.status == JobStatus.FAILED:\n            raise Exception(\"Job failed\")\n        else:\n            break\n\n    # Print the job status\n    print(f\"Job status: {job.status}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    from . import get_machine\n\n    if machine is None:\n        machine = get_machine()\n\n    if command is None:\n        command = f\"python -m {os.getcwd()}\"\n\n    job = machine.jobs.create(name=name, num_nodes=num_nodes, command=command)\n    print(f\"Job started: {job.url}\")\n\n    while True:\n        job = machine.jobs.get(job.id)\n        if job.status == \"running\":\n            print(f\"Job status: {job.status}\")\n            time.sleep(10)\n        elif job.status == \"failed\":\n            raise Exception(f\"Job failed: {job.url}\")\n        elif job.status == \"succeeded\":\n            print(f\"Job succeeded: {job.url}\")\n            break\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    from typing import Optional\n\n    from .machine import Machine\n\n    try:\n        from databand import Databand\n    except ImportError:\n        raise ImportError(\n            \"The databand package is required to execute this function. Please install it using 'pip install databand'.\"\n        )\n\n    if machine is None:\n        machine = Machine.default()\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.environ.get('PYTHON')} -m databand.sdk.execute\"\n\n    databand = Databand()\n    job = databand.jobs.create_data_prep_job(\n        name=name,\n        command=command,\n        num_nodes=num_nodes,\n        machine=machine,\n    )\n\n    job.wait_for_completion()\n\n    if job.status == \"failed\":\n        raise Exception(f\"Job {job.name} failed with status {job.status}\")\n\n    print(f\"Job {job.name} completed with status {job.status}\")\n    print(f\"Job URL: {job.url}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not command:\n        command = (\n            f\"cd {os.getcwd()} && \"\n            f\"export {get_env_variables_as_string()} && \"\n            f\"{get_current_operator().name}\"\n        )\n\n    if not machine:\n        machine = Machine(\n            instance_type=\"ml.t3.medium\",\n            instance_count=num_nodes,\n            volume_size=30,\n            spot_instance=False,\n        )\n\n    job = get_current_operator().create_job(name, machine)\n    job.command = command\n    job.start()\n\n    while job.status not in [\"Failed\", \"Completed\"]:\n        print(f\"Job URL: {job.url}\")\n        time.sleep(10)\n        job.refresh()\n\n    if job.status == \"Failed\":\n        raise RuntimeError(f\"Job failed with status: {job.status}\")\n\n    print(f\"Job completed with status: {job.status}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    try:\n        from airflow.providers.microsoft.azure.hooks.azure_data_factory import AzureDataFactoryHook\n    except ImportError:\n        raise ImportError(\n            \"The Azure Data Factory SDK is not available. Please install it using `pip install azure-datafactory`.\"\n        )\n\n    if not machine:\n        machine = Machine(\n            type=\"VirtualMachine\",\n            compute_power=1000,\n            core_count=4,\n            memory_in_mb=8192,\n        )\n\n    if not command:\n        command = f\"python -m pip install --upgrade pip && pip install -r requirements.txt && python {os.path.basename(os.getcwd())}\"\n\n    hook = AzureDataFactoryHook()\n    job = hook.create_data_prep_job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    print(f\"Job URL: {job.url}\")\n\n    while job.state != \"Succeeded\" and job.state != \"Failed\":\n        job = hook.get_data_prep_job(job.id)\n        print(f\"Job status: {job.state}\")\n        time.sleep(10)\n\n    if job.state == \"Failed\":\n        raise Exception(f\"Job failed with error: {job.error}\")\n\n    print(\"Job completed successfully.\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is available\n    if not is_sdk_available():\n        raise Exception(\"SDK is not available\")\n\n    # Fetch the current working directory\n    cwd = os.getcwd()\n\n    # Get the environment variables as a dictionary\n    env_vars = os.environ.copy()\n\n    # Define the command to be executed in the job\n    if command is None:\n        command = f\"cd {cwd} && {env_vars.get('SHELL', '/bin/bash')}\"\n\n    # Create a machine configuration if not provided\n    if machine is None:\n        machine = Machine(\n            instance_type=\"ml.m5.xlarge\",\n            instance_count=num_nodes,\n            volume_size_in_gb=100,\n        )\n\n    # Create a job definition\n    job_definition = JobDefinition(\n        name=name,\n        command=command,\n        machine=machine,\n        job_type=JobType.DATA_PREP,\n    )\n\n    # Create a job\n    job = Job(\n        name=name,\n        job_definition=job_definition,\n        job_type=JobType.DATA_PREP,\n        machine=machine,\n    )\n\n    # Start the job\n    job.start()\n\n    # Print the job URL\n    print(f\"Job URL: {job.get_url()}\")\n\n    # Wait for the job to finish\n    job.wait()\n\n    # Check if the job failed\n    if job.status == JobStatus.FAILED:\n        raise Exception(\"Job failed\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    import os\n    import time\n    from typing import Optional\n\n    from .machine import Machine\n    from .studio import Studio\n\n    # Check if the required SDK is installed\n    if not Studio.is_installed():\n        raise RuntimeError(\n            \"The required SDK is not installed. Please install the SDK and try again.\"\n        )\n\n    # Get the current working directory\n    cwd = os.getcwd()\n\n    # Get the environment variables\n    env_vars = os.environ.copy()\n\n    # Define the default command\n    if command is None:\n        command = f\"cd {cwd} && {env_vars.get('SHELL', 'sh')}\"\n\n    # Create a Studio instance\n    studio = Studio()\n\n    # Get the machine configuration\n    if machine is None:\n        machine = studio.get_machine()\n\n    # Create a data preparation machine job\n    job = studio.create_job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    # Start the job\n    job.start()\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")\n\n    # Wait for the job to complete\n    while job.status not in [\"COMPLETED\", \"FAILED\", \"CANCELLED\"]:\n        time.sleep(1)\n        job.refresh()\n\n    # Print the job status\n    print(f\"Job status: {job.status}\")\n\n    # Raise an exception if the job failed\n    if job.status == \"FAILED\":\n        raise RuntimeError(f\"Job failed with error: {job.error_message}\")\n\n    # Raise an exception if the job was cancelled\n    if job.status == \"CANCELLED\":\n        raise RuntimeError(\"Job was cancelled\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not command:\n        command = f\"cd {os.getcwd()} && {os.environ['SHELL']}\"\n\n    if not machine:\n        machine = Machine.get_default()\n\n    job = Job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n        image=\"python:3.9\",\n    )\n\n    job.run()\n\n    print(f\"Job started: {job.url}\")\n\n    job.wait_for_completion()\n\n    if job.status == \"Failed\":\n        raise Exception(f\"Job failed: {job.url}\")\n\n    print(f\"Job completed: {job.url}\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is available\n    if not is_sdk_available():\n        raise ImportError(\n            \"The execute function requires the SDK to be available. Please install the SDK and try again.\"\n        )\n\n    # Fetch the default machine configuration if not provided\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, memory=2, image=\"default\")\n\n    # Fetch the default command if not provided\n    if command is None:\n        command = f\"cd {os.getcwd()} && source {os.environ['VIRTUAL_ENV']}/bin/activate && python -m {__name__}\"\n\n    # Create a data preparation machine job through the Studio API\n    job = Job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n    job.create()\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")\n\n    # Continuously check the job status and print the job URL when it starts\n    while True:\n        job.refresh()\n        if job.status == \"running\":\n            print(f\"Job URL: {job.url}\")\n            break\n        elif job.status == \"failed\":\n            raise Exception(f\"Job failed with error: {job.error_message}\")\n        time.sleep(5)\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")\n\n    # Wait for the job to complete\n    while True:\n        job.refresh()\n        if job.status == \"completed\":\n            break\n        time.sleep(5)\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is installed\n    if not _check_sdk_installed():\n        raise Exception(\"SDK is not installed. Please install it using 'pip install -U dvc[ssh]'\")\n\n    # Check if the required SDK version is compatible\n    if not _check_sdk_version():\n        raise Exception(\n            \"SDK version is not compatible. Please upgrade to the latest version using 'pip install -U dvc[ssh]'\"\n        )\n\n    # Get the current working directory\n    cwd = os.getcwd()\n\n    # Get the environment variables\n    env = os.environ.copy()\n\n    # Create a job\n    job = Job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command or f\"python -m dvc.main remote -d {cwd}\",\n        env=env,\n    )\n\n    # Submit the job\n    job_id = job.submit()\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")\n\n    # Wait for the job to complete\n    while True:\n        status = job.status()\n        if status == \"failed\":\n            raise Exception(f\"Job failed with error: {job.error}\")\n        elif status == \"finished\":\n            break\n        time.sleep(1)\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not machine:\n        machine = Machine(\n            cpu=1,\n            ram_gb=2,\n            gpu=0,\n            gpu_type=None,\n            gpu_per_node=0,\n            gpu_ram_gb=0,\n            gpu_ram_per_node=0,\n        )\n\n    if not command:\n        command = f\"cd {os.getcwd()} && {os.environ['SHELL']}\"\n\n    job = Job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    try:\n        job.submit()\n    except Exception as e:\n        raise e\n\n    job.wait()\n\n    if job.status == \"FAILED\":\n        raise Exception(\"Job failed\")\n\n    if job.status == \"COMPLETED\":\n        print(f\"Job completed: {job.url}\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is installed\n    if not _is_sdk_installed():\n        raise Exception(\"SDK is not installed. Please install the SDK.\")\n\n    # Get the current working directory\n    cwd = os.getcwd()\n\n    # Get the environment variables\n    env_vars = os.environ\n\n    # Get the machine configuration\n    if not machine:\n        machine = _get_default_machine()\n\n    # Construct the command to be executed\n    if not command:\n        command = f\"cd {cwd} && {env_vars.get('SHELL', '/bin/bash')}\"\n\n    # Create the job through the Studio API\n    job = _create_job(name, num_nodes, machine, command)\n\n    # Print the job URL when the job starts\n    job.on_started(lambda job: print(f\"Job URL: {job.url}\"))\n\n    # Print the job status and log when the job finishes\n    job.on_finished(lambda job: print(f\"Job {job.name} finished with status {job.status}\"))\n    job.on_log_message(lambda job, message: print(f\"Job {job.name} log: {message}\"))\n\n    # Wait for the job to finish\n    job.wait_until_finished()\n\n    # Check if the job failed\n    if job.status != \"SUCCESS\":\n        raise Exception(f\"Job {job.name} failed with status {job.status}\")\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        self.deletion_queue.put(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        self.deletion_queue.put(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        self.deletion_queue.put(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.deletion_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.deletion_queue.put(chunk_index)\n\n        return"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.deletion_queue.put(chunk_index)\n\n        return"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        self.delete_queue.put(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.delete_queue.put(chunk_index)\n\n        return"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        self.deletion_queue.append(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        # Add the chunk indexes to the deletion queue.\n        self.deletion_queue.put(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.deletion_queue.put(chunk_index)\n\n        self.deletion_queue.put(None)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.deletion_queue.put(chunk_index)\n\n        # Wait for the deletion queue to be empty\n        while not self.deletion_queue.empty():\n            pass\n\n        # Wait for the deletion queue to be empty\n        while not self.deletion_queue.empty():\n            pass"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        self.deletion_queue.put(chunk_indexes)\n\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.delete_queue.put(chunk_index)\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        self.chunk_indexes_to_delete.extend(chunk_indexes)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        if self.is_running:\n            self.delete_queue.put(chunk_indexes)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._chunk_indexes_to_delete.append(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self.delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the index files are available\n        if not self._index_files_available():\n            return None\n\n        # Attempt to load the configuration\n        config = self._load_config()\n\n        # Update the instance's configuration\n        self._config = config\n\n        return config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._cache_dir is None:\n            return None\n\n        config_path = os.path.join(self._cache_dir, \"chunks_config.json\")\n        if not os.path.exists(config_path):\n            return None\n\n        with open(config_path, \"r\") as f:\n            config_data = json.load(f)\n\n        config = ChunksConfig.from_dict(config_data)\n\n        self._serializers = config.serializers\n        self._remote_input_dir = config.remote_input_dir\n        self._item_loader = config.item_loader\n\n        return config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the index files are available\n        if not self._index_files_available():\n            return None\n\n        # Attempt to load the configuration\n        try:\n            config = ChunksConfig.load(\n                self._cache_dir,\n                self._serializers,\n                self._remote_input_dir,\n                self._item_loader,\n            )\n        except Exception as e:\n            # Log the error and return None if the configuration could not be loaded\n            logger.error(f\"Failed to load chunks configuration: {e}\")\n            return None\n\n        # Update the instance's configuration with the loaded ChunksConfig object\n        self._config = config\n        return config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._remote_input_dir is None:\n            return None\n\n        config_file_path = os.path.join(self._cache_dir, \"chunks_config.json\")\n        if os.path.exists(config_file_path):\n            with open(config_file_path, \"r\") as f:\n                config_json = json.load(f)\n                config = ChunksConfig.from_json(config_json)\n                self._config = config\n                return config\n        else:\n            return None"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the index files are available\n        if not self._index_files_available:\n            return None\n\n        # Attempt to load the configuration\n        try:\n            config = ChunksConfig.load(\n                self._cache_dir, self._serializers, self._remote_input_dir, self._item_loader\n            )\n        except Exception as e:\n            logger.warning(f\"Failed to load chunks config: {e}\")\n            return None\n\n        # Update the instance's configuration with the loaded ChunksConfig object\n        self._config = config\n        return config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._remote_input_dir:\n            return None\n\n        if self._item_loader:\n            return None\n\n        config_file = os.path.join(self._cache_dir, \"chunks_config.json\")\n        if not os.path.exists(config_file):\n            return None\n\n        with open(config_file, \"r\") as f:\n            config_dict = json.load(f)\n\n        config = ChunksConfig.from_dict(config_dict)\n        self._serializers = config.serializers\n        self._item_loader = config.item_loader\n        return config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._cache_dir is None:\n            return None\n\n        config_path = os.path.join(self._cache_dir, \"chunks.json\")\n        if not os.path.exists(config_path):\n            return None\n\n        with open(config_path, \"r\") as f:\n            config_data = json.load(f)\n\n        config = ChunksConfig(\n            config_data[\"chunk_size\"],\n            config_data[\"chunk_count\"],\n            config_data[\"serializers\"],\n            config_data[\"remote_input_dir\"],\n            config_data[\"item_loader\"],\n        )\n\n        self._serializers = config.serializers\n        self._remote_input_dir = config.remote_input_dir\n        self._item_loader = config.item_loader\n\n        return config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        try:\n            config = ChunksConfig.load_from_cache(\n                self._cache_dir,\n                self._serializers,\n                self._remote_input_dir,\n                self._item_loader,\n            )\n            self.config = config\n            return config\n        except Exception as e:\n            logger.warning(\n                \"Failed to load chunks config from cache. Exception: %s\", str(e)\n            )\n            return None"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if not self._remote_input_dir:\n            return None\n\n        if not self._cache_dir:\n            return None\n\n        if not self._serializers:\n            return None\n\n        if not self._item_loader:\n            return None\n\n        try:\n            config_path = os.path.join(self._cache_dir, \"chunks_config.json\")\n            with open(config_path, \"r\") as f:\n                config_dict = json.load(f)\n            config = ChunksConfig.from_dict(config_dict)\n        except FileNotFoundError:\n            return None\n\n        self._config = config\n        return config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if not self._item_loader:\n            return None\n\n        config_path = os.path.join(self._cache_dir, \"config.json\")\n        if not os.path.exists(config_path):\n            return None\n\n        with open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n\n        config = ChunksConfig.from_dict(config_dict)\n        self._config = config\n        return config\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._cache_dir is None:\n            return None\n\n        config_path = os.path.join(self._cache_dir, \"chunks_config.json\")\n        if not os.path.exists(config_path):\n            return None\n\n        with open(config_path, \"r\") as f:\n            config = ChunksConfig.from_json(json.load(f))\n\n        if self._serializers is None:\n            self._serializers = config.serializers\n        if self._remote_input_dir is None:\n            self._remote_input_dir = config.remote_input_dir\n        if self._item_loader is None:\n            self._item_loader = config.item_loader\n\n        return config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if not self._item_loader:\n            return None\n\n        if not self._item_loader.has_index():\n            return None\n\n        config_path = os.path.join(self._cache_dir, \"chunks.json\")\n        if os.path.exists(config_path):\n            with open(config_path, \"r\") as f:\n                config = json.load(f)\n            return ChunksConfig(**config)\n\n        config = self._item_loader.load_chunks_config()\n        if config:\n            with open(config_path, \"w\") as f:\n                json.dump(config.dict(), f)\n        return config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Try to load the configuration from the cache\n        config = self._load_config_from_cache()\n\n        if config is not None:\n            self._config = config\n            return config\n\n        # Try to load the configuration from the remote input directory\n        config = self._load_config_from_remote_input_dir()\n\n        if config is not None:\n            self._config = config\n            return config\n\n        # Try to load the configuration from the local input directory\n        config = self._load_config_from_local_input_dir()\n\n        if config is not None:\n            self._config = config\n            return config\n\n        # No configuration could be loaded\n        return None\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._remote_input_dir:\n            return None\n\n        if not self._cache_dir:\n            return None\n\n        if not self._item_loader:\n            return None\n\n        if not self._serializers:\n            return None\n\n        config_path = self._get_chunks_config_path()\n\n        if not config_path:\n            return None\n\n        if not os.path.isfile(config_path):\n            return None\n\n        with open(config_path, \"rb\") as f:\n            config = pickle.load(f)\n\n        if not isinstance(config, ChunksConfig):\n            return None\n\n        self._chunks_config = config\n        return config\n\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._remote_input_dir is not None:\n            remote_input_dir = self._remote_input_dir\n        else:\n            remote_input_dir = self._cache_dir\n        if self._serializers is not None:\n            serializers = self._serializers\n        else:\n            serializers = self._item_loader.serializers\n        if self._item_loader is not None:\n            item_loader = self._item_loader\n        else:\n            item_loader = self._item_loader\n\n        if not isinstance(remote_input_dir, str):\n            raise ValueError(\"remote_input_dir must be a string\")\n        if not isinstance(serializers, dict):\n            raise ValueError(\"serializers must be a dict\")\n        if not isinstance(item_loader, ItemLoader):\n            raise ValueError(\"item_loader must be an ItemLoader\")\n\n        if not os.path.exists(remote_input_dir):\n            raise ValueError(f\"remote_input_dir {remote_input_dir} does not exist\")\n\n        config_path = os.path.join(remote_input_dir, \"config.json\")\n        if not os.path.exists(config_path):\n            return None\n\n        with open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n\n        config = ChunksConfig(\n            remote_input_dir=remote_input_dir,\n            serializers=serializers,\n            item_loader=item_loader,\n            **config_dict\n        )\n\n        self._config = config\n\n        return config\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._cache_dir is not None:\n            config_path = os.path.join(self._cache_dir, \"config.json\")\n            if os.path.exists(config_path):\n                with open(config_path, \"r\") as f:\n                    config = json.load(f)\n                return ChunksConfig.from_dict(config)\n        elif self._remote_input_dir is not None:\n            config_path = os.path.join(self._remote_input_dir, \"config.json\")\n            if self._item_loader.exists(config_path):\n                config = self._item_loader.load(config_path)\n                return ChunksConfig.from_dict(config)\n        return None\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        try:\n            config = ChunksConfig.load(\n                self._cache_dir,\n                self._serializers,\n                self._remote_input_dir,\n                self._item_loader,\n            )\n            self._config = config\n            return config\n        except Exception:\n            return None\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        config = self._load_config()\n        if config is not None:\n            self._config = config\n            return config\n        return None\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        if self._cache_dir is not None:\n            config_path = os.path.join(self._cache_dir, \"chunks.json\")\n            if os.path.exists(config_path):\n                with open(config_path, \"r\") as f:\n                    config = ChunksConfig.from_dict(json.load(f))\n                self._config = config\n                return config\n\n        if self._remote_input_dir is not None:\n            config_path = os.path.join(self._remote_input_dir, \"chunks.json\")\n            if self._item_loader is not None:\n                config = self._item_loader.load_item(config_path)\n                if config is not None:\n                    self._config = config\n                    return config\n\n        return None\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        config = None\n        if self._cache_dir and self._serializers and self._remote_input_dir and self._item_loader:\n            config = ChunksConfig(\n                self._cache_dir,\n                self._serializers,\n                self._remote_input_dir,\n                self._item_loader,\n            )\n\n        return config\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        self._to_download_queue.extend(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        self._to_download_queue.extend(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        self._to_download_queue.extend(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        self._to_download_queue.extend(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        self._to_download_queue.extend(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        self._to_download_queue.put(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        self._to_download_queue.extend(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        self._to_download_queue.put(chunk_indexes)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        self._to_download_queue.put(chunk_indexes)\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it.\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it.\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration not set\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration not defined\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration not defined\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration is not set\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Config not set\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError('Configuration should be defined before accessing it')\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration is not set\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration not set\")\n\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError('The configuration is not set.')\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it\")\n        return self._config\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it\")\n\n        return self._config\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration is not set\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration not defined\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration is not defined\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"Index must be an instance of ChunkedIndex\")\n\n        if self.index_config is None:\n            raise Exception(\"Index configuration is not defined\")\n\n        if index.chunk_id not in self.chunks:\n            self.load_chunk(index.chunk_id)\n\n        chunk = self.chunks[index.chunk_id]\n        item = chunk.get_item(index.item_id)\n\n        if item is None:\n            self.load_chunk(index.chunk_id)\n            chunk = self.chunks[index.chunk_id]\n            item = chunk.get_item(index.item_id)\n\n        if item is None:\n            raise Exception(f\"Item {index.item_id} not found in chunk {index.chunk_id}\")\n\n        return item"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"Invalid index type: {}\".format(type(index)))\n\n        if self._index_config is None:\n            raise Exception(\"Index configuration not defined\")\n\n        if index.chunk_id not in self._chunks:\n            self._load_chunk(index.chunk_id)\n\n        if index.chunk_id not in self._chunks:\n            raise Exception(\"Chunk {} not loaded\".format(index.chunk_id))\n\n        if index.item_id not in self._chunks[index.chunk_id]:\n            self._load_item(index.chunk_id, index.item_id)\n\n        if index.chunk_id not in self._chunks:\n            raise Exception(\"Chunk {} not loaded\".format(index.chunk_id))\n\n        if index.item_id not in self._chunks[index.chunk_id]:\n            raise Exception(\"Item {} not loaded\".format(index.item_id))\n\n        return self._chunks[index.chunk_id][index.item_id]"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\n                f\"Expected ChunkedIndex, got {type(index).__name__}\")\n\n        if self.index_config is None:\n            raise Exception(\"Index configuration is not defined\")\n\n        if index.chunk_id not in self.chunks:\n            self.chunks[index.chunk_id] = self.load_chunk(index.chunk_id)\n\n        assert self.prepare_thread is not None\n\n        self.prepare_thread.prefetch(index.chunk_id)\n\n        return self.chunks[index.chunk_id].read(index.item_id)"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"Index must be an instance of ChunkedIndex\")\n\n        if self.index_config is None:\n            raise Exception(\"Index configuration not defined\")\n\n        # Ensure chunk is available either locally or in memory\n        self._ensure_chunk_available(index)\n\n        # Prefetch chunk if necessary\n        if self.prefetch_config is not None:\n            self._prefetch_chunk(index)\n\n        # Load item from chunk\n        item = self._load_item(index)\n\n        # Handle lifecycle of chunk\n        self._handle_chunk_lifecycle(index)\n\n        return item"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"index must be an instance of ChunkedIndex\")\n\n        if self.index_config is None:\n            raise Exception(\"index_config is not defined\")\n\n        if index.chunk_id not in self.chunks:\n            self.prefetch(index)\n\n        assert self.prepare_thread is not None\n\n        if index.chunk_id not in self.chunks:\n            raise Exception(\"Chunk not found\")\n\n        chunk = self.chunks[index.chunk_id]\n        item = chunk.read(index.item_id)\n\n        if chunk.is_empty():\n            self.chunks.pop(index.chunk_id)\n            self.prepare_thread.delete_chunk(chunk)\n\n        return item"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index must be an instance of ChunkedIndex.\")\n\n        if self.index_config is None:\n            raise Exception(\"The index configuration is not defined.\")\n\n        if index.chunk_id not in self.chunks:\n            self.chunks[index.chunk_id] = self._load_chunk(index.chunk_id)\n\n        chunk = self.chunks[index.chunk_id]\n\n        if index.item_id >= len(chunk):\n            raise Exception(\"The item id is out of range.\")\n\n        item = chunk[index.item_id]\n\n        if item is None:\n            item = self._load_item(index)\n            chunk[index.item_id] = item\n\n        return item"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"Index must be of type ChunkedIndex\")\n\n        if not self.index_config:\n            raise Exception(\"Index configuration not defined\")\n\n        # If the chunk is not available locally or in memory, download it\n        if not self.is_chunk_available(index):\n            self.download_chunk(index)\n\n        # Ensure the chunk is available locally or in memory\n        if not self.is_chunk_available(index):\n            raise Exception(\"Chunk not available\")\n\n        # Prefetch the chunk if it is not already being prefetched\n        if not self.is_chunk_prefetched(index):\n            self.prefetch_chunk(index)\n\n        # Ensure the chunk is prefetched\n        if not self.is_chunk_prefetched(index):\n            raise Exception(\"Chunk not prefetched\")\n\n        # Ensure the chunk is not being deleted\n        if self.is_chunk_being_deleted(index):\n            raise Exception(\"Chunk is being deleted\")\n\n        # Ensure the chunk is not being prefetched\n        if self.is_chunk_being_prefetched(index):\n            raise Exception(\"Chunk is being prefetched\")\n\n        # Ensure the chunk is not being downloaded\n        if self.is_chunk_being_downloaded(index):\n            raise Exception(\"Chunk is being downloaded\")\n\n        # Ensure the chunk is not being read\n        if self.is_chunk_being_read(index):\n            raise Exception(\"Chunk is being read\")\n\n        # Ensure the chunk is not being written\n        if self.is_chunk_being_written(index):\n            raise Exception(\"Chunk is being written\")\n\n        # Ensure the chunk is not being deleted\n        if self.is_chunk_being_deleted(index):\n            raise Exception(\"Chunk is being deleted\")\n\n        # Ensure the chunk is not being prefetched\n        if self.is_chunk_being_prefetched(index):\n            raise Exception"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"Invalid index type. Expected ChunkedIndex.\")\n\n        if self.index_config is None:\n            raise Exception(\"Index configuration is not defined.\")\n\n        # Get the chunk from the index\n        chunk = self.index_config.get_chunk(index)\n\n        # Check if the chunk is available locally or in memory\n        if chunk.is_available():\n            # Chunk is available, load the item from the chunk\n            item = chunk.load_item(index)\n        else:\n            # Chunk is not available, initiate its download\n            self._download_chunk(chunk)\n\n            # Prefetch the next chunk if available\n            self._prefetch_next_chunk(chunk)\n\n            # Load the item from the chunk\n            item = chunk.load_item(index)\n\n        # Delete the chunk if it has been fully consumed\n        if chunk.is_fully_consumed():\n            self._delete_chunk(chunk)\n\n        return item"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"Index must be an instance of ChunkedIndex\")\n\n        if self._index_config is None:\n            raise Exception(\"Index config must be defined\")\n\n        if self._index_config.get_chunk_size() == 0:\n            raise Exception(\"Chunk size must be defined\")\n\n        chunk_id = index.get_chunk_id()\n        item_id = index.get_item_id()\n\n        # Check if the chunk is available locally\n        if self._local_chunk_loader.has_chunk(chunk_id):\n            chunk = self._local_chunk_loader.get_chunk(chunk_id)\n            item = chunk.get_item(item_id)\n            return item\n\n        # Check if the chunk is available in memory\n        if self._memory_chunk_loader.has_chunk(chunk_id):\n            chunk = self._memory_chunk_loader.get_chunk(chunk_id)\n            item = chunk.get_item(item_id)\n            return item\n\n        # Check if the chunk is available in the cache\n        if self._cache_chunk_loader.has_chunk(chunk_id):\n            chunk = self._cache_chunk_loader.get_chunk(chunk_id)\n            item = chunk.get_item(item_id)\n            return item\n\n        # Check if the chunk is available in the prefetch cache\n        if self._prefetch_chunk_loader.has_chunk(chunk_id):\n            chunk = self._prefetch_chunk_loader.get_chunk(chunk_id)\n            item = chunk.get_item(item_id)\n            return item\n\n        # Check if the chunk is available in the remote cache\n        if self._remote_chunk_loader.has_chunk(chunk_id):\n            chunk = self._remote_chunk_loader.get_chunk(chunk_id)\n            item = chunk.get_item(item_id)\n            return item"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"{index} is not a valid index\")\n\n        if not self.index_config:\n            raise Exception(\"index config is not defined\")\n\n        if self.index_config.get_chunk_size() != index.get_chunk_size():\n            raise Exception(\"index config chunk size is not equal to index chunk size\")\n\n        if self.index_config.get_chunk_count() != index.get_chunk_count():\n            raise Exception(\"index config chunk count is not equal to index chunk count\")\n\n        if index.get_chunk_number() >= self.index_config.get_chunk_count():\n            raise Exception(\"index chunk number is greater than index config chunk count\")\n\n        if index.get_chunk_number() < 0:\n            raise Exception(\"index chunk number is less than 0\")\n\n        if index.get_chunk_number() >= self.index_config.get_chunk_count():\n            raise Exception(\"index chunk number is greater than index config chunk count\")\n\n        if index.get_chunk_number() < 0:\n            raise Exception(\"index chunk number is less than 0\")\n\n        if index.get_item_number() >= self.index_config.get_chunk_size():\n            raise Exception(\"index item number is greater than index config chunk size\")\n\n        if index.get_item_number() < 0:\n            raise Exception(\"index item number is less than 0\")\n\n        if index.get_item_number() >= self.index_config.get_chunk_size():\n            raise Exception(\"index item number is greater than index config chunk size\")\n\n        if index.get_item_number() < 0:\n            raise Exception(\"index item number is less than 0\")\n\n        if self.index_config.get_chunk_size() == 0:\n            raise Exception(\"index config chunk size is 0\")\n\n        if self.index_config.get_chunk_count() == 0:\n            raise Exception(\"index config chunk count is 0\")"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"Invalid index type: {type(index)}\")\n\n        if not self.index_config:\n            raise Exception(\"Index configuration is not defined\")\n\n        if index.chunk_id not in self.chunks:\n            self.chunks[index.chunk_id] = self.chunk_loader.load(index)\n\n        chunk = self.chunks[index.chunk_id]\n\n        if not isinstance(chunk, Chunk):\n            raise Exception(f\"Invalid chunk type: {type(chunk)}\")\n\n        if chunk.state != ChunkState.READY:\n            if chunk.state == ChunkState.NOT_LOADED:\n                self.prepare_thread.start()\n\n            if chunk.state == ChunkState.DOWNLOADING:\n                self.prepare_thread.join()\n\n        if chunk.state == ChunkState.READY:\n            return self.item_loader.load(chunk, index.item_id)\n\n        raise Exception(f\"Invalid chunk state: {chunk.state}\")"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"Index must be a ChunkedIndex instance.\")\n\n        if self.index_config is None:\n            raise Exception(\"Index configuration not defined.\")\n\n        # Ensure the chunk is available either locally or in memory\n        self.prepare_thread.assert_thread_presence()\n        chunk = self.prepare_thread.get_chunk(index)\n\n        if chunk is None:\n            # Initiate the download of the chunk\n            self.prepare_thread.download_chunk(index)\n            self.prepare_thread.assert_thread_presence()\n            chunk = self.prepare_thread.get_chunk(index)\n\n        if chunk is None:\n            raise Exception(\"Chunk not found.\")\n\n        # Load the item from the chunk\n        item = self.item_loader.load(chunk)\n\n        # Check if the chunk should be deleted\n        if chunk.is_consumed():\n            self.prepare_thread.delete_chunk(index)\n\n        return item"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index must be an instance of ChunkedIndex.\")\n\n        if self.index_config is None:\n            raise Exception(\"The reader's index configuration is not defined.\")\n\n        # Prepare the chunk if it is not already prepared.\n        self.prepare_thread.ensure_chunk_prepared(index)\n\n        # Get the chunk from the cache.\n        chunk = self.cache.get_chunk(index)\n\n        # If the chunk is not in the cache, load it from the storage.\n        if chunk is None:\n            chunk = self.storage.load_chunk(index)\n\n        # Get the item from the chunk.\n        item = chunk.get_item(index)\n\n        # Delete the chunk from the cache if it is fully consumed.\n        if chunk.is_fully_consumed():\n            self.cache.delete_chunk(index)\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\n                \"The provided index is not an instance of ChunkedIndex.\"\n            )\n        if not self.index_config:\n            raise Exception(\"The reader's index configuration is not defined.\")\n\n        # Ensure the chunk is available either locally or in memory\n        chunk = self.get_chunk(index)\n        if chunk is None:\n            self.download_chunk(index)\n            chunk = self.get_chunk(index)\n\n        # Ensure the chunk is not fully consumed\n        if chunk.is_fully_consumed():\n            self.delete_chunk(index)\n            raise Exception(\n                \"The chunk is fully consumed. Please ensure that the chunk is not fully consumed before attempting to read from it.\"\n            )\n\n        # Prefetch the chunk if needed\n        if self.prefetch:\n            self.prefetch_chunk(index)\n\n        # Return the item\n        return chunk.read()\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"Invalid index type\")\n\n        if not self.index_config:\n            raise Exception(\"Index configuration not defined\")\n\n        if not self.prefetch_thread.is_alive():\n            self.prefetch_thread.start()\n\n        if not self.index_config.get_chunk_by_index(index):\n            self.prefetch_thread.add_index(index)\n\n        while not self.index_config.get_chunk_by_index(index):\n            time.sleep(0.0001)\n\n        chunk = self.index_config.get_chunk_by_index(index)\n\n        assert self.prepare_thread.is_alive(), \"Prepare thread is not alive\"\n\n        if chunk.is_loading:\n            while chunk.is_loading:\n                time.sleep(0.0001)\n\n        if chunk.is_deleting:\n            while chunk.is_deleting:\n                time.sleep(0.0001)\n\n        if not chunk.is_loaded:\n            chunk.load()\n\n        assert chunk.is_loaded, \"Chunk is not loaded\"\n\n        return self.item_loader.load(chunk, index)"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"Invalid index type\")\n\n        if not self.index_config:\n            raise Exception(\"Index configuration not defined\")\n\n        if index.chunk_id not in self.chunk_cache:\n            self.prefetch(index)\n\n        chunk = self.chunk_cache[index.chunk_id]\n\n        if chunk.status != ChunkStatus.AVAILABLE:\n            raise Exception(\n                f\"Chunk {index.chunk_id} is not available. Status: {chunk.status}\"\n            )\n\n        if chunk.status == ChunkStatus.AVAILABLE:\n            if chunk.item_loader:\n                item = chunk.item_loader(chunk.data, index.item_index)\n            else:\n                item = chunk.data[index.item_index]\n\n            if chunk.lifecycle == ChunkLifecycle.EMPTY:\n                self.delete_chunk(index)\n\n            return item\n\n        raise Exception(\n            f\"Chunk {index.chunk_id} is not available. Status: {chunk.status}\"\n        )\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\n                \"The provided index is not an instance of ChunkedIndex\"\n            )\n\n        if not self._index_config:\n            raise Exception(\n                \"The reader's index configuration is not defined\"\n            )\n\n        if not self._prepare_thread:\n            raise Exception(\"The reader's prepare thread is not defined\")\n\n        assert self._prepare_thread.is_alive()\n\n        # Check if the chunk is available locally\n        if index.chunk_id in self._chunks:\n            chunk = self._chunks[index.chunk_id]\n        else:\n            # Check if the chunk is available in memory\n            chunk = self._chunk_manager.get_chunk(index.chunk_id)\n            if not chunk:\n                # The chunk is not available locally or in memory, so it must be downloaded\n                self._prepare_thread.add_chunk(index.chunk_id)\n\n        # Check if the chunk is fully consumed\n        if chunk.is_fully_consumed:\n            # The chunk is fully consumed, so it must be deleted\n            self._chunk_manager.delete_chunk(index.chunk_id)\n\n        # Check if the item is available in the chunk\n        if index.item_id in chunk.items:\n            item = chunk.items[index.item_id]\n        else:\n            # The item is not available in the chunk, so it must be loaded\n            self._prepare_thread.add_item(index)\n\n        return item\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"Expected ChunkedIndex, got {type(index)}\")\n\n        if self._index_config is None:\n            raise Exception(\"Reader index configuration is not defined.\")\n\n        if index.chunk_id not in self._chunk_lifecycle:\n            self._chunk_lifecycle[index.chunk_id] = ChunkLifecycle(\n                self._index_config.chunk_size,\n                self._index_config.chunk_prefetch_size,\n                self._index_config.chunk_prefetch_interval,\n                self._index_config.chunk_lifetime,\n            )\n\n        chunk_lifecycle = self._chunk_lifecycle[index.chunk_id]\n\n        if chunk_lifecycle.is_deleted():\n            raise Exception(\n                f\"Chunk {index.chunk_id} has been deleted. Please call `delete` method to delete the chunk.\"\n            )\n\n        if chunk_lifecycle.is_expired():\n            raise Exception(\n                f\"Chunk {index.chunk_id} has expired. Please call `delete` method to delete the chunk.\"\n            )\n\n        if chunk_lifecycle.is_available():\n            self._logger.debug(\n                f\"Chunk {index.chunk_id} is available. Loading item {index.item_id} from chunk.\"\n            )\n            return self._item_loader.load(index.chunk_id, index.item_id)\n\n        if chunk_lifecycle.is_downloading():\n            self._logger.debug(\n                f\"Chunk {index.chunk_id} is downloading. Waiting for download to complete.\"\n            )\n            chunk_lifecycle.wait_for_download()\n\n        if chunk_lifecycle.is_downloaded():\n            self._logger.debug(\n                f\"Chunk {index.chunk_id} is downloaded. Loading item {index.item_"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"Index must be a ChunkedIndex\")\n\n        if not self.index_config:\n            raise Exception(\"Index config not defined\")\n\n        if index.chunk_id not in self.chunks:\n            self._prefetch_chunk(index.chunk_id)\n\n        assert self.prepare_thread is not None\n\n        if index.chunk_id not in self.chunks:\n            raise Exception(\"Chunk not found\")\n\n        chunk = self.chunks[index.chunk_id]\n\n        if not chunk.is_available:\n            self._wait_for_chunk_to_be_available(chunk)\n\n        assert chunk.is_available\n\n        return self.item_loader.read(chunk, index.item_id)\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\n                f\"index must be an instance of ChunkedIndex, not {type(index)}\")\n\n        if not self.index_config:\n            raise Exception(\"index_config is not defined\")\n\n        # Get the chunk from the index\n        chunk = self.index_config.get_chunk(index)\n\n        # If the chunk is not available locally, initiate its download\n        if chunk.status != ChunkStatus.AVAILABLE:\n            if chunk.status == ChunkStatus.INITIATED:\n                self.download_manager.add_chunk(chunk)\n            elif chunk.status == ChunkStatus.PREFETCHED:\n                self.download_manager.prefetch_chunk(chunk)\n\n        # Wait for the chunk to be available\n        chunk.wait_until_available()\n\n        # If the chunk is not in memory, load it\n        if chunk.status == ChunkStatus.AVAILABLE and not chunk.in_memory:\n            self.load_manager.load_chunk(chunk)\n\n        # Ensure the chunk is in memory\n        if chunk.status == ChunkStatus.AVAILABLE and not chunk.in_memory:\n            chunk.wait_until_in_memory()\n\n        # If the chunk is not in memory, raise an exception\n        if chunk.status == ChunkStatus.AVAILABLE and not chunk.in_memory:\n            raise Exception(\n                f\"Chunk {chunk.index} is not in memory after waiting for it to be loaded\")\n\n        # Ensure the chunk is not being deleted\n        chunk.wait_until_not_deleting()\n\n        # Ensure the chunk is not being deleted\n        chunk.wait_until_not_deleting()\n\n        # Get the item from the chunk\n        item = self.item_loader.load_item(chunk, index)\n\n        # If the chunk is not being deleted, delete it\n        if not chunk.deleting:\n            self.delete_manager.delete_chunk(chunk)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if is_distributed():\n        return get_distributed_map().broadcast_object(key, obj)\n    else:\n        return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if is_distributed():\n        return get_distributed_map().get(key, obj)\n    return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if \"EXTERNAL_URL\" in os.environ:\n        return dmap.get(key, obj)\n    else:\n        return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if hasattr(app, \"external_url\"):\n        if not hasattr(app, \"distributed_map\"):\n            app.distributed_map = {}\n        app.distributed_map[key] = obj\n        return app.distributed_map[key]\n    else:\n        return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if 'EXTERNAL_URL' in os.environ:\n        if key not in distributed_map:\n            distributed_map[key] = obj\n        return distributed_map[key]\n    else:\n        return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if is_distributed():\n        from dask.distributed import get_client\n        client = get_client()\n        client.broadcast(obj, key)\n        return client.get(key)\n    else:\n        return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.environ.get(\"EXTERNAL_URL\", None) is not None:\n        return distributed_map.get(key, obj)\n    else:\n        return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if hasattr(obj, \"__getstate__\"):\n        obj = obj.__getstate__()\n\n    if hasattr(obj, \"__setstate__\"):\n        obj = obj.__setstate__(obj)\n\n    if hasattr(obj, \"__getitem__\"):\n        obj = obj.__getitem__(obj)\n\n    if hasattr(obj, \"__setitem__\"):\n        obj = obj.__setitem__(obj)\n\n    if hasattr(obj, \"__getattr__\"):\n        obj = obj.__getattr__(obj)\n\n    if hasattr(obj, \"__setattr__\"):\n        obj = obj.__setattr__(obj)\n\n    if hasattr(obj, \"__getattribute__\"):\n        obj = obj.__getattribute__(obj)\n\n    if hasattr(obj, \"__setattribute__\"):\n        obj = obj.__setattribute__(obj)\n\n    if hasattr(obj, \"__getstate__\"):\n        obj = obj.__getstate__()\n\n    if hasattr(obj, \"__setstate__\"):\n        obj = obj.__setstate__(obj)\n\n    if hasattr(obj, \"__getitem__\"):\n        obj = obj.__getitem__(obj)\n\n    if hasattr(obj, \"__setitem__\"):\n        obj = obj.__setitem__(obj)\n\n    if hasattr(obj, \"__getattr__\"):\n        obj = obj.__getattr__(obj)\n\n    if hasattr(obj, \"__setattr__\"):\n        obj = obj.__setattr__(obj)\n\n    if hasattr(obj, \"__getattribute__\"):\n        obj = obj.__getattribute__(obj)\n\n    if hasattr(obj, \"__setattribute__\"):\n        obj = obj.__setattribute__(obj)\n\n    if hasattr(obj, \"__getstate__\"):\n        obj = obj.__getstate__()\n\n    if hasattr(obj, \"__setstate__\"):\n        obj = obj.__setstate__(obj)\n\n    if hasattr(obj, \"__getitem__\"):"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if hasattr(obj, \"__dask_token__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_delayed__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_graph__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_keys__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_keys__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_token__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_delayed__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_graph__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_keys__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_keys__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_token__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_delayed__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_graph__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_keys__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_keys__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_token__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_delayed__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_graph__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_keys__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_keys__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_token__\"):\n        return obj\n\n    if hasattr(obj, \"__dask_delayed__\"):\n        return obj\n\n    if hasattr(obj, \"__"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if \"EXTERNAL_URL\" in os.environ:\n        obj_map = distributed_map.DistributedMap()\n        obj_map.put(key, obj)\n        return obj_map.get(key)\n    else:\n        return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    # Check if the application is running in an environment with an external URL (indicating a distributed setting)\n    if os.environ.get(\"EXTERNAL_URL\"):\n        # Use an immutable distributed map to share the object\n        distributed_map = distributed.immutable_distributed_map()\n        distributed_map[key] = obj\n        return distributed_map[key]\n    else:\n        # Return the object as is if not in a distributed environment\n        return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if hasattr(obj, \"__dict__\"):\n        obj = obj.__dict__\n    if hasattr(obj, \"__getstate__\"):\n        obj = obj.__getstate__()\n    if hasattr(obj, \"__getitem__\"):\n        obj = obj.__getitem__()\n    if hasattr(obj, \"__getattribute__\"):\n        obj = obj.__getattribute__()\n    if hasattr(obj, \"__getattr__\"):\n        obj = obj.__getattr__()\n    if hasattr(obj, \"__get__\"):\n        obj = obj.__get__()\n    if hasattr(obj, \"__getter__\"):\n        obj = obj.__getter__()\n    if hasattr(obj, \"__getter__\"):\n        obj = obj.__getter__()\n    if hasattr(obj, \"__getter__\"):\n        obj = obj.__getter__()\n    if hasattr(obj, \"__getter__\"):\n        obj = obj.__getter__()\n    if hasattr(obj, \"__getter__\"):\n        obj = obj.__getter__()\n    if hasattr(obj, \"__getter__\"):\n        obj = obj.__getter__()\n    if hasattr(obj, \"__getter__\"):\n        obj = obj.__getter__()\n    if hasattr(obj, \"__getter__\"):\n        obj = obj.__getter__()\n    if hasattr(obj, \"__getter__\"):\n        obj = obj.__getter__()\n    if hasattr(obj, \"__getter__\"):\n        obj = obj.__getter__()\n    if hasattr(obj, \"__getter__\"):\n        obj = obj.__getter__()\n    if hasattr(obj, \"__getter__\"):\n        obj = obj.__getter__()\n    if hasattr(obj, \"__getter__\"):\n        obj = obj.__getter__()\n    if hasattr(obj, \"__getter__\"):\n        obj = obj.__getter__()\n    if hasattr"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if is_distributed():\n        if not is_rank_zero():\n            obj = get_distributed_map().get(key)\n        else:\n            get_distributed_map().put(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if not is_distributed_environment():\n        return obj\n\n    global_dist_map = get_global_dist_map()\n    if global_dist_map is None:\n        return obj\n\n    return global_dist_map.get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if hasattr(obj, '__dict__'):\n        obj = obj.__dict__\n\n    if 'EXTERNAL_URL' in os.environ:\n        # Get the distributed map\n        distributed_map = get_distributed_map()\n\n        # Broadcast the object to all machines\n        distributed_map.put(key, obj)\n\n        # Retrieve the object from the distributed map\n        obj = distributed_map.get(key)\n\n    return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if is_distributed():\n        from ray.experimental.internal_api import internal_get_global_context\n        from ray.experimental.internal_kv import _internal_kv_get\n\n        global_context = internal_get_global_context()\n        if global_context.node_ip_address is not None:\n            return _internal_kv_get(key)\n        else:\n            global_context.distributed_map.put(key, obj)\n            return obj\n    else:\n        return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if is_distributed():\n        return get_distributed_map().get(key, obj)\n    else:\n        return obj\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if get_external_url():\n        return get_distributed_map().get(key, obj)\n    else:\n        return obj\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if hasattr(obj, \"__call__\"):\n        # If the object is callable, we assume it is a function and we wrap it in a lambda function to ensure it is serializable\n        obj = lambda: obj()\n\n    if isinstance(obj, dict):\n        # If the object is a dictionary, we recursively broadcast the values\n        for key, value in obj.items():\n            obj[key] = broadcast_object(key, value)\n    elif isinstance(obj, list):\n        # If the object is a list, we recursively broadcast the elements\n        for i, element in enumerate(obj):\n            obj[i] = broadcast_object(f\"{key}_{i}\", element)\n    elif isinstance(obj, tuple):\n        # If the object is a tuple, we recursively broadcast the elements\n        obj = tuple(broadcast_object(f\"{key}_{i}\", element) for i, element in enumerate(obj))\n    elif isinstance(obj, set):\n        # If the object is a set, we recursively broadcast the elements\n        obj = set(broadcast_object(f\"{key}_{i}\", element) for i, element in enumerate(obj))\n\n    if hasattr(obj, \"__dict__\"):\n        # If the object has a dictionary representation, we recursively broadcast the values\n        for key, value in obj.__dict__.items():\n            obj.__dict__[key] = broadcast_object(key, value)\n\n    if hasattr(obj, \"__slots__\"):\n        # If the object has slots, we recursively broadcast the values\n        for slot in obj.__slots__:\n            value = getattr(obj, slot)\n            setattr(obj, slot, broadcast_object(f\"{key}_{slot}\", value))\n\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if not is_distributed():\n        return obj\n\n    if not isinstance(obj, (tuple, list, dict)):\n        raise ValueError(\"broadcast_object can only broadcast tuples, lists, or dicts\")\n\n    if is_master():\n        _broadcast_object_master(key, obj)\n    else:\n        _broadcast_object_worker(key, obj)\n\n    return obj\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the number of chunks per node\n    chunks_per_node = [sum(chunks_per_ranks[i]) for i in range(distributed_env.world_size)]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks_per_ranks[i]) for i in range(distributed_env.world_size)]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks_per_ranks[i]) for i in range(distributed_env.world_size)]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks_per_ranks[i]) for i in range(distributed_env.world_size)]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks_per_ranks[i]) for i in range(distributed_env.world_size)]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks_per_ranks[i]) for i in range(distributed_env.world_size)]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks_per_ranks[i]) for i in range(distributed_env.world_size)]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks_per_ranks[i]) for i in range(distributed_env.world_size)]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks_per_ranks[i]) for i in range(distributed_env.world_size)]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks_per_ranks[i]) for i in range(distributed_env.world_size)]\n\n    # Calculate the number of ch"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the seed for the current epoch\n    seed = seed + current_epoch\n\n    # Create a random number generator with the seed\n    rng = np.random.default_rng(seed)\n\n    # Shuffle the chunk indexes for each rank using the random number generator\n    shuffled_chunks_per_ranks = [rng.permutation(chunks) for chunks in chunks_per_ranks]\n\n    # Flatten the list of shuffled chunk indexes\n    shuffled_chunks = [chunk for chunks in shuffled_chunks_per_ranks for chunk in chunks]\n\n    # Return the flattened list of shuffled chunk indexes\n    return shuffled_chunks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Set the seed for the random number generator based on the current epoch.\n    random.seed(seed + current_epoch)\n\n    # Shuffle the chunk indexes assigned to each rank using the random number generator.\n    shuffled_chunks_per_ranks = [random.sample(chunks, len(chunks)) for chunks in chunks_per_ranks]\n\n    # Flatten the shuffled chunk indexes and return them.\n    return [chunk for chunks in shuffled_chunks_per_ranks for chunk in chunks]"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the total number of chunks across all nodes\n    total_chunks = sum(len(chunks) for chunks in chunks_per_ranks)\n\n    # Calculate the number of chunks per node\n    chunks_per_node = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the number of nodes\n    num_nodes = len(chunks_per_ranks)\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the number of ranks per node\n    ranks_per_node = [\n        sum(chunks_per_rank[:i + 1]) for i in range(num_nodes)\n    ]\n\n    # Calculate the number of chunks per node\n    chunks_per_node = [\n        sum(chunks_per_rank[ranks_per_node[i]:ranks_per_node[i + 1]])\n        for i in range(num_nodes)\n    ]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [\n        sum(chunks_per_rank[ranks_per_node[i]:ranks_per_node[i + 1]])\n        for i in range(num_nodes)\n    ]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [\n        sum(chunks_per_rank[ranks_per_node[i]:ranks_per_node[i + 1]])\n        for i in range(num_nodes)\n    ]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [\n        sum(chunks_per_rank[ranks_per_node[i]:ranks_per_node[i + 1]])\n        for i in range(num_nodes)\n    ]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [\n        sum(chunks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the number of chunks assigned to each rank\n    num_chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the number of chunks assigned to each node\n    num_chunks_per_node = [\n        sum(num_chunks_per_rank[i : i + distributed_env.world_size])\n        for i in range(0, len(num_chunks_per_rank), distributed_env.world_size)\n    ]\n\n    # Calculate the number of chunks assigned to each node and rank\n    num_chunks_per_node_rank = [\n        [\n            num_chunks_per_rank[i + j]\n            for j in range(0, len(num_chunks_per_rank), distributed_env.world_size)\n        ]\n        for i in range(distributed_env.world_size)\n    ]\n\n    # Calculate the number of chunks assigned to each node and rank\n    num_chunks_per_node_rank = [\n        [\n            num_chunks_per_rank[i + j]\n            for j in range(0, len(num_chunks_per_rank), distributed_env.world_size)\n        ]\n        for i in range(distributed_env.world_size)\n    ]\n\n    # Calculate the number of chunks assigned to each node and rank\n    num_chunks_per_node_rank = [\n        [\n            num_chunks_per_rank[i + j]\n            for j in range(0, len(num_chunks_per_rank), distributed_env.world_size)\n        ]\n        for i in range(distributed_env.world_size)\n    ]\n\n    # Calculate the number of chunks assigned to each node and rank\n    num_chunks_per_node_rank = [\n        [\n            num_chunks_per_rank[i + j]\n            for j in range(0, len(num_chunks_per_rank), distributed_env.world_size"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Create a random number generator with the provided seed and current epoch\n    rng = torch.Generator()\n    rng.manual_seed(seed + current_epoch)\n\n    # Shuffle the chunk indexes for each rank using the random number generator\n    for rank, chunk_idxes in enumerate(chunks_per_ranks):\n        chunks_per_ranks[rank] = torch.randperm(len(chunk_idxes), generator=rng).tolist()\n\n    # Flatten the list of shuffled chunk indexes\n    shuffled_chunk_idxes = [chunk_idx for chunk_idxes in chunks_per_ranks for chunk_idx in chunk_idxes]\n\n    return shuffled_chunk_idxes\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Initialize the random number generator with the provided seed and current epoch\n    rng = np.random.default_rng(seed=seed + current_epoch)\n\n    # Shuffle each sublist of chunk indexes assigned to a specific rank using the random number generator\n    for rank_chunks in chunks_per_ranks:\n        rng.shuffle(rank_chunks)\n\n    # Flatten the list of shuffled chunk indexes across all nodes\n    flattened_chunks_per_ranks = [\n        chunk for rank_chunks in chunks_per_ranks for chunk in rank_chunks\n    ]\n\n    # Return the flattened list of shuffled chunk indexes\n    return flattened_chunks_per_ranks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    rank = distributed_env.get_rank()\n    world_size = distributed_env.get_world_size()\n\n    # Get the chunk indexes assigned to the current rank\n    chunk_indexes = chunks_per_ranks[rank]\n\n    # Set the seed for the current epoch\n    seed = seed + current_epoch\n\n    # Shuffle the chunk indexes using the provided seed\n    chunk_indexes = np.random.RandomState(seed).permutation(chunk_indexes)\n\n    # Gather the shuffled chunk indexes from all nodes\n    chunk_indexes = distributed_env.all_gather(chunk_indexes)\n\n    # Flatten the list of lists into a single list\n    chunk_indexes = [item for sublist in chunk_indexes for item in sublist]\n\n    return chunk_indexes"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the chunk indexes for each rank\n    chunks_per_ranks = [\n        list(range(chunks_per_ranks[i][0], chunks_per_ranks[i][1]))\n        for i in range(distributed_env.world_size)\n    ]\n\n    # Calculate the seed for the current epoch\n    epoch_seed = seed + current_epoch\n\n    # Shuffle the chunk indexes for each rank\n    for i in range(distributed_env.world_size):\n        random.seed(epoch_seed + i)\n        random.shuffle(chunks_per_ranks[i])\n\n    # Flatten the chunk indexes list\n    flattened_chunks_per_ranks = [\n        chunk for rank_chunks in chunks_per_ranks for chunk in rank_chunks\n    ]\n\n    return flattened_chunks_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the number of chunks per node\n    chunks_per_node = [sum(chunks_per_ranks[i]) for i in range(distributed_env.world_size)]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks_per_ranks[i]) for i in range(distributed_env.world_size)]\n\n    # Calculate the total number of chunks\n    total_chunks = sum(chunks_per_node)\n\n    # Calculate the seed for the current epoch\n    epoch_seed = seed + current_epoch\n\n    # Create a random number generator with the seed\n    rng = np.random.default_rng(epoch_seed)\n\n    # Create a list of chunk indexes for each rank\n    chunk_idxs = [list(range(chunks_per_rank[i])) for i in range(distributed_env.world_size)]\n\n    # Shuffle the chunk indexes for each rank\n    for i in range(distributed_env.world_size):\n        rng.shuffle(chunk_idxs[i])\n\n    # Create a list to store the shuffled chunk indexes\n    shuffled_chunk_idxs = []\n\n    # Iterate through each rank and its corresponding chunk indexes\n    for i in range(distributed_env.world_size):\n        # Iterate through each chunk index for the current rank\n        for j in range(chunks_per_rank[i]):\n            # Append the shuffled chunk index to the list\n            shuffled_chunk_idxs.append(chunk_idxs[i][j])\n\n    # Return the flattened list of shuffled chunk indexes\n    return shuffled_chunk_idxs"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Generate a random permutation of chunk indexes for each rank\n    np.random.seed(seed + current_epoch)\n    shuffled_chunk_indexes = [np.random.permutation(chunk_indexes) for chunk_indexes in chunks_per_ranks]\n\n    # Flatten the shuffled chunk indexes\n    flattened_shuffled_chunk_indexes = [chunk_index for chunk_indexes in shuffled_chunk_indexes for chunk_index in chunk_indexes]\n\n    # Return the flattened shuffled chunk indexes\n    return flattened_shuffled_chunk_indexes\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Generate a random seed based on the provided seed and current epoch.\n    random.seed(seed + current_epoch)\n\n    # Initialize an empty list to store the shuffled chunk indexes.\n    shuffled_chunks = []\n\n    # Iterate through the chunks assigned to each rank.\n    for rank, chunks in enumerate(chunks_per_ranks):\n        # Shuffle the chunk indexes for the current rank.\n        shuffled_chunks += random.sample(chunks, len(chunks))\n\n    # Return the flattened list of shuffled chunk indexes.\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the total number of chunks assigned to all ranks\n    total_chunks = sum(len(chunks) for chunks in chunks_per_ranks)\n\n    # Calculate the seed for the current epoch\n    epoch_seed = seed + current_epoch\n\n    # Initialize a random number generator with the calculated seed\n    rng = np.random.default_rng(epoch_seed)\n\n    # Shuffle the chunks for each rank using the random number generator\n    shuffled_chunks = []\n    for chunks in chunks_per_ranks:\n        rng.shuffle(chunks)\n        shuffled_chunks.extend(chunks)\n\n    # Shuffle the shuffled chunks across all ranks using the same random number generator\n    rng.shuffle(shuffled_chunks)\n\n    # Return the flattened list of shuffled chunk indexes\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the number of chunks assigned to each rank\n    chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the total number of chunks across all ranks\n    total_chunks = sum(chunks_per_rank)\n\n    # Calculate the number of chunks per node\n    chunks_per_node = total_chunks // distributed_env.world_size\n\n    # Calculate the number of chunks remaining after distributing evenly across nodes\n    remaining_chunks = total_chunks % distributed_env.world_size\n\n    # Initialize a list to store the shuffled chunk indexes for each node\n    shuffled_chunks_per_node = []\n\n    # Iterate over each node\n    for node_rank in range(distributed_env.world_size):\n\n        # Calculate the starting and ending chunk indexes for the current node\n        start_chunk = node_rank * chunks_per_node\n        end_chunk = (node_rank + 1) * chunks_per_node\n\n        # If there are remaining chunks, distribute them evenly across the nodes\n        if remaining_chunks > 0:\n            start_chunk += min(node_rank, remaining_chunks)\n            end_chunk += min(node_rank + 1, remaining_chunks)\n            remaining_chunks -= min(node_rank + 1, remaining_chunks)\n\n        # Get the chunk indexes assigned to the current node\n        node_chunks = list(range(start_chunk, end_chunk))\n\n        # Shuffle the chunk indexes using the provided seed and current epoch\n        random.seed(seed + current_epoch)\n        random.shuffle(node_chunks)\n\n        # Append the shuffled chunk indexes to the list\n        shuffled_chunks_per_node.extend(node_chunks)\n\n    # Return the flattened list of shuffled chunk indexes\n    return shuffled_chunks_per_node\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the number of nodes and the world size\n    num_nodes = distributed_env.num_nodes\n    world_size = distributed_env.world_size\n\n    # Calculate the chunk size for each node\n    chunk_size = math.ceil(len(chunks_per_ranks) / num_nodes)\n\n    # Shuffle the chunk indexes for each node\n    chunk_indexes_per_node = []\n    for i in range(num_nodes):\n        # Calculate the start and end indexes for the current node\n        start = i * chunk_size\n        end = min((i + 1) * chunk_size, len(chunks_per_ranks))\n\n        # Get the chunk indexes for the current node\n        chunk_indexes = chunks_per_ranks[start:end]\n\n        # Flatten the chunk indexes\n        flattened_chunk_indexes = [chunk_index for chunk_indexes_per_rank in chunk_indexes for chunk_index in chunk_indexes_per_rank]\n\n        # Shuffle the flattened chunk indexes\n        rng = np.random.default_rng(seed + current_epoch + i)\n        rng.shuffle(flattened_chunk_indexes)\n\n        # Append the shuffled chunk indexes to the list\n        chunk_indexes_per_node.append(flattened_chunk_indexes)\n\n    # Flatten the shuffled chunk indexes for all nodes\n    flattened_shuffled_chunk_indexes = [chunk_index for chunk_indexes_per_node in chunk_indexes_per_node for chunk_index in chunk_indexes_per_node]\n\n    # Return the flattened shuffled chunk indexes\n    return flattened_shuffled_chunk_indexes"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the world size and the rank of the current process\n    world_size = distributed_env.world_size\n    rank = distributed_env.rank\n\n    # Calculate the chunk indexes for the current rank\n    chunks_per_rank = chunks_per_ranks[rank]\n\n    # Calculate the seed for the current rank and epoch\n    rank_seed = seed + current_epoch + rank\n\n    # Shuffle the chunk indexes for the current rank using the rank seed\n    random.seed(rank_seed)\n    random.shuffle(chunks_per_rank)\n\n    # Flatten the list of chunk indexes for all ranks\n    chunks_per_ranks_flat = [chunk for rank_chunks in chunks_per_ranks for chunk in rank_chunks]\n\n    return chunks_per_ranks_flat"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the world size from the distributed environment\n    world_size = distributed_env.world_size\n\n    # Initialize a list to store the shuffled chunk indexes\n    shuffled_chunks_per_ranks = []\n\n    # Loop through each rank and shuffle the chunk indexes\n    for rank in range(world_size):\n        # Get the chunk indexes assigned to the current rank\n        chunks = chunks_per_ranks[rank]\n\n        # Calculate the seed for this rank and epoch\n        seed_for_rank = seed + rank\n        seed_for_epoch = seed_for_rank + current_epoch\n\n        # Shuffle the chunk indexes using the seed for this rank and epoch\n        shuffled_chunks = np.random.RandomState(seed_for_epoch).permutation(chunks)\n\n        # Append the shuffled chunk indexes to the list\n        shuffled_chunks_per_ranks.append(shuffled_chunks)\n\n    # Flatten the list of shuffled chunk indexes into a single list\n    shuffled_chunks = [\n        chunk for shuffled_chunks_per_rank in shuffled_chunks_per_ranks for chunk in shuffled_chunks_per_rank\n    ]\n\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the total number of chunks across all nodes\n    total_chunks = sum([len(chunks) for chunks in chunks_per_ranks])\n\n    # Calculate the seed for the current epoch\n    current_epoch_seed = seed + current_epoch\n\n    # Generate a random permutation of chunk indexes using the seed and the world size\n    chunk_indexes = list(range(total_chunks))\n    random.Random(current_epoch_seed).shuffle(chunk_indexes)\n\n    # Shuffle the chunk indexes assigned to each rank\n    shuffled_chunk_indexes = []\n    for rank_chunks in chunks_per_ranks:\n        shuffled_chunk_indexes.extend(chunk_indexes[sum(rank_chunks): sum(rank_chunks) + len(rank_chunks)])\n\n    # Return the flattened list of shuffled chunk indexes\n    return shuffled_chunk_indexes\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the total number of chunks\n    num_chunks = sum(len(chunks) for chunks in chunks_per_ranks)\n\n    # Calculate the chunk size based on the number of chunks and the world size\n    chunk_size = num_chunks // distributed_env.world_size\n\n    # Calculate the starting and ending chunk indexes for each rank\n    start_chunk_idx = chunk_size * distributed_env.rank\n    end_chunk_idx = start_chunk_idx + chunk_size\n\n    # Get the chunks assigned to the current rank\n    chunks = chunks_per_ranks[distributed_env.rank]\n\n    # Calculate the number of chunks to be shuffled\n    num_chunks_to_shuffle = len(chunks)\n\n    # Create a random generator with the provided seed and epoch\n    random_generator = np.random.default_rng(seed + current_epoch)\n\n    # Shuffle the chunks using the random generator\n    random_generator.shuffle(chunks)\n\n    # Calculate the starting and ending chunk indexes for the shuffled chunks\n    shuffled_start_chunk_idx = start_chunk_idx + chunk_size\n    shuffled_end_chunk_idx = shuffled_start_chunk_idx + num_chunks_to_shuffle\n\n    # Get the chunks assigned to the current rank after shuffling\n    shuffled_chunks = chunks_per_ranks[distributed_env.rank]\n\n    # Flatten the shuffled chunks and return them\n    return [\n        chunk_idx\n        for chunks in shuffled_chunks\n        for chunk_idx in chunks[start_chunk_idx:end_chunk_idx]\n    ]"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the world size and rank of the current process\n    world_size = distributed_env.world_size\n    rank = distributed_env.rank\n\n    # Compute the number of chunks per node\n    num_chunks_per_node = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Compute the total number of chunks\n    total_num_chunks = sum(num_chunks_per_node)\n\n    # Compute the number of chunks per node for each rank\n    num_chunks_per_rank = [num_chunks_per_node[i] for i in range(world_size)]\n\n    # Compute the cumulative number of chunks per rank\n    cum_num_chunks_per_rank = [0]\n    for i in range(1, world_size):\n        cum_num_chunks_per_rank.append(cum_num_chunks_per_rank[i - 1] + num_chunks_per_rank[i - 1])\n\n    # Compute the seed for the current epoch\n    seed = seed + current_epoch\n\n    # Create a random number generator with the seed\n    rng = np.random.default_rng(seed)\n\n    # Compute the number of chunks per rank for the current epoch\n    num_chunks_per_rank_epoch = [num_chunks_per_rank[i] for i in range(world_size)]\n\n    # Shuffle the chunk indexes for the current rank\n    rng.shuffle(chunks_per_ranks[rank])\n\n    # Compute the start and end index for the current rank's chunks\n    start_index = cum_num_chunks_per_rank[rank]\n    end_index = start_index + num_chunks_per_rank[rank]\n\n    # Compute the start and end index for the current rank's chunks for the current epoch\n    start_index_epoch = cum_num_chunks_per_rank[rank]\n    end_index_epoch = start_index_ep"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    input_dir = None\n    for input_ in inputs:\n        if isinstance(input_, str):\n            if input_dir is None:\n                input_dir = input_\n            elif input_dir != input_:\n                raise ValueError(\n                    f\"Inconsistent input paths: {input_dir} and {input_}. Please provide a single input path.\"\n                )\n        elif isinstance(input_, Path):\n            if input_dir is None:\n                input_dir = str(input_)\n            elif input_dir != str(input_):\n                raise ValueError(\n                    f\"Inconsistent input paths: {input_dir} and {input_}. Please provide a single input path.\"\n                )\n        elif isinstance(input_, IndexedPath):\n            if input_dir is None:\n                input_dir = input_.path\n            elif input_dir != input_.path:\n                raise ValueError(\n                    f\"Inconsistent input paths: {input_dir} and {input_}. Please provide a single input path.\"\n                )\n    if input_dir is None:\n        raise ValueError(\"No valid input paths found.\")\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Extract file paths from inputs\n    file_paths = [\n        input_element for input_element in inputs if isinstance(input_element, str)\n    ]\n\n    # Check if file paths are consistent\n    if len(set(file_paths)) > 1:\n        raise ValueError(\"Inconsistent file paths found in inputs.\")\n\n    # Get the absolute path of the first file path\n    input_dir = os.path.dirname(os.path.abspath(file_paths[0]))\n\n    return input_dir"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    input_dirs = [input for input in inputs if isinstance(input, str) and os.path.isfile(input)]\n\n    if len(input_dirs) > 2:\n        raise ValueError(\"Too many input files.\")\n\n    if len(input_dirs) == 0:\n        return None\n\n    input_dirs = [os.path.dirname(input_dir) for input_dir in input_dirs]\n\n    if len(input_dirs) == 1:\n        return input_dirs[0]\n\n    if input_dirs[0] != input_dirs[1]:\n        raise ValueError(\"Input files must be in the same directory.\")\n\n    return input_dirs[0]\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Extract file paths from the inputs\n    file_paths = [\n        os.path.abspath(input) if isinstance(input, str) else None\n        for input in inputs\n    ]\n\n    # Filter out None values\n    file_paths = [path for path in file_paths if path is not None]\n\n    # Check if at least two file paths are present\n    if len(file_paths) < 2:\n        return None\n\n    # Check if all file paths are the same\n    if len(set(file_paths)) == 1:\n        return os.path.dirname(file_paths[0])\n\n    # Check if the file paths are consistent\n    if not all(\n        os.path.dirname(file_paths[0]) == os.path.dirname(file_path)\n        for file_path in file_paths\n    ):\n        raise ValueError(\"Inconsistent file paths in inputs.\")\n\n    # Determine the input directory\n    input_dir = os.path.dirname(file_paths[0])\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Get the first two elements of the input sequence\n    first_input = inputs[0] if len(inputs) > 0 else None\n    second_input = inputs[1] if len(inputs) > 1 else None\n\n    # If both elements are not None and are file paths, extract the directory path\n    if first_input is not None and isinstance(first_input, str) and os.path.isfile(first_input):\n        first_input_dir = os.path.dirname(first_input)\n    else:\n        first_input_dir = None\n\n    if second_input is not None and isinstance(second_input, str) and os.path.isfile(second_input):\n        second_input_dir = os.path.dirname(second_input)\n    else:\n        second_input_dir = None\n\n    # If both directories are not None and are different, raise an error\n    if first_input_dir is not None and second_input_dir is not None and first_input_dir != second_input_dir:\n        raise ValueError(\n            f\"Inconsistent input directories: {first_input_dir} and {second_input_dir}\")\n\n    # If only one directory is not None, use that directory\n    if first_input_dir is not None and second_input_dir is None:\n        input_dir = first_input_dir\n    elif first_input_dir is None and second_input_dir is not None:\n        input_dir = second_input_dir\n    else:\n        input_dir = None\n\n    # If the input directory is not None, resolve it to an absolute path and return it\n    if input_dir is not None:\n        input_dir = os.path.abspath(input_dir)\n        return input_dir\n\n    # If no valid file paths are found, return None\n    return None\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    input_dir = None\n    for input_ in inputs:\n        if isinstance(input_, str):\n            input_dir = input_\n        elif isinstance(input_, Path):\n            input_dir = input_.as_posix()\n        elif isinstance(input_, dict):\n            if \"path\" in input_:\n                input_dir = input_[\"path\"]\n        elif isinstance(input_, list):\n            input_dir = input_[0]\n\n    if input_dir is None:\n        raise ValueError(\"No valid input directory found.\")\n\n    # If input_dir is a relative path, make it absolute using the project root\n    if not os.path.isabs(input_dir):\n        input_dir = os.path.join(project_root, input_dir)\n\n    # If input_dir is a relative path, make it absolute using the project root\n    if not os.path.isabs(input_dir):\n        input_dir = os.path.join(project_root, input_dir)\n\n    # If input_dir is a relative path, make it absolute using the project root\n    if not os.path.isabs(input_dir):\n        input_dir = os.path.join(project_root, input_dir)\n\n    # If input_dir is a relative path, make it absolute using the project root\n    if not os.path.isabs(input_dir):\n        input_dir = os.path.join(project_root, input_dir)\n\n    # If input_dir is a relative path, make it absolute using the project root\n    if not os.path.isabs(input_dir):\n        input_dir = os.path.join(project_root, input_dir)\n\n    # If input_dir is a relative path, make it absolute using the project root\n    if not os.path.isabs(input_dir):\n        input_dir = os.path.join(project_root, input_dir)\n\n    # If input_dir is a relative path, make it absolute using the project root\n    if not os.path.isabs(input_dir):\n        input_dir"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    input_dir = None\n    input_paths = []\n    for input_ in inputs:\n        if isinstance(input_, str):\n            input_path = Path(input_)\n            if input_path.exists() and input_path.is_file():\n                input_paths.append(input_path)\n    if len(input_paths) > 1:\n        if input_paths[0].parent != input_paths[1].parent:\n            raise ValueError(\n                f\"Inputs must be in the same directory. Found {input_paths[0]} and {input_paths[1]}.\"\n            )\n        input_dir = input_paths[0].parent\n    elif len(input_paths) == 1:\n        input_dir = input_paths[0].parent\n    if input_dir is None:\n        raise ValueError(\"No valid input file paths found.\")\n    return input_dir.absolute()\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    input_dir = None\n    for input in inputs:\n        if isinstance(input, str):\n            if input_dir is None:\n                input_dir = input\n            elif input_dir != input:\n                raise ValueError(\n                    \"Inconsistent input directories found: {} and {}\".format(\n                        input_dir, input\n                    )\n                )\n        elif isinstance(input, Path):\n            if input_dir is None:\n                input_dir = str(input)\n            elif input_dir != str(input):\n                raise ValueError(\n                    \"Inconsistent input directories found: {} and {}\".format(\n                        input_dir, input\n                    )\n                )\n        elif isinstance(input, dict):\n            if \"input_dir\" in input:\n                input_dir = input[\"input_dir\"]\n            elif \"input_dirs\" in input:\n                input_dir = input[\"input_dirs\"]\n            elif \"input_path\" in input:\n                input_dir = input[\"input_path\"]\n            elif \"input_paths\" in input:\n                input_dir = input[\"input_paths\"]\n        elif isinstance(input, list):\n            input_dir = _get_input_dir(input)\n        elif isinstance(input, tuple):\n            input_dir = _get_input_dir(input)\n        elif isinstance(input, int):\n            input_dir = str(input)\n        elif isinstance(input, float):\n            input_dir = str(input)\n        elif isinstance(input, bool):\n            input_dir = str(input)\n        elif isinstance(input, set):\n            input_dir = _get_input_dir(list(input))\n        elif isinstance(input, frozenset):\n            input_dir = _get_input_dir(list(input))\n        elif isinstance(input, dict):\n            input_dir = _get_input_dir(list(input.values()))\n        elif isinstance(input, tuple):\n            input_dir = _get_input_dir(list(input))\n        elif isinstance(input, list):\n            input_dir"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Get the first two inputs\n    input_paths = inputs[:2]\n\n    # Check if the inputs are valid file paths\n    if not all(os.path.isfile(path) for path in input_paths):\n        return None\n\n    # Get the common parent directory of the input paths\n    common_parent = os.path.commonprefix(input_paths)\n\n    # Check if the common parent directory is a file\n    if os.path.isfile(common_parent):\n        raise ValueError(\n            f\"The first two inputs must be directories, not files: {input_paths}\"\n        )\n\n    # Check if the common parent directory is a directory\n    if not os.path.isdir(common_parent):\n        raise ValueError(\n            f\"The first two inputs must have a common directory: {input_paths}\"\n        )\n\n    # Get the absolute path to the input directory\n    input_dir = os.path.abspath(common_parent)\n\n    # Return the input directory\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    input_paths = []\n    for input in inputs:\n        if isinstance(input, str):\n            input_paths.append(input)\n        elif isinstance(input, Path):\n            input_paths.append(str(input))\n        elif isinstance(input, dict):\n            input_paths.append(input[\"path\"])\n        elif isinstance(input, list):\n            input_paths.extend(input)\n        else:\n            raise TypeError(f\"Invalid input type: {type(input)}\")\n\n    if len(input_paths) == 0:\n        return None\n\n    if len(input_paths) == 1:\n        input_path = input_paths[0]\n        if input_path.startswith(\"/\"):\n            return input_path\n        else:\n            return os.path.join(os.getcwd(), input_path)\n\n    if len(input_paths) == 2:\n        input_path1, input_path2 = input_paths\n        if input_path1.startswith(\"/\") and input_path2.startswith(\"/\"):\n            return input_path1\n        elif input_path1.startswith(\"/\") and not input_path2.startswith(\"/\"):\n            return input_path1\n        elif not input_path1.startswith(\"/\") and input_path2.startswith(\"/\"):\n            return input_path2\n        else:\n            return os.path.commonpath([input_path1, input_path2])\n\n    raise ValueError(\"Invalid input sequence: too many file paths\")\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    if len(inputs) == 1:\n        input_path = inputs[0]\n        if isinstance(input_path, str):\n            return os.path.dirname(os.path.abspath(input_path))\n        else:\n            return None\n\n    input_paths = [inputs[0], inputs[1]]\n    input_paths = [\n        os.path.abspath(input_path) if isinstance(input_path, str) else input_path\n        for input_path in input_paths\n    ]\n\n    if isinstance(input_paths[0], str) and isinstance(input_paths[1], str):\n        if not os.path.exists(input_paths[0]) and not os.path.exists(input_paths[1]):\n            raise ValueError(\n                f\"Input paths {input_paths[0]} and {input_paths[1]} do not exist.\"\n            )\n        elif not os.path.exists(input_paths[0]) and os.path.exists(input_paths[1]):\n            raise ValueError(\n                f\"Input path {input_paths[0]} does not exist, but input path {input_paths[1]} does.\"\n            )\n        elif os.path.exists(input_paths[0]) and not os.path.exists(input_paths[1]):\n            raise ValueError(\n                f\"Input path {input_paths[1]} does not exist, but input path {input_paths[0]} does.\"\n            )\n        elif os.path.exists(input_paths[0]) and os.path.exists(input_paths[1]):\n            if input_paths[0].endswith(\"/\"):\n                input_paths[0] = input_paths[0][:-1]\n            if input_paths[1].endswith(\"/\"):\n                input_paths[1] = input_paths[1][:-1]\n\n            if os.path.dirname(input_paths[0]) == os.path.dirname(input_paths[1]):"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    input_dir: Optional[str] = None\n\n    if len(inputs) > 0 and isinstance(inputs[0], str):\n        input_dir = inputs[0]\n    elif len(inputs) > 1 and isinstance(inputs[1], str):\n        input_dir = inputs[1]\n\n    if input_dir is not None:\n        input_dir = os.path.abspath(input_dir)\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Initialize variables to store the input directory and the indexed paths\n    input_dir: Optional[str] = None\n    indexed_paths: List[str] = []\n\n    # Iterate through the inputs\n    for input_ in inputs:\n        # Check if the input is a string\n        if isinstance(input_, str):\n            # Check if the input is a path\n            if os.path.isfile(input_):\n                # Add the input to the indexed paths\n                indexed_paths.append(input_)\n\n    # Check if there are at least two indexed paths\n    if len(indexed_paths) >= 2:\n        # Get the project root from the first indexed path\n        project_root = os.path.dirname(indexed_paths[0])\n\n        # Iterate through the remaining indexed paths\n        for indexed_path in indexed_paths[1:]:\n            # Check if the indexed path is a subdirectory of the project root\n            if not os.path.commonpath([project_root, indexed_path]):\n                # Raise an error if the indexed path is not a subdirectory of the project root\n                raise ValueError(\n                    f\"Inputs must be indexed paths within the same directory: {indexed_path}\"\n                )\n\n        # Set the input directory to the project root\n        input_dir = project_root\n\n    # Return the input directory\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Extract file paths from the input sequence\n    file_paths = [\n        input_element\n        for input_element in inputs\n        if isinstance(input_element, str) and os.path.isfile(input_element)\n    ]\n\n    # Check if at least two file paths are present\n    if len(file_paths) < 2:\n        return None\n\n    # Check if all file paths are consistent\n    base_dir = os.path.dirname(file_paths[0])\n    if not all(\n        os.path.dirname(file_path) == base_dir for file_path in file_paths[1:]\n    ):\n        raise ValueError(\n            \"All input files must be in the same directory. Found inconsistent paths.\"\n        )\n\n    # Format the input directory path\n    input_dir = os.path.dirname(base_dir)\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    input_dir = None\n\n    for input in inputs:\n        if isinstance(input, str):\n            input_path = Path(input)\n            if input_path.is_file():\n                input_dir = input_path.parent\n                break\n        elif isinstance(input, Path):\n            input_dir = input\n            break\n\n    if input_dir is None:\n        raise ValueError(\"No valid file paths found in inputs.\")\n\n    return input_dir.resolve().as_posix()\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Get the first two elements of the input sequence\n    input_files = inputs[:2]\n\n    # Extract file paths from the input files\n    file_paths = [\n        input_file.path for input_file in input_files if isinstance(input_file, File)\n    ]\n\n    # Check if the file paths are consistent\n    if len(set(file_paths)) != 1:\n        raise ValueError(\"Input file paths are not consistent\")\n\n    # Extract the absolute path from the file path\n    input_dir = file_paths[0]\n\n    # Get the absolute path of the input directory\n    input_dir = os.path.dirname(input_dir)\n\n    # Get the project root from the input directory\n    project_root = get_project_root(input_dir)\n\n    # Return the absolute path of the input directory\n    return project_root\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    input_dir = None\n    for input_ in inputs:\n        if isinstance(input_, str):\n            if input_dir is None:\n                input_dir = input_\n            elif input_dir != input_:\n                raise ValueError(\n                    f\"Input file paths are inconsistent. Expected: {input_dir} but got: {input_}\"\n                )\n        elif isinstance(input_, Path):\n            if input_dir is None:\n                input_dir = str(input_)\n            elif input_dir != str(input_):\n                raise ValueError(\n                    f\"Input file paths are inconsistent. Expected: {input_dir} but got: {input_}\"\n                )\n        elif isinstance(input_, list):\n            input_dir = _get_input_dir(input_)\n        elif isinstance(input_, tuple):\n            input_dir = _get_input_dir(input_)\n        elif isinstance(input_, dict):\n            input_dir = _get_input_dir(input_.values())\n        else:\n            raise ValueError(\n                f\"Unsupported input type: {type(input_)}. Expected a string, Path, list, tuple, or dict.\"\n            )\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    input_dir = None\n\n    if len(inputs) >= 2:\n        input_dir = inputs[0]\n        if isinstance(input_dir, str):\n            input_dir = os.path.dirname(input_dir)\n        if isinstance(inputs[1], str):\n            input_dir2 = os.path.dirname(inputs[1])\n            if input_dir != input_dir2:\n                raise ValueError(\n                    f\"Inputs have inconsistent paths: {input_dir} and {input_dir2}\"\n                )\n\n    if input_dir is None:\n        raise ValueError(f\"Inputs have no valid file paths: {inputs}\")\n\n    input_dir = os.path.abspath(input_dir)\n    input_dir = os.path.normpath(input_dir)\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    input_dir: Optional[str] = None\n\n    for input in inputs:\n        if isinstance(input, str) and os.path.isfile(input):\n            if input_dir is None:\n                input_dir = os.path.dirname(input)\n            elif input_dir != os.path.dirname(input):\n                raise ValueError(\n                    f\"Inconsistent input directories found: {input_dir} and {os.path.dirname(input)}\"\n                )\n\n    if input_dir is None:\n        raise ValueError(\"No valid input directories found\")\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Get the absolute path of the first input\n    first_input = inputs[0]\n    if isinstance(first_input, str):\n        first_input_path = os.path.abspath(first_input)\n    elif isinstance(first_input, Path):\n        first_input_path = first_input.absolute()\n    else:\n        raise ValueError(\"Invalid input type. Expected str or Path.\")\n\n    # Get the absolute path of the second input\n    second_input = inputs[1]\n    if isinstance(second_input, str):\n        second_input_path = os.path.abspath(second_input)\n    elif isinstance(second_input, Path):\n        second_input_path = second_input.absolute()\n    else:\n        raise ValueError(\"Invalid input type. Expected str or Path.\")\n\n    # Check if the file paths are consistent\n    if first_input_path != second_input_path:\n        raise ValueError(\"Inconsistent file paths. Please provide the same file path for both inputs.\")\n\n    # Get the parent directory of the file path\n    parent_dir = os.path.dirname(first_input_path)\n\n    # Check if the parent directory is a subdirectory of the project root\n    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n    if not parent_dir.startswith(project_root):\n        raise ValueError(f\"Input directory '{parent_dir}' is not a subdirectory of the project root '{project_root}'.\")\n\n    # Return the parent directory as the input directory\n    return parent_dir\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            # Enable DNS optimization\n            pass\n        else:\n            # Disable DNS optimization\n            pass\n        yield\n    finally:\n        # Disable DNS optimization\n        pass"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if enable:\n        _enable_dns_optimization()\n    else:\n        _disable_dns_optimization()\n\n    try:\n        yield\n    finally:\n        _disable_dns_optimization()"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    with dns_optimization_enabled(enable):\n        pass"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    with contextlib.suppress(Exception):\n        if enable:\n            # Enable DNS optimization\n            pass\n        else:\n            # Disable DNS optimization\n            pass\n\n    # Disable DNS optimization\n    pass\n\n    # Return None to indicate successful completion of the context\n    return None"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            print(\"DNS optimization enabled\")\n            # Perform DNS optimization-related operations\n        else:\n            print(\"DNS optimization disabled\")\n            # Perform DNS optimization-related operations\n    finally:\n        print(\"DNS optimization always disabled\")\n        # Disable DNS optimization"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            # Enable DNS optimization\n            # Perform operations that require DNS optimization\n            pass\n        else:\n            # Disable DNS optimization\n            # Perform operations that require DNS optimization disabled\n            pass\n    except Exception as e:\n        # Handle any exceptions that occur during the execution of the context\n        raise e\n    finally:\n        # Disable DNS optimization\n        # Ensure that DNS optimization is disabled even if an exception occurs\n        pass"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            # Enable DNS optimization\n            pass\n        else:\n            # Disable DNS optimization\n            pass\n\n        # Perform DNS optimization-related operations\n        # ...\n\n    finally:\n        # Ensure DNS optimization is disabled\n        pass\n\n    # Return None or a default value if needed\n    return None"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Get the current DNS optimization state\n    current_dns_optimization = get_dns_optimization()\n\n    # Set DNS optimization to the desired state\n    if enable:\n        set_dns_optimization(True)\n    else:\n        set_dns_optimization(False)\n\n    try:\n        # Perform the desired operations within the context\n        yield\n    finally:\n        # Ensure that DNS optimization is always disabled after the context\n        set_dns_optimization(False)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    with contextlib.suppress(Exception):\n        if enable:\n            os.system(\"netsh interface ipv4 set dnsoption \\\"Internet\\\" \\\"enabled\\\"\")\n        else:\n            os.system(\"netsh interface ipv4 set dnsoption \\\"Internet\\\" \\\"disabled\\\"\")\n\n    try:\n        # Perform DNS optimization-related operations here\n        pass\n    finally:\n        # Disable DNS optimization regardless of whether an exception occurred\n        os.system(\"netsh interface ipv4 set dnsoption \\\"Internet\\\" \\\"disabled\\\"\")"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            dns_optimization_enabled = True\n            # Perform operations that require DNS optimization\n        else:\n            dns_optimization_enabled = False\n            # Perform operations that do not require DNS optimization\n    finally:\n        # Disable DNS optimization\n        dns_optimization_enabled = False\n\n    # Perform operations that do not require DNS optimization\n\n    # Return None as there is no return value for this function\n    return None"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Initialize a flag to indicate whether DNS optimization was previously enabled\n    was_enabled = False\n\n    # Check if DNS optimization is currently enabled\n    if dns.get_dns_enabled():\n        # If DNS optimization is enabled, disable it and set the flag\n        dns.set_dns_enabled(False)\n        was_enabled = True\n\n    try:\n        # Execute the code within the context where DNS optimization is enabled or disabled as specified\n        yield\n    finally:\n        # Restore DNS optimization to its previous state\n        if was_enabled:\n            dns.set_dns_enabled(True)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    from contextlib import contextmanager\n    from netmiko import ConnectHandler\n    from netmiko.ssh_exception import NetMikoTimeoutException\n    from netmiko.ssh_exception import NetMikoAuthenticationException\n    from netmiko.ssh_exception import SSHException\n\n    @contextmanager\n    def _dns_context(enable: bool):\n        try:\n            with ConnectHandler(**device_dict) as ssh_connection:\n                ssh_connection.enable()\n                ssh_connection.send_config_set([\"ip dns enable\" if enable else \"no ip dns enable\"])\n                yield\n        except (NetMikoTimeoutException, NetMikoAuthenticationException, SSHException) as error:\n            print(f\"An error occurred: {error}\")\n        finally:\n            ssh_connection.send_config_set([\"no ip dns enable\"])\n\n    return _dns_context(enable)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            # Enable DNS optimization\n            print(\"DNS optimization enabled\")\n        else:\n            # Disable DNS optimization\n            print(\"DNS optimization disabled\")\n        # Perform DNS optimization-related operations\n        # ...\n    except Exception as e:\n        print(f\"An error occurred during DNS optimization: {e}\")\n    finally:\n        # Disable DNS optimization\n        print(\"DNS optimization disabled\")\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    def __enter__() -> None:\n        if enable:\n            enable_dns_optimization()\n        else:\n            disable_dns_optimization()\n\n    def __exit__(exc_type: Any, exc_value: Any, exc_traceback: Any) -> None:\n        disable_dns_optimization()\n\n    return __enter__, __exit__\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    with contextlib.suppress(Exception):\n        if enable:\n            if not is_dns_optimization_enabled():\n                set_dns_optimization_enabled(True)\n        else:\n            if is_dns_optimization_enabled():\n                set_dns_optimization_enabled(False)\n    finally:\n        if is_dns_optimization_enabled():\n            set_dns_optimization_enabled(False)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        # Check if DNS optimization is already enabled or disabled\n        if enable:\n            if not is_dns_optimization_enabled():\n                # Enable DNS optimization\n                enable_dns_optimization()\n        else:\n            if is_dns_optimization_enabled():\n                # Disable DNS optimization\n                disable_dns_optimization()\n\n        # Perform desired operations within the context\n        # ...\n\n    finally:\n        # Disable DNS optimization regardless of whether it was enabled or disabled before the context\n        disable_dns_optimization()\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        # Perform DNS optimization-related operations within the context\n        # ...\n\n        # If DNS optimization was not enabled before, enable it now\n        if not enable:\n            enable_dns_optimization()\n\n        # Return None to indicate that the context was successfully completed\n        return None\n\n    finally:\n        # Disable DNS optimization regardless of whether the context was successful or not\n        disable_dns_optimization()\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    from contextlib import contextmanager\n    from dns import resolver\n    from dns.rdtypes.ANY import A\n    from dns.rdtypes.ANY import AAAA\n    from dns.rdtypes.ANY import ANY\n    from dns.rdtypes.ANY import CNAME\n    from dns.rdtypes.ANY import MX\n    from dns.rdtypes.ANY import NAPTR\n    from dns.rdtypes.ANY import NS\n    from dns.rdtypes.ANY import PTR\n    from dns.rdtypes.ANY import SOA\n    from dns.rdtypes.ANY import SRV\n    from dns.rdtypes.ANY import TXT\n    from dns.rdtypes.ANY import SPF\n    from dns.rdtypes.ANY import DNAME\n    from dns.rdtypes.ANY import OPT\n    from dns.rdtypes.ANY import RP\n    from dns.rdtypes.ANY import TLSA\n    from dns.rdtypes.ANY import DHCID\n    from dns.rdtypes.ANY import NSEC\n    from dns.rdtypes.ANY import NSEC3\n    from dns.rdtypes.ANY import NSEC3PARAM\n    from dns.rdtypes.ANY import HIP\n    from dns.rdtypes.ANY import CDS\n    from dns.rdtypes.ANY import CDNSKEY\n    from dns.rdtypes.ANY import OPENPGPKEY\n    from dns.rdtypes.ANY import CSYNC\n    from dns.rdtypes.ANY import ZONEMD\n    from dns.rdtypes.ANY import SVCB\n    from dns.rdtypes.ANY import HTTPS\n    from dns.rdtypes.ANY import TKEY\n    from dns.rdtypes.ANY import TSIG\n    from dns.rdtypes.ANY import URI\n    from dns.rdtypes.ANY import CAA\n    from dns.rdtypes.ANY import TA\n    from dns."}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        # Attempt to enable DNS optimization\n        if enable:\n            # Enable DNS optimization\n            enable_dns_optimization()\n        else:\n            # Disable DNS optimization\n            disable_dns_optimization()\n\n        # Perform operations within the context\n        # ...\n\n    finally:\n        # Ensure DNS optimization is always disabled\n        disable_dns_optimization()\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Get the current DNS optimization state\n    current_state = get_dns_optimization_state()\n\n    # If the current state is the same as the desired state, return immediately\n    if current_state == enable:\n        return\n\n    # If the current state is different from the desired state, enable or disable DNS optimization accordingly\n    if enable:\n        enable_dns_optimization()\n    else:\n        disable_dns_optimization()\n\n    # Ensure that DNS optimization is disabled after the context is exited\n    try:\n        yield\n    finally:\n        disable_dns_optimization()\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items_per_rank = _calculate_num_items_per_rank(\n        len(indexes), distributed_env.world_size, drop_last\n    )\n\n    # Assign chunks and their intervals to each rank\n    chunks_per_rank = []\n    intervals_per_rank = []\n    for rank in range(distributed_env.world_size):\n        start_index = rank * num_items_per_rank\n        end_index = min(start_index + num_items_per_rank, len(indexes))\n        chunks_per_rank.append(indexes[start_index:end_index])\n        intervals_per_rank.append(chunk_intervals[start_index:end_index])\n\n    return chunks_per_rank, intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items_per_rank = _calculate_num_items_per_rank(\n        len(indexes), distributed_env.world_size, drop_last\n    )\n\n    # Assign chunks and their intervals to each rank\n    chunks_per_rank = []\n    intervals_per_rank = []\n    for i in range(distributed_env.world_size):\n        start = i * num_items_per_rank\n        end = start + num_items_per_rank\n        chunks_per_rank.append(indexes[start:end])\n        intervals_per_rank.append(chunk_intervals[start:end])\n\n    return chunks_per_rank, intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items = len(indexes)\n    num_items_per_rank = _get_num_items_per_rank(num_items, distributed_env.world_size, drop_last)\n\n    # Assign chunks and their intervals to each rank\n    chunks_per_rank = []\n    intervals_per_rank = []\n    for rank in range(distributed_env.world_size):\n        start = sum(num_items_per_rank[:rank])\n        end = start + num_items_per_rank[rank]\n        chunks_per_rank.append(indexes[start:end])\n        intervals_per_rank.append(chunk_intervals[start:end])\n\n    return chunks_per_rank, intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    total_items = len(indexes)\n    items_per_rank = _calculate_items_per_rank(\n        distributed_env, total_items, drop_last\n    )\n\n    # Assign chunks and their intervals to each rank\n    chunks_per_rank = []\n    intervals_per_rank = []\n    start = 0\n    for rank in range(distributed_env.world_size):\n        end = start + items_per_rank[rank]\n        chunks_per_rank.append(indexes[start:end])\n        intervals_per_rank.append(chunk_intervals[start:end])\n        start = end\n\n    return chunks_per_rank, intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items = len(indexes)\n    num_items_per_rank = (\n        num_items // distributed_env.world_size\n        if not drop_last\n        else (num_items - num_items % distributed_env.world_size) // distributed_env.world_size\n    )\n\n    # Assign chunks and their intervals to each rank\n    chunk_indexes_per_rank = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_rank = [[] for _ in range(distributed_env.world_size)]\n    for i in range(num_items):\n        rank = i // num_items_per_rank\n        chunk_indexes_per_rank[rank].append(indexes[i])\n        chunk_intervals_per_rank[rank].append(chunk_intervals[i])\n\n    return chunk_indexes_per_rank, chunk_intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items_per_rank = _calculate_num_items_per_rank(\n        len(indexes), distributed_env.world_size, drop_last\n    )\n\n    # Initialize lists to hold chunk indexes and intervals for each rank\n    chunk_indexes_per_rank = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_rank = [[] for _ in range(distributed_env.world_size)]\n\n    # Assign chunks and their intervals to each rank\n    for rank in range(distributed_env.world_size):\n        start = rank * num_items_per_rank\n        end = start + num_items_per_rank\n        chunk_indexes_per_rank[rank] = indexes[start:end]\n        chunk_intervals_per_rank[rank] = chunk_intervals[start:end]\n\n    return chunk_indexes_per_rank, chunk_intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    world_size = distributed_env.world_size\n    num_items = len(indexes)\n    num_items_per_rank = _calculate_num_items_per_rank(num_items, world_size, drop_last)\n\n    ranks_chunks_indexes = [[] for _ in range(world_size)]\n    ranks_chunks_intervals = [[] for _ in range(world_size)]\n\n    for rank in range(world_size):\n        start_index = rank * num_items_per_rank\n        end_index = min((rank + 1) * num_items_per_rank, num_items)\n\n        ranks_chunks_indexes[rank] = indexes[start_index:end_index]\n        ranks_chunks_intervals[rank] = chunk_intervals[start_index:end_index]\n\n    return ranks_chunks_indexes, ranks_chunks_intervals"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    num_items = len(indexes)\n    num_ranks = distributed_env.world_size\n\n    if drop_last:\n        num_items_per_rank = num_items // num_ranks\n    else:\n        num_items_per_rank = (num_items + num_ranks - 1) // num_ranks\n\n    rank_indexes = [[] for _ in range(num_ranks)]\n    rank_chunk_intervals = [[] for _ in range(num_ranks)]\n\n    for rank in range(num_ranks):\n        start_index = rank * num_items_per_rank\n        end_index = min((rank + 1) * num_items_per_rank, num_items)\n\n        rank_indexes[rank] = indexes[start_index:end_index]\n        rank_chunk_intervals[rank] = chunk_intervals[start_index:end_index]\n\n    return rank_indexes, rank_chunk_intervals"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    items_per_rank = _calculate_items_per_rank(len(indexes), distributed_env.world_size, drop_last)\n\n    # Distribute chunks and their intervals to different ranks\n    chunks_per_rank = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_rank = [[] for _ in range(distributed_env.world_size)]\n    for rank, (start, end) in enumerate(zip(items_per_rank, items_per_rank[1:] + [len(indexes)])):\n        chunks_per_rank[rank] = indexes[start:end]\n        intervals_per_rank[rank] = chunk_intervals[start:end]\n\n    return chunks_per_rank, intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items_per_rank = _calculate_num_items_per_rank(\n        distributed_env, len(indexes), drop_last\n    )\n\n    # Distribute chunks and their intervals to different ranks\n    chunks_per_rank, chunks_intervals_per_rank = _distribute_chunks_and_intervals(\n        num_items_per_rank, indexes, chunk_intervals\n    )\n\n    return chunks_per_rank, chunks_intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    world_size = distributed_env.world_size\n    num_items = len(indexes)\n    num_items_per_rank = _get_num_items_per_rank(num_items, world_size, drop_last)\n\n    rank_indexes: List[List[int]] = []\n    rank_chunk_intervals: List[List[Any]] = []\n\n    for rank in range(world_size):\n        start = rank * num_items_per_rank\n        end = min((rank + 1) * num_items_per_rank, num_items)\n        rank_indexes.append(indexes[start:end])\n        rank_chunk_intervals.append(chunk_intervals[start:end])\n\n    return rank_indexes, rank_chunk_intervals"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Get the world size and the number of items to distribute\n    world_size = distributed_env.world_size\n    num_items = len(indexes)\n\n    # Calculate the number of items each rank should process\n    items_per_rank = num_items // world_size\n    if drop_last:\n        items_per_rank -= 1\n\n    # Distribute chunks and their intervals to each rank\n    chunks_per_rank = []\n    intervals_per_rank = []\n    for rank in range(world_size):\n        start_index = rank * items_per_rank\n        end_index = (rank + 1) * items_per_rank\n        if drop_last and rank == world_size - 1:\n            end_index = num_items\n        chunks_per_rank.append(indexes[start_index:end_index])\n        intervals_per_rank.append(chunk_intervals[start_index:end_index])\n\n    return chunks_per_rank, intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Get the world size from the distributed environment\n    world_size = distributed_env.world_size\n\n    # Calculate the number of items to process per rank\n    num_items_per_rank = len(indexes) // world_size\n\n    # If drop_last is True, adjust the number of items per rank to make sure the distribution is even\n    if drop_last:\n        num_items_per_rank -= len(indexes) % world_size\n\n    # Initialize lists to store the chunk indexes and intervals assigned to each rank\n    chunk_indexes_per_rank = [[] for _ in range(world_size)]\n    chunk_intervals_per_rank = [[] for _ in range(world_size)]\n\n    # Iterate over the indexes and intervals and assign them to the appropriate rank\n    for rank, (index, interval) in enumerate(zip(indexes, chunk_intervals)):\n        rank = rank % world_size\n        chunk_indexes_per_rank[rank].append(index)\n        chunk_intervals_per_rank[rank].append(interval)\n\n    # Return the lists of chunk indexes and intervals assigned to each rank\n    return chunk_indexes_per_rank, chunk_intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    world_size = distributed_env.world_size\n    rank = distributed_env.rank\n    total_items = len(indexes)\n    if drop_last:\n        items_per_rank = (total_items + world_size - 1) // world_size\n    else:\n        items_per_rank = (total_items + world_size - 1) // world_size\n\n    # Calculate the start and end indices for the current rank\n    start_idx = rank * items_per_rank\n    end_idx = min(start_idx + items_per_rank, total_items)\n\n    # Distribute the chunks and their intervals to the ranks\n    chunks_per_rank = []\n    intervals_per_rank = []\n    for i in range(start_idx, end_idx):\n        chunks_per_rank.append(indexes[i])\n        intervals_per_rank.append(chunk_intervals[i])\n\n    return chunks_per_rank, intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    def _get_number_of_items_per_rank(\n        num_items: int,\n        world_size: int,\n        drop_last: bool,\n    ) -> List[int]:\n\n        \"\"\"\n        This function calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not.\n\n        Input Arguments\n        :param num_items: Int. The total number of items to be distributed.\n        :param world_size: Int. The number of ranks in the distributed environment.\n        :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n        :return: List[Int]. A list of integers representing the number of items each rank should process.\n        \"\"\"\n\n        if drop_last:\n            num_items_per_rank = [num_items // world_size] * world_size\n            num_items_per_rank[-1] -= num_items_per_rank[-1] % world_size\n        else:\n            num_items_per_rank = [num_items // world_size] * world_size\n            num_items_per_rank[-1] += num_items % world_size\n\n        return num_items_per_rank\n\n    def _get_chunks_per_rank(\n        num_items_per_rank: List[int],\n        indexes: Any,\n        chunk_intervals: Any,\n    ) -> Tuple[List[List[int]], List[Any]]:\n\n        \"\"\"\n        This function assigns chunks and their corresponding intervals to each rank based on the number of items each rank should process.\n\n        Input Arguments\n        :param num_items_per_rank: List[Int]. A list of integers representing the number of items each rank should process.\n        :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n        :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n        :return"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    rank = distributed_env.rank\n    world_size = distributed_env.world_size\n\n    # Calculate the number of items each rank should process\n    num_items_per_rank = math.ceil(len(indexes) / world_size)\n\n    # If drop_last is True, calculate the number of items to drop\n    if drop_last:\n        num_items_to_drop = (world_size * num_items_per_rank) - len(indexes)\n    else:\n        num_items_to_drop = 0\n\n    # Calculate the number of items each rank should process after dropping the last items\n    num_items_per_rank -= num_items_to_drop\n\n    # Calculate the start and end indexes for the current rank\n    start_index = rank * num_items_per_rank\n    end_index = min(start_index + num_items_per_rank, len(indexes))\n\n    # Create a list of lists to store the chunk indexes assigned to each rank\n    chunk_indexes_per_rank = [[] for _ in range(world_size)]\n\n    # Create a list of lists of lists to store the intervals of chunks assigned to each rank\n    chunk_intervals_per_rank = [[] for _ in range(world_size)]\n\n    # Assign chunks and their intervals to each rank\n    for i in range(start_index, end_index):\n        chunk_indexes_per_rank[rank].append(indexes[i])\n        chunk_intervals_per_rank[rank].append(chunk_intervals[i])\n\n    return chunk_indexes_per_rank, chunk_intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    assert len(indexes) == len(chunk_intervals)\n    assert len(indexes) > 0\n\n    num_items_per_rank = _get_num_items_per_rank(\n        distributed_env.world_size, len(indexes), drop_last\n    )\n\n    # Distribute chunks to ranks\n    chunks_per_rank = _distribute_chunks_to_ranks(\n        distributed_env.world_size, num_items_per_rank, indexes\n    )\n\n    # Distribute intervals to ranks\n    intervals_per_rank = _distribute_intervals_to_ranks(\n        distributed_env.world_size, num_items_per_rank, chunk_intervals\n    )\n\n    return chunks_per_rank, intervals_per_rank\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Get the world size of the distributed environment\n    world_size = distributed_env.world_size\n\n    # Calculate the number of items each rank should process\n    num_items = len(indexes)\n    num_items_per_rank = num_items // world_size\n\n    # If drop_last is True, distribute the chunks evenly across all ranks, including the last rank\n    if drop_last:\n        num_items_per_rank_last_rank = num_items_per_rank + num_items % world_size\n        num_items_per_rank_other_ranks = num_items_per_rank\n    # If drop_last is False, distribute the chunks evenly across all ranks, except for the last rank, which may have more items\n    else:\n        num_items_per_rank_last_rank = num_items_per_rank\n        num_items_per_rank_other_ranks = num_items_per_rank - 1\n\n    # Initialize lists to store the chunk indexes and intervals assigned to each rank\n    chunks_per_rank = [[] for _ in range(world_size)]\n    chunk_intervals_per_rank = [[] for _ in range(world_size)]\n\n    # Assign chunks and their intervals to each rank\n    for i in range(world_size):\n        start_index = i * num_items_per_rank\n        end_index = start_index + num_items_per_rank_other_ranks\n        if i == world_size - 1:\n            end_index = start_index + num_items_per_rank_last_rank\n        chunks_per_rank[i] = indexes[start_index:end_index]\n        chunk_intervals_per_rank[i] = chunk_intervals[start_index:end_index]\n\n    return chunks_per_rank, chunk_intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process based on the total items and the world size of the distributed environment.\n    # If drop_last is True, the last items are dropped to make the distribution even across all ranks.\n    # If drop_last is False, the last items are distributed to the last rank, which may have a different number of items than the other ranks.\n    items_per_rank = (\n        len(indexes) // distributed_env.world_size\n        if drop_last\n        else (len(indexes) + distributed_env.world_size - 1) // distributed_env.world_size\n    )\n\n    # Initialize two lists to store the chunk indexes and intervals assigned to each rank.\n    chunk_indexes_per_rank = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_rank = [[] for _ in range(distributed_env.world_size)]\n\n    # Iterate over each rank and distribute the chunks and their intervals to it.\n    for rank in range(distributed_env.world_size):\n        # Calculate the start and end indexes of the chunks to be assigned to the current rank.\n        start = rank * items_per_rank\n        end = min((rank + 1) * items_per_rank, len(indexes))\n\n        # Assign the chunk indexes and intervals to the current rank.\n        chunk_indexes_per_rank[rank] = indexes[start:end]\n        chunk_intervals_per_rank[rank] = chunk_intervals[start:end]\n\n    return chunk_indexes_per_rank, chunk_intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items_per_rank = _calculate_num_items_per_rank(\n        distributed_env.world_size, len(indexes), drop_last\n    )\n\n    # Initialize lists to store the assigned chunks and their intervals\n    assigned_chunks = [[] for _ in range(distributed_env.world_size)]\n    assigned_intervals = [[] for _ in range(distributed_env.world_size)]\n\n    # Iterate through the items and distribute them among the ranks\n    for rank_idx, num_items in enumerate(num_items_per_rank):\n        assigned_chunks[rank_idx] = indexes[:num_items]\n        assigned_intervals[rank_idx] = chunk_intervals[:num_items]\n        indexes = indexes[num_items:]\n        chunk_intervals = chunk_intervals[num_items:]\n\n    return assigned_chunks, assigned_intervals\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n        if self._contains_device_info:\n            kwargs[\"device_info\"] = self._device_info\n        self._fn(item_metadata, output_dir, **kwargs)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n        if self._contains_device_info:\n            kwargs[\"device_info\"] = self._device_info\n        self._fn(item_metadata, output_dir, **kwargs)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        # TODO: add is_last to kwargs if _contains_is_last\n        kwargs = {}\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n        if self._contains_device_info:\n            kwargs[\"device_info\"] = self._device_info\n        self._fn(item_metadata, output_dir, **kwargs)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n        if self._contains_device_info:\n            kwargs[\"device_info\"] = self._device_info\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n        if self._contains_device_info:\n            kwargs['device_info'] = self._device_info\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except s3.exceptions.NoSuchKey:\n            time.sleep(sleep_time)\n        except Exception as e:\n            raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except s3.exceptions.NoSuchKey:\n            time.sleep(sleep_time)\n        except Exception as e:\n            raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except s3.exceptions.NoSuchKey:\n            time.sleep(sleep_time)\n        except Exception as e:\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.bucket, Key=obj.key)\n        except s3.exceptions.NoSuchKey:\n            time.sleep(sleep_time)\n        except Exception as e:\n            raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except s3.exceptions.NoSuchKey:\n            time.sleep(sleep_time)\n        except Exception as e:\n            raise e\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                time.sleep(sleep_time)\n            else:\n                raise e\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket_name, Key=obj.key)\n            return response\n        except s3.exceptions.NoSuchKey:\n            time.sleep(sleep_time)\n        except Exception as e:\n            raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except s3.exceptions.NoSuchKey:\n            time.sleep(sleep_time)\n        except Exception as e:\n            raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                time.sleep(sleep_time)\n            else:\n                raise e\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except s3.exceptions.NoSuchKey:\n            time.sleep(sleep_time)\n        except Exception as e:\n            raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.bucket, Key=obj.key)\n        except s3.exceptions.NoSuchKey:\n            time.sleep(sleep_time)\n        except Exception as e:\n            raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except s3.exceptions.NoSuchKey:\n            time.sleep(sleep_time)\n        except Exception as e:\n            raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.bucket_name, Key=obj.object_key)\n        except s3.exceptions.NoSuchKey:\n            time.sleep(sleep_time)\n        except Exception as e:\n            raise e\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.bucket, Key=obj.key)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                time.sleep(sleep_time)\n            else:\n                raise\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                time.sleep(sleep_time)\n            else:\n                raise e\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": ""}
{"namespace": "litdata.processing.functions.optimize", "completion": "    pass"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, (list, tuple)):\n        raise TypeError(f\"Expected inputs to be a list or tuple, got {type(inputs)}\")\n\n    if not isinstance(output_dir, str):\n        raise TypeError(f\"Expected output_dir to be a str, got {type(output_dir)}\")\n\n    if weights is not None and not isinstance(weights, list):\n        raise TypeError(f\"Expected weights to be a list, got {type(weights)}\")\n\n    if chunk_size is not None and not isinstance(chunk_size, int):\n        raise TypeError(f\"Expected chunk_size to be an int, got {type(chunk_size)}\")\n\n    if chunk_bytes is not None and not isinstance(chunk_bytes, (int, str)):\n        raise TypeError(f\"Expected chunk_bytes to be an int or str, got {type(chunk_bytes)}\")\n\n    if compression is not None and not isinstance(compression, str):\n        raise TypeError(f\"Expected compression to be a str, got {type(compression)}\")\n\n    if num_workers is not None and not isinstance(num_workers, int):\n        raise TypeError(f\"Expected num_workers to be an int, got {type(num_workers)}\")\n\n    if not isinstance(fast_dev_run, bool):\n        raise TypeError(f\"Expected fast_dev_run to be a bool, got {type(fast_dev_run)}\")\n\n    if num_nodes is not None and not isinstance(num_nodes, int):\n        raise TypeError(f\"Expected num_nodes to be an int, got {type(num_nodes)}\")\n\n    if machine is not None and not isinstance(machine, str):\n        raise TypeError(f\"Expected machine to be a str, got {type(machine)}\")\n\n    if num_downloaders is not None and not isinstance(num_downloaders, int):\n        raise TypeError(f\"Expected num_downloaders to be an int, got {type(num_downloaders"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, Sequence):\n        raise TypeError(f\"inputs should be a sequence, got {type(inputs)}\")\n    if not isinstance(output_dir, str):\n        raise TypeError(f\"output_dir should be a string, got {type(output_dir)}\")\n    if weights is not None and not isinstance(weights, Sequence):\n        raise TypeError(f\"weights should be a sequence, got {type(weights)}\")\n    if weights is not None and len(weights) != len(inputs):\n        raise ValueError(f\"weights should have the same length as inputs, got {len(weights)} and {len(inputs)}\")\n    if chunk_size is not None and not isinstance(chunk_size, int):\n        raise TypeError(f\"chunk_size should be an integer, got {type(chunk_size)}\")\n    if chunk_bytes is not None and not isinstance(chunk_bytes, (int, str)):\n        raise TypeError(f\"chunk_bytes should be an integer or a string, got {type(chunk_bytes)}\")\n    if compression is not None and compression not in [\"zstd\", \"lz4\", \"lzma\"]:\n        raise ValueError(f\"compression should be one of ['zstd', 'lz4', 'lzma'], got {compression}\")\n    if num_workers is not None and not isinstance(num_workers, int):\n        raise TypeError(f\"num_workers should be an integer, got {type(num_workers)}\")\n    if fast_dev_run is not None and not isinstance(fast_dev_run, bool):\n        raise TypeError(f\"fast_dev_run should be a boolean, got {type(fast_dev_run)}\")\n    if num_nodes is not None and not isinstance(num_nodes, int):\n        raise TypeError(f\"num_nodes should be an integer, got {type(num_nodes)}\")\n    if machine is not None and not isinstance(machine, str):\n        raise TypeError"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, Sequence):\n        raise TypeError(\"inputs must be a sequence\")\n    if not all(isinstance(x, str) for x in inputs):\n        raise TypeError(\"inputs must be a sequence of strings\")\n\n    if weights is not None and not isinstance(weights, list):\n        raise TypeError(\"weights must be a list\")\n    if weights is not None and len(weights) != len(inputs):\n        raise ValueError(\"weights must be the same length as inputs\")\n\n    if chunk_size is not None and not isinstance(chunk_size, int):\n        raise TypeError(\"chunk_size must be an integer\")\n    if chunk_bytes is not None and not isinstance(chunk_bytes, (int, str)):\n        raise TypeError(\"chunk_bytes must be an integer or string\")\n    if compression is not None and compression not in [\"gzip\", \"zstd\"]:\n        raise ValueError(\"compression must be 'gzip' or 'zstd'\")\n    if num_workers is not None and not isinstance(num_workers, int):\n        raise TypeError(\"num_workers must be an integer\")\n    if fast_dev_run is not None and not isinstance(fast_dev_run, bool):\n        raise TypeError(\"fast_dev_run must be a boolean\")\n    if num_nodes is not None and not isinstance(num_nodes, int):\n        raise TypeError(\"num_nodes must be an integer\")\n    if machine is not None and not isinstance(machine, str):\n        raise TypeError(\"machine must be a string\")\n    if num_downloaders is not None and not isinstance(num_downloaders, int):\n        raise TypeError(\"num_downloaders must be an integer\")\n    if num_uploaders is not None and not isinstance(num_uploaders, int):\n        raise TypeError(\"num_uploaders must be an integer\")\n    if reorder_files is not None and not isinstance(reorder_files, bool):\n        raise TypeError(\"reorder_files must be a boolean\")\n    if reader"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if inputs is a list of filepaths\n    if not isinstance(inputs, list) or not all(isinstance(input, str) for input in inputs):\n        raise ValueError(\"inputs must be a list of filepaths\")\n\n    # Check if output_dir exists and is a directory\n    if not os.path.exists(output_dir):\n        raise ValueError(f\"output_dir '{output_dir}' does not exist\")\n    if not os.path.isdir(output_dir):\n        raise ValueError(f\"output_dir '{output_dir}' is not a directory\")\n\n    # Check if weights is a list of integers\n    if weights is not None:\n        if not isinstance(weights, list) or not all(isinstance(weight, int) for weight in weights):\n            raise ValueError(\"weights must be a list of integers\")\n        if len(weights) != len(inputs):\n            raise ValueError(\"weights must be the same length as inputs\")\n\n    # Check if chunk_size is an integer\n    if chunk_size is not None:\n        if not isinstance(chunk_size, int):\n            raise ValueError(\"chunk_size must be an integer\")\n\n    # Check if chunk_bytes is an integer or a string\n    if chunk_bytes is not None:\n        if not isinstance(chunk_bytes, int) and not isinstance(chunk_bytes, str):\n            raise ValueError(\"chunk_bytes must be an integer or a string\")\n\n    # Check if compression is a valid string\n    if compression is not None:\n        if not isinstance(compression, str):\n            raise ValueError(\"compression must be a string\")\n        if compression not in [\"gzip\", \"bz2\", \"xz\", \"lzma\", \"zstd\"]:\n            raise ValueError(\"compression must be one of 'gzip', 'bz2', 'xz', 'lzma', or 'zstd'\")\n\n    # Check if num_workers is an integer\n    if num_workers is not None:\n        if not isinstance(num_work"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the inputs are valid\n    if not isinstance(inputs, (list, tuple)):\n        raise ValueError(\"Inputs must be a list or tuple.\")\n\n    # Check if the output directory exists and is writable\n    if not os.path.exists(output_dir):\n        raise ValueError(\"Output directory does not exist.\")\n    if not os.access(output_dir, os.W_OK):\n        raise ValueError(\"Output directory is not writable.\")\n\n    # Check if the weights are valid\n    if weights is not None:\n        if not isinstance(weights, list):\n            raise ValueError(\"Weights must be a list.\")\n        if len(weights) != len(inputs):\n            raise ValueError(\"Weights must have the same length as inputs.\")\n        if any(w < 0 for w in weights):\n            raise ValueError(\"Weights must be non-negative.\")\n\n    # Check if the chunk_size is valid\n    if chunk_size is not None:\n        if not isinstance(chunk_size, int):\n            raise ValueError(\"Chunk size must be an integer.\")\n        if chunk_size <= 0:\n            raise ValueError(\"Chunk size must be positive.\")\n\n    # Check if the chunk_bytes is valid\n    if chunk_bytes is not None:\n        if not isinstance(chunk_bytes, (int, str)):\n            raise ValueError(\"Chunk bytes must be an integer or a string.\")\n        if isinstance(chunk_bytes, int) and chunk_bytes <= 0:\n            raise ValueError(\"Chunk bytes must be positive.\")\n        if isinstance(chunk_bytes, str) and chunk_bytes not in [\"1B\", \"1KB\", \"1MB\", \"1GB\"]:\n            raise ValueError(\"Chunk bytes must be one of '1B', '1KB', '1MB', or '1GB'.\")\n\n    # Check if the compression is valid\n    if compression is not None:\n        if compression not in [\"gzip\", \"bz2\", \"xz\"]:\n            raise ValueError(\"Compression must be one of 'gzip', 'bz2"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the inputs are a sequence\n    if not isinstance(inputs, Sequence):\n        raise TypeError(\"Inputs must be a sequence\")\n\n    # Check if the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Check if the output directory is writable\n    if not os.access(output_dir, os.W_OK):\n        raise PermissionError(\"Output directory is not writable\")\n\n    # Check if the output directory is empty\n    if os.listdir(output_dir):\n        raise ValueError(\"Output directory is not empty\")\n\n    # Check if the function is callable\n    if not callable(fn):\n        raise TypeError(\"Function must be callable\")\n\n    # Check if the weights are provided\n    if weights is not None:\n        if not isinstance(weights, list):\n            raise TypeError(\"Weights must be a list\")\n        if len(weights) != len(inputs):\n            raise ValueError(\"Weights must be the same length as inputs\")\n\n    # Check if the chunk size is provided\n    if chunk_size is not None:\n        if not isinstance(chunk_size, int):\n            raise TypeError(\"Chunk size must be an integer\")\n        if chunk_size <= 0:\n            raise ValueError(\"Chunk size must be positive\")\n\n    # Check if the chunk bytes is provided\n    if chunk_bytes is not None:\n        if not isinstance(chunk_bytes, (int, str)):\n            raise TypeError(\"Chunk bytes must be an integer or a string\")\n        if isinstance(chunk_bytes, int) and chunk_bytes <= 0:\n            raise ValueError(\"Chunk bytes must be positive\")\n        if isinstance(chunk_bytes, str) and chunk_bytes not in [\"auto\", \"max\"]:\n            raise ValueError(\"Chunk bytes must be 'auto' or 'max'\")\n\n    # Check if the compression is provided\n    if compression is not None:\n        if not isinstance(compression, str):\n            raise TypeError(\"Com"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the input is a list of filepaths\n    if not isinstance(inputs, (list, tuple)):\n        raise ValueError(\"Inputs must be a list or tuple of filepaths\")\n\n    # Check if the output directory exists and is writable\n    if not os.path.exists(output_dir) or not os.access(output_dir, os.W_OK):\n        raise ValueError(\"Output directory does not exist or is not writable\")\n\n    # Check if the weights are provided and have the same length as the inputs\n    if weights is not None and len(weights) != len(inputs):\n        raise ValueError(\"Weights must have the same length as inputs\")\n\n    # Check if the chunk size is provided and is positive\n    if chunk_size is not None and chunk_size <= 0:\n        raise ValueError(\"Chunk size must be positive\")\n\n    # Check if the chunk bytes is provided and is positive\n    if chunk_bytes is not None and chunk_bytes <= 0:\n        raise ValueError(\"Chunk bytes must be positive\")\n\n    # Check if the compression algorithm is supported\n    if compression is not None and compression not in [\"gzip\", \"bz2\", \"lzma\"]:\n        raise ValueError(\"Unsupported compression algorithm\")\n\n    # Check if the number of workers is provided and is positive\n    if num_workers is not None and num_workers <= 0:\n        raise ValueError(\"Number of workers must be positive\")\n\n    # Check if the number of nodes is provided and is positive\n    if num_nodes is not None and num_nodes <= 0:\n        raise ValueError(\"Number of nodes must be positive\")\n\n    # Check if the number of downloaders is provided and is positive\n    if num_downloaders is not None and num_downloaders <= 0:\n        raise ValueError(\"Number of downloaders must be positive\")\n\n    # Check if the number of uploaders is provided and is positive\n    if num_uploaders is not None and num_uploaders <= 0:\n        raise ValueError(\"Number of uploaders must be positive\")\n\n    # Check if the batch size is provided and is positive\n    if"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the inputs are valid\n    if not inputs:\n        raise ValueError(\"No inputs provided.\")\n\n    # Check if the output directory exists and create it if it doesn't\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Check if the weights are valid\n    if weights:\n        if len(weights) != len(inputs):\n            raise ValueError(\"Number of weights must match number of inputs.\")\n        if not all(isinstance(w, int) for w in weights):\n            raise ValueError(\"Weights must be integers.\")\n        if not all(w >= 0 for w in weights):\n            raise ValueError(\"Weights must be non-negative.\")\n\n    # Check if the chunk size is valid\n    if chunk_size and chunk_size <= 0:\n        raise ValueError(\"Chunk size must be a positive integer.\")\n\n    # Check if the chunk bytes are valid\n    if chunk_bytes and chunk_bytes <= 0:\n        raise ValueError(\"Chunk bytes must be a positive integer.\")\n\n    # Check if the compression is valid\n    if compression and compression not in [\"gzip\", \"bz2\", \"lzma\", \"xz\"]:\n        raise ValueError(\"Invalid compression algorithm.\")\n\n    # Check if the number of workers is valid\n    if num_workers and num_workers <= 0:\n        raise ValueError(\"Number of workers must be a positive integer.\")\n\n    # Check if the number of nodes is valid\n    if num_nodes and num_nodes <= 0:\n        raise ValueError(\"Number of nodes must be a positive integer.\")\n\n    # Check if the number of downloaders is valid\n    if num_downloaders and num_downloaders <= 0:\n        raise ValueError(\"Number of downloaders must be a positive integer.\")\n\n    # Check if the number of uploaders is valid\n    if num_uploaders and num_uploaders <= 0:\n        raise ValueError(\"Number of uploaders must be a positive integer.\")\n\n    # Check if the batch size is valid\n    if batch_size and batch_size <= 0:\n        raise ValueError(\"Batch size"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, (list, tuple)):\n        raise TypeError(\"`inputs` must be a list or tuple of inputs\")\n\n    if len(inputs) == 0:\n        raise ValueError(\"`inputs` must be a non-empty list or tuple of inputs\")\n\n    if not isinstance(output_dir, str):\n        raise TypeError(\"`output_dir` must be a string\")\n\n    if not isinstance(weights, (list, tuple)) and weights is not None:\n        raise TypeError(\"`weights` must be a list or tuple of integers\")\n\n    if weights is not None and len(weights) != len(inputs):\n        raise ValueError(\"`weights` must be the same length as `inputs`\")\n\n    if chunk_size is not None and not isinstance(chunk_size, int):\n        raise TypeError(\"`chunk_size` must be an integer\")\n\n    if chunk_bytes is not None and not isinstance(chunk_bytes, (int, str)):\n        raise TypeError(\"`chunk_bytes` must be an integer or string\")\n\n    if compression is not None and compression not in [\"gzip\", \"zstd\"]:\n        raise ValueError(\"`compression` must be one of 'gzip' or 'zstd'\")\n\n    if num_workers is not None and not isinstance(num_workers, int):\n        raise TypeError(\"`num_workers` must be an integer\")\n\n    if not isinstance(fast_dev_run, bool):\n        raise TypeError(\"`fast_dev_run` must be a boolean\")\n\n    if num_nodes is not None and not isinstance(num_nodes, int):\n        raise TypeError(\"`num_nodes` must be an integer\")\n\n    if machine is not None and not isinstance(machine, str):\n        raise TypeError(\"`machine` must be a string\")\n\n    if num_downloaders is not None and not isinstance(num_downloaders, int):\n        raise TypeError(\"`num_downloaders` must be an integer\")\n\n    if num_"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if num_nodes and machine:\n        raise ValueError(\"Only one of `num_nodes` and `machine` can be specified.\")\n\n    if num_nodes and not machine:\n        machine = \"cpu\"\n\n    if machine and not num_nodes:\n        num_nodes = 1\n\n    if num_nodes and num_nodes > 1:\n        if not num_downloaders:\n            num_downloaders = 1\n        if not num_uploaders:\n            num_uploaders = 1\n\n    if num_nodes and num_nodes > 1 and not num_workers:\n        num_workers = num_nodes\n\n    if num_nodes and num_nodes > 1 and not num_downloaders:\n        num_downloaders = num_nodes\n\n    if num_nodes and num_nodes > 1 and not num_uploaders:\n        num_uploaders = num_nodes\n\n    if num_workers and num_workers > 1 and not num_downloaders:\n        num_downloaders = num_workers\n\n    if num_workers and num_workers > 1 and not num_uploaders:\n        num_uploaders = num_workers\n\n    if num_downloaders and num_downloaders > 1 and not num_workers:\n        num_workers = num_downloaders\n\n    if num_uploaders and num_uploaders > 1 and not num_workers:\n        num_workers = num_uploaders\n\n    if num_downloaders and num_downloaders > 1 and not num_workers:\n        num_workers = num_downloaders\n\n    if num_uploaders and num_uploaders > 1 and not num_workers:\n        num_workers = num_uploaders\n\n    if num_workers and num_workers > 1 and not num_downloaders:\n        num_downloaders = num_workers\n\n    if num_workers and num_workers > 1 and not num_uploaders:\n        num_uploaders = num_workers\n\n    if num_workers and num_workers > 1 and not num_downloaders:\n        num_downloaders = num"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the inputs are a sequence\n    if not isinstance(inputs, Sequence):\n        raise TypeError(\"Inputs must be a sequence.\")\n\n    # Check if the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Check if the weights are provided and valid\n    if weights is not None:\n        if not isinstance(weights, Sequence):\n            raise TypeError(\"Weights must be a sequence.\")\n        if len(weights) != len(inputs):\n            raise ValueError(\"Weights must have the same length as inputs.\")\n\n    # Check if the chunk_size is provided and valid\n    if chunk_size is not None:\n        if not isinstance(chunk_size, int):\n            raise TypeError(\"Chunk size must be an integer.\")\n        if chunk_size <= 0:\n            raise ValueError(\"Chunk size must be greater than 0.\")\n\n    # Check if the chunk_bytes is provided and valid\n    if chunk_bytes is not None:\n        if isinstance(chunk_bytes, int):\n            if chunk_bytes <= 0:\n                raise ValueError(\"Chunk bytes must be greater than 0.\")\n        elif isinstance(chunk_bytes, str):\n            if chunk_bytes.endswith(\"MB\"):\n                chunk_bytes = int(chunk_bytes[:-2]) * 1024 * 1024\n            elif chunk_bytes.endswith(\"GB\"):\n                chunk_bytes = int(chunk_bytes[:-2]) * 1024 * 1024 * 1024\n            elif chunk_bytes.endswith(\"TB\"):\n                chunk_bytes = int(chunk_bytes[:-2]) * 1024 * 1024 * 1024 * 1024\n            else:\n                raise ValueError(\"Invalid chunk bytes unit.\")\n        else:\n            raise TypeError(\"Chunk bytes must be an integer or a string.\")\n\n    # Check if the compression is provided and valid\n    if compression is not"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Create a list of chunks from the input sequence\n    chunks = create_chunks(\n        inputs,\n        weights,\n        chunk_size,\n        chunk_bytes,\n        compression,\n        num_workers,\n        fast_dev_run,\n        num_nodes,\n        machine,\n        num_downloaders,\n        num_uploaders,\n        reorder_files,\n        reader,\n        batch_size,\n    )\n\n    # Process each chunk in parallel\n    for chunk in chunks:\n        process_chunk(chunk, fn, output_dir)\n\n    # Wait for all chunks to finish processing\n    for chunk in chunks:\n        chunk.wait()\n\n    # Merge the processed chunks into a single dataset\n    merge_chunks(chunks, output_dir)\n\n    # Delete the processed chunks\n    for chunk in chunks:\n        chunk.delete()"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the inputs are valid\n    if not inputs:\n        raise ValueError(\"No inputs provided\")\n\n    # Check if the output directory exists and is a directory\n    if not os.path.isdir(output_dir):\n        raise ValueError(\"Output directory does not exist or is not a directory\")\n\n    # Check if the weights are valid\n    if weights and len(weights) != len(inputs):\n        raise ValueError(\"Weights must be the same length as inputs\")\n\n    # Check if the chunk_size is valid\n    if chunk_size and chunk_size <= 0:\n        raise ValueError(\"Chunk size must be a positive integer\")\n\n    # Check if the chunk_bytes is valid\n    if chunk_bytes and chunk_bytes <= 0:\n        raise ValueError(\"Chunk bytes must be a positive integer\")\n\n    # Check if the compression is valid\n    if compression and compression not in [\"gzip\", \"zstd\"]:\n        raise ValueError(\"Invalid compression algorithm\")\n\n    # Check if the num_workers is valid\n    if num_workers and num_workers <= 0:\n        raise ValueError(\"Number of workers must be a positive integer\")\n\n    # Check if the num_nodes is valid\n    if num_nodes and num_nodes <= 0:\n        raise ValueError(\"Number of nodes must be a positive integer\")\n\n    # Check if the num_downloaders is valid\n    if num_downloaders and num_downloaders <= 0:\n        raise ValueError(\"Number of downloaders must be a positive integer\")\n\n    # Check if the num_uploaders is valid\n    if num_uploaders and num_uploaders <= 0:\n        raise ValueError(\"Number of uploaders must be a positive integer\")\n\n    # Check if the batch_size is valid\n    if batch_size and batch_size <= 0:\n        raise ValueError(\"Batch size must be a positive integer\")\n\n    # Check if the reader is valid\n    if reader and not isinstance(reader, BaseReader):\n        raise ValueError(\"Invalid reader\")\n\n    # Check if the inputs are valid\n    for input_ in inputs:\n        if not os.path"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if num_workers is not None and num_workers <= 0:\n        raise ValueError(\"num_workers must be greater than 0\")\n\n    if chunk_size is not None and chunk_size <= 0:\n        raise ValueError(\"chunk_size must be greater than 0\")\n\n    if chunk_bytes is not None and chunk_bytes <= 0:\n        raise ValueError(\"chunk_bytes must be greater than 0\")\n\n    if batch_size is not None and batch_size <= 0:\n        raise ValueError(\"batch_size must be greater than 0\")\n\n    if num_downloaders is not None and num_downloaders <= 0:\n        raise ValueError(\"num_downloaders must be greater than 0\")\n\n    if num_uploaders is not None and num_uploaders <= 0:\n        raise ValueError(\"num_uploaders must be greater than 0\")\n\n    if num_nodes is not None and num_nodes <= 0:\n        raise ValueError(\"num_nodes must be greater than 0\")\n\n    if weights is not None:\n        if len(weights) != len(inputs):\n            raise ValueError(\n                f\"weights must have the same length as inputs ({len(inputs)} != {len(weights)})\"\n            )\n\n    if batch_size is not None and batch_size > len(inputs):\n        raise ValueError(\n            f\"batch_size must be less than or equal to the number of inputs ({len(inputs)} > {batch_size})\"\n        )\n\n    if reader is not None and not isinstance(reader, BaseReader):\n        raise ValueError(\"reader must be an instance of BaseReader\")\n\n    if not isinstance(fn, Callable):\n        raise ValueError(\"fn must be a callable\")\n\n    if not isinstance(inputs, Sequence):\n        raise ValueError(\"inputs must be a sequence\")\n\n    if not isinstance(output_dir, str):\n        raise ValueError(\"output_dir must be a string\")\n\n    if not isinstance(fast_dev_run, bool):\n        raise ValueError"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if inputs are valid\n    if not inputs:\n        raise ValueError(\"Inputs cannot be empty\")\n\n    # Check if output_dir is valid\n    if not output_dir:\n        raise ValueError(\"Output directory cannot be empty\")\n\n    # Check if weights are valid\n    if weights and len(weights) != len(inputs):\n        raise ValueError(\"Weights must have the same length as inputs\")\n\n    # Check if chunk_size and chunk_bytes are valid\n    if chunk_size and chunk_bytes:\n        raise ValueError(\"Cannot specify both chunk_size and chunk_bytes\")\n\n    # Check if compression is valid\n    if compression and compression not in [\"gzip\", \"bz2\"]:\n        raise ValueError(\"Invalid compression algorithm\")\n\n    # Check if num_workers is valid\n    if num_workers and num_workers <= 0:\n        raise ValueError(\"Invalid number of workers\")\n\n    # Check if num_nodes is valid\n    if num_nodes and num_nodes <= 0:\n        raise ValueError(\"Invalid number of nodes\")\n\n    # Check if num_downloaders and num_uploaders are valid\n    if num_downloaders and num_downloaders <= 0:\n        raise ValueError(\"Invalid number of downloaders\")\n    if num_uploaders and num_uploaders <= 0:\n        raise ValueError(\"Invalid number of uploaders\")\n\n    # Check if reader is valid\n    if reader and not isinstance(reader, BaseReader):\n        raise ValueError(\"Invalid reader\")\n\n    # Check if batch_size is valid\n    if batch_size and batch_size <= 0:\n        raise ValueError(\"Invalid batch size\")\n\n    # Check if fast_dev_run is valid\n    if fast_dev_run and len(inputs) > 10:\n        raise ValueError(\"Cannot use fast_dev_run with more than 10 inputs\")\n\n    # Check if reorder_files is valid\n    if reorder_files and len(inputs) < 100:\n        raise ValueError(\"Cannot use reorder_files with less than 100 inputs\")\n\n    # Check if output"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if num_workers is None:\n        num_workers = os.cpu_count()\n\n    if num_workers == 0:\n        raise ValueError(\"num_workers cannot be zero\")\n\n    if num_nodes is not None and machine is None:\n        raise ValueError(\"machine must be specified when num_nodes is specified\")\n\n    if num_nodes is not None and machine not in [\"cpu\", \"gpu\"]:\n        raise ValueError(\"machine must be 'cpu' or 'gpu' when num_nodes is specified\")\n\n    if num_downloaders is None:\n        num_downloaders = num_workers\n\n    if num_uploaders is None:\n        num_uploaders = num_workers\n\n    if num_downloaders < 1:\n        raise ValueError(\"num_downloaders must be greater than or equal to 1\")\n\n    if num_uploaders < 1:\n        raise ValueError(\"num_uploaders must be greater than or equal to 1\")\n\n    if num_downloaders > num_workers:\n        raise ValueError(\"num_downloaders must be less than or equal to num_workers\")\n\n    if num_uploaders > num_workers:\n        raise ValueError(\"num_uploaders must be less than or equal to num_workers\")\n\n    if num_downloaders % num_workers != 0:\n        raise ValueError(\"num_downloaders must be a multiple of num_workers\")\n\n    if num_uploaders % num_workers != 0:\n        raise ValueError(\"num_uploaders must be a multiple of num_workers\")\n\n    if num_workers > len(inputs):\n        raise ValueError(\"num_workers cannot be greater than the number of inputs\")\n\n    if chunk_size is not None and chunk_bytes is not None:\n        raise ValueError(\"Only one of chunk_size or chunk_bytes can be specified\")\n\n    if chunk_size is not None and chunk_size < 1:\n        raise ValueError(\"chunk_size must be greater than or equal to 1\")\n\n    if chunk_bytes is not None and chunk_bytes < "}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if num_workers is None:\n        num_workers = os.cpu_count() or 1\n    if num_workers > 1:\n        if weights is not None:\n            raise ValueError(\"weights are not supported with num_workers > 1\")\n        if num_downloaders is not None:\n            raise ValueError(\"num_downloaders is not supported with num_workers > 1\")\n        if num_uploaders is not None:\n            raise ValueError(\"num_uploaders is not supported with num_workers > 1\")\n        if reorder_files:\n            raise ValueError(\"reorder_files is not supported with num_workers > 1\")\n    if num_workers < 1:\n        raise ValueError(\"num_workers must be >= 1\")\n    if num_nodes is not None and num_nodes < 1:\n        raise ValueError(\"num_nodes must be >= 1\")\n    if num_downloaders is not None and num_downloaders < 1:\n        raise ValueError(\"num_downloaders must be >= 1\")\n    if num_uploaders is not None and num_uploaders < 1:\n        raise ValueError(\"num_uploaders must be >= 1\")\n    if batch_size is not None and batch_size < 1:\n        raise ValueError(\"batch_size must be >= 1\")\n    if num_workers == 1:\n        _optimize_single_worker(\n            fn,\n            inputs,\n            output_dir,\n            weights,\n            chunk_size,\n            chunk_bytes,\n            compression,\n            fast_dev_run,\n            num_downloaders,\n            num_uploaders,\n            reorder_files,\n            reader,\n            batch_size,\n        )\n    else:\n        _optimize_multiple_workers(\n            fn,\n            inputs,\n            output_dir,\n            weights,\n            chunk_size,\n            chunk_bytes,\n            compression,\n            num_workers,\n            fast_dev_run,\n            num_nodes,\n            machine,\n            num_downloaders,\n            num_uploaders,\n            reorder_files,"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if num_nodes is not None and machine is None:\n        raise ValueError(\"`machine` must be specified when `num_nodes` is not None.\")\n\n    if num_nodes is not None and num_nodes < 1:\n        raise ValueError(\"`num_nodes` must be greater than or equal to 1.\")\n\n    if num_nodes is not None and num_nodes > 1 and num_workers is None:\n        num_workers = num_nodes\n\n    if num_nodes is not None and num_nodes > 1 and num_workers > 1:\n        raise ValueError(\"`num_nodes` and `num_workers` cannot be greater than 1 at the same time.\")\n\n    if num_downloaders is not None and num_downloaders < 1:\n        raise ValueError(\"`num_downloaders` must be greater than or equal to 1.\")\n\n    if num_uploaders is not None and num_uploaders < 1:\n        raise ValueError(\"`num_uploaders` must be greater than or equal to 1.\")\n\n    if num_nodes is not None and num_downloaders is not None:\n        raise ValueError(\"`num_downloaders` cannot be specified when `num_nodes` is not None.\")\n\n    if num_nodes is not None and num_uploaders is not None:\n        raise ValueError(\"`num_uploaders` cannot be specified when `num_nodes` is not None.\")\n\n    if num_nodes is not None and num_workers is not None:\n        raise ValueError(\"`num_workers` cannot be specified when `num_nodes` is not None.\")\n\n    if num_nodes is not None and num_workers is None:\n        num_workers = num_nodes\n\n    if num_nodes is not None and num_workers > 1:\n        raise ValueError(\"`num_workers` cannot be specified when `num_nodes` is not None.\")\n\n    if num_nodes is not None and num_workers > 1 and num_downloaders is not None:\n        raise ValueError(\"`num_downloaders` cannot be specified when `num_nodes` is not None.\")"}
{"namespace": "litdata.processing.functions.map", "completion": ""}
{"namespace": "litdata.processing.functions.map", "completion": ""}
{"namespace": "litdata.processing.functions.map", "completion": ""}
{"namespace": "litdata.processing.functions.map", "completion": ""}
{"namespace": "litdata.processing.functions.map", "completion": ""}
{"namespace": "litdata.processing.functions.map", "completion": ""}
{"namespace": "litdata.processing.functions.map", "completion": "    pass"}
{"namespace": "litdata.processing.functions.map", "completion": "    def _map(fn, inputs, output_dir, weights, num_workers, fast_dev_run, num_nodes, machine, num_downloaders, num_uploaders, reorder_files, error_when_not_empty, reader, batch_size):\n        # Implementation of the map function goes here\n        pass\n\n    return _map(fn, inputs, output_dir, weights, num_workers, fast_dev_run, num_nodes, machine, num_downloaders, num_uploaders, reorder_files, error_when_not_empty, reader, batch_size)"}
{"namespace": "litdata.processing.functions.map", "completion": "    # Convert the output_dir to a Dir object if it's a string path\n    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    # Create a new instance of the Mapper class\n    mapper = Mapper(\n        fn=fn,\n        inputs=inputs,\n        output_dir=output_dir,\n        weights=weights,\n        num_workers=num_workers,\n        fast_dev_run=fast_dev_run,\n        num_nodes=num_nodes,\n        machine=machine,\n        num_downloaders=num_downloaders,\n        num_uploaders=num_uploaders,\n        reorder_files=reorder_files,\n        error_when_not_empty=error_when_not_empty,\n        reader=reader,\n        batch_size=batch_size,\n    )\n\n    # Run the mapper\n    mapper.run()"}
{"namespace": "litdata.processing.functions.map", "completion": "    # Initialize the LightningDataModule\n    dm = LightningDataModule(\n        inputs,\n        output_dir,\n        weights=weights,\n        num_workers=num_workers,\n        fast_dev_run=fast_dev_run,\n        num_nodes=num_nodes,\n        machine=machine,\n        num_downloaders=num_downloaders,\n        num_uploaders=num_uploaders,\n        reorder_files=reorder_files,\n        error_when_not_empty=error_when_not_empty,\n        reader=reader,\n        batch_size=batch_size,\n    )\n\n    # Initialize the LightningTrainer\n    trainer = LightningTrainer(\n        fn,\n        dm,\n        num_workers=num_workers,\n        fast_dev_run=fast_dev_run,\n        num_nodes=num_nodes,\n        machine=machine,\n        num_downloaders=num_downloaders,\n        num_uploaders=num_uploaders,\n        reorder_files=reorder_files,\n        error_when_not_empty=error_when_not_empty,\n        reader=reader,\n        batch_size=batch_size,\n    )\n\n    # Start the trainer\n    trainer.fit()\n\n    # Clean up the temporary directory\n    dm.cleanup()"}
{"namespace": "litdata.processing.functions.map", "completion": "    # Check if the output directory is empty\n    if error_when_not_empty and os.listdir(output_dir):\n        raise ValueError(f\"Output directory {output_dir} is not empty\")\n\n    # Create a LightningDataModule with the input data\n    datamodule = LightningDataModule(inputs)\n\n    # Create a LightningModule with the processing logic\n    module = LightningModule(fn)\n\n    # Create a LightningTrainer and fit the module on the data\n    trainer = LightningTrainer(\n        datamodule=datamodule,\n        num_workers=num_workers,\n        fast_dev_run=fast_dev_run,\n        num_nodes=num_nodes,\n        machine=machine,\n        num_downloaders=num_downloaders,\n        num_uploaders=num_uploaders,\n        reorder_files=reorder_files,\n        reader=reader,\n        batch_size=batch_size,\n    )\n    trainer.fit(module)\n\n    # Save the processed data to the output directory\n    trainer.save_outputs(output_dir)\n\n    # Save the weights to the output directory\n    trainer.save_weights(output_dir)\n\n    # Save the config to the output directory\n    trainer.save_config(output_dir)\n\n    # Save the logs to the output directory\n    trainer.save_logs(output_dir)\n\n    # Save the checkpoints to the output directory\n    trainer.save_checkpoints(output_dir)\n\n    # Save the model to the output directory\n    trainer.save_model(output_dir)\n\n    # Save the model summary to the output directory\n    trainer.save_model_summary(output_dir)\n\n    # Save the model config to the output directory\n    trainer.save_model_config(output_dir)\n\n    # Save the model weights to the output directory\n    trainer.save_model_weights(output_dir)\n\n    # Save the model architecture to the output directory\n    trainer.save_"}
{"namespace": "litdata.processing.functions.map", "completion": "    # Initialize the output directory\n    output_dir = _init_output_dir(output_dir, error_when_not_empty)\n\n    # Create a new ProcessPoolExecutor with the specified number of workers\n    with ProcessPoolExecutor(num_workers) as executor:\n        # Create a list of tasks for each input\n        tasks = [\n            executor.submit(\n                _process_input,\n                fn,\n                input,\n                output_dir,\n                weights,\n                num_downloaders,\n                num_uploaders,\n                reader,\n            )\n            for input in inputs\n        ]\n\n        # Wait for all tasks to complete\n        for task in as_completed(tasks):\n            # Get the result of the task\n            result = task.result()\n\n            # If the result is not None, log the result\n            if result is not None:\n                logger.info(result)\n\n    # Log the total number of processed inputs\n    logger.info(f\"Processed {len(tasks)} inputs.\")\n\n"}
{"namespace": "litdata.processing.functions.map", "completion": "    if num_workers is None:\n        num_workers = cpu_count()\n\n    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    if num_workers > 1 and weights is None:\n        weights = [1] * len(inputs)\n\n    if weights is not None and len(weights) != len(inputs):\n        raise ValueError(\"weights must be the same length as inputs\")\n\n    if weights is not None:\n        weights = np.array(weights)\n\n    if num_workers > 1 and num_downloaders is None:\n        num_downloaders = num_workers\n\n    if num_workers > 1 and num_uploaders is None:\n        num_uploaders = num_workers\n\n    if num_workers == 1 and num_downloaders is None:\n        num_downloaders = 1\n\n    if num_workers == 1 and num_uploaders is None:\n        num_uploaders = 1\n\n    if num_workers > 1 and num_uploaders is not None and num_uploaders != num_workers:\n        raise ValueError(\"num_uploaders must be equal to num_workers\")\n\n    if num_workers > 1 and num_downloaders is not None and num_downloaders != num_workers:\n        raise ValueError(\"num_downloaders must be equal to num_workers\")\n\n    if num_workers == 1 and num_uploaders is not None and num_uploaders != 1:\n        raise ValueError(\"num_uploaders must be 1 when num_workers is 1\")\n\n    if num_workers == 1 and num_downloaders is not None and num_downloaders != 1:\n        raise ValueError(\"num_downloaders must be 1 when num_workers is 1\")\n\n    if num_workers > 1 and num_uploaders is None:\n        num_uploaders = num_workers\n\n    if num_workers > 1 and num_downloaders is None:\n        num_downloaders = num_workers"}
{"namespace": "litdata.processing.functions.map", "completion": "    if not isinstance(output_dir, Dir):\n        output_dir = Dir(output_dir)\n\n    if not output_dir.exists():\n        output_dir.mkdir()\n\n    if error_when_not_empty and output_dir.exists() and len(output_dir.ls()) > 0:\n        raise ValueError(f\"Output directory {output_dir} is not empty.\")\n\n    if isinstance(fast_dev_run, bool) and fast_dev_run:\n        fast_dev_run = 1\n\n    if isinstance(fast_dev_run, int):\n        if fast_dev_run < 1:\n            raise ValueError(f\"fast_dev_run must be a positive integer, got {fast_dev_run}\")\n        inputs = inputs[:fast_dev_run]\n\n    if not isinstance(inputs, (list, tuple)):\n        inputs = list(inputs)\n\n    if weights is None:\n        weights = [1] * len(inputs)\n\n    if len(inputs) != len(weights):\n        raise ValueError(\n            f\"Number of inputs ({len(inputs)}) does not match number of weights ({len(weights)}).\"\n        )\n\n    if num_workers is None:\n        num_workers = get_num_workers()\n\n    if num_workers > len(inputs):\n        num_workers = len(inputs)\n\n    if num_workers < 1:\n        raise ValueError(f\"num_workers must be at least 1, got {num_workers}\")\n\n    if num_workers > 1 and num_downloaders is None:\n        num_downloaders = 1\n\n    if num_workers > 1 and num_uploaders is None:\n        num_uploaders = 1\n\n    if batch_size is None:\n        batch_size = get_batch_size()\n\n    if batch_size < 1:\n        raise ValueError(f\"batch_size must be at least 1, got {batch_size}\")\n\n    if num_workers > "}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n    elif not isinstance(output_dir, Dir):\n        raise ValueError(f\"output_dir must be a string or a Dir object, got {type(output_dir)}\")\n\n    if num_workers is None:\n        num_workers = get_num_workers()\n\n    if num_downloaders is None:\n        num_downloaders = num_workers\n    if num_uploaders is None:\n        num_uploaders = num_workers\n\n    if fast_dev_run:\n        if isinstance(fast_dev_run, bool):\n            fast_dev_run = 1\n        inputs = inputs[:fast_dev_run]\n\n    if weights is not None:\n        if len(weights) != len(inputs):\n            raise ValueError(\"weights must be the same length as inputs\")\n\n    if batch_size is not None:\n        inputs = list(batch(inputs, batch_size))\n\n    if reorder_files:\n        inputs = reorder_inputs(inputs)\n\n    if reader is None:\n        reader = BaseReader()\n\n    if error_when_not_empty and output_dir.exists():\n        raise ValueError(f\"output_dir {output_dir} is not empty\")\n\n    if num_workers == 1:\n        for input in inputs:\n            fn(input, output_dir)\n    else:\n        with Pool(num_workers) as pool:\n            pool.starmap(\n                fn,\n                [(input, output_dir) for input in inputs],\n                weights=weights,\n                num_downloaders=num_downloaders,\n                num_uploaders=num_uploaders,\n                reader=reader,\n            )\n\n"}
{"namespace": "litdata.processing.functions.map", "completion": "    # Code snippet\n    import os\n    import sys\n    import multiprocessing as mp\n    from multiprocessing import Pool\n    from typing import Any, Callable, List, Optional, Sequence, Union\n    from lightning.pytorch.utilities.types import Dir, BaseReader\n\n    def process_input(input_data: Any) -> None:\n        \"\"\"\n        Processes a single input element.\n        :param input_data: Any. The input element to be processed.\n        :return: No return values. The function processes the input and stores the result in the output directory.\n        \"\"\"\n        # Process the input data using the provided function\n        fn(input_data, output_dir)\n\n    # Check if the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Check if the output directory is empty\n    if not os.listdir(output_dir):\n        # If the output directory is empty, process the inputs\n        if num_workers is None:\n            # If num_workers is not specified, use the number of available CPUs\n            num_workers = mp.cpu_count()\n        with Pool(num_workers) as pool:\n            pool.map(process_input, inputs)\n    else:\n        # If the output directory is not empty, raise an error or exit the program\n        if error_when_not_empty:\n            raise ValueError(\"Output directory is not empty\")\n        else:\n            sys.exit(\"Output directory is not empty\")\n\n"}
{"namespace": "litdata.processing.functions.map", "completion": "    if not isinstance(output_dir, Dir):\n        output_dir = Dir(output_dir)\n\n    if error_when_not_empty and output_dir.exists():\n        raise Exception(f\"{output_dir} is not empty.\")\n\n    if weights is not None:\n        assert len(weights) == len(inputs), \"weights must be same length as inputs\"\n\n    if fast_dev_run:\n        if isinstance(fast_dev_run, bool):\n            fast_dev_run = 1\n        inputs = inputs[:fast_dev_run]\n\n    if num_workers is None:\n        num_workers = min(multiprocessing.cpu_count(), len(inputs))\n\n    if num_workers == 1:\n        for i, input in enumerate(inputs):\n            fn(input, output_dir, i)\n        return\n\n    if batch_size is not None:\n        inputs = [inputs[i : i + batch_size] for i in range(0, len(inputs), batch_size)]\n\n    if weights is not None:\n        weights = [weights[i : i + batch_size] for i in range(0, len(weights), batch_size)]\n\n    if num_nodes is not None:\n        num_nodes = min(num_nodes, len(inputs))\n\n    if num_downloaders is None:\n        num_downloaders = min(multiprocessing.cpu_count(), len(inputs))\n\n    if num_uploaders is None:\n        num_uploaders = min(multiprocessing.cpu_count(), len(inputs))\n\n    if num_nodes is not None:\n        num_downloaders = int(num_downloaders / num_nodes)\n        num_uploaders = int(num_uploaders / num_nodes)\n\n    if num_downloaders == 0:\n        num_downloaders = 1\n\n    if num_uploaders == 0:\n        num_uploaders = 1\n\n    if weights is not None:\n        weights = [sum(w) for w in"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    if error_when_not_empty and output_dir.exists():\n        raise ValueError(\n            f\"The output directory {output_dir} already exists. \"\n            \"If you want to overwrite it, please set error_when_not_empty=False\"\n        )\n\n    if fast_dev_run:\n        inputs = inputs[:fast_dev_run]\n\n    if weights is not None:\n        assert len(inputs) == len(weights)\n\n    if batch_size is not None:\n        inputs = list(chunks(inputs, batch_size))\n\n    if reader is None:\n        reader = get_reader(inputs)\n\n    if num_workers is None:\n        num_workers = get_num_workers(num_nodes=num_nodes)\n\n    if num_downloaders is None:\n        num_downloaders = num_workers\n\n    if num_uploaders is None:\n        num_uploaders = num_workers\n\n    if num_workers == 1:\n        for input in inputs:\n            fn(input, output_dir)\n\n    else:\n        with Pool(\n            num_workers,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            machine=machine,\n            num_nodes=num_nodes,\n            reorder_files=reorder_files,\n        ) as pool:\n            pool.map(fn, inputs, output_dir, weights=weights)\n\n"}
{"namespace": "litdata.processing.functions.map", "completion": "    # If the output directory is not a string, convert it to a string\n    if not isinstance(output_dir, str):\n        output_dir = str(output_dir)\n\n    # Check if the output directory is empty\n    if os.listdir(output_dir) and error_when_not_empty:\n        raise ValueError(f\"Output directory {output_dir} is not empty\")\n\n    # If weights are provided, check if they have the same length as the inputs\n    if weights is not None and len(weights) != len(inputs):\n        raise ValueError(\"Weights must have the same length as inputs\")\n\n    # If weights are provided, create a mapping from inputs to weights\n    if weights is not None:\n        weights_map = {inputs[i]: weights[i] for i in range(len(inputs))}\n    else:\n        weights_map = None\n\n    # If a custom reader is provided, use it to read the inputs\n    if reader is not None:\n        inputs = reader.read(inputs)\n\n    # If batch_size is provided, group the inputs into batches of that size\n    if batch_size is not None:\n        inputs = [\n            inputs[i : i + batch_size] for i in range(0, len(inputs), batch_size)\n        ]\n\n    # Create a LightningDataModule to manage the data\n    data_module = LightningDataModule(\n        inputs=inputs,\n        output_dir=output_dir,\n        weights_map=weights_map,\n        num_workers=num_workers,\n        fast_dev_run=fast_dev_run,\n        num_nodes=num_nodes,\n        machine=machine,\n        num_downloaders=num_downloaders,\n        num_uploaders=num_uploaders,\n        reorder_files=reorder_files,\n        error_when_not_empty=error_when_not_empty,\n    )\n\n    # Create a LightningDataModule to manage the data\n    data_module = LightningDataModule(\n        inputs=inputs,\n        output"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    if isinstance(reader, str):\n        reader = get_reader_class(reader)()\n\n    if num_workers is None:\n        num_workers = min(os.cpu_count() or 1, 16)\n\n    if isinstance(fast_dev_run, bool) and fast_dev_run:\n        fast_dev_run = 1\n\n    if fast_dev_run:\n        inputs = inputs[:fast_dev_run]\n\n    if weights is not None:\n        weights = weights[:len(inputs)]\n\n    if num_downloaders is None:\n        num_downloaders = num_workers\n\n    if num_uploaders is None:\n        num_uploaders = num_workers\n\n    if num_nodes is not None and machine is None:\n        machine = \"cpu\"\n\n    if num_nodes is not None and machine is not None:\n        machine = f\"{machine}:{num_nodes}\"\n\n    if machine is not None:\n        num_workers = 1\n\n    if num_workers > 1:\n        if num_downloaders > num_workers:\n            num_downloaders = num_workers\n\n        if num_uploaders > num_workers:\n            num_uploaders = num_workers\n\n        if num_workers > 1 and num_downloaders == 1 and num_uploaders == 1:\n            num_workers = 1\n\n    if num_workers == 1:\n        if num_downloaders != 1 or num_uploaders != 1:\n            raise ValueError(\n                \"If num_workers == 1, num_downloaders and num_uploaders must be 1\"\n            )\n\n    if num_workers > 1 and num_downloaders != num_workers:\n        raise ValueError(\n            \"If num_workers > 1, num_downloaders must equal num_workers\"\n        )\n\n    if num_workers > 1 and num_uploaders != num_workers"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        task_index, file_paths = queue_in.get()\n        for file_path in file_paths:\n            source_path = input_dir.get_source_path(file_path)\n            target_path = os.path.join(cache_dir, file_path)\n            if not os.path.exists(target_path):\n                os.makedirs(os.path.dirname(target_path), exist_ok=True)\n                download_file(source_path, target_path)\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        task_index, file_paths = queue_in.get()\n        for file_path in file_paths:\n            if not os.path.exists(os.path.join(cache_dir, file_path)):\n                try:\n                    download_file(input_dir, file_path, cache_dir)\n                except Exception as e:\n                    print(f\"Error downloading file {file_path}: {e}\")\n                    continue\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        index, file_paths = queue_in.get()\n        for file_path in file_paths:\n            file_name = file_path.split(\"/\")[-1]\n            local_path = os.path.join(cache_dir, file_name)\n            if not os.path.exists(local_path):\n                remote_path = input_dir.get_path(file_path)\n                download_file(remote_path, local_path)\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Create the cache directory if it doesn't exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n        task_index, file_paths = queue_in.get()\n\n        # Check if the files are already downloaded\n        downloaded_files = os.listdir(cache_dir)\n        missing_files = [path for path in file_paths if os.path.basename(path) not in downloaded_files]\n\n        # Download missing files\n        for file_path in missing_files:\n            # Construct the source path or URL\n            source_path = input_dir.get_path(file_path)\n\n            # Construct the destination path\n            dest_path = os.path.join(cache_dir, os.path.basename(file_path))\n\n            # Download the file\n            download_file(source_path, dest_path)\n\n        # Signal completion by putting the task index into the output queue\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        task_index, file_paths = queue_in.get()\n        for file_path in file_paths:\n            file_name = os.path.basename(file_path)\n            file_path_cache = os.path.join(cache_dir, file_name)\n            if not os.path.exists(file_path_cache):\n                file_path_remote = input_dir.get_file_path(file_path)\n                if not os.path.exists(file_path_remote):\n                    raise ValueError(f\"File not found: {file_path_remote}\")\n                download_file(file_path_remote, file_path_cache)\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        task = queue_in.get()\n        index = task[0]\n        file_paths = task[1]\n\n        for file_path in file_paths:\n            if not os.path.exists(os.path.join(cache_dir, file_path)):\n                input_path = os.path.join(input_dir.path, file_path)\n                output_path = os.path.join(cache_dir, file_path)\n                os.makedirs(os.path.dirname(output_path), exist_ok=True)\n                download_file(input_path, output_path)\n\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        task_index, file_paths = queue_in.get()\n\n        if not file_paths:\n            queue_out.put(task_index)\n            continue\n\n        for file_path in file_paths:\n            source_path = input_dir.get_path(file_path)\n            target_path = os.path.join(cache_dir, file_path)\n\n            if not os.path.exists(target_path):\n                if input_dir.is_remote():\n                    download_file(source_path, target_path)\n                else:\n                    shutil.copyfile(source_path, target_path)\n\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Create the cache directory if it doesn't exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n        # Get the next download task from the queue\n        task_index, file_paths = queue_in.get()\n\n        # Check if the files are already downloaded\n        downloaded_file_paths = [os.path.join(cache_dir, os.path.basename(file_path)) for file_path in file_paths]\n        if all(os.path.exists(file_path) for file_path in downloaded_file_paths):\n            # All files are already downloaded, so signal completion by putting the task index into the output queue\n            queue_out.put(task_index)\n            continue\n\n        # Download the missing files\n        for file_path, downloaded_file_path in zip(file_paths, downloaded_file_paths):\n            # Get the source path or URL for the file\n            source_path = input_dir.get_path(file_path)\n\n            # Download the file\n            download_file(source_path, downloaded_file_path)\n\n        # Signal completion by putting the task index into the output queue\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Create the cache directory if it doesn't exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n        task_index, file_paths = queue_in.get()\n\n        # Check if the files are already downloaded\n        downloaded_files = [os.path.join(cache_dir, os.path.basename(file_path)) for file_path in file_paths]\n        if all(os.path.exists(file_path) for file_path in downloaded_files):\n            queue_out.put(task_index)\n            continue\n\n        # Download missing files\n        for file_path in file_paths:\n            file_name = os.path.basename(file_path)\n            file_url = os.path.join(input_dir.url, file_name)\n            file_path_cache = os.path.join(cache_dir, file_name)\n            download_file(file_url, file_path_cache)\n\n        # Signal completion by putting the task index into the output queue\n        queue_out.put(task_index)\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Create the cache directory if it doesn't exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n        task_index, file_paths = queue_in.get()\n\n        # Check if the files are already downloaded\n        file_paths_to_download = []\n        for file_path in file_paths:\n            # Get the local path for the file\n            local_path = os.path.join(cache_dir, file_path)\n\n            # Check if the file is already downloaded\n            if not os.path.exists(local_path):\n                file_paths_to_download.append(file_path)\n\n        # If there are files to download, download them\n        if file_paths_to_download:\n            # Get the source path or URL for the files\n            source_path = input_dir.get_path(file_paths_to_download[0])\n\n            # Download the files\n            download_files(source_path, cache_dir, file_paths_to_download)\n\n        # Signal completion by putting the task index into the output queue\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Create the cache directory if it does not exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n        # Fetch the next download task from the queue\n        task_index, file_paths = queue_in.get()\n\n        # Iterate over the file paths for the task\n        for file_path in file_paths:\n            # Get the source path or URL for the file\n            source_path = input_dir.get_path(file_path)\n\n            # Get the destination path for the file in the cache directory\n            dest_path = os.path.join(cache_dir, file_path)\n\n            # Check if the file is already downloaded\n            if os.path.exists(dest_path):\n                # If the file is already downloaded, skip downloading it\n                continue\n\n            # Download the file from the source path or URL to the destination path in the cache directory\n            download_file(source_path, dest_path)\n\n        # Signal completion by putting the task index into the output queue\n        queue_out.put(task_index)\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Create the cache directory if it doesn't exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n        # Fetch a download task from the input queue\n        task_index, file_paths = queue_in.get()\n\n        # Check if the files are already downloaded\n        for file_path in file_paths:\n            # Construct the local cache file path\n            cache_file_path = os.path.join(cache_dir, file_path)\n\n            # Check if the file is already downloaded\n            if os.path.exists(cache_file_path):\n                continue\n\n            # Construct the source path or URL for the file\n            source_path = input_dir.get_path(file_path)\n\n            # Download the file to the cache directory\n            download_file(source_path, cache_file_path)\n\n        # Put the task index into the output queue to signal completion\n        queue_out.put(task_index)\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Initialize the downloader\n    downloader = Downloader()\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n\n        # Fetch a download task from the input queue\n        task_index, file_paths = queue_in.get()\n\n        # Check if the files are already downloaded\n        if all(downloader.is_downloaded(file_path, cache_dir) for file_path in file_paths):\n\n            # If all files are downloaded, put the task index into the output queue\n            queue_out.put(task_index)\n\n        else:\n\n            # If any file is missing, download the missing files\n            for file_path in file_paths:\n\n                # Get the source path or URL for the file\n                source_path = input_dir.get_file_path(file_path)\n\n                # Download the file to the cache directory\n                downloader.download(source_path, cache_dir)\n\n            # Put the task index into the output queue\n            queue_out.put(task_index)\n\n        # Signal that the task is completed\n        queue_in.task_done()\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Initialize the queue for storing download tasks\n    queue_in = Queue()\n\n    # Initialize the queue for communicating the completion of download tasks\n    queue_out = Queue()\n\n    # Initialize the list of download tasks\n    download_tasks = []\n\n    # Initialize the list of downloaded files\n    downloaded_files = []\n\n    # Initialize the list of files to download\n    files_to_download = []\n\n    # Initialize the list of files to skip\n    files_to_skip = []\n\n    # Initialize the list of files that failed to download\n    failed_files = []\n\n    # Initialize the list of files that were not downloaded due to errors\n    not_downloaded_files = []\n\n    # Initialize the list of files that were not downloaded due to errors\n    not_downloaded_files = []\n\n    # Initialize the list of files that were not downloaded due to errors\n    not_downloaded_files = []\n\n    # Initialize the list of files that were not downloaded due to errors\n    not_downloaded_files = []\n\n    # Initialize the list of files that were not downloaded due to errors\n    not_downloaded_files = []\n\n    # Initialize the list of files that were not downloaded due to errors\n    not_downloaded_files = []\n\n    # Initialize the list of files that were not downloaded due to errors\n    not_downloaded_files = []\n\n    # Initialize the list of files that were not downloaded due to errors\n    not_downloaded_files = []\n\n    # Initialize the list of files that were not downloaded due to errors\n    not_downloaded_files = []\n\n    # Initialize the list of files that were not downloaded due to errors\n    not_downloaded_files = []\n\n    # Initialize the list of files that were not downloaded due to errors\n    not_downloaded_files = []\n\n    # Initialize the list of files that were not downloaded due to errors\n    not_downloaded_files = []\n\n    # Initialize the list of files that were not downloaded due to errors\n    not_downloaded_files = []\n\n   "}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Set up a progress bar for the download\n    progress_bar = tqdm(total=input_dir.num_files, desc='Downloading data')\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n        task = queue_in.get()\n        if task is None:\n            break\n\n        # Check if the files are already downloaded\n        for file_path in task.file_paths:\n            if not os.path.exists(os.path.join(cache_dir, file_path)):\n                # If the file is not downloaded, download it\n                url = input_dir.get_url(file_path)\n                download_file(url, os.path.join(cache_dir, file_path))\n\n        # Signal completion by putting the task index into the output queue\n        queue_out.put(task.index)\n        progress_bar.update(len(task.file_paths))\n\n    # Close the progress bar\n    progress_bar.close()\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Loop indefinitely\n    while True:\n\n        # Fetch a task from the input queue\n        task = queue_in.get()\n\n        # If the task is None, it means the queue is closed, so break the loop\n        if task is None:\n            break\n\n        # Get the index and file paths from the task\n        index, file_paths = task\n\n        # Loop over the file paths\n        for file_path in file_paths:\n\n            # Get the source path or URL for the file\n            source_path = input_dir.get_path(file_path)\n\n            # Get the destination path for the file\n            dest_path = os.path.join(cache_dir, file_path)\n\n            # Check if the file is already downloaded\n            if os.path.exists(dest_path):\n                continue\n\n            # If the file is not downloaded, download it\n            download_file(source_path, dest_path)\n\n        # Signal the completion of the task by putting the index into the output queue\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Create the cache directory if it doesn't exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    while True:\n        # Fetch a download task from the input queue\n        task_index, file_paths = queue_in.get()\n\n        # Iterate over the file paths for the task\n        for file_path in file_paths:\n            # Construct the source path or URL for the file\n            source_path = input_dir.get_path(file_path)\n\n            # Construct the destination path for the file in the cache directory\n            dest_path = os.path.join(cache_dir, file_path)\n\n            # Check if the file already exists in the cache directory\n            if not os.path.exists(dest_path):\n                # Download the file from the source path or URL to the destination path in the cache directory\n                download_file(source_path, dest_path)\n\n        # Signal completion of the download task by putting the task index into the output queue\n        queue_out.put(task_index)\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Initialize a progress bar for the download process\n    pbar = tqdm(total=input_dir.num_files, desc=f\"Downloading {input_dir.name}\", unit=\"files\", leave=False)\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n        # Get the next download task from the queue\n        task_index, file_paths = queue_in.get()\n\n        # Check if the files for the task are already downloaded\n        if all(os.path.exists(os.path.join(cache_dir, file_path)) for file_path in file_paths):\n            # If all files are already downloaded, put the task index into the output queue and update the progress bar\n            queue_out.put(task_index)\n            pbar.update(len(file_paths))\n        else:\n            # If some files are missing, download them and put the task index into the output queue\n            for file_path in file_paths:\n                # Construct the source path or URL for the file\n                source_path = os.path.join(input_dir.path, file_path)\n\n                # Construct the destination path for the downloaded file\n                dest_path = os.path.join(cache_dir, file_path)\n\n                # Create the destination directory if it doesn't exist\n                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n\n                # Download the file to the destination path\n                download_file(source_path, dest_path)\n\n            # Put the task index into the output queue and update the progress bar\n            queue_out.put(task_index)\n            pbar.update(len(file_paths))\n\n        # Mark the task as done in the progress bar\n        pbar.refresh()\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Create a dictionary to store the file paths for each task index\n    task_index_to_file_paths = {}\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n\n        # Fetch a download task from the input queue\n        task_index, file_paths = queue_in.get()\n\n        # Add the file paths for the task index to the dictionary\n        task_index_to_file_paths[task_index] = file_paths\n\n        # Check if the files for the task index are already downloaded\n        for file_path in file_paths:\n\n            # Get the relative path of the file within the input directory\n            relative_file_path = file_path.relative_to(input_dir.path)\n\n            # Get the full path of the file in the cache directory\n            cache_file_path = Path(cache_dir) / relative_file_path\n\n            # Check if the file is already downloaded\n            if cache_file_path.exists():\n                continue\n\n            # Download the file\n            _download_file(file_path, cache_file_path)\n\n        # Signal completion by putting the task index into the output queue\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Initialize the logger\n    logger = logging.getLogger(f\"{__name__}._download_data_target\")\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n        try:\n            # Fetch a download task from the input queue\n            task_index, file_paths = queue_in.get(block=True, timeout=0.1)\n\n            # Check if the files are already downloaded\n            if not all(\n                [\n                    os.path.exists(\n                        os.path.join(\n                            cache_dir,\n                            os.path.relpath(file_path, input_dir.path),\n                        )\n                    )\n                    for file_path in file_paths\n                ]\n            ):\n                # If not, download the missing files\n                for file_path in file_paths:\n                    # Get the source path or URL for the file\n                    source_path = os.path.join(input_dir.path, file_path)\n                    if input_dir.is_url:\n                        source_path = os.path.join(input_dir.url, file_path)\n\n                    # Get the destination path for the file\n                    dest_path = os.path.join(cache_dir, file_path)\n\n                    # Download the file\n                    logger.info(f\"Downloading {source_path} to {dest_path}\")\n                    _download_file(source_path, dest_path)\n\n            # Put the task index into the output queue to signal completion\n            queue_out.put(task_index)\n        except Empty:\n            # If the input queue is empty, wait for a short time before checking again\n            time.sleep(0.1)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n        else:\n            temp_dir = None\n            file_path = item\n\n        if temp_dir is not None:\n            file_path = os.path.join(temp_dir, file_path)\n\n        if not os.path.isfile(file_path):\n            continue\n\n        if output_dir.scheme == \"s3\":\n            output_path = os.path.join(output_dir.path, file_path)\n            output_dir.upload_file(file_path, output_path)\n        else:\n            output_path = os.path.join(output_dir.path, file_path)\n            if not os.path.exists(os.path.dirname(output_path)):\n                os.makedirs(os.path.dirname(output_path))\n            shutil.copyfile(file_path, output_path)\n\n        remove_queue.put(file_path)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n            file_path = os.path.join(temp_dir, file_path)\n        else:\n            file_path = item\n\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n\n        if output_dir.scheme == \"s3\":\n            output_file_path = file_path.replace(cache_dir, output_dir.path)\n            output_file_path = output_file_path.replace(\"\\\\\", \"/\")\n            output_file_path = output_file_path.replace(\"//\", \"/\")\n            output_file_path = output_file_path.replace(\"/./\", \"/\")\n            output_file_path = output_file_path.replace(\"/.\", \"/\")\n            output_file_path = output_file_path.replace(\"//\", \"/\")\n            output_file_path = output_file_path.replace(\"/./\", \"/\")\n            output_file_path = output_file_path.replace(\"/.\", \"/\")\n            output_file_path = output_file_path.replace(\"//\", \"/\")\n            output_file_path = output_file_path.replace(\"/./\", \"/\")\n            output_file_path = output_file_path.replace(\"/.\", \"/\")\n            output_file_path = output_file_path.replace(\"//\", \"/\")\n            output_file_path = output_file_path.replace(\"/./\", \"/\")\n            output_file_path = output_file_path.replace(\"/.\", \"/\")\n            output_file_path = output_file_path.replace(\"//\", \"/\")\n            output_file_path = output_file_path.replace(\"/./\", \"/\")\n            output_file_path = output_file_path.replace(\"/.\", \"/\")\n            output_file_path = output_file_path.replace(\"//\", \"/\")\n            output_file_"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        try:\n            item = upload_queue.get(timeout=1)\n        except Empty:\n            return\n\n        if isinstance(item, tuple):\n            # item is a tuple, representing a temporary directory and a file path\n            temp_dir, file_path = item\n            file_path = os.path.join(temp_dir, file_path)\n        else:\n            # item is a file path\n            file_path = item\n\n        # Upload the file to the output directory\n        if output_dir.scheme == \"s3\":\n            # Upload to S3\n            output_path = os.path.join(output_dir.path, os.path.basename(file_path))\n            output_dir.upload_file(file_path, output_path)\n        else:\n            # Move to local directory\n            output_path = os.path.join(output_dir.path, os.path.basename(file_path))\n            shutil.move(file_path, output_path)\n\n        # Send the file path to the remove queue for removal\n        remove_queue.put(file_path)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n        elif isinstance(item, tuple):\n            temp_dir, file_path = item\n            file_path = os.path.join(temp_dir, file_path)\n        else:\n            file_path = item\n\n        if output_dir.scheme == \"file\":\n            target_path = os.path.join(output_dir.path, file_path)\n            target_dir = os.path.dirname(target_path)\n            os.makedirs(target_dir, exist_ok=True)\n            shutil.move(os.path.join(cache_dir, file_path), target_path)\n        elif output_dir.scheme == \"s3\":\n            target_path = os.path.join(output_dir.path, file_path)\n            s3_client = boto3.client(\"s3\")\n            s3_client.upload_file(os.path.join(cache_dir, file_path), output_dir.bucket, target_path)\n\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n        if isinstance(item, tuple):\n            dir_path, file_path = item\n            file_path = os.path.join(dir_path, file_path)\n        else:\n            file_path = item\n        if file_path.startswith(cache_dir):\n            file_path = file_path[len(cache_dir):]\n        output_path = os.path.join(output_dir, file_path)\n        if output_dir.scheme == \"s3\":\n            output_dir.upload_file(file_path, output_path)\n        else:\n            output_dir.upload_file(file_path, output_path)\n        remove_queue.put(file_path)\n    upload_queue.task_done()\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # Create a thread-safe logger\n    logger = logging.getLogger(f\"{__name__}\")\n\n    # Create a thread-safe counter for uploaded files\n    uploaded_files = AtomicCounter()\n\n    # Create a thread-safe counter for failed uploads\n    failed_uploads = AtomicCounter()\n\n    # Create a thread-safe counter for uploaded bytes\n    uploaded_bytes = AtomicCounter()\n\n    # Create a thread-safe counter for failed bytes\n    failed_bytes = AtomicCounter()\n\n    # Create a thread-safe counter for skipped files\n    skipped_files = AtomicCounter()\n\n    # Create a thread-safe counter for skipped bytes\n    skipped_bytes = AtomicCounter()\n\n    # Create a thread-safe counter for total files\n    total_files = AtomicCounter()\n\n    # Create a thread-safe counter for total bytes\n    total_bytes = AtomicCounter()\n\n    # Create a thread-safe counter for total uploads\n    total_uploads = AtomicCounter()\n\n    # Create a thread-safe counter for total failed uploads\n    total_failed_uploads = AtomicCounter()\n\n    # Create a thread-safe counter for total failed bytes\n    total_failed_bytes = AtomicCounter()\n\n    # Create a thread-safe counter for total skipped files\n    total_skipped_files = AtomicCounter()\n\n    # Create a thread-safe counter for total skipped bytes\n    total_skipped_bytes = AtomicCounter()\n\n    # Create a thread-safe counter for total total files\n    total_total_files = AtomicCounter()\n\n    # Create a thread-safe counter for total total bytes\n    total_total_bytes = AtomicCounter()\n\n    # Create a thread-safe counter for total total uploads\n    total_total_uploads = AtomicCounter()\n\n    # Create a thread-safe counter for total total failed uploads\n    total_total_failed_uploads = AtomicCounter()\n\n    # Create a thread-safe counter for total total failed bytes\n    total_total_failed_bytes = AtomicCounter()\n\n    # Create a thread-"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item == 'terminate':\n            break\n\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n            file_path = os.path.join(temp_dir, file_path)\n        else:\n            file_path = item\n\n        if output_dir.scheme == 'file':\n            target_path = os.path.join(output_dir.path, file_path)\n            if not os.path.exists(os.path.dirname(target_path)):\n                os.makedirs(os.path.dirname(target_path))\n            shutil.copy(os.path.join(cache_dir, file_path), target_path)\n        elif output_dir.scheme == 's3':\n            target_path = os.path.join(output_dir.path, file_path)\n            s3_client = boto3.client('s3')\n            s3_client.upload_file(os.path.join(cache_dir, file_path), output_dir.bucket, target_path)\n        else:\n            raise ValueError(f\"Unsupported output directory scheme: {output_dir.scheme}\")\n\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n            file_path = os.path.join(temp_dir, file_path)\n        else:\n            file_path = item\n\n        if output_dir.scheme == \"s3\":\n            output_path = output_dir.path + file_path[len(cache_dir):]\n            output_dir.upload(file_path, output_path)\n        else:\n            output_path = os.path.join(output_dir.path, file_path[len(cache_dir):])\n            shutil.move(file_path, output_path)\n\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n\n        if isinstance(item, tuple):\n            tmp_dir, file_path = item\n            file_path = os.path.join(tmp_dir, file_path)\n        else:\n            file_path = item\n\n        if output_dir.scheme == \"file\":\n            output_file_path = os.path.join(output_dir.path, file_path)\n            os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n            shutil.copy(os.path.join(cache_dir, file_path), output_file_path)\n            remove_queue.put(file_path)\n        elif output_dir.scheme == \"s3\":\n            output_file_path = os.path.join(output_dir.path, file_path)\n            output_dir.client.upload_file(os.path.join(cache_dir, file_path), output_dir.bucket, output_file_path)\n            remove_queue.put(file_path)\n        else:\n            raise ValueError(f\"Unsupported scheme: {output_dir.scheme}\")\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n        if isinstance(item, tuple):\n            upload_dir, upload_path = item\n            upload_path = os.path.join(upload_dir, upload_path)\n        else:\n            upload_path = item\n        if not upload_path.startswith(cache_dir):\n            upload_path = os.path.join(cache_dir, upload_path)\n        if output_dir.scheme == \"s3\":\n            output_path = os.path.join(output_dir.path, upload_path)\n            output_dir.s3.upload_file(upload_path, output_path)\n        else:\n            output_path = os.path.join(output_dir.path, upload_path)\n            output_dir.fs.copy(upload_path, output_path)\n        remove_queue.put(upload_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if isinstance(item, tuple):\n            dir_path, file_path = item\n            upload_path = os.path.join(dir_path, file_path)\n        else:\n            upload_path = item\n\n        # If the file path starts with the cache directory, remove it\n        if upload_path.startswith(cache_dir):\n            upload_path = upload_path[len(cache_dir):]\n\n        # Upload the file to the target directory\n        output_dir.upload(upload_path)\n\n        # Send the file path to the remove queue for removal\n        remove_queue.put(upload_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n            file_path = os.path.join(temp_dir, file_path)\n        else:\n            file_path = item\n\n        if output_dir.scheme == \"s3\":\n            # Upload to S3\n            s3_path = os.path.join(output_dir.path, os.path.basename(file_path))\n            s3_client = boto3.client(\"s3\")\n            s3_client.upload_file(file_path, output_dir.bucket, s3_path)\n        else:\n            # Move to local directory\n            output_path = os.path.join(output_dir.path, os.path.basename(file_path))\n            shutil.move(file_path, output_path)\n\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        try:\n            item = upload_queue.get(timeout=1)\n        except Empty:\n            break\n\n        if isinstance(item, str):\n            src = item\n            dst = item\n        else:\n            src = item[1]\n            dst = item[0]\n\n        try:\n            if output_dir.scheme == \"s3\":\n                output_dir.upload(src, dst)\n            else:\n                shutil.copyfile(cache_dir + src, output_dir.path + dst)\n        except Exception as e:\n            logging.error(f\"Failed to upload {src} to {output_dir}: {e}\")\n            continue\n\n        remove_queue.put(src)\n\n    logging.info(\"Upload worker finished\")"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n\n        if isinstance(item, tuple):\n            tmp_dir, file_path = item\n            file_path = os.path.join(tmp_dir, file_path)\n        else:\n            file_path = item\n\n        if not os.path.exists(file_path):\n            continue\n\n        if file_path.startswith(cache_dir):\n            file_path = file_path[len(cache_dir) + 1:]\n\n        if output_dir.scheme == \"file\":\n            output_path = os.path.join(output_dir.path, file_path)\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n            shutil.copy(file_path, output_path)\n        else:\n            output_path = os.path.join(output_dir.path, file_path)\n            output_dir.put_file(file_path, output_path)\n\n        remove_queue.put(file_path)\n\n    upload_queue.task_done()\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n        elif isinstance(item, tuple):\n            tmp_dir, file_path = item\n            file_path = os.path.join(tmp_dir, file_path)\n        else:\n            file_path = item\n\n        if output_dir.scheme == \"s3\":\n            # Upload the file to S3\n            output_path = os.path.join(output_dir.path, os.path.basename(file_path))\n            output_dir.upload_file(file_path, output_path)\n        else:\n            # Move the file to the output directory\n            output_path = os.path.join(output_dir.path, os.path.basename(file_path))\n            shutil.move(file_path, output_path)\n\n        # Send the file path for removal\n        remove_queue.put(file_path)\n\n    return None\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n\n        if item is None:\n            break\n\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n            file_path = os.path.join(temp_dir, file_path)\n        else:\n            file_path = item\n\n        if file_path.startswith(cache_dir):\n            file_path = file_path[len(cache_dir) + 1 :]\n\n        output_path = os.path.join(output_dir, file_path)\n        output_dir = os.path.dirname(output_path)\n\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        if output_dir.startswith(\"s3://\"):\n            s3_bucket, s3_key = output_path[5:].split(\"/\", 1)\n            s3_client.upload_file(file_path, s3_bucket, s3_key)\n        else:\n            shutil.move(file_path, output_path)\n\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n        if isinstance(item, tuple):\n            dir_path, file_path = item\n            if dir_path.startswith(cache_dir):\n                file_path = file_path[len(cache_dir) :]\n            file_path = output_dir.join(file_path)\n            file_path.upload(dir_path, progress=False)\n        else:\n            file_path = output_dir.join(item[len(cache_dir) :])\n            file_path.upload(item, progress=False)\n        remove_queue.put(item)\n    upload_queue.task_done()\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n            file_path = os.path.join(temp_dir, file_path)\n        else:\n            file_path = item\n        if file_path.startswith(cache_dir):\n            file_path = file_path[len(cache_dir) :]\n        output_path = os.path.join(output_dir, file_path)\n        if output_dir.scheme == \"s3\":\n            output_path = output_dir.url + file_path\n        if not os.path.exists(os.path.dirname(output_path)):\n            os.makedirs(os.path.dirname(output_path))\n        shutil.move(file_path, output_path)\n        remove_queue.put(file_path)\n    upload_queue.task_done()\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item == \"stop\":\n            break\n\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n            file_path = os.path.join(temp_dir, file_path)\n        else:\n            file_path = item\n\n        if output_dir.scheme == \"s3\":\n            upload_to_s3(file_path, output_dir.path)\n        else:\n            shutil.copy(file_path, output_dir.path)\n\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        try:\n            item = upload_queue.get(block=True, timeout=1)\n            if item == \"terminate\":\n                break\n            if isinstance(item, tuple):\n                file_path = item[1]\n                temp_dir = item[0]\n            else:\n                file_path = item\n                temp_dir = None\n\n            # If the file path is relative, prepend the cache directory\n            if not os.path.isabs(file_path):\n                file_path = os.path.join(cache_dir, file_path)\n\n            # If the file path starts with the cache directory, strip it\n            if file_path.startswith(cache_dir):\n                file_path = file_path[len(cache_dir) :]\n\n            # If the file path is empty, skip it\n            if not file_path:\n                continue\n\n            # If the output directory is a local directory, move the file to it\n            if output_dir.scheme == \"file\":\n                target_path = os.path.join(output_dir.path, file_path)\n                target_dir = os.path.dirname(target_path)\n                os.makedirs(target_dir, exist_ok=True)\n                shutil.move(file_path, target_path)\n            # If the output directory is an S3 bucket, upload the file to it\n            elif output_dir.scheme == \"s3\":\n                target_path = os.path.join(output_dir.path, file_path)\n                s3_client.upload_file(file_path, output_dir.bucket, target_path)\n            # If the output directory is a local directory, move the file to it\n            elif output_dir.scheme == \"file\":\n                target_path = os.path.join(output_dir.path, file_path)\n                target_dir = os.path.dirname(target_path)\n                os.makedirs(target_dir, exist_ok=True)\n                shutil.move(file_path, target_path)\n            # If the output directory"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_items_among_workers(num_workers, user_items, weights=None, file_size=False):\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        world_size = num_nodes * num_workers\n\n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n        worker_items_this_node = [worker_items[i] for i in worker_ids_this_node]\n        worker_weights_this_node = [worker_weights[i] for i in worker_ids_this_node]\n\n        if file_size:\n            print(f\"Worker IDs on node {node_rank}: {worker_ids_this_node}\")\n            print(f\"Worker file sizes on node {node_rank}: {worker_weights_this_node} MB\")\n        else:\n            print(f\"Worker IDs on node {node_rank}: {worker_ids_this_node}\")\n            print(f\"Worker weights on node {node_rank}: {worker_weights_this_node}\")\n\n        return worker_items_this_node\n\n\n    \"\"\"\n    This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution details for workers on the current node and returns a list of items for each worker, with the items shuffled.\n\n    Input-Output Arguments\n    :param num_workers: Int. The number of workers per node.\n    :param user_items: List[Any]. The items to be distributed among the workers.\n    :param weights: Optional[List[int]]. The weights for each item, used for distribution. If not provided, all items are considered to have"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_greedily(num_workers, user_items, weights=None, file_size=False):\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Print distribution details for workers on this node\n    _print_worker_distribution(worker_items, worker_weights, worker_ids_this_node, file_size)\n\n    # Shuffle items for each worker\n    worker_items_shuffled = [np.random.permutation(worker_items[i]) for i in worker_ids_this_node]\n\n    return worker_items_shuffled\n\n\n    \"\"\"\n    This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution details for workers on the current node and returns a list of items for each worker, with the items shuffled.\n\n    Input-Output Arguments\n    :param num_workers: Int. The number of workers per node.\n    :param user_items: List[Any]. The items to be distributed among the workers.\n    :param weights: Optional[List[int]]. The weights for each item, used for distribution. If not provided, all items are considered to have equal weight.\n    :param file_size: Bool. A flag indicating whether to consider the items as files with sizes for printing purposes. If True, sizes are printed in megabytes; otherwise, the total weight of items is printed.\n    :return: List[List[Any]]. A list of lists, where each sublist contains the items assigned to a worker, shuffled randomly.\n    \"\"\"\n    def _distribute_gre"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_items(num_workers, user_items, weights=None, file_size=False):\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        world_size = num_nodes * num_workers\n\n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n        if file_size:\n            print(f\"Node {node_rank}:\")\n            for worker_id in worker_ids_this_node:\n                worker_items_this_node = worker_items[worker_id]\n                worker_weights_this_node = worker_weights[worker_id]\n                print(f\"Worker {worker_id}: {len(worker_items_this_node)} items ({worker_weights_this_node / 1024 / 1024:.2f} MB)\")\n        else:\n            print(f\"Node {node_rank}:\")\n            for worker_id in worker_ids_this_node:\n                worker_items_this_node = worker_items[worker_id]\n                worker_weights_this_node = worker_weights[worker_id]\n                print(f\"Worker {worker_id}: {len(worker_items_this_node)} items ({worker_weights_this_node} weight)\")\n\n        worker_items_shuffled = []\n        for worker_id in worker_ids_this_node:\n            worker_items_this_node = worker_items[worker_id]\n            worker_items_shuffled.append(np.random.permutation(worker_items_this_node).tolist())\n\n        return worker_items_shuffled\n\n\n    \"\"\"\n    This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _pack_greedily(items, weights, num_bins):\n        \"\"\"\n        This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution details for workers on the current node and returns a list of items for each worker, with the items shuffled.\n\n        Input-Output Arguments\n        :param num_workers: Int. The number of workers per node.\n        :param user_items: List[Any]. The items to be distributed among the workers.\n        :param weights: Optional[List[int]]. The weights for each item, used for distribution. If not provided, all items are considered to have equal weight.\n        :param file_size: Bool. A flag indicating whether to consider the items as files with sizes for printing purposes. If True, sizes are printed in megabytes; otherwise, the total weight of items is printed.\n        :return: List[List[Any]]. A list of lists, where each sublist contains the items assigned to a worker, shuffled randomly.\n        \"\"\"\n        num_workers = num_bins\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        world_size = num_nodes * num_workers\n\n        # If weights are not provided, consider all items to have equal weight\n        if weights is None:\n            weights = [1] * len(items)\n\n        # Calculate the total weight of items\n        total_weight = sum(weights)\n\n        # Distribute items to workers based on weights\n        worker_items = [[] for _ in range(num_workers)]\n        for item, weight in zip(items, weights):\n            worker_idx = int(weight / total_weight * num_workers)\n            worker_items[worker_idx].append(item)\n\n        # Shuffle items within each worker\n        for worker_idx in range(num_workers):\n            random.shuffle(worker_items[worker_idx])\n\n        # Print distribution details for workers"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_items(num_workers, user_items, weights=None, file_size=False):\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        world_size = num_nodes * num_workers\n    \n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n    \n        # Print the distribution details for workers on the current node\n        if file_size:\n            print(\"Distribution details for workers on node {}\".format(node_rank))\n            for worker_id in worker_ids_this_node:\n                print(\"Worker {}: {} items, total size: {:.2f} MB\".format(worker_id, len(worker_items[worker_id]), sum(worker_weights[worker_id]) / 1024 / 1024))\n        else:\n            print(\"Distribution details for workers on node {}\".format(node_rank))\n            for worker_id in worker_ids_this_node:\n                print(\"Worker {}: {} items, total weight: {}\".format(worker_id, len(worker_items[worker_id]), sum(worker_weights[worker_id])))\n    \n        # Shuffle the items assigned to each worker\n        for worker_id in worker_ids_this_node:\n            random.shuffle(worker_items[worker_id])\n    \n        return worker_items\n\n\n    \"\"\"\n    This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution details for workers on the current node and returns a list of items for each worker, with the items shuffled.\n\n    Input-Output Arguments\n    :param num_workers: Int. The number of workers per node"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_items_among_workers(num_workers, user_items, weights=None, file_size=False):\n        # Calculate the total number of workers across all nodes\n        num_nodes = _get_num_nodes()\n        world_size = num_nodes * num_workers\n\n        # Pack the items and weights into bins based on the world size\n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n\n        # Get the rank of the current node\n        node_rank = _get_node_rank()\n\n        # Get the worker IDs for the current node\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n        # Print the distribution details for workers on the current node\n        _print_distribution_details(worker_ids_this_node, worker_items, worker_weights, file_size)\n\n        # Return the worker items for the current node\n        return worker_items\n\n\n    \"\"\"\n    This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution details for workers on the current node and returns a list of items for each worker, with the items shuffled.\n\n    Input-Output Arguments\n    :param num_workers: Int. The number of workers per node.\n    :param user_items: List[Any]. The items to be distributed among the workers.\n    :param weights: Optional[List[int]]. The weights for each item, used for distribution. If not provided, all items are considered to have equal weight.\n    :param file_size: Bool. A flag indicating whether to consider the items as files with sizes for printing purposes. If True, sizes are printed in megabytes; otherwise, the total weight of items is printed.\n    :return: List[List[Any]]. A list of lists, where each sublist contains the items assigned to a"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_greedily(num_workers, user_items, weights=None, file_size=False):\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        world_size = num_nodes * num_workers\n\n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n        # Print distribution details for workers on this node\n        for worker_id in worker_ids_this_node:\n            items_str = \" \".join([str(item) for item in worker_items[worker_id]])\n            if file_size:\n                size_str = f\"({sum(worker_weights[worker_id]) / 1024 / 1024:.2f} MB)\"\n            else:\n                size_str = f\"({sum(worker_weights[worker_id]):,} bytes)\"\n            print(f\"Worker {worker_id}: {items_str} {size_str}\")\n\n        # Shuffle items for each worker\n        for worker_id in worker_ids_this_node:\n            random.shuffle(worker_items[worker_id])\n\n        return worker_items\n\n    \"\"\"\n    This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution details for workers on the current node and returns a list of items for each worker, with the items shuffled.\n\n    Input-Output Arguments\n    :param num_workers: Int. The number of workers per node.\n    :param user_items: List[Any]. The items to be distributed among the workers.\n    :param weights: Optional[List[int]]. The weights for each item, used for distribution. If not provided, all items are considered to"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_items_among_workers(num_workers: int, user_items: List[Any], weights: Optional[List[int]] = None, file_size: bool = False) -> List[List[Any]]:\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        world_size = num_nodes * num_workers\n\n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n        # Print the distribution details for workers on the current node\n        for worker_id in worker_ids_this_node:\n            items = worker_items[worker_id]\n            if file_size:\n                sizes = [os.path.getsize(item) for item in items]\n                total_size = sum(sizes) / 1e6\n                print(f\"Worker {worker_id} on node {node_rank} has {len(items)} items with total size {total_size:.2f} MB\")\n            else:\n                total_weight = sum(worker_weights[worker_id])\n                print(f\"Worker {worker_id} on node {node_rank} has {len(items)} items with total weight {total_weight}\")\n\n        # Shuffle items for each worker and return the result\n        worker_items_shuffled = [list(np.random.permutation(items)) for items in worker_items]\n        return worker_items_shuffled\n\n\n    \"\"\"\n    This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution details for workers on the current node and returns a list of items for each worker, with the items shuffled.\n\n    Input-Output Arguments\n    :param num_work"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_items_among_workers(num_workers, user_items, weights=None, file_size=False):\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        world_size = num_nodes * num_workers\n\n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n        # Print the distribution details for workers on the current node\n        for worker_id in worker_ids_this_node:\n            worker_items_this_node = worker_items[worker_id]\n            worker_weights_this_node = worker_weights[worker_id]\n            if file_size:\n                worker_sizes_this_node = [item.size for item in worker_items_this_node]\n                worker_sizes_this_node = [size / 1024 / 1024 for size in worker_sizes_this_node]\n                worker_sizes_this_node = [f\"{size:.2f} MB\" for size in worker_sizes_this_node]\n                worker_sizes_this_node = \" \".join(worker_sizes_this_node)\n                print(f\"Worker {worker_id}: {worker_sizes_this_node} ({worker_weights_this_node} MB)\")\n            else:\n                print(f\"Worker {worker_id}: {worker_weights_this_node}\")\n\n        # Shuffle the items for each worker and return them\n        worker_items_shuffled = []\n        for worker_id in worker_ids_this_node:\n            worker_items_this_node = worker_items[worker_id]\n            random.shuffle(worker_items_this_node)\n            worker_items_shuffled.append(worker_items_this_node)"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_items_among_workers(num_workers: int, user_items: List[Any], weights: Optional[List[int]] = None, file_size: bool = False) -> List[List[Any]]:\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        world_size = num_nodes * num_workers\n    \n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n    \n        # Print distribution details\n        if file_size:\n            print(f\"Worker IDs on node {node_rank}: {worker_ids_this_node}\")\n            for worker_id, (items, weight) in zip(worker_ids_this_node, zip(worker_items, worker_weights)):\n                print(f\"Worker {worker_id}: {len(items)} items ({weight / 1e6:.2f} MB)\")\n        else:\n            print(f\"Worker IDs on node {node_rank}: {worker_ids_this_node}\")\n            for worker_id, (items, weight) in zip(worker_ids_this_node, zip(worker_items, worker_weights)):\n                print(f\"Worker {worker_id}: {len(items)} items ({weight} weight)\")\n    \n        # Shuffle items for each worker\n        shuffled_worker_items = [shuffle(items) for items in worker_items]\n    \n        return shuffled_worker_items\n\n\n    \"\"\"\n    This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution details for workers on the current node and returns a list of items for each worker, with the items shuffled"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_items(num_workers, user_items, weights=None, file_size=False):\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        world_size = num_nodes * num_workers\n\n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n        if file_size:\n            worker_sizes = [sum(item.size for item in items) for items in worker_items]\n            worker_sizes = [size / 1024 / 1024 for size in worker_sizes]\n            worker_sizes = [f\"{size:.2f}MB\" for size in worker_sizes]\n        else:\n            worker_sizes = [sum(weight for weight in weights) for weights in worker_weights]\n            worker_sizes = [f\"{size:.2f}GB\" for size in worker_sizes]\n\n        for worker_id, items, size in zip(worker_ids_this_node, worker_items, worker_sizes):\n            print(f\"Worker {worker_id} has {len(items)} items ({size})\")\n\n        worker_items = [shuffle(items) for items in worker_items]\n\n        return worker_items\n\n\n    \"\"\"\n    This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution details for workers on the current node and returns a list of items for each worker, with the items shuffled.\n\n    Input-Output Arguments\n    :param num_workers: Int. The number of workers per node.\n    :param user_items: List[Any]. The items to be distributed among the workers.\n    :param weights:"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_items(num_workers, user_items, weights=None, file_size=False):\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        world_size = num_nodes * num_workers\n\n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n        # Print the distribution details for workers on the current node\n        print(f\"Distribution details for workers on node {node_rank}:\")\n        for worker_id in worker_ids_this_node:\n            worker_items_this_node = worker_items[worker_id]\n            worker_weights_this_node = worker_weights[worker_id]\n            total_weight = sum(worker_weights_this_node)\n            if file_size:\n                total_size = sum([item.size for item in worker_items_this_node]) / 1e6\n                print(f\"Worker {worker_id}: {len(worker_items_this_node)} items, {total_size:.2f} MB\")\n            else:\n                print(f\"Worker {worker_id}: {len(worker_items_this_node)} items, {total_weight} weight\")\n\n        # Shuffle the items for each worker and return the result\n        worker_shuffled_items = []\n        for worker_id in worker_ids_this_node:\n            worker_items_this_node = worker_items[worker_id]\n            worker_weights_this_node = worker_weights[worker_id]\n            worker_shuffled_items.append(_shuffle_items(worker_items_this_node, worker_weights_this_node))\n        return worker_shuffled_items\n\n    \"\"\"\n    This function shuffles a list of items based on their weights. It first calculates the"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_items(num_workers, user_items, weights=None, file_size=False):\n        # Calculate the total number of workers across all nodes\n        num_nodes = _get_num_nodes()\n        world_size = num_nodes * num_workers\n\n        # Distribute items to workers based on weights\n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n\n        # Get the rank of the current node\n        node_rank = _get_node_rank()\n\n        # Get the worker IDs on the current node\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n        # Print distribution details for workers on the current node\n        if file_size:\n            print(f\"Worker IDs on node {node_rank}: {worker_ids_this_node}\")\n            for worker_id, worker_items, worker_weights in zip(worker_ids_this_node, worker_items, worker_weights):\n                print(f\"Worker {worker_id}: {len(worker_items)} items ({sum(worker_weights) / 1e6:.2f} MB)\")\n        else:\n            print(f\"Worker IDs on node {node_rank}: {worker_ids_this_node}\")\n            for worker_id, worker_items, worker_weights in zip(worker_ids_this_node, worker_items, worker_weights):\n                print(f\"Worker {worker_id}: {len(worker_items)} items ({sum(worker_weights):.2f} weight)\")\n\n        # Shuffle items for each worker and return the result\n        worker_items_shuffled = [np.random.permutation(worker_items).tolist() for worker_items in worker_items]\n        return worker_items_shuffled\n\n    \"\"\"\n    This function distributes a list of items among workers in a weighted manner, optionally"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_items(num_workers, user_items, weights=None, file_size=False):\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        world_size = num_nodes * num_workers\n\n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n        # Print distribution details for workers on the current node\n        print(f\"Node {node_rank} of {num_nodes} nodes\")\n        print(f\"Worker distribution:\")\n        for worker_id in worker_ids_this_node:\n            items = worker_items[worker_id]\n            weight = sum(worker_weights[worker_id])\n            if file_size:\n                weight = sum(item.size for item in items) / 1e6  # Convert to megabytes\n            print(f\"Worker {worker_id} ({len(items)} items, {weight:.2f} MB): {items}\")\n\n        # Shuffle items for each worker\n        worker_items_shuffled = []\n        for worker_id in worker_ids_this_node:\n            items = worker_items[worker_id]\n            np.random.shuffle(items)\n            worker_items_shuffled.append(items)\n\n        return worker_items_shuffled\n\n\n    \"\"\"\n    This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution details for workers on the current node and returns a list of items for each worker, with the items shuffled.\n\n    Input-Output Arguments\n    :param num_workers: Int. The number of workers per node.\n    :param user_items: List[Any]. The items to"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_items_to_workers(num_workers, user_items, weights=None, file_size=False):\n        world_size = num_workers * num_nodes\n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n        # Print distribution details for workers on this node\n        print(f\"Worker distribution details for node {node_rank}:\")\n        for worker_id in worker_ids_this_node:\n            worker_items_this_node = worker_items[worker_id]\n            worker_weights_this_node = worker_weights[worker_id]\n            if file_size:\n                print(f\"Worker {worker_id}: {len(worker_items_this_node)} items, {worker_weights_this_node / 1e6:.2f} MB\")\n            else:\n                print(f\"Worker {worker_id}: {len(worker_items_this_node)} items, {worker_weights_this_node} weight\")\n\n        # Shuffle items for each worker and return\n        worker_items_shuffled = []\n        for worker_id in worker_ids_this_node:\n            worker_items_this_node = worker_items[worker_id]\n            worker_items_shuffled.append(random.sample(worker_items_this_node, len(worker_items_this_node)))\n        return worker_items_shuffled\n\n    \"\"\"\n    This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution details for workers on the current node and returns a list of items for each worker, with the items shuffled.\n\n    Input-Output Arguments\n    :param num_workers: Int. The number of"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_items_among_workers(num_workers, user_items, weights=None, file_size=False):\n    # Calculate the total number of workers across all nodes\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    # Pack the items into bins based on their weights\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n\n    # Get the worker IDs on the current node\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Print the distribution details for workers on the current node\n    _print_distribution_details(worker_items, worker_weights, worker_ids_this_node, file_size)\n\n    # Shuffle the items for each worker\n    worker_items_shuffled = [np.random.permutation(worker_items[worker_id]) for worker_id in worker_ids_this_node]\n\n    return worker_items_shuffled\n\n\n\n    def _print_distribution_details(worker_items, worker_weights, worker_ids_this_node, file_size):\n    # Print the distribution details for workers on the current node\n    print(f\"Distribution details for workers on node {node_rank}:\")\n    for worker_id in worker_ids_this_node:\n        print(f\"Worker {worker_id}:\")\n        if file_size:\n            print(f\"  Total file size: {sum(worker_weights[worker_id]) / 1024 / 1024:.2f} MB\")\n        else:\n            print(f\"  Total weight: {sum(worker_weights[worker_id])}\")\n        print(f\"  Number of items: {len(worker_items[worker_id])}\")\n        print(f\"  Items: {worker_items["}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Print distribution details\n    if file_size:\n        for worker_id, worker_items in zip(worker_ids_this_node, worker_items):\n            worker_sizes = [item.size for item in worker_items]\n            print(f\"Worker {worker_id}: {sum(worker_sizes) / 1024 / 1024:.2f} MB\")\n    else:\n        for worker_id, worker_items in zip(worker_ids_this_node, worker_items):\n            worker_weights = [item.weight for item in worker_items]\n            print(f\"Worker {worker_id}: {sum(worker_weights)}\")\n\n    # Shuffle items for each worker\n    for worker_items in worker_items:\n        random.shuffle(worker_items)\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _pack_greedily(items, weights, num_bins):\n        \"\"\"\n        This function packs items into bins based on their weights, using a greedy algorithm.\n\n        Input-Output Arguments\n        :param items: List[Any]. The items to be packed into bins.\n        :param weights: List[int]. The weights for each item, used for bin packing.\n        :param num_bins: Int. The number of bins to pack items into.\n        :return: Tuple[List[Any], List[int]]. A tuple containing two lists. The first list contains the items packed into bins, and the second list contains the weights of these items.\n        \"\"\"\n        bins = [[] for _ in range(num_bins)]\n        bin_weights = [0 for _ in range(num_bins)]\n\n        # Sort items by weight in descending order\n        items_with_weights = [(item, weight) for item, weight in zip(items, weights)]\n        items_with_weights.sort(key=lambda x: x[1], reverse=True)\n\n        # Pack items into bins\n        for item, weight in items_with_weights:\n            # Find the bin with the minimum weight\n            min_bin_index = min(range(num_bins), key=lambda i: bin_weights[i])\n            bins[min_bin_index].append(item)\n            bin_weights[min_bin_index] += weight\n\n        return bins, bin_weights\n\n    def _get_num_nodes():\n        \"\"\"\n        This function returns the number of nodes in the current MPI environment.\n\n        :return: Int. The number of nodes in the current MPI environment.\n        \"\"\"\n        return int(os.environ.get(\"SLURM_NNODES\", 1))\n\n    def _get_node_rank():\n        \"\"\"\n        This function returns the rank of the current node in the MPI environment.\n\n        :return: Int. The rank of the current node in the MPI environment.\n        \"\"\"\n        return int(os.environ.get(\"SLURM_NODEID"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _pack_greedily(items, weights, num_bins):\n        # Initialize the bins to hold the items\n        bins = [[] for _ in range(num_bins)]\n\n        # Sort the items based on their weights\n        sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n        # Greedily distribute the items to the bins\n        for item, weight in sorted_items:\n            # Find the bin with the least weight\n            min_weight_bin = min(bins, key=lambda x: sum(x[1] for x in x))\n\n            # Add the item to the bin with the least weight\n            min_weight_bin[0].append(item)\n            min_weight_bin[1].append(weight)\n\n        # Return the bins with the items and their weights\n        return bins\n\n\n    def _get_num_nodes():\n        return int(os.environ.get(\"SLURM_NNODES\", 1))\n\n    def _get_node_rank():\n        return int(os.environ.get(\"SLURM_NODEID\", 0))\n\n    def _get_rank_in_node():\n        return int(os.environ.get(\"SLURM_LOCALID\", 0))\n\n    def _get_local_rank():\n        return int(os.environ.get(\"SLURM_PROCID\", 0))\n\n    def _get_world_size():\n        return int(os.environ.get(\"SLURM_NTASKS\", 1))\n\n    def _get_node_name():\n        return os.environ.get(\"SLURMD_NODENAME\", \"localhost\")\n\n    def _get_node_ip():\n        return os.environ.get(\"SLURM_NODE_IP_ADDRESS\", \"127.0.0.1\")\n\n    def _get_node_hostname():\n        return os.environ.get(\"SLURM_NODE_HOSTNAME\", \"localhost\")\n\n    def _get_node_addr"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_items_among_workers(num_workers: int, user_items: List[Any], weights: Optional[List[int]] = None, file_size: bool = False) -> List[List[Any]]:\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        world_size = num_nodes * num_workers\n\n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n        # Shuffle items within each worker\n        worker_items = [np.random.permutation(worker_items[i]) for i in worker_ids_this_node]\n\n        # Print distribution details\n        if file_size:\n            _print_dist_details(worker_ids=worker_ids_this_node, worker_items=worker_items, worker_weights=worker_weights, unit='MB')\n        else:\n            _print_dist_details(worker_ids=worker_ids_this_node, worker_items=worker_items, worker_weights=worker_weights, unit='weight')\n\n        return worker_items\n\n    \"\"\"\n    This function packs a list of items into bins using a greedy algorithm. Items are sorted by weight and then distributed to bins in a greedy manner, starting from the bin with the least weight. The function returns a list of items for each bin and a list of weights for each bin.\n\n    Input-Output Arguments\n    :param items: List[Any]. The items to be packed into bins.\n    :param weights: Optional[List[int]]. The weights for each item, used for packing. If not provided, all items are considered to have equal weight.\n    :param num_bins: Int. The number of bins to pack the items into.\n    :return: Tuple[List[List[Any]], List[int]]. A tuple containing two lists:"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    total_workers = num_workers * num_nodes\n    items_per_worker = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    start_indices = np.cumsum([items_per_worker + 1 if i < remainder else items_per_worker for i in range(total_workers)])\n    end_indices = np.cumsum([items_per_worker + 1 if i < remainder else items_per_worker for i in range(total_workers)])\n\n    start_indices = np.roll(start_indices, -node_rank * num_workers)\n    end_indices = np.roll(end_indices, -node_rank * num_workers)\n\n    worker_items = [user_items[start_indices[i]:end_indices[i]] for i in range(total_workers)]\n\n    if len(worker_items) != num_workers:\n        raise RuntimeError(f\"Improper assignment of items to workers. Expected {num_workers} workers, but got {len(worker_items)}.\")\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    total_workers = num_workers * num_nodes\n    items_per_worker = math.ceil(len(user_items) / total_workers)\n    worker_items = [user_items[i:i + items_per_worker] for i in range(0, len(user_items), items_per_worker)]\n\n    if len(worker_items) != num_workers:\n        raise RuntimeError(f\"Improper assignment: {len(worker_items)} workers instead of {num_workers}\")\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of nodes in the environment\n    num_nodes = _get_num_nodes()\n\n    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    items_per_worker = len(user_items) // total_workers\n\n    # Calculate the number of items to assign to the last workers\n    remainder = len(user_items) % total_workers\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.arange(0, total_workers * items_per_worker, items_per_worker)\n    end_indices = np.arange(items_per_worker, total_workers * items_per_worker + remainder, items_per_worker)\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Check if the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    # Return the list of lists\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    total_workers = num_workers * num_nodes\n    items_per_worker = math.ceil(len(user_items) / total_workers)\n    extra_items = len(user_items) % total_workers\n\n    start_indices = np.cumsum([items_per_worker] * total_workers)\n    start_indices[-extra_items:] += extra_items\n    end_indices = np.cumsum(start_indices)\n\n    start_indices = start_indices[node_rank * num_workers : (node_rank + 1) * num_workers]\n    end_indices = end_indices[node_rank * num_workers : (node_rank + 1) * num_workers]\n\n    worker_items = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // num_workers\n    num_items_per_worker_remainder = len(user_items) % num_workers\n\n    # Calculate the start and end indices for each worker's items\n    worker_start_indices = np.cumsum([num_items_per_worker] * num_workers)\n    worker_end_indices = np.cumsum([num_items_per_worker + 1] * num_workers)\n\n    # Adjust for any remainder\n    worker_start_indices[-1] += num_items_per_worker_remainder\n    worker_end_indices[-1] += num_items_per_worker_remainder\n\n    # Calculate the start and end indices for the current node's workers\n    node_start_index = node_rank * num_workers\n    node_end_index = (node_rank + 1) * num_workers\n\n    # Distribute the items among the workers\n    worker_items = []\n    for worker_index in range(node_start_index, node_end_index):\n        start_index = worker_start_indices[worker_index]\n        end_index = worker_end_indices[worker_index]\n        worker_items.append(user_items[start_index:end_index])\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    num_workers_per_node = num_workers // num_nodes\n    num_workers_per_node_remainder = num_workers % num_nodes\n\n    num_workers_per_node_remainder_per_node = num_workers_per_node_remainder // (node_rank + 1)\n    num_workers_per_node_remainder_per_node_remainder = num_workers_per_node_remainder % (node_rank + 1)\n\n    num_workers_per_node = num_workers_per_node + num_workers_per_node_remainder_per_node\n    num_workers_per_node_remainder = num_workers_per_node_remainder_per_node_remainder\n\n    num_items_per_worker = len(user_items) // num_workers\n    num_items_per_worker_remainder = len(user_items) % num_workers\n\n    num_items_per_worker_remainder_per_node = num_items_per_worker_remainder // (node_rank + 1)\n    num_items_per_worker_remainder_per_node_remainder = num_items_per_worker_remainder % (node_rank + 1)\n\n    num_items_per_worker = num_items_per_worker + num_items_per_worker_remainder_per_node\n    num_items_per_worker_remainder = num_items_per_worker_remainder_per_node_remainder\n\n    items_per_worker_start_indices = np.cumsum(\n        [0] + [num_items_per_worker] * (num_workers_per_node - 1) + [num_items_per_worker + num_items_per_worker_remainder]\n    )\n    items_per_worker_"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    num_workers_per_node = num_workers // num_nodes\n    num_workers_remainder = num_workers % num_nodes\n\n    if node_rank < num_workers_remainder:\n        num_workers_per_node += 1\n\n    num_workers_per_node_cumsum = np.cumsum(\n        np.ones(num_workers_per_node, dtype=np.int32)\n    )\n    num_workers_per_node_cumsum = np.pad(\n        num_workers_per_node_cumsum, (0, num_workers - num_workers_per_node_cumsum[-1])\n    )\n    num_workers_per_node_cumsum = num_workers_per_node_cumsum.tolist()\n\n    user_items_per_node = [\n        user_items[num_workers_per_node_cumsum[i] : num_workers_per_node_cumsum[i + 1]]\n        for i in range(num_workers_per_node)\n    ]\n\n    if len(user_items_per_node) != num_workers:\n        raise RuntimeError(\n            f\"Improper assignment of items to workers. Expected {num_workers} workers, but got {len(user_items_per_node)} workers.\"\n        )\n\n    return user_items_per_node"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    num_workers_per_node = num_workers // num_nodes\n    num_workers_per_node_remainder = num_workers % num_nodes\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // num_workers\n    num_items_per_worker_remainder = len(user_items) % num_workers\n\n    # Distribute remainder of items to workers\n    if num_items_per_worker_remainder > 0:\n        num_items_per_worker += 1\n        num_items_per_worker_remainder -= 1\n\n    # Calculate the start and end indices for each worker's items\n    worker_start_indices = np.cumsum([0] + [num_items_per_worker] * (num_workers - 1))\n    worker_end_indices = np.cumsum([num_items_per_worker] * num_workers)\n\n    # Adjust for any remainder\n    if num_items_per_worker_remainder > 0:\n        worker_end_indices[-num_items_per_worker_remainder:] += num_items_per_worker_remainder\n\n    # Calculate the start and end indices for each worker's items in the current node\n    node_worker_start_indices = worker_start_indices[node_rank * num_workers_per_node : (node_rank + 1) * num_workers_per_node]\n    node_worker_end_indices = worker_end_indices[node_rank * num_workers_per_node : (node_rank + 1) * num_workers_per_node]\n\n    # Distribute remainder of items to workers in the current node\n    if num_items_per_worker_remainder > 0:\n        node_worker_end_indices[-num_items_per_worker_remainder:] +="}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    num_workers_per_node = num_workers // num_nodes\n    num_workers_per_node_remainder = num_workers % num_nodes\n    num_workers_per_node_remainder_offset = num_workers_per_node_remainder * node_rank\n    num_workers_per_node_remainder_offset_end = num_workers_per_node_remainder_offset + num_workers_per_node_remainder\n\n    num_items_per_node_worker = len(user_items) // num_workers\n    num_items_per_node_worker_remainder = len(user_items) % num_workers\n    num_items_per_node_worker_remainder_offset = num_items_per_node_worker * node_rank\n    num_items_per_node_worker_remainder_offset_end = num_items_per_node_worker_remainder_offset + num_items_per_node_worker_remainder\n\n    num_workers_per_node_remainder_offset_end_cumsum = np.cumsum(\n        [num_workers_per_node_remainder_offset, num_workers_per_node_remainder]\n    )\n    num_items_per_node_worker_remainder_offset_end_cumsum = np.cumsum(\n        [num_items_per_node_worker_remainder_offset, num_items_per_node_worker_remainder]\n    )\n\n    worker_items = []\n    for worker_id in range(num_workers):\n        worker_items.append(\n            user_items[\n                num_items_per_node_worker_remainder_offset_end_cumsum[worker_id]\n                : num_items_per_node_worker_remainder_offset_end_cumsum[worker_"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * _get_num_nodes()\n\n    # Calculate the number of items each worker should process\n    num_items = len(user_items)\n    items_per_worker = (num_items + total_workers - 1) // total_workers\n\n    # Adjust for any remainder by adding extra items to the workers starting from the end of the list\n    remainder = num_items % total_workers\n    items_per_worker += (remainder + _get_node_rank()) // total_workers\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = list(range(0, num_items, items_per_worker))\n    end_indices = start_indices[1:] + [num_items]\n\n    # Distribute the items to the workers\n    worker_items = []\n    for start, end in zip(start_indices, end_indices):\n        worker_items.append(user_items[start:end])\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of nodes in the environment\n    num_nodes = _get_num_nodes()\n\n    # Get the rank of the current node within the environment\n    node_rank = _get_node_rank()\n\n    # Calculate the total number of workers across all nodes\n    num_workers_total = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = math.ceil(len(user_items) / num_workers_total)\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = list(range(0, len(user_items), num_items_per_worker))\n    end_indices = start_indices[1:] + [len(user_items)]\n\n    # Adjust for any remainder by adding extra items to the workers starting from the end of the list\n    extra_items = len(user_items) % num_workers_total\n    if extra_items > 0:\n        start_indices[-extra_items:] = [\n            start + (len(user_items) - start) // extra_items\n            for start in start_indices[-extra_items:]\n        ]\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [\n        user_items[start:end] for start, end in zip(start_indices, end_indices)\n    ]\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\n            f\"Improper assignment. Expected {num_workers} workers, got {len(worker_items)}.\"\n        )\n\n    # Return the list of lists\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of nodes in the distributed environment\n    num_nodes = _get_num_nodes()\n\n    # Get the rank of the current node\n    node_rank = _get_node_rank()\n\n    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_workers\n\n    # Calculate the number of extra items to distribute among the workers\n    num_extra_items = len(user_items) % total_workers\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = [i * num_items_per_worker for i in range(total_workers)]\n    end_indices = [start_indices[i] + num_items_per_worker for i in range(total_workers)]\n\n    # Distribute the extra items to the last workers\n    for i in range(num_extra_items):\n        end_indices[-i - 1] += 1\n\n    # Calculate the start and end indices for the current node's workers\n    start_index = start_indices[node_rank]\n    end_index = end_indices[node_rank]\n\n    # Slice the list of items to get the items assigned to the current node's workers\n    worker_items = user_items[start_index:end_index]\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of nodes in the environment\n    num_nodes = _get_num_nodes()\n\n    # Get the rank of the current node\n    node_rank = _get_node_rank()\n\n    # Calculate the total number of workers across all nodes\n    total_num_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_num_workers\n\n    # Calculate the number of extra items that need to be distributed among the workers\n    num_extra_items = len(user_items) % total_num_workers\n\n    # Calculate the start and end indices for each worker's items\n    worker_start_indices = np.arange(0, len(user_items), num_items_per_worker + int(num_extra_items > 0))\n    worker_end_indices = np.concatenate((worker_start_indices[1:], [len(user_items)]))\n\n    # Adjust the start indices for the last workers to account for the extra items\n    worker_start_indices[-num_extra_items:] += num_extra_items - 1\n\n    # Calculate the start and end indices for the current node's workers\n    node_start_index = node_rank * num_workers\n    node_end_index = node_start_index + num_workers\n\n    # Get the indices of the current node's workers\n    node_worker_indices = np.arange(node_start_index, node_end_index)\n\n    # Assign the items to the current node's workers\n    worker_items = [user_items[start:end] for start, end in zip(worker_start_indices[node_worker_indices], worker_end_indices[node_worker_indices])]\n\n    # Check if the output list has the correct length\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of nodes in the environment\n    num_nodes = _get_num_nodes()\n\n    # Get the rank of the current node\n    node_rank = _get_node_rank()\n\n    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_workers\n\n    # Calculate the number of items that will be assigned to the last workers\n    num_remainder_items = len(user_items) % total_workers\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = [\n        num_items_per_worker * (node_rank * num_workers + worker_rank)\n        for worker_rank in range(num_workers)\n    ]\n    end_indices = [\n        start_indices[worker_rank] + num_items_per_worker\n        for worker_rank in range(num_workers)\n    ]\n\n    # Adjust the start and end indices for the last workers\n    start_indices[-1] += num_remainder_items\n    end_indices[-1] += num_remainder_items\n\n    # Create a list of lists to store the items assigned to each worker\n    items_per_worker = [\n        user_items[start_indices[worker_rank] : end_indices[worker_rank]]\n        for worker_rank in range(num_workers)\n    ]\n\n    # Check if the output list has the same length as the number of workers\n    if len(items_per_worker) != num_workers:\n        raise RuntimeError(\n            f\"Incorrect assignment. Expected {num_workers} workers, but got {len(items_per_worker)} workers.\"\n        )\n\n    # Return the list of lists containing the items assigned to each worker\n    return items_per_worker"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = _get_num_nodes() * num_workers\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_workers\n\n    # Calculate the number of items to be assigned to the last worker\n    remainder = len(user_items) % total_workers\n\n    # Calculate the start and end indices for each worker's items\n    worker_indices = np.arange(total_workers) * num_items_per_worker\n    worker_indices = np.append(worker_indices, worker_indices[-1] + remainder)\n    worker_indices = np.cumsum(worker_indices)\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [user_items[worker_indices[i]:worker_indices[i + 1]] for i in range(total_workers)]\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != total_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return worker_items\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // num_workers\n    num_items_per_worker_with_remainder = len(user_items) % num_workers\n\n    # Distribute the remainder of items to the last workers\n    if num_items_per_worker_with_remainder > 0:\n        num_items_per_worker += 1\n        num_items_per_worker_with_remainder -= 1\n\n    # Calculate the start and end indices for each worker's items\n    worker_start_indices = np.cumsum([0] + [num_items_per_worker] * (num_workers - 1))\n    worker_end_indices = np.cumsum([num_items_per_worker] * num_workers)\n\n    # Adjust the end indices for the last workers with extra items\n    worker_end_indices[-num_items_per_worker_with_remainder:] = worker_start_indices[-num_items_per_worker_with_remainder:] + num_items_per_worker_with_remainder\n\n    # Calculate the start and end indices for the current node's workers\n    node_start_index = node_rank * num_workers\n    node_end_index = node_start_index + num_workers\n\n    # Slice the user_items list for the current node's workers\n    node_items = user_items[node_start_index:node_end_index]\n\n    # Map the user_items list to the current node's workers\n    mapped_items = [node_items[worker_start_indices[i]:worker_end_indices[i]] for i in range(num_workers)]\n\n    # Check if the output list has the correct length\n    if len(mapped_items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    num_nodes = _get_num_nodes()\n    total_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    # Distribute the remainder items to the last workers\n    for i in range(remainder):\n        user_items[num_items_per_worker * (total_workers - i - 1) + i] += 1\n\n    # Calculate the start and end indices for each worker's items\n    worker_start_indices = np.cumsum([0] + [num_items_per_worker] * (total_workers - 1))\n    worker_end_indices = np.cumsum([num_items_per_worker] * total_workers)\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [user_items[start:end] for start, end in zip(worker_start_indices, worker_end_indices)]\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != total_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    num_nodes = _get_num_nodes()\n    total_workers = num_workers * num_nodes\n\n    # Determine how many items each worker should process\n    items_per_worker = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    # Adjust for any remainder by adding extra items to the workers starting from the end of the list\n    adjusted_items_per_worker = items_per_worker + (1 if remainder > 0 else 0)\n    adjusted_remainder = remainder - (1 if remainder > 0 else 0)\n    adjusted_items_per_worker_list = [adjusted_items_per_worker] * total_workers\n    adjusted_items_per_worker_list[-1] += adjusted_remainder\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.cumsum([0] + adjusted_items_per_worker_list[:-1])\n    end_indices = np.cumsum(adjusted_items_per_worker_list)\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(f\"Improper assignment: {len(worker_items)} != {num_workers}\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    num_nodes = _get_num_nodes()\n    total_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    items_per_worker = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    # Distribute the remainder items to the last workers\n    for i in range(total_workers - remainder, total_workers):\n        items_per_worker += 1\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.cumsum([0] + [items_per_worker] * (total_workers - 1))\n    end_indices = np.cumsum([items_per_worker] * total_workers)\n\n    # Split the user items into worker-specific lists\n    worker_items = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != total_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of nodes in the environment\n    num_nodes = _get_num_nodes()\n\n    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    items_per_worker = math.ceil(len(user_items) / total_workers)\n\n    # Calculate the number of extra items to distribute to the last workers\n    extra_items = len(user_items) % total_workers\n\n    # Calculate the start and end indices for each worker's items\n    worker_starts = list(np.cumsum([0] + [items_per_worker] * (total_workers - 1)))\n    worker_ends = list(np.cumsum([items_per_worker] * (total_workers - 1) + [extra_items]))\n\n    # Adjust the end indices for the last workers\n    worker_ends[-extra_items:] = [len(user_items)] * extra_items\n\n    # Get the rank of the current node\n    node_rank = _get_node_rank()\n\n    # Calculate the start and end indices for the current node's workers\n    node_start = node_rank * num_workers\n    node_end = min((node_rank + 1) * num_workers, total_workers)\n\n    # Distribute the items among the workers\n    worker_items = [user_items[start:end] for start, end in zip(worker_starts[node_start:node_end], worker_ends[node_start:node_end])]\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(f\"Improper assignment: {len(worker_items)} workers instead of {num_workers}.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        if os.path.isdir(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n        os.makedirs(self.cache_dir)\n\n        if os.path.isdir(self.cache_dir + \"/\" + \"raw\"):\n            shutil.rmtree(self.cache_dir + \"/\" + \"raw\")\n        os.makedirs(self.cache_dir + \"/\" + \"raw\")\n\n        if os.path.isdir(self.cache_dir + \"/\" + \"processed\"):\n            shutil.rmtree(self.cache_dir + \"/\" + \"processed\")\n        os.makedirs(self.cache_dir + \"/\" + \"processed\")\n\n        if os.path.isdir(self.cache_dir + \"/\" + \"processed\" + \"/\" + \"train\"):\n            shutil.rmtree(self.cache_dir + \"/\" + \"processed\" + \"/\" + \"train\")\n        os.makedirs(self.cache_dir + \"/\" + \"processed\" + \"/\" + \"train\")\n\n        if os.path.isdir(self.cache_dir + \"/\" + \"processed\" + \"/\" + \"test\"):\n            shutil.rmtree(self.cache_dir + \"/\" + \"processed\" + \"/\" + \"test\")\n        os.makedirs(self.cache_dir + \"/\" + \"processed\" + \"/\" + \"test\")\n\n        if os.path.isdir(self.cache_dir + \"/\" + \"processed\" + \"/\" + \"val\"):\n            shutil.rmtree(self.cache_dir + \"/\" + \"processed\" + \"/\" + \"val\")\n        os.makedirs(self.cache_dir + \"/\" + \"processed\" + \"/\" + \"val\")\n\n        if os.path.isdir(self.cache_dir + \"/\" + \"processed\" + \"/\" + \"val\" + \"/\" + \"images\"):\n            shutil.rmtree(self.cache_dir + \"/\" + \"processed\""}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        if os.path.exists(self.cache_directory):\n            shutil.rmtree(self.cache_directory)\n        os.makedirs(self.cache_directory)\n\n        if os.path.exists(self.cache_directory + \"/\" + self.data_directory):\n            shutil.rmtree(self.cache_directory + \"/\" + self.data_directory)\n        os.makedirs(self.cache_directory + \"/\" + self.data_directory)\n\n        if os.path.exists(self.cache_directory + \"/\" + self.data_directory + \"/\" + self.raw_directory):\n            shutil.rmtree(self.cache_directory + \"/\" + self.data_directory + \"/\" + self.raw_directory)\n        os.makedirs(self.cache_directory + \"/\" + self.data_directory + \"/\" + self.raw_directory)\n\n        if os.path.exists(self.cache_directory + \"/\" + self.data_directory + \"/\" + self.processed_directory):\n            shutil.rmtree(self.cache_directory + \"/\" + self.data_directory + \"/\" + self.processed_directory)\n        os.makedirs(self.cache_directory + \"/\" + self.data_directory + \"/\" + self.processed_directory)\n\n        if os.path.exists(self.cache_directory + \"/\" + self.data_directory + \"/\" + self.processed_directory + \"/\" + self.training_directory):\n            shutil.rmtree(self.cache_directory + \"/\" + self.data_directory + \"/\" + self.processed_directory + \"/\" + self.training_directory)\n        os.makedirs(self.cache_directory + \"/\" + self.data_directory + \"/\" + self.processed_directory + \"/\" + self.training_directory)\n\n        if os.path.exists(self.cache_directory + \"/\" + self.data_directory + \"/\" + self.processed_directory + \"/\" + self.validation_directory):\n            shutil.rmtree(self.cache_directory +"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Delete the cache directories if they exist\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n\n        # Create the cache directories\n        os.makedirs(self.cache_dir)\n        os.makedirs(os.path.join(self.cache_dir, 'images'))\n        os.makedirs(os.path.join(self.cache_dir, 'labels'))\n\n        # Delete the cache directories if they exist\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n\n        # Create the cache directories\n        os.makedirs(self.cache_dir)\n        os.makedirs(os.path.join(self.cache_dir, 'images'))\n        os.makedirs(os.path.join(self.cache_dir, 'labels'))\n\n        # Delete the cache directories if they exist\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n\n        # Create the cache directories\n        os.makedirs(self.cache_dir)\n        os.makedirs(os.path.join(self.cache_dir, 'images'))\n        os.makedirs(os.path.join(self.cache_dir, 'labels'))\n\n        # Delete the cache directories if they exist\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n\n        # Create the cache directories\n        os.makedirs(self.cache_dir)\n        os.makedirs(os.path.join(self.cache_dir, 'images'))\n        os.makedirs(os.path.join(self.cache_dir, 'labels'))\n\n        # Delete the cache directories if they exist\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n\n        # Create the cache directories\n        os.makedirs(self.cache_dir)\n        os.maked"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Remove cache directories if they exist\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n        if os.path.exists(self.cache_dir_2):\n            shutil.rmtree(self.cache_dir_2)\n        if os.path.exists(self.cache_dir_3):\n            shutil.rmtree(self.cache_dir_3)\n\n        # Create cache directories\n        os.makedirs(self.cache_dir, exist_ok=True)\n        os.makedirs(self.cache_dir_2, exist_ok=True)\n        os.makedirs(self.cache_dir_3, exist_ok=True)\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Delete the cache directory if it exists\n        if os.path.exists(self.cache_directory):\n            shutil.rmtree(self.cache_directory)\n\n        # Create the cache directory\n        os.makedirs(self.cache_directory)\n\n        # Delete the cache directory if it exists\n        if os.path.exists(self.output_directory):\n            shutil.rmtree(self.output_directory)\n\n        # Create the cache directory\n        os.makedirs(self.output_directory)\n\n        # Delete the cache directory if it exists\n        if os.path.exists(self.temp_directory):\n            shutil.rmtree(self.temp_directory)\n\n        # Create the cache directory\n        os.makedirs(self.temp_directory)\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Check if the cache directories exist and remove them if they do\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n\n        if os.path.exists(self.cache_dir_2):\n            shutil.rmtree(self.cache_dir_2)\n\n        # Recreate the cache directories\n        os.makedirs(self.cache_dir)\n        os.makedirs(self.cache_dir_2)\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n        os.makedirs(self.cache_dir)\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Remove the cache directories if they exist\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n\n        # Create the cache directories\n        os.makedirs(self.cache_dir, exist_ok=True)\n        os.makedirs(self.cache_dir + \"/\" + self.cache_dir_name, exist_ok=True)\n        os.makedirs(self.cache_dir + \"/\" + self.cache_dir_name + \"/\" + self.cache_dir_name_2, exist_ok=True)\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Remove cache directories\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n\n        # Create cache directories\n        os.makedirs(self.cache_dir)\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Check if the cache directories exist and delete them if they do\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n\n        # Create the cache directories\n        os.makedirs(self.cache_dir)\n        os.makedirs(self.cache_dir + '/' + self.data_dir)\n        os.makedirs(self.cache_dir + '/' + self.model_dir)\n        os.makedirs(self.cache_dir + '/' + self.model_dir + '/' + self.model_name)\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        if os.path.isdir(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n        os.makedirs(self.cache_dir)\n        if os.path.isdir(self.cache_dir + '/' + self.cache_dir_name):\n            shutil.rmtree(self.cache_dir + '/' + self.cache_dir_name)\n        os.makedirs(self.cache_dir + '/' + self.cache_dir_name)\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # remove cache directories if they exist\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n\n        # create cache directories\n        os.makedirs(self.cache_dir)\n        os.makedirs(os.path.join(self.cache_dir, \"images\"))\n        os.makedirs(os.path.join(self.cache_dir, \"json\"))\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # remove cache directories if they exist\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n\n        # create cache directories\n        os.makedirs(self.cache_dir)\n        os.makedirs(self.cache_dir + '/' + 'raw')\n        os.makedirs(self.cache_dir + '/' + 'clean')\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Get the paths to the cache directories\n        cache_dir_path = self.cache_dir_path\n        cache_dir_path_images = self.cache_dir_path_images\n        cache_dir_path_labels = self.cache_dir_path_labels\n\n        # Check if the cache directories exist\n        if os.path.exists(cache_dir_path):\n            # Remove the cache directories if they exist\n            shutil.rmtree(cache_dir_path)\n\n        # Create the cache directories\n        os.makedirs(cache_dir_path)\n        os.makedirs(cache_dir_path_images)\n        os.makedirs(cache_dir_path_labels)\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n        os.makedirs(self.cache_dir)\n\n        if os.path.exists(self.cache_dir_processed):\n            shutil.rmtree(self.cache_dir_processed)\n        os.makedirs(self.cache_dir_processed)\n\n        if os.path.exists(self.cache_dir_model):\n            shutil.rmtree(self.cache_dir_model)\n        os.makedirs(self.cache_dir_model)\n\n        if os.path.exists(self.cache_dir_results):\n            shutil.rmtree(self.cache_dir_results)\n        os.makedirs(self.cache_dir_results)\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Remove cache directories if they exist\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n        if os.path.exists(self.cache_dir_temp):\n            shutil.rmtree(self.cache_dir_temp)\n\n        # Create cache directories\n        os.makedirs(self.cache_dir)\n        os.makedirs(self.cache_dir_temp)\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        if os.path.exists(self.data_cache_dir):\n            shutil.rmtree(self.data_cache_dir)\n\n        if os.path.exists(self.data_cache_dir):\n            shutil.rmtree(self.data_cache_dir)\n\n        os.makedirs(self.data_cache_dir)\n        os.makedirs(self.data_cache_dir + '/raw')\n        os.makedirs(self.data_cache_dir + '/processed')\n        os.makedirs(self.data_cache_dir + '/processed/images')\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Remove the cache directories if they exist\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n        if os.path.exists(self.cache_dir_raw):\n            shutil.rmtree(self.cache_dir_raw)\n        if os.path.exists(self.cache_dir_processed):\n            shutil.rmtree(self.cache_dir_processed)\n\n        # Create the cache directories\n        os.makedirs(self.cache_dir)\n        os.makedirs(self.cache_dir_raw)\n        os.makedirs(self.cache_dir_processed)\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Remove cache directories\n        if os.path.isdir(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n        if os.path.isdir(self.cache_dir_2):\n            shutil.rmtree(self.cache_dir_2)\n\n        # Create cache directories\n        os.makedirs(self.cache_dir)\n        os.makedirs(self.cache_dir_2)\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n        os.makedirs(self.cache_dir)\n\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n        os.makedirs(self.cache_dir)\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    def get_num_bytes_parallel(items: List[Any], base_path: str = \"\") -> List[int]:\n        with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n            futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n            return [future.result() for future in futures]\n\n    \"\"\"\n    Computes and returns a list of file sizes for each item in the given list by parallelizing the file size retrieval process to improve performance. It uses a ProcessPoolExecutor to execute the file size retrieval in parallel, adjusting the number of workers based on the CPU count.\n\n    Input-Output Arguments\n    :param items: List[Any]. A list of items for which to compute file sizes. The nature of these items is not specified, but they are used to determine file sizes in some way.\n    :param base_path: str, optional. A base path to be prepended to each item's path before computing its file size. Defaults to an empty string, indicating no base path is used.\n    :return: List[int]. A list of file sizes corresponding to each item in the input list. Each size is an integer representing the number of bytes.\n    \"\"\"\n    def get_num_bytes_parallel_process(items: List[Any], base_path: str = \"\") -> List[int]:\n        with ProcessPoolExecutor(max_workers=cpu_count()) as executor:\n            futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n            return [future.result() for future in futures]\n\n    \"\"\"\n    Computes and returns a list of file sizes for each item in the given list by parallelizing the file size retrieval process to improve performance. It uses a ProcessPoolExecutor to execute the file size retrieval in parallel, adjusting the number of workers based on the CPU count.\n\n    Input-Output Arguments\n    :param items: List[Any]. A list of items for which to compute file sizes. The nature of these items is not specified, but they are used to determine file sizes in some way.\n    :"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    def _get_num_bytes(item, base_path):\n        path = item if isinstance(item, str) else item.path\n        return os.path.getsize(os.path.join(base_path, path))\n\n    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n        return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    def _get_num_bytes(item, base_path):\n        \"\"\"\n        Computes and returns the file size of the given item, prepending the base path if provided.\n\n        Input-Output Arguments\n        :param item: Any. The item for which to compute the file size. The nature of this item is not specified, but it is used to determine the file size in some way.\n        :param base_path: str, optional. A base path to be prepended to the item's path before computing its file size. Defaults to an empty string, indicating no base path is used.\n        :return: int. The file size of the item, represented as an integer representing the number of bytes.\n        \"\"\"\n        item_path = item if isinstance(item, str) else item.path\n        path = base_path + item_path\n        return os.path.getsize(path)\n\n    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    def _get_num_bytes(item, base_path):\n        \"\"\"\n        Helper function that computes the file size of a single item.\n\n        Input-Output Arguments\n        :param item: Any. The item for which to compute the file size. The nature of this item is not specified, but it is used to determine its file size in some way.\n        :param base_path: str. A base path to be prepended to the item's path before computing its file size.\n        :return: int. The file size of the item, represented as an integer number of bytes.\n        \"\"\"\n        path = item if isinstance(item, str) else os.path.join(base_path, item)\n        return os.path.getsize(path)\n\n    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        return False\n\n    if input_dir is not None and not element.startswith(input_dir):\n        return False\n\n    return Path(element).exists()\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        input_dir = str(Path(input_dir).absolute())\n        if not element.startswith(input_dir):\n            return False\n\n    element = str(Path(element).absolute())\n    if not os.path.exists(element):\n        return False\n\n    return True\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        return False\n\n    element = str(Path(element).absolute())\n\n    if input_dir is not None:\n        input_dir = str(Path(input_dir).absolute())\n        if not element.startswith(input_dir):\n            return False\n\n    return Path(element).exists()\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        element = str(Path(element).absolute())\n        if input_dir is not None:\n            input_dir = str(Path(input_dir).absolute())\n            if not element.startswith(input_dir):\n                return False\n        return Path(element).exists()\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        element = str(Path(element).absolute())\n        if input_dir is not None:\n            input_dir = str(Path(input_dir).absolute())\n            return element.startswith(input_dir)\n        else:\n            return os.path.exists(element)\n    else:\n        return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        element = str(element)\n\n    if not os.path.isabs(element):\n        element = os.path.join(input_dir, element)\n\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        if not isinstance(input_dir, str):\n            input_dir = str(Path(input_dir).absolute())\n        if not element.startswith(input_dir):\n            return False\n\n    if not isinstance(element, str):\n        element = str(Path(element).absolute())\n\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        element = str(element)\n\n    if not input_dir:\n        return Path(element).exists()\n\n    if not isinstance(input_dir, str):\n        input_dir = str(input_dir)\n\n    return Path(element).exists() or Path(element).absolute().as_posix().startswith(input_dir)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        input_dir = str(Path(input_dir).absolute())\n\n    if isinstance(element, str):\n        if input_dir is not None:\n            element = str(Path(element).absolute())\n            return element.startswith(input_dir)\n        else:\n            return Path(element).exists()\n    else:\n        return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is None:\n        return os.path.exists(element)\n    else:\n        return os.path.exists(element) or os.path.commonprefix([input_dir, element]) == input_dir\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        element = str(Path(element).absolute())\n        if input_dir is not None:\n            if element.startswith(input_dir):\n                return True\n            else:\n                return False\n        else:\n            return Path(element).exists()\n    else:\n        return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        element = str(element)\n\n    if element == '':\n        return False\n\n    if input_dir is not None:\n        if not element.startswith(input_dir):\n            return False\n\n    if os.path.exists(element):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        return False\n\n    if input_dir is not None:\n        input_dir = str(Path(input_dir).absolute())\n        if not element.startswith(input_dir):\n            return False\n\n    element = str(Path(element).absolute())\n\n    return Path(element).exists()\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        if not isinstance(input_dir, str):\n            raise TypeError(\"input_dir must be a string\")\n        input_dir = Path(input_dir)\n\n    if not isinstance(element, str):\n        raise TypeError(\"element must be a string\")\n    element = Path(element)\n\n    if input_dir is not None:\n        if not element.is_absolute():\n            element = input_dir / element\n\n        if not element.is_absolute():\n            raise ValueError(f\"element must be an absolute path or start with input_dir\")\n\n        if not element.exists():\n            return False\n\n        if element.is_dir():\n            return True\n\n        if element.is_file():\n            return True\n\n    if element.exists():\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir:\n        if not isinstance(input_dir, str):\n            raise TypeError(f\"input_dir must be a string, but got {type(input_dir)}.\")\n        input_dir = str(Path(input_dir).absolute())\n\n    if not isinstance(element, str):\n        raise TypeError(f\"element must be a string, but got {type(element)}.\")\n\n    if not element:\n        raise ValueError(\"element must not be empty.\")\n\n    element = str(Path(element).absolute())\n\n    if input_dir:\n        return element.startswith(input_dir)\n    else:\n        return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        if not isinstance(input_dir, str):\n            raise TypeError(\"input_dir must be a string\")\n        if not os.path.isdir(input_dir):\n            raise ValueError(\"input_dir must be a valid directory\")\n        input_dir = os.path.abspath(input_dir)\n\n    if not isinstance(element, str):\n        raise TypeError(\"element must be a string\")\n\n    if os.path.isabs(element):\n        element = os.path.abspath(element)\n    else:\n        if input_dir is not None:\n            element = os.path.abspath(os.path.join(input_dir, element))\n\n    if os.path.exists(element):\n        return True\n\n    if input_dir is not None:\n        if element.startswith(input_dir):\n            return True\n\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is None:\n        return os.path.exists(element)\n    else:\n        input_dir = str(Path(input_dir).absolute())\n        if os.path.exists(element):\n            return True\n        elif element.startswith(input_dir):\n            return True\n        else:\n            return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        if not isinstance(input_dir, str):\n            raise TypeError(f\"input_dir must be a string, not {type(input_dir)}\")\n        if not os.path.isdir(input_dir):\n            raise ValueError(f\"input_dir must be a valid directory, not {input_dir}\")\n        if not isinstance(element, str):\n            raise TypeError(f\"element must be a string, not {type(element)}\")\n        if not os.path.isabs(element):\n            element = os.path.join(input_dir, element)\n        return os.path.exists(element)\n    else:\n        if not isinstance(element, str):\n            raise TypeError(f\"element must be a string, not {type(element)}\")\n        if not os.path.isabs(element):\n            raise ValueError(f\"element must be an absolute path, not {element}\")\n        return os.path.exists(element)\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        return False\n\n    if input_dir is not None:\n        if not isinstance(input_dir, str):\n            raise ValueError(\"input_dir must be a string\")\n        input_dir = str(Path(input_dir).absolute())\n\n    if input_dir is not None:\n        if not element.startswith(input_dir):\n            return False\n\n    element = str(Path(element).absolute())\n\n    if not os.path.exists(element):\n        return False\n\n    return True\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is None:\n        input_dir = Path(os.getcwd()).absolute()\n    else:\n        input_dir = Path(input_dir).absolute()\n\n    if isinstance(element, str):\n        element = Path(element)\n\n    if element.is_absolute():\n        element = element.absolute()\n    else:\n        element = input_dir / element\n\n    return element.exists() or str(element).startswith(str(input_dir))\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons == 1:\n                return tcnn.TinyCNN(n_input_dims, n_output_dims)\n            elif n_neurons == 2:\n                return tcnn.TinyCNN2(n_input_dims, n_output_dims)\n            elif n_neurons == 3:\n                return tcnn.TinyCNN3(n_input_dims, n_output_dims)\n            elif n_neurons == 4:\n                return tcnn.TinyCNN4(n_input_dims, n_output_dims)\n            elif n_neurons == 5:\n                return tcnn.TinyCNN5(n_input_dims, n_output_dims)\n            elif n_neurons == 6:\n                return tcnn.TinyCNN6(n_input_dims, n_output_dims)\n            elif n_neurons == 7:\n                return tcnn.TinyCNN7(n_input_dims, n_output_dims)\n            elif n_neurons == 8:\n                return tcnn.TinyCNN8(n_input_dims, n_output_dims)\n            elif n_neurons == 9:\n                return tcnn.TinyCNN9(n_input_dims, n_output_dims)\n            elif n_neurons == 10:\n                return tcnn.TinyCNN10(n_input_dims, n_output_dims)\n            elif n_neurons == 11:\n                return tcnn.TinyCNN11(n_input_dims, n_output_dims)\n            elif n_neurons == 12:\n                return tcnn.TinyCNN12(n_input_dims, n_output_dims)\n            elif n_neurons == 1"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"Number of layers must be greater than 0\"\n        assert n_neurons > 0, \"Number of neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons <= 1024:\n                return tcnn.Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_neurons=n_neurons,\n                    n_hidden_layers=n_layers - 1,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n            else:\n                return tcnn.Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_neurons=1024,\n                    n_hidden_layers=n_layers - 2,\n                    activation=activation,\n                    output_activation=\"None\",\n                )\n        else:\n            layers = []\n            layers.append(nn.Linear(n_input_dims, n_neurons))\n            if activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif activation == \"None\":\n                pass\n            else:\n                raise ValueError(f\"Unknown activation function: {activation}\")\n\n            for i in range(n_layers - 2):\n                layers.append(nn.Linear(n_neurons, n_neurons))\n                if activation == \"ReLU\":\n                    layers.append(nn.ReLU())\n                elif activation == \"None\":\n                    pass\n                else:\n                    raise ValueError(f\"Unknown activation function: {activation}\")\n\n            layers.append(nn.Linear(n_neurons, n_output_dims))\n            if output_activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                layers.append(nn."}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"Number of layers must be greater than 0\"\n        assert n_neurons > 0, \"Number of neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons < 1024:\n                return self.get_tcnn_network_small(\n                    n_input_dims,\n                    n_output_dims,\n                    n_layers,\n                    n_neurons,\n                    activation,\n                    output_activation,\n                )\n            else:\n                return self.get_tcnn_network_large(\n                    n_input_dims,\n                    n_output_dims,\n                    n_layers,\n                    n_neurons,\n                    activation,\n                    output_activation,\n                )\n        else:\n            return self.get_pytorch_network(\n                n_input_dims,\n                n_output_dims,\n                n_layers,\n                n_neurons,\n                activation,\n                output_activation,\n            )\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            n_neurons = max(n_neurons, 1)\n            if n_neurons == 1:\n                model = tcnn.Linear(n_input_dims, n_output_dims)\n            else:\n                model = tcnn.Sequential()\n                model.append(tcnn.Linear(n_input_dims, n_neurons))\n                if activation == \"ReLU\":\n                    model.append(tcnn.ReLU())\n                for _ in range(n_layers - 2):\n                    model.append(tcnn.Linear(n_neurons, n_neurons))\n                    if activation == \"ReLU\":\n                        model.append(tcnn.ReLU())\n                model.append(tcnn.Linear(n_neurons, n_output_dims))\n                if output_activation == \"ReLU\":\n                    model.append(tcnn.ReLU())\n                elif output_activation == \"Sigmoid\":\n                    model.append(tcnn.Sigmoid())\n        else:\n            model = nn.Sequential()\n            model.append(nn.Linear(n_input_dims, n_neurons))\n            if activation == \"ReLU\":\n                model.append(nn.ReLU())\n            for _ in range(n_layers - 2):\n                model.append(nn.Linear(n_neurons, n_neurons))\n                if activation == \"ReLU\":\n                    model.append(nn.ReLU())\n            model.append(nn.Linear(n_neurons, n_output_dims))\n            if output_activation == \"ReLU\":\n                model.append(nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                model.append(nn.Sigmoid())\n        return model"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons == 1:\n                return tcnn.Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_neurons=n_neurons,\n                    n_hidden_layers=n_layers - 1,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n            else:\n                return tcnn.Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_neurons=n_neurons,\n                    n_hidden_layers=n_layers,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n        else:\n            layers = []\n            for i in range(n_layers - 1):\n                layers.append(nn.Linear(n_input_dims, n_neurons))\n                layers.append(nn.ReLU())\n                n_input_dims = n_neurons\n            layers.append(nn.Linear(n_input_dims, n_output_dims))\n            if output_activation == \"Sigmoid\":\n                layers.append(nn.Sigmoid())\n            elif output_activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            return nn.Sequential(*layers)"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons <= 256:\n                net = tcnn.NetworkWithInputEncoding(\n                    n_inputs=n_input_dims,\n                    n_outputs=n_output_dims,\n                    n_neurons=n_neurons,\n                    n_hidden_layers=n_layers - 1,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n            else:\n                net = tcnn.NetworkWithInputEncoding(\n                    n_inputs=n_input_dims,\n                    n_outputs=n_output_dims,\n                    n_neurons=256,\n                    n_hidden_layers=n_layers - 1,\n                    activation=activation,\n                    output_activation=\"tanh\",\n                )\n                net.add_subnetwork(\n                    tcnn.NetworkWithInputEncoding(\n                        n_inputs=256,\n                        n_outputs=n_output_dims,\n                        n_neurons=n_neurons - 256,\n                        n_hidden_layers=n_layers - 2,\n                        activation=activation,\n                        output_activation=output_activation,\n                    )\n                )\n        else:\n            net = torch.nn.Sequential()\n            net.add_module(\"input\", torch.nn.Linear(n_input_dims, n_neurons))\n            if activation == \"ReLU\":\n                net.add_module(\"input_activation\", torch.nn.ReLU())\n            for i in range(n_layers - 2):\n                net.add_module(f\"hidden_{i}\", torch.nn.Linear(n_neurons, n_neurons))\n                if activation == \"ReLU\":\n                    net.add_"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            # Use tinycudann\n            if n_neurons > 1024:\n                network_type = \"large\"\n            else:\n                network_type = \"small\"\n            return tcnn.Network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_neurons=n_neurons,\n                n_hidden_layers=n_layers - 1,\n                activation=activation,\n                output_activation=output_activation,\n                network_type=network_type,\n            )\n        else:\n            # Use PyTorch\n            layers = []\n            layers.append(nn.Linear(n_input_dims, n_neurons))\n            if activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            for _ in range(n_layers - 2):\n                layers.append(nn.Linear(n_neurons, n_neurons))\n                if activation == \"ReLU\":\n                    layers.append(nn.ReLU())\n            layers.append(nn.Linear(n_neurons, n_output_dims))\n            if output_activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                layers.append(nn.Sigmoid())\n            return nn.Sequential(*layers)"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons <= 1024:\n                model = tcnn.Network(\n                    n_inputs=n_input_dims,\n                    n_outputs=n_output_dims,\n                    n_neurons=n_neurons,\n                    n_hidden_layers=n_layers - 1,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n            else:\n                model = tcnn.Network(\n                    n_inputs=n_input_dims,\n                    n_outputs=n_output_dims,\n                    n_neurons=1024,\n                    n_hidden_layers=n_layers - 1,\n                    activation=activation,\n                    output_activation=\"None\",\n                )\n                model.add_layer(\n                    tcnn.Dense(\n                        n_inputs=1024,\n                        n_outputs=n_output_dims,\n                        activation=output_activation,\n                    )\n                )\n        else:\n            model = torch.nn.Sequential()\n            model.add_module(\"input\", torch.nn.Linear(n_input_dims, n_neurons))\n            model.add_module(\"input_activation\", self.get_activation(activation))\n\n            for i in range(n_layers - 2):\n                model.add_module(f\"hidden{i}\", torch.nn.Linear(n_neurons, n_neurons))\n                model.add_module(f\"hidden{i}_activation\", self.get_activation(activation))\n\n            model.add_module(\"output\", torch.nn.Linear(n_neurons, n_output_dims))\n            model.add_module(\"output_activation\", self.get_activation(output_activation))\n\n        return model\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons <= 1024:\n                return tinycudann.Network(\n                    n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation\n                )\n            else:\n                return tinycudann.Network(\n                    n_input_dims, n_output_dims, n_layers, 1024, activation, output_activation\n                )\n        else:\n            model = torch.nn.Sequential()\n            model.append(torch.nn.Linear(n_input_dims, n_neurons))\n            if activation == \"ReLU\":\n                model.append(torch.nn.ReLU())\n            elif activation == \"None\":\n                pass\n            else:\n                raise ValueError(f\"Invalid activation function: {activation}\")\n            for _ in range(n_layers - 2):\n                model.append(torch.nn.Linear(n_neurons, n_neurons))\n                if activation == \"ReLU\":\n                    model.append(torch.nn.ReLU())\n                elif activation == \"None\":\n                    pass\n                else:\n                    raise ValueError(f\"Invalid activation function: {activation}\")\n            model.append(torch.nn.Linear(n_neurons, n_output_dims))\n            if output_activation == \"ReLU\":\n                model.append(torch.nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                model.append(torch.nn.Sigmoid())\n            elif output_activation == \"None\":\n                pass\n            else:\n                raise ValueError(f\"Invalid output activation function: {output_activation}\")\n            return model\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons <= 256:\n                network_type = \"tiny\"\n            elif n_neurons <= 1024:\n                network_type = \"small\"\n            elif n_neurons <= 2048:\n                network_type = \"medium\"\n            else:\n                network_type = \"large\"\n\n            if activation == \"ReLU\":\n                activation_function = tcnn.ActivationReLU\n            elif activation == \"None\":\n                activation_function = tcnn.ActivationNone\n            else:\n                raise ValueError(f\"Invalid activation function: {activation}\")\n\n            if output_activation == \"ReLU\":\n                output_activation_function = tcnn.ActivationReLU\n            elif output_activation == \"Sigmoid\":\n                output_activation_function = tcnn.ActivationSigmoid\n            elif output_activation == \"None\":\n                output_activation_function = tcnn.ActivationNone\n            else:\n                raise ValueError(f\"Invalid output activation function: {output_activation}\")\n\n            return tcnn.Network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_neurons=n_neurons,\n                n_hidden_layers=n_layers - 1,\n                network_type=network_type,\n                activation_function=activation_function,\n                output_activation_function=output_activation_function,\n            )\n        else:\n            layers = []\n\n            if activation == \"ReLU\":\n                activation_function = nn.ReLU\n            elif activation == \"None\":\n                activation_function = nn.Identity\n            else:\n                raise ValueError(f\"Invalid activation function: {activation}\")\n\n            if output_activation == \"ReLU\":\n                output_activation_function = nn."}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons > 1024:\n                n_neurons = 1024\n            n_neurons = int(n_neurons)\n            n_layers = int(n_layers)\n\n            if n_neurons == 1024:\n                model = TCNN.TinyCudann(\n                    n_input_dims,\n                    n_output_dims,\n                    n_layers,\n                    n_neurons,\n                    activation,\n                    output_activation,\n                )\n            elif n_neurons == 512:\n                model = TCNN.TinyCudann(\n                    n_input_dims,\n                    n_output_dims,\n                    n_layers,\n                    n_neurons,\n                    activation,\n                    output_activation,\n                )\n            elif n_neurons == 256:\n                model = TCNN.TinyCudann(\n                    n_input_dims,\n                    n_output_dims,\n                    n_layers,\n                    n_neurons,\n                    activation,\n                    output_activation,\n                )\n            elif n_neurons == 128:\n                model = TCNN.TinyCudann(\n                    n_input_dims,\n                    n_output_dims,\n                    n_layers,\n                    n_neurons,\n                    activation,\n                    output_activation,\n                )\n            elif n_neurons == 64:\n                model = TCNN.TinyCudann(\n                    n_input_dims,\n                    n_output_dims,\n                    n_layers,\n                    n_neurons,\n                    activation,\n                    output_activation,\n                )\n            elif n_neurons == 32:\n                model = TCNN.TinyCudann(\n                    n_input_dims,\n                    n_output_dims,\n                    n_layers,\n                   "}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"Number of layers must be greater than 0\"\n        assert n_neurons > 0, \"Number of neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons == 1:\n                return tcnn.TinyCNN(\n                    n_input_dims,\n                    n_output_dims,\n                    n_layers,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n            elif n_neurons > 1:\n                return tcnn.TinyCNN(\n                    n_input_dims,\n                    n_output_dims,\n                    n_layers,\n                    n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n        else:\n            if n_neurons == 1:\n                return torch.nn.Sequential(\n                    torch.nn.Linear(n_input_dims, n_output_dims),\n                    self.get_activation(output_activation),\n                )\n            elif n_neurons > 1:\n                layers = []\n                for i in range(n_layers - 1):\n                    if i == 0:\n                        layers.append(torch.nn.Linear(n_input_dims, n_neurons))\n                    else:\n                        layers.append(torch.nn.Linear(n_neurons, n_neurons))\n                    layers.append(self.get_activation(activation))\n                layers.append(torch.nn.Linear(n_neurons, n_output_dims))\n                layers.append(self.get_activation(output_activation))\n                return torch.nn.Sequential(*layers)"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"Number of layers must be greater than 0\"\n        assert n_neurons > 0, \"Number of neurons per layer must be greater than 0\"\n\n        if self.tcnn:\n            # TinyCUDANN\n            if n_neurons > 1024:\n                n_neurons = 1024\n            elif n_neurons > 512:\n                n_neurons = 512\n            elif n_neurons > 256:\n                n_neurons = 256\n            elif n_neurons > 128:\n                n_neurons = 128\n\n            if activation == \"ReLU\":\n                activation = tcnn.Activation.RELU\n            elif activation == \"None\":\n                activation = tcnn.Activation.NONE\n            else:\n                raise ValueError(f\"Invalid activation function: {activation}\")\n\n            if output_activation == \"ReLU\":\n                output_activation = tcnn.Activation.RELU\n            elif output_activation == \"Sigmoid\":\n                output_activation = tcnn.Activation.SIGMOID\n            elif output_activation == \"None\":\n                output_activation = tcnn.Activation.NONE\n            else:\n                raise ValueError(f\"Invalid output activation function: {output_activation}\")\n\n            layers = []\n            for i in range(n_layers - 1):\n                layers.append(\n                    tcnn.Layer(\n                        n_input_dims=n_input_dims,\n                        n_output_dims=n_neurons,\n                        activation=activation,\n                    )\n                )\n                n_input_dims = n_neurons\n\n            layers.append(\n                tcnn.Layer(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    activation=output_activation,\n                )\n            )\n\n           "}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons < 100:\n                return TCNNetwork(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_layers=n_layers,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n            else:\n                return TCNNetwork(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_layers=n_layers,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n        else:\n            layers = []\n            for i in range(n_layers):\n                if i == 0:\n                    layers.append(nn.Linear(n_input_dims, n_neurons))\n                elif i == n_layers - 1:\n                    if output_activation == \"ReLU\":\n                        layers.append(nn.ReLU())\n                    elif output_activation == \"Sigmoid\":\n                        layers.append(nn.Sigmoid())\n                    layers.append(nn.Linear(n_neurons, n_output_dims))\n                else:\n                    if activation == \"ReLU\":\n                        layers.append(nn.ReLU())\n                    elif activation == \"None\":\n                        layers.append(nn.Identity())\n                    layers.append(nn.Linear(n_neurons, n_neurons))\n            return nn.Sequential(*layers)\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"Number of layers must be greater than 0\"\n        assert n_neurons > 0, \"Number of neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons == 1:\n                model = tinycudann.MLPClassifier(\n                    n_input_dims, n_output_dims, n_layers, 1, activation, output_activation\n                )\n            else:\n                model = tinycudann.MLPClassifier(\n                    n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation\n                )\n        else:\n            model = torch.nn.Sequential()\n            model.add_module(\"input_layer\", torch.nn.Linear(n_input_dims, n_neurons))\n            if activation == \"ReLU\":\n                model.add_module(\"activation_input\", torch.nn.ReLU())\n            for i in range(n_layers - 2):\n                model.add_module(f\"hidden_layer_{i}\", torch.nn.Linear(n_neurons, n_neurons))\n                if activation == \"ReLU\":\n                    model.add_module(f\"activation_hidden_{i}\", torch.nn.ReLU())\n            model.add_module(\"output_layer\", torch.nn.Linear(n_neurons, n_output_dims))\n            if output_activation == \"ReLU\":\n                model.add_module(\"activation_output\", torch.nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                model.add_module(\"activation_output\", torch.nn.Sigmoid())\n\n        return model\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons > 1024:\n                n_neurons = 1024\n            if n_neurons == 1024:\n                n_neurons = 512\n            if n_neurons == 512:\n                n_neurons = 256\n\n            model = TCN(\n                n_input_dims,\n                n_output_dims,\n                n_neurons,\n                n_layers,\n                activation,\n                output_activation,\n            )\n        else:\n            model = torch.nn.Sequential()\n            model.add_module(\"input\", torch.nn.Linear(n_input_dims, n_neurons))\n            model.add_module(\"input_activation\", self.get_activation(activation))\n\n            for i in range(n_layers - 1):\n                model.add_module(\n                    f\"hidden_{i}\", torch.nn.Linear(n_neurons, n_neurons)\n                )\n                model.add_module(\n                    f\"hidden_activation_{i}\", self.get_activation(activation)\n                )\n\n            model.add_module(\n                \"output\", torch.nn.Linear(n_neurons, n_output_dims)\n            )\n            model.add_module(\n                \"output_activation\", self.get_activation(output_activation)\n            )\n\n        return model\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"Number of layers must be greater than 0\"\n        assert n_neurons > 0, \"Number of neurons per layer must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons < 1000:\n                n_neurons = 1000\n            elif n_neurons > 20000:\n                n_neurons = 20000\n            return tcnn.Network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_neurons=n_neurons,\n                n_hidden_layers=n_layers,\n                activation=activation,\n                output_activation=output_activation,\n            )\n        else:\n            if n_layers == 1:\n                return nn.Sequential(\n                    nn.Linear(n_input_dims, n_output_dims),\n                    nn.ReLU() if activation == \"ReLU\" else nn.Identity(),\n                )\n            else:\n                layers = [nn.Linear(n_input_dims, n_neurons), nn.ReLU()]\n                for _ in range(n_layers - 2):\n                    layers.append(nn.Linear(n_neurons, n_neurons))\n                    layers.append(nn.ReLU())\n                layers.append(nn.Linear(n_neurons, n_output_dims))\n                if output_activation == \"ReLU\":\n                    layers.append(nn.ReLU())\n                elif output_activation == \"Sigmoid\":\n                    layers.append(nn.Sigmoid())\n                return nn.Sequential(*layers)\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons > 1024:\n                model = tc.models.MLPClassifier(\n                    n_input_dims,\n                    n_output_dims,\n                    n_layers,\n                    n_neurons,\n                    activation,\n                    output_activation,\n                )\n            else:\n                model = tc.models.MLPClassifier(\n                    n_input_dims,\n                    n_output_dims,\n                    n_layers,\n                    n_neurons,\n                    \"ReLU\",\n                    output_activation,\n                )\n        else:\n            model = nn.Sequential()\n            model.add_module(\"input\", nn.Linear(n_input_dims, n_neurons))\n            model.add_module(\"input_activation\", self.activation_dict[activation]())\n            for i in range(n_layers - 1):\n                model.add_module(f\"hidden{i}\", nn.Linear(n_neurons, n_neurons))\n                model.add_module(f\"hidden{i}_activation\", self.activation_dict[activation]())\n            model.add_module(\"output\", nn.Linear(n_neurons, n_output_dims))\n            model.add_module(\"output_activation\", self.activation_dict[output_activation]())\n        return model\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        # Assert that n_layers and n_neurons are greater than 0\n        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        # If using tinycudann, create a network with the specified number of neurons\n        if self.tcnn:\n            n_neurons = min(n_neurons, 1024)\n            if n_neurons == 1024:\n                return self.create_network_1024(\n                    n_input_dims, n_output_dims, n_layers, activation, output_activation\n                )\n            else:\n                return self.create_network_512(\n                    n_input_dims, n_output_dims, n_layers, activation, output_activation\n                )\n\n        # If using PyTorch, create a network sequentially, layer by layer\n        else:\n            layers = []\n\n            # Create the input layer\n            layers.append(nn.Linear(n_input_dims, n_neurons))\n            if activation == \"ReLU\":\n                layers.append(nn.ReLU())\n\n            # Create the hidden layers\n            for _ in range(n_layers - 2):\n                layers.append(nn.Linear(n_neurons, n_neurons))\n                if activation == \"ReLU\":\n                    layers.append(nn.ReLU())\n\n            # Create the output layer\n            layers.append(nn.Linear(n_neurons, n_output_dims))\n            if output_activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                layers.append(nn.Sigmoid())\n\n            # Return the network\n            return nn.Sequential(*layers)\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons > 1024:\n                n_neurons = 1024\n                print(\n                    f\"Warning: Number of neurons ({n_neurons}) exceeds the maximum (1024) for tinycudann. The number of neurons has been reduced to {n_neurons}.\"\n                )\n\n            if n_neurons > 100:\n                n_layers_per_block = int(n_neurons / 100)\n                n_neurons_per_block = 100\n            else:\n                n_layers_per_block = 1\n                n_neurons_per_block = n_neurons\n\n            if n_layers_per_block > 1:\n                n_layers_per_block = 1\n                n_neurons_per_block = n_neurons\n\n            n_layers_per_block = max(n_layers_per_block, 1)\n            n_neurons_per_block = max(n_neurons_per_block, 1)\n\n            n_neurons_per_block = min(n_neurons_per_block, 100)\n\n            n_blocks = int(n_layers / n_layers_per_block)\n\n            n_neurons = n_blocks * n_neurons_per_block\n\n            if n_neurons < 100:\n                n_neurons = 100\n                n_blocks = int(n_layers / n_layers_per_block)\n\n            if n_blocks == 0:\n                n_blocks = 1\n\n            n_layers = n_blocks * n_layers_per_block\n\n            if n_layers < 1:\n                n_layers = 1\n\n            if n_neurons < 100:\n                n_neurons = 100\n\n            if n_layers < 1:\n                n_layers = "}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements to be trimmed from each end of the signal\n        trim_left = kernel_offset\n        trim_right = kernel_offset\n\n        # Calculate the number of elements to be trimmed from each end of the signal\n        trim_left = kernel_offset\n        trim_right = kernel_offset\n\n        # Calculate the number of elements to be trimmed from each end of the signal\n        trim_left = kernel_offset\n        trim_right = kernel_offset\n\n        # Calculate the number of elements to be trimmed from each end of the signal\n        trim_left = kernel_offset\n        trim_right = kernel_offset\n\n        # Calculate the number of elements to be trimmed from each end of the signal\n        trim_left = kernel_offset\n        trim_right = kernel_offset\n\n        # Calculate the number of elements to be trimmed from each end of the signal\n        trim_left = kernel_offset\n        trim_right = kernel_offset\n\n        # Calculate the number of elements to be trimmed from each end of the signal\n        trim_left = kernel_offset\n        trim_right = kernel_offset\n\n        # Calculate the number of elements to be trimmed from each end of the signal\n        trim_left = kernel_offset\n        trim_right = kernel_offset\n\n        # Calculate the number of elements to be trimmed from each end of the signal\n        trim_left = kernel_offset\n        trim_right = kernel_offset\n\n        # Calculate the number of elements to be trimmed from each end of the signal\n        trim_left = kernel_offset\n        trim_right = kernel_offset\n\n        # Calculate the number of elements to be trimmed from each end of the signal\n        trim_left = kernel_offset\n        trim_right = kernel_offset\n\n        # Calculate the number of elements to be trimmed from each end of the signal\n        trim_left = kernel_offset\n        trim_right = kernel_offset\n\n        # Calculate the number of elements to be trimmed from each end of the signal\n        trim_left = kernel_offset\n        trim_right = kernel_offset\n\n        # Calculate the"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements to trim from each end of the signal\n        trim_left = kernel_offset\n        trim_right = kernel_offset\n\n        # Calculate the number of elements to shift the signal by\n        shift_left = kernel_offset\n        shift_right = -kernel_offset\n\n        # Shift the signal by the specified amount\n        shifted_signal = np.roll(signal, shift_left)\n\n        # Calculate the rolling median of the shifted signal\n        rolling_median = np.median(shifted_signal, axis=0)\n\n        # Trim the rolling median to remove edge effects\n        rolling_median = rolling_median[trim_left:-trim_right]\n\n        return rolling_median\n\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the number of elements to trim from the beginning and end of the resulting median array\n        trim_start = kernel_offset\n        trim_end = kernel_offset\n\n        # Compute the number of elements to shift the signal by\n        shift_start = kernel_offset\n        shift_end = -kernel_offset\n\n        # Compute the number of elements to shift the signal by\n        shift_start = kernel_offset\n        shift_end = -kernel_offset\n\n        # Compute the number of elements to shift the signal by\n        shift_start = kernel_offset\n        shift_end = -kernel_offset\n\n        # Compute the number of elements to shift the signal by\n        shift_start = kernel_offset\n        shift_end = -kernel_offset\n\n        # Compute the number of elements to shift the signal by\n        shift_start = kernel_offset\n        shift_end = -kernel_offset\n\n        # Compute the number of elements to shift the signal by\n        shift_start = kernel_offset\n        shift_end = -kernel_offset\n\n        # Compute the number of elements to shift the signal by\n        shift_start = kernel_offset\n        shift_end = -kernel_offset\n\n        # Compute the number of elements to shift the signal by\n        shift_start = kernel_offset\n        shift_end = -kernel_offset\n\n        # Compute the number of elements to shift the signal by\n        shift_start = kernel_offset\n        shift_end = -kernel_offset\n\n        # Compute the number of elements to shift the signal by\n        shift_start = kernel_offset\n        shift_end = -kernel_offset\n\n        # Compute the number of elements to shift the signal by\n        shift_start = kernel_offset\n        shift_end = -kernel_offset\n\n        # Compute the number of elements to shift the signal by\n        shift_start = kernel_offset\n        shift_end = -kernel_offset\n\n        # Compute the number of elements to shift the signal by\n        shift_start = kernel_offset\n        shift_end = -kernel_offset\n\n        # Compute the number of elements to shift the signal by\n        shift_"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Shift the signal to the left by kernel_offset\n        shifted_signal = np.roll(signal, kernel_offset)\n\n        # Shift the signal to the right by kernel_offset\n        shifted_signal_right = np.roll(signal, -kernel_offset)\n\n        # Calculate the rolling median by taking the median of the shifted signals\n        rolling_median = np.median(np.vstack((shifted_signal, shifted_signal_right)), axis=0)\n\n        # Trim the resulting median array to account for edge effects\n        trimmed_rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return trimmed_rolling_median\n\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal\n        signal_length = len(signal)\n        # Calculate the length of the rolling median\n        rolling_median_length = signal_length - 2 * kernel_offset\n        # Initialize an empty array to store the rolling median\n        rolling_median = np.zeros(rolling_median_length)\n        # Loop over the rolling median array\n        for i in range(rolling_median_length):\n            # Calculate the start and end indices for the shifted signal\n            start_index = i + kernel_offset\n            end_index = start_index + 2 * kernel_offset + 1\n            # Extract the shifted signal from the input signal\n            shifted_signal = signal[start_index:end_index]\n            # Calculate the median of the shifted signal\n            rolling_median[i] = np.median(shifted_signal)\n        # Return the rolling median array\n        return rolling_median\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements to trim from the beginning and end of the signal\n        trim_start = kernel_offset\n        trim_end = kernel_offset\n\n        # Create a copy of the input signal to avoid modifying the original array\n        signal_copy = signal.copy()\n\n        # Compute the rolling median by shifting the signal by a range defined by the kernel offset\n        for i in range(kernel_offset, len(signal) - kernel_offset):\n            signal_copy[i] = np.median(signal[i - kernel_offset:i + kernel_offset + 1])\n\n        # Trim the resulting median array to account for edge effects introduced by the shifting process\n        signal_copy = signal_copy[trim_start:-trim_end]\n\n        return signal_copy\n\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal and the kernel size\n        signal_length = len(signal)\n        kernel_size = 2 * kernel_offset + 1\n\n        # Create an empty array to store the rolling median\n        rolling_median = np.zeros(signal_length)\n\n        # Loop through the signal and compute the rolling median\n        for i in range(signal_length):\n            # Calculate the start and end indices for the shifted signal\n            start_idx = max(0, i - kernel_offset)\n            end_idx = min(signal_length, i + kernel_offset + 1)\n\n            # Extract the shifted signal\n            shifted_signal = signal[start_idx:end_idx]\n\n            # Calculate the rolling median of the shifted signal\n            rolling_median[i] = np.median(shifted_signal)\n\n        # Trim the rolling median to remove edge effects\n        rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return rolling_median\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal and the number of elements to shift\n        signal_length = len(signal)\n        kernel_size = 2 * kernel_offset + 1\n\n        # Initialize an empty array to store the shifted signals\n        shifted_signals = np.zeros((signal_length, kernel_size))\n\n        # Shift the signal by the specified offset and store the shifted signals in the shifted_signals array\n        for i in range(signal_length):\n            shifted_signals[i] = np.roll(signal, i - kernel_offset)\n\n        # Calculate the rolling median by taking the median of the shifted signals along the second axis (axis=1)\n        rolling_median = np.median(shifted_signals, axis=1)\n\n        # Trim the rolling median to remove edge effects due to the shifting process\n        rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return rolling_median\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements to trim from each side of the signal\n        trim_left = kernel_offset\n        trim_right = kernel_offset\n\n        # Calculate the number of elements to shift the signal by\n        shift_left = kernel_offset\n        shift_right = -kernel_offset\n\n        # Create a copy of the input signal for the rolling median calculation\n        signal_copy = signal.copy()\n\n        # Shift the signal to the left by shift_left elements\n        signal_copy = np.roll(signal_copy, shift_left)\n\n        # Shift the signal to the right by shift_right elements\n        signal_copy = np.roll(signal_copy, shift_right)\n\n        # Calculate the rolling median of the shifted signal\n        rolling_median = np.median(signal_copy, axis=1)\n\n        # Trim the rolling median to remove edge effects\n        rolling_median = rolling_median[trim_left:-trim_right]\n\n        # Return the rolling median\n        return rolling_median\n\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements in the input signal\n        signal_len = signal.shape[0]\n        # Calculate the number of elements in the output signal\n        out_len = signal_len - 2 * kernel_offset\n        # Initialize an empty array to store the output signal\n        out = np.zeros(out_len)\n        # Loop over the output signal\n        for i in range(out_len):\n            # Calculate the start and end indices for the current window\n            start = i + kernel_offset\n            end = start + kernel_offset + 1\n            # Extract the current window from the input signal\n            window = signal[start:end]\n            # Calculate the median of the current window and assign it to the current element of the output signal\n            out[i] = np.median(window)\n        # Return the output signal\n        return out\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        signal_length = len(signal)\n        kernel_size = 2 * kernel_offset + 1\n        signal_padded = np.pad(signal, (kernel_offset, kernel_offset), 'edge')\n        rolling_median = np.zeros(signal_length)\n        for i in range(signal_length):\n            rolling_median[i] = np.median(signal_padded[i:i + kernel_size])\n        return rolling_median\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal and the number of elements to trim from the edges\n        signal_length = len(signal)\n        trim_length = kernel_offset * 2\n\n        # Initialize the output array with zeros\n        output = np.zeros(signal_length - trim_length)\n\n        # Loop over the signal and compute the rolling median for each element\n        for i in range(signal_length - trim_length):\n            # Get the shifted signal for this element\n            shifted_signal = signal[i:i + 2 * kernel_offset + 1]\n\n            # Compute the median of the shifted signal\n            output[i] = np.median(shifted_signal)\n\n        # Return the trimmed output array\n        return output[:signal_length - trim_length]\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the input signal\n        signal_length = len(signal)\n\n        # Calculate the total number of shifts that will be applied\n        num_shifts = 2 * kernel_offset + 1\n\n        # Calculate the length of the trimmed rolling median\n        trimmed_length = signal_length - 2 * kernel_offset\n\n        # Initialize an empty array to store the trimmed rolling median\n        trimmed_median = np.zeros(trimmed_length)\n\n        # Loop over the trimmed length\n        for i in range(trimmed_length):\n            # Calculate the start and end indices for the current shift\n            start_index = i\n            end_index = i + num_shifts\n\n            # Extract the shifted signal\n            shifted_signal = signal[start_index:end_index]\n\n            # Compute the median of the shifted signal\n            median = np.median(shifted_signal)\n\n            # Store the median in the trimmed rolling median array\n            trimmed_median[i] = median\n\n        # Return the trimmed rolling median\n        return trimmed_median\n\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements to trim from each end of the signal\n        trim_length = kernel_offset * 2\n\n        # Calculate the number of elements to shift the signal by\n        shift_length = kernel_offset\n\n        # Initialize the output array with zeros\n        output = np.zeros_like(signal)\n\n        # Loop over the signal, shifting it by shift_length and taking the median of the shifted signal\n        for i in range(trim_length, len(signal) - trim_length):\n            shifted_signal = signal[i - shift_length:i + shift_length + 1]\n            output[i] = np.median(shifted_signal)\n\n        # Trim the output array to remove edge effects\n        output = output[trim_length:-trim_length]\n\n        return output\n\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements to be trimmed from the beginning and end of the result\n        trim_start = kernel_offset\n        trim_end = kernel_offset\n\n        # Calculate the number of elements to be trimmed from the beginning and end of the result\n        trim_start = kernel_offset\n        trim_end = kernel_offset\n\n        # Calculate the length of the resulting trimmed array\n        trimmed_length = len(signal) - 2 * kernel_offset\n\n        # Initialize the result array with zeros\n        result = np.zeros(trimmed_length)\n\n        # Loop over the elements of the trimmed array\n        for i in range(trimmed_length):\n            # Calculate the start and end indices for the current window\n            window_start = i - kernel_offset\n            window_end = i + kernel_offset + 1\n\n            # Extract the current window from the original signal\n            window = signal[window_start:window_end]\n\n            # Compute the median of the current window\n            result[i] = np.median(window)\n\n        # Return the trimmed result array\n        return result[trim_start:trim_end]\n\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements to trim from the beginning and end of the signal\n        trim_begin = kernel_offset\n        trim_end = kernel_offset\n        # Calculate the length of the trimmed signal\n        trimmed_length = signal.shape[0] - trim_begin - trim_end\n        # Initialize an empty array to store the rolling median\n        rolling_median = np.zeros(trimmed_length)\n        # Calculate the number of shifts required to cover the entire signal\n        num_shifts = trimmed_length\n        # Loop over the number of shifts\n        for shift in range(num_shifts):\n            # Calculate the start and end indices for the current shift\n            start = shift\n            end = start + trimmed_length\n            # Extract the shifted signal\n            shifted_signal = signal[start:end]\n            # Calculate the median of the shifted signal\n            rolling_median[shift] = np.median(shifted_signal)\n        # Return the trimmed rolling median\n        return rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements to trim from the beginning and end of the signal\n        trim_start = kernel_offset\n        trim_end = kernel_offset\n\n        # Calculate the number of elements to shift the signal by\n        shift_start = kernel_offset\n        shift_end = -kernel_offset\n\n        # Calculate the number of elements to include in the rolling median\n        kernel_size = (2 * kernel_offset) + 1\n\n        # Calculate the number of elements to shift the signal by\n        shift_start = kernel_offset\n        shift_end = -kernel_offset\n\n        # Calculate the number of elements to include in the rolling median\n        kernel_size = (2 * kernel_offset) + 1\n\n        # Shift the signal by the specified offset to generate shifted versions of itself\n        shifted_signal = np.roll(signal, shift_start)\n        shifted_signal = np.roll(shifted_signal, shift_end)\n\n        # Calculate the rolling median of the shifted signal\n        rolling_median = np.zeros(len(signal))\n        for i in range(kernel_size):\n            rolling_median += np.roll(shifted_signal, i)\n        rolling_median /= kernel_size\n\n        # Trim the rolling median to remove edge effects introduced by the shifting process\n        rolling_median = rolling_median[trim_start:-trim_end]\n\n        return rolling_median\n\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal array\n        signal_length = len(signal)\n        # Calculate the total number of elements to be trimmed from both ends\n        trim_length = 2 * kernel_offset\n        # Calculate the number of elements to be trimmed from each end\n        trim_length_each = trim_length // 2\n        # Initialize an empty list to store the rolling medians\n        rolling_median = []\n        # Loop over the signal array\n        for i in range(signal_length):\n            # Calculate the start and end indices for the current window\n            start_index = max(0, i - kernel_offset)\n            end_index = min(signal_length, i + kernel_offset + 1)\n            # Extract the current window from the signal\n            window = signal[start_index:end_index]\n            # Calculate the median of the current window and append it to the rolling median list\n            rolling_median.append(np.median(window))\n        # Convert the rolling median list to a NumPy array\n        rolling_median = np.array(rolling_median)\n        # Trim the rolling median array to remove edge effects\n        rolling_median = rolling_median[trim_length_each:-trim_length_each]\n        # Return the trimmed rolling median array\n        return rolling_median\n\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal and the number of elements to trim from both sides\n        signal_length = len(signal)\n        trim_length = kernel_offset * 2\n\n        # Calculate the number of elements to trim from the start and end of the signal\n        start_trim = kernel_offset\n        end_trim = signal_length - kernel_offset\n\n        # Initialize an empty array to store the rolling median\n        rolling_median = np.zeros(signal_length)\n\n        # Loop through the signal and calculate the rolling median\n        for i in range(start_trim, end_trim):\n            # Get the shifted signal from the current position to the left and right\n            shifted_left = signal[i - kernel_offset:i]\n            shifted_right = signal[i + 1:i + kernel_offset + 1]\n\n            # Calculate the median of the shifted signals\n            median = np.median(np.concatenate((shifted_left, shifted_right)))\n\n            # Store the median in the rolling median array\n            rolling_median[i] = median\n\n        # Trim the rolling median array to remove edge effects\n        rolling_median = rolling_median[start_trim:end_trim]\n\n        return rolling_median\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal and the number of shifts required\n        signal_length = len(signal)\n        num_shifts = kernel_offset * 2 + 1\n\n        # Initialize an empty array to store the shifted signals\n        shifted_signals = np.zeros((num_shifts, signal_length))\n\n        # Generate shifted signals by shifting the input signal by different offsets\n        for i in range(num_shifts):\n            shifted_signals[i] = np.roll(signal, i - kernel_offset)\n\n        # Calculate the rolling median by taking the median of the shifted signals along the first axis (axis=0)\n        rolling_median = np.median(shifted_signals, axis=0)\n\n        # Trim the rolling median to account for edge effects introduced by the shifting process\n        trimmed_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return trimmed_median\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance between the two iris templates\n    # The Hamming distance is the number of bits that differ between the two iris templates\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two iris templates and counting the number of bits that are different\n    # The Hamming distance is calculated by comparing the two ir"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance for each rotation shift\n    hamming_distances = np.zeros(rotation_shift + 1)\n    for i in range(rotation_shift + 1):\n        hamming_distances[i] = hamming_distance_calculation(\n            template_probe, template_gallery, i, nm_dist, weights\n        )\n\n    # Find the minimum Hamming distance and its corresponding rotation shift\n    min_hamming_distance = np.min(hamming_distances)\n    min_hamming_distance_index = np.argmin(hamming_distances)\n\n    return min_hamming_distance, min_hamming_distance_index"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance for each rotation shift\n    hamming_distances = [\n        hamming_distance_rotation(\n            template_probe, template_gallery, rotation_shift, nm_dist, weights\n        )\n        for rotation_shift in range(rotation_shift)\n    ]\n\n    # Find the minimum Hamming distance and its corresponding rotation shift\n    min_hamming_distance, min_rotation_shift = min(\n        hamming_distances, key=lambda x: x[0]\n    )\n\n    return min_hamming_distance, min_rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance between the two iris templates\n    # using the specified rotation shift and optional parameters\n    hamming_dist, rot_shift = hamming_distance_rot_shift(\n        template_probe, template_gallery, rotation_shift, nm_dist, weights\n    )\n\n    # Return the minimum Hamming distance and the corresponding rotation shift\n    return hamming_dist, rot_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance between the two iris templates for each rotation shift\n    hamming_distances = [\n        hamming_distance_rotated(\n            template_probe,\n            template_gallery,\n            rotation_shift,\n            rotation_shift_idx,\n            nm_dist,\n            weights,\n        )\n        for rotation_shift_idx in range(rotation_shift + 1)\n    ]\n\n    # Find the minimum Hamming distance and its corresponding rotation shift\n    min_hamming_distance, min_rotation_shift = min(\n        zip(hamming_distances, range(rotation_shift + 1))\n    )\n\n    return min_hamming_distance, min_rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance between the two iris templates\n    hamming_dist, rotation_shift = hamming_distance_calculation(\n        template_probe, template_gallery, rotation_shift\n    )\n\n    # Check if normalized Hamming distance is requested\n    if nm_dist is not None:\n        # Calculate the normalized Hamming distance\n        hamming_dist = hamming_dist / nm_dist\n\n    # Check if weighted Hamming distance is requested\n    if weights is not None:\n        # Calculate the weighted Hamming distance\n        hamming_dist = weighted_hamming_distance(\n            template_probe, template_gallery, weights, rotation_shift\n        )\n\n    # Return the minimum Hamming distance and the corresponding rotation shift\n    return hamming_dist, rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance for each rotation shift\n    min_hamming_dist = float(\"inf\")\n    min_rotation_shift = 0\n    for i in range(rotation_shift + 1):\n        # Calculate the Hamming distance for this rotation shift\n        hamming_dist = hamming_distance_calculation(\n            template_probe, template_gallery, i, nm_dist, weights\n        )\n\n        # Update the minimum Hamming distance and rotation shift if necessary\n        if hamming_dist < min_hamming_dist:\n            min_hamming_dist = hamming_dist\n            min_rotation_shift = i\n\n    return min_hamming_dist, min_rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Initialize the minimum distance and the corresponding rotation shift\n    min_dist = np.inf\n    min_rotation_shift = 0\n\n    # Loop through the rotation shifts\n    for rotation_shift_i in range(rotation_shift + 1):\n\n        # Calculate the Hamming distance considering the rotation shift\n        dist = _hamming_distance(\n            template_probe,\n            template_gallery,\n            rotation_shift_i,\n            nm_dist=nm_dist,\n            weights=weights,\n        )\n\n        # Update the minimum distance and rotation shift if the current distance is smaller\n        if dist < min_dist:\n            min_dist = dist\n            min_rotation_shift = rotation_shift_i\n\n    # Return the minimum distance and the corresponding rotation shift\n    return min_dist, min_rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance between the two templates\n    hamming_dist, rotation_shift = _hamming_distance(\n        template_probe, template_gallery, rotation_shift\n    )\n\n    # If the normalized distance is provided, calculate the normalized Hamming distance\n    if nm_dist is not None:\n        hamming_dist = hamming_dist / nm_dist\n\n    # If the weights are provided, calculate the weighted Hamming distance\n    if weights is not None:\n        hamming_dist = hamming_dist * weights\n\n    return hamming_dist, rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance for all rotation shifts\n    distances = []\n    for i in range(rotation_shift + 1):\n        rotated_template = np.roll(template_probe.iris_template, i, axis=1)\n        distance = hamming_distance_calculation(\n            rotated_template, template_gallery.iris_template, nm_dist, weights\n        )\n        distances.append(distance)\n\n    # Find the minimum Hamming distance and its corresponding rotation shift\n    min_distance = min(distances)\n    min_index = distances.index(min_distance)\n\n    return min_distance, min_index\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Initialize the minimum distance and rotation shift variables\n    min_dist = np.inf\n    min_rot_shift = None\n\n    # Loop through all possible rotation shifts\n    for rot_shift in range(rotation_shift + 1):\n        # Calculate the Hamming distance between the two iris templates\n        dist = hamming_distance_calculator(\n            template_probe, template_gallery, rot_shift, nm_dist, weights\n        )\n\n        # Update the minimum distance and rotation shift if a lower distance is found\n        if dist < min_dist:\n            min_dist = dist\n            min_rot_shift = rot_shift\n\n    # Return the minimum distance and rotation shift\n    return min_dist, min_rot_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance for all rotation shifts\n    dist_list = []\n    for i in range(rotation_shift + 1):\n        # Calculate the Hamming distance between the two templates\n        dist = hamming_distance_calculation(\n            template_probe, template_gallery, i, nm_dist, weights\n        )\n        # Append the distance and the corresponding rotation shift\n        dist_list.append((dist, i))\n\n    # Find the minimum distance and its corresponding rotation shift\n    min_dist, min_rotation_shift = min(dist_list, key=lambda x: x[0])\n\n    return min_dist, min_rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the input parameters are valid\n    if (\n        not isinstance(template_probe, IrisTemplate)\n        or not isinstance(template_gallery, IrisTemplate)\n        or not isinstance(rotation_shift, int)\n        or (nm_dist is not None and not isinstance(nm_dist, float))\n        or (weights is not None and not isinstance(weights, list))\n    ):\n        raise TypeError(\"Invalid input parameters\")\n\n    # Calculate the Hamming distance between the two iris templates\n    if nm_dist is not None and weights is not None:\n        # Calculate the normalized Hamming distance\n        hd_nm = hamming_distance(template_probe, template_gallery, rotation_shift)\n        # Calculate the weighted Hamming distance\n        hd_wt = hamming_distance(template_probe, template_gallery, rotation_shift, weights=weights)\n        # Calculate the Hamming distance\n        hd = (hd_nm + hd_wt) / 2\n    elif nm_dist is not None:\n        # Calculate the normalized Hamming distance\n        hd = hamming_distance(template_probe, template_gallery, rotation_shift, nm_dist=nm_dist)\n    elif weights is not None:\n        # Calculate the weighted Hamming distance\n        hd = hamming_distance(template_probe, template_gallery, rotation_shift, weights=weights)\n    else:\n        # Calculate the Hamming distance\n        hd = hamming_distance(template_probe, template_gallery, rotation_shift)\n\n    # Return the Hamming distance\n    return hd\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the input arguments are valid\n    if not isinstance(template_probe, IrisTemplate):\n        raise ValueError(\"Invalid input: template_probe must be an instance of IrisTemplate\")\n    if not isinstance(template_gallery, IrisTemplate):\n        raise ValueError(\"Invalid input: template_gallery must be an instance of IrisTemplate\")\n    if not isinstance(rotation_shift, int):\n        raise ValueError(\"Invalid input: rotation_shift must be an integer\")\n    if nm_dist is not None and not isinstance(nm_dist, float):\n        raise ValueError(\"Invalid input: nm_dist must be a float\")\n    if weights is not None and not isinstance(weights, list):\n        raise ValueError(\"Invalid input: weights must be a list\")\n\n    # Calculate the Hamming distance considering the allowed rotation shift\n    distance = np.zeros(rotation_shift + 1)\n    for i in range(rotation_shift + 1):\n        if i == 0:\n            distance[i] = hamming_distance_rotation_shift(template_probe, template_gallery, i)\n        else:\n            distance[i] = hamming_distance_rotation_shift(template_probe, template_gallery, i)\n\n    # Calculate the minimum Hamming distance and its corresponding rotation shift\n    min_distance = np.min(distance)\n    min_rotation_shift = np.argmin(distance)\n\n    # Calculate the normalized Hamming distance if the nm_dist parameter is provided\n    if nm_dist is not None:\n        min_distance = min_distance / nm_dist\n\n    # Calculate the weighted Hamming distance if the weights parameter is provided\n    if weights is not None:\n        min_distance = min_distance * weights[min_rotation_shift]\n\n    return min_distance, min_rotation_shift"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance for all rotations\n    hamming_distances = []\n    for rotation in range(rotation_shift + 1):\n        hamming_distance = template_probe.hamming_distance(template_gallery, rotation)\n        hamming_distances.append(hamming_distance)\n\n    # Find the minimum Hamming distance and corresponding rotation\n    min_hamming_distance = min(hamming_distances)\n    min_rotation = hamming_distances.index(min_hamming_distance)\n\n    # Calculate the normalized Hamming distance if nm_dist is provided\n    if nm_dist is not None:\n        min_hamming_distance = min_hamming_distance / nm_dist\n\n    # Calculate the weighted Hamming distance if weights are provided\n    if weights is not None:\n        min_hamming_distance = min_hamming_distance * weights[min_rotation]\n\n    return min_hamming_distance, min_rotation\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if weights is not None:\n        assert len(weights) == 3, \"The number of weights tables must be 3.\"\n\n    # Calculate the Hamming distance between the two iris templates\n    hamming_dist = np.sum(\n        np.abs(\n            np.subtract(\n                template_probe.iris_template_normalized,\n                template_gallery.iris_template_normalized,\n            )\n        )\n    )\n\n    # Calculate the normalized Hamming distance if the nonmatch distance is provided\n    if nm_dist is not None:\n        hamming_dist = hamming_dist / nm_dist\n\n    # Calculate the weighted Hamming distance if weights are provided\n    if weights is not None:\n        hamming_dist = np.sum(\n            np.multiply(\n                np.subtract(\n                    template_probe.iris_template_normalized,\n                    template_gallery.iris_template_normalized,\n                ),\n                weights,\n            )\n        )\n\n    # Calculate the Hamming distance considering the rotation shift\n    hamming_dist_rot = np.sum(\n        np.abs(\n            np.subtract(\n                template_probe.iris_template_normalized,\n                np.roll(\n                    template_gallery.iris_template_normalized,\n                    rotation_shift,\n                    axis=1,\n                ),\n            )\n        )\n    )\n\n    # Calculate the normalized Hamming distance considering the rotation shift if the nonmatch distance is provided\n    if nm_dist is not None:\n        hamming_dist_rot = hamming_dist_rot / nm_dist\n\n    # Calculate the weighted Hamming distance considering the rotation shift if weights are provided\n    if weights is not None:\n        hamming_dist_rot = np.sum(\n            np.multiply(\n                np.subtract(\n                    template_probe.iris_template_normalized,\n                    np.roll(\n                        template_gallery.iris_template_normalized,\n                        rotation_shift,\n                        axis=1,"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance for all possible rotation shifts\n    hamming_distances = []\n    for shift in range(rotation_shift + 1):\n        # Shift the template_gallery to the left by shift columns\n        shifted_gallery = np.roll(template_gallery.template, shift, axis=1)\n\n        # Calculate the Hamming distance between the shifted gallery and probe\n        hamming_distance = np.count_nonzero(template_probe.template != shifted_gallery)\n\n        # Append the Hamming distance and the corresponding shift to the list\n        hamming_distances.append((hamming_distance, shift))\n\n    # Find the minimum Hamming distance and its corresponding rotation shift\n    min_distance, min_shift = min(hamming_distances, key=lambda x: x[0])\n\n    # If nm_dist is provided, calculate the normalized Hamming distance\n    if nm_dist is not None:\n        min_distance = min_distance / nm_dist\n\n    # If weights are provided, calculate the weighted Hamming distance\n    if weights is not None:\n        min_distance = np.sum(np.abs(weights[0] * template_probe.template + weights[1] * shifted_gallery))\n\n    return min_distance, min_shift"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance for the given rotation shift\n    hamming_dist = _hamming_distance(template_probe, template_gallery, rotation_shift)\n\n    # Calculate the normalized Hamming distance if a nonmatch distance is provided\n    if nm_dist is not None:\n        hamming_dist = hamming_dist / nm_dist\n\n    # Calculate the weighted Hamming distance if weights are provided\n    if weights is not None:\n        hamming_dist = np.sum(weights * hamming_dist)\n\n    return hamming_dist, rotation_shift\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if nm_dist is not None:\n        if weights is None:\n            weights = [np.ones((1, 1))]\n        elif len(weights) != 1:\n            raise ValueError(\"Hamming distance with weights and nonmatch distance are mutually exclusive\")\n\n    if weights is None:\n        weights = [np.ones((1, 1))]\n\n    if len(weights) == 1:\n        weights = weights * 2\n\n    if len(weights) != 2:\n        raise ValueError(\"Hamming distance with weights and nonmatch distance are mutually exclusive\")\n\n    if len(weights[0].shape) != 2:\n        raise ValueError(\"Hamming distance with weights and nonmatch distance are mutually exclusive\")\n\n    if len(weights[1].shape) != 2:\n        raise ValueError(\"Hamming distance with weights and nonmatch distance are mutually exclusive\")\n\n    if weights[0].shape[0] != weights[0].shape[1]:\n        raise ValueError(\"Hamming distance with weights and nonmatch distance are mutually exclusive\")\n\n    if weights[1].shape[0] != weights[1].shape[1]:\n        raise ValueError(\"Hamming distance with weights and nonmatch distance are mutually exclusive\")\n\n    if weights[0].shape[0] != weights[1].shape[0]:\n        raise ValueError(\"Hamming distance with weights and nonmatch distance are mutually exclusive\")\n\n    if weights[0].shape[0] != template_probe.iris_data.shape[0]:\n        raise ValueError(\"Hamming distance with weights and nonmatch distance are mutually exclusive\")\n\n    if weights[0].shape[0] != template_gallery.iris_data.shape[0]:\n        raise ValueError(\"Hamming distance with weights and nonmatch distance are mutually exclusive\")\n\n    if nm_dist is not None:\n        if nm_dist <= 0:\n            raise ValueError(\"Hamming distance with weights and nonmatch distance are mutually exclusive\")\n\n    if rotation_shift < 0:\n        raise ValueError(\"Rotation shift must be a positive integer"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Convert the rotation shift to columns for the Hamming distance calculation\n    columns = rotation_shift\n\n    # Get the iris templates from the probe and gallery\n    probe = template_probe.iris_template\n    gallery = template_gallery.iris_template\n\n    # Calculate the Hamming distance between the iris templates\n    dist = hamming_distance_calc(probe, gallery, columns)\n\n    # Calculate the normalized Hamming distance if a nonmatch distance is provided\n    if nm_dist is not None:\n        dist = dist / nm_dist\n\n    # Calculate the weighted Hamming distance if weights are provided\n    if weights is not None:\n        dist = np.sum(np.multiply(dist, weights))\n\n    # Return the minimum Hamming distance and the corresponding rotation shift\n    return dist, columns\n\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        if self.number_of_bisectors > len(polygon):\n            self.number_of_bisectors = len(polygon)\n\n        bisectors = np.zeros((self.number_of_bisectors, 4))\n        for i in range(self.number_of_bisectors):\n            bisectors[i] = self._calculate_perpendicular_bisector(\n                polygon, min_distance_between_sector_points_in_px\n            )\n\n        if np.all(bisectors == 0):\n            raise EyeCentersEstimationError(\n                \"Failed to find a sufficient number of point pairs that meet the distance criterion within the maximum number of iterations allowed.\"\n            )\n\n        return (\n            bisectors[:, 0:2],\n            bisectors[:, 2:4],\n        )"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize the number of iterations to zero\n        iteration = 0\n\n        # Initialize a list to store the starting and ending points of the perpendicular bisectors\n        bisector_start_points = []\n        bisector_end_points = []\n\n        # Initialize a list to store the indices of the points that have been used in the current iteration\n        used_points_indices = []\n\n        # Initialize a boolean flag to indicate whether the required number of perpendicular bisectors have been found\n        found_required_number_of_bisectors = False\n\n        # Initialize a list to store the indices of the points that have been used in the current iteration\n        used_points_indices = []\n\n        # Initialize a list to store the indices of the points that have been used in the current iteration\n        used_points_indices = []\n\n        # Initialize a list to store the indices of the points that have been used in the current iteration\n        used_points_indices = []\n\n        # Initialize a list to store the indices of the points that have been used in the current iteration\n        used_points_indices = []\n\n        # Initialize a list to store the indices of the points that have been used in the current iteration\n        used_points_indices = []\n\n        # Initialize a list to store the indices of the points that have been used in the current iteration\n        used_points_indices = []\n\n        # Initialize a list to store the indices of the points that have been used in the current iteration\n        used_points_indices = []\n\n        # Initialize a list to store the indices of the points that have been used in the current iteration\n        used_points_indices = []\n\n        # Initialize a list to store the indices of the points that have been used in the current iteration\n        used_points_indices = []\n\n        # Initialize a list to store the indices of the points that have been used in the current iteration\n        used_points_indices = []\n\n        # Initialize a list to store the indices of the points that have been used in the current iteration\n        used_points_indices = []\n\n        # Initialize a list"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        bisectors_start_points = []\n        bisectors_end_points = []\n        for _ in range(self.number_of_bisectors_to_calculate):\n            bisector_start_point, bisector_end_point = self._calculate_perpendicular_bisector(\n                polygon, min_distance_between_sector_points_in_px\n            )\n            bisectors_start_points.append(bisector_start_point)\n            bisectors_end_points.append(bisector_end_point)\n\n        if len(bisectors_start_points) != self.number_of_bisectors_to_calculate:\n            raise EyeCentersEstimationError(\n                f\"Failed to find {self.number_of_bisectors_to_calculate} perpendicular bisectors that meet the distance criterion.\"\n            )\n\n        return np.array(bisectors_start_points), np.array(bisectors_end_points)\n\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        bisectors_start_points = np.zeros((self.number_of_bisectors, 2))\n        bisectors_end_points = np.zeros((self.number_of_bisectors, 2))\n        for i in range(self.number_of_bisectors):\n            for j in range(self.max_iterations):\n                random_indices = np.random.choice(len(polygon), 2, replace=False)\n                point_1 = polygon[random_indices[0]]\n                point_2 = polygon[random_indices[1]]\n                if np.linalg.norm(point_1 - point_2) > min_distance_between_sector_points_in_px:\n                    bisectors_start_points[i] = point_1\n                    bisectors_end_points[i] = point_2\n                    break\n            if j == self.max_iterations - 1:\n                raise EyeCentersEstimationError(\n                    f\"Failed to find a sufficient number of point pairs with a distance greater than {min_distance_between_sector_points_in_px} pixels.\"\n                )\n        return bisectors_start_points, bisectors_end_points\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        def _calculate_bisector(\n            p1: np.ndarray, p2: np.ndarray\n        ) -> Tuple[np.ndarray, np.ndarray]:\n            \"\"\"\n            Calculates the perpendicular bisector of a line segment defined by two points.\n\n            Input-Output Arguments\n            :param p1: np.ndarray. The starting point of the line segment.\n            :param p2: np.ndarray. The ending point of the line segment.\n            :return: Tuple[np.ndarray, np.ndarray]. A tuple containing two numpy arrays. The first array represents the starting point of the perpendicular bisector, and the second array represents the ending point of this bisector.\n            \"\"\"\n            mid_point = (p1 + p2) / 2\n            direction = p2 - p1\n            direction = direction / np.linalg.norm(direction)\n            direction = np.array([-direction[1], direction[0]])\n            return mid_point, mid_point + direction\n\n        def _calculate_distance_between_points(\n            p1: np.ndarray, p2: np.ndarray\n        ) -> float:\n            \"\"\"\n            Calculates the Euclidean distance between two points.\n\n            Input-Output Arguments\n            :param p1: np.ndarray. The first point.\n            :param p2: np.ndarray. The second point.\n            :return: float. The Euclidean distance between the two points.\n            \"\"\"\n            return np.linalg.norm(p1 - p2)\n\n        def _calculate_distance_between_point_and_line_segment(\n            point: np.ndarray, p1: np.ndarray, p2: np.ndarray\n        ) -> float:\n            \"\"\"\n            Calculates the minimum distance between a point and a line segment defined by two points.\n\n            Input-Output Arguments\n            :param point: np.ndarray. The point for which the distance is calculated.\n            :param p1: np.ndarray. The starting point of the line segment.\n            :param p2: np."}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        if self.number_of_bisectors > len(polygon):\n            raise EyeCentersEstimationError(\n                \"The number of bisectors cannot be greater than the number of vertices in the polygon.\"\n            )\n\n        if min_distance_between_sector_points_in_px <= 0:\n            raise EyeCentersEstimationError(\n                \"The minimum distance between sector points must be a positive value.\"\n            )\n\n        bisectors_start_points = np.zeros(\n            (self.number_of_bisectors, 2), dtype=np.float32\n        )\n        bisectors_end_points = np.zeros(\n            (self.number_of_bisectors, 2), dtype=np.float32\n        )\n\n        for i in range(self.number_of_bisectors):\n            for j in range(self.max_iterations):\n                random_pair_of_points = np.random.choice(\n                    len(polygon), 2, replace=False\n                )\n                if (\n                    np.linalg.norm(\n                        polygon[random_pair_of_points[0]]\n                        - polygon[random_pair_of_points[1]]\n                    )\n                    > min_distance_between_sector_points_in_px\n                ):\n                    bisectors_start_points[i] = polygon[\n                        random_pair_of_points[0]\n                    ]\n                    bisectors_end_points[i] = polygon[\n                        random_pair_of_points[1]\n                    ]\n                    break\n\n            if (\n                np.linalg.norm(\n                    bisectors_start_points[i] - bisectors_end_points[i]\n                )\n                <= min_distance_between_sector_points_in_px\n            ):\n                raise EyeCentersEstimationError(\n                    \"Failed to find a sufficient number of point pairs that meet the distance criterion within the maximum number of iterations allowed.\"\n                )\n\n        return bisectors_start_points, bisectors_end_points"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize variables\n        bisectors_start_points = []\n        bisectors_end_points = []\n\n        # Calculate the number of points in the polygon\n        number_of_points = len(polygon)\n\n        # Iterate over the maximum number of iterations\n        for i in range(self.max_iterations):\n\n            # Choose two random points from the polygon\n            point1_index = np.random.randint(number_of_points)\n            point2_index = np.random.randint(number_of_points)\n\n            # Calculate the distance between the chosen points\n            distance = np.linalg.norm(polygon[point1_index] - polygon[point2_index])\n\n            # If the distance is greater than the minimum distance, add the points to the lists\n            if distance > min_distance_between_sector_points_in_px:\n                bisectors_start_points.append(polygon[point1_index])\n                bisectors_end_points.append(polygon[point2_index])\n\n            # If the number of bisectors is sufficient, break the loop\n            if len(bisectors_start_points) >= self.number_of_bisectors:\n                break\n\n        # If the number of bisectors is not sufficient, raise an exception\n        if len(bisectors_start_points) < self.number_of_bisectors:\n            raise EyeCentersEstimationError(\n                f\"Could not find {self.number_of_bisectors} point pairs with a minimum distance of {min_distance_between_sector_points_in_px} px in {self.max_iterations} iterations.\"\n            )\n\n        # Convert the lists to numpy arrays\n        bisectors_start_points = np.array(bisectors_start_points)\n        bisectors_end_points = np.array(bisectors_end_points)\n\n        # Return the calculated bisectors\n        return bisectors_start_points, bisectors_end_points\n\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize the list of bisectors with the first pair of points\n        bisectors = [polygon[0:2]]\n\n        # Loop through the maximum number of iterations\n        for _ in range(self._max_iterations):\n\n            # Choose two random points from the polygon\n            point1 = polygon[np.random.randint(0, len(polygon))]\n            point2 = polygon[np.random.randint(0, len(polygon))]\n\n            # Check if the distance between the chosen points is greater than the minimum distance\n            if np.linalg.norm(point1 - point2) > min_distance_between_sector_points_in_px:\n                bisectors.append(np.array([point1, point2]))\n\n            # If the number of bisectors reaches the desired number, return the bisectors\n            if len(bisectors) >= self._number_of_bisectors:\n                return np.array([bisector[0] for bisector in bisectors]), np.array(\n                    [bisector[1] for bisector in bisectors]\n                )\n\n        # If the maximum number of iterations is reached and not enough bisectors are found, raise an error\n        raise EyeCentersEstimationError(\n            f\"Failed to find {self._number_of_bisectors} perpendicular bisectors in {self._max_iterations} iterations.\"\n        )"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Calculate the number of points in the polygon\n        num_points = polygon.shape[0]\n\n        # Initialize the arrays to store the starting and ending points of the perpendicular bisectors\n        bisector_start_points = np.zeros((self.num_bisectors, 2))\n        bisector_end_points = np.zeros((self.num_bisectors, 2))\n\n        # Iterate over the maximum number of iterations\n        for _ in range(self.max_iterations):\n\n            # Generate random indices for the points in the polygon\n            random_indices = np.random.choice(num_points, size=self.num_bisectors * 2, replace=False)\n\n            # Extract the corresponding points from the polygon\n            random_points = polygon[random_indices]\n\n            # Calculate the perpendicular bisectors between the randomly chosen points\n            bisector_start_points, bisector_end_points = self._calculate_perpendicular_bisectors_from_points(\n                random_points\n            )\n\n            # Check if all the perpendicular bisectors meet the minimum distance criterion\n            if np.all(\n                np.linalg.norm(bisector_start_points - bisector_end_points, axis=1)\n                >= min_distance_between_sector_points_in_px\n            ):\n                break\n\n        # If the loop has completed without meeting the minimum distance criterion, raise an error\n        if _ == self.max_iterations - 1:\n            raise EyeCentersEstimationError(\n                f\"Failed to find a sufficient number of point pairs that meet the distance criterion within {self.max_iterations} iterations.\"\n            )\n\n        # Return the calculated perpendicular bisectors\n        return bisector_start_points, bisector_end_points\n\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Calculate the number of pairs of points to choose\n        num_pairs = int(len(polygon) / 2)\n\n        # Initialize the arrays to store the starting and ending points of the perpendicular bisectors\n        bisector_start_points = np.zeros((num_pairs, 2))\n        bisector_end_points = np.zeros((num_pairs, 2))\n\n        # Initialize the counter for the number of iterations\n        iterations = 0\n\n        # Initialize the flag to indicate whether the required number of point pairs have been found\n        found_points = False\n\n        # Loop until the required number of point pairs have been found or the maximum number of iterations is reached\n        while not found_points and iterations < self.max_iterations:\n\n            # Choose two random points from the polygon\n            point_1_index = np.random.randint(0, len(polygon))\n            point_2_index = np.random.randint(0, len(polygon))\n\n            # Ensure that the chosen points are not the same\n            while point_1_index == point_2_index:\n                point_2_index = np.random.randint(0, len(polygon))\n\n            # Calculate the perpendicular bisector of the chosen points\n            bisector_start_points[iterations] = polygon[point_1_index]\n            bisector_end_points[iterations] = polygon[point_2_index]\n\n            # Check if the distance between the chosen points is greater than the minimum distance\n            distance = np.linalg.norm(\n                bisector_start_points[iterations] - bisector_end_points[iterations]\n            )\n            if distance > min_distance_between_sector_points_in_px:\n                found_points = True\n\n            iterations += 1\n\n        # If the required number of point pairs have not been found after the maximum number of iterations, raise an exception\n        if not found_points:\n            raise EyeCentersEstimationError(\n                \"Could not find enough point pairs with a sufficient distance between them.\"\n            )\n\n        return"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        n_bisectors = self.n_bisectors\n        max_iter = self.max_iter\n\n        n_vertices = polygon.shape[0]\n        n_pairs = n_bisectors\n        n_points = 2\n\n        bisectors_start = np.zeros((n_pairs, n_points, 2), dtype=np.float32)\n        bisectors_end = np.zeros((n_pairs, n_points, 2), dtype=np.float32)\n\n        for i in range(n_pairs):\n            for j in range(n_points):\n                for k in range(max_iter):\n                    # Randomly select two points from the polygon\n                    idx_1, idx_2 = np.random.choice(n_vertices, 2, replace=False)\n                    point_1 = polygon[idx_1]\n                    point_2 = polygon[idx_2]\n\n                    # Calculate the distance between the two points\n                    distance = np.linalg.norm(point_1 - point_2)\n\n                    if distance > min_distance_between_sector_points_in_px:\n                        # Calculate the midpoint between the two points\n                        midpoint = (point_1 + point_2) / 2\n\n                        # Calculate the direction vector from the midpoint to the first point\n                        direction = point_1 - midpoint\n\n                        # Normalize the direction vector\n                        direction /= np.linalg.norm(direction)\n\n                        # Calculate the perpendicular bisector\n                        bisector_start = midpoint - direction\n                        bisector_end = midpoint + direction\n\n                        bisectors_start[i, j] = bisector_start\n                        bisectors_end[i, j] = bisector_end\n\n                        break\n\n                if distance > min_distance_between_sector_points_in_px:\n                    break\n\n            if distance <= min_distance_between_sector_points_in_px:\n                raise EyeCentersEstimationError(\n                    f\"Failed to find a sufficient number of point pairs with"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize the starting points and ending points arrays\n        starting_points = np.zeros((self.number_of_bisectors, 2))\n        ending_points = np.zeros((self.number_of_bisectors, 2))\n\n        # Iterate for a maximum number of times\n        for iteration in range(self.max_iterations):\n\n            # Choose two random points from the polygon's vertices\n            random_points = np.random.choice(polygon.shape[0], size=2, replace=False)\n            random_point_1 = polygon[random_points[0]]\n            random_point_2 = polygon[random_points[1]]\n\n            # Calculate the midpoint between the two points\n            midpoint = (random_point_1 + random_point_2) / 2\n\n            # Calculate the direction vector between the two points\n            direction_vector = random_point_2 - random_point_1\n\n            # Normalize the direction vector\n            direction_vector = direction_vector / np.linalg.norm(direction_vector)\n\n            # Calculate the perpendicular vector\n            perpendicular_vector = np.array([-direction_vector[1], direction_vector[0]])\n\n            # Calculate the starting point and ending point of the perpendicular bisector\n            starting_point = midpoint - perpendicular_vector * min_distance_between_sector_points_in_px\n            ending_point = midpoint + perpendicular_vector * min_distance_between_sector_points_in_px\n\n            # Check if the perpendicular bisector meets the distance criterion\n            if np.linalg.norm(starting_point - ending_point) >= min_distance_between_sector_points_in_px * 2:\n\n                # Add the perpendicular bisector to the arrays\n                starting_points[iteration] = starting_point\n                ending_points[iteration] = ending_point\n\n                # Check if the required number of bisectors have been found\n                if iteration == self.number_of_bisectors - 1:\n\n                    # Return the starting points and ending points\n                    return starting_"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize variables to store the starting and ending points of the perpendicular bisectors\n        start_points = []\n        end_points = []\n\n        # Loop over the maximum number of iterations\n        for _ in range(self.max_iterations):\n\n            # Choose two random points from the polygon\n            p1 = polygon[np.random.randint(0, len(polygon))]\n            p2 = polygon[np.random.randint(0, len(polygon))]\n\n            # Check if the distance between the chosen points is greater than the minimum distance\n            if np.linalg.norm(p1 - p2) > min_distance_between_sector_points_in_px:\n\n                # Calculate the midpoint between the chosen points\n                midpoint = (p1 + p2) / 2\n\n                # Calculate the vector from the midpoint to the first point\n                v1 = p1 - midpoint\n\n                # Calculate the vector from the midpoint to the second point\n                v2 = p2 - midpoint\n\n                # Calculate the normal vector to the line connecting the midpoint and the first point\n                normal1 = np.array([-v1[1], v1[0]])\n\n                # Calculate the normal vector to the line connecting the midpoint and the second point\n                normal2 = np.array([-v2[1], v2[0]])\n\n                # Calculate the starting point of the perpendicular bisector\n                start_point = midpoint + normal1\n\n                # Calculate the ending point of the perpendicular bisector\n                end_point = midpoint + normal2\n\n                # Append the starting and ending points to the lists\n                start_points.append(start_point)\n                end_points.append(end_point)\n\n                # If the number of perpendicular bisectors is equal to the desired number, break the loop\n                if len(start_points) == self.num_bisectors:\n                    break\n\n        # If the number of perpendicular bisectors is not equal to the desired number, raise an exception\n        if len(start_points) != self.num_bisectors"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Set the maximum number of iterations to 1000\n        max_iterations = 1000\n\n        # Initialize a counter for the number of iterations\n        iteration = 0\n\n        # Initialize a list to store the perpendicular bisectors\n        bisectors = []\n\n        # Initialize a list to store the starting points of the bisectors\n        bisectors_start_points = []\n\n        # Initialize a list to store the ending points of the bisectors\n        bisectors_end_points = []\n\n        # Initialize a flag to indicate whether the desired number of point pairs have been found\n        found_point_pairs = False\n\n        # Generate the perpendicular bisectors\n        while not found_point_pairs and iteration < max_iterations:\n            # Select two random points from the polygon's vertices\n            point_1, point_2 = random.sample(list(polygon), 2)\n\n            # Calculate the perpendicular bisector\n            bisector = self._calculate_perpendicular_bisector(point_1, point_2)\n\n            # Check if the distance between the two points is greater than the minimum distance\n            if self._calculate_distance_between_points(point_1, point_2) > min_distance_between_sector_points_in_px:\n                # Add the bisector to the list\n                bisectors.append(bisector)\n\n                # Add the starting point of the bisector to the list\n                bisectors_start_points.append(point_1)\n\n                # Add the ending point of the bisector to the list\n                bisectors_end_points.append(point_2)\n\n                # Check if the desired number of point pairs have been found\n                if len(bisectors) == self.number_of_bisectors:\n                    found_point_pairs = True\n\n            # Increment the iteration counter\n            iteration += 1\n\n        # Check if the desired number of point pairs have been found\n        if not found_point_pairs:\n            # Raise an exception if the maximum number of iterations is reached\n            raise EyeCentersEstimationError"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize the number of iterations to 0\n        iterations = 0\n\n        # Initialize the number of point pairs that meet the distance criterion to 0\n        number_of_point_pairs_that_meet_distance_criterion = 0\n\n        # Initialize the starting points and ending points arrays\n        starting_points = np.empty((0, 2))\n        ending_points = np.empty((0, 2))\n\n        # Keep iterating until the number of point pairs that meet the distance criterion is equal to the desired number of bisectors\n        while number_of_point_pairs_that_meet_distance_criterion < self.number_of_bisectors:\n\n            # Increment the number of iterations\n            iterations += 1\n\n            # If the maximum number of iterations has been reached, raise an error\n            if iterations > self.max_iterations:\n                raise EyeCentersEstimationError(\n                    f\"Could not find {self.number_of_bisectors} point pairs that meet the distance criterion within {self.max_iterations} iterations\"\n                )\n\n            # Select two random points from the polygon's vertices\n            random_point_indices = np.random.choice(len(polygon), 2)\n            random_points = polygon[random_point_indices]\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(random_points[0] - random_points[1])\n\n            # If the distance is greater than the minimum distance, add the points to the starting and ending points arrays\n            if distance > min_distance_between_sector_points_in_px:\n                starting_points = np.append(starting_points, [random_points[0]], axis=0)\n                ending_points = np.append(ending_points, [random_points[1]], axis=0)\n                number_of_point_pairs_that_meet_distance_criterion += 1\n\n        # Return the starting and ending points arrays\n        return starting_points, ending_points"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        if self.n_bisectors > len(polygon):\n            raise EyeCentersEstimationError(\n                f\"Cannot calculate {self.n_bisectors} bisectors from {len(polygon)} points.\"\n            )\n\n        # Initialize arrays to store bisector points\n        bisector_start_points = np.zeros((self.n_bisectors, 2))\n        bisector_end_points = np.zeros((self.n_bisectors, 2))\n\n        # Initialize counter for bisector points\n        bisector_counter = 0\n\n        # Loop until sufficient number of bisector points are found\n        while bisector_counter < self.n_bisectors:\n            # Select two random points from polygon\n            point1_index = np.random.randint(0, len(polygon))\n            point2_index = np.random.randint(0, len(polygon))\n\n            # Ensure that points are not the same\n            while point1_index == point2_index:\n                point2_index = np.random.randint(0, len(polygon))\n\n            # Calculate bisector points\n            bisector_start_points[bisector_counter, :] = polygon[point1_index, :]\n            bisector_end_points[bisector_counter, :] = polygon[point2_index, :]\n\n            # Ensure that points are not too close\n            if np.linalg.norm(\n                bisector_start_points[bisector_counter, :]\n                - bisector_end_points[bisector_counter, :]\n            ) > min_distance_between_sector_points_in_px:\n                bisector_counter += 1\n\n        # Check if sufficient number of bisector points were found\n        if bisector_counter < self.n_bisectors:\n            raise EyeCentersEstimationError(\n                f\"Failed to find {self.n_bisectors} bisector points within {self.max_iterations} iterations.\"\n            )\n\n        return bisector_start_points, bisector_end_points"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Create an empty list to store the bisectors\n        bisectors = []\n\n        # Iterate until we have found enough bisectors\n        for i in range(self.number_of_bisectors):\n\n            # Keep track of the number of iterations\n            iterations = 0\n\n            # Iterate until we have found a pair of points that meet the distance criterion\n            while True:\n\n                # Increment the number of iterations\n                iterations += 1\n\n                # Choose two random points from the polygon\n                point_1, point_2 = np.random.choice(polygon, size=2, replace=False)\n\n                # Calculate the distance between the two points\n                distance = np.linalg.norm(point_2 - point_1)\n\n                # Check if the distance is greater than the minimum distance\n                if distance > min_distance_between_sector_points_in_px:\n\n                    # Calculate the midpoint between the two points\n                    midpoint = (point_1 + point_2) / 2\n\n                    # Calculate the perpendicular bisector\n                    bisector = self._calculate_perpendicular_bisector(point_1, point_2)\n\n                    # Add the bisector to the list\n                    bisectors.append(bisector)\n\n                    # Break out of the loop\n                    break\n\n                # Check if we have exceeded the maximum number of iterations\n                if iterations > self.max_iterations:\n\n                    # Raise an exception if we have exceeded the maximum number of iterations\n                    raise EyeCentersEstimationError(\n                        f\"Could not find a sufficient number of point pairs that meet the distance criterion within {self.max_iterations} iterations.\"\n                    )\n\n        # Convert the bisectors list to a numpy array\n        bisectors = np.array(bisectors)\n\n        # Return the bisectors\n        return bisectors[:, 0], bisectors[:, 1]\n\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        n_bisectors = self._n_bisectors\n        max_iterations = self._max_iterations\n\n        if n_bisectors > len(polygon):\n            raise EyeCentersEstimationError(\n                \"The number of bisectors cannot be greater than the number of polygon vertices.\"\n            )\n\n        if n_bisectors < 2:\n            raise EyeCentersEstimationError(\n                \"The number of bisectors must be at least 2.\"\n            )\n\n        if max_iterations < 1:\n            raise EyeCentersEstimationError(\n                \"The maximum number of iterations must be at least 1.\"\n            )\n\n        if min_distance_between_sector_points_in_px < 0:\n            raise EyeCentersEstimationError(\n                \"The minimum distance between sector points must be a positive value.\"\n            )\n\n        if n_bisectors > len(polygon) / 2:\n            raise EyeCentersEstimationError(\n                \"The number of bisectors cannot be greater than half the number of polygon vertices.\"\n            )\n\n        if min_distance_between_sector_points_in_px > self._max_distance_between_sector_points_in_px:\n            raise EyeCentersEstimationError(\n                f\"The minimum distance between sector points must be less than or equal to {self._max_distance_between_sector_points_in_px}.\"\n            )\n\n        # Initialize arrays to store the starting and ending points of the perpendicular bisectors\n        bisector_start_points = np.zeros((n_bisectors, 2))\n        bisector_end_points = np.zeros((n_bisectors, 2))\n\n        # Iterate over the maximum number of iterations\n        for _ in range(max_iterations):\n\n            # Randomly choose n_bisectors pairs of points from the polygon vertices\n            point_indices = np.random.choice(\n                len(polygon), size=n_bisectors * 2, replace=False\n            )\n            points = polygon[point_indices]"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize the list of bisectors\n        bisectors = []\n\n        # Iterate until the required number of bisectors is found\n        for _ in range(self.number_of_bisectors):\n\n            # Initialize the number of iterations\n            iterations = 0\n\n            # Initialize the flag for successful bisector generation\n            bisector_generated = False\n\n            # Iterate until a bisector is generated or the maximum number of iterations is reached\n            while not bisector_generated and iterations < self.max_iterations:\n\n                # Generate two random points from the polygon\n                point1 = polygon[np.random.randint(0, polygon.shape[0])]\n                point2 = polygon[np.random.randint(0, polygon.shape[0])]\n\n                # Check if the distance between the two points is greater than the minimum distance\n                if np.linalg.norm(point1 - point2) > min_distance_between_sector_points_in_px:\n\n                    # Calculate the midpoint between the two points\n                    midpoint = (point1 + point2) / 2\n\n                    # Calculate the direction vector from the midpoint to the first point\n                    direction_vector = point1 - midpoint\n\n                    # Calculate the perpendicular vector to the direction vector\n                    perpendicular_vector = np.array([-direction_vector[1], direction_vector[0]])\n\n                    # Calculate the start and end points of the perpendicular bisector\n                    start_point = midpoint - perpendicular_vector\n                    end_point = midpoint + perpendicular_vector\n\n                    # Check if the perpendicular bisector intersects any of the polygon's edges\n                    if not self._does_perpendicular_bisector_intersect_any_polygon_edge(\n                        polygon, start_point, end_point\n                    ):\n\n                        # If the perpendicular bisector does not intersect any edges, add it to the list of bisectors\n                        bisectors.append((start_point, end_point))\n                        bisector_generated = True\n\n                # Increment the number of iterations\n                iterations += 1\n\n            # If the maximum number of iterations is"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        if min_distance_between_sector_points_in_px <= 0:\n            raise EyeCentersEstimationError(\n                \"min_distance_between_sector_points_in_px must be greater than 0\"\n            )\n\n        if min_distance_between_sector_points_in_px >= np.linalg.norm(\n            polygon[0] - polygon[1]\n        ):\n            raise EyeCentersEstimationError(\n                \"min_distance_between_sector_points_in_px must be smaller than the distance between the first two points in polygon\"\n            )\n\n        if self.n_bisectors <= 0:\n            raise EyeCentersEstimationError(\n                \"n_bisectors must be greater than 0\"\n            )\n\n        if self.max_iterations <= 0:\n            raise EyeCentersEstimationError(\n                \"max_iterations must be greater than 0\"\n            )\n\n        if self.n_bisectors > len(polygon):\n            raise EyeCentersEstimationError(\n                \"n_bisectors must be smaller than or equal to the number of points in polygon\"\n            )\n\n        if self.n_bisectors == len(polygon):\n            return polygon, polygon\n\n        bisectors_start_points = np.zeros((self.n_bisectors, 2))\n        bisectors_end_points = np.zeros((self.n_bisectors, 2))\n\n        for i in range(self.n_bisectors):\n            found_pair = False\n            iteration_count = 0\n            while not found_pair:\n                if iteration_count >= self.max_iterations:\n                    raise EyeCentersEstimationError(\n                        \"Could not find sufficient point pairs with a distance greater than min_distance_between_sector_points_in_px\"\n                    )\n\n                idx = np.random.randint(len(polygon), size=2)\n                point1 = polygon[idx[0]]\n                point2 = polygon[idx[1]]\n\n                if np.linalg"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n        result = self.run(*args, **kwargs)\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, result, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for hook in self._callbacks:\n            hook.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for hook in self._callbacks:\n            hook.post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for hook in self._callbacks:\n            hook.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for hook in self._callbacks:\n            hook.post_execute(self, result, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(self, result, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        # Call the pre-execution hooks\n        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        # Call the main algorithm run method\n        result = self.run(*args, **kwargs)\n\n        # Call the post-execution hooks\n        for callback in self._callbacks:\n            callback.post_execute(self, result, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for hook in self._callbacks:\n            hook.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for hook in self._callbacks:\n            hook.after_execute(self, *args, **kwargs, result=result)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        # Execute pre-execution hooks\n        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        # Execute the main algorithm\n        result = self.run(*args, **kwargs)\n\n        # Execute post-execution hooks\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self._pre_execute_hooks()\n        result = self.run(*args, **kwargs)\n        self._post_execute_hooks()\n        return result\n\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        # Call pre-execution hooks\n        for hook in self._callbacks:\n            hook.pre_execute(self, *args, **kwargs)\n\n        # Call main algorithm run method\n        result = self.run(*args, **kwargs)\n\n        # Call post-execution hooks\n        for hook in self._callbacks:\n            hook.post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self._pre_execute_hooks(*args, **kwargs)\n        result = self.run(*args, **kwargs)\n        self._post_execute_hooks(result, *args, **kwargs)\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self.pre_execute_hooks()\n        result = self.run(*args, **kwargs)\n        self.post_execute_hooks()\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self.pre_execute_hooks()\n        result = self.run(*args, **kwargs)\n        self.post_execute_hooks()\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result\n\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self._pre_execute_hooks()\n        result = self.run(*args, **kwargs)\n        self._post_execute_hooks()\n        return result\n\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, result, *args, **kwargs)\n\n        return result\n\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self._pre_execution()\n        result = self._run(*args, **kwargs)\n        self._post_execution()\n        return result\n\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n        result = self.run(*args, **kwargs)\n        for callback in self._callbacks:\n            callback.post_execute(self, result, *args, **kwargs)\n        return result\n\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except (json.JSONDecodeError, TypeError):\n            return False"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            if not self.check_type(deserialized_output, type_definition):\n                return False\n        except json.JSONDecodeError:\n            return False\n        return True"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output_json = json.loads(output)\n            return self.check_type(output_json, type_definition)\n        except json.JSONDecodeError:\n            return False"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = self.deserialize(output)\n            return self.check_type(deserialized_output, type_definition)\n        except:\n            return False"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = self.deserialize_output(output)\n            return self.check_type(deserialized_output, type_definition)\n        except Exception as e:\n            return False"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output_object = json.loads(output)\n            if self.check_type(output_object, type_definition):\n                return True\n            else:\n                return False\n        except json.JSONDecodeError:\n            return False"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output = json.loads(output)\n            if self.check_type(output, type_definition):\n                return True\n        except Exception:\n            pass\n        return False"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output_deserialized = json.loads(output)\n            if not self.check_type(output_deserialized, type_definition):\n                return False\n            return True\n        except Exception:\n            return False"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError as e:\n            self.logger.error(f\"Error while deserializing output: {e}\")\n            return False\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = self.deserialize(output)\n            return self.check_type(deserialized_output, type_definition)\n        except Exception as e:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = self.deserialize_output(output)\n            return self.check_type(deserialized_output, type_definition)\n        except Exception:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = self.deserialize(output)\n            return self.check_type(deserialized_output, type_definition)\n        except Exception:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output = json.loads(output)\n        except:\n            return False\n        return self.check_type(output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output_json = json.loads(output)\n            return self.check_type(output_json, type_definition)\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except (json.JSONDecodeError, TypeError):\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = self.deserialize(output)\n            return self.check_type(deserialized_output, type_definition)\n        except Exception:\n            return False\n\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = self.deserialize(output)\n            if self.check_type(deserialized_output, type_definition):\n                return True\n            else:\n                return False\n        except Exception as e:\n            print(f\"Error while validating output: {e}\")\n            return False\n\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output_data = json.loads(output)\n            return self.check_type(output_data, type_definition)\n        except Exception:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except Exception as e:\n            logging.error(\"Error occurred while validating output: {}\".format(str(e)))\n            return False\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        input_type_hints = {\n            param: type_hints[param]\n            for param in signature.parameters\n            if param != \"self\"\n        }\n        output_type_hints = type_hints[\"return\"]\n\n        input_class_definitions = {\n            param: get_class_definition(type_hints[param])\n            for param in input_type_hints\n        }\n        output_class_definition = get_class_definition(output_type_hints)\n\n        if issubclass(output_type_hints, Union):\n            for type_hint in output_type_hints.__args__:\n                if issubclass(type_hint, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                    output_class_definition = get_class_definition(type_hint)\n                    break\n            else:\n                function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = (\n                FunctionType.EMBEDDABLE\n                if issubclass(output_type_hints, Embedding)\n                else FunctionType.SYMBOLIC\n            )\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {\n            param.name: param.annotation\n            for param in signature.parameters.values()\n        }\n        output_type_hints = type_hints.get(\"return\", None)\n\n        input_class_definitions = {\n            param_name: get_class_definition(param_type)\n            for param_name, param_type in input_type_hints.items()\n        }\n\n        output_class_definition = (\n            get_class_definition(output_type_hints)\n            if output_type_hints is not None\n            else None\n        )\n\n        if output_class_definition is not None:\n            if issubclass(output_class_definition, Union):\n                output_class_definition = output_class_definition.__args__[0]\n\n            if issubclass(output_class_definition, Embedding):\n                function_type = FunctionType.SYMBOLIC\n            else:\n                function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_type_hints=input_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_type_hints=output_type_hints,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hints = {\n            param: hints[param]\n            for param in signature.parameters\n            if param != \"self\"\n        }\n        output_type_hints = hints[\"return\"]\n\n        input_class_definitions = {\n            param: get_class_definition(hints[param])\n            for param in input_type_hints\n        }\n        output_class_definition = get_class_definition(output_type_hints)\n\n        function_type = FunctionType.SYMBOLIC\n        if issubclass(output_type_hints, Union):\n            for subtype in output_type_hints.__args__:\n                if issubclass(subtype, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                    output_class_definition = get_class_definition(subtype)\n                    break\n        elif issubclass(output_type_hints, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        func_signature = inspect.signature(func_object)\n        func_type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        input_class_definitions = {}\n        output_class_definitions = {}\n\n        for parameter_name, parameter_type_hint in func_type_hints.items():\n            if parameter_name in func_signature.parameters:\n                input_type_hints[parameter_name] = parameter_type_hint\n                input_class_definitions[parameter_name] = get_class_definition(parameter_type_hint)\n            else:\n                output_type_hints[parameter_name] = parameter_type_hint\n                output_class_definitions[parameter_name] = get_class_definition(parameter_type_hint)\n\n        output_type_hint = func_type_hints.get(\"return\", None)\n        output_class_definition = get_class_definition(output_type_hint)\n\n        if isinstance(output_type_hint, type) and issubclass(output_type_hint, Union):\n            output_class_definition = output_type_hint.__args__[0]\n            output_type_hint = output_type_hint.__args__[0]\n\n        if isinstance(output_type_hint, type) and issubclass(output_type_hint, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif isinstance(output_type_hint, type) and issubclass(output_type_hint, Union):\n            function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.annotation != inspect._empty:\n                input_type_hints[param_name] = param.annotation\n\n        if signature.return_annotation != inspect._empty:\n            output_type_hints = signature.return_annotation\n\n        input_class_definitions = {\n            name: get_class_definition(type_hint)\n            for name, type_hint in input_type_hints.items()\n        }\n\n        output_class_definitions = {}\n\n        if isinstance(output_type_hints, Union):\n            output_type_hints = output_type_hints.__args__\n\n        if issubclass(output_type_hints, Embedding):\n            output_class_definitions = get_class_definition(output_type_hints)\n        else:\n            for type_hint in output_type_hints:\n                if issubclass(type_hint, Embedding):\n                    output_class_definitions = get_class_definition(type_hint)\n                    break\n\n        function_type = (\n            FunctionType.SYMBOLIC\n            if output_class_definitions\n            else FunctionType.EMBEDDABLE\n        )\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definitions=output_class_definitions,\n            function_type=function_type,\n        )"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.annotation != inspect.Parameter.empty:\n                input_type_hints[param_name] = param.annotation\n\n        if signature.return_annotation != inspect.Signature.empty:\n            output_type_hints = signature.return_annotation\n\n        input_class_definitions = {}\n        output_class_definitions = {}\n\n        for param_name, param_type in input_type_hints.items():\n            input_class_definitions[param_name] = get_class_definition(param_type)\n\n        if isinstance(output_type_hints, Union):\n            output_type_hints = output_type_hints.__args__\n\n        if issubclass(output_type_hints, Embedding):\n            output_class_definitions = output_type_hints\n        else:\n            output_class_definitions = get_class_definition(output_type_hints)\n\n        function_type = (\n            FunctionType.SYMBOLIC\n            if issubclass(output_class_definitions, Embedding)\n            else FunctionType.EMBEDDABLE\n        )\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definitions=output_class_definitions,\n            function_type=function_type,\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_hints = {}\n        output_hints = {}\n        input_classes = {}\n        output_classes = {}\n\n        for name, hint in type_hints.items():\n            if name in signature.parameters:\n                input_hints[name] = hint\n                input_classes[name] = get_class_definition(hint)\n            else:\n                output_hints[name] = hint\n                output_classes[name] = get_class_definition(hint)\n\n        output_type = output_hints.get(\"return\")\n        output_class = output_classes.get(\"return\")\n\n        if isinstance(output_type, type) and issubclass(output_type, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif isinstance(output_type, Union) and any(\n            issubclass(t, Embedding) for t in output_type.__args__\n        ):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_hints=input_hints,\n            output_hints=output_hints,\n            input_classes=input_classes,\n            output_classes=output_classes,\n            function_type=function_type,\n        )"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        output_class_definition = None\n        function_type = FunctionType.SYMBOLIC\n\n        for param_name, param in signature.parameters.items():\n            if param.annotation == inspect.Parameter.empty:\n                continue\n\n            if param.kind == inspect.Parameter.VAR_POSITIONAL or param.kind == inspect.Parameter.VAR_KEYWORD:\n                continue\n\n            if param_name == \"self\":\n                continue\n\n            input_type_hints[param_name] = param.annotation\n\n        if \"return\" in type_hints:\n            output_type_hints = type_hints[\"return\"]\n\n            if isinstance(output_type_hints, type):\n                output_class_definition = get_class_definition(output_type_hints)\n            elif isinstance(output_type_hints, Union):\n                for output_type_hint in output_type_hints.__args__:\n                    if issubclass(output_type_hint, Embedding):\n                        output_class_definition = get_class_definition(output_type_hint)\n                        function_type = FunctionType.EMBEDDABLE\n                        break\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        sig = inspect.signature(func_object)\n        hints = get_type_hints(func_object)\n        input_hints = {\n            name: hints[name]\n            for name in sig.parameters\n            if name not in hints\n        }\n        output_hint = hints.get(\"return\", None)\n        input_classes = {\n            name: get_class_definition(hint)\n            for name, hint in input_hints.items()\n        }\n        output_class = get_class_definition(output_hint)\n        function_type = (\n            FunctionType.SYMBOLIC\n            if issubclass(output_hint, Embedding)\n            else FunctionType.EMBEDDABLE\n        )\n        if isinstance(output_hint, Union):\n            for hint in output_hint.__args__:\n                if issubclass(hint, Embedding):\n                    function_type = FunctionType.SYMBOLIC\n                    output_class = get_class_definition(hint)\n                    break\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_hints=input_hints,\n            input_classes=input_classes,\n            output_hint=output_hint,\n            output_class=output_class,\n            function_type=function_type,\n        )"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param in signature.parameters.items():\n            if param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:\n                if param_name in type_hints:\n                    input_type_hints[param_name] = type_hints[param_name]\n\n        if len(signature.parameters) == 1:\n            output_type_hints = type_hints\n        else:\n            output_type_hints = {\n                \"return\": type_hints[\"return\"]\n            }\n\n        input_class_definitions = {}\n        for param_name, param_type in input_type_hints.items():\n            input_class_definitions[param_name] = get_class_definition(param_type)\n\n        output_class_definition = None\n        if \"return\" in output_type_hints:\n            output_class_definition = get_class_definition(output_type_hints[\"return\"])\n\n        function_type = FunctionType.SYMBOLIC\n        if output_class_definition:\n            if issubclass(output_class_definition, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n            elif issubclass(output_class_definition, Union):\n                for subclass in output_class_definition.__args__:\n                    if issubclass(subclass, Embedding):\n                        function_type = FunctionType.EMBEDDABLE\n                        output_class_definition = subclass\n                        break\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_type_hints=input_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_type_hints=output_type_hints,\n            output_class_definition=output_class_definition,\n           "}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        input_type_hints = {\n            param: type_hints[param]\n            for param in signature.parameters\n            if param != \"self\"\n        }\n        output_type_hints = type_hints.get(\"return\", None)\n        docstring = func_object.__doc__\n\n        input_class_definitions = {\n            param: get_class_definition(type_hints[param])\n            for param in signature.parameters\n            if param != \"self\"\n        }\n        output_class_definition = get_class_definition(output_type_hints)\n\n        if output_type_hints is not None:\n            if issubclass(output_type_hints, Union):\n                output_class_definition = get_class_definition(\n                    output_type_hints.__args__[0]\n                )\n                function_type = FunctionType.EMBEDDABLE\n            elif issubclass(output_type_hints, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n            else:\n                function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n        )\n\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param_name, param_type_hint in signature.parameters.items():\n            if param_name in type_hints:\n                input_type_hints[param_name] = type_hints[param_name]\n\n        if \"return\" in type_hints:\n            output_type_hints[\"return\"] = type_hints[\"return\"]\n\n        input_class_definitions = {}\n        output_class_definitions = {}\n\n        for param_name, param_type_hint in input_type_hints.items():\n            input_class_definitions[param_name] = get_class_definition(param_type_hint)\n\n        for param_name, param_type_hint in output_type_hints.items():\n            output_class_definitions[param_name] = get_class_definition(param_type_hint)\n\n        if \"return\" in output_type_hints:\n            output_type_hint = output_type_hints[\"return\"]\n            if is_subclass(output_type_hint, Union):\n                output_type_hint = output_type_hint.__args__[0]\n                output_class_definitions[\"return\"] = get_class_definition(output_type_hint)\n            elif is_subclass(output_type_hint, Embedding):\n                output_type = output_type_hint.__args__[0]\n                output_class_definitions[\"return\"] = get_class_definition(output_type)\n                function_type = FunctionType.EMBEDDABLE\n            else:\n                function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n           "}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        func_signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Get input and output type hints\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param in func_signature.parameters.items():\n            if param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:\n                input_type_hints[param_name] = type_hints.get(param_name)\n            elif param.kind == inspect.Parameter.VAR_POSITIONAL:\n                input_type_hints[param_name] = type_hints.get(param_name)\n            elif param.kind == inspect.Parameter.VAR_KEYWORD:\n                input_type_hints[param_name] = type_hints.get(param_name)\n\n        output_type_hint = type_hints.get(\"return\")\n        if isinstance(output_type_hint, Union):\n            for type_hint in output_type_hint.__args__:\n                if issubclass(type_hint, Embedding):\n                    output_type_hints[\"output\"] = type_hint\n                    break\n        else:\n            if issubclass(output_type_hint, Embedding):\n                output_type_hints[\"output\"] = output_type_hint\n\n        # Get class definitions for input and output types\n        input_class_definitions = {}\n        output_class_definitions = {}\n        for param_name, param_type in input_type_hints.items():\n            input_class_definitions[param_name] = get_class_definition(param_type)\n        for output_name, output_type in output_type_hints.items():\n            output_class_definitions[output_name] = get_class_definition(output_type)\n\n        # Determine function type\n        function_type = FunctionType.SYMBOLIC\n        if output_type_hints.get(\"output\"):\n            if issubclass(output_type_"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        input_class_definitions = {}\n        output_class_definitions = {}\n\n        for parameter_name, parameter in signature.parameters.items():\n            if parameter.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:\n                input_type_hints[parameter_name] = type_hints.get(parameter_name, None)\n                input_class_definitions[parameter_name] = get_class_definition(input_type_hints[parameter_name])\n\n        output_type_hints = type_hints.get(\"return\", None)\n        output_class_definitions = get_class_definition(output_type_hints)\n\n        if output_type_hints is not None and isinstance(output_type_hints, type) and issubclass(output_type_hints, Union):\n            if len(output_type_hints.__args__) == 2:\n                if issubclass(output_type_hints.__args__[1], Embedding):\n                    function_type = FunctionType.SYMBOLIC\n                    output_class_definitions = output_type_hints.__args__[1]\n                else:\n                    function_type = FunctionType.EMBEDDABLE\n                    output_class_definitions = output_type_hints.__args__[0]\n            else:\n                function_type = FunctionType.EMBEDDABLE\n                output_class_definitions = output_type_hints.__args__[0]\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n           "}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = typing.get_type_hints(func_object)\n\n        def get_class_definition(type_hint: typing.Type[typing.Any]) -> typing.Optional[str]:\n            if typing.get_origin(type_hint) is not None:\n                origin = typing.get_origin(type_hint)\n                args = typing.get_args(type_hint)\n                if origin in (list, tuple, set, frozenset):\n                    return f\"{origin.__name__}[{','.join(get_class_definition(arg) for arg in args)}]\"\n                elif origin in (dict, collections.abc.Mapping):\n                    return f\"{origin.__name__}[{get_class_definition(args[0])},{get_class_definition(args[1])}]\"\n                elif origin in (collections.abc.Sequence, collections.abc.Iterable):\n                    return f\"{origin.__name__}[{get_class_definition(args[0])}]\"\n                elif origin in (collections.abc.Container, collections.abc.Collection):\n                    return f\"{origin.__name__}[{get_class_definition(args[0])}]\"\n                elif origin in (collections.abc.Hashable, collections.abc.Sized):\n                    return f\"{origin.__name__}\"\n                elif origin in (collections.abc.Callable, collections.abc.Generator):\n                    return f\"{origin.__name__}\"\n                elif origin in (collections.abc.Awaitable, collections.abc.Coroutine):\n                    return f\"{origin.__name__}\"\n                elif origin in (collections.abc.AsyncIterable, collections.abc.AsyncIterator):\n                    return f\"{origin.__name__}\"\n                elif origin in (collections.abc.AsyncGenerator, collections.abc.AsyncContextManager):\n                    return f\"{origin.__name__}\"\n                elif origin in (collections.abc.AsyncGenerator, collections.abc.AsyncContextManager):\n                    return f\"{origin.__name__}\"\n                elif origin in (collections"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_hints = {\n            name: hint\n            for name, hint in signature.parameters.items()\n            if name not in type_hints\n        }\n\n        output_hints = type_hints.get(\"return\", None)\n\n        input_class_defs = {\n            name: get_class_definition(hint)\n            for name, hint in input_hints.items()\n        }\n\n        output_class_def = get_class_definition(output_hints)\n\n        if output_hints is not None:\n            if issubclass(output_hints, Union):\n                function_type = FunctionType.SYMBOLIC\n                output_class_def = get_class_definition(output_hints.__args__[0])\n            elif issubclass(output_hints, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n            else:\n                function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_hints=input_hints,\n            output_hints=output_hints,\n            input_class_defs=input_class_defs,\n            output_class_def=output_class_def,\n            function_type=function_type,\n        )"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        def get_class_definition(type_hint: Type[Any]) -> Optional[str]:\n            if is_generic_type(type_hint):\n                return get_class_definition(get_origin(type_hint))\n            if is_type_var(type_hint):\n                return get_class_definition(get_origin(type_hint))\n            if is_union(type_hint):\n                return get_class_definition(get_args(type_hint)[0])\n            if is_class(type_hint):\n                return type_hint.__module__ + \".\" + type_hint.__name__\n            return None\n\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        input_type_hints = {\n            name: type_hint\n            for name, type_hint in type_hints.items()\n            if name in signature.parameters\n        }\n        output_type_hint = type_hints.get(\"return\")\n        input_class_definitions = {\n            name: get_class_definition(type_hint)\n            for name, type_hint in input_type_hints.items()\n        }\n        output_class_definition = get_class_definition(output_type_hint)\n        if is_subclass(output_type_hint, Union):\n            output_class_definition = get_class_definition(get_args(output_type_hint)[0])\n        function_type = (\n            FunctionType.SYMBOLIC\n            if output_class_definition is None\n            or is_subclass(output_type_hint, Embedding)\n            else FunctionType.EMBEDDABLE\n        )\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_type_hints=input_type_hints,\n            output_type_hint=output_type_hint,\n            input_class_definitions=input_class_"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param in signature.parameters.items():\n            if param.annotation == inspect.Parameter.empty:\n                continue\n            if param.kind == param.VAR_KEYWORD:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.POSITIONAL_ONLY:\n                continue\n\n            if param_name in type_hints:\n                type_hint = type_hints[param_name]\n            else:\n                type_hint = param.annotation\n\n            if param.kind == param.POSITIONAL_OR_KEYWORD:\n                input_type_hints[param_name] = type_hint\n            elif param.kind == param.KEYWORD_ONLY:\n                input_type_hints[param_name] = type_hint\n\n        output_type_hints = type_hints.get(\"return\", None)\n        if output_type_hints is None:\n            output_type_hints = signature.return_annotation\n\n        input_class_definitions = {}\n        for param_name, param_type in input_type_hints.items():\n            input_class_definitions[param_name] = get_class_definition(\n                param_type\n            )\n\n        output_class_definition = get_class_definition(output_type_hints)\n\n        function_type = FunctionType.SYMBOLIC\n        if issubclass(output_type_hints, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif issubclass(output_type_hints, Union):\n            for subtype in output_type_hints.__args__:\n                if issubclass(subtype, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                    output_class_definition"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = []\n        output_type_hints = []\n\n        for name, param in signature.parameters.items():\n            if param.kind in (param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD):\n                input_type_hints.append(type_hints.get(name, param.annotation))\n\n        if signature.return_annotation is not inspect.Signature.empty:\n            output_type_hints.append(signature.return_annotation)\n\n        input_class_definitions = [\n            get_class_definition(type_hint) for type_hint in input_type_hints\n        ]\n        output_class_definitions = [\n            get_class_definition(type_hint) for type_hint in output_type_hints\n        ]\n\n        function_type = FunctionType.SYMBOLIC\n        output_class_definition = None\n        if is_embeddable(output_type_hints[0]):\n            function_type = FunctionType.EMBEDDABLE\n            output_class_definition = get_class_definition(output_type_hints[0])\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_type_hints=input_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_type_hints=output_type_hints,\n            output_class_definitions=output_class_definitions,\n            function_type=function_type,\n            output_class_definition=output_class_definition,\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        hints = get_type_hints(func_object)\n\n        input_hints = [\n            param.annotation\n            for param in signature.parameters.values()\n            if param.annotation is not inspect.Parameter.empty\n        ]\n\n        output_hints = [\n            hints[param]\n            for param in signature.parameters.values()\n            if param in hints\n        ]\n\n        input_classes = [\n            get_class_definition(hint) for hint in input_hints if hint is not None\n        ]\n\n        output_classes = [\n            get_class_definition(hint) for hint in output_hints if hint is not None\n        ]\n\n        output_class = None\n        function_type = FunctionType.SYMBOLIC\n\n        if len(output_classes) == 1:\n            output_class = output_classes[0]\n\n            if issubclass(output_class, Union):\n                for cls in output_class.__args__:\n                    if issubclass(cls, Embedding):\n                        function_type = FunctionType.EMBEDDABLE\n                        output_class = cls\n                        break\n            elif issubclass(output_class, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_type_hints=input_hints,\n            input_classes=input_classes,\n            output_type_hints=output_hints,\n            output_classes=output_classes,\n            output_class=output_class,\n            function_type=function_type,\n        )\n\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for i in range(self.hash_count):\n            self.bit_array[self.hash_func(string, i) % self.size] = 1\n\n        return None"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for i in range(self.k):\n            index = self.hash_function(string, i)\n            self.bit_array[index] = 1\n        return\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            self.bit_array[index] = 1\n        return None\n    "}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for i in range(self.num_hashes):\n            hash_value = self.hash_functions[i](string) % self.size\n            self.bit_array[hash_value] = 1\n        return\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        # Calculate the hash values for the string using the hash functions\n        hash_values = [self.hash_function_1(string), self.hash_function_2(string)]\n        # Set the bits at the calculated indices in the bit array to 1\n        for hash_value in hash_values:\n            self.bit_array[hash_value % self.bit_array_size] = 1\n    "}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        # Calculate the hash values for the input string\n        hash_values = self.hash_values(string)\n        # Set the bits at the calculated indices in the bit array to 1\n        for hash_value in hash_values:\n            index = hash_value % self.bit_array_size\n            self.bit_array[index] = 1\n    "}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        # Calculate the hash values for the string\n        hash_values = self.hash_functions(string)\n        # Set the bits at the calculated indices in the bit array to 1\n        for hash_value in hash_values:\n            index = hash_value % self.bit_array_size\n            self.bit_array[index] = 1\n        return\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for i in range(self.k):\n            hash_value = self.hash_functions[i](string)\n            index = hash_value % self.m\n            self.bit_array[index] = 1\n        return\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        # Calculate the hash values for the input string\n        hash_values = self.hash_values(string)\n        # Set the bits at the calculated indices in the bit array to 1\n        for hash_value in hash_values:\n            self.bit_array[hash_value % self.size] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        # Calculate the hash values for the string\n        hash_values = self.get_hash_values(string)\n        # Set the bits at the calculated indices in the bit array to 1\n        for i in hash_values:\n            self.bit_array[i] = 1\n        return\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        # Calculate the hash values for the input string\n        hash_values = self.hash_values(string)\n        # Set the corresponding bits in the bit array to 1\n        for hash_value in hash_values:\n            index = hash_value % self.bit_array_size\n            self.bit_array[index] = 1\n    "}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for i in range(self.hash_count):\n            hash_value = self.hash_functions[i](string) % self.size\n            self.bit_array[hash_value] = 1\n        return\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for i in range(self.num_hash_functions):\n            index = self.hash_function(string, i)\n            self.bit_array[index] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        # Calculate the hash values for the given string\n        hash_values = self.hash_values(string)\n        # Set the bits at the calculated indices in the bit array to 1\n        for index in hash_values:\n            self.bit_array[index] = 1\n    "}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        # Calculate the hash values for the input string\n        hash_values = self.hash_functions(string)\n        # Set the corresponding bits in the bit array to 1\n        for hash_value in hash_values:\n            index = hash_value % self.bit_array_size\n            self.bit_array[index] = 1\n    "}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        # Calculate the hash values for the input string\n        hash_values = self.hash_values(string)\n        # Set the bits at the calculated indices in the bit array to 1\n        for hash_value in hash_values:\n            self.bit_array[hash_value] = 1\n        # Increment the number of items in the Bloom filter\n        self.num_items += 1\n        # Check if the Bloom filter is full and resize it if necessary\n        if self.num_items == self.size:\n            self.resize()\n    "}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        # Calculate the hash values for the input string\n        hash_values = self.hash_functions(string)\n        \n        # Set the bits at the calculated indices in the bit array to 1\n        for hash_value in hash_values:\n            index = hash_value % self.size\n            self.bit_array[index] = 1\n    "}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for i in range(self.hash_count):\n            self.bit_array[self.hash_values(string, i)] = 1\n    "}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for i in range(self.k):\n            self.bit_array[self.hash_functions[i](string) % self.m] = 1\n    "}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for i in range(self.k):\n            h = self.hash(string, i)\n            self.bit_array[h % self.m] = 1\n    "}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            bit_array = self.persistence.load()\n            if len(bit_array) != self.size:\n                logging.warning(\"Corrupt bit array detected. Reinitializing.\")\n                self.init_bit_array()\n                self.save()\n        except FileNotFoundError:\n            logging.warning(\"Bit array not found. Initializing new bit array.\")\n            self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        # Load the bit array from persistence\n        bit_array = self.persistence.load()\n        # Check if the loaded bit array's length matches the expected length based on the BloomFilter size\n        if len(bit_array) != self.size:\n            # If there is a mismatch, indicating potential corruption, log a warning and reinitialize the bit array and indices\n            logging.warning(\"Bit array length mismatch. Reinitializing bit array and indices.\")\n            self.init_bit_array()\n            self.save()\n        else:\n            # If there is no mismatch, set the bit array and indices to the loaded state\n            self.bit_array = bit_array\n            self.indices = self.get_indices()\n    "}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if len(bit_array) != self.size:\n            self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if len(bit_array) != self.size:\n            logging.warning(\"BloomFilter: Loaded bit array length does not match expected length. Reinitializing and saving.\")\n            self.bit_array = self.init_bit_array()\n            self.save()\n        else:\n            self.bit_array = bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if len(loaded_bit_array) != self.size:\n            logging.warning(\"Loaded bit array length does not match expected length. Reinitializing bit array.\")\n            self.init_bit_array()\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        # Load the bit array from persistence\n        bit_array = self.persistence.load()\n        # Check if the loaded bit array's length matches the expected length based on the BloomFilter size\n        if len(bit_array) != self.size:\n            # If there is a mismatch, indicating potential corruption, log a warning and reinitialize the bit array and indices\n            logging.warning(\"Loaded bit array length does not match expected length. Reinitializing bit array and indices.\")\n            self.init_bit_array()\n            self.save()\n        else:\n            # If the lengths match, update the bit array and indices with the loaded values\n            self.bit_array = bit_array\n            self.indices = self.generate_indices()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if loaded_bit_array is None:\n            self.init_bit_array()\n            return\n        if len(loaded_bit_array) != self.size:\n            self.init_bit_array()\n            self.save()\n            return\n        self.bit_array = loaded_bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            loaded_bit_array = self.persistence.load()\n            if len(loaded_bit_array) != self.size:\n                self.init_bit_array()\n                self.save()\n        except FileNotFoundError:\n            self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        self.bit_array = self.persistence.load()\n        if len(self.bit_array) != self.size:\n            self.logger.warning(\"Loaded bit array has a different size than expected. Reinitializing bit array.\")\n            self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        # Load the bit array from persistence\n        bit_array = self.persistence.load()\n        \n        # Check if the loaded bit array's length matches the expected length based on the BloomFilter size\n        if len(bit_array) != self.size:\n            # Log a warning indicating potential corruption\n            print(\"Warning: Loaded bit array length does not match expected length. Reinitializing bit array.\")\n            \n            # Reinitialize the bit array and indices\n            self.init_bit_array()\n            self.indices = []\n            \n            # Save the new state\n            self.save()\n        else:\n            # Load the bit array into the BloomFilter instance\n            self.bit_array = bit_array\n            self.indices = []\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        # Load the bit array from persistence\n        bit_array = self.persistence.load()\n        # Check if the loaded bit array's length matches the expected length based on the BloomFilter size\n        if len(bit_array) != self.size:\n            # Log a warning indicating potential corruption\n            logging.warning(\"BloomFilter: Potential corruption detected. Reinitializing bit array and indices.\")\n            # Reinitialize the bit array and indices\n            self.init_bit_array()\n            self.init_indices()\n            # Save the new state\n            self.save()\n        else:\n            # If the lengths match, set the bit array\n            self.bit_array = bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        # Load the bit array from persistence\n        bit_array = self.persistence.load()\n        # Check if the loaded bit array's length matches the expected length based on the BloomFilter size\n        if len(bit_array) != self.size:\n            # If there is a mismatch, indicating potential corruption, log a warning and reinitialize the bit array and indices\n            logging.warning(\"Bit array length mismatch. Reinitializing bit array and indices.\")\n            self.init_bit_array()\n            self.save()\n        else:\n            # If the lengths match, update the BloomFilter instance's bit array and indices with the loaded values\n            self.bit_array = bit_array\n            self.indices = self.get_indices()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            self.bit_array = self.persistence.load()\n            if len(self.bit_array) != self.size:\n                logging.warning(\n                    f\"Loaded bit array has length {len(self.bit_array)}, expected {self.size}\"\n                )\n                self.bit_array = self.init_bit_array()\n                self.save()\n        except FileNotFoundError:\n            self.bit_array = self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        self.bit_array = self.persistence.load()\n        if len(self.bit_array) != self.size:\n            logging.warning(\"Bit array length mismatch. Reinitializing bit array.\")\n            self.bit_array = self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        # Load the bit array from persistence\n        loaded_bit_array = self.persistence.load()\n        # Check if the loaded bit array's length matches the expected length based on the BloomFilter size\n        if len(loaded_bit_array) != self.size:\n            # Log a warning indicating potential corruption\n            logging.warning(\"BloomFilter size mismatch detected. Reinitializing bit array and indices.\")\n            # Reinitialize the bit array and indices\n            self.init_bit_array()\n            self.init_indices()\n            # Save the new state\n            self.save()\n        else:\n            # If the length matches, set the loaded bit array as the current bit array\n            self.bit_array = loaded_bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        self.bit_array = self.persistence.load()\n        if len(self.bit_array) != self.size:\n            logging.warning(\"Bit array size mismatch. Reinitializing bit array.\")\n            self.bit_array = self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            self.bit_array = self.persistence.load()\n            if len(self.bit_array) != self.size:\n                self.logger.warning(\"Bit array length mismatch. Reinitializing bit array.\")\n                self.init_bit_array()\n                self.save()\n        except (FileNotFoundError, EOFError, pickle.UnpicklingError):\n            self.logger.warning(\"Bit array not found or corrupted. Reinitializing bit array.\")\n            self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            bit_array = self.persistence.load()\n            if len(bit_array) != self.size:\n                self.logger.warning(f\"Corruption detected: loaded bit array has length {len(bit_array)} instead of expected {self.size}\")\n                self.init_bit_array()\n                self.save()\n            else:\n                self.bit_array = bit_array\n        except FileNotFoundError:\n            self.logger.warning(\"No persistence found. Initializing new bit array.\")\n            self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if bit_array is not None and len(bit_array) == self.size:\n            self.bit_array = bit_array\n            self.indices = self.init_bit_array()\n        else:\n            self.bit_array = [0] * self.size\n            self.indices = self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            bit_array = self.persistence.load()\n        except FileNotFoundError:\n            bit_array = None\n        if bit_array is None:\n            bit_array = self.init_bit_array()\n        elif len(bit_array) != self.size:\n            logger.warning(\n                f\"Loaded bit array has length {len(bit_array)} which does not match the expected length {self.size}. Reinitializing bit array and indices.\")\n            bit_array = self.init_bit_array()\n        self.bit_array = bit_array\n        self.indices = self.init_indices()\n        self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for hash_function in self.hash_functions:\n            index = hash_function(string, self.size)\n            if not self.bit_array[index]:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for hash_function in self.hash_functions:\n            index = hash_function(string, self.size)\n            if not self.bit_array[index]:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for hash_func in self.hash_functions:\n            index = hash_func(string, self.size)\n            if not self.bit_array[index]:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for hash_func in self.hash_functions:\n            index = hash_func(string, self.size)\n            if not self.bit_array[index]:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for i in range(self.hash_count):\n            index = self.hash_functions[i](string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for hash_function in self.hash_functions:\n            index = hash_function(string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Check if the string is in the Bloom Filter\n        for hash_function in self.hash_functions:\n            index = hash_function(string, self.size)\n            if not self.bit_array[index]:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        if string is None:\n            return False\n        for hash_function in self.hash_functions:\n            index = hash_function(string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for hash_function in self.hash_functions:\n            index = hash_function(string, self.size)\n            if self.bit_array[index] == 0:\n                return False\n        return True\n\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for hash_func in self.hash_functions:\n            index = hash_func(string, self.size)\n            if not self.bit_array[index]:\n                return False\n        return True\n\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for hash_function in self.hash_functions:\n            index = hash_function(string, self.size)\n            if self.bit_array[index] == 0:\n                return False\n        return True\n\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for hash_function in self.hash_functions:\n            index = hash_function(string, self.size)\n            if not self.bit_array[index]:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Check if the input string is empty\n        if not string:\n            return False\n\n        # Generate indices using the hash functions\n        indices = [self.hash_functions[i](string) % self.size for i in range(self.hash_count)]\n\n        # Check if all bits at the generated indices are set in the bit array\n        for index in indices:\n            if not self.bit_array[index]:\n                return False\n\n        return True\n\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Initialize a variable to store the result of the lookup\n        result = True\n\n        # Iterate through the hash functions\n        for hash_function in self.hash_functions:\n            # Calculate the index using the hash function and the string\n            index = hash_function(string, self.size)\n            # Check if the bit at the calculated index is not set\n            if not self.bit_array[index]:\n                # If any bit is not set, set the result to False and break out of the loop\n                result = False\n                break\n\n        # Return the result of the lookup\n        return result"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for hash_function in self.hash_functions:\n            index = hash_function(string, self.size)\n            if not self.bit_array[index]:\n                return False\n        return True\n\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for hash_function in self.hash_functions:\n            index = hash_function(string, self.size)\n            if not self.bit_array[index]:\n                return False\n        return True\n\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for hash_function in self.hash_functions:\n            index = hash_function(string, self.size)\n            if not self.bit_array[index]:\n                return False\n        return True\n\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        for func in self.hash_functions:\n            index = func(string) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n\n\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Initialize a counter to keep track of the number of hash functions that return a non-zero value.\n        counter = 0\n        # Iterate over the hash functions in the instance's hash_functions list.\n        for i in self.hash_functions:\n            # Calculate the index using the hash function and the input string.\n            index = i(string) % self.size\n            # If the bit at the calculated index in the bit array is not set (i.e., 0), return False.\n            if self.bit_array[index] == 0:\n                return False\n            # Increment the counter.\n            counter += 1\n        # If the counter is equal to the number of hash functions, return True.\n        if counter == self.hash_count:\n            return True\n        # If the counter is less than the number of hash functions, return False.\n        else:\n            return False"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Initialize a variable to keep track of the number of bits set to 1 in the bit array\n        bits_set = 0\n\n        # Iterate through the hash functions and generate indices using the hash function and the input string\n        for hash_function in self.hash_functions:\n            index = hash_function(string, self.size)\n            # Check if the bit at the generated index is set to 1\n            if self.bit_array[index] == 1:\n                # If the bit is set to 1, increment the bits_set variable\n                bits_set += 1\n\n        # If all bits at the generated indices are set to 1, return True\n        if bits_set == self.hash_count:\n            return True\n        else:\n            # If any bit at the generated indices is not set to 1, return False\n            return False\n\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict.get('distilled_model', None)\n        self.current_model_stats = json_dict.get('current_model_stats', None)\n        self.last_training_run = json_dict.get('last_training_run', None)\n        self.current_training_run = json_dict.get('current_training_run', None)\n        self.nr_of_training_runs = json_dict.get('nr_of_training_runs', None)\n        self.teacher_models = json_dict.get('teacher_models', None)\n\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict.get('distilled_model', None)\n        self.current_model_stats = json_dict.get('current_model_stats', None)\n        self.last_training_run = json_dict.get('last_training_run', None)\n        self.current_training_run = json_dict.get('current_training_run', None)\n        self.nr_of_training_runs = json_dict.get('nr_of_training_runs', None)\n        self.teacher_models = json_dict.get('teacher_models', None)\n\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if 'distilled_model' in json_dict:\n            self.distilled_model = json_dict['distilled_model']\n        if 'current_model_stats' in json_dict:\n            self.current_model_stats = json_dict['current_model_stats']\n        if 'last_training_run' in json_dict:\n            self.last_training_run = json_dict['last_training_run']\n        if 'current_training_run' in json_dict:\n            self.current_training_run = json_dict['current_training_run']\n        if 'nr_of_training_runs' in json_dict:\n            self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if 'distilled_model' in json_dict:\n            self.distilled_model = json_dict['distilled_model']\n\n        if 'current_model_stats' in json_dict:\n            self.current_model_stats = json_dict['current_model_stats']\n\n        if 'last_training_run' in json_dict:\n            self.last_training_run = json_dict['last_training_run']\n\n        if 'current_training_run' in json_dict:\n            self.current_training_run = json_dict['current_training_run']\n\n        if 'nr_of_training_runs' in json_dict:\n            self.nr_of_training_runs = json_dict['nr_of_training_runs']\n\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict.get(\"distilled_model\", None)\n        self.current_model_stats = json_dict.get(\"current_model_stats\", None)\n        self.last_training_run = json_dict.get(\"last_training_run\", None)\n        self.current_training_run = json_dict.get(\"current_training_run\", None)\n        self.nr_of_training_runs = json_dict.get(\"nr_of_training_runs\", None)\n        self.teacher_models = json_dict.get(\"teacher_models\", None)\n\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        else:\n            self.teacher_models = []"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        else:\n            self.teacher_models = None\n\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        else:\n            self.teacher_models = []\n\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if 'distilled_model' in json_dict:\n            self.distilled_model = json_dict['distilled_model']\n        if 'current_model_stats' in json_dict:\n            self.current_model_stats = json_dict['current_model_stats']\n        if 'last_training_run' in json_dict:\n            self.last_training_run = json_dict['last_training_run']\n        if 'current_training_run' in json_dict:\n            self.current_training_run = json_dict['current_training_run']\n        if 'nr_of_training_runs' in json_dict:\n            self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        else:\n            self.teacher_models = None\n\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict.get('distilled_model', None)\n        self.current_model_stats = json_dict.get('current_model_stats', None)\n        self.last_training_run = json_dict.get('last_training_run', None)\n        self.current_training_run = json_dict.get('current_training_run', None)\n        self.nr_of_training_runs = json_dict.get('nr_of_training_runs', 0)\n        self.teacher_models = json_dict.get('teacher_models', [])"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict.get('distilled_model', None)\n        self.current_model_stats = json_dict.get('current_model_stats', None)\n        self.last_training_run = json_dict.get('last_training_run', None)\n        self.current_training_run = json_dict.get('current_training_run', None)\n        self.nr_of_training_runs = json_dict.get('nr_of_training_runs', None)\n        self.teacher_models = json_dict.get('teacher_models', None)\n\n        return self\n\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict.get('distilled_model', None)\n        self.current_model_stats = json_dict.get('current_model_stats', None)\n        self.last_training_run = json_dict.get('last_training_run', None)\n        self.current_training_run = json_dict.get('current_training_run', None)\n        self.nr_of_training_runs = json_dict.get('nr_of_training_runs', None)\n        self.teacher_models = json_dict.get('teacher_models', None)\n\n        return self\n\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict['distilled_model']\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self\n\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if json_dict is None:\n            return self\n\n        if 'distilled_model' in json_dict:\n            self.distilled_model = json_dict['distilled_model']\n        if 'current_model_stats' in json_dict:\n            self.current_model_stats = json_dict['current_model_stats']\n        if 'last_training_run' in json_dict:\n            self.last_training_run = json_dict['last_training_run']\n        if 'current_training_run' in json_dict:\n            self.current_training_run = json_dict['current_training_run']\n        if 'nr_of_training_runs' in json_dict:\n            self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = json_dict['teacher_models']\n        return self\n\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify that the API key is set\n        if not self.api_key:\n            raise ValueError(\"OpenAI API key is not set\")\n\n        # Verify that the model name is set\n        if not model.name:\n            raise ValueError(\"Model name is not set\")\n\n        # Verify that the system message is not empty\n        if not system_message:\n            raise ValueError(\"System message is empty\")\n\n        # Verify that the prompt is not empty\n        if not prompt:\n            raise ValueError(\"Prompt is empty\")\n\n        # Verify that the max_new_tokens is not less than 1\n        if \"max_new_tokens\" in kwargs and kwargs[\"max_new_tokens\"] < 1:\n            raise ValueError(\"max_new_tokens must be greater than 0\")\n\n        # Verify that the temperature is not less than 0 or greater than 2\n        if \"temperature\" in kwargs and (kwargs[\"temperature\"] < 0 or kwargs[\"temperature\"] > 2):\n            raise ValueError(\"temperature must be between 0 and 2\")\n\n        # Verify that the top_p is not less than 0 or greater than 1\n        if \"top_p\" in kwargs and (kwargs[\"top_p\"] < 0 or kwargs[\"top_p\"] > 1):\n            raise ValueError(\"top_p must be between 0 and 1\")\n\n        # Verify that the frequency_penalty is not less than -2 or greater than 2\n        if \"frequency_penalty\" in kwargs and (kwargs[\"frequency_penalty\"] < -2 or kwargs[\"frequency_penalty\"] > 2):\n            raise ValueError(\"frequency_penalty must be between -2 and 2\")\n\n        # Verify that the presence_penalty is not less than -2 or greater than 2\n        if \"presence_penalty\" in kwargs and (kwargs[\"presence_penalty\"] < -2 or kwargs[\"presence_penalty\"] > 2):\n            raise ValueError(\"presence_penal"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if not self.api_key:\n            raise ValueError(\"API key is not set\")\n\n        # Validate parameters\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration\")\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message\")\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt\")\n\n        # Set default parameters\n        if \"temperature\" not in kwargs:\n            kwargs[\"temperature\"] = 0.7\n        if \"top_p\" not in kwargs:\n            kwargs[\"top_p\"] = 1\n        if \"frequency_penalty\" not in kwargs:\n            kwargs[\"frequency_penalty\"] = 0\n        if \"presence_penalty\" not in kwargs:\n            kwargs[\"presence_penalty\"] = 0\n        if \"max_new_tokens\" not in kwargs:\n            kwargs[\"max_new_tokens\"] = 150\n\n        # Set headers and data\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n        data = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            **kwargs\n        }\n\n        # Send request and handle retries\n        for i in range(5):\n            try:\n                response = requests.post(self.api_url, headers=headers, data=json.dumps(data))\n                response.raise_for_status()\n                break\n            except requests.exceptions.RequestException as e:\n                if i < 4:\n                    time.sleep(2 ** i)\n                else:\n                    raise e\n\n        # Process response\n        response_json = response.json()\n        text = response_json[\"choices\"][0][\"message\"][\"content\"]"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if self.api_key is None:\n            raise ValueError(\"API key is not set\")\n\n        # Validate input parameters\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"model must be an instance of OpenAIConfig\")\n        if not isinstance(system_message, str):\n            raise ValueError(\"system_message must be a string\")\n        if not isinstance(prompt, str):\n            raise ValueError(\"prompt must be a string\")\n        if not isinstance(kwargs, dict):\n            raise ValueError(\"kwargs must be a dictionary\")\n\n        # Set default values for optional parameters\n        kwargs.setdefault(\"temperature\", 0.5)\n        kwargs.setdefault(\"top_p\", 1)\n        kwargs.setdefault(\"frequency_penalty\", 0)\n        kwargs.setdefault(\"presence_penalty\", 0)\n        kwargs.setdefault(\"max_new_tokens\", 200)\n\n        # Set the API endpoint URL\n        url = \"https://api.openai.com/v1/chat/completions\"\n\n        # Set the API request headers\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n\n        # Set the API request data\n        data = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            \"temperature\": kwargs.get(\"temperature\"),\n            \"top_p\": kwargs.get(\"top_p\"),\n            \"frequency_penalty\": kwargs.get(\"frequency_penalty\"),\n            \"presence_penalty\": kwargs.get(\"presence_penalty\"),\n            \"max_new_tokens\": kwargs.get(\"max_new_tokens\")\n        }\n\n        # Set the number of retries and the initial backoff time"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if not self.api_key:\n            raise ValueError(\"API key not set\")\n\n        # Validate input parameters\n        if not isinstance(model, OpenAIConfig):\n            raise TypeError(\"Invalid model type. Expected OpenAIConfig.\")\n        if not isinstance(system_message, str):\n            raise TypeError(\"Invalid system_message type. Expected str.\")\n        if not isinstance(prompt, str):\n            raise TypeError(\"Invalid prompt type. Expected str.\")\n        if not isinstance(kwargs, dict):\n            raise TypeError(\"Invalid kwargs type. Expected dict.\")\n\n        # Set default values for optional parameters\n        kwargs.setdefault(\"temperature\", 0.7)\n        kwargs.setdefault(\"top_p\", 1)\n        kwargs.setdefault(\"frequency_penalty\", 0)\n        kwargs.setdefault(\"presence_penalty\", 0)\n        kwargs.setdefault(\"max_new_tokens\", 100)\n\n        # Create the request body\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            **kwargs\n        }\n\n        # Retry up to 5 times with exponential backoff\n        for attempt in range(5):\n            try:\n                # Send the request to the OpenAI API\n                response = requests.post(\n                    \"https://api.openai.com/v1/chat/completions\",\n                    headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                    json=request_body\n                )\n\n                # Check the response status code\n                response.raise_for_status()\n\n                # Parse the response and extract the generated text\n                response_data = response.json()\n                generated_text = response_data[\"choices\"][0][\"message\"][\"content\"]\n\n                # Remove any parsing helper tokens from the generated text\n                generated_text ="}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        self.verify_api_key()\n\n        # Validate model\n        self.validate_model(model)\n\n        # Validate system message\n        self.validate_system_message(system_message)\n\n        # Validate prompt\n        self.validate_prompt(prompt)\n\n        # Validate kwargs\n        self.validate_kwargs(kwargs)\n\n        # Build request\n        request = self.build_request(model, system_message, prompt, **kwargs)\n\n        # Retry up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                # Send request to OpenAI API\n                response = self.send_request(request)\n\n                # Process response\n                text = self.process_response(response, model)\n\n                # Return final text\n                return text\n\n            except Exception as e:\n                # Log error\n                self.logger.error(f\"Error generating response: {e}\")\n\n                # Sleep for exponential backoff\n                sleep_time = 2 ** i\n                self.logger.info(f\"Retrying in {sleep_time} seconds...\")\n                time.sleep(sleep_time)\n\n        # If all retries fail, raise error\n        raise Exception(\"Failed to generate response after 5 retries\")"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if not self.api_key:\n            raise ValueError(\"API key is not set. Please set the API key using the set_api_key() method.\")\n\n        # Validate model\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model. Please provide a valid OpenAIConfig object.\")\n\n        # Validate system_message\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system_message. Please provide a valid string.\")\n\n        # Validate prompt\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt. Please provide a valid string.\")\n\n        # Validate kwargs\n        if not isinstance(kwargs, dict):\n            raise ValueError(\"Invalid kwargs. Please provide a valid dictionary.\")\n\n        # Set default values for kwargs\n        kwargs = {\n            \"temperature\": kwargs.get(\"temperature\", 0.7),\n            \"top_p\": kwargs.get(\"top_p\", 1.0),\n            \"frequency_penalty\": kwargs.get(\"frequency_penalty\", 0.0),\n            \"presence_penalty\": kwargs.get(\"presence_penalty\", 0.0),\n            \"max_new_tokens\": kwargs.get(\"max_new_tokens\", 100),\n        }\n\n        # Set default values for model\n        model.temperature = kwargs.get(\"temperature\", model.temperature)\n        model.top_p = kwargs.get(\"top_p\", model.top_p)\n        model.frequency_penalty = kwargs.get(\"frequency_penalty\", model.frequency_penalty)\n        model.presence_penalty = kwargs.get(\"presence_penalty\", model.presence_penalty)\n        model.max_new_tokens = kwargs.get(\"max_new_tokens\", model.max_new_tokens)\n\n        # Set the model name\n        model.model_name = model"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if not self.api_key:\n            raise ValueError(\"API key not set\")\n\n        # Validate input parameters\n        if not model:\n            raise ValueError(\"Model not specified\")\n        if not system_message:\n            raise ValueError(\"System message not specified\")\n        if not prompt:\n            raise ValueError(\"Prompt not specified\")\n\n        # Set default values for optional parameters\n        if \"temperature\" not in kwargs:\n            kwargs[\"temperature\"] = 0.7\n        if \"top_p\" not in kwargs:\n            kwargs[\"top_p\"] = 1\n        if \"frequency_penalty\" not in kwargs:\n            kwargs[\"frequency_penalty\"] = 0\n        if \"presence_penalty\" not in kwargs:\n            kwargs[\"presence_penalty\"] = 0\n        if \"max_new_tokens\" not in kwargs:\n            kwargs[\"max_new_tokens\"] = 100\n\n        # Retry up to 5 times with exponential backoff\n        for attempt in range(5):\n            try:\n                # Make API request\n                response = openai.Completion.create(\n                    engine=model.name,\n                    prompt=system_message + prompt,\n                    temperature=kwargs[\"temperature\"],\n                    top_p=kwargs[\"top_p\"],\n                    frequency_penalty=kwargs[\"frequency_penalty\"],\n                    presence_penalty=kwargs[\"presence_penalty\"],\n                    max_tokens=kwargs[\"max_new_tokens\"],\n                    stop=model.stop,\n                    n=1,\n                    stream=False,\n                )\n\n                # Process response and remove any parsing helper tokens\n                text = response.choices[0].text\n                text = self.remove_parsing_helper_tokens(text, model.parsing_helper_tokens)\n\n                return text\n\n            except Exception as e:\n                if attempt == 4:\n                    raise e\n                else:\n                    time.sleep(2 ** attempt)\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        self.verify_api_key()\n\n        # Validate parameters\n        self.validate_parameters(model, system_message, prompt, **kwargs)\n\n        # Create request body\n        request_body = {\n            \"model\": model.name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            \"temperature\": kwargs.get(\"temperature\", 0.7),\n            \"top_p\": kwargs.get(\"top_p\", 1),\n            \"frequency_penalty\": kwargs.get(\"frequency_penalty\", 0),\n            \"presence_penalty\": kwargs.get(\"presence_penalty\", 0),\n            \"max_new_tokens\": kwargs.get(\"max_new_tokens\", 100),\n            \"stop\": kwargs.get(\"stop\", model.stop_sequence)\n        }\n\n        # Send request and handle retries\n        for i in range(5):\n            try:\n                response = requests.post(\n                    \"https://api.openai.com/v1/chat/completions\",\n                    headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                    json=request_body\n                )\n                response.raise_for_status()\n                break\n            except requests.exceptions.RequestException as e:\n                if i < 4:\n                    time.sleep(2 ** i)\n                else:\n                    raise e\n\n        # Process response\n        response_json = response.json()\n        generated_text = response_json[\"choices\"][0][\"message\"][\"content\"]\n\n        # Remove parsing helper tokens\n        generated_text = self.remove_parsing_helper_tokens(\n            generated_text, model.parsing_helper_tokens)\n\n        return generated_text"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if self.api_key is None:\n            raise ValueError(\"API key not set\")\n\n        # Validate parameters\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration\")\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message\")\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt\")\n\n        # Set default parameters\n        kwargs.setdefault(\"temperature\", 0.7)\n        kwargs.setdefault(\"top_p\", 1)\n        kwargs.setdefault(\"frequency_penalty\", 0)\n        kwargs.setdefault(\"presence_penalty\", 0)\n        kwargs.setdefault(\"max_new_tokens\", 256)\n\n        # Set headers\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n\n        # Set API endpoint\n        url = f\"{self.api_base_url}/v1/engines/{model.model_name}/completions\"\n\n        # Set request data\n        data = {\n            \"prompt\": system_message + prompt,\n            \"max_tokens\": kwargs[\"max_new_tokens\"],\n            \"temperature\": kwargs[\"temperature\"],\n            \"top_p\": kwargs[\"top_p\"],\n            \"frequency_penalty\": kwargs[\"frequency_penalty\"],\n            \"presence_penalty\": kwargs[\"presence_penalty\"],\n            \"stop\": model.stop_tokens,\n        }\n\n        # Set retry parameters\n        max_retries = 5\n        retry_delay = 1\n\n        # Send request with retries\n        for i in range(max_retries):\n            try:\n                response = requests.post(url, headers=headers, json=data)\n                response.raise_for_status()\n                break\n            except requests.exceptions.RequestException as e:\n                if i == max_retries - 1:\n                    raise e"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        self._verify_api_key()\n\n        # Validate model\n        self._validate_model(model)\n\n        # Validate system message\n        self._validate_system_message(system_message)\n\n        # Validate prompt\n        self._validate_prompt(prompt)\n\n        # Validate kwargs\n        self._validate_kwargs(kwargs)\n\n        # Construct request body\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        }\n\n        # Add kwargs to request body\n        request_body.update(kwargs)\n\n        # Retry on failure up to 5 times with exponential backoff\n        for attempt in range(5):\n            try:\n                # Send request to OpenAI API\n                response = self._send_request(request_body)\n\n                # Process response\n                text = self._process_response(response, model)\n\n                # Return final text\n                return text\n\n            except Exception as e:\n                # Print error message\n                print(f\"Error: {e}\")\n\n                # Sleep for exponential backoff\n                time.sleep(2 ** attempt)\n\n        # Raise exception if all attempts failed\n        raise Exception(\"Failed to generate response after 5 attempts.\")"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if not self.api_key:\n            raise ValueError(\"API key not set\")\n\n        # Validate parameters\n        if not isinstance(model, OpenAIConfig):\n            raise TypeError(\"model must be an instance of OpenAIConfig\")\n        if not isinstance(system_message, str):\n            raise TypeError(\"system_message must be a string\")\n        if not isinstance(prompt, str):\n            raise TypeError(\"prompt must be a string\")\n        if not isinstance(kwargs, dict):\n            raise TypeError(\"kwargs must be a dictionary\")\n\n        # Set default values for optional parameters\n        kwargs.setdefault(\"temperature\", 0.7)\n        kwargs.setdefault(\"top_p\", 1)\n        kwargs.setdefault(\"frequency_penalty\", 0)\n        kwargs.setdefault(\"presence_penalty\", 0)\n        kwargs.setdefault(\"max_new_tokens\", 150)\n\n        # Retry up to 5 times with exponential backoff\n        for attempt in range(5):\n            try:\n                # Send request to OpenAI API\n                response = openai.ChatCompletion.create(\n                    model=model.model_name,\n                    messages=[\n                        {\"role\": \"system\", \"content\": system_message},\n                        {\"role\": \"user\", \"content\": prompt},\n                    ],\n                    **kwargs,\n                )\n\n                # Process response to remove parsing helper tokens\n                text = response.choices[0].message.content\n                text = self._remove_parsing_helper_tokens(text, model)\n\n                # Return final text\n                return text\n\n            except Exception as e:\n                # Handle API errors\n                if attempt < 4:\n                    # Retry with exponential backoff\n                    wait_time = (2 ** attempt) * 0.5\n                    time.sleep(wait_time)\n                else:\n                    # Raise exception after 5 attempts\n                    raise e\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if not self.api_key:\n            raise ValueError(\"API key not set\")\n\n        # Validate parameters\n        for key, value in kwargs.items():\n            if key not in self.valid_parameters:\n                raise ValueError(f\"Invalid parameter: {key}\")\n\n        # Set default values for parameters\n        kwargs.setdefault(\"temperature\", 0.7)\n        kwargs.setdefault(\"top_p\", 1.0)\n        kwargs.setdefault(\"frequency_penalty\", 0.0)\n        kwargs.setdefault(\"presence_penalty\", 0.0)\n        kwargs.setdefault(\"max_new_tokens\", 100)\n\n        # Construct the request\n        request = {\n            \"model\": model.name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            **kwargs\n        }\n\n        # Send the request to the OpenAI API\n        for attempt in range(5):\n            try:\n                response = self.openai.ChatCompletion.create(\n                    model=model.name,\n                    messages=[\n                        {\"role\": \"system\", \"content\": system_message},\n                        {\"role\": \"user\", \"content\": prompt}\n                    ],\n                    **kwargs\n                )\n                break\n            except Exception as e:\n                if attempt == 4:\n                    raise e\n                time.sleep(2 ** attempt)\n\n        # Process the response\n        generated_text = response.choices[0].message.content\n        if model.parsing_helper_tokens:\n            generated_text = self.remove_parsing_helper_tokens(generated_text, model.parsing_helper_tokens)\n\n        return generated_text"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if not self.api_key:\n            raise ValueError(\"API key not set\")\n\n        # Validate input parameters\n        if not model:\n            raise ValueError(\"Model not provided\")\n        if not system_message:\n            raise ValueError(\"System message not provided\")\n        if not prompt:\n            raise ValueError(\"Prompt not provided\")\n\n        # Set default parameters\n        kwargs.setdefault(\"temperature\", 0.7)\n        kwargs.setdefault(\"top_p\", 1.0)\n        kwargs.setdefault(\"frequency_penalty\", 0.0)\n        kwargs.setdefault(\"presence_penalty\", 0.0)\n        kwargs.setdefault(\"max_new_tokens\", 100)\n\n        # Set up API request\n        request_data = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n        }\n        request_data.update(kwargs)\n\n        # Send API request and handle retries\n        for i in range(5):\n            try:\n                response = requests.post(\n                    f\"{self.api_base_url}/v1/chat/completions\",\n                    headers={\n                        \"Content-Type\": \"application/json\",\n                        \"Authorization\": f\"Bearer {self.api_key}\",\n                    },\n                    json=request_data,\n                    timeout=10,\n                )\n                response.raise_for_status()\n                break\n            except requests.exceptions.RequestException as e:\n                if i < 4:\n                    sleep_time = (2 ** i) * random.uniform(0.5, 1.5)\n                    time.sleep(sleep_time)\n                else:\n                    raise e\n\n        # Process response\n        response_data = response.json()\n        generated_text = response_data[\"choices\"][0][\"message\"][\"content\"]\n\n        # Remove any parsing"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if not self.api_key:\n            raise ValueError(\"OpenAI API key not set\")\n\n        # Validate parameters\n        if not model or not system_message or not prompt:\n            raise ValueError(\"Missing required parameters\")\n\n        # Set default parameters\n        if \"temperature\" not in kwargs:\n            kwargs[\"temperature\"] = 0.7\n        if \"top_p\" not in kwargs:\n            kwargs[\"top_p\"] = 1\n        if \"frequency_penalty\" not in kwargs:\n            kwargs[\"frequency_penalty\"] = 0\n        if \"presence_penalty\" not in kwargs:\n            kwargs[\"presence_penalty\"] = 0\n        if \"max_new_tokens\" not in kwargs:\n            kwargs[\"max_new_tokens\"] = 100\n\n        # Set up API request\n        request_data = {\n            \"model\": model.name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        }\n        request_data.update(kwargs)\n\n        # Set up API request headers\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n\n        # Retry up to 5 times with exponential backoff\n        for attempt in range(5):\n            try:\n                # Make API request\n                response = requests.post(\n                    f\"{self.api_base_url}/v1/chat/completions\",\n                    headers=headers,\n                    json=request_data\n                )\n\n                # Check response status code\n                if response.status_code == 200:\n                    # Process response\n                    response_data = response.json()\n                    response_text = response_data[\"choices\"][0][\"message\"][\"content\"]\n\n                    # Remove parsing helper tokens\n                    response_text = self.remove_parsing_helper_tokens("}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if not self.api_key:\n            raise ValueError(\"API key is not set\")\n\n        # Validate parameters\n        if not model:\n            raise ValueError(\"Model is not set\")\n        if not system_message:\n            raise ValueError(\"System message is not set\")\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # Set default values for optional parameters\n        kwargs.setdefault(\"temperature\", 0.7)\n        kwargs.setdefault(\"top_p\", 1)\n        kwargs.setdefault(\"frequency_penalty\", 0)\n        kwargs.setdefault(\"presence_penalty\", 0)\n        kwargs.setdefault(\"max_new_tokens\", 100)\n\n        # Create the request body\n        request_body = {\n            \"model\": model.name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            \"temperature\": kwargs.get(\"temperature\"),\n            \"top_p\": kwargs.get(\"top_p\"),\n            \"frequency_penalty\": kwargs.get(\"frequency_penalty\"),\n            \"presence_penalty\": kwargs.get(\"presence_penalty\"),\n            \"max_new_tokens\": kwargs.get(\"max_new_tokens\"),\n            \"stream\": False\n        }\n\n        # Retry up to 5 times with exponential backoff\n        for attempt in range(5):\n            try:\n                # Send the request to the OpenAI API\n                response = self.request(\"POST\", \"/v1/chat/completions\", request_body)\n                # Process the response and remove any parsing helper tokens\n                response_text = self.process_response(response, model.parsing_helper_tokens)\n                return response_text\n            except Exception as e:\n                # Handle any errors\n                print(f\"Error generating response: {e}\")\n                # Exponential backoff"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify that the API key is valid and accessible\n        self.verify_api_key()\n\n        # Validate the input parameters\n        self.validate_input_parameters(model, system_message, prompt, **kwargs)\n\n        # Prepare the generation request\n        request_data = {\n            \"model\": model.name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        }\n\n        # Add any additional parameters to the request\n        for key, value in kwargs.items():\n            request_data[key] = value\n\n        # Set the maximum number of retries for the generation request\n        max_retries = 5\n\n        # Set the initial backoff time for exponential backoff\n        backoff_time = 1\n\n        # Initialize the response variable\n        response = None\n\n        # Retry the generation request up to the maximum number of retries with exponential backoff\n        for i in range(max_retries):\n            try:\n                # Send the generation request to the OpenAI API\n                response = self.openai_api.chat(request_data)\n\n                # If the response is not None, break the loop\n                if response is not None:\n                    break\n\n            except Exception as e:\n                # If there is an exception, wait for the backoff time and then try again\n                time.sleep(backoff_time)\n                backoff_time *= 2\n\n        # If the response is still None after all retries, raise an exception\n        if response is None:\n            raise Exception(\"Failed to generate response after {} retries\".format(max_retries))\n\n        # Process the response to remove any parsing helper tokens\n        response_text = self.process_response(response, model)\n\n        # Return the final text response\n        return response_text"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if not self.api_key:\n            raise ValueError(\"API key is not set\")\n\n        # Validate model configuration\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration\")\n\n        # Validate system message\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message\")\n\n        # Validate prompt\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt\")\n\n        # Validate kwargs\n        if not isinstance(kwargs, dict):\n            raise ValueError(\"Invalid kwargs\")\n\n        # Set default kwargs\n        default_kwargs = {\n            \"temperature\": 0.7,\n            \"max_tokens\": 200,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n            \"stop\": [model.parse_helper_token],\n        }\n        default_kwargs.update(kwargs)\n\n        # Set headers\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n\n        # Set data\n        data = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n        }\n        data.update(default_kwargs)\n\n        # Set retries\n        retries = 5\n\n        # Set backoff factor\n        backoff_factor = 2\n\n        # Set initial delay\n        delay = 1\n\n        # Set initial response\n        response = None\n\n        # Set initial error\n        error = None\n\n        # Loop through retries\n        for i in range(retries):\n\n            # Try to generate response\n            try:\n\n                # Make request\n                response = requests.post(\n                    f\"{self.base_url}/chat/completions\",\n                    headers=headers,\n                    json=data,\n                    timeout=self.timeout,\n                )"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if self.api_key is None:\n            raise ValueError(\"OpenAI API key not set. Please set the `api_key` property before calling `generate`.\")\n\n        # Validate parameters\n        valid_parameters = self.get_valid_parameters(model)\n        for key, value in kwargs.items():\n            if key not in valid_parameters:\n                raise ValueError(f\"Invalid parameter: {key}\")\n\n        # Set default parameters if not provided\n        for key, value in valid_parameters.items():\n            if key not in kwargs:\n                kwargs[key] = value\n\n        # Create request\n        request = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            **kwargs\n        }\n\n        # Retry on failure up to 5 times with exponential backoff\n        for attempt in range(5):\n            try:\n                response = openai.ChatCompletion.create(**request)\n                break\n            except Exception as e:\n                if attempt == 4:\n                    raise e\n                else:\n                    time.sleep(2 ** attempt)\n\n        # Process response\n        response_text = response[\"choices\"][0][\"message\"][\"content\"]\n\n        # Remove parsing helper tokens\n        if model.parsing_helper_tokens is not None:\n            response_text = self.remove_parsing_helper_tokens(response_text, model.parsing_helper_tokens)\n\n        return response_text\n\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        self.verify_api_key()\n\n        # Validate parameters\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration. Expected OpenAIConfig object.\")\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message. Expected string.\")\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt. Expected string.\")\n\n        # Set default parameters\n        kwargs.setdefault(\"temperature\", 0.7)\n        kwargs.setdefault(\"top_p\", 1.0)\n        kwargs.setdefault(\"frequency_penalty\", 0.0)\n        kwargs.setdefault(\"presence_penalty\", 0.0)\n        kwargs.setdefault(\"max_new_tokens\", 256)\n\n        # Set the API endpoint\n        api_endpoint = self.api_base_url + \"/v1/chat/completions\"\n\n        # Set the request headers\n        request_headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": \"Bearer \" + self.api_key\n        }\n\n        # Set the request body\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            **kwargs\n        }\n\n        # Set the number of retries\n        num_retries = 5\n\n        # Set the initial retry delay\n        retry_delay = 1\n\n        # Set the maximum retry delay\n        max_retry_delay = 10\n\n        # Set the retry backoff factor\n        backoff_factor = 2\n\n        # Set the maximum number of retries\n        max_retries = 5\n\n        # Initialize the response\n        response = None\n\n        # Initialize the retry count\n        retry_count = 0\n\n        # Set the retry flag\n        retry ="}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify that the API key is set\n        if not self.api_key:\n            raise ValueError(\"API key is not set\")\n\n        # Validate the input parameters\n        if not model:\n            raise ValueError(\"Model is not set\")\n        if not system_message:\n            raise ValueError(\"System message is not set\")\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # Set the base parameters for the generation request\n        params = {\n            \"model\": model.model_name,\n            \"messages\": [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": prompt}],\n        }\n\n        # Add any additional parameters to the request\n        for key, value in kwargs.items():\n            params[key] = value\n\n        # Set the initial number of retries and delay\n        retries = 0\n        delay = 1\n\n        # Retry up to 5 times with exponential backoff\n        while retries < 5:\n            try:\n                # Make the generation request\n                response = openai.ChatCompletion.create(**params)\n\n                # Process the response to remove any parsing helper tokens\n                generated_text = self._remove_parsing_helper_tokens(response, model)\n\n                # Return the final text\n                return generated_text\n\n            except openai.error.APIError as e:\n                # Handle API errors\n                print(f\"OpenAI API error: {e}\")\n                if e.http_status == 429:\n                    # Retry with exponential backoff\n                    print(f\"Retrying in {delay} seconds...\")\n                    time.sleep(delay)\n                    delay *= 2\n                    retries += 1\n                else:\n                    # Raise the exception if it's not a rate limit error\n                    raise e\n\n        # Raise an exception if the maximum number of retries is exceeded\n        raise Exception(\"Maximum number of retries exceeded\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n    # Check if the matrix is equal to its transpose\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if not x.shape[0] == x.shape[1]:\n        raise ValueError(\"x is not square\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"x is not symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n    # Check if the matrix is equal to its transpose\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n    # Check if the matrix is equal to its transpose\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Input matrix is not square.\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Input matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square.\")\n\n    # Check if the matrix is equal to its transpose\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square.\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\n            f\"Matrix must be square. Got shape {x.shape} instead of (n, n).\"\n        )\n\n    if not np.allclose(x, x.T):\n        raise ValueError(f\"Matrix is not symmetric. Got {x} instead of symmetric matrix.\")\n\n    return None\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if not x.shape[0] == x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2:\n        raise ValueError(\"Input matrix must be 2-dimensional\")\n\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Input matrix must be square\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Input matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if not x.shape[0] == x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if not x.shape[0] == x.shape[1]:\n        raise ValueError(\"The matrix is not square\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2:\n        raise ValueError(\n            \"assert_is_symmetric: input must be a 2-dimensional numpy array.\"\n        )\n\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\n            \"assert_is_symmetric: input must be a square matrix.\"\n        )\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\n            \"assert_is_symmetric: input must be a symmetric matrix.\"\n        )\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(f\"Matrix is not square. Shape is {x.shape}\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements are not close to zero\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert x.shape[0] == x.shape[1]\n    assert np.allclose(x, x.T)\n    assert np.allclose(np.diag(x), 0)\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")\n\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"Diagonal is not zero\")\n\n    return None"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n    # Check if the matrix is symmetric\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")\n\n    # Check if the diagonal elements are close to zero\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"Diagonal elements are not close to zero\")\n\n    return None\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert x.shape[0] == x.shape[1], \"The given matrix is not a square matrix!\"\n    assert np.allclose(x, x.T, atol=1e-8), \"The given matrix is not symmetric!\"\n    assert np.allclose(np.diag(x), 0, atol=1e-8), \"The diagonal elements of the given matrix are not close to zero!\"\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not np.allclose(x, x.T):\n        raise ValueError(\"Input matrix is not symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"Input matrix diagonal is not zero\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The given matrix is not a square matrix.\")\n\n    if not np.allclose(x, x.T, atol=1e-8):\n        raise ValueError(\"The given matrix is not symmetric.\")\n\n    if not np.allclose(np.diag(x), 0, atol=1e-8):\n        raise ValueError(\"The diagonal elements of the given matrix are not close to zero.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")\n\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"Diagonal elements are not close to zero\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not np.allclose(x, x.T, atol=1e-8):\n        raise ValueError(\"The input matrix is not symmetric.\")\n\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0]), atol=1e-8):\n        raise ValueError(\"The diagonal elements of the input matrix are not close to zero.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not np.allclose(x, x.T, atol=1e-8):\n        raise ValueError(\"Input matrix is not symmetric\")\n    if not np.allclose(np.diag(x), 0, atol=1e-8):\n        raise ValueError(\"Input matrix diagonal is not zero\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not np.allclose(x, x.T):\n        raise ValueError(\"Input matrix is not symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"Diagonal elements are not close to zero\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input must be a numpy array\")\n\n    if not x.ndim == 2:\n        raise ValueError(\"Input must be a 2D array\")\n\n    if not x.shape[0] == x.shape[1]:\n        raise ValueError(\"Input must be a square matrix\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Input must be a symmetric matrix\")\n\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"Diagonal elements of input must be close to zero\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The given matrix is not a square matrix.\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The given matrix is not symmetric.\")\n\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements of the given matrix are not close to zero.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not np.allclose(x, x.T):\n        raise ValueError('The given matrix is not symmetric.')\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError('The diagonal elements are not close to zero.')\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not isinstance(x, np.ndarray):\n        raise TypeError(\"The given matrix is not a numpy array.\")\n\n    if x.ndim != 2:\n        raise ValueError(\"The given matrix is not a two-dimensional matrix.\")\n\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The given matrix is not a square matrix.\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The given matrix is not a symmetric matrix.\")\n\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0])):\n        raise ValueError(\"The diagonal elements of the given matrix are not close to zero.\")\n\n    return None\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square.\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric.\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"Diagonal elements are not close to zero.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not np.allclose(x, x.T, atol=1e-8):\n        raise ValueError(\"Input matrix is not symmetric\")\n    if not np.allclose(np.diag(x), 0, atol=1e-8):\n        raise ValueError(\"Diagonal of input matrix is not zero\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(f\"The input matrix is not a square matrix: {x.shape}\")\n\n    if not np.allclose(x, x.T, rtol=1e-05, atol=1e-08):\n        raise ValueError(\"The input matrix is not symmetric\")\n\n    if not np.allclose(np.diag(x), 0, rtol=1e-05, atol=1e-08):\n        raise ValueError(\"The diagonal elements of the input matrix are not close to zero\")\n\n    return\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not np.allclose(x, x.T):\n        raise ValueError(\"The given matrix is not symmetric.\")\n\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements are not close to zero.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not np.allclose(x, x.T):\n        raise ValueError(\"The given matrix is not a distance matrix\")\n\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The given matrix is not a distance matrix\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Get the model and prompt for the function\n        model_name = function_description.model_name\n        prompt = function_description.prompt\n        if model_name is None:\n            # If no model name is provided, use the default model\n            model_name = self.default_model_name\n        if prompt is None:\n            # If no prompt is provided, use the default prompt\n            prompt = self.default_prompt\n        # Get the model and prompt for the function\n        model_name = function_description.model_name\n        prompt = function_description.prompt\n        if model_name is None:\n            # If no model name is provided, use the default model\n            model_name = self.default_model_name\n        if prompt is None:\n            # If no prompt is provided, use the default prompt\n            prompt = self.default_prompt\n        # Get the model and prompt for the function\n        model_name = function_description.model_name\n        prompt = function_description.prompt\n        if model_name is None:\n            # If no model name is provided, use the default model\n            model_name = self.default_model_name\n        if prompt is None:\n            # If no prompt is provided, use the default prompt\n            prompt = self.default_prompt\n        # Get the model and prompt for the function\n        model_name = function_description.model_name\n        prompt = function_description.prompt\n        if model_name is None:\n            # If no model name is provided, use the default model\n            model_name = self.default_model_name\n        if prompt is None:\n            # If no prompt is provided, use the default prompt\n            prompt = self.default_prompt\n        # Get the model and prompt for the function\n        model_name = function_description.model_name\n        prompt = function_description.prompt\n        if model_name is None:\n            # If no model name is provided, use the default model\n            model_name = self.default_model_name\n        if prompt is None:\n            # If no prompt is provided, use the default prompt\n            prompt = self.default_prompt"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        model = function_description.model\n        is_distillation_suitable = function_description.is_distillation_suitable\n        is_initialized = function_description.is_initialized\n        is_suitable_for_fine_tuning = function_description.is_suitable_for_fine_tuning\n        is_suitable_for_zero_shot = function_description.is_suitable_for_zero_shot\n        is_suitable_for_distillation = function_description.is_suitable_for_distillation\n        is_suitable_for_zero_shot = function_description.is_suitable_for_zero_shot\n        is_suitable_for_fine_tuning = function_description.is_suitable_for_fine_tuning\n        is_suitable_for_distillation = function_description.is_suitable_for_distillation\n        is_suitable_for_zero_shot = function_description.is_suitable_for_zero_shot\n        is_suitable_for_fine_tuning = function_description.is_suitable_for_fine_tuning\n        is_suitable_for_distillation = function_description.is_suitable_for_distillation\n        is_suitable_for_zero_shot = function_description.is_suitable_for_zero_shot\n        is_suitable_for_fine_tuning = function_description.is_suitable_for_fine_tuning\n        is_suitable_for_distillation = function_description.is_suitable_for_distillation\n        is_suitable_for_zero_shot = function_description.is_suitable_for_zero_shot\n        is_suitable_for_fine_tuning = function_description.is_suitable_for_fine_tuning\n        is_suitable_for_distillation = function_description.is_suitable_for"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            # If the function is already initialized, check if it needs to be saved for fine-tuning\n            if self.initialized_functions[func_hash][\"needs_saving\"]:\n                # If the function needs to be saved for fine-tuning, update the examples\n                self.update_examples(func_hash, args, kwargs)\n            # Return the prompt, model, and distillation flag\n            return self.initialized_functions[func_hash][\"prompt\"], self.initialized_functions[func_hash][\"model\"], self.initialized_functions[func_hash][\"distillation_flag\"], self.initialized_functions[func_hash][\"needs_saving\"]\n        else:\n            # If the function is not already initialized, initialize it\n            self.initialize_function(func_hash, function_description, args, kwargs)\n            # Return the prompt, model, and distillation flag\n            return self.initialized_functions[func_hash][\"prompt\"], self.initialized_functions[func_hash][\"model\"], self.initialized_functions[func_hash][\"distillation_flag\"], self.initialized_functions[func_hash][\"needs_saving\"]\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if not self.is_initialized(function_description, func_hash):\n            # If not, initialize the function\n            self.initialize_function(function_description, func_hash)\n            # Return a prompt for zero-shot prompting\n            return self.get_zero_shot_prompt(function_description, args, kwargs, llm_parameters, func_hash)\n        else:\n            # If the function is already initialized, check if the examples need to be updated\n            if self.is_examples_updated(function_description, func_hash):\n                # If the examples need to be updated, update them\n                self.update_examples(function_description, args, kwargs, func_hash)\n            # Check if the model is suitable for distillation\n            if self.is_model_suitable_for_distillation(function_description, func_hash):\n                # If the model is suitable for distillation, return a prompt for zero-shot prompting\n                return self.get_zero_shot_prompt(function_description, args, kwargs, llm_parameters, func_hash)\n            else:\n                # If the model is not suitable for distillation, return a prompt for fine-tuning\n                return self.get_fine_tuning_prompt(function_description, args, kwargs, llm_parameters, func_hash)"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        if func_hash not in self.function_data:\n            self.function_data[func_hash] = {\n                \"model\": None,\n                \"is_suitable_for_distillation\": None,\n                \"is_initialized\": False,\n                \"examples\": None\n            }\n\n        if not self.function_data[func_hash][\"is_initialized\"]:\n            self.function_data[func_hash][\"model\"] = self.get_model(function_description)\n            self.function_data[func_hash][\"is_suitable_for_distillation\"] = self.is_suitable_for_distillation(\n                self.function_data[func_hash][\"model\"])\n            self.function_data[func_hash][\"is_initialized\"] = True\n\n        if self.function_data[func_hash][\"is_suitable_for_distillation\"]:\n            prompt = self.get_distillation_prompt(function_description, args, kwargs)\n        else:\n            if self.function_data[func_hash][\"examples\"] is None:\n                self.function_data[func_hash][\"examples\"] = self.get_examples(function_description, args, kwargs)\n            else:\n                self.function_data[func_hash][\"examples\"] = self.update_examples(function_description, args, kwargs,\n                                                                                self.function_data[func_hash][\"examples\"])\n            prompt = self.get_fine_tuning_prompt(function_description, args, kwargs,\n                                                 self.function_data[func_hash][\"examples\"])\n\n        return prompt, self.function_data[func_hash][\"model\"], self.function_data[func_hash][\"is_suitable_for_distillation\"], self.function_data[func_hash][\"is_initialized\"]\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        if function_description.model_name in self.model_name_to_model_description:\n            model_description = self.model_name_to_model_description[function_description.model_name]\n        else:\n            model_description = self.model_name_to_model_description[function_description.model_name] = ModelDescription(\n                model_name=function_description.model_name,\n                model_type=function_description.model_type,\n                model_class=function_description.model_class,\n                model_kwargs=function_description.model_kwargs,\n                model_description=function_description.model_description,\n                model_description_kwargs=function_description.model_description_kwargs,\n                model_description_kwargs_for_llm=function_description.model_description_kwargs_for_llm,\n                model_description_kwargs_for_llm_for_prompt=function_description.model_description_kwargs_for_llm_for_prompt,\n                model_description_kwargs_for_llm_for_prompt_for_llm=function_description.model_description_kwargs_for_llm_for_prompt_for_llm,\n                model_description_kwargs_for_llm_for_prompt_for_llm_for_prompt=function_description.model_description_kwargs_for_llm_for_prompt_for_llm_for_prompt,\n                model_description_kwargs_for_llm_for_prompt_for_llm_for_prompt_for_llm=function_description.model_description_kwargs_for_llm_for_prompt_for_llm_for_prompt_for_llm,\n                model_description_kwargs_for_llm_for_prompt_for_llm_for_prompt_for_llm_for_prompt=function_description.model_description_kwargs_for_llm_for_prompt_for_llm_for_prompt_for_llm_for_prompt,\n                model_description_kwargs_for_ll"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            # If the function is already initialized, check if it requires saving examples for fine-tuning\n            if func_hash in self.functions_require_saving_examples:\n                # If the function requires saving examples for fine-tuning, update the examples for fine-tuning\n                self.update_examples_for_fine_tuning(func_hash, args, kwargs)\n            # Return the prompt, the selected model, and a flag indicating that the model is suitable for distillation\n            return self.prompts[func_hash], self.models[func_hash], True, True\n\n        # If the function is not initialized, initialize it\n        self.initialize_function(func_hash, function_description)\n\n        # Check if the function requires saving examples for fine-tuning\n        if func_hash in self.functions_require_saving_examples:\n            # If the function requires saving examples for fine-tuning, update the examples for fine-tuning\n            self.update_examples_for_fine_tuning(func_hash, args, kwargs)\n\n        # Return the prompt, the selected model, and a flag indicating that the model is suitable for distillation\n        return self.prompts[func_hash], self.models[func_hash], True, False\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        if not self.is_initialized(func_hash):\n            self.initialize(function_description, func_hash)\n            return self.get_initialization_case(args, kwargs, function_description, llm_parameters, func_hash)\n        else:\n            if self.is_suitable_for_distillation(func_hash):\n                return self.get_distillation_case(args, kwargs, function_description, llm_parameters, func_hash)\n            else:\n                if self.is_suitable_for_fine_tuning(func_hash):\n                    self.update_examples_for_fine_tuning(args, kwargs, function_description, func_hash)\n                    return self.get_fine_tuning_case(args, kwargs, function_description, llm_parameters, func_hash)\n                else:\n                    return self.get_teacher_case(args, kwargs, function_description, llm_parameters, func_hash)\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if func_hash in self.initialized_functions:\n            return self.prompt_constructor.construct_prompt(args, kwargs, function_description, llm_parameters), self.get_model(function_description), True, True\n\n        # Check if the function is suitable for distillation\n        is_suitable_for_distillation = self.prompt_constructor.is_suitable_for_distillation(args, kwargs, function_description)\n\n        # Initialize the function if it is not already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialize_function(args, kwargs, function_description, func_hash)\n\n        # Update examples for fine-tuning if necessary\n        if not self.prompt_constructor.is_suitable_for_distillation(args, kwargs, function_description):\n            self.update_examples_for_fine_tuning(args, kwargs, function_description, func_hash)\n\n        # Construct the prompt to be used for generation\n        prompt = self.prompt_constructor.construct_prompt(args, kwargs, function_description, llm_parameters)\n\n        # Select the model to be used for generation\n        model = self.get_model(function_description)\n\n        return prompt, model, is_suitable_for_distillation, False\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Get the model and prompt for the function description\n        model = self.get_model(function_description)\n        prompt = self.get_prompt(function_description)\n        # Check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # If not, initialize the function\n            self.initialize_function(function_description)\n        # Check if the function requires saving examples for fine-tuning\n        if func_hash in self.functions_to_save_examples:\n            # If so, update the examples for fine-tuning\n            self.update_examples_for_fine_tuning(func_hash, args, kwargs)\n        # Determine if the model is suitable for distillation\n        if self.is_model_suitable_for_distillation(model):\n            # If so, use a distilled model\n            model = self.get_distilled_model(model)\n            # Construct the prompt for distillation\n            prompt = self.construct_distillation_prompt(prompt)\n            # Return the prompt, model, and distillation flag\n            return prompt, model, True, False\n        else:\n            # If not, use a teacher model\n            model = self.get_teacher_model(model)\n            # Construct the prompt for generation\n            prompt = self.construct_generation_prompt(prompt, args, kwargs)\n            # Return the prompt, model, and distillation flag\n            return prompt, model, False, False"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if function has been initialized\n        if func_hash not in self.initialized_functions:\n            # Initialize function\n            self.initialize_function(function_description, func_hash)\n\n        # Check if function is suitable for distillation\n        if self.is_suitable_for_distillation(function_description, func_hash):\n            # Use distilled model\n            model = self.distilled_model\n            prompt = self.get_distillation_prompt(function_description, args, kwargs, func_hash)\n            is_suitable_for_distillation = True\n            is_initialized = True\n        else:\n            # Use teacher model\n            model = self.teacher_model\n            prompt = self.get_teacher_prompt(function_description, args, kwargs, func_hash)\n            is_suitable_for_distillation = False\n            is_initialized = False\n\n        return prompt, model, is_suitable_for_distillation, is_initialized\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        if func_hash not in self.function_data:\n            self.initialize_function_data(function_description)\n        if self.function_data[func_hash][\"is_distillable\"] and self.function_data[func_hash][\"is_fine_tunable\"]:\n            if self.function_data[func_hash][\"examples\"] is None:\n                self.function_data[func_hash][\"examples\"] = self.get_function_examples(function_description, args, kwargs)\n            self.function_data[func_hash][\"examples\"] = self.update_function_examples(function_description, args, kwargs, self.function_data[func_hash][\"examples\"])\n            prompt = self.construct_prompt(function_description, args, kwargs, self.function_data[func_hash][\"examples\"], llm_parameters)\n            return prompt, self.function_data[func_hash][\"model\"], self.function_data[func_hash][\"is_distillable\"], False\n        elif not self.function_data[func_hash][\"is_distillable\"] and self.function_data[func_hash][\"is_fine_tunable\"]:\n            prompt = self.construct_prompt(function_description, args, kwargs, None, llm_parameters)\n            return prompt, self.function_data[func_hash][\"model\"], self.function_data[func_hash][\"is_distillable\"], False\n        else:\n            prompt = self.construct_prompt(function_description, args, kwargs, None, llm_parameters)\n            return prompt, self.function_data[func_hash][\"model\"], self.function_data[func_hash][\"is_distillable\"], True\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialize_function(function_description, func_hash)\n            return self.get_generation_case(args, kwargs, function_description, llm_parameters, func_hash)\n        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if func_hash in self.initialized_functions and self.initialized_functions[func_hash][\"examples\"] is None:\n            return self.get_generation_case(args, kwargs, function_description, llm_parameters, func_hash)\n        # Check if the function is suitable for distillation\n        if self.initialized_functions[func_hash][\"suitable_for_distillation\"]:\n            # Use a distilled model for zero-shot prompting\n            model = self.distilled_model\n            prompt = self.get_prompt_for_distillation(function_description, args, kwargs)\n            return prompt, model, True, False\n        else:\n            # Use a teacher model for fine-tuning\n            model = self.teacher_model\n            prompt = self.get_prompt_for_fine_tuning(function_description, args, kwargs)\n            return prompt, model, False, False\n    "}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Get the language model and prompt for the function\n        model = self.get_model(function_description)\n        prompt = self.get_prompt(function_description, args, kwargs)\n        # Check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            # If the function is already initialized, check if examples need to be updated\n            if self.initialized_functions[func_hash][\"examples\"] != self.examples[func_hash]:\n                # If examples need to be updated, update them and save the new examples\n                self.initialized_functions[func_hash][\"examples\"] = self.examples[func_hash]\n                self.save_examples(func_hash)\n        else:\n            # If the function is not initialized, initialize it and save the examples\n            self.initialized_functions[func_hash] = {\"examples\": self.examples[func_hash]}\n            self.save_examples(func_hash)\n        # Determine if the model is suitable for distillation\n        is_suitable_for_distillation = self.is_suitable_for_distillation(model, prompt)\n        # Determine if the function is already initialized and does not require saving examples for fine-tuning\n        is_initialized = func_hash in self.initialized_functions\n        return prompt, model, is_suitable_for_distillation, is_initialized\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if function is already initialized\n        if func_hash in self.function_data:\n            # Check if function is already initialized and does not require saving examples for fine-tuning\n            if self.function_data[func_hash][\"initialized\"] and not self.function_data[func_hash][\"require_examples_save\"]:\n                # Use a distilled model for zero-shot prompting\n                model = self.distilled_model\n                suitable_for_distillation = True\n                require_examples_save = False\n            else:\n                # Use a teacher model for fine-tuning\n                model = self.teacher_model\n                suitable_for_distillation = False\n                require_examples_save = self.function_data[func_hash][\"require_examples_save\"]\n        else:\n            # Initialize function-specific data\n            self.function_data[func_hash] = {\"initialized\": False, \"require_examples_save\": False}\n            # Use a teacher model for fine-tuning\n            model = self.teacher_model\n            suitable_for_distillation = False\n            require_examples_save = False\n\n        # Construct the prompt\n        prompt = self.construct_prompt(args, kwargs, function_description, llm_parameters)\n\n        # Return the prompt, model, suitable_for_distillation, and require_examples_save\n        return prompt, model, suitable_for_distillation, require_examples_save\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        if function_description.model_type == \"zero_shot\":\n            model = self.zero_shot_model\n        elif function_description.model_type == \"teacher\":\n            model = self.teacher_model\n        else:\n            raise ValueError(f\"Invalid model type: {function_description.model_type}\")\n        \n        if function_description.model_type == \"zero_shot\":\n            is_suitable_for_distillation = True\n        else:\n            is_suitable_for_distillation = False\n        \n        if function_description.model_type == \"teacher\":\n            if func_hash in self.teacher_function_data:\n                if self.teacher_function_data[func_hash].is_suitable_for_distillation:\n                    is_suitable_for_distillation = True\n                if self.teacher_function_data[func_hash].is_function_initialized:\n                    is_function_initialized = True\n                else:\n                    is_function_initialized = False\n            else:\n                is_function_initialized = False\n        else:\n            is_function_initialized = True\n        \n        if is_function_initialized:\n            if function_description.model_type == \"teacher\":\n                prompt = self.teacher_function_data[func_hash].prompt\n            else:\n                prompt = self.zero_shot_function_data[func_hash].prompt\n        else:\n            if function_description.model_type == \"teacher\":\n                prompt = self.initialize_teacher_function(function_description, args, kwargs, func_hash)\n            else:\n                prompt = self.initialize_zero_shot_function(function_description, args, kwargs, func_hash)\n        \n        return prompt, model, is_suitable_for_distillation, is_function_initialized\n    "}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialize_function(function_description, func_hash)\n            return None, None, None, True\n        else:\n            # Check if the function is suitable for distillation\n            if self.is_suitable_for_distillation(function_description, func_hash):\n                # Use a distilled model for zero-shot prompting\n                model = self.distilled_model\n                prompt = self.get_distilled_prompt(function_description, args, kwargs)\n                return prompt, model, True, False\n            else:\n                # Use a teacher model for fine-tuning\n                model = self.teacher_model\n                prompt = self.get_fine_tuning_prompt(function_description, args, kwargs)\n                return prompt, model, False, False\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        if function_description.model_name is not None:\n            model_name = function_description.model_name\n        else:\n            model_name = self.get_model_name_from_function_description(function_description)\n\n        if not self.model_is_initialized(model_name):\n            self.initialize_model(model_name)\n\n        if not self.model_is_suitable_for_distillation(model_name):\n            model_name = self.get_teacher_model_name(model_name)\n\n        if self.model_is_suitable_for_distillation(model_name):\n            prompt = self.get_distillation_prompt(function_description, args, kwargs)\n            model = self.get_distilled_model(model_name)\n            is_suitable_for_distillation = True\n        else:\n            prompt = self.get_fine_tuning_prompt(function_description, args, kwargs)\n            model = self.get_teacher_model(model_name)\n            is_suitable_for_distillation = False\n\n        if not self.function_is_initialized(func_hash):\n            self.initialize_function(func_hash)\n\n        if self.function_is_initialized(func_hash):\n            if not self.function_is_suitable_for_fine_tuning(func_hash):\n                self.update_examples_for_fine_tuning(func_hash)\n\n            is_function_initialized = True\n        else:\n            is_function_initialized = False\n\n        return prompt, model, is_suitable_for_distillation, is_function_initialized\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Get the function name from the function description\n        func_name = function_description.name\n\n        # Check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # Initialize the function\n            self.initialize_function(func_name, function_description)\n\n        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if func_hash in self.initialized_functions and self.initialized_functions[func_hash].examples_saved:\n            # Get the function description from the initialized functions\n            function_description = self.initialized_functions[func_hash].function_description\n\n            # Check if the function description has been modified\n            if function_description != self.initialized_functions[func_hash].function_description:\n                # Update the examples for fine-tuning\n                self.update_examples(func_name, function_description, func_hash)\n\n            # Get the function description from the initialized functions\n            function_description = self.initialized_functions[func_hash].function_description\n\n        # Get the model and prompt for the function\n        model, prompt = self.get_model_and_prompt(func_name, function_description, args, kwargs)\n\n        # Check if the model is suitable for distillation\n        if self.is_suitable_for_distillation(model):\n            # Use a distilled model for zero-shot prompting\n            return prompt, model, True, True\n        else:\n            # Use a teacher model for fine-tuning\n            return prompt, model, False, False\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if func_hash in self.function_data:\n            function_data = self.function_data[func_hash]\n            # Check if the function is already initialized and does not require saving examples for fine-tuning\n            if function_data.is_initialized:\n                if not function_data.is_fine_tuned:\n                    return function_data.prompt, function_data.model, function_data.is_suitable_for_distillation, True\n            # If the function is already initialized and requires saving examples for fine-tuning, update the examples\n            else:\n                function_data.examples = self.update_function_examples(function_data.examples, args, kwargs, function_description, func_hash)\n                function_data.is_initialized = True\n                return function_data.prompt, function_data.model, function_data.is_suitable_for_distillation, True\n        # If the function is not initialized, initialize it and construct the prompt\n        else:\n            # Initialize the function data\n            self.initialize_function(function_description, func_hash)\n            # Construct the prompt\n            function_data = self.function_data[func_hash]\n            function_data.prompt = self.construct_prompt(function_data.examples, function_description, args, kwargs)\n            # Determine the model to use\n            if function_data.is_suitable_for_distillation:\n                model = self.get_distilled_model(function_data.model)\n            else:\n                model = self.get_teacher_model(function_data.model)\n            return function_data.prompt, model, function_data.is_suitable_for_distillation, False\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        return cov_nearest_clip(cov)\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov_nearest = cov.copy()\n        for _ in range(higham_max_iteration):\n            eigvals, eigvecs = np.linalg.eigh(cov_nearest)\n            eigvals[eigvals < 0] = 0\n            cov_nearest = eigvecs @ np.diag(eigvals) @ eigvecs.T\n        return cov_nearest\n    else:\n        eigvals, eigvecs = np.linalg.eigh(cov)\n        eigvals[eigvals < 0] = 0\n        return eigvecs @ np.diag(eigvals) @ eigvecs.T"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use the Higham & Nick (2002) algorithm to find the nearest positive definite matrix\n        cov_nearest = cov.copy()\n        for _ in range(higham_max_iteration):\n            # Compute the eigenvalues and eigenvectors of the covariance matrix\n            eigenvalues, eigenvectors = np.linalg.eigh(cov_nearest)\n            # Clip the eigenvalues to ensure they are all positive\n            eigenvalues = np.clip(eigenvalues, a_min=0, a_max=None)\n            # Reconstruct the covariance matrix from the clipped eigenvalues and eigenvectors\n            cov_nearest = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n        return cov_nearest\n    else:\n        # Clip the eigenvalues to ensure they are all positive\n        eigenvalues, eigenvectors = np.linalg.eigh(cov)\n        eigenvalues = np.clip(eigenvalues, a_min=0, a_max=None)\n        # Reconstruct the covariance matrix from the clipped eigenvalues and eigenvectors\n        cov_nearest = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n        return cov_nearest"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov_nearest = cov.copy()\n        for i in range(higham_max_iteration):\n            w, v = np.linalg.eig(cov_nearest)\n            w[w < 0] = 0\n            cov_nearest = v @ np.diag(w) @ v.T\n        return cov_nearest\n    else:\n        w, v = np.linalg.eig(cov)\n        w[w < 0] = 0\n        return v @ np.diag(w) @ v.T\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use the Higham & Nick (2002) algorithm to compute the nearest positive definite matrix\n        cov_nearest = cov.copy()\n        for _ in range(higham_max_iteration):\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest -= np.diag(np.diag(cov_nearest))\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest -= np.diag(np.diag(cov_nearest))\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest = (cov_nearest + cov_nearest.T) / 2\n            cov_nearest ="}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Compute the nearest positive definite matrix using the Higham & Nick (2002) algorithm\n        cov_nearest = cov\n        for _ in range(higham_max_iteration):\n            # Compute the eigenvalues and eigenvectors of the covariance matrix\n            eigvals, eigvecs = np.linalg.eigh(cov_nearest)\n\n            # Clip the eigenvalues to ensure they are positive\n            eigvals = np.clip(eigvals, a_min=0, a_max=None)\n\n            # Reconstruct the covariance matrix from the clipped eigenvalues and eigenvectors\n            cov_nearest = np.dot(eigvecs * eigvals, eigvecs.T)\n\n            # Check if the covariance matrix is positive definite\n            if np.all(np.linalg.eigvals(cov_nearest) > 0):\n                break\n\n        return cov_nearest\n\n    else:\n        # Compute the eigenvalues and eigenvectors of the covariance matrix\n        eigvals, eigvecs = np.linalg.eigh(cov)\n\n        # Clip the eigenvalues to ensure they are positive\n        eigvals = np.clip(eigvals, a_min=0, a_max=None)\n\n        # Reconstruct the covariance matrix from the clipped eigenvalues and eigenvectors\n        cov_nearest = np.dot(eigvecs * eigvals, eigvecs.T)\n\n        return cov_nearest"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use the Higham & Nick (2002) algorithm to find the nearest positive definite matrix\n        cov_nearest = higham_nearest(cov, max_iteration=higham_max_iteration)\n    else:\n        # Clip eigenvalues to ensure the resulting matrix is positive definite\n        cov_nearest = cov.copy()\n        cov_nearest[np.diag_indices_from(cov_nearest)] = np.maximum(cov_nearest[np.diag_indices_from(cov_nearest)], 0)\n        cov_nearest = np.maximum(cov_nearest, cov_nearest.T)\n\n    return cov_nearest"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov_nearest = cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        cov_nearest = cov_nearest_clip(cov)\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    # Compute the nearest covariance matrix using the Higham & Nick (2002) algorithm\n    if higham:\n        cov_nearest = higham_cov_nearest(cov, max_iteration=higham_max_iteration)\n    else:\n        # Compute the eigenvalues and eigenvectors of the input covariance matrix\n        eigvals, eigvecs = np.linalg.eigh(cov)\n\n        # Clip the eigenvalues to ensure they are positive\n        eigvals = np.clip(eigvals, a_min=0, a_max=None)\n\n        # Reconstruct the covariance matrix from the clipped eigenvalues and eigenvectors\n        cov_nearest = eigvecs @ np.diag(eigvals) @ eigvecs.T\n\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use Higham & Nick (2002) algorithm\n        for _ in range(higham_max_iteration):\n            w, v = np.linalg.eig(cov)\n            delta = np.min(w)\n            if delta >= 0:\n                break\n            s = np.maximum(w - delta + 1e-8, 0)\n            cov = np.dot(v, np.dot(np.diag(s), v.T))\n        return cov\n    else:\n        # Clip eigenvalues\n        w, v = np.linalg.eig(cov)\n        w = np.maximum(w, 1e-8)\n        return np.dot(v, np.dot(np.diag(w), v.T))"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Higham & Nick (2002) algorithm\n        # Compute the nearest positive definite matrix using the Higham & Nick (2002) algorithm\n        cov_nearest = higham_nearest(cov, max_iteration=higham_max_iteration)\n    else:\n        # Clip eigenvalues to ensure the resulting matrix is positive definite\n        cov_nearest = clip_eigenvalues(cov)\n\n    return cov_nearest\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        return cov_nearest_clip(cov)\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use the Higham & Nick (2002) algorithm to find the nearest positive definite matrix\n        cov_nearest = cov_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        # Clip eigenvalues to ensure the resulting matrix is positive definite\n        cov_nearest = cov_clip(cov)\n\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        return cov_nearest_clipping(cov)\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use the Higham & Nick (2002) algorithm\n        cov = cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        # Clip eigenvalues\n        cov = cov_nearest_clip(cov)\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    # Compute the nearest covariance matrix using Higham & Nick (2002) algorithm\n    if higham:\n        cov_nearest = higham_cov_nearest(cov, max_iteration=higham_max_iteration)\n    else:\n        # Compute the nearest covariance matrix by clipping eigenvalues\n        cov_nearest = cov_clip(cov)\n\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use Higham & Nick (2002) algorithm to find the nearest positive definite matrix\n        cov_nearest = cov.copy()\n        for _ in range(higham_max_iteration):\n            w, v = np.linalg.eigh(cov_nearest)\n            w[w < 0] = 0\n            cov_nearest = np.dot(v, np.dot(np.diag(w), v.T))\n\n        return cov_nearest\n    else:\n        # Clip eigenvalues based on the specified parameters to ensure the resulting matrix is positive definite\n        cov_nearest = cov.copy()\n        w, v = np.linalg.eigh(cov_nearest)\n        w[w < 0] = 0\n        cov_nearest = np.dot(v, np.dot(np.diag(w), v.T))\n\n        return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov = higham_nearest(cov, max_iteration=higham_max_iteration)\n    else:\n        cov = cov_clip(cov)\n\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Higham & Nick (2002) algorithm\n        # Compute the nearest correlation matrix to the input covariance matrix\n        # by using the Higham & Nick (2002) algorithm.\n        cov_corr = cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        # Clip eigenvalues\n        # Compute the nearest covariance matrix to the input covariance matrix\n        # by clipping eigenvalues.\n        cov_corr = cov_nearest_clip(cov)\n\n    return cov_corr\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        return cov_nearest_clip(cov)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    from pathlib import Path\n    from shutil import rmtree\n\n    data_home = Path(data_home) if data_home else Path.home() / \"skfolio_data\"\n    rmtree(data_home, ignore_errors=True)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    import shutil\n    from pathlib import Path\n\n    data_home = Path(data_home) if data_home else Path.home() / \"skfolio_data\"\n    shutil.rmtree(data_home, ignore_errors=True)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    import shutil\n    from pathlib import Path\n    from skfolio.utils import _get_data_home\n\n    data_home = _get_data_home(data_home)\n\n    if data_home.exists():\n        shutil.rmtree(data_home)\n\n    data_home.mkdir(parents=True)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    from pathlib import Path\n    from shutil import rmtree\n\n    if data_home is None:\n        from skfolio.utils.config import get_data_home\n\n        data_home = get_data_home()\n    else:\n        data_home = Path(data_home)\n\n    rmtree(data_home, ignore_errors=True)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    if data_home is None:\n        data_home = get_data_home()\n    else:\n        data_home = Path(data_home)\n\n    if data_home.exists():\n        shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    from pathlib import Path\n    import shutil\n    from skfolio.utils.paths import get_data_home\n\n    data_home = get_data_home(data_home=data_home)\n    if data_home.exists():\n        shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    from pathlib import Path\n    from shutil import rmtree\n\n    from skfolio.utils import _get_data_home\n\n    data_home = _get_data_home(data_home=data_home)\n    rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    # Importing required packages\n    import os\n    import shutil\n    from pathlib import Path\n\n    # Determining the data home directory path\n    if data_home is None:\n        data_home = Path(os.path.expanduser(\"~\")) / \"skfolio_data\"\n    elif isinstance(data_home, str):\n        data_home = Path(data_home)\n    elif isinstance(data_home, Path):\n        data_home = data_home\n    else:\n        raise TypeError(\"data_home must be a string or path-like object\")\n\n    # Removing the contents of the data home directory\n    if data_home.exists():\n        shutil.rmtree(data_home)\n\n    return None"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    import os\n    import shutil\n    from pathlib import Path\n\n    data_home = Path(data_home) if data_home else Path.home() / \"skfolio_data\"\n\n    if data_home.is_dir():\n        shutil.rmtree(data_home)\n    else:\n        os.remove(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    from pathlib import Path\n    from shutil import rmtree\n\n    if data_home is None:\n        data_home = Path.home() / \"skfolio_data\"\n    elif isinstance(data_home, str):\n        data_home = Path(data_home)\n\n    if data_home.is_dir():\n        rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    if data_home is None:\n        data_home = get_data_home()\n    else:\n        data_home = Path(data_home)\n\n    if data_home.exists():\n        shutil.rmtree(data_home)\n\n    data_home.mkdir(parents=True, exist_ok=True)\n\n    return None\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    from pathlib import Path\n    import shutil\n\n    # Determine the data home directory path\n    data_home = get_data_home(data_home)\n\n    # Remove all the contents of the data home directory\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    from skfolio.utils.data_utils import get_data_home\n    from pathlib import Path\n    import shutil\n\n    data_home = get_data_home(data_home=data_home)\n\n    if Path(data_home).exists():\n        shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    from pathlib import Path\n    from shutil import rmtree\n    from skfolio.utils.config import get_data_home\n\n    data_home = get_data_home(data_home)\n    if data_home is not None:\n        rmtree(data_home, ignore_errors=True)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    from pathlib import Path\n\n    if data_home is None:\n        from skfolio.utils import get_data_home\n\n        data_home = get_data_home()\n    elif isinstance(data_home, str):\n        data_home = Path(data_home)\n\n    if not data_home.exists():\n        return\n\n    for file in data_home.iterdir():\n        file.unlink()\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    from pathlib import Path\n    from shutil import rmtree\n\n    if data_home is None:\n        data_home = Path.home() / \".skfolio_data\"\n    elif isinstance(data_home, str):\n        data_home = Path(data_home)\n\n    if data_home.exists():\n        rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    if data_home is None:\n        data_home = get_data_home()\n    else:\n        data_home = Path(data_home)\n\n    if data_home.exists():\n        shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    import os\n    from pathlib import Path\n\n    data_home = Path(data_home) if data_home else Path.home() / \"skfolio_data\"\n\n    if not data_home.exists():\n        raise FileNotFoundError(f\"The specified data home directory does not exist: {data_home}\")\n\n    if not data_home.is_dir():\n        raise NotADirectoryError(f\"The specified data home directory is not a directory: {data_home}\")\n\n    if not os.access(data_home, os.W_OK):\n        raise PermissionError(f\"You do not have permission to write to the specified data home directory: {data_home}\")\n\n    for file_name in os.listdir(data_home):\n        file_path = data_home / file_name\n        if file_path.is_file():\n            os.remove(file_path)\n        else:\n            shutil.rmtree(file_path)\n\n    print(f\"All files and subdirectories in the specified data home directory have been deleted: {data_home}\")\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    from pathlib import Path\n    from shutil import rmtree\n\n    # Determine the data home directory path\n    if data_home is None:\n        data_home = Path.home() / \"skfolio_data\"\n    elif isinstance(data_home, str):\n        data_home = Path(data_home)\n\n    # Remove all contents of the data home directory\n    rmtree(data_home, ignore_errors=True)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    from pathlib import Path\n    from shutil import rmtree\n\n    data_home = Path(data_home).expanduser() if data_home else Path.home() / \"skfolio_data\"\n    rmtree(data_home)\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj, str), lambda x: x\n    elif isinstance(obj, bytes):\n        return (obj, bytes), lambda x: x\n    elif isinstance(obj, list):\n        return (obj, list), lambda x: x\n    elif isinstance(obj, tuple):\n        return (obj, tuple), lambda x: x\n    elif isinstance(obj, dict):\n        return (obj, dict), lambda x: x\n    elif isinstance(obj, set):\n        return (obj, set), lambda x: x\n    elif isinstance(obj, frozenset):\n        return (obj, frozenset), lambda x: x\n    elif isinstance(obj, slice):\n        return (obj, slice), lambda x: x\n    elif isinstance(obj, type):\n        return (obj, type), lambda x: x\n    elif isinstance(obj, np.ndarray):\n        return (obj, np.ndarray), lambda x: x\n    elif isinstance(obj, torch.Tensor):\n        return (obj, torch.Tensor), lambda x: x\n    elif isinstance(obj, torch.nn.Module):\n        return (obj, torch.nn.Module), lambda x: x\n    elif isinstance(obj, torch.nn.Parameter):\n        return (obj, torch.nn.Parameter), lambda x: x\n    elif isinstance(obj, torch.nn.ModuleDict):\n        return (obj, torch.nn.ModuleDict), lambda x: x\n    elif isinstance(obj, torch.nn.ModuleList):\n        return (obj, torch.nn.ModuleList), lambda x: x\n    elif isinstance(obj, torch.nn.ParameterDict):\n        return (obj, torch.nn.ParameterDict), lambda x: x\n    elif isinstance(obj, torch.nn.ParameterList):\n        return (obj, torch.nn.ParameterList), lambda x: x\n    elif isinstance(obj, torch.nn.Sequential):"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj, lambda x: x)\n    elif isinstance(obj, bytes):\n        return (obj, lambda x: x)\n    elif isinstance(obj, list):\n        res = []\n        schema = []\n        for o in obj:\n            r, s = flatten_to_tuple(o)\n            res.append(r)\n            schema.append(s)\n        return (tuple(res), lambda x: [s(r) for s, r in zip(schema, x)])\n    elif isinstance(obj, tuple):\n        res = []\n        schema = []\n        for o in obj:\n            r, s = flatten_to_tuple(o)\n            res.append(r)\n            schema.append(s)\n        return (tuple(res), lambda x: tuple([s(r) for s, r in zip(schema, x)]))\n    elif isinstance(obj, dict):\n        res = {}\n        schema = {}\n        for k, v in obj.items():\n            r, s = flatten_to_tuple(v)\n            res[k] = r\n            schema[k] = s\n        return (res, lambda x: {k: s(r) for k, (s, r) in x.items()})\n    elif isinstance(obj, set):\n        res = []\n        schema = []\n        for o in obj:\n            r, s = flatten_to_tuple(o)\n            res.append(r)\n            schema.append(s)\n        return (tuple(res), lambda x: set([s(r) for s, r in zip(schema, x)]))\n    elif isinstance(obj, Instances):\n        res = []\n        schema = []\n        for o in obj:\n            r, s = flatten_to_tuple(o)\n            res.append(r)\n            schema.append(s)\n        return (tuple(res), lambda x: Instances([s(r) for s, r in zip(schema, x)],"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,)\n    elif isinstance(obj, list):\n        return (tuple(obj),)\n    elif isinstance(obj, tuple):\n        return (tuple(obj),)\n    elif isinstance(obj, (dict, OrderedDict)):\n        return (tuple(obj.items()),)\n    elif isinstance(obj, Instances):\n        return (\n            tuple(obj.fields),\n            tuple(obj.pred_boxes),\n            tuple(obj.gt_boxes),\n            tuple(obj.scores),\n            tuple(obj.gt_classes),\n            tuple(obj.pred_classes),\n        )\n    elif isinstance(obj, Boxes):\n        return (tuple(obj.tensor),)\n    elif isinstance(obj, ROIMasks):\n        return (tuple(obj.tensor),)\n    else:\n        raise TypeError(f\"Unsupported object type: {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), str\n    elif isinstance(obj, bytes):\n        return (obj,), bytes\n    elif isinstance(obj, list):\n        return tuple(obj), list\n    elif isinstance(obj, tuple):\n        return obj, tuple\n    elif isinstance(obj, dict):\n        return tuple(obj.items()), dict\n    elif isinstance(obj, set):\n        return tuple(obj), set\n    elif isinstance(obj, frozenset):\n        return tuple(obj), frozenset\n    elif isinstance(obj, slice):\n        return (obj.start, obj.stop, obj.step), slice\n    elif isinstance(obj, complex):\n        return (obj.real, obj.imag), complex\n    elif isinstance(obj, range):\n        return tuple(obj), range\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist(), np.ndarray\n    elif isinstance(obj, torch.Tensor):\n        return obj.tolist(), torch.Tensor\n    elif isinstance(obj, torch.nn.Module):\n        return obj.state_dict(), torch.nn.Module\n    elif isinstance(obj, torch.nn.Parameter):\n        return obj.data.tolist(), torch.nn.Parameter\n    elif isinstance(obj, torch.nn.ParameterList):\n        return tuple(obj.parameters()), torch.nn.ParameterList\n    elif isinstance(obj, torch.nn.ParameterDict):\n        return tuple(obj.items()), torch.nn.ParameterDict\n    elif isinstance(obj, torch.nn.ModuleList):\n        return tuple(obj.modules()), torch.nn.ModuleList\n    elif isinstance(obj, torch.nn.ModuleDict):\n        return tuple(obj.items()), torch.nn.ModuleDict\n    elif isinstance(obj, torch.nn.Sequential):\n        return tuple(obj.children()), torch.nn.Sequential\n    elif isinstance(obj, torch.nn.LazyLinear):\n       "}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj, str)\n    elif isinstance(obj, list):\n        return (obj, list)\n    elif isinstance(obj, tuple):\n        return (obj, tuple)\n    elif isinstance(obj, dict):\n        return (obj, dict)\n    elif isinstance(obj, set):\n        return (obj, set)\n    elif isinstance(obj, Instances):\n        return (obj, Instances)\n    elif isinstance(obj, Boxes):\n        return (obj, Boxes)\n    elif isinstance(obj, ROIMasks):\n        return (obj, ROIMasks)\n    else:\n        raise ValueError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    def _flatten_to_tuple_str(obj):\n        return (obj,)\n\n    def _flatten_to_tuple_bytes(obj):\n        return (obj,)\n\n    def _flatten_to_tuple_list(obj):\n        res = []\n        for o in obj:\n            res.extend(flatten_to_tuple(o))\n        return (tuple(res),)\n\n    def _flatten_to_tuple_tuple(obj):\n        res = []\n        for o in obj:\n            res.extend(flatten_to_tuple(o))\n        return (tuple(res),)\n\n    def _flatten_to_tuple_mapping(obj):\n        res = []\n        for k, v in obj.items():\n            res.extend(flatten_to_tuple(k))\n            res.extend(flatten_to_tuple(v))\n        return (tuple(res),)\n\n    def _flatten_to_tuple_instance(obj):\n        res = []\n        for k, v in obj.items():\n            res.extend(flatten_to_tuple(k))\n            res.extend(flatten_to_tuple(v))\n        return (tuple(res),)\n\n    def _flatten_to_tuple_boxes(obj):\n        res = []\n        for o in obj:\n            res.extend(flatten_to_tuple(o))\n        return (tuple(res),)\n\n    def _flatten_to_tuple_roimasks(obj):\n        res = []\n        for o in obj:\n            res.extend(flatten_to_tuple(o))\n        return (tuple(res),)\n\n    flatten_funcs = {\n        str: _flatten_to_tuple_str,\n        bytes: _flatten_to_tuple_bytes,\n        list: _flatten_to_tuple_list,\n        tuple: _flatten_to_tuple_tuple,\n        dict: _flatten_to"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    def _flatten_to_tuple(obj):\n        if isinstance(obj, (str, bytes, list, tuple)):\n            return obj\n        elif isinstance(obj, dict):\n            return tuple([_flatten_to_tuple(obj[k]) for k in sorted(obj.keys())])\n        elif isinstance(obj, (Instance, Boxes, ROIMasks)):\n            return tuple([_flatten_to_tuple(getattr(obj, k)) for k in obj.fields()])\n        else:\n            raise TypeError(f\"Unsupported type: {type(obj)}\")\n\n    def _build_schema(obj):\n        if isinstance(obj, (str, bytes, list, tuple)):\n            return lambda x: x\n        elif isinstance(obj, dict):\n            return lambda x: {k: _build_schema(obj[k])(x[i]) for i, k in enumerate(sorted(obj.keys()))}\n        elif isinstance(obj, (Instance, Boxes, ROIMasks)):\n            return lambda x: obj.__class__(_build_schema(obj.fields())(x))\n        else:\n            raise TypeError(f\"Unsupported type: {type(obj)}\")\n\n    res = _flatten_to_tuple(obj)\n    schema = _build_schema(obj)\n    return res, schema"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj, lambda x: x)\n    elif isinstance(obj, bytes):\n        return (obj, lambda x: x)\n    elif isinstance(obj, list):\n        return (tuple(obj), lambda x: list(x))\n    elif isinstance(obj, tuple):\n        return (tuple(obj), lambda x: tuple(x))\n    elif isinstance(obj, dict):\n        return (tuple(obj.items()), lambda x: dict(x))\n    elif isinstance(obj, set):\n        return (tuple(obj), lambda x: set(x))\n    elif isinstance(obj, Instances):\n        return (\n            (\n                obj.pred_classes,\n                obj.scores,\n                obj.pred_boxes,\n                obj.pred_masks,\n                obj.gt_classes,\n                obj.gt_boxes,\n                obj.gt_masks,\n                obj.gt_keypoints,\n                obj.gt_polygons,\n                obj.gt_semantic_seg,\n                obj.has_gt,\n                obj.has_pred,\n                obj.scores_per_image,\n            ),\n            lambda x: Instances(\n                pred_classes=x[0],\n                scores=x[1],\n                pred_boxes=x[2],\n                pred_masks=x[3],\n                gt_classes=x[4],\n                gt_boxes=x[5],\n                gt_masks=x[6],\n                gt_keypoints=x[7],\n                gt_polygons=x[8],\n                gt_semantic_seg=x[9],\n                has_gt=x[10],\n                has_pred=x[11],\n                scores_per_image=x[12],\n            ),\n        )\n    elif isinstance(obj, Boxes):\n        return (tuple(obj.tensor), lambda x: Boxes(x))\n    elif isinstance(obj, RO"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return obj, lambda res: res\n    elif isinstance(obj, list):\n        return tuple(obj), lambda res: list(res)\n    elif isinstance(obj, tuple):\n        return obj, lambda res: res\n    elif isinstance(obj, dict):\n        return tuple(obj.items()), lambda res: dict(res)\n    elif isinstance(obj, (Instances, Boxes, ROIMasks)):\n        return obj.flatten(), lambda res: obj.__class__(res)\n    else:\n        raise TypeError(f\"Flattening of type {type(obj)} is not supported.\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    from detectron2.structures import Boxes, Instances, ROIMasks\n\n    if isinstance(obj, str):\n        return (obj, str), lambda x: x\n    elif isinstance(obj, bytes):\n        return (obj, bytes), lambda x: x\n    elif isinstance(obj, list):\n        return (\n            tuple(flatten_to_tuple(item) for item in obj),\n            lambda x: [item[1](item[0]) for item in x],\n        )\n    elif isinstance(obj, tuple):\n        return (\n            tuple(flatten_to_tuple(item) for item in obj),\n            lambda x: tuple(item[1](item[0]) for item in x),\n        )\n    elif isinstance(obj, dict):\n        return (\n            tuple(\n                flatten_to_tuple(key) + flatten_to_tuple(value)\n                for key, value in obj.items()\n            ),\n            lambda x: {\n                item[1](item[0]): item[3](item[2])\n                for item in zip(\n                    *[iter(x)] * 4,\n                )\n            },\n        )\n    elif isinstance(obj, Instances):\n        return (\n            (\n                flatten_to_tuple(obj.image_size)\n                + flatten_to_tuple(obj.gt_classes)\n                + flatten_to_tuple(obj.gt_boxes)\n                + flatten_to_tuple(obj.gt_masks)\n            ),\n            lambda x: Instances(\n                image_size=x[0][1](x[0][0]),\n                gt_classes=x[1][1](x[1][0]),\n                gt_boxes=Boxes(x[2][1](x[2][0])),\n                gt_masks=ROIMasks(x[3][1](x[3][0])),\n            ),\n        )\n    elif isinstance(obj, Boxes):\n        return (\n            flat"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    # TODO: Add support for Instances, Boxes, and ROIMasks.\n\n    if isinstance(obj, (str, bytes)):\n        return (obj, lambda x: x)\n\n    if isinstance(obj, (list, tuple)):\n        return (obj, lambda x: x)\n\n    if isinstance(obj, dict):\n        return (\n            tuple(obj.values()),\n            lambda x: {k: v for k, v in zip(obj.keys(), x)},\n        )\n\n    raise TypeError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj, _FlattenStr)\n    elif isinstance(obj, list):\n        return (obj, _FlattenList)\n    elif isinstance(obj, tuple):\n        return (obj, _FlattenTuple)\n    elif isinstance(obj, dict):\n        return (obj, _FlattenDict)\n    elif isinstance(obj, (set, frozenset)):\n        return (obj, _FlattenSet)\n    elif isinstance(obj, (int, float, bool)):\n        return (obj, _FlattenScalar)\n    elif isinstance(obj, _FlattenScalar):\n        return (obj.val, _FlattenScalar)\n    elif isinstance(obj, _FlattenStr):\n        return (obj.val, _FlattenStr)\n    elif isinstance(obj, _FlattenList):\n        return (obj.val, _FlattenList)\n    elif isinstance(obj, _FlattenTuple):\n        return (obj.val, _FlattenTuple)\n    elif isinstance(obj, _FlattenDict):\n        return (obj.val, _FlattenDict)\n    elif isinstance(obj, _FlattenSet):\n        return (obj.val, _FlattenSet)\n    elif isinstance(obj, _FlattenObject):\n        return (obj.val, obj.schema)\n    else:\n        raise TypeError(f\"Flattening of type {type(obj)} is not supported\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj, _StrSchema)\n    elif isinstance(obj, list):\n        return (obj, _ListSchema)\n    elif isinstance(obj, tuple):\n        return (obj, _TupleSchema)\n    elif isinstance(obj, dict):\n        return (obj, _DictSchema)\n    elif isinstance(obj, (set, frozenset)):\n        return (obj, _SetSchema)\n    elif isinstance(obj, _Instances):\n        return (obj, _InstancesSchema)\n    elif isinstance(obj, _Boxes):\n        return (obj, _BoxesSchema)\n    elif isinstance(obj, _ROIMasks):\n        return (obj, _ROIMasksSchema)\n    else:\n        raise ValueError(\"Unsupported type: {}\".format(type(obj)))\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj, str)\n    elif isinstance(obj, list):\n        return (obj, list)\n    elif isinstance(obj, tuple):\n        return (obj, tuple)\n    elif isinstance(obj, dict):\n        return (obj, dict)\n    elif isinstance(obj, set):\n        return (obj, set)\n    elif isinstance(obj, (int, float)):\n        return (obj, float)\n    elif isinstance(obj, bool):\n        return (obj, bool)\n    elif isinstance(obj, Instances):\n        return (obj, Instances)\n    elif isinstance(obj, Boxes):\n        return (obj, Boxes)\n    elif isinstance(obj, ROIMasks):\n        return (obj, ROIMasks)\n    else:\n        raise ValueError(f\"Unsupported object type: {type(obj)}\")\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    # TODO: add more types here\n    if isinstance(obj, (str, bytes)):\n        return obj, lambda x: x\n    elif isinstance(obj, (list, tuple)):\n        return tuple(flatten_to_tuple(x) for x in obj), lambda x: [x[0] for x in x]\n    elif isinstance(obj, dict):\n        return tuple(flatten_to_tuple(x) for x in obj.values()), lambda x: {k: x[k] for k, x in enumerate(x)}\n    elif isinstance(obj, (Instance, Boxes, ROIMasks)):\n        return flatten_to_tuple(obj.get_fields()), lambda x: obj.from_fields(x)\n    else:\n        raise NotImplementedError(f\"Flattening of type {type(obj)} is not implemented\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return obj, str\n    elif isinstance(obj, (list, tuple)):\n        return tuple(flatten_to_tuple(x) for x in obj), list\n    elif isinstance(obj, dict):\n        return tuple(flatten_to_tuple(x) for x in obj.items()), dict\n    elif isinstance(obj, (int, float, bool)):\n        return obj, type(obj)\n    elif isinstance(obj, (Instance, Boxes, ROIMasks)):\n        return flatten_to_tuple(obj.get_fields()), type(obj)\n    else:\n        raise ValueError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj, StringSchema)\n    elif isinstance(obj, list):\n        return (obj, ListSchema)\n    elif isinstance(obj, tuple):\n        return (obj, TupleSchema)\n    elif isinstance(obj, (dict, OrderedDict)):\n        return (obj, DictSchema)\n    elif isinstance(obj, Instances):\n        return (obj, InstancesSchema)\n    elif isinstance(obj, Boxes):\n        return (obj, BoxesSchema)\n    elif isinstance(obj, ROIMasks):\n        return (obj, ROIMasksSchema)\n    else:\n        raise TypeError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, tuple):\n        res = obj\n        schema = type(obj)\n    elif isinstance(obj, list):\n        res = tuple(obj)\n        schema = type(obj)\n    elif isinstance(obj, str):\n        res = (obj,)\n        schema = str\n    elif isinstance(obj, bytes):\n        res = (obj,)\n        schema = bytes\n    elif isinstance(obj, dict):\n        res = tuple(obj.items())\n        schema = type(obj)\n    elif isinstance(obj, set):\n        res = tuple(obj)\n        schema = type(obj)\n    elif isinstance(obj, Boxes):\n        res = obj.tensor\n        schema = Boxes\n    elif isinstance(obj, ROIMasks):\n        res = obj.tensor\n        schema = ROIMasks\n    elif isinstance(obj, Instances):\n        res = (\n            obj.gt_boxes,\n            obj.gt_classes,\n            obj.gt_keypoints,\n            obj.gt_masks,\n            obj.gt_segmentation,\n            obj.gt_keypoints_metadata,\n            obj.gt_masks_metadata,\n            obj.gt_segmentation_metadata,\n            obj.gt_panoptic_seg,\n            obj.gt_panoptic_seg_metadata,\n            obj.gt_sem_seg,\n            obj.gt_sem_seg_metadata,\n            obj.gt_panoptic_seg_full_size,\n            obj.gt_panoptic_seg_full_size_metadata,\n            obj.gt_sem_seg_full_size,\n            obj.gt_sem_seg_full_size_metadata,\n            obj.gt_keypoints_full_size,\n            obj.gt_keypoints_full_size_metadata,\n            obj.gt_masks_full_size,\n            obj.gt_masks_full_size_metadata,\n            obj.gt_segmentation_full_size,\n            obj.gt_segmentation_full_size_metadata,\n           "}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, tuple):\n        return obj, lambda x: x\n    elif isinstance(obj, list):\n        return tuple(obj), lambda x: list(x)\n    elif isinstance(obj, str):\n        return (obj,), lambda x: x[0]\n    elif isinstance(obj, bytes):\n        return (obj,), lambda x: x[0]\n    elif isinstance(obj, dict):\n        return (tuple(obj.keys()), tuple(obj.values())), lambda x: dict(zip(x[0], x[1]))\n    elif isinstance(obj, set):\n        return tuple(obj), lambda x: set(x)\n    elif isinstance(obj, int):\n        return (obj,), lambda x: x[0]\n    elif isinstance(obj, float):\n        return (obj,), lambda x: x[0]\n    elif isinstance(obj, bool):\n        return (obj,), lambda x: x[0]\n    elif isinstance(obj, slice):\n        return (obj.start, obj.stop, obj.step), lambda x: slice(x[0], x[1], x[2])\n    elif isinstance(obj, Instances):\n        return (\n            (\n                obj.image_size,\n                obj.gt_boxes,\n                obj.gt_classes,\n                obj.gt_masks,\n                obj.gt_keypoints,\n                obj.gt_polygons,\n                obj.gt_semantic_seg,\n                obj.gt_panoptic_seg,\n                obj.gt_instance_masks,\n                obj.gt_boxes3d,\n                obj.gt_boxes3d_cam_coords,\n                obj.gt_boxes3d_cam_coords_3d,\n                obj.gt_boxes3d_cam_coords_3d_dims,\n                obj.gt_boxes3d_cam_coords_3d_dims_classes,\n                obj.gt_boxes3d_cam_coords_3"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    # We use a global variable to store the schema and avoid creating a new one for each object.\n    global _SCHEMA\n\n    # If the object is already a tuple, we can directly return it.\n    if isinstance(obj, tuple):\n        return obj\n\n    # If the object is a string, we can directly return it.\n    if isinstance(obj, str):\n        return obj\n\n    # If the object is a bytes object, we can directly return it.\n    if isinstance(obj, bytes):\n        return obj\n\n    # If the object is a list, we can recursively flatten each element and concatenate them.\n    if isinstance(obj, list):\n        res = []\n        for o in obj:\n            res.extend(flatten_to_tuple(o))\n        return tuple(res)\n\n    # If the object is a tuple, we can recursively flatten each element and concatenate them.\n    if isinstance(obj, tuple):\n        res = []\n        for o in obj:\n            res.extend(flatten_to_tuple(o))\n        return tuple(res)\n\n    # If the object is a mapping collection, we can recursively flatten each key and value and concatenate them.\n    if isinstance(obj, dict):\n        res = []\n        for k, v in obj.items():\n            res.extend(flatten_to_tuple(k))\n            res.extend(flatten_to_tuple(v))\n        return tuple(res)\n\n    # If the object is an Instances object, we can recursively flatten each field and concatenate them.\n    if isinstance(obj, Instances):\n        res = []\n        for field in obj.fields():\n            res.extend(flatten_to_tuple(getattr(obj, field)))\n        return tuple(res)\n\n    # If the object is a Boxes object, we can recursively flatten each box and concatenate them.\n    if isinstance(obj, Boxes):\n        res = []\n        for box in obj:\n            res.extend(flatten_to_tuple(box))"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if groups is None:\n        return None\n\n    if equations is None:\n        return None\n\n    n_groups = groups.shape[0]\n    n_assets = groups.shape[1]\n    n_equations = equations.shape[0]\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        parts = equation.split(\"<=\")\n        if len(parts) != 2:\n            raise ValueError(\n                f\"Invalid equation format: {equation}. Expected format is 'group_name <= value'.\"\n            )\n\n        group_name = parts[0].strip()\n        value = float(parts[1].strip())\n\n        group_indices = np.where(groups == group_name)[0]\n\n        if len(group_indices) == 0:\n            if raise_if_group_missing:\n                raise ValueError(\n                    f\"Group {group_name} not found in {names[0]} array.\"\n                )\n            else:\n                warnings.warn(\n                    f\"Group {group_name} not found in {names[0]} array. Equation {equation} will be ignored.\"\n                )\n                continue\n\n        left[i, group_indices] = 1\n        right[i] = value\n\n    if sum_to_one:\n        left = np.hstack((left, np.ones((n_equations, 1))))\n        right = np.hstack((right, np.ones(n_equations)))\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    n_groups = groups.shape[0]\n    n_assets = groups.shape[1]\n    n_equations = equations.shape[0]\n\n    # Create a dictionary to map group names to indices\n    group_names = {f\"group_{i}\": i for i in range(n_groups)}\n\n    # Create a dictionary to store the left and right sides of the equations\n    left_matrix = np.zeros((n_equations, n_assets))\n    right_matrix = np.zeros(n_equations)\n\n    # Iterate over each equation\n    for i, equation in enumerate(equations):\n        # Split the equation into its components\n        components = equation.split(\"<=\")\n        left_components = components[0].split(\"+\")\n        right_components = components[1].split(\"+\")\n\n        # Iterate over each component of the left side\n        for component in left_components:\n            # Split the component into its coefficient and group name\n            coefficient, group_name = component.split(\"*\")\n            coefficient = float(coefficient)\n\n            # Check if the group name is in the dictionary\n            if group_name not in group_names:\n                if raise_if_group_missing:\n                    raise ValueError(\n                        f\"Group '{group_name}' not found in {names[0]} array.\"\n                    )\n                else:\n                    warnings.warn(\n                        f\"Group '{group_name}' not found in {names[0]} array.\"\n                    )\n                    continue\n\n            # Get the index of the group\n            group_index = group_names[group_name]\n\n            # Add the coefficient to the left matrix\n            left_matrix[i, group_index] = coefficient\n\n        # Iterate over each component of the right side\n        for component in right_components:\n            # Split the component into its coefficient and group name\n            coefficient, group_name = component.split(\"*\")\n            coefficient = float(coefficient)\n\n            # Check if the group name is in the dictionary"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert input arrays to numpy arrays\n    groups = np.array(groups)\n    equations = np.array(equations)\n\n    # Check if all groups mentioned in the equations are part of the input groups\n    missing_groups = []\n    for equation in equations:\n        groups_in_equation = re.findall(r\"\\b\\w+\\b\", equation)\n        if not all(group in groups for group in groups_in_equation):\n            missing_groups.extend(group for group in groups_in_equation if group not in groups)\n\n    if missing_groups:\n        if raise_if_group_missing:\n            raise ValueError(\n                f\"The following groups mentioned in the {names[1]} are not part of the {names[0]} array: {missing_groups}\"\n            )\n        else:\n            warnings.warn(\n                f\"The following groups mentioned in the {names[1]} are not part of the {names[0]} array: {missing_groups}\"\n            )\n            return None\n\n    # Extract the coefficients and the groups from the equations\n    coefficients = []\n    groups_in_equation = []\n    for equation in equations:\n        coefficients_in_equation = re.findall(r\"[+-]?\\d+\\.?\\d*\", equation)\n        coefficients_in_equation = [float(coefficient) for coefficient in coefficients_in_equation]\n        groups_in_equation_in_equation = re.findall(r\"\\b\\w+\\b\", equation)\n        coefficients.append(coefficients_in_equation)\n        groups_in_equation.append(groups_in_equation_in_equation)\n\n    # Create the left and right matrices\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n    for i, (coefficients_in_equation, groups_in_equation_in_equation) in enumerate(\n        zip(coefficients, groups_in_equation)\n    ):\n        for j, group in enumerate(groups_in_equation_in_"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.array(groups)\n    equations = np.array(equations)\n\n    if groups.ndim != 2:\n        raise ValueError(f\"{names[0]} must be a 2D array\")\n\n    if equations.ndim != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array\")\n\n    if groups.shape[0] != groups.shape[1]:\n        raise ValueError(f\"{names[0]} must be a square matrix\")\n\n    if groups.shape[0] != equations.shape[0]:\n        raise ValueError(\n            f\"{names[0]} and {names[1]} must have the same number of rows\"\n        )\n\n    if np.any(np.sum(groups, axis=1) == 0):\n        raise ValueError(f\"{names[0]} must not contain any zero rows\")\n\n    if np.any(np.sum(groups, axis=0) == 0):\n        raise ValueError(f\"{names[0]} must not contain any zero columns\")\n\n    if np.any(groups < 0):\n        raise ValueError(f\"{names[0]} must not contain any negative values\")\n\n    if sum_to_one:\n        if np.any(np.sum(groups, axis=1) != 1):\n            raise ValueError(f\"{names[0]} must sum to one along the rows\")\n\n    # Find the indices of the groups mentioned in the equations\n    group_indices = []\n    for equation in equations:\n        groups_in_equation = [\n            group_name for group_name in groups if group_name in equation\n        ]\n        if len(groups_in_equation) == 0:\n            if raise_if_group_missing:\n                raise ValueError(\n                    f\"No groups found in {names[1]} for equation: {equation}\"\n                )\n            else:\n                warnings.warn(\n                    f\"No groups found in {names[1]} for equation: {equation}\"\n                )\n                return None\n        group_indices.append(\n            ["}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.array(groups)\n    equations = np.array(equations)\n\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"The '{names[0]}' parameter should be a 2D array of shape (n_groups, n_assets).\"\n        )\n\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"The '{names[1]}' parameter should be a 1D array of shape (n_equations,).\"\n        )\n\n    if not np.all(np.isin(groups, [0, 1])):\n        raise ValueError(\n            f\"The '{names[0]}' parameter should contain only 0s and 1s, representing the presence or absence of assets in each group.\"\n        )\n\n    if not np.all(np.isin(equations, [\"=\", \"<=\"])):\n        raise ValueError(\n            f\"The '{names[1]}' parameter should contain only '=' and '<=' strings, representing the type of inequality for each equation.\"\n        )\n\n    if sum_to_one and not np.all(np.isclose(groups.sum(axis=1), 1)):\n        raise ValueError(\n            f\"If '{names[0]}' contains groups with different numbers of assets, the 'sum_to_one' parameter should be set to False.\"\n        )\n\n    n_groups, n_assets = groups.shape\n    n_equations = equations.shape[0]\n\n    if n_equations != n_assets:\n        raise ValueError(\n            f\"The number of equations ({n_equations}) should be equal to the number of assets ({n_assets}).\"\n        )\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        groups_in_equation = []\n        for j, group in enumerate(groups):\n            if np.any(group == 1):\n                groups_in_equation.append(j"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    if groups.ndim != 2:\n        raise ValueError(f\"{names[0]} must be a 2D array\")\n\n    if equations.ndim != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array\")\n\n    n_groups, n_assets = groups.shape\n\n    if n_groups == 0:\n        if raise_if_group_missing:\n            raise ValueError(f\"{names[0]} must not be empty\")\n        else:\n            warnings.warn(f\"{names[0]} must not be empty\")\n            return None, None\n\n    if sum_to_one:\n        groups = np.concatenate((groups, np.ones((1, n_assets))), axis=0)\n\n    left = []\n    right = []\n\n    for eq in equations:\n        if \"<=\" in eq:\n            eq = eq.replace(\"<=\", \"\")\n            left_side, right_side = eq.split(\"=\")\n            right_side = right_side.strip()\n        else:\n            left_side, right_side = eq.split(\"=\")\n            right_side = right_side.strip()\n\n        left_side = left_side.strip()\n        left_side = left_side.split(\"+\")\n\n        left_row = np.zeros(n_assets)\n\n        for group in left_side:\n            group = group.strip()\n            if group.startswith(\"-\"):\n                group = group[1:]\n                sign = -1\n            else:\n                sign = 1\n\n            if group.endswith(\"*\"):\n                group = group[:-1]\n                weight = 1\n            else:\n                weight = float(group.split(\"*\")[-1])\n\n            if group.endswith(\"^2\"):\n                group = group[:-3]\n                power = 2\n            elif group.endswith(\"^3\"):\n                group = group[:-3]"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if groups and equations are provided as numpy arrays\n    if not isinstance(groups, np.ndarray):\n        groups = np.array(groups)\n    if not isinstance(equations, np.ndarray):\n        equations = np.array(equations)\n\n    # Check if groups and equations are provided as 2D arrays\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"{names[0]} should be a 2D array, but got {groups.ndim}D array.\"\n        )\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"{names[1]} should be a 1D array, but got {equations.ndim}D array.\"\n        )\n\n    # Check if all elements in groups are non-negative\n    if (groups < 0).any():\n        raise ValueError(\n            f\"All elements in {names[0]} should be non-negative, but found negative values.\"\n        )\n\n    # Check if all elements in equations are strings\n    if not all(isinstance(equation, str) for equation in equations):\n        raise ValueError(\n            f\"All elements in {names[1]} should be strings, but found non-string values.\"\n        )\n\n    # Check if all groups mentioned in equations are part of the groups array\n    groups_mentioned = set()\n    for equation in equations:\n        groups_mentioned.update(re.findall(r\"\\d+\", equation))\n    if not all(group in groups_mentioned for group in range(groups.shape[0])):\n        if raise_if_group_missing:\n            raise ValueError(\n                f\"All groups mentioned in {names[1]} should be part of {names[0]}, but found missing groups.\"\n            )\n        else:\n            warnings.warn(\n                f\"All groups mentioned in {names[1]} should be part of {names[0]}, but found missing groups.\"\n            )\n\n    # Split equations into left and right sides\n    left, right = [], []\n    for equation in equations:\n        left_side, right_side = equation.split(\""}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Initialize an empty list to store the left and right matrices\n    left, right = [], []\n\n    # Loop over each equation\n    for equation in equations:\n\n        # Split the equation into its components\n        left_side, right_side = equation.split(\"<=\")\n\n        # Initialize the left and right matrices for this equation\n        left_matrix, right_matrix = np.zeros(groups.shape), np.zeros(groups.shape[0])\n\n        # Loop over each group in the left side of the equation\n        for group in left_side.split(\"+\"):\n\n            # Split the group into its components\n            group_name, group_weight = group.split(\"*\")\n\n            # Find the index of the group in the groups array\n            group_index = np.where(groups == group_name)[0][0]\n\n            # Add the group weight to the left matrix\n            left_matrix[group_index, :] = float(group_weight)\n\n        # Add the left matrix and right matrix to the list of matrices\n        left.append(left_matrix)\n        right.append(float(right_side))\n\n    # Check if all groups in the equations are part of the input groups\n    if np.all(np.isin(groups, left)):\n\n        # Convert the left and right matrices to numpy arrays\n        left = np.array(left)\n        right = np.array(right)\n\n        # If sum_to_one is True, add a constraint to the left matrix that all elements in a group sum to one\n        if sum_to_one:\n            left = np.concatenate((left, np.ones((1, left.shape[1]))), axis=0)\n\n        # Return the left and right matrices\n        return left, right\n\n    # If all groups in the equations are not part of the input groups, raise an error or issue a warning\n    else:\n        if raise_if_group_missing:\n            raise ValueError(\n                f\"All groups mentioned in the {names[1]} must be part of the {names[0]}.\"\n            )\n       "}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert input arrays to numpy arrays\n    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    # Initialize empty lists for left and right sides of the inequality\n    left = []\n    right = []\n\n    # Iterate over each equation\n    for equation in equations:\n        # Split the equation into left and right sides\n        left_side, right_side = equation.split(\"<=\")\n\n        # Initialize empty lists for the left and right sides of the current equation\n        left_side_list = []\n        right_side_list = []\n\n        # Iterate over each group in the left side of the equation\n        for group in left_side.split(\"+\"):\n            # Remove any leading or trailing whitespace from the group\n            group = group.strip()\n\n            # Check if the group is present in the input groups\n            group_indices = np.where(groups == group)[0]\n\n            # If the group is not present, raise an error or issue a warning\n            if len(group_indices) == 0:\n                if raise_if_group_missing:\n                    raise ValueError(\n                        f\"Group '{group}' mentioned in equation '{equation}' is not present in the {names[0]} array.\"\n                    )\n                else:\n                    warnings.warn(\n                        f\"Group '{group}' mentioned in equation '{equation}' is not present in the {names[0]} array. Ignoring this equation.\"\n                    )\n                    continue\n\n            # If the group is present, add it to the left side of the current equation\n            left_side_list.append(group_indices[0])\n\n        # If the left side of the current equation is empty, skip it\n        if not left_side_list:\n            continue\n\n        # Convert the left side of the current equation to a numpy array\n        left_side_array = np.zeros(groups.shape[1])\n        left_side_array[left_side_list] = 1\n\n        # Add the left side of the current equation to the list of left sides\n        left.append(left_side_array"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.array(groups)\n    equations = np.array(equations)\n\n    if len(groups.shape) != 2:\n        raise ValueError(\n            f\"The input '{names[0]}' must be a 2D array. The shape of the input '{names[0]}' is {groups.shape}, which is not 2D.\"\n        )\n\n    if len(equations.shape) != 1:\n        raise ValueError(\n            f\"The input '{names[1]}' must be a 1D array. The shape of the input '{names[1]}' is {equations.shape}, which is not 1D.\"\n        )\n\n    n_groups, n_assets = groups.shape\n    n_equations = len(equations)\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        equation = equation.replace(\" \", \"\")\n        left[i] = np.zeros(n_assets)\n\n        if \"<=\" in equation:\n            equation = equation.split(\"<=\")[0]\n        elif \">=\" in equation:\n            equation = equation.split(\">=\")[0]\n\n        for group in equation.split(\"+\"):\n            if group.startswith(\"-\"):\n                group = group[1:]\n                sign = -1\n            else:\n                sign = 1\n\n            if \"*\" in group:\n                group, value = group.split(\"*\")\n            else:\n                value = 1\n\n            if group.isdigit():\n                value = int(group)\n                group = \"\"\n\n            if group == \"\":\n                right[i] += sign * float(value)\n            else:\n                group_index = np.where(group == groups)[0][0]\n                left[i, group_index] += sign * float(value)\n\n    if sum_to_one:\n        left = np.concatenate((left, np.ones((n_equations, 1))), axis=1)"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if groups is None:\n        return None\n\n    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    n_groups, n_assets = groups.shape\n\n    if n_assets != groups.shape[1]:\n        raise ValueError(\n            f\"The shape of the '{names[0]}' array is inconsistent: {groups.shape}.\"\n        )\n\n    if len(equations.shape) != 1:\n        raise ValueError(\n            f\"The shape of the '{names[1]}' array is inconsistent: {equations.shape}.\"\n        )\n\n    if n_assets != groups.shape[1]:\n        raise ValueError(\n            f\"The shape of the '{names[0]}' array is inconsistent: {groups.shape}.\"\n        )\n\n    left = np.zeros((len(equations), n_assets))\n    right = np.zeros(len(equations))\n\n    for i, equation in enumerate(equations):\n        equation = equation.strip()\n\n        if equation.startswith(\"-\"):\n            equation = equation[1:]\n            sign = -1\n        else:\n            sign = 1\n\n        parts = equation.split(\"<=\")\n\n        if len(parts) != 2:\n            raise ValueError(\n                f\"Invalid equation: {equation}. Expected format: 'group1 <= group2'.\"\n            )\n\n        group1, group2 = parts\n\n        if group1.strip() == \"\":\n            group1 = \"0\"\n\n        if group2.strip() == \"\":\n            group2 = \"0\"\n\n        group1 = group1.strip()\n        group2 = group2.strip()\n\n        if group1 not in groups:\n            if raise_if_group_missing:\n                raise ValueError(f\"Group '{group1}' not found in '{names[0]}' array.\")\n            else:\n                warnings.warn(\n                    f\"Group '{group1}' not found in '{names[0]}' array. Ignoring equation '{equation}'.\""}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if not isinstance(groups, np.ndarray):\n        groups = np.array(groups)\n\n    if not isinstance(equations, np.ndarray):\n        equations = np.array(equations)\n\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"The '{names[0]}' parameter must be a 2D array, but got shape {groups.shape}.\"\n        )\n\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"The '{names[1]}' parameter must be a 1D array, but got shape {equations.shape}.\"\n        )\n\n    if equations.size == 0:\n        raise ValueError(\n            f\"The '{names[1]}' parameter must have at least one element, but got an empty array.\"\n        )\n\n    if groups.shape[0] != equations.shape[0]:\n        raise ValueError(\n            f\"The '{names[1]}' and '{names[0]}' parameters must have the same number of elements, but got {equations.shape[0]} equations and {groups.shape[0]} groups.\"\n        )\n\n    if sum_to_one:\n        if groups.shape[0] != groups.shape[1]:\n            raise ValueError(\n                f\"If 'sum_to_one' is True, the '{names[0]}' parameter must be a square matrix, but got shape {groups.shape}.\"\n            )\n\n    # Find all groups mentioned in the equations\n    groups_mentioned = set()\n    for equation in equations:\n        groups_mentioned.update(re.findall(r\"\\b\\w+\\b\", equation))\n\n    # Find the indices of the groups mentioned in the equations\n    group_indices = []\n    for group in groups_mentioned:\n        indices = np.where(groups == group)[0]\n        if indices.size == 0:\n            if raise_if_group_missing:\n                raise ValueError(\n                    f\"The '{names[1]}' parameter mentions group '{group}', but this group is not found in the '"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert input arrays to numpy arrays if they are not already\n    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    # Check if all groups mentioned in the equations are part of the input groups\n    all_groups_in_input = True\n    for eq in equations:\n        groups_in_eq = re.findall(r\"[A-Z]+\", eq)\n        if len(groups_in_eq) > 0:\n            if not all(g in groups for g in groups_in_eq):\n                all_groups_in_input = False\n                if raise_if_group_missing:\n                    raise ValueError(\n                        f\"All groups mentioned in the {names[1]} must be part of the {names[0]} array.\"\n                    )\n                else:\n                    warnings.warn(\n                        f\"All groups mentioned in the {names[1]} must be part of the {names[0]} array.\"\n                    )\n\n    if not all_groups_in_input:\n        return None\n\n    # Initialize the left and right matrices\n    n_assets = groups.shape[1]\n    n_equations = equations.shape[0]\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    # Iterate over each equation and update the left and right matrices accordingly\n    for i, eq in enumerate(equations):\n        # Extract the coefficients and group names from the equation\n        coeffs = re.findall(r\"-?\\d+\\.\\d+|-?\\d+\", eq)\n        groups_in_eq = re.findall(r\"[A-Z]+\", eq)\n\n        # Convert the coefficients to float\n        coeffs = [float(c) for c in coeffs]\n\n        # Find the indices of the groups in the input groups array\n        group_indices = [np.where(groups == g)[0][0] for g in groups_in_eq]\n\n        # Update the left and right matrices\n        left[i, group_indices] = coeffs"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Define a dictionary to store the coefficients of each group\n    group_coefficients = {}\n\n    # Loop through each equation and extract the coefficients of each group\n    for i, equation in enumerate(equations):\n        # Split the equation into parts\n        parts = equation.split(\"<=\")\n        left_side = parts[0].strip()\n        right_side = parts[1].strip()\n\n        # Loop through each group and extract the coefficients\n        for group in groups:\n            group_name = group[0]\n            group_coef = group[1]\n            if group_name in left_side:\n                group_coefficients[group_name] = group_coef\n            elif group_name in right_side:\n                group_coefficients[group_name] = -group_coef\n\n    # If no groups were found, return None\n    if not group_coefficients:\n        if raise_if_group_missing:\n            raise ValueError(\n                f\"No groups found in {names[1]} that are part of the {names[0]}.\"\n            )\n        else:\n            warnings.warn(\n                f\"No groups found in {names[1]} that are part of the {names[0]}.\"\n            )\n            return None\n\n    # Create a matrix of coefficients for each group\n    group_coefficients_matrix = np.zeros((len(group_coefficients), len(groups[0])))\n    for i, group_name in enumerate(group_coefficients):\n        group_coefficients_matrix[i] = group_coefficients[group_name]\n\n    # Create a matrix of coefficients for each equation\n    equation_coefficients_matrix = np.zeros((len(equations), len(groups[0])))\n    for i, equation in enumerate(equations):\n        parts = equation.split(\"<=\")\n        left_side = parts[0].strip()\n        right_side = parts[1].strip()\n\n        # Loop through each group and extract the coefficients\n        for group in groups:\n            group_name = group[0]"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.array(groups)\n    equations = np.array(equations)\n\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"{names[0]} must be a 2D array of shape (n_groups, n_assets)\"\n        )\n\n    if equations.ndim != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array of shape (n_equations,)\")\n\n    if sum_to_one:\n        if not np.allclose(groups.sum(axis=1), 1):\n            raise ValueError(f\"All groups in {names[0]} must sum to 1\")\n\n    if raise_if_group_missing:\n        for group in groups:\n            if not np.any(group):\n                raise ValueError(f\"Group must not be all zeros in {names[0]}\")\n\n    left = []\n    right = []\n\n    for equation in equations:\n        left_row = []\n        right_row = 0\n\n        for group in groups:\n            if np.any(group):\n                group_sum = group.sum()\n                if group_sum == 0:\n                    left_row.append(0)\n                else:\n                    left_row.append(group / group_sum)\n\n                if np.any(group):\n                    right_row += np.dot(group, group) / group_sum\n            else:\n                left_row.append(0)\n\n        left.append(left_row)\n        right.append(right_row)\n\n    if not left:\n        return None\n\n    return np.array(left), np.array(right)"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check that the input arrays have the correct shape\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"Invalid shape for {names[0]} array. Expected 2D array, but got {groups.shape}.\"\n        )\n\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"Invalid shape for {names[1]} array. Expected 1D array, but got {equations.shape}.\"\n        )\n\n    # Create a dictionary to store the equations\n    equation_dict = {}\n\n    # Iterate over the equations and extract the group names and coefficients\n    for equation in equations:\n        # Split the equation into parts\n        parts = equation.split(\"=\")\n\n        # Check if the equation has the correct format\n        if len(parts) != 2:\n            raise ValueError(\n                f\"Invalid equation format: {equation}. Expected '=' to separate the left and right sides.\"\n            )\n\n        # Extract the group name and coefficients\n        group_name, coefficients = parts\n        group_name = group_name.strip()\n        coefficients = coefficients.strip()\n\n        # Split the coefficients into a list of floats\n        try:\n            coefficients = [float(c) for c in coefficients.split()]\n        except ValueError:\n            raise ValueError(\n                f\"Invalid coefficient format: {coefficients}. Expected a list of floats.\"\n            )\n\n        # Check if the group name is in the groups array\n        if group_name not in groups:\n            if raise_if_group_missing:\n                raise ValueError(\n                    f\"Group '{group_name}' not found in {names[0]} array.\"\n                )\n            else:\n                warnings.warn(\n                    f\"Group '{group_name}' not found in {names[0]} array. Ignoring equation '{equation}'.\"\n                )\n                continue\n\n        # Add the equation to the dictionary\n        equation_dict[group_name] = coefficients\n\n    # Check if any equations were found\n    if not equation_dict:\n        raise ValueError(\n            f\""}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if all groups are present in the groups array\n    all_groups_present = np.all(\n        [\n            np.any(np.all(groups == group, axis=1))\n            for group in [\n                group\n                for group in [\n                    group.strip() for group in [group for group in equations]\n                ]\n            ]\n        ]\n    )\n\n    # If any group is missing, raise an error or issue a warning\n    if not all_groups_present:\n        if raise_if_group_missing:\n            raise ValueError(\n                f\"Some groups mentioned in the {names[1]} are not present in the {names[0]}.\"\n            )\n        else:\n            warnings.warn(\n                f\"Some groups mentioned in the {names[1]} are not present in the {names[0]}.\"\n            )\n            return None\n\n    # Split the equations into left and right sides\n    left_sides = [\n        group.strip() for group in [group for group in equations] if \"<=\" in group\n    ]\n    right_sides = [\n        group.strip() for group in [group for group in equations] if \"<=\" not in group\n    ]\n\n    # Initialize the left and right matrices\n    left = np.zeros((len(left_sides), groups.shape[1]))\n    right = np.zeros(len(right_sides))\n\n    # Loop through the left and right sides and fill in the matrices\n    for i, group in enumerate(left_sides):\n        left_group, right_group = group.split(\"<=\")\n        left_group = left_group.strip()\n        right_group = right_group.strip()\n\n        # Find the indices of the left and right groups in the groups array\n        left_indices = np.where(\n            np.any(groups == left_group, axis=1)\n        )[0]  # Find indices of left group\n        right_indices = np.where(\n            np.any(groups == right_group, axis=1)\n        )[0] "}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert input to numpy arrays\n    groups = np.array(groups)\n    equations = np.array(equations)\n\n    # Extract the group names from the equations\n    group_names = [group_name.strip() for eq in equations for group_name in re.findall(r\"\\b\\w+\\b\", eq)]\n\n    # Check if all groups mentioned in the equations are part of the input groups\n    missing_groups = [group for group in group_names if group not in groups]\n    if missing_groups and raise_if_group_missing:\n        raise ValueError(f\"The following groups are not part of the input {names[0]}: {', '.join(missing_groups)}\")\n    elif missing_groups:\n        warnings.warn(f\"The following groups are not part of the input {names[0]}: {', '.join(missing_groups)}\")\n\n    # Create a dictionary mapping group names to their indices in the groups array\n    group_indices = {group: i for i, group in enumerate(groups)}\n\n    # Create the left and right matrices\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n\n    for i, equation in enumerate(equations):\n        # Extract the coefficients for each group in the equation\n        coeffs = {}\n        for group_name in group_names:\n            coeff = re.search(rf\"\\b{group_name}\\b\", equation)\n            if coeff:\n                coeffs[group_name] = float(coeff.group())\n\n        # Update the left and right matrices with the coefficients\n        for group_name, coeff in coeffs.items():\n            left[i, group_indices[group_name]] = coeff\n\n        # Extract the constant term from the equation\n        constant = re.search(r\"\\d+\\.?\\d*\", equation)\n        if constant:\n            right[i] = float(constant.group())\n\n    # Check if all elements in a group should sum to one\n    if sum_to"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.array(groups)\n    equations = np.array(equations)\n\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"`{names[0]}` must be a 2D array, but got {groups.ndim} dimensions.\"\n        )\n\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"`{names[1]}` must be a 1D array, but got {equations.ndim} dimensions.\"\n        )\n\n    n_groups, n_assets = groups.shape\n\n    if n_groups == 0:\n        raise ValueError(f\"`{names[0]}` must have at least one group.\")\n\n    if n_assets == 0:\n        raise ValueError(f\"`{names[0]}` must have at least one asset.\")\n\n    if n_equations == 0:\n        raise ValueError(f\"`{names[1]}` must have at least one equation.\")\n\n    # Create a dictionary to store the coefficients for each group\n    coefs = {}\n    for group in groups:\n        coefs[tuple(group)] = np.zeros(n_assets)\n\n    # Parse each equation and update the coefficients for each group\n    for equation in equations:\n        # Split the equation into its components\n        parts = equation.split(\"<=\")\n        if len(parts) != 2:\n            raise ValueError(\n                f\"Invalid equation format: {equation}. Expected format: 'group1 <= group2'\"\n            )\n\n        # Parse the left and right sides of the equation\n        left, right = parts\n\n        # Extract the groups and coefficients from the left and right sides\n        left_groups, left_coefs = parse_equation(left)\n        right_groups, right_coefs = parse_equation(right)\n\n        # Update the coefficients for each group\n        for group, coef in zip(left_groups, left_coefs):\n            coefs[tuple(group)] += coef\n\n        for group, coef in zip(right_groups, right_co"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Define a regular expression pattern to extract the group names from the equations\n    pattern = r\"\\b\\w+\\b\"\n\n    # Extract the group names from the equations\n    group_names = [re.findall(pattern, eq) for eq in equations]\n\n    # Create a dictionary to map group names to indices\n    group_dict = {group: i for i, group in enumerate(groups)}\n\n    # Initialize the left and right arrays\n    left = []\n    right = []\n\n    # Iterate over the equations and group names\n    for eq, names in zip(equations, group_names):\n        # Initialize the equation's left and right sides\n        eq_left = np.zeros((len(names), len(groups)))\n        eq_right = 0\n\n        # Iterate over the group names in the equation\n        for name in names:\n            # If the group name is not in the group dictionary, raise an error or issue a warning\n            if name not in group_dict:\n                if raise_if_group_missing:\n                    raise ValueError(f\"Group '{name}' not found in {names[0]}.\")\n                else:\n                    warnings.warn(f\"Group '{name}' not found in {names[0]}.\")\n                    continue\n\n            # Get the index of the group in the groups array\n            idx = group_dict[name]\n\n            # Add the group to the left side of the equation\n            eq_left[idx] = 1\n\n            # If the sum_to_one flag is True, add the negative of the group to the right side of the equation\n            if sum_to_one:\n                eq_right -= 1\n\n        # Append the left and right sides of the equation to the left and right arrays\n        left.append(eq_left)\n        right.append(eq_right)\n\n    # If no groups were found in the equations, return None\n    if not left:\n        return None\n\n    # Return the left and right arrays as numpy arrays\n    return np.array(left), np.array(right)"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import os\n    import sys\n    import tempfile\n    import importlib\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import Dataset\n    import detectron2.data.transforms as T\n    import detectron2.data.detection_utils as D\n    import detectron2.data.datasets as DD\n    import detectron2.data.build as DB\n    import detectron2.data.common as DC\n    import detectron2.modeling.roi_heads as DRH\n    import detectron2.modeling.roi_heads.fast_rcnn as DRHF\n    import detectron2.modeling.roi_heads.roi_head as DRHROI\n    import detectron2.modeling.roi_heads.mask_head as DRHM\n    import detectron2.modeling.roi_heads.keypoint_head as DRHKP\n    import detectron2.modeling.roi_heads.fast_rcnn as DRHF\n    import detectron2.modeling.roi_heads.fast_rcnn as DRHF\n    import detectron2.modeling.roi_heads.fast_rcnn as DRHF\n    import detectron2.modeling.roi_heads.fast_rcnn as DRHF\n    import detectron2.modeling.roi_heads.fast_rcnn as DRHF\n    import detectron2.modeling.roi_heads.fast_rcnn as DRHF\n    import detectron2.modeling.roi_heads.fast_rcnn as DRHF\n    import detectron2.modeling.roi_heads.fast_rcnn as DRHF\n    import detectron2.modeling.roi_heads.fast_rcnn as DRHF\n    import detectron2.modeling.roi_heads.fast_rcnn as DRHF\n    import detectron2.modeling.roi_heads.fast_rcnn as DRHF\n    import detectron2."}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import inspect\n    import os\n    import sys\n    import torch\n    from detectron2.structures import Instances\n\n    # Get the current working directory\n    cwd = os.getcwd()\n\n    # Create a temporary directory for the new module\n    tmp_dir = os.path.join(cwd, \"tmp\")\n    if not os.path.exists(tmp_dir):\n        os.mkdir(tmp_dir)\n\n    # Create a new module for the class\n    module_name = \"tmp_module\"\n    module_path = os.path.join(tmp_dir, module_name + \".py\")\n    with open(module_path, \"w\") as f:\n        f.write(\"class Instances:\\n\")\n        for field in fields:\n            f.write(f\"    {field[0]} = {field[1]}\\n\")\n\n    # Add the temporary directory to the path\n    sys.path.append(tmp_dir)\n\n    # Import the new module\n    module = __import__(module_name)\n\n    # Get the new class\n    new_class = getattr(module, \"Instances\")\n\n    # Set up the environment so that torchscript can recognize and work with the new class\n    torch._C._jit_set_class_export(Instances, new_class)\n\n    # Yield the new class\n    yield new_class\n\n    # Clean up\n    torch._C._jit_set_class_export(Instances, None)\n    sys.path.remove(tmp_dir)\n    os.remove(module_path)\n    os.rmdir(tmp_dir)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import os\n    import sys\n    import tempfile\n    import types\n\n    from detectron2.structures import Instances\n    from torch import nn\n\n    class NewInstances(nn.Module):\n        def __init__(self):\n            super().__init__()\n\n            for field in fields:\n                setattr(self, field[0], field[1])\n\n        def __call__(self, *args, **kwargs):\n            return self\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        tmp_file = os.path.join(tmpdir, 'instances.py')\n        with open(tmp_file, 'w') as f:\n            f.write('from detectron2.structures import Instances\\n')\n            f.write('class NewInstances(Instances):\\n')\n            for field in fields:\n                f.write(f'    {field[0]} = {field[1]}\\n')\n\n        sys.path.insert(0, tmpdir)\n        import instances\n        sys.path.pop(0)\n\n        instances.Instances = NewInstances\n\n        yield NewInstances"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import torch\n    import os\n    import sys\n    import tempfile\n    import importlib\n    from torch import nn\n    from detectron2.structures import Instances\n\n    class NewInstances(Instances):\n        def __init__(self, *args, **kwargs):\n            super(NewInstances, self).__init__(*args, **kwargs)\n\n    for field in fields:\n        setattr(NewInstances, field[0], field[1])\n\n    # Write the new class to a temporary file\n    temp_file = tempfile.NamedTemporaryFile(delete=False)\n    temp_file.write(f'class NewInstances(Instances):\\n'.encode())\n    for field in fields:\n        temp_file.write(f'    {field[0]} = {field[1]}\\n'.encode())\n    temp_file.close()\n\n    # Import the new class from the temporary file\n    module_name = 'temp_module'\n    spec = importlib.util.spec_from_file_location(module_name, temp_file.name)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n\n    # Set up the environment to recognize the new class\n    sys.modules[module_name] = module\n\n    # Yield the new class\n    yield module.NewInstances\n\n    # Clean up after ourselves\n    del sys.modules[module_name]\n    os.remove(temp_file.name)"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import inspect\n    import os\n    import sys\n    import tempfile\n    import torch\n    import torch.utils.data\n    from torch.utils.data.dataset import T_co\n    from torch.utils.data.dataloader import _DataLoaderIter\n    from torch.utils.data.dataloader import _MultiProcessingDataLoaderIter\n    from torch.utils.data.dataloader import _SingleProcessDataLoaderIter\n    from torch.utils.data.dataloader import DataLoader\n    from torch.utils.data.dataloader import _utils\n    from torch.utils.data.dataloader import _utils as utils\n    from torch.utils.data.dataloader import default_collate\n    from torch.utils.data.dataloader import default_collate_fn\n    from torch.utils.data.dataloader import _collate_fn_wrapper\n    from torch.utils.data.dataloader import _collate_fn_wrapper_fn\n    from torch.utils.data.dataloader import _collate_fn_wrapper_fn_with_indices\n    from torch.utils.data.dataloader import _collate_fn_wrapper_with_indices\n    from torch.utils.data.dataloader import _worker_init_fn\n    from torch.utils.data.dataloader import _WorkerInfo\n    from torch.utils.data.dataloader import _MultiProcessingDataLoaderIter\n    from torch.utils.data.dataloader import _MultiProcessingDataLoaderIter_with_indices\n    from torch.utils.data.dataloader import _SingleProcessDataLoaderIter\n    from torch.utils.data.dataloader import _SingleProcessDataLoaderIter_with_indices\n    from torch.utils.data.dataloader import _DataLoaderIter\n    from torch.utils.data.dataloader import _DataLoaderIter_with_indices\n    from torch.utils.data.dataloader import _"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import os\n    import sys\n    import inspect\n    import tempfile\n    import importlib\n    from pathlib import Path\n    from typing import List, Optional, Union\n    from detectron2.structures import Instances\n\n    class NewInstances(Instances):\n        \"\"\"\n        This class is a subclass of 'Instances' and is used to replace 'Instances' in detectron2.\n        \"\"\"\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n\n        def __getitem__(self, item):\n            return super().__getitem__(item)\n\n        def __setitem__(self, key, value):\n            return super().__setitem__(key, value)\n\n        def __delitem__(self, key):\n            return super().__delitem__(key)\n\n        def __iter__(self):\n            return super().__iter__()\n\n        def __len__(self):\n            return super().__len__()\n\n        def __repr__(self):\n            return super().__repr__()\n\n        def __str__(self):\n            return super().__str__()\n\n        def __eq__(self, other):\n            return super().__eq__(other)\n\n        def __ne__(self, other):\n            return super().__ne__(other)\n\n        def __hash__(self):\n            return super().__hash__()\n\n        def __lt__(self, other):\n            return super().__lt__(other)\n\n        def __le__(self, other):\n            return super().__le__(other)\n\n        def __gt__(self, other):\n            return super().__gt__(other)\n\n        def __ge__(self, other):\n            return super().__ge__(other)\n\n        def __add__(self, other):\n            return super().__add__(other)\n\n        def __iadd__(self, other):\n            return super().__iadd__(other)\n\n        def __mul__(self, other):\n            return super().__mul__(other)\n\n        def __imul__(self, other):\n            return super().__im"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import os\n    import sys\n    import inspect\n    import tempfile\n    import importlib\n    import torch\n    import torch.nn as nn\n    from detectron2.structures import Instances\n\n    # Get the current module\n    current_module = inspect.getmodule(sys._getframe(1))\n\n    # Create a temporary file to write the new class definition\n    with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as f:\n        # Write the class definition to the temporary file\n        f.write(b'class Instances(\\nnn.Module):\\n')\n        for field in fields:\n            f.write(f'    {field}\\n'.encode())\n        f.write(b'\\n')\n        f.flush()\n\n        # Import the new class definition from the temporary file\n        module_name = os.path.splitext(os.path.basename(f.name))[0]\n        spec = importlib.util.spec_from_file_location(module_name, f.name)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n\n        # Set up the environment to recognize and work with the new class\n        torch.classes.register_module(module_name, module)\n\n        # Yield the new class\n        yield module.Instances\n\n        # Clean up after the context\n        torch.classes.unregister_module(module_name)\n        os.remove(f.name)"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import importlib\n    import os\n    import sys\n    import tempfile\n    from contextlib import contextmanager\n    from types import ModuleType\n\n    import torch\n    from detectron2.structures import Instances\n\n    # Create a new module for the new class\n    new_module = ModuleType(\"new_module\")\n    sys.modules[\"new_module\"] = new_module\n\n    # Define the new class\n    class_fields = \"\".join(\n        [\n            f\"{field_name}: torch.Tensor = torch.empty(0)\\n\"\n            for field_name, _ in fields\n        ]\n    )\n    new_class_str = f\"class NewClass(Instances):\\n{class_fields}\"\n    exec(new_class_str, new_module.__dict__)\n\n    # Create a temporary file to hold the new module\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n        f.write(new_class_str)\n        f.flush()\n        module_path = f.name\n\n    # Import the new module\n    spec = importlib.util.spec_from_file_location(\"new_module\", module_path)\n    new_module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(new_module)\n\n    # Set up the environment to recognize the new class\n    torch._C._jit_set_class_path(module_path)\n\n    # Yield the new class\n    yield new_module.NewClass\n\n    # Clean up\n    os.remove(module_path)\n    del sys.modules[\"new_module\"]\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import inspect\n    import os\n    import sys\n    import tempfile\n    from contextlib import contextmanager\n\n    import torch\n    from detectron2.structures import Instances\n\n    @contextmanager\n    def patch_instances_context(fields):\n        # Generate a new module for the class\n        new_module = tempfile.NamedTemporaryFile(mode=\"w\", delete=False)\n        new_module.write(\"class Instances:\\n\")\n        for field in fields:\n            new_module.write(f\"    {field[0]}: {field[1]}\\n\")\n        new_module.close()\n\n        # Add the new module to the system path\n        sys.path.append(os.path.dirname(new_module.name))\n\n        # Import the new class\n        import Instances as new_instances\n\n        # Set up the environment so that torchscript can recognize and work with the new class\n        torch._C._jit_set_class_exported(\"Instances\", new_instances.Instances)\n\n        # Yield the new class\n        yield new_instances.Instances\n\n        # Clean up after ourselves\n        sys.path.remove(os.path.dirname(new_module.name))\n        os.remove(new_module.name)\n\n    return patch_instances_context(fields)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import os\n    import sys\n    import torch\n    import importlib\n    from detectron2.structures.instances import Instances\n    from detectron2.utils.env import TORCH_VERSION\n\n    # Get the current working directory\n    cwd = os.getcwd()\n\n    # Create a temporary directory\n    temp_dir = os.path.join(cwd, \"temp\")\n    os.makedirs(temp_dir, exist_ok=True)\n\n    # Write the new class to a temporary file\n    with open(os.path.join(temp_dir, \"instances.py\"), \"w\") as f:\n        f.write(\"class Instances:\\n\")\n        for field in fields:\n            f.write(f\"    {field[0]} = {field[1]}\\n\")\n\n    # Import the new class\n    sys.path.append(temp_dir)\n    import instances\n    Instances = getattr(instances, \"Instances\")\n\n    # Set up the environment to recognize the new class\n    if TORCH_VERSION < (1, 11, 0):\n        torch._C._jit_set_class_exported(\"Instances\", True)\n\n    # Yield the new class\n    yield Instances\n\n    # Clean up by removing the temporary directory\n    os.rmdir(temp_dir)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import os\n    import sys\n    import inspect\n    import tempfile\n    import importlib\n    import torch\n    import detectron2.structures.instances as instances\n\n    # Get the path to the detectron2 directory\n    detectron2_dir = os.path.dirname(os.path.abspath(instances.__file__))\n\n    # Create a temporary directory for the new module\n    with tempfile.TemporaryDirectory() as temp_dir:\n        # Create a new module file in the temporary directory\n        module_file = os.path.join(temp_dir, 'instances.py')\n        with open(module_file, 'w') as f:\n            # Write the new class definition to the module file\n            f.write('class Instances:\\n')\n            for field in fields:\n                f.write(f'    {field[0]} = {field[1]}\\n')\n\n        # Add the temporary directory to the system path\n        sys.path.append(temp_dir)\n\n        # Import the new module\n        new_module = importlib.import_module('instances')\n\n        # Get the new class from the module\n        new_class = getattr(new_module, 'Instances')\n\n        # Replace the 'Instances' class in detectron2 with the new class\n        instances.Instances = new_class\n\n        # Yield the new class for use in the context\n        yield new_class\n\n        # Clean up by removing the temporary directory and removing the module from the system path\n        sys.path.remove(temp_dir)\n        del sys.modules['instances']\n        os.remove(module_file)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import os\n    import sys\n    import tempfile\n    import torch\n    from torch import nn\n    import detectron2.structures\n    from detectron2.structures import Instances\n\n    # Create a new module for the new class\n    module = nn.Module()\n\n    # Add fields to the module\n    for field_name, field_type in fields:\n        module.register_buffer(field_name, torch.empty(0))\n\n    # Create a temporary file to write the new module to\n    with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as f:\n        # Write the new module to the temporary file\n        f.write(module.__module__.encode('utf-8'))\n        f.write(b'\\n')\n        f.write(module.__class__.__name__.encode('utf-8'))\n        f.write(b' = nn.Module()\\n')\n        for field_name, field_type in fields:\n            f.write(field_name.encode('utf-8'))\n            f.write(b' = nn.Parameter(torch.empty(0))\\n')\n\n    # Add the temporary file to the Python path\n    sys.path.append(os.path.dirname(f.name))\n\n    # Import the new module\n    module = __import__(os.path.basename(f.name).replace('.py', ''))\n\n    # Get the new class\n    new_class = getattr(module, os.path.basename(f.name).replace('.py', ''))\n\n    # Replace the 'Instances' class in detectron2 with the new class\n    detectron2.structures.Instances = new_class\n\n    # Yield the new class\n    yield new_class\n\n    # Clean up\n    sys.path.remove(os.path.dirname(f.name))\n    os.remove(f.name)\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import importlib\n    import os\n    import sys\n    import tempfile\n    from contextlib import contextmanager\n    from inspect import signature\n\n    import torch\n\n    from detectron2.structures.instances import Instances\n\n    # Define the new class\n    class_fields = []\n    for field in fields:\n        class_fields.append(f\"{field[0]}: {field[1]}\")\n    class_fields = \"\\n\".join(class_fields)\n    class_def = f\"class Instances(\\n{class_fields}\\n):\\n\"\n\n    # Write the new class to a temporary file\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n        f.write(class_def)\n        f.flush()\n        file_path = f.name\n\n    # Import the new class\n    module_name = \"temp_instances\"\n    spec = importlib.util.spec_from_file_location(module_name, file_path)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n\n    # Set up the environment\n    sys.modules[module_name] = module\n    torch._C._jit_set_class_schema(module.Instances, Instances)\n\n    # Return the new class\n    yield module.Instances\n\n    # Clean up\n    del sys.modules[module_name]\n    os.remove(file_path)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import inspect\n    import os\n    import sys\n    import tempfile\n    import torch\n    from detectron2.structures import Instances\n    from detectron2.utils.env import TORCH_VERSION\n\n    # Create a new module for the class\n    module_name = \"temp_module\"\n    module = types.ModuleType(module_name)\n\n    # Define the new class\n    class_name = \"TempInstances\"\n    class_fields = fields\n    class_definition = f\"class {class_name}(Instances):\\n\"\n    for field in class_fields:\n        class_definition += f\"    {field[0]} = {field[1]}\\n\"\n\n    # Execute the class definition in the new module\n    exec(class_definition, module.__dict__)\n\n    # Get the new class from the module\n    new_class = getattr(module, class_name)\n\n    # Create a temporary file to hold the module\n    temp_file = tempfile.NamedTemporaryFile(delete=False)\n    temp_file.write(module.__str__().encode())\n    temp_file.close()\n\n    # Add the temporary file to the sys.path\n    sys.path.append(os.path.dirname(temp_file.name))\n\n    # Import the new class from the temporary file\n    new_class = importlib.import_module(module_name).TempInstances\n\n    # Set up the environment to recognize the new class\n    torch._C._jit_set_class_export_hooks(new_class, new_class.export_class)\n\n    # Return the new class\n    yield new_class\n\n    # Clean up the temporary file and remove it from sys.path\n    os.remove(temp_file.name)\n    sys.path.remove(os.path.dirname(temp_file.name))\n\n    # Remove the temporary module from the global namespace\n    del globals()[module_name]"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import os\n    import sys\n    import tempfile\n    import importlib\n    from detectron2.structures import Instances\n\n    # Create a new module for the new class\n    new_module = tempfile.NamedTemporaryFile(suffix=\".py\", delete=False)\n\n    # Write the new class to the module\n    new_module.write(\n        f\"class Instances(Instances):\\n\"\n        f\"    def __init__(self, *args, **kwargs):\\n\"\n        f\"        super().__init__(*args, **kwargs)\\n\"\n    )\n\n    # Add the fields to the class\n    for field in fields:\n        new_module.write(f\"        self.{field[0]} = {field[1]}\\n\")\n\n    # Close the module\n    new_module.close()\n\n    # Add the module to the sys.path\n    sys.path.append(os.path.dirname(new_module.name))\n\n    # Import the new module\n    importlib.invalidate_caches()\n    new_instances = importlib.import_module(os.path.basename(new_module.name).split(\".\")[0])\n\n    # Set up the environment so that torchscript can recognize and work with the new class\n    from torch.utils.checkpoint import checkpoint\n\n    def _checkpoint_instances(instances):\n        return checkpoint(instances, instances.fields())\n\n    setattr(new_instances.Instances, \"checkpoint\", _checkpoint_instances)\n\n    # Yield the new class\n    yield new_instances.Instances\n\n    # Clean up\n    os.remove(new_module.name)\n    sys.path.remove(os.path.dirname(new_module.name))\n    importlib.invalidate_caches()\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import torch\n    from detectron2.structures import Instances\n\n    import inspect\n    import os\n    import sys\n    import tempfile\n    import types\n    import importlib\n\n    # Create a new module for the class\n    new_module = types.ModuleType(\"new_module\")\n    sys.modules[\"new_module\"] = new_module\n\n    # Define the new class\n    class NewInstances(Instances):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n\n    # Add fields to the new class\n    for field_name, field_type in fields:\n        setattr(NewInstances, field_name, field_type)\n\n    # Write the new class to a temporary file\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n        f.write(\"from detectron2.structures import Instances\\n\\n\")\n        f.write(\"class NewInstances(Instances):\\n\")\n        for field_name, field_type in fields:\n            f.write(f\"    {field_name}: {field_type}\\n\")\n        f.write(\"\\n\")\n        f.write(\"    def __init__(self, *args, **kwargs):\\n\")\n        f.write(\"        super().__init__(*args, **kwargs)\\n\")\n\n        # Import the new class from the temporary file\n        spec = importlib.util.spec_from_file_location(\"new_module\", f.name)\n        new_module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(new_module)\n\n        # Replace 'Instances' with the new class\n        setattr(sys.modules[\"detectron2.structures\"], \"Instances\", new_module.NewInstances)\n\n        # Set up the environment for torchscript\n        torch._C._jit_set_class_schema(torch.jit.script(new_module.NewInstances), \"Instances\")\n\n        # Yield the new class\n        yield"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import os\n    import importlib\n    import tempfile\n    import inspect\n    import sys\n    import types\n    import detectron2.structures.instances as instances\n    import detectron2.utils.env as env\n\n    class Instances(nn.Module):\n        def __init__(self, *args, **kwargs):\n            super(Instances, self).__init__()\n            self.fields = fields\n            self.args = args\n            self.kwargs = kwargs\n\n        def forward(self, *args, **kwargs):\n            return self\n\n    def generate_module(fields):\n        \"\"\"\n        This function generates a new module for the class, writes it to a temporary file, imports it, and sets up the environment so that torchscript can recognize and work with this new class.\n\n        Input-Output Arguments\n        :param fields: List of tuples or similar structure. Defines the fields of the new 'Instances' class, where each element in the list represents a field in the class.\n        :return: None\n        \"\"\"\n\n        # Create a new module\n        module = types.ModuleType(\"instances\")\n\n        # Add a class to the module\n        module.Instances = type(\"Instances\", (nn.Module,), {})\n\n        # Add fields to the class\n        for field in fields:\n            setattr(module.Instances, field[0], field[1])\n\n        # Create a temporary file to store the module\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            # Write the module to the temporary file\n            f.write(inspect.getsource(module))\n            f.flush()\n\n            # Add the temporary file to the module search path\n            sys.path.append(os.path.dirname(f.name))\n\n            # Import the module\n            module = importlib.import_module(\"instances\")\n\n            # Remove the temporary file from the module search path\n            sys.path.remove(os.path.dirname(f.name"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import torch\n    import detectron2\n    import importlib\n    import inspect\n    import os\n    import tempfile\n    import sys\n\n    # Get the current directory of the detectron2 module\n    detectron2_dir = os.path.dirname(detectron2.__file__)\n\n    # Create a temporary file for the new class\n    with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as f:\n        # Write the new class to the temporary file\n        f.write(f'class Instances(\\n'.encode())\n        for field in fields:\n            f.write(f'    {field[0]}: {field[1]}\\n'.encode())\n        f.write(f'):\\n'.encode())\n        f.write(f'    pass\\n'.encode())\n\n        # Flush and close the temporary file\n        f.flush()\n\n        # Get the path of the temporary file\n        temp_file_path = f.name\n\n    # Import the new class from the temporary file\n    spec = importlib.util.spec_from_file_location('instances', temp_file_path)\n    instances_module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(instances_module)\n\n    # Get the new class from the imported module\n    instances_class = getattr(instances_module, 'Instances')\n\n    # Set up the environment to use the new class\n    detectron2.structures.instances.Instances = instances_class\n\n    # Yield the new class\n    yield instances_class\n\n    # Clean up the temporary file and environment\n    os.remove(temp_file_path)\n    detectron2.structures.instances.Instances = detectron2.structures.instances.Instances\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import torch\n    import detectron2.structures\n    import importlib\n    import tempfile\n    import os\n    import sys\n    import inspect\n    import types\n\n    # Generate the new class\n    class_fields = []\n    for field in fields:\n        if isinstance(field, tuple):\n            class_fields.append(field)\n        else:\n            class_fields.append((field, field))\n\n    class_fields.append((\"__annotations__\", {}))\n    for field in class_fields:\n        field[1][\"__annotations__\"][field[0]] = field[1]\n\n    class_fields.append((\"__module__\", \"detectron2.structures.instances\"))\n    class_fields.append((\"__qualname__\", \"Instances\"))\n    class_fields.append((\"__doc__\", \"A class to represent a set of instances with bounding boxes and segmentation masks.\"))\n\n    class_fields.append((\"__init__\", types.MethodType(lambda self, **kwargs: None, None)))\n\n    class_fields.append((\"_fields\", class_fields))\n\n    new_class = types.new_class(\"Instances\", (detectron2.structures.Instances,), class_fields)\n\n    # Create a temporary file for the new class\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n        f.write(\"class Instances:\\n\")\n        for field in class_fields:\n            if field[0] == \"__init__\":\n                f.write(f\"    def __init__(self, **kwargs):\\n\")\n                for arg in inspect.signature(field[1]).parameters:\n                    f.write(f\"        self.{arg} = kwargs.get('{arg}')\\n\")\n            elif field[0] == \"__annotations__\":\n                f.write(\"    __annotations__ = {\\n\")\n                for arg in field[1]:\n                    f.write(f\"        '{arg}': {field[1][arg]"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import importlib\n    import inspect\n    import os\n    import sys\n    import tempfile\n    from pathlib import Path\n    from typing import Any, Dict, List, Optional, Tuple, Union\n\n    import torch\n    from detectron2.structures import Boxes, Instances\n\n    # Generate a new module for the class\n    module = importlib.util.module_from_spec(importlib.machinery.ModuleSpec(\"tmp_module\", None))\n\n    # Define the new class\n    class NewInstances(Instances):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n\n        def __setattr__(self, key: str, value: Any) -> None:\n            if key in self.__annotations__:\n                self.__annotations__[key] = value\n            super().__setattr__(key, value)\n\n        def __getattr__(self, key: str) -> Any:\n            if key in self.__annotations__:\n                return self.__annotations__[key]\n            return super().__getattr__(key)\n\n        def __getitem__(self, key: Union[int, slice, torch.BoolTensor]) -> \"Instances\":\n            if isinstance(key, torch.BoolTensor):\n                return self.select(key)\n            return super().__getitem__(key)\n\n    # Add fields to the class\n    for field in fields:\n        name, type_ = field\n        setattr(NewInstances, name, type_)\n\n    # Write the new class to a temporary file\n    with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as f:\n        filename = f.name\n        f.write(\n            inspect.getsource(NewInstances).encode(\"utf-8\")\n        )  # type: ignore # https://github.com/python/typeshed/issues/3040\n\n    # Import the new class\n    spec = importlib.util.spec_from_file_location(\"tmp_module\", filename)\n    imported_module = importlib."}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Read the image using PIL\n    image = Image.open(file_name)\n\n    # Apply any necessary orientation corrections based on the image's EXIF data\n    if hasattr(image, 'getexif'):\n        exif = image.getexif()\n        if exif is not None and 274 in exif:\n            orientation = exif[274]\n            if orientation == 3:\n                image = image.rotate(180, expand=True)\n            elif orientation == 6:\n                image = image.rotate(270, expand=True)\n            elif orientation == 8:\n                image = image.rotate(90, expand=True)\n\n    # Convert the image to the specified format\n    if format is None:\n        image = np.array(image)\n    elif format == \"BGR\":\n        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n    elif format == \"YUV-BT.601\":\n        image = np.array(image.convert(\"YCbCr\"))\n        image = image[:, :, 0] * 0.299 + image[:, :, 1] * 0.587 + image[:, :, 2] * 0.114\n        image = image.astype(np.float32) / 255.0\n    else:\n        image = np.array(image.convert(format))\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Read the image from the file\n    image = Image.open(file_name)\n\n    # Apply any necessary orientation corrections based on the image's EXIF data\n    image = image.rotate(get_orientation_angle(image))\n\n    # Convert the image to the specified format\n    if format == \"BGR\":\n        image = image.convert(\"RGB\")\n        image = np.array(image)\n        image = image[..., ::-1]\n    elif format == \"YUV-BT.601\":\n        image = image.convert(\"YCbCr\")\n        image = np.array(image)\n        image = image.astype(np.float32)\n        image = image / 255.0\n        image[:, :, 0] = (image[:, :, 0] - 16.0) / 219.0\n        image[:, :, 1] = (image[:, :, 1] - 128.0) / 224.0\n        image[:, :, 2] = (image[:, :, 2] - 128.0) / 224.0\n    else:\n        image = np.array(image)\n\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    import numpy as np\n    from PIL import Image\n\n    img = Image.open(file_name)\n\n    if img.mode == \"RGBA\":\n        img = img.convert(\"RGB\")\n\n    if format is None:\n        format = img.mode\n\n    if format == \"BGR\":\n        img = img.convert(\"RGB\")\n        img = np.array(img)[:, :, ::-1]\n    elif format == \"YUV-BT.601\":\n        img = img.convert(\"YCbCr\")\n        img = np.array(img, dtype=np.float32)\n        img[:, :, 0] = img[:, :, 0] / 255.0\n        img[:, :, 1:] = img[:, :, 1:] - 128.0\n    else:\n        img = np.array(img)\n\n    return img\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Read the image\n    image = Image.open(file_name)\n\n    # Apply any necessary orientation corrections based on the image's EXIF data\n    if hasattr(image, '_getexif'):\n        exif = image._getexif()\n        if exif is not None:\n            exif = {\n                ExifTags.TAGS[k]: v\n                for k, v in exif.items()\n                if k in ExifTags.TAGS\n            }\n\n            orientation = exif.get('Orientation', None)\n            if orientation == 2:\n                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n            elif orientation == 3:\n                image = image.transpose(Image.ROTATE_180)\n            elif orientation == 4:\n                image = image.transpose(Image.FLIP_TOP_BOTTOM)\n            elif orientation == 5:\n                image = image.transpose(Image.ROTATE_90).transpose(Image.FLIP_TOP_BOTTOM)\n            elif orientation == 6:\n                image = image.transpose(Image.ROTATE_270)\n            elif orientation == 7:\n                image = image.transpose(Image.ROTATE_90).transpose(Image.FLIP_LEFT_RIGHT)\n            elif orientation == 8:\n                image = image.transpose(Image.ROTATE_90)\n\n    # Convert the image to the specified format\n    if format is None:\n        format = image.mode\n    elif format == 'BGR':\n        image = image.convert('RGB')\n        image = np.array(image)\n        image = image[..., ::-1]\n    elif format == 'YUV-BT.601':\n        image = image.convert('L')\n        image = np.array(image)\n        image = image / 255.0\n    else:\n        image = image.convert(format)\n        image = np.array(image)\n\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Import the necessary modules\n    import numpy as np\n    from PIL import Image\n\n    # Read the image using PIL\n    image = Image.open(file_name)\n\n    # If the image has an EXIF orientation tag, rotate it accordingly\n    if hasattr(image, 'getexif'):\n        exif = image.getexif()\n        if exif is not None and 274 in exif:\n            orientation = exif[274]\n            if orientation == 3:\n                image = image.rotate(180, expand=True)\n            elif orientation == 6:\n                image = image.rotate(270, expand=True)\n            elif orientation == 8:\n                image = image.rotate(90, expand=True)\n\n    # Convert the image to the specified format\n    if format is None:\n        image = np.array(image)\n    elif format == 'BGR':\n        image = np.array(image.convert('RGB'))\n        image = image[:, :, ::-1]\n    elif format == 'YUV-BT.601':\n        image = np.array(image.convert('YCbCr'))\n        image = image[:, :, 0] / 255.0\n    else:\n        image = np.array(image.convert(format))\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # read the image\n    image = Image.open(file_name)\n\n    # check if the image has EXIF data\n    if hasattr(image, '_getexif'):\n\n        # get the EXIF data\n        exif_data = image._getexif()\n\n        # check if the EXIF data contains orientation information\n        if exif_data and 274 in exif_data:\n\n            # get the orientation from the EXIF data\n            orientation = exif_data[274]\n\n            # perform the necessary orientation correction based on the orientation value\n            if orientation == 3:\n                image = image.transpose(Image.ROTATE_180)\n            elif orientation == 6:\n                image = image.transpose(Image.ROTATE_270)\n            elif orientation == 8:\n                image = image.transpose(Image.ROTATE_90)\n\n    # if the format is not specified, use the image's original format\n    if format is None:\n        format = image.format\n\n    # convert the image to the specified format\n    if format in [\"BGR\", \"YUV-BT.601\"]:\n        image = np.array(image)\n        if format == \"BGR\":\n            image = image[:, :, ::-1]\n        else:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n    else:\n        image = np.array(image.convert(format))\n\n    # return the image\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Read the image from the file\n    image = Image.open(file_name)\n\n    # Apply any necessary orientation corrections based on the image's EXIF data\n    exif_data = image.getexif()\n    if exif_data:\n        orientation = exif_data.get(0x0112, 1)\n        if orientation == 3:\n            image = image.transpose(Image.ROTATE_180)\n        elif orientation == 6:\n            image = image.transpose(Image.ROTATE_270)\n        elif orientation == 8:\n            image = image.transpose(Image.ROTATE_90)\n\n    # Convert the image to the specified format\n    if format:\n        if format == \"BGR\":\n            image = image.convert(\"RGB\")\n            image = np.array(image)\n            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        elif format == \"YUV-BT.601\":\n            image = image.convert(\"YCbCr\")\n            image = np.array(image)\n            image = image.astype(np.float32)\n            image = image[:, :, 0] / 255.0\n        else:\n            image = image.convert(format)\n            image = np.array(image)\n    else:\n        image = np.array(image)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    from PIL import Image\n    from PIL import ExifTags\n    import numpy as np\n\n    # Read the image using PIL\n    img = Image.open(file_name)\n\n    # Check if the image has an EXIF orientation tag\n    if hasattr(img, 'getexif'):\n        exif = img.getexif()\n        if exif is not None:\n            exif = {ExifTags.TAGS[k]: v for k, v in exif.items() if k in ExifTags.TAGS}\n            if 'Orientation' in exif:\n                orientation = exif['Orientation']\n                if orientation == 2:  # Flip left-to-right\n                    img = img.transpose(Image.FLIP_LEFT_RIGHT)\n                elif orientation == 3:  # Rotate 180 degrees\n                    img = img.rotate(180)\n                elif orientation == 4:  # Flip top-to-bottom\n                    img = img.transpose(Image.FLIP_TOP_BOTTOM)\n                elif orientation == 5:  # Rotate 90 degrees clockwise and flip left-to-right\n                    img = img.rotate(-90).transpose(Image.FLIP_LEFT_RIGHT)\n                elif orientation == 6:  # Rotate 90 degrees clockwise\n                    img = img.rotate(-90)\n                elif orientation == 7:  # Rotate 90 degrees counter-clockwise and flip left-to-right\n                    img = img.rotate(90).transpose(Image.FLIP_LEFT_RIGHT)\n                elif orientation == 8:  # Rotate 90 degrees counter-clockwise\n                    img = img.rotate(90)\n\n    # Convert the image to a numpy array\n    img_arr = np.array(img)\n\n    # Convert the image to the specified format\n    if format is not None:\n        if format == 'BGR':\n            img_arr = img_arr[:, :, ::-1]\n        elif format == 'YUV-BT.601':"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    import numpy as np\n    from PIL import Image\n\n    # Read the image file\n    image = Image.open(file_name)\n\n    # Apply any necessary orientation corrections based on EXIF data\n    exif_data = image.getexif()\n    if exif_data:\n        orientation = exif_data.get(274, 1)\n        if orientation == 2:\n            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n        elif orientation == 3:\n            image = image.transpose(Image.ROTATE_180)\n        elif orientation == 4:\n            image = image.transpose(Image.FLIP_TOP_BOTTOM)\n        elif orientation == 5:\n            image = image.transpose(Image.ROTATE_270)\n            image = image.transpose(Image.FLIP_TOP_BOTTOM)\n        elif orientation == 6:\n            image = image.transpose(Image.ROTATE_270)\n        elif orientation == 7:\n            image = image.transpose(Image.ROTATE_90)\n            image = image.transpose(Image.FLIP_TOP_BOTTOM)\n        elif orientation == 8:\n            image = image.transpose(Image.ROTATE_90)\n\n    # Convert the image to the specified format\n    if format is None:\n        format = image.mode\n    if format == \"BGR\":\n        image = image.convert(\"RGB\")\n        image = np.array(image)\n        image = image[:, :, ::-1]\n        image = image.astype(np.uint8)\n    elif format == \"YUV-BT.601\":\n        image = image.convert(\"YCbCr\")\n        image = np.array(image)\n        image = image[:, :, 0]\n        image = image.astype(np.float32) / 255.0\n    else:\n        image = np.array(image)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    if format is None:\n        format = get_image_format(file_name)\n\n    image = read_image_exif(file_name)\n\n    if format == \"BGR\":\n        image = image[..., ::-1]\n    elif format == \"YUV-BT.601\":\n        image = rgb2yuv(image)\n    else:\n        image = image.astype(np.uint8)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    from PIL import Image\n    import numpy as np\n\n    # Read the image file\n    image = Image.open(file_name)\n\n    # Apply any orientation corrections based on the image's EXIF data\n    if image._getexif():\n        exif = image._getexif()\n        if exif:\n            orientation = exif.get(0x0112)\n            if orientation == 2:\n                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n            elif orientation == 3:\n                image = image.transpose(Image.ROTATE_180)\n            elif orientation == 4:\n                image = image.transpose(Image.FLIP_TOP_BOTTOM)\n            elif orientation == 5:\n                image = image.transpose(Image.ROTATE_90).transpose(Image.FLIP_LEFT_RIGHT)\n            elif orientation == 6:\n                image = image.transpose(Image.ROTATE_270)\n            elif orientation == 7:\n                image = image.transpose(Image.ROTATE_90).transpose(Image.FLIP_TOP_BOTTOM)\n            elif orientation == 8:\n                image = image.transpose(Image.ROTATE_90)\n\n    # Convert the image to the specified format\n    if format == \"BGR\":\n        image = image.convert(\"RGB\")\n        image = np.array(image)\n        image = image[:, :, ::-1]\n    elif format == \"YUV-BT.601\":\n        image = image.convert(\"YCbCr\")\n        image = np.array(image)\n        image = image.astype(np.float32)\n        image[:, :, 0] = (image[:, :, 0] - 16) / 219\n        image[:, :, 1:] = (image[:, :, 1:] - 128) / 224\n    else:\n        image = np.array(image)\n\n    return image\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Read the image using PIL\n    image = Image.open(file_name)\n\n    # Check if the image has EXIF data and contains orientation information\n    if hasattr(image, 'getexif'):\n        exif_data = image.getexif()\n        if exif_data:\n            orientation = exif_data.get(0x0112, 1)\n            if orientation != 1:\n                image = image.rotate(-90 * (orientation - 1), expand=True)\n\n    # Convert the image to the specified format\n    if format is not None:\n        if format == \"BGR\":\n            image = image.convert(\"RGB\")\n            image = np.array(image)\n            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        elif format == \"YUV-BT.601\":\n            image = image.convert(\"YCbCr\")\n            image = np.array(image)\n            image = cv2.cvtColor(image, cv2.COLOR_YCrCb2RGB)\n            image = image / 255.0\n        else:\n            image = image.convert(format)\n            image = np.array(image)\n\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Read the image using PIL\n    img = Image.open(file_name)\n\n    # If the image has EXIF data, apply any necessary orientation corrections\n    if hasattr(img, 'getexif'):\n        exif = img.getexif()\n        if exif is not None:\n            exif = dict(exif.items())\n            if exif.get(0x0112, None) == 3:\n                img = img.rotate(180, expand=True)\n            elif exif.get(0x0112, None) == 6:\n                img = img.rotate(270, expand=True)\n            elif exif.get(0x0112, None) == 8:\n                img = img.rotate(90, expand=True)\n\n    # Convert the image to the specified format\n    if format is not None:\n        if format == 'BGR':\n            img = img.convert('RGB')\n            img = np.array(img)\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        elif format == 'YUV-BT.601':\n            img = img.convert('YCbCr')\n            img = np.array(img)\n            img = cv2.cvtColor(img, cv2.COLOR_YCrCb2YUV)\n        else:\n            img = img.convert(format)\n            img = np.array(img)\n\n    return img\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    if format is None:\n        return read_image_no_format(file_name)\n    else:\n        return read_image_format(file_name, format)\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    from PIL import Image\n    import numpy as np\n\n    # Read the image file using PIL\n    image = Image.open(file_name)\n\n    # Check if the image has EXIF data\n    if image._getexif():\n        # Orient the image based on the EXIF data\n        image = image.rotate(0, expand=True)\n\n    # Convert the image to a numpy array\n    image_array = np.array(image)\n\n    # Convert the image to the specified format\n    if format == \"BGR\":\n        image_array = image_array[:, :, ::-1]\n    elif format == \"YUV-BT.601\":\n        image_array = image_array.astype(np.float32)\n        image_array = image_array / 255.0\n        image_array = np.dot(image_array, [[0.299, 0.587, 0.114], [-0.1687, -0.3313, 0.5], [0.5, -0.4187, -0.0813]])\n    elif format is not None:\n        image_array = image_array.astype(np.uint8)\n\n    return image_array\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    import cv2\n    import numpy as np\n    from PIL import Image\n    from PIL import ImageFile\n    from PIL.ExifTags import TAGS\n\n    # Read image\n    pil_image = Image.open(file_name)\n\n    # Orient image correctly\n    try:\n        for orientation in TAGS.keys():\n            if TAGS[orientation] == 'Orientation':\n                break\n        exif = pil_image._getexif()\n        if exif[orientation] == 3:\n            pil_image = pil_image.rotate(180, expand=True)\n        elif exif[orientation] == 6:\n            pil_image = pil_image.rotate(270, expand=True)\n        elif exif[orientation] == 8:\n            pil_image = pil_image.rotate(90, expand=True)\n    except (AttributeError, KeyError, IndexError):\n        # cases: image don't have getexif\n        pass\n\n    # Convert to OpenCV format\n    cv_image = np.array(pil_image)\n\n    # Convert to desired format\n    if format == \"BGR\":\n        cv_image = cv2.cvtColor(cv_image, cv2.COLOR_RGB2BGR)\n    elif format == \"YUV-BT.601\":\n        cv_image = cv2.cvtColor(cv_image, cv2.COLOR_RGB2YUV)\n\n    return cv_image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Read the image using PIL\n    with Image.open(file_name) as img:\n\n        # Get the image's EXIF data\n        exif_data = img.getexif()\n\n        # Get the orientation tag from the EXIF data\n        orientation = exif_data.get(0x0112, 1)\n\n        # Perform the necessary orientation correction\n        if orientation == 2:\n            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n        elif orientation == 3:\n            img = img.rotate(180)\n        elif orientation == 4:\n            img = img.rotate(180).transpose(Image.FLIP_LEFT_RIGHT)\n        elif orientation == 5:\n            img = img.rotate(-90, expand=True).transpose(Image.FLIP_LEFT_RIGHT)\n        elif orientation == 6:\n            img = img.rotate(-90, expand=True)\n        elif orientation == 7:\n            img = img.rotate(90, expand=True).transpose(Image.FLIP_LEFT_RIGHT)\n        elif orientation == 8:\n            img = img.rotate(90, expand=True)\n\n        # Convert the image to the specified format\n        if format is None:\n            format = img.mode\n        img = img.convert(format)\n\n        # Convert the image to a numpy array\n        img_arr = np.array(img)\n\n    return img_arr\n\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Import the necessary libraries\n    import numpy as np\n    from PIL import Image\n\n    # Read the image using PIL\n    image = Image.open(file_name)\n\n    # Apply any necessary orientation corrections based on the image's EXIF data\n    if image._getexif():\n        exif_data = image._getexif()\n        if exif_data:\n            orientation = exif_data.get(274, 1)\n            if orientation == 2:\n                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n            elif orientation == 3:\n                image = image.rotate(180, expand=True)\n            elif orientation == 4:\n                image = image.rotate(180, expand=True).transpose(Image.FLIP_LEFT_RIGHT)\n            elif orientation == 5:\n                image = image.rotate(-90, expand=True).transpose(Image.FLIP_LEFT_RIGHT)\n            elif orientation == 6:\n                image = image.rotate(-90, expand=True)\n            elif orientation == 7:\n                image = image.rotate(90, expand=True).transpose(Image.FLIP_LEFT_RIGHT)\n            elif orientation == 8:\n                image = image.rotate(90, expand=True)\n\n    # Convert the image to the specified format\n    if format is None:\n        # Use the image's original format\n        image_data = np.array(image)\n    elif format == \"BGR\":\n        # Convert the image to BGR format\n        image_data = np.array(image.convert(\"RGB\"))\n        image_data = image_data[:, :, ::-1]\n    elif format == \"YUV-BT.601\":\n        # Convert the image to YUV-BT.601 format\n        image_data = np.array(image.convert(\"YCbCr\"))\n        image_data = image_data[:, :, 0]\n    else:\n        # Convert the image to the specified format\n        image_data = np."}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Read the image using PIL\n    image = Image.open(file_name)\n\n    # Apply any necessary orientation correction based on the image's EXIF data\n    if hasattr(image, 'getexif'):\n        exif = image.getexif()\n        if exif is not None and 274 in exif:\n            orientation = exif[274]\n            if orientation == 2:\n                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n            elif orientation == 3:\n                image = image.transpose(Image.ROTATE_180)\n            elif orientation == 4:\n                image = image.transpose(Image.FLIP_TOP_BOTTOM)\n            elif orientation == 5:\n                image = image.transpose(Image.ROTATE_90).transpose(Image.FLIP_TOP_BOTTOM)\n            elif orientation == 6:\n                image = image.transpose(Image.ROTATE_270)\n            elif orientation == 7:\n                image = image.transpose(Image.ROTATE_90).transpose(Image.FLIP_LEFT_RIGHT)\n            elif orientation == 8:\n                image = image.transpose(Image.ROTATE_90)\n\n    # Convert the image to the specified format\n    if format is not None:\n        if format == 'BGR':\n            image = image.convert('RGB')\n            image = np.array(image)\n            image = image[:, :, ::-1]\n        elif format == 'YUV-BT.601':\n            image = image.convert('YCbCr')\n            image = np.array(image)\n            image = image[:, :, 0]\n        else:\n            image = image.convert(format)\n            image = np.array(image)\n\n    # Return the image as a numpy array\n    return image\n\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    import numpy as np\n    from PIL import Image\n    from PIL.ExifTags import TAGS\n    from PIL import ImageOps\n\n    # Open the image file\n    image = Image.open(file_name)\n\n    # Get the EXIF data from the image\n    exif_data = image.getexif()\n\n    # Check if the image has EXIF data\n    if exif_data:\n        # Iterate over the EXIF data and print the tag name and value\n        for tag_id, value in exif_data.items():\n            # Get the tag name from the tag ID\n            tag_name = TAGS.get(tag_id, tag_id)\n            # Print the tag name and value\n            # print(f\"{tag_name}: {value}\")\n\n            # Check if the tag name is \"Orientation\"\n            if tag_name == \"Orientation\":\n                # Check if the value is 6 (rotated 90 degrees clockwise)\n                if value == 6:\n                    # Rotate the image 90 degrees clockwise\n                    image = image.rotate(-90, expand=True)\n                # Check if the value is 8 (rotated 90 degrees counter-clockwise)\n                elif value == 8:\n                    # Rotate the image 90 degrees counter-clockwise\n                    image = image.rotate(90, expand=True)\n                # Check if the value is 3 (rotated 180 degrees)\n                elif value == 3:\n                    # Rotate the image 180 degrees\n                    image = image.rotate(180, expand=True)\n                # Check if the value is 2 (mirrored)\n                elif value == 2:\n                    # Flip the image horizontally\n                    image = ImageOps.mirror(image)\n\n    # Convert the image to the specified format\n    if format is not None:\n        if format == \"BGR\":\n            # Convert the image to BGR format\n            image = image.convert(\"RGB\")\n            image = np.array(image)\n            image = image[:, :"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (list, tuple)):\n        transforms = TransformList(transforms)\n\n    # bbox\n    target_bbox = transforms.apply_box(annotation[\"bbox\"])\n    annotation[\"bbox\"] = target_bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation\n    if \"segmentation\" in annotation:\n        target_segmentation = transforms.apply_segmentation(annotation[\"segmentation\"])\n        annotation[\"segmentation\"] = target_segmentation\n\n    # keypoints\n    if \"keypoints\" in annotation:\n        target_keypoints = transforms.apply_keypoint(annotation[\"keypoints\"])\n        if keypoint_hflip_indices is not None:\n            target_keypoints[keypoint_hflip_indices] = target_keypoints[keypoint_hflip_indices][:, ::-1]\n        annotation[\"keypoints\"] = target_keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox_mode = annotation[\"bbox_mode\"]\n    if bbox_mode == \"XYXY_ABS\":\n        bbox = annotation[\"bbox\"]\n        bbox = transforms.apply_box([bbox])[0]\n        bbox = clip_bbox_to_image_size(bbox, image_size)\n        annotation[\"bbox\"] = bbox\n        annotation[\"bbox_mode\"] = \"XYXY_ABS\"\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            for i in range(len(annotation[\"segmentation\"])):\n                annotation[\"segmentation\"][i] = transforms.apply_coords(\n                    np.array(annotation[\"segmentation\"][i])\n                ).tolist()\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE\n            annotation[\"segmentation\"] = transforms.apply_coords(\n                np.array(annotation[\"segmentation\"])\n            ).tolist()\n\n    if \"keypoints\" in annotation:\n        if keypoint_hflip_indices is not None:\n            keypoints = annotation[\"keypoints\"]\n            keypoints[keypoint_hflip_indices] = keypoints[keypoint_hflip_indices][\n                :, ::-1\n            ]\n            keypoints[:, 0] = image_size[1] - keypoints[:, 0]\n            annotation[\"keypoints\"] = keypoints\n\n        annotation[\"keypoints\"] = transforms.apply_coords(\n            np.array(annotation[\"keypoints\"])\n        ).tolist()\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bounding box\n    if \"bbox\" in annotation:\n        xmin, ymin, xmax, ymax = annotation[\"bbox\"]\n        xmin, ymin, xmax, ymax = transforms.apply_box([xmin, ymin, xmax, ymax])\n        annotation[\"bbox\"] = [xmin, ymin, xmax, ymax]\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            transformed_segmentation = []\n            for seg in annotation[\"segmentation\"]:\n                seg = np.array(seg).reshape(-1, 2)\n                seg = transforms.apply_coords(seg).reshape(-1).tolist()\n                transformed_segmentation.append(seg)\n            annotation[\"segmentation\"] = transformed_segmentation\n        else:\n            # RLE\n            rle = transforms.apply_coords(\n                utils.convert_polygon_to_rle(annotation[\"segmentation\"])\n            )\n            annotation[\"segmentation\"] = utils.convert_rle_to_mask(rle, image_size)\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = np.array(annotation[\"keypoints\"]).reshape(-1, 3)\n        keypoints = transforms.apply_coords(keypoints).reshape(-1).tolist()\n        annotation[\"keypoints\"] = keypoints\n\n        if keypoint_hflip_indices is not None:\n            annotation[\"keypoints\"][keypoint_hflip_indices] = -1\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox_mode = annotation[\"bbox_mode\"]\n    if bbox_mode == \"XYXY_ABS\":\n        bbox = annotation[\"bbox\"]\n        bbox = transforms.apply_box(bbox)\n        bbox = clip_box(bbox, image_size)\n        annotation[\"bbox\"] = bbox\n    elif bbox_mode == \"XYWH_ABS\":\n        bbox = box_xywh_to_xyxy(annotation[\"bbox\"])\n        bbox = transforms.apply_box(bbox)\n        bbox = clip_box(bbox, image_size)\n        annotation[\"bbox\"] = box_xyxy_to_xywh(bbox)\n    elif bbox_mode == \"XYWH_REL\":\n        bbox = box_xywh_to_xyxy(annotation[\"bbox\"])\n        bbox = transforms.apply_box(bbox)\n        bbox = clip_box(bbox, image_size)\n        annotation[\"bbox\"] = box_xyxy_to_xywh(bbox)\n    else:\n        raise ValueError(\n            \"Only XYXY_ABS, XYWH_ABS and XYWH_REL bbox_mode are supported\"\n        )\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            for i, seg in enumerate(annotation[\"segmentation\"]):\n                annotation[\"segmentation\"][i] = transforms.apply_coords(\n                    np.array(seg).reshape(-1, 2)\n                ).reshape(-1).tolist()\n        else:\n            # RLE\n            annotation[\"segmentation\"] = transforms.apply_coords(\n                np.array(annotation[\"segmentation\"]).reshape(-1, 2)\n            ).reshape(-1).tolist()\n\n    if \"keypoints\" in annotation:\n        annotation[\"keypoints\"] = transforms.apply_coords(\n            np.array(annotation[\"keypoints\"])."}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bounding box\n    if transforms.transforms[0].is_box_transform:\n        if transforms.transforms[0].is_relative_box:\n            annotation[\"bbox\"] = transforms.apply_relative_box(annotation[\"bbox\"])\n        else:\n            annotation[\"bbox\"] = transforms.apply_box(annotation[\"bbox\"])\n        annotation[\"bbox_mode\"] = transforms.transforms[0].box_mode\n        annotation[\"bbox\"] = box_clip_all(annotation[\"bbox\"], *image_size)\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            for i, seg in enumerate(annotation[\"segmentation\"]):\n                annotation[\"segmentation\"][i] = transforms.apply_segmentation(seg)\n        else:\n            # rle\n            annotation[\"segmentation\"] = transforms.apply_segmentation(\n                annotation[\"segmentation\"]\n            )\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        annotation[\"keypoints\"] = transforms.apply_coords(annotation[\"keypoints\"])\n        if keypoint_hflip_indices is not None:\n            annotation[\"keypoints\"][keypoint_hflip_indices] *= -1\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox_mode = annotation[\"bbox_mode\"]\n    if bbox_mode == \"XYWH_ABS\":\n        bbox = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]\n        bbox_mode = \"XYXY_ABS\"\n\n    if bbox_mode == \"XYXY_ABS\":\n        bbox = transforms.apply_box(bbox)\n        bbox = [\n            max(0, bbox[0]),\n            max(0, bbox[1]),\n            min(image_size[0], bbox[2]),\n            min(image_size[1], bbox[3]),\n        ]\n\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = \"XYXY_ABS\"\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            for i, seg in enumerate(annotation[\"segmentation\"]):\n                annotation[\"segmentation\"][i] = transforms.apply_coords(\n                    np.array(seg).reshape(-1, 2)\n                ).reshape(-1).tolist()\n        else:\n            # rle\n            annotation[\"segmentation\"] = transforms.apply_coords(\n                np.array(annotation[\"segmentation\"][\"counts\"]).reshape(-1, 2)\n            ).reshape(-1).tolist()\n\n    if \"keypoints\" in annotation:\n        annotation[\"keypoints\"] = transforms.apply_coords(\n            np.array(annotation[\"keypoints\"]).reshape(-1, 2)\n        ).reshape(-1).tolist()\n        if keypoint_hflip_indices is not None:\n            annotation[\"keypoints\"][keypoint_hflip_indices] = -1\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if not isinstance(transforms, (list, tuple)):\n        transforms = [transforms]\n    if \"bbox\" in annotation:\n        if \"bbox_mode\" in annotation:\n            bbox_mode = annotation[\"bbox_mode\"]\n        else:\n            bbox_mode = \"XYXY_ABS\"\n        if bbox_mode == \"XYXY_ABS\":\n            annotation[\"bbox\"] = transforms[0].apply_box(annotation[\"bbox\"])\n        elif bbox_mode == \"XYWH_ABS\":\n            annotation[\"bbox\"] = transforms[0].apply_box(\n                box_xywh_to_xyxy(annotation[\"bbox\"])\n            )\n            annotation[\"bbox\"] = box_xyxy_to_xywh(annotation[\"bbox\"])\n        elif bbox_mode == \"XYWH_REL\":\n            h, w = image_size\n            annotation[\"bbox\"][0] *= w\n            annotation[\"bbox\"][2] *= w\n            annotation[\"bbox\"][1] *= h\n            annotation[\"bbox\"][3] *= h\n            annotation[\"bbox\"] = transforms[0].apply_box(annotation[\"bbox\"])\n            annotation[\"bbox\"][0] /= w\n            annotation[\"bbox\"][2] /= w\n            annotation[\"bbox\"][1] /= h\n            annotation[\"bbox\"][3] /= h\n        else:\n            raise ValueError(f\"Unsupported bounding box mode {bbox_mode}\")\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            transformed_segmentation = []\n            for seg in annotation[\"segmentation\"]:\n                transformed_segmentation.append(transforms[0].apply_coords(seg).tolist())\n            annotation[\"segmentation\"] = transformed_segmentation\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE\n            if \"counts\" in annotation[\"segmentation\"]:\n                counts = annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = transforms.apply_to_bbox(annotation[\"bbox\"])\n    bbox = clip_bbox_to_image_size(bbox, image_size)\n    annotation[\"bbox\"] = bbox\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            transformed_segmentations = []\n            for segmentation in annotation[\"segmentation\"]:\n                transformed_segmentation = transforms.apply_to_polygon(segmentation)\n                transformed_segmentation = clip_segmentation_to_image_size(\n                    transformed_segmentation, image_size\n                )\n                transformed_segmentations.append(transformed_segmentation)\n            annotation[\"segmentation\"] = transformed_segmentations\n        else:\n            # RLE\n            transformed_segmentation = transforms.apply_to_rle(\n                annotation[\"segmentation\"]\n            )\n            transformed_segmentation = clip_segmentation_to_image_size(\n                transformed_segmentation, image_size\n            )\n            annotation[\"segmentation\"] = transformed_segmentation\n\n    if \"keypoints\" in annotation:\n        keypoints = transforms.apply_to_keypoints(annotation[\"keypoints\"])\n        keypoints = clip_and_convert_keypoints_to_xywh(keypoints, image_size)\n        if keypoint_hflip_indices is not None:\n            keypoints = flip_keypoints_horizontal(\n                keypoints, keypoint_hflip_indices\n            )\n        annotation[\"keypoints\"] = keypoints\n\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (list, tuple)):\n        transforms = TransformList(transforms)\n\n    if \"bbox\" in annotation:\n        bbox = annotation[\"bbox\"]\n        bbox = transforms.apply_box(bbox)\n        annotation[\"bbox\"] = bbox\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            transformed_segmentation = []\n            for segmentation in annotation[\"segmentation\"]:\n                if len(segmentation) == 0:\n                    continue\n                transformed_segmentation.append(\n                    transforms.apply_coords(np.array(segmentation).reshape(-1, 2))\n                    .reshape(-1)\n                    .tolist()\n                )\n            annotation[\"segmentation\"] = transformed_segmentation\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE\n            rle = annotation[\"segmentation\"]\n            if \"counts\" in rle:\n                counts = rle[\"counts\"]\n                counts = transforms.apply_coords(\n                    np.array(counts).reshape(-1, 2)\n                ).reshape(-1).tolist()\n                rle[\"counts\"] = counts\n            if \"size\" in rle:\n                size = rle[\"size\"]\n                size = list(transforms.apply_box(np.array(size).reshape(1, 2))[0])\n                rle[\"size\"] = size\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        if len(keypoints) == 0:\n            return annotation\n        keypoints = transforms.apply_coords(\n            np.array(keypoints).reshape(-1, 3)\n        ).reshape(-1)\n        if keypoint_hflip_indices is not None:\n            keypoints[keypoint_h"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # transform bounding box\n    if transforms:\n        if \"bbox\" in annotation:\n            annotation[\"bbox\"] = transforms.apply_box([annotation[\"bbox\"]])[0]\n            annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # transform segmentation\n    if transforms:\n        if \"segmentation\" in annotation:\n            if isinstance(annotation[\"segmentation\"], list):\n                # polygon\n                transformed_segmentations = []\n                for segmentation in annotation[\"segmentation\"]:\n                    if len(segmentation) == 0:\n                        continue\n                    assert len(segmentation) % 2 == 0\n                    xs = segmentation[0::2]\n                    ys = segmentation[1::2]\n                    xs, ys = transforms.apply_coords(\n                        np.stack([xs, ys], axis=0).T\n                    ).T\n                    segmentation = xs + ys\n                    transformed_segmentations.append(segmentation)\n                annotation[\"segmentation\"] = transformed_segmentations\n            elif isinstance(annotation[\"segmentation\"], dict):\n                # RLE\n                rle = transforms.apply_coords(\n                    np.array(annotation[\"segmentation\"][\"counts\"], dtype=np.uint8)\n                ).tolist()\n                annotation[\"segmentation\"][\"counts\"] = rle\n\n    # transform keypoints\n    if transforms:\n        if \"keypoints\" in annotation:\n            keypoints = annotation[\"keypoints\"]\n            if len(keypoints) == 0:\n                return annotation\n            assert len(keypoints) % 3 == 0\n            xs = keypoints[0::3]\n            ys = keypoints[1::3]\n            visibility = keypoints[2::3]\n            xs, ys = transforms.apply_coords(np.stack([xs, ys], axis=0).T).T\n            keypoints = xs + ys + visibility\n            annotation[\"keypoints\"] ="}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = transforms.apply_box(annotation[\"bbox\"])\n    bbox = clip_bbox_if_necessary(bbox, image_size)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        # FIXME: this logic is not robust to wrong annotation format\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            transformed_segmentations = []\n            for seg in annotation[\"segmentation\"]:\n                transformed_segmentations.append(\n                    transforms.apply_coords(np.array(seg).reshape(-1, 2)).reshape(\n                        -1\n                    ).tolist()\n                )\n            annotation[\"segmentation\"] = transformed_segmentations\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE\n            transformed_segmentation = transforms.apply_coords(\n                np.array(annotation[\"segmentation\"][\"counts\"]).reshape(-1, 2)\n            ).reshape(-1).tolist()\n            annotation[\"segmentation\"] = {\n                \"counts\": transformed_segmentation,\n                \"size\": annotation[\"segmentation\"][\"size\"],\n            }\n        else:\n            raise ValueError(\n                \"Only support segmentation of polygon or RLE format, \"\n                f\"got {annotation['segmentation']}.\"\n            )\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        if len(keypoints) > 0 and keypoints[0] != -1:\n            keypoints = transforms.apply_coords(\n                keypoints.reshape(-1, 3)\n            ).reshape(-1)\n            if keypoint_hflip_indices is not None:\n                # Flip keypoints horizontally\n                keypoints[keypoint_hflip_indices] = keypoints[\n                    keypoint"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Convert the annotations to COCO format\n    annotation = convert_coco_instance_annotation(annotation)\n\n    # Apply the transformations\n    annotation = apply_transforms(annotation, transforms, image_size)\n\n    # Convert the annotations back to the original format\n    annotation = convert_coco_instance_annotation(annotation, True)\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (list, tuple)):\n        if isinstance(transforms[0], (tuple, list)):\n            # for backward compatibility\n            transforms = TransformList(transforms)\n        else:\n            transforms = TransformList([transforms])\n    for transform in transforms:\n        if transform.is_no_op:\n            continue\n        if transform.has_polygon_modifier:\n            if \"segmentation\" in annotation:\n                annotation[\"segmentation\"] = transform.apply_polygons(\n                    annotation[\"segmentation\"]\n                )\n        if transform.has_keypoints:\n            if \"keypoints\" in annotation:\n                if keypoint_hflip_indices is not None:\n                    keypoints = annotation[\"keypoints\"]\n                    keypoints[::3] = image_size[1] - keypoints[keypoint_hflip_indices]\n                    annotation[\"keypoints\"] = keypoints\n                annotation[\"keypoints\"] = transform.apply_coords(\n                    np.asarray(annotation[\"keypoints\"])\n                )\n        if transform.has_bbox_modifier:\n            if \"bbox\" in annotation:\n                annotation[\"bbox\"] = transform.apply_box(\n                    torch.as_tensor(annotation[\"bbox\"])\n                )\n                annotation[\"bbox\"] = transform.post_apply_box(\n                    torch.tensor(annotation[\"bbox\"])\n                ).tolist()\n            if \"segmentation\" in annotation:\n                if isinstance(annotation[\"segmentation\"], list):\n                    # polygon\n                    annotation[\"segmentation\"] = transform.apply_polygons(\n                        annotation[\"segmentation\"]\n                    )\n                else:\n                    # mask\n                    annotation[\"segmentation\"] = transform.apply_coords(\n                        annotation[\"segmentation\"]\n                    )\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox_mode = BoxMode.XYXY_ABS\n    if isinstance(transforms, (list, tuple)):\n        if isinstance(transforms[0], (list, tuple)):\n            # For backward compatibility\n            transforms = TransformList(transforms)\n        else:\n            transforms = TransformList([transforms])\n    image_shape = np.array(image_size)\n\n    # Transform bounding box\n    if annotation[\"bbox\"] is not None:\n        # TODO: Support more sophisticated bbox modes (e.g., relative or aspect_ratio)\n        if bbox_mode == BoxMode.XYXY_ABS:\n            transforms.apply_box(annotation[\"bbox\"])\n            annotation[\"bbox\"] = transforms.apply_box(annotation[\"bbox\"])\n            annotation[\"bbox\"] = clip_box(annotation[\"bbox\"], image_shape)\n        else:\n            raise NotImplementedError(\n                \"Other modes than XYXY_ABS are not supported now.\"\n            )\n\n    # Transform segmentation\n    if annotation[\"segmentation\"] is not None:\n        # Make segmentation pixel-wise format consistent with bbox mode\n        if bbox_mode == BoxMode.XYXY_ABS:\n            if isinstance(annotation[\"segmentation\"], list):\n                # polygon\n                transformed_segmentations = []\n                for seg in annotation[\"segmentation\"]:\n                    seg = np.array(seg).reshape((-1, 2))\n                    transformed_segmentations.append(\n                        transforms.apply_coords(seg).reshape((-1,))\n                    )\n                annotation[\"segmentation\"] = transformed_segmentations\n            elif isinstance(annotation[\"segmentation\"], dict):\n                # RLE\n                for seg in annotation[\"segmentation\"]:\n                    annotation[\"segmentation\"][seg] = transforms.apply_coords(\n                        np.array(annotation[\"segmentation\"][seg]).reshape(\n                            (-1, 2)\n                        )\n                    ).reshape((-1,))\n            else:\n                raise Not"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox_mode = annotation[\"bbox_mode\"]\n    if bbox_mode == BoxMode.XYXY_ABS:\n        bbox = transforms(torch.as_tensor(annotation[\"bbox\"]).reshape(1, 4))[0]\n        annotation[\"bbox\"] = bbox.tolist()\n    else:\n        raise ValueError(f\"{bbox_mode} not supported\")\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            transformed_segmentations = []\n            for seg in annotation[\"segmentation\"]:\n                # filter out invalid polygons\n                if len(seg) % 2 != 0 or len(seg) < 6:\n                    continue\n                points = transforms(torch.as_tensor(seg).reshape(-1, 2))\n                points = points.reshape(-1).tolist()\n                transformed_segmentations.append(points)\n            annotation[\"segmentation\"] = transformed_segmentations\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # rle\n            if \"counts\" in annotation[\"segmentation\"]:\n                counts = annotation[\"segmentation\"][\"counts\"]\n                counts = transform_segmentations_to_box_mode_xyxy_abs(\n                    counts, image_size, transforms\n                )\n                annotation[\"segmentation\"][\"counts\"] = counts\n        else:\n            raise ValueError(\n                f\"{annotation['segmentation']} not supported\"\n            )\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        if keypoint_hflip_indices is not None:\n            keypoints[keypoint_hflip_indices] = keypoints[\n                keypoint_hflip_indices\n            ] * -1\n        keypoints = transforms(torch.as_tensor(keypoints).reshape(-1, 2))\n        annotation[\"keypoints\"] = keypoint"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if not isinstance(transforms, (list, tuple)):\n        transforms = [transforms]\n\n    # Transform bounding boxes\n    if \"bbox\" in annotation:\n        # Convert bounding box to XYXY_ABS mode\n        annotation[\"bbox\"] = convert_bbox_mode(annotation[\"bbox\"], \"XYXY_ABS\", \"XYXY_ABS\")\n        annotation[\"bbox\"] = transforms_bbox(\n            annotation[\"bbox\"], transforms, image_size\n        )\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # Polygon\n            for i, seg in enumerate(annotation[\"segmentation\"]):\n                annotation[\"segmentation\"][i] = transform_polygon(\n                    np.array(seg).reshape(-1, 2), transforms, image_size\n                )\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE\n            annotation[\"segmentation\"] = transform_polygon_rle(\n                annotation[\"segmentation\"], transforms, image_size\n            )\n        else:\n            raise ValueError(\n                \"Invalid segmentation type: {}\".format(type(annotation[\"segmentation\"]))\n            )\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        # Convert keypoints to XY format\n        annotation[\"keypoints\"] = convert_keypoints_format(\n            annotation[\"keypoints\"], \"XY\", \"XY\"\n        )\n        annotation[\"keypoints\"] = transform_keypoints(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n\n    # Set the bounding box mode to XYXY_ABS\n    annotation[\"bbox_mode\"] = \"XYXY_ABS\"\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if \"bbox\" in annotation:\n        # FIXME: temporarily set box_mode=XYXY_ABS, as we need bbox in LTRB for\n        #        visualization, but should revert our API and fix all the\n        #        visualization code in callbacks/visualizer.py\n        #        related issue: https://github.com/facebookresearch/detectron2/issues/652\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n        annotation[\"bbox\"] = transforms.apply_box(\n            [annotation[\"bbox\"]],\n            box_mode=annotation[\"bbox_mode\"],\n            image_size=image_size,\n        )[0]\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            transformed_segm = []\n            for seg in annotation[\"segmentation\"]:\n                transformed_segm.append(\n                    transforms.apply_coords(np.array(seg).reshape(-1, 2)).reshape(\n                        -1\n                    ).tolist()\n                )\n            annotation[\"segmentation\"] = transformed_segm\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE\n            annotation[\"segmentation\"] = transforms.apply_coords(\n                np.array(annotation[\"segmentation\"])\n            ).tolist()\n        else:\n            raise ValueError(\n                \"Only support segmentation list or dict, but got {}\".format(\n                    annotation[\"segmentation\"]\n                )\n            )\n\n    if \"keypoints\" in annotation:\n        keypoints = np.asarray(annotation[\"keypoints\"])\n        keypoints = keypoints.reshape(-1, 3)\n        keypoints[:, :2] = transforms.apply_coords(keypoints[:, :2]).reshape(-1)\n        if keypoint_hflip_indices is not None:\n            keypoints[keypoint_hflip_"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Convert bounding boxes into the `XYXY_ABS` mode\n    transforms = TransformList(transforms, image_size)\n    if \"bbox\" in annotation:\n        annotation[\"bbox\"] = transforms.apply_box([annotation[\"bbox\"]])[0]\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        # FIXME ideally this would use the `transforms.apply_coords` function\n        # instead of re-implementing it\n        assert annotation[\"segmentation\"][\"counts\"]\n        counts = annotation[\"segmentation\"][\"counts\"]\n        size = annotation[\"segmentation\"][\"size\"]\n        if isinstance(counts, list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            annotation[\"segmentation\"] = merge_polygons_to_mask(\n                [\n                    transforms.apply_coords(np.array(part, dtype=np.int32).reshape(-1, 2))\n                    for part in annotation[\"segmentation\"][\"counts\"]\n                ],\n                size=size,\n            )\n        else:\n            # mask\n            annotation[\"segmentation\"] = transforms.apply_coords(\n                np.array(counts, dtype=np.int32).reshape(size[1], size[0])\n            ).reshape(-1).tolist()\n            annotation[\"segmentation\"][\"counts\"] = annotation[\"segmentation\"]\n            annotation[\"segmentation\"][\"size\"] = list(reversed(size))\n\n    if \"keypoints\" in annotation:\n        kpts = np.array(annotation[\"keypoints\"])\n        kpts = kpts.reshape(-1, 3)\n        # xy -> xy'\n        kpts[:, :2] = transforms.apply_coords(kpts[:, :2])\n        # flip x coordinates\n        if keypoint_hflip_indices is"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    segmentation = annotation[\"segmentation\"]\n    keypoints = annotation[\"keypoints\"]\n    num_keypoints = len(keypoints) // 3\n\n    # Transform bounding box\n    if \"bbox_mode\" in annotation:\n        transforms = transforms.convert_and_apply_format(\n            annotation[\"bbox_mode\"], bbox, image_size\n        )\n    else:\n        transforms = transforms.convert_and_apply_format(\n            \"xyxy_abs\", bbox, image_size\n        )\n    bbox = transforms.apply_box(bbox)\n    bbox = transforms.apply_coords(bbox).clip(min=0, max=image_size)\n\n    # Transform segmentation\n    if isinstance(segmentation, list):\n        # polygon\n        for i in range(0, len(segmentation), 2):\n            segmentation[i] = transforms.apply_coords(\n                np.array([segmentation[i], segmentation[i + 1]])\n            ).ravel()\n        segmentation = [p for p in segmentation if len(p) >= 6]\n        if len(segmentation) == 0:\n            segmentation = [0.0, 0.0, 0.0, 0.0]\n    elif isinstance(segmentation, dict):\n        # RLE\n        if \"counts\" in segmentation:\n            segmentation = transforms.apply_coords(\n                np.array(segmentation[\"counts\"])\n            ).tolist()\n        if \"size\" in segmentation:\n            segmentation[\"size\"] = list(image_size)\n    else:\n        raise TypeError(\n            f\"Expected segmentation of type list or dict, got {type(segmentation)}\"\n        )\n\n    # Transform keypoints\n    if len(keypoints) > 0:\n        keypoints = np.array(keypoints).reshape(-1, 3)\n        keypoints = transforms.apply_co"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Convert bounding box to XYXY_ABS mode\n    annotation[\"bbox_mode\"] = XYXY_ABS\n\n    # Transform bounding box\n    if \"bbox\" in annotation:\n        bbox = transforms.apply_box([annotation[\"bbox\"]])[0]\n        annotation[\"bbox\"] = bbox\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            if not annotation[\"segmentation\"]:\n                annotation[\"segmentation\"] = []\n            else:\n                # The format of segmentation is <x1, y1, x2, y2, ...>\n                transformed_segmentation = transforms.apply_coords(\n                    np.asarray(annotation[\"segmentation\"]).reshape(1, -1)\n                )\n                annotation[\"segmentation\"] = transformed_segmentation.reshape(-1).tolist()\n\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE\n            transformed_segmentation = transforms.apply_coords(\n                np.asarray(annotation[\"segmentation\"][\"counts\"]).reshape(1, -1)\n            )\n            annotation[\"segmentation\"][\"counts\"] = (\n                transformed_segmentation.reshape(-1).tolist()\n            )\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = np.asarray(annotation[\"keypoints\"]).reshape(-1, 3)\n        # Flip keypoints horizontally\n        if keypoint_hflip_indices is not None:\n            keypoints = keypoints[keypoint_hflip_indices]\n        keypoints = transforms.apply_coords(keypoints)\n        # Clip keypoints to image size\n        keypoints = clip_box(keypoints, *image_size)\n        annotation[\"keypoints\"] = keypoints"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if coords.size == 0:\n            return coords\n\n        if self.angle % 360 == 0:\n            return coords\n\n        # Rotate the coordinates\n        rotated_coords = np.dot(coords, self.rm_coords)\n\n        return rotated_coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        # Check if the input is empty\n        if len(coords) == 0:\n            return coords\n\n        # Check if the rotation angle is a multiple of 360 degrees\n        if self.angle % 360 == 0:\n            return coords\n\n        # Apply the rotation transformation to the coordinates\n        rotated_coords = np.dot(coords, self.rm_coords)\n\n        return rotated_coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        # Check if the input coordinates are empty\n        if coords.size == 0:\n            return coords\n\n        # Check if the rotation angle is a multiple of 360 degrees\n        if self.angle % 360 == 0:\n            return coords\n\n        # Apply the rotation transformation\n        rotated_coords = np.dot(coords, self.rm_coords)\n\n        return rotated_coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        # Check if input is empty\n        if coords.size == 0:\n            return coords\n\n        # Check if rotation angle is a multiple of 360 degrees\n        if self.angle % 360 == 0:\n            return coords\n\n        # Apply rotation transformation\n        rotated_coords = np.dot(coords, self.rm_coords)\n\n        return rotated_coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if coords.size == 0:\n            return coords\n\n        if self.angle % 360 == 0:\n            return coords\n\n        coords_rotated = np.dot(coords, self.rm_coords)\n\n        return coords_rotated"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if coords.size == 0:\n            return coords\n\n        # If the angle is a multiple of 360, return the original coordinates\n        if self.angle % 360 == 0:\n            return coords\n\n        # Apply the rotation transformation to the coordinates\n        rotated_coords = np.dot(self.rm_coords, coords.T).T\n\n        return rotated_coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0:\n            return coords\n\n        if self.angle % 360 == 0:\n            return coords\n\n        if len(coords.shape) == 1:\n            coords = coords.reshape((1, 2))\n\n        coords_trans = np.dot(coords, self.rm_coords)\n\n        return coords_trans"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        # Check if input is empty\n        if len(coords) == 0:\n            return coords\n\n        # Check if rotation angle is a multiple of 360 degrees\n        if self.angle % 360 == 0:\n            return coords\n\n        # Apply rotation transformation\n        transformed_coords = np.dot(coords, self.rm_coords)\n\n        return transformed_coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0:\n            return coords\n\n        # Get the rotation matrix and angle from the instance\n        rm_coords = self.rm_coords\n        angle = self.angle\n\n        # Check if the angle is a multiple of 360 degrees\n        if angle % 360 == 0:\n            return coords\n\n        # Apply the rotation transformation to the input coordinates\n        rotated_coords = np.dot(coords, rm_coords)\n\n        return rotated_coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0:\n            return coords\n\n        if self.angle % 360 == 0:\n            return coords\n\n        # Get the rotation matrix\n        rm = self.rm_coords\n\n        # Apply the rotation matrix to the input coordinates\n        rotated_coords = np.dot(rm, np.transpose(coords))\n\n        # Transpose the result back to get the rotated coordinates\n        rotated_coords = np.transpose(rotated_coords)\n\n        return rotated_coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0:\n            return coords\n\n        if self.angle % 360 == 0:\n            return coords\n\n        coords_rot = np.dot(coords, self.rm_coords)\n\n        return coords_rot\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0:\n            return coords\n        if self.angle % 360 == 0:\n            return coords\n\n        coords = np.asarray(coords)\n        coords = np.transpose(coords)\n        coords = np.dot(self.rm_coords, coords)\n        coords = np.transpose(coords)\n\n        return coords\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if self.angle == 0:\n            return coords\n\n        if coords.shape[0] == 0:\n            return coords\n\n        # Convert the angle to radians\n        angle_rad = np.deg2rad(self.angle)\n\n        # Define the rotation matrix\n        rm = np.array([[np.cos(angle_rad), -np.sin(angle_rad)],\n                       [np.sin(angle_rad), np.cos(angle_rad)]])\n\n        # Apply the rotation matrix to the input coordinates\n        transformed_coords = np.dot(coords, rm)\n\n        return transformed_coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        # Check if input is empty\n        if coords.size == 0:\n            return coords\n\n        # Check if rotation angle is a multiple of 360 degrees\n        if np.isclose(self.angle % 360, 0):\n            return coords\n\n        # Apply rotation transformation\n        transformed_coords = np.dot(coords, self.rm_coords)\n\n        return transformed_coords\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        # If the rotation angle is a multiple of 360 degrees or the input is empty, return the original coordinates without modification.\n        if self.angle % 360 == 0 or len(coords) == 0:\n            return coords\n\n        # If the rotation angle is not a multiple of 360 degrees, apply the rotation transformation to the input coordinates.\n        else:\n            # Calculate the rotation matrix for the given angle.\n            rm_coords = self.rm_coords\n            # Apply the rotation matrix to the input coordinates.\n            coords_rot = np.dot(coords, rm_coords)\n            return coords_rot\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if coords.size == 0:\n            return coords\n\n        if np.mod(self.angle, 360) == 0:\n            return coords\n\n        return np.dot(coords, self.rm_coords)\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0:\n            return coords\n        if self.angle % 360 == 0:\n            return coords\n        else:\n            return np.dot(coords, self.rm_coords)\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0:\n            return coords\n\n        if self.angle % 360 == 0:\n            return coords\n\n        # Rotate coordinates\n        coords_rotated = np.dot(coords, self.rm_coords)\n\n        return coords_rotated\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if self.angle == 0 or len(coords) == 0:\n            return coords\n        else:\n            # Apply the rotation transformation to the input coordinates\n            rotated_coords = np.dot(coords, self.rm_coords)\n            return rotated_coords\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if coords.size == 0:\n            return coords\n\n        if self.angle % 360 == 0:\n            return coords\n\n        # Rotation matrix\n        rm = self.rm_coords\n\n        # Apply rotation matrix\n        coords_transformed = np.dot(coords, rm)\n\n        return coords_transformed\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flops_dict = defaultdict(float)\n    for input_dict in inputs:\n        with torch.no_grad():\n            with torch.cuda.amp.autocast(enabled=False):\n                with torch.cuda.amp.autocast(enabled=False):\n                    with torch.cuda.amp.autocast(enabled=False):\n                        with torch.cuda.amp.autocast(enabled=False):\n                            with torch.cuda.amp.autocast(enabled=False):\n                                with torch.cuda.amp.autocast(enabled=False):\n                                    with torch.cuda.amp.autocast(enabled=False):\n                                        with torch.cuda.amp.autocast(enabled=False):\n                                            with torch.cuda.amp.autocast(enabled=False):\n                                                with torch.cuda.amp.autocast(enabled=False):\n                                                    with torch.cuda.amp.autocast(enabled=False):\n                                                        with torch.cuda.amp.autocast(enabled=False):\n                                                            with torch.cuda.amp.autocast(enabled=False):\n                                                                with torch.cuda.amp.autocast(enabled=False):\n                                                                    with torch.cuda.amp.autocast(enabled=False):\n                                                                        with torch.cuda.amp.autocast(enabled=False):\n                                                                            with torch.cuda.amp.autocast(enabled=False):\n                                                                                with torch.cuda.amp.autocast(enabled=False):\n                                                                                    with torch.cuda.amp.autocast(enabled=False):\n                                                                                        with torch.cuda.amp.autocast(enabled=False):\n                                                                                            with torch.cuda.amp.autocast(enabled=False):\n                                                                                                with torch.cuda.amp.autocast(enabled=False):\n                                                                                                    with torch.cuda.amp.autocast(enabled=False):\n                                                                                                        with torch.cuda.amp.autocast(enabled=False"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flops_dict = collections.defaultdict(float)\n    for inp in inputs:\n        with torch.no_grad():\n            model(inp)\n\n        def accumulate_flops(module, inputs, outputs):\n            if hasattr(module, \"flops\"):\n                flops_dict[module.__class__.__name__] += module.flops\n\n        model.apply(accumulate_flops)\n\n    return flops_dict"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Create a deep copy of the model to avoid modifying the original model\n    model_copy = copy.deepcopy(model)\n\n    # Set the model to evaluation mode\n    model_copy.eval()\n\n    # Disable gradient calculation\n    with torch.no_grad():\n        # Jit compile the model\n        model_jit = torch.jit.script(model_copy)\n\n        # Get the flops count\n        flops_count = flop_count_table(model_jit, inputs)\n\n    return flops_count"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Initialize an empty dictionary to store the flops count for each operator\n    flops_count = defaultdict(float)\n\n    # Define a function to compute the flops for a single input\n    def compute_flops(input):\n        # Jit compile the model with the input\n        jit_model = torch.jit.trace(model, input)\n\n        # Use the jit model to compute the flops\n        flops = flop_count_str(jit_model, input)\n\n        # Add the flops to the flops count dictionary\n        for key, value in flops.items():\n            flops_count[key] += value\n\n    # Compute the flops for each input\n    for input in inputs:\n        compute_flops(input)\n\n    # Return the flops count dictionary\n    return flops_count"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flops_dict = defaultdict(float)\n    for input in inputs:\n        with torch.no_grad():\n            model(input)\n\n        for module in model.modules():\n            if isinstance(module, torch.nn.Conv2d):\n                input = input[\"image\"]\n                kernel_size = module.kernel_size\n                in_channels = input.shape[1]\n                output_channels = module.out_channels\n                output_height = input.shape[2]\n                output_width = input.shape[3]\n\n                # Calculate the number of multiply-accumulates (MACs)\n                num_macs = output_channels * (kernel_size[0] * kernel_size[1] * in_channels) * (\n                    output_height * output_width\n                )\n\n                # Convert MACs to GFLOPs\n                num_gflops = num_macs / 1e9\n\n                # Add the GFLOPs to the dictionary\n                flops_dict[module._get_name()] += num_gflops\n\n    return flops_dict"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flops_dict = collections.defaultdict(float)\n    model.eval()\n    with torch.no_grad():\n        for i, input in enumerate(inputs):\n            input_image = input[\"image\"]\n            # Run the model with the input image\n            with torch.jit.optimized_execution(True):\n                model(input_image)\n\n            # Get the flops for the model\n            flops = flop_count_table(model, input_image)\n\n            # Add the flops to the dictionary\n            for op, count in flops.items():\n                flops_dict[op] += count\n\n    # Compute the average flops across the inputs\n    for op in flops_dict:\n        flops_dict[op] /= len(inputs)\n\n    return flops_dict"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flops_dict = collections.defaultdict(float)\n    for input_dict in inputs:\n        with torch.no_grad():\n            input_image = input_dict[\"image\"]\n            input_image = input_image.to(model.device)\n            model(input_image)\n        flops_dict = add_flops(flops_dict, model)\n    return flops_dict\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Get the model's device\n    device = next(model.parameters()).device\n\n    # Move the model to the device\n    model = model.to(device)\n\n    # Create a dummy input to the model\n    dummy_input = [{\"image\": torch.randn(inputs[0][\"image\"].shape).to(device)}]\n\n    # Create a dummy model that outputs the flops\n    flop_model = torch.jit.trace(model, dummy_input)\n\n    # Get the flops using the dummy model\n    flops = flop_count_table(flop_model, dummy_input)\n\n    # Convert the flops to Gflops\n    flops = {key: value / 1e9 for key, value in flops.items()}\n\n    return flops\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Set the model to evaluation mode\n    model.eval()\n\n    # Create a defaultdict to store the flops count for each operator\n    flops_count = collections.defaultdict(float)\n\n    # Create a dummy input to feed the model\n    dummy_input = [{\"image\": torch.randn(inputs[0][\"image\"].shape).to(inputs[0][\"image\"].device)}]\n\n    # Disable gradient computation\n    with torch.no_grad():\n        # Create a jit traced model\n        traced_model = torch.jit.trace(model, dummy_input)\n\n        # Run the traced model with the provided inputs\n        _ = traced_model(inputs)\n\n        # Compute the flops count for each operator\n        for module_name, module in traced_model.named_modules():\n            if hasattr(module, \"flops_count\"):\n                flops_count[module_name] += module.flops_count\n\n    # Return the flops count dictionary\n    return flops_count"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Create a copy of the model to avoid affecting the original model\n    model_copy = copy.deepcopy(model)\n\n    # Initialize a defaultdict to store the flops counts for each operator\n    flops_dict = defaultdict(float)\n\n    # Iterate over the inputs and compute the flops for each model\n    for input_data in inputs:\n        # Run the model with the input data\n        with torch.no_grad():\n            model_copy(input_data)\n\n        # Compute the flops for the model\n        flops = profile(model_copy, inputs=(input_data,), record_flops=True)\n\n        # Update the flops_dict with the computed flops\n        for key, value in flops.items():\n            flops_dict[key] += value\n\n    # Compute the average flops for each operator\n    avg_flops_dict = {key: value / len(inputs) for key, value in flops_dict.items()}\n\n    return avg_flops_dict"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Initialize an empty dictionary to store the flops count for each operator\n    flops_count = collections.defaultdict(float)\n\n    # Create a dummy input tensor for the model\n    dummy_input = torch.randn(inputs[0][\"image\"].shape).to(inputs[0][\"image\"].device)\n\n    # Use jit compilation to compute the flops\n    with torch.no_grad():\n        with torch.jit.optimized_execution(True):\n            with torch.jit.fuser(\"fuser2\"):\n                model(dummy_input)\n\n                # Iterate over the named modules in the model\n                for module_name, module in model.named_modules():\n                    # Check if the module has a flop_count_str attribute\n                    if hasattr(module, \"flop_count_str\"):\n                        # Get the flops count for the module\n                        flops_count[module_name] = module.flop_count_str()\n\n    return flops_count"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flops_dict = collections.defaultdict(float)\n    for input_dict in inputs:\n        with torch.no_grad():\n            model(input_dict)\n            flops_dict = add_flops(flops_dict, model)\n\n    for key in flops_dict.keys():\n        flops_dict[key] /= len(inputs)\n\n    return flops_dict\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Initialize a defaultdict to store the flops count for each operator\n    flops_dict = collections.defaultdict(float)\n\n    # Iterate over the inputs\n    for input_dict in inputs:\n        # Extract the image from the input dictionary\n        image = input_dict[\"image\"]\n\n        # Jit compile the model with the input image\n        model_jit = torch.jit.script(model)\n\n        # Run the model with the input image\n        model_jit(image)\n\n        # Get the flops count for each operator in the model\n        flops_dict = add_flops_dicts(flops_dict, profile(model_jit, inputs=(image,), custom_ops={nn.Upsample: None}))\n\n    # Return the flops count dictionary\n    return flops_dict\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Initialize a defaultdict to store the flop counts for each operator\n    flop_count_dict = defaultdict(float)\n\n    # Iterate over the inputs\n    for input_dict in inputs:\n        # Extract the image tensor from the input dictionary\n        image = input_dict[\"image\"]\n\n        # Define a function that computes the flops for a given model and input\n        def compute_flops(model, image):\n            # Create a new model instance\n            model_instance = model\n\n            # Enable the model for evaluation\n            model_instance.eval()\n\n            # Create a new jit model instance\n            jit_model = torch.jit.trace(model_instance, image)\n\n            # Enable the jit model for evaluation\n            jit_model.eval()\n\n            # Run the jit model with the input image\n            jit_model(image)\n\n            # Return the number of flops performed by the jit model\n            return jit_model.graph.flops()\n\n        # Compute the flops for the model and input\n        flops = compute_flops(model, image)\n\n        # Update the flop count dictionary with the computed flops\n        flop_count_dict[model.__class__.__name__] += flops\n\n    # Return the flop count dictionary\n    return flop_count_dict"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flop_dict = defaultdict(float)\n    model.eval()\n    with torch.no_grad():\n        for input in inputs:\n            torch.cuda.empty_cache()\n            image = input[\"image\"]\n            image = image.to(device=\"cuda\", non_blocking=True)\n            with torch.cuda.amp.autocast(enabled=False):\n                with torch.jit.optimized_execution(True):\n                    with torch.jit.fuser(\"fuser2\"):\n                        with torch.jit.backends.cuda.graph(True):\n                            with torch.autograd.profiler.profile(\n                                enabled=True, use_cuda=True\n                            ) as prof:\n                                model(image)\n                            for event in prof.key_averages():\n                                if event.key == \"aten::addmm\":\n                                    flop_dict[\"addmm\"] += event.flops\n                                elif event.key == \"aten::mul\":\n                                    flop_dict[\"mul\"] += event.flops\n                                elif event.key == \"aten::add\":\n                                    flop_dict[\"add\"] += event.flops\n                                elif event.key == \"aten::cat\":\n                                    flop_dict[\"cat\"] += event.flops\n                                elif event.key == \"aten::_convolution\":\n                                    flop_dict[\"conv\"] += event.flops\n                                elif event.key == \"aten::_convolution_nogroup\":\n                                    flop_dict[\"conv\"] += event.flops\n                                elif event.key == \"aten::_convolution_nogroup_backward\":\n                                    flop_dict[\"conv\"] += event.flops\n                                elif event.key == \"aten::_convolution_backward\":\n                                    flop_dict[\"conv\"] += event.flops\n                                elif event.key == \"aten::_convolution_nogroup_backward_overrideable\":\n                                    flop_dict[\"conv\"] += event.flops\n                                elif event.key == \"aten::_convolution_backward_overrideable\":\n                                    flop_dict"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Define a function that takes a model and a list of inputs and returns the flops count for each operator in the model\n    def flop_count_func(model, inputs):\n        # Create a defaultdict to store the flops count for each operator\n        flop_count = collections.defaultdict(float)\n\n        # Define a function that computes the flops for a single input\n        def compute_flops(input):\n            # Use jit to compile the model\n            model_jit = torch.jit.script(model)\n\n            # Create a module that can track the flops\n            module = flop_counter.FlopCountAnalysis(model_jit.forward, device=\"cpu\")\n\n            # Run the model with the input and track the flops\n            module.run_with_tensors(input)\n\n            # Return the flops count\n            return module.total()\n\n        # Compute the flops for each input and accumulate the results in the defaultdict\n        for input in inputs:\n            flops = compute_flops(input)\n            for op_name, op_flops in flops.items():\n                flop_count[op_name] += op_flops\n\n        # Return the flops count for each operator\n        return flop_count\n\n    # Compute the flops count for the model with the given inputs\n    flop_count = flop_count_func(model, inputs)\n\n    # Return the flops count\n    return flop_count"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Initialize a defaultdict to store the flops count for each operator\n    flops_dict = defaultdict(float)\n\n    # Create a dummy input to the model\n    dummy_input = [{\"image\": torch.randn(3, 224, 224).to(device)}]\n\n    # Jit compile the model\n    model = torch.jit.script(model)\n\n    # Get the number of inputs to the model\n    num_inputs = len(inputs)\n\n    # Iterate over the inputs and compute the flops for each\n    for i in range(num_inputs):\n        # Run the model with the current input\n        output = model(inputs[i])\n\n        # Compute the flops for the current input\n        flops = profile(model, inputs=(dummy_input,), custom_ops={\n            nn.modules.conv._ConvNd: None,\n            nn.modules.linear.Linear: None,\n            nn.modules.batchnorm._BatchNorm: None,\n            nn.modules.activation.ReLU: None,\n            nn.modules.pooling.MaxPool2d: None,\n            nn.modules.pooling.AvgPool2d: None,\n            nn.modules.pooling.AdaptiveMaxPool2d: None,\n            nn.modules.pooling.AdaptiveAvgPool2d: None,\n            nn.modules.activation.Sigmoid: None,\n            nn.modules.activation.Softmax: None,\n            nn.modules.activation.LogSoftmax: None,\n            nn.modules.activation.Tanh: None,\n            nn.modules.activation.LeakyReLU: None,\n            nn.modules.activation.ELU: None,\n            nn.modules.activation.ReLU6: None,\n            nn.modules.activation.PReLU: None,\n            nn.modules.activation.SELU: None,\n            nn.modules.activation.CE"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    import torch\n    import torch.nn as nn\n    import torch.nn.utils.stateless as stateless_nn\n    from collections import defaultdict\n    import torch.jit\n    from torch.utils._pytree import tree_flatten\n    from torch.utils._pytree import tree_unflatten\n    from torch.utils._pytree import tree_map\n    from torch.utils._pytree import tree_multimap\n\n    # Compute the flops for each operator in the model\n    flop_counts = defaultdict(float)\n    for input_dict in inputs:\n        # Run the model with the input and get the output\n        output = model(input_dict)\n\n        # Compute the flops for the output\n        flops = torch.jit.get_num_flops(model, input_dict)\n\n        # Add the flops to the total flop count for each operator\n        for name, value in flops.items():\n            flop_counts[name] += value\n\n    # Normalize the flop count by the number of inputs\n    num_inputs = len(inputs)\n    flop_counts = {name: value / num_inputs for name, value in flop_counts.items()}\n\n    return flop_counts\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    from detectron2.utils.flop_count import flop_count_table\n    from detectron2.utils.flop_count import flop_count_str\n\n    # Define the device to run the model on\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n    # Move the model to the device\n    model.to(device)\n\n    # Define the dummy input for the model\n    dummy_input = [{\"image\": torch.randn(1, 3, 224, 224).to(device)}]\n\n    # Run the model with the dummy input and compute the flops\n    flops_count_table = flop_count_table(model, dummy_input)\n\n    # Convert the flops_count_table to a dictionary\n    flops_count_dict = {\n        op_name: flops_count_str(flops_count_table[op_name])\n        for op_name in flops_count_table\n    }\n\n    # Compute the total flops\n    total_flops = sum(\n        [\n            flops_count_table[op_name].flops\n            for op_name in flops_count_table\n        ]\n    )\n\n    # Compute the flops per operator\n    flops_per_operator = {\n        op_name: flops_count_table[op_name].flops / total_flops\n        for op_name in flops_count_table\n    }\n\n    # Compute the flops per input\n    flops_per_input = {\n        op_name: flops_count_table[op_name].flops / len(inputs)\n        for op_name in flops_count_table\n    }\n\n    # Compute the flops per image\n    flops_per_image = {\n        op_name: flops_count_table[op_name].flops / inputs[0][\"image\"].shape[0]\n        for op_name in flops_count_table\n    }"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    import torch\n    import torch.nn as nn\n    from torch.profiler import profile, record_function, ProfilerActivity\n    from collections import defaultdict\n\n    # Define a helper function to get the flops of a module\n    def get_flops(module: nn.Module, input: torch.Tensor, output: torch.Tensor) -> None:\n        \"\"\"\n        This helper function updates the flops_dict with the flops of a given module. It is designed to be called by the torch.profiler.profile method.\n\n        Input-Output Arguments\n        :param module: nn.Module. The module whose flops are being calculated.\n        :param input: torch.Tensor. The input to the module.\n        :param output: torch.Tensor. The output of the module.\n        \"\"\"\n        flops_dict[module.__class__.__name__] += torch.numel(output)\n\n    # Create a default dictionary to store the flops of each operator\n    flops_dict = defaultdict(float)\n\n    # Run the model with the provided inputs and record the flops\n    with profile(activities=[ProfilerActivity.CPU], record_flops=True) as prof:\n        with record_function(\"model_inference\"):\n            model(inputs)\n    # Get the flops from the profiler and update the flops_dict\n    for k, v in prof.key_averages(group_by_input_shape=False).items():\n        get_flops(k.target, k.input_size, k.output_size)\n\n    # Convert the flops_dict to a default dictionary and return it\n    flops_dict = defaultdict(float, flops_dict)\n    return flops_dict\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return None\n\n        if self.rm_image is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return img\n\n        if self.angle == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        h, w = img.shape[:2]\n\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or img.size == 0:\n            return img\n\n        if self.rm_image is None:\n            return img\n\n        if self.angle == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or self.angle == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        h, w = img.shape[:2]\n        rm_image = self.rm_image\n        bound_w, bound_h = self.bound_w, self.bound_h\n\n        img = cv2.warpAffine(img, rm_image, (bound_w, bound_h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n\n        return img"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or self.rm_image is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        h, w = img.shape[:2]\n\n        if self.bound_h == h and self.bound_w == w:\n            return img\n\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR,\n                              borderMode=cv2.BORDER_REPLICATE)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.shape[0] == 0 or img.shape[1] == 0:\n            return img\n\n        if self.angle == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp, borderMode=cv2.BORDER_CONSTANT)\n\n        return img"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or img.size == 0:\n            return img\n\n        if self.angle == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        h, w = img.shape[:2]\n\n        rm = self.rm_image\n\n        img = cv2.warpAffine(img, rm, (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n\n        return img"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or self.angle == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        if self.angle % 360 == 0:\n            return img\n\n        rm_image = self.rm_image\n        h, w = self.h, self.w\n        bound_w, bound_h = self.bound_w, self.bound_h\n\n        img = cv2.warpAffine(img, rm_image, (bound_w, bound_h), flags=cv2.INTER_LINEAR,\n                             borderMode=cv2.BORDER_REFLECT_101)\n\n        return img"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or img.shape[0] == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        if self.rm_image is None:\n            return img\n\n        if self.angle == 0:\n            return img\n\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR,\n                              borderMode=cv2.BORDER_REFLECT_101)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        if self.rm_image is None:\n            return img\n\n        if self.angle == 0:\n            return img\n\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(128, 128, 128))"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if not img.size:\n            return img\n\n        if self.angle == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        rm_image = self.rm_image\n        h, w = self.h, self.w\n        bound_h, bound_w = self.bound_h, self.bound_w\n\n        return cv2.warpAffine(img, rm_image, (bound_w, bound_h), flags=interp, borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        # If the image is empty, return it unchanged\n        if img is None:\n            return img\n\n        # If the angle results in no change, return the original image\n        if self.rm_image is None:\n            return img\n\n        # If no interpolation method is provided, use the default method\n        if interp is None:\n            interp = self.interp\n\n        # Apply the rotation transformation using OpenCV's warpAffine function\n        rotated = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR,\n                                 borderMode=cv2.BORDER_REPLICATE)\n\n        return rotated"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or self.angle == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        h, w = img.shape[:2]\n        rm = cv2.getRotationMatrix2D((w / 2, h / 2), self.angle, 1)\n        self.rm_image = rm\n        self.h = h\n        self.w = w\n        self.bound_h, self.bound_w = h, w\n        return cv2.warpAffine(img, rm, (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        # If the image is empty or the angle results in no change, return the original image unchanged\n        if img is None or self.rm_image is None:\n            return img\n\n        # If the image is not empty and the angle results in a change, rotate the image using OpenCV's warpAffine function\n        if img is not None and self.rm_image is not None:\n            # If interp is not provided, use the instance's default interpolation method\n            if interp is None:\n                interp = self.interp\n            # Apply the rotation transformation to the image using OpenCV's warpAffine function\n            img_out = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR,\n                                     borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n\n        return img_out"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return None\n\n        if interp is None:\n            interp = self.interp\n\n        if self.angle == 0:\n            return img\n\n        # Get the image dimensions\n        h, w = img.shape[:2]\n\n        # Compute the new bounding dimensions after rotation\n        bound_w = int(abs(w * np.cos(self.angle)) + abs(h * np.sin(self.angle)))\n        bound_h = int(abs(h * np.cos(self.angle)) + abs(w * np.sin(self.angle)))\n\n        # Compute the rotation center based on the original image dimensions\n        center = (bound_w // 2, bound_h // 2)\n\n        # Compute the rotation matrix\n        rm_image = cv2.getRotationMatrix2D(center, self.angle, 1.0)\n\n        # Apply the rotation transformation using OpenCV's warpAffine function\n        img = cv2.warpAffine(img, rm_image, (bound_w, bound_h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n\n        return img"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or self.angle == 0:\n            return img\n\n        h, w = img.shape[:2]\n\n        if interp is None:\n            interp = self.interp\n\n        if self.angle % 360 == 0:\n            return img\n\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR,\n                              borderMode=cv2.BORDER_REFLECT_101)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.rm_image is None:\n            return img\n\n        if np.all(self.rm_image == np.eye(3)):\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        return cv2.warpAffine(img, self.rm_image[:2, :], (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR,\n                              borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or img.size == 0:\n            return img\n\n        if self.angle == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        h, w, _ = img.shape\n        M = self.rm_image\n\n        return cv2.warpAffine(img, M, (self.bound_w, self.bound_h), flags=interp)\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return None\n        if self.angle == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n\n        # get rotation matrix\n        rm = self.rm_image\n\n        # compute new image shape\n        h, w = img.shape[:2]\n        bound_w, bound_h = self.bound_w, self.bound_h\n        # get transformation matrix\n        m = np.float32([[1, 0, bound_w / 2 - w / 2], [0, 1, bound_h / 2 - h / 2]])\n        # get the image with borders\n        img_with_borders = cv2.warpAffine(img, m, (bound_w, bound_h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n        # rotate the image with the new transformation matrix\n        img_rotated = cv2.warpAffine(img_with_borders, rm, (bound_w, bound_h), flags=interp)\n\n        return img_rotated"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None:\n            return img\n\n        if self.angle == 0.0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        # Get the height and width of the input image\n        h, w = img.shape[:2]\n\n        # Compute the bounding dimensions after rotation\n        bound_w = int(abs(h * np.sin(self.angle)) + abs(w * np.cos(self.angle)))\n        bound_h = int(abs(h * np.cos(self.angle)) + abs(w * np.sin(self.angle)))\n\n        # Compute the center of the image\n        center = (bound_w // 2, bound_h // 2)\n\n        # Compute the rotation matrix\n        rm_image = cv2.getRotationMatrix2D(center, self.angle, 1)\n\n        # Apply the rotation transformation\n        img = cv2.warpAffine(img, rm_image, (bound_w, bound_h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n\n        return img\n\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create an empty image for drawing the predictions on\n        img = self.output.get_image()\n        if img is None:\n            img = self.output.get_original_image()\n        if img is None:\n            return self.output\n\n        # if self.metadata.get(\"thing_classes\", None) is None:\n        #     self.metadata.update({\"thing_classes\": self._get_thing_classes(predictions)})\n\n        # if self.metadata.get(\"thing_colors\", None) is None:\n        #     self.metadata.update({\"thing_colors\": self._get_colors(predictions)})\n\n        # if self.metadata.get(\"thing_colors\", None) is None:\n        #     self.metadata.update({\"thing_colors\": self._get_colors(predictions)})\n\n        # if self.metadata.get(\"thing_colors\", None) is None:\n        #     self.metadata.update({\"thing_colors\": self._get_colors(predictions)})\n\n        # if self.metadata.get(\"thing_colors\", None) is None:\n        #     self.metadata.update({\"thing_colors\": self._get_colors(predictions)})\n\n        # if self.metadata.get(\"thing_colors\", None) is None:\n        #     self.metadata.update({\"thing_colors\": self._get_colors(predictions)})\n\n        # if self.metadata.get(\"thing_colors\", None) is None:\n        #     self.metadata.update({\"thing_colors\": self._get_colors(predictions)})\n\n        # if self.metadata.get(\"thing_colors\", None) is None:\n        #     self.metadata.update({\"thing_colors\": self._get_colors(predictions)})\n\n        # if self.metadata.get(\"thing_colors\", None) is None:\n        #     self.metadata.update({\"thing_colors\": self._get_colors(predictions)})\n\n        # if self.metadata.get(\"thing_colors\", None) is None"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create an image copy\n        img = self.output.get_image()\n        vis_img = VisImage(img)\n\n        # Draw instance-level prediction results\n        if predictions.has(\"pred_boxes\"):\n            boxes = predictions.pred_boxes\n            boxes.scale(1.2)\n            boxes.clip(vis_img.image.shape[:2])\n            vis_img.draw_boxes(boxes.tensor.cpu(), getattr(predictions, \"pred_classes\", None))\n        if predictions.has(\"pred_masks\"):\n            masks = predictions.pred_masks\n            masks = masks > 0.5\n            vis_img.draw_masks(masks, boxes.tensor.cpu(), getattr(predictions, \"pred_classes\", None))\n        if predictions.has(\"pred_keypoints\"):\n            keypoints = predictions.pred_keypoints\n            keypoints = keypoints[keypoints.sum(axis=1) > 0]\n            vis_img.draw_keypoints(keypoints)\n\n        # Update self.output\n        self.output = vis_img\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create an all white image of the same size as the original image\n        img = self.output.get_image() * 0 + 255\n\n        # Get the height and width of the image\n        height, width = img.shape[:2]\n\n        # Define a color map for class labels\n        colormap = np.array([[128, 64, 128],\n                             [244, 35, 232],\n                             [70, 70, 70],\n                             [102, 102, 156],\n                             [190, 153, 153],\n                             [153, 153, 153],\n                             [250, 170, 30],\n                             [220, 220, 0],\n                             [107, 142, 35],\n                             [152, 251, 152],\n                             [70, 130, 180],\n                             [220, 20, 60],\n                             [255, 0, 0],\n                             [0, 0, 142],\n                             [0, 0, 70],\n                             [0, 60, 100],\n                             [0, 80, 100],\n                             [0, 0, 230],\n                             [119, 11, 32]], dtype=np.uint8)\n\n        # Define a list of colors for class labels\n        colors = [colormap[i] for i in range(len(colormap))]\n\n        # Define a list of class labels\n        class_labels = [\n            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n            'train', 'truck', 'boat', 'traffic light', 'fire hydrant',\n            'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n            'horse"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        from detectron2.utils.visualizer import ColorMode\n\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n        if keypoints is not None:\n            keypoints = keypoints.to(self.output.image.device)\n        if isinstance(boxes, Boxes) and boxes.tensor.shape[1] == 4:\n            boxes = boxes.tensor.cpu().numpy()\n        if isinstance(keypoints, torch.Tensor) and keypoints.numel() > 0 and keypoints.shape[1] == 2:\n            keypoints = keypoints.to(dtype=torch.int32).view(keypoints.size(0), -1, 2).cpu().numpy()\n        if isinstance(keypoints, (list, tuple)):\n            keypoints = np.array(keypoints)\n\n        if self.metadata.get(\"thing_classes\", None) is not None:\n            labels = [self.metadata.thing_classes[i] for i in classes]\n        else:\n            labels = None\n\n        if self.output.panoptic_seg is not None:\n            panoptic_seg, segments_info = self.output.panoptic_seg\n            panoptic_seg = panoptic_seg.to(self.output.image.device)\n            segments_info = segments_info[0]\n            vis_segments = VisImage(self.output.image)\n            vis_segments.draw_panoptic(panoptic_seg.squeeze(0), segments_info)\n            if vis_segments.output.image.shape[2] < 4:\n                vis_segments.output.image = vis_segments."}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create an all white image of the same size as the original image\n        img = self.output.get_image() * 0 + 255\n\n        # If there are no predictions, return the original image\n        if len(predictions) == 0:\n            return VisImage(img)\n\n        # If there are predictions, draw the predictions on the image\n        for prediction in predictions:\n            # Get the bounding box coordinates and class label\n            box = prediction.pred_boxes.tensor.cpu().numpy()[0]\n            class_label = self.metadata.thing_classes[prediction.pred_classes.item()]\n\n            # Draw the bounding box and class label on the image\n            cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n            cv2.putText(img, class_label, (int(box[0]), int(box[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n            # If the prediction has a mask, draw the mask on the image\n            if prediction.has(\"pred_masks\"):\n                mask = prediction.pred_masks.cpu().numpy()[0]\n                img[mask] = (0, 255, 0)\n\n        # Return the image with the visualizations drawn on it\n        return VisImage(img)"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        from detectron2.structures import Boxes, BoxMode\n        from detectron2.utils.visualizer import ColorMode\n\n        boxes = predictions.pred_boxes\n        scores = predictions.scores\n        classes = predictions.pred_classes\n        masks = predictions.pred_masks\n        keypoints = predictions.pred_keypoints\n\n        if isinstance(boxes, Boxes) and boxes.tensor.shape[0] == 0:\n            logger.warning(\n                \"No bounding boxes to visualize of '{}' format.\".format(\n                    boxes.format\n                )\n            )\n            return self.output\n\n        if isinstance(boxes, Boxes) and boxes.tensor.shape[0] > 0:\n            boxes = boxes.tensor.cpu().numpy()\n            boxes = BoxMode.convert(boxes, predictions.pred_boxes.tensor.shape[-1], BoxMode.XYXY_ABS)\n        elif isinstance(boxes, (list, tuple)):\n            boxes = np.array(boxes, dtype=np.float32)\n        else:\n            raise NotImplementedError(\n                \"Currently do not support {} format as 'boxes'\".format(\n                    type(boxes)\n                )\n            )\n\n        if isinstance(scores, torch.Tensor):\n            scores = scores.cpu().numpy()\n        elif isinstance(scores, (list, tuple)):\n            scores = np.array(scores, dtype=np.float32)\n        else:\n            raise NotImplementedError(\n                \"Currently do not support {} format as 'scores'\".format(\n                    type(scores)\n                )\n            )\n\n        if isinstance(classes, torch.Tensor):\n            classes = classes.cpu().numpy()\n        elif isinstance(classes, (list, tuple)):\n            classes = np.array(classes, dtype=np.int64)\n        else:\n            raise NotImplementedError(\n                \"Currently do not support {} format as 'classes'\".format("}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if boxes is not None and classes is not None:\n            boxes_vis_params = BoxVisualizationParams(\n                self.metadata.get(\"thing_colors\", None), self.metadata.get(\"thing_radius\", None)\n            )\n            self.draw_boxes(boxes, boxes_vis_params)\n\n        if scores is not None and classes is not None:\n            scores_vis_params = ScoreVisualizationParams(\n                self.metadata.get(\"thing_colors\", None), self.metadata.get(\"thing_radius\", None)\n            )\n            self.draw_scores(boxes, scores, classes, scores_vis_params)\n\n        if keypoints is not None and classes is not None:\n            keypoints_vis_params = KeypointsVisualizationParams(\n                self.metadata.get(\"thing_colors\", None), self.metadata.get(\"thing_radius\", None)\n            )\n            self.draw_keypoints(keypoints, classes, keypoints_vis_params)\n\n        if self.instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\", None) is not None:\n            self.draw_segmentation_masks(predictions, self.metadata.get(\"thing_colors\", None))\n\n        if self.instance_mode == ColorMode.PANOPRINT:\n            self.draw_panoptic_seg_predictions(predictions)\n\n        if self.instance_mode == ColorMode.IMAGE_BW:\n            self.draw_grayscale_image()\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create an all white image of the same size as the original image\n        img = np.ones(self.output.shape[:2] + (3,), dtype=np.uint8) * 255\n        # Convert color from BGR to RGB\n        img = img[..., ::-1]\n        # Convert image to a PIL image\n        pil_image = Image.fromarray(img)\n        # Create a PIL drawing object\n        draw = ImageDraw.Draw(pil_image)\n        # Loop over each prediction\n        for prediction in predictions:\n            # Get the bounding box coordinates\n            box = prediction.bbox\n            # Get the class label\n            class_label = prediction.get_field(\"pred_classes\").item()\n            # Get the score\n            score = prediction.get_field(\"scores\").item()\n            # Get the mask\n            mask = prediction.get_field(\"pred_masks\").squeeze().cpu().numpy()\n            # Get the keypoints\n            keypoints = prediction.get_field(\"pred_keypoints\").cpu().numpy()\n            # Draw the bounding box\n            draw.rectangle(box.tolist(), outline=self.colors[class_label], width=3)\n            # Draw the class label\n            draw.text((box[0], box[1] - 10), str(class_label), fill=self.colors[class_label])\n            # Draw the score\n            draw.text((box[0], box[1] - 20), str(round(score, 2)), fill=self.colors[class_label])\n            # Draw the mask\n            mask = mask.astype(np.uint8)\n            mask = Image.fromarray(mask)\n            mask = mask.resize((box[2] - box[0], box[3] - box[1]))\n            pil_image.paste(mask, box[:2])\n            # Draw the keypoints\n            for keypoint in keypoints:\n                draw.ellipse(keypoint.tolist(), fill=self.colors"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        masks = predictions.pred_masks.tobitmask() if predictions.has(\"pred_masks\") else None\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if boxes is not None:\n            boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, self.image_size)\n            boxes = BoxMode.scale(boxes, 1.2, self.image_size)\n            if self.instance_mode == InstanceMode.Predict:\n                self.overlay_instances(\n                    labels=classes, boxes=boxes, masks=masks, keypoints=keypoints\n                )\n            else:\n                self.overlay_boxes(boxes=boxes)\n                if classes:\n                    self.overlay_class_names(classes)\n\n        if scores is not None:\n            if self.instance_mode == InstanceMode.Predict:\n                self.overlay_instances(scores=scores)\n            else:\n                self.overlay_scores(scores=scores)\n\n        return self.output\n\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create an empty image for drawing\n        img = self.output.copy()\n\n        # Get the height and width of the image\n        height, width = img.shape[:2]\n\n        # Create a list to store the drawn boxes\n        boxes_drawn = []\n\n        # Loop through the predictions\n        for i, pred in enumerate(predictions):\n\n            # If the prediction is a person, draw the bounding box\n            if pred.pred_classes.item() == 0:\n                # Get the bounding box coordinates\n                box = pred.pred_boxes.tensor.cpu().numpy()[0]\n                box = box.astype(int)\n\n                # Scale the bounding box coordinates to the image size\n                box[0] = int(box[0] * width)\n                box[1] = int(box[1] * height)\n                box[2] = int(box[2] * width)\n                box[3] = int(box[3] * height)\n\n                # Draw the bounding box on the image\n                cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n\n                # Add the bounding box coordinates to the list of drawn boxes\n                boxes_drawn.append(box)\n\n        # Return the image with the drawn boxes\n        return img"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create an image copy to draw on\n        self.output = self.input.copy()\n        # Get the image height and width\n        img_height, img_width = self.input.shape[:2]\n\n        # If there are no predictions, return the image as is\n        if len(predictions) == 0:\n            return self.output\n\n        # If the instance mode is \"prediction\", draw the boxes and labels on the image\n        if self.instance_mode == \"prediction\":\n            boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n            scores = predictions.scores if predictions.has(\"scores\") else None\n            classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n            self.draw_boxes(boxes, scores, classes)\n\n        # If the instance mode is \"groundtruth\", draw the ground truth boxes and labels on the image\n        elif self.instance_mode == \"groundtruth\":\n            self.draw_groundtruth(predictions)\n\n        # If the instance mode is \"both\", draw both the predicted and ground truth boxes and labels on the image\n        elif self.instance_mode == \"both\":\n            inds = np.where(predictions.get_field(\"gt\"))[0]\n            self.draw_groundtruth(predictions[inds])\n            self.draw_boxes(predictions, None, None)\n\n        # If the instance mode is \"keypoints\", draw the predicted keypoints on the image\n        if self.instance_mode == \"keypoints\":\n            self.draw_keypoints(predictions, img_height, img_width)\n\n        # If the instance mode is \"heatmap\", draw the predicted heatmaps on the image\n        if self.instance_mode == \"heatmap\":\n            self.output = self.output.astype(np.uint8)\n            self.output = cv2.addWeighted(self.output, 1, self.vissl_heatmap, 0.5, 0)\n\n        # Return the image with the visualizations drawn on it\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create an empty image for drawing the predictions on\n        img = self.output.get_image()\n        if img.shape[2] == 4:\n            img = img[:, :, :3]\n        vis_output = VisImage(img)\n\n        # Process predictions\n        if \"instances\" in predictions:\n            predictions = predictions[\"instances\"].to(self.cpu_device)\n        elif \"proposals\" in predictions:\n            # https://github.com/facebookresearch/detectron2/issues/298\n            predictions = predictions[\"proposals\"].to(self.cpu_device)\n\n        # Draw predictions\n        if self.instance_mode == InstanceMode.IMAGE:\n            vis_output.draw_instance_predictions(predictions)\n        elif self.instance_mode == InstanceMode.MASK:\n            vis_output.draw_instance_predictions(predictions)\n        elif self.instance_mode == InstanceMode.POINT:\n            vis_output.draw_instance_predictions(predictions)\n\n        # Draw bounding boxes around boxes that are partially occluded\n        if self.show_boxes_in_image:\n            if \"instances\" in predictions:\n                vis_output.draw_box(\n                    predictions.pred_boxes,\n                    thickness=2,\n                    alpha=0.8,\n                    colors=colors,\n                )\n            elif \"proposals\" in predictions:\n                # https://github.com/facebookresearch/detectron2/issues/298\n                vis_output.draw_box(\n                    predictions.proposal_boxes,\n                    thickness=2,\n                    alpha=0.8,\n                    colors=colors,\n                )\n\n        # Draw keypoint predictions\n        if self.show_keypoints:\n            vis_output.draw_keypoints(predictions.pred_keypoints, radius=2)\n\n        # Return the output image\n        return vis_output\n\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create an empty image for drawing the predictions on\n        img = self.output.get_image()\n        img = img.copy()\n\n        # Get the predicted classes, scores, and bounding boxes\n        if predictions.has(\"pred_boxes\"):\n            boxes = predictions.pred_boxes\n        elif predictions.has(\"proposal_boxes\"):\n            boxes = predictions.proposal_boxes\n\n        # Get the predicted classes, scores, and masks\n        classes = predictions.pred_classes\n        scores = predictions.scores\n        masks = predictions.pred_masks\n\n        # Get the predicted keypoints, if available\n        keypoints = None\n        if predictions.has(\"pred_keypoints\"):\n            keypoints = predictions.pred_keypoints\n\n        # Convert the bounding boxes to a format that can be used by the visualization functions\n        boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, self.output.image_scale)\n\n        # Draw the predicted boxes, classes, scores, and masks on the image\n        if self.instance_mode == InstanceMode.IMAGE:\n            img = self.draw_instance_predictions_on_image(img, boxes, classes, scores, masks, keypoints)\n        elif self.instance_mode == InstanceMode.RGB:\n            img = self.draw_instance_predictions_on_image(img, boxes, classes, scores, masks, keypoints)\n        elif self.instance_mode == InstanceMode.PANOPRINT:\n            img = self.draw_instance_predictions_on_panoptic(img, boxes, classes, scores, masks, keypoints)\n        elif self.instance_mode == InstanceMode.PDF:\n            img = self.draw_instance_predictions_on_pdf(img, boxes, classes, scores, masks, keypoints)\n        elif self.instance_mode == InstanceMode.LABEL:\n            img = self.draw_instance_predictions_on_label(img, boxes, classes"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create an empty image for drawing the predictions on\n        img = self.output.get_image()\n        # Get the height and width of the image\n        height, width = img.shape[:2]\n        # Get the class names\n        class_names = self.metadata.get(\"thing_classes\", None)\n        # Create a copy of the image\n        img_copy = img.copy()\n        # Get the instance mode\n        instance_mode = self.metadata.get(\"instance_mode\", ColorMode.IMAGE)\n        # Get the color mode\n        color_mode = self.metadata.get(\"color_mode\", ColorMode.IMAGE)\n        # Get the segmentation colors\n        segmentation_colors = self.metadata.get(\n            \"segmentation_colors\", None\n        )\n        # If the instance mode is \"predicted_classes\", set the color mode to \"grayscale\"\n        if instance_mode == InstanceMode.PREDICTED_CLASSES:\n            color_mode = ColorMode.SEGMENTATION\n        # If the instance mode is \"predicted_masks\", set the color mode to \"grayscale\"\n        if instance_mode == InstanceMode.PREDICTED_MASKS:\n            color_mode = ColorMode.SEGMENTATION\n        # If the instance mode is \"predicted_boxes\", set the color mode to \"grayscale\"\n        if instance_mode == InstanceMode.PREDICTED_BOXES:\n            color_mode = ColorMode.SEGMENTATION\n        # If the color mode is \"grayscale\", convert the image to grayscale\n        if color_mode == ColorMode.SEGMENTATION:\n            img_copy = img_copy.convert(\"L\")\n        # If the color mode is \"grayscale\", convert the image to grayscale\n        if color_mode == ColorMode.IMAGE:\n            img_copy = img_copy.convert(\"RGB\")\n        # If the instance mode is \"predicted_boxes\", set the color mode to \"grayscale\"\n        if instance_mode == InstanceMode.PREDICTED_BOXES:"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create an empty image for drawing the predictions on\n        img = self.output.get_image()\n        # Get the height and width of the image\n        h, w = img.shape[:2]\n        # Get the color mode of the image\n        color_mode = self.output.metadata.get(\"color_mode\", \"rgb\")\n        # If the image is in grayscale mode, convert it to RGB\n        if color_mode == \"grayscale\":\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n        # Get the instance mode from the visualizer\n        instance_mode = self.metadata.get(\"instance_mode\", \"bbox\")\n        # Get the score threshold from the visualizer\n        score_threshold = self.metadata.get(\"score_threshold\", self.score_threshold)\n        # Get the minimum image size from the visualizer\n        min_image_size = self.metadata.get(\"min_image_size\", 0)\n        # If the image is smaller than the minimum size, skip the visualization\n        if h < min_image_size or w < min_image_size:\n            return img\n        # If the image is in grayscale mode, convert it to RGB\n        if color_mode == \"grayscale\":\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n        # Get the predicted boxes from the predictions\n        pred_boxes = predictions.pred_boxes\n        # Get the predicted classes from the predictions\n        pred_classes = predictions.pred_classes\n        # Get the scores from the predictions\n        scores = predictions.scores\n        # Get the predicted masks from the predictions\n        pred_masks = predictions.pred_masks\n        # Get the predicted keypoints from the predictions\n        pred_keypoints = predictions.pred_keypoints\n        # If the instance mode is \"keypoint\", convert the predicted keypoints to a list of tuples\n        if instance_mode == \"keypoint\":\n            pred_keypoints = [\n                [(k"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create an empty array for the image\n        img = np.zeros(predictions.image_size + (3,), dtype=np.uint8)\n\n        # Create an image object from the array\n        vis_image = VisImage(img)\n\n        # Create an instance of the Predictor class\n        predictor = Predictor(self.cfg)\n\n        # Get the prediction results\n        instances = predictor.run_on_image(vis_image.image)\n\n        # Get the prediction results from the instances object\n        pred_boxes = instances.pred_boxes\n        pred_classes = instances.pred_classes\n        scores = instances.scores\n        pred_masks = instances.pred_masks\n\n        # Get the image size\n        img_height, img_width = vis_image.image.shape[:2]\n\n        # Loop over the instances\n        for i in range(len(pred_boxes)):\n            # Get the box coordinates\n            box = pred_boxes[i].tensor.numpy()\n            box = box.astype(np.int32)\n\n            # Get the class label\n            class_label = pred_classes[i].item()\n\n            # Get the score\n            score = scores[i].item()\n\n            # Get the mask\n            mask = pred_masks[i].numpy()\n\n            # Draw the box on the image\n            vis_image.draw_box(box, class_label, score)\n\n            # Draw the mask on the image\n            vis_image.draw_mask(mask)\n\n        # Return the visualization image\n        return vis_image"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create a copy of the original image and prepare it for drawing boxes, masks, keypoints, etc.\n        img = self.output.get_image()\n        img = img.copy()\n        vis_img = VisImage(img)\n\n        # Draw instance-level prediction results\n        if predictions.has(\"pred_boxes\"):\n            boxes = predictions.pred_boxes if predictions.mode == \"xyxy_abs\" else predictions.pred_boxes.convert(\n                \"xywh\")\n            vis_img = self.draw_instance_boxes(vis_img, boxes)\n        if predictions.has(\"pred_classes\"):\n            vis_img = self.draw_instance_predictions_classes(vis_img, predictions)\n        if predictions.has(\"scores\"):\n            vis_img = self.draw_instance_predictions_scores(vis_img, predictions)\n        if predictions.has(\"pred_masks\"):\n            vis_img = self.draw_instance_predictions_masks(vis_img, predictions)\n        if predictions.has(\"pred_keypoints\"):\n            vis_img = self.draw_instance_predictions_keypoints(vis_img, predictions)\n\n        return vis_img\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create a copy of the original image and prepare it to draw boxes on\n        img = self.output.get_image().copy()\n        if img.shape[2] == 4:\n            img = img[..., :3]\n\n        # Generate colors for drawing bounding boxes.\n        colors = self.compute_colors_for_labels(predictions.get(\"pred_classes\"))\n\n        # If not in inference mode, annotations are unavailable\n        if not self.with_instances:\n            return img\n\n        # Draw the bounding boxes and segmentation masks on the image\n        if self.mode == \"prediction\":\n            boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n            scores = predictions.scores if predictions.has(\"scores\") else None\n            classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n            masks = predictions.pred_masks if predictions.has(\"pred_masks\") else None\n            keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n            # Draw bounding boxes around the instances\n            if boxes:\n                if self.with_masks and masks:\n                    img = self.draw_masks(img, boxes, masks, colors, alpha=0.3)\n                img = self.draw_boxes(img, boxes, colors, alpha=0.5)\n\n            # Draw segmentation masks\n            if self.with_masks and masks:\n                if self.with_boxes and boxes:\n                    img = self.draw_masks(img, boxes, masks, colors, alpha=0.3)\n                else:\n                    img = self.draw_masks(img, None, masks, colors, alpha=0.5)\n\n            # Draw keypoints\n            if self.with_keypoints and keypoints:\n                img = self.draw_keypoints(img, keypoints, colors)\n\n            # Draw scores of instances (if available)\n            if scores:\n                img = self.draw_"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create an empty image for drawing\n        img = np.zeros_like(self.output.get_image())\n\n        # Create a color map for visualizing instance segmentation masks\n        colormap = self.create_color_map()\n\n        # Draw instance masks\n        if self.instance_mode == \"pred_masks\":\n            masks = predictions.pred_masks\n            masks = tf.squeeze(masks)\n            masks = np.transpose(masks, (1, 2, 0))\n            masks = np.argmax(masks, axis=-1)\n            masks = colormap[masks]\n\n            # Create a mask image\n            mask_image = np.zeros_like(img)\n            mask_image = cv2.addWeighted(mask_image, 0.5, masks, 0.5, 1)\n\n            # Add the mask image to the output image\n            img = cv2.addWeighted(img, 1, mask_image, 0.5, 0)\n\n        # Draw instance bounding boxes\n        if self.instance_mode == \"pred_boxes\":\n            boxes = predictions.pred_boxes\n            boxes = boxes.tensor.numpy()\n            boxes = boxes.astype(np.int32)\n            img = self.draw_boxes(img, boxes)\n\n        # Draw instance keypoints\n        if self.instance_mode == \"pred_keypoints\":\n            keypoints = predictions.pred_keypoints\n            keypoints = keypoints.tensor.numpy()\n            keypoints = keypoints.astype(np.int32)\n            img = self.draw_keypoints(img, keypoints)\n\n        # Draw instance scores\n        if self.instance_mode == \"pred_scores\":\n            scores = predictions.scores\n            scores = scores.numpy()\n            scores = scores.astype(np.int32)\n            img = self.draw_scores("}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create a copy of the original image and prepare it to draw on\n        img = self.output.get_image()\n        img = img.copy()\n\n        # Convert color space to BGR\n        if img.shape[2] == 4:\n            img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGR)\n        else:\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n\n        # If there are no predictions to draw, return the image as is\n        if len(predictions) == 0:\n            return VisImage(img)\n\n        # If there are instances, draw them\n        if self.instance_mode == InstanceMode.PANOPtic:\n            panoptic_seg, segments_info = predictions\n            vis_segments(img, panoptic_seg.segments_info)\n        elif self.instance_mode == InstanceMode.Semantic:\n            if \"pred_masks\" in predictions:\n                masks = np.asarray(predictions[\"pred_masks\"])\n                masks = masks > 0.5\n                contours = get_bounding_boxes(masks)\n                for i, c in enumerate(contours):\n                    color = self._get_color(i)\n                    cv2.drawContours(img, [c], -1, color, 1, cv2.LINE_AA)\n            else:\n                raise ValueError(\"No masks found in predictions.\")\n        elif self.instance_mode == InstanceMode.Polygon:\n            if \"pred_masks_rle\" in predictions:\n                masks = predictions[\"pred_masks_rle\"]\n                # Use model_type to choose between test and RLE mask formats\n                if isinstance(masks, list) and isinstance(masks[0], torch.Tensor):\n                    # RLEs to masks\n                    masks = [\n                        mask_util.decode(obj) for obj in masks\n                    ]\n                    masks = torch.stack(masks).to(self.output"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        image = self.canvas.get_image()\n        image = image[:, :, :3]\n        return image"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        image = self.canvas.get_image()\n        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n        return image"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the image from RGBA to RGB format\n        image = self.canvas.get_image()\n        image = image.convert(\"RGB\")\n        image = np.array(image)\n        return image"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        return np.array(self.canvas.convert('RGB'))"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        img = self.canvas.get_image()\n        img = img[:, :, :3]\n        return img"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the canvas from the VisImage instance\n        canvas = self.canvas\n\n        # Convert the canvas from RGBA to RGB format\n        image = canvas.get_image()\n\n        # Convert the image to uint8 type\n        image = image.astype(np.uint8)\n\n        return image"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the canvas from the VisImage instance\n        canvas = self.canvas\n\n        # Convert the canvas from RGBA to RGB format\n        image = canvas.get_image()\n\n        # Convert the image from RGBA to RGB format\n        image = image[:, :, :3]\n\n        # Convert the image to uint8 type\n        image = image.astype(np.uint8)\n\n        return image"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        img = self.canvas.get_image()\n        img = img[:, :, :3]\n        return img\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        img = self.canvas.get_image()\n        if img.shape[2] == 4:\n            img = img[:, :, :3]\n        return img\n\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        image = self.canvas.get_image()\n        image = image.convert(\"RGB\")\n        image = np.array(image)\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        return np.array(self.canvas.get_image())\n\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the image to RGB format\n        image = self.canvas.get_image()\n        image = image[:, :, :3]\n        image = image.astype(np.uint8)\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the canvas from the VisImage instance\n        canvas = self.canvas\n\n        # Convert the canvas from RGBA to RGB format\n        image = Image.fromarray(canvas.get_image_array(), mode='RGBA')\n        image = image.convert('RGB')\n\n        # Convert the image to a numpy ndarray of uint8 type and return it\n        return np.array(image, dtype=np.uint8)"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        image = self.canvas.get_image()\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        self.canvas.save(\"temp.png\", format=\"png\")\n        image = Image.open(\"temp.png\")\n        image = np.array(image)\n        image = image[:, :, :3]\n        os.remove(\"temp.png\")\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        return cv2.cvtColor(self.canvas, cv2.COLOR_RGBA2RGB)\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the canvas from the VisImage instance\n        canvas = self.canvas\n        # Get the image from the canvas\n        image = canvas.get_image()\n        # Convert the image from RGBA to RGB format\n        image = image[:, :, :3]\n        # Convert the image to uint8 type\n        image = image.astype(np.uint8)\n        # Return the image\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        return cv2.cvtColor(self.canvas.get_image(), cv2.COLOR_RGBA2RGB)\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        return cv2.cvtColor(self.canvas.get_image(), cv2.COLOR_RGBA2RGB)\n\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the visualized image from the canvas\n        image = self.canvas.get_image()\n        # Convert the image from RGBA to RGB format\n        image = image[:, :, :3]\n        # Convert the image to a numpy ndarray of uint8 type\n        image = np.array(image, dtype=np.uint8)\n        # Return the image\n        return image\n\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            for ann in dic[\"annotations\"]:\n                self.draw_annotation(ann)\n        if \"sem_seg_file_name\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg_file_name\"])\n        if \"panoptic_seg_file_name\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg_file_name\"])\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if 'annotations' in dic:\n            for ann in dic['annotations']:\n                self.draw_annotation(ann)\n        if 'sem_seg_file_name' in dic:\n            self.draw_sem_seg(dic['sem_seg_file_name'])\n        if 'panoptic_seg_file_name' in dic:\n            self.draw_panoptic_seg(dic['panoptic_seg_file_name'])\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"annotations\"])\n        if \"sem_seg_file_name\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg_file_name\"])\n        if \"panoptic_seg_file_name\" in dic:\n            self.draw_panoptic(dic[\"panoptic_seg_file_name\"])\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            for obj in dic[\"annotations\"]:\n                self.draw_instance_predictions(obj)\n\n        if \"sem_seg_file_name\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg_file_name\"])\n\n        if \"panoptic_seg_file_name\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg_file_name\"])\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"annotations\"])\n        if \"sem_seg_file_name\" in dic:\n            sem_seg = cv2.imread(dic[\"sem_seg_file_name\"], cv2.IMREAD_UNCHANGED)\n            sem_seg = self.convert_sem_seg(sem_seg)\n            self.draw_sem_seg(sem_seg)\n        if \"panoptic_seg_file_name\" in dic:\n            panoptic_seg, panoptic_seg_colors = cv2.imread(\n                dic[\"panoptic_seg_file_name\"], cv2.IMREAD_UNCHANGED\n            ).split(2, 2)\n            panoptic_seg = self.convert_panoptic_seg(panoptic_seg)\n            panoptic_seg_colors = self.convert_panoptic_seg_colors(panoptic_seg_colors)\n            self.draw_panoptic_seg(panoptic_seg, panoptic_seg_colors)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw keypoints\n        if 'keypoints' in dic:\n            self.draw_keypoints(dic['keypoints'])\n\n        # Draw bounding boxes\n        if 'instances' in dic:\n            self.draw_instance_predictions(dic['instances'])\n\n        # Draw segmentation masks\n        if 'sem_seg' in dic:\n            self.draw_sem_seg(dic['sem_seg'])\n\n        # Draw panoptic segmentation\n        if 'panoptic_seg' in dic:\n            self.draw_panoptic_seg(dic['panoptic_seg'])\n\n        return self.output\n\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # draw semantic segmentation\n        if \"sem_seg_file_name\" in dic:\n            sem_seg = np.asarray(Image.open(dic[\"sem_seg_file_name\"]))\n            sem_seg = sem_seg.astype(np.uint8)\n            sem_seg = self.draw_sem_seg(sem_seg, dic.get(\"panoptic_seg_file_name\", None))\n            self.output.set_image(sem_seg)\n\n        # draw instance segmentation\n        if \"instances\" in dic:\n            instances = dic[\"instances\"].to(\"cpu\")\n            self.draw_instance_predictions(predictions=instances)\n\n        # draw panoptic segmentation\n        if \"panoptic_seg_file_name\" in dic:\n            panoptic_seg, segments_info = load_panoptic_gt(dic[\"file_name\"], dic[\"panoptic_seg_file_name\"])\n            panoptic_seg = panoptic_seg.astype(np.uint8)\n            panoptic_seg = self.draw_panoptic(panoptic_seg, segments_info)\n            self.output.set_image(panoptic_seg)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if 'annotations' in dic:\n            self.draw_instance_predictions(dic['annotations'])\n        if 'sem_seg_file_name' in dic:\n            self.draw_sem_seg(dic['sem_seg_file_name'])\n        if 'panoptic_seg_file_name' in dic:\n            self.draw_panoptic_seg(dic['panoptic_seg_file_name'])\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw segmentation masks\n        if \"segmentation_masks\" in dic:\n            masks = dic.get(\"segmentation_masks\")\n            if masks.any():\n                masks = masks.to(self.output.device)\n                masks = masks.cpu()\n                if self.output.format == \"BGR\":\n                    masks = masks.flip(dims=(0,))\n                self.output.show(masks.to(torch.uint8))\n\n        # Draw keypoints\n        if \"keypoints\" in dic:\n            kpts = dic[\"keypoints\"]\n            if kpts.shape[0] > 0:\n                kpts = kpts.cpu()\n                self.output.show_keypoints(kpts)\n\n        # Draw bounding boxes\n        if \"instances\" in dic:\n            instances = dic[\"instances\"]\n            if instances.has(\"pred_boxes\"):\n                boxes = instances.pred_boxes.tensor\n                if boxes.shape[0] > 0:\n                    boxes = boxes.cpu()\n                    self.output.show_boxes(boxes)\n\n        # Draw semantic segmentation\n        if \"sem_seg\" in dic:\n            sem_seg = dic[\"sem_seg\"]\n            if sem_seg.any():\n                sem_seg = sem_seg.to(self.output.device)\n                sem_seg = sem_seg.cpu()\n                if self.output.format == \"BGR\":\n                    sem_seg = sem_seg.flip(dims=(0,))\n                self.output.show(sem_seg.to(torch.uint8))\n\n        # Draw panoptic segmentation\n        if \"panoptic_seg\" in dic:\n            panoptic_seg = dic[\"panoptic_seg\"]\n            if panoptic_seg.any():\n                panoptic_seg = panoptic_seg.to(self.output.device)\n                panoptic_seg = panoptic_seg.cpu()\n                if self.output."}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            for obj in dic[\"annotations\"]:\n                self.draw_instance_predictions(obj)\n        if \"sem_seg_file_name\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg_file_name\"])\n        if \"panoptic_seg_file_name\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg_file_name\"])\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        img = self.output.get_image()\n        if \"annotations\" in dic:\n            for anno in dic[\"annotations\"]:\n                if not self.metadata.get(\"thing_colors\", None):\n                    idx = anno[\"category_id\"]\n                    idx = self._remap_id(idx)\n                    color = self.metadata.thing_colors[idx]\n                else:\n                    color = anno[\"category_id\"]\n                self.draw_instance_predictions(anno, color)\n        if \"sem_seg_file_name\" in dic:\n            sem_seg = Image.open(dic[\"sem_seg_file_name\"])\n            sem_seg = np.asarray(sem_seg, dtype=\"uint8\")\n            sem_seg = self.metadata.thing_dataset_id_to_contiguous_id[sem_seg]\n            self.draw_sem_seg(sem_seg)\n        if \"panoptic_seg_file_name\" in dic:\n            panoptic_seg, panoptic_seg_colors = load_panoptic_gt(\n                dic[\"panoptic_seg_file_name\"]\n            )\n            self.draw_panoptic(panoptic_seg, panoptic_seg_colors)\n        if \"file_name\" in dic:\n            self.output.set_image(img)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"instances\" in dic:\n            instances = dic[\"instances\"].to(\"cpu\")\n            if instances.has(\"pred_boxes\"):\n                boxes = instances.pred_boxes\n                self.draw_boxes(boxes)\n            if instances.has(\"pred_classes\"):\n                classes = instances.pred_classes\n                self.draw_instance_predictions(classes)\n            if instances.has(\"pred_masks\"):\n                masks = instances.pred_masks\n                self.draw_instance_predictions(masks)\n        if \"sem_seg\" in dic:\n            sem_seg = dic[\"sem_seg\"].argmax(dim=0).to(\"cpu\")\n            self.draw_sem_seg(sem_seg)\n        if \"panoptic_seg\" in dic:\n            panoptic_seg, segments_info = dic[\"panoptic_seg\"].to(\"cpu\")\n            self.draw_panoptic(panoptic_seg.to(\"cpu\"), segments_info)\n        if \"keypoints\" in dic:\n            keypoints = dic[\"keypoints\"]\n            self.draw_keypoints(keypoints)\n        return self.output\n    "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"instances\"].to(\"cpu\"))\n        if \"sem_seg_file_name\" in dic:\n            self.draw_sem_seg(dic.pop(\"sem_seg_file_name\"), dic.pop(\"panoptic_seg_file_name\"), area_threshold=0)\n        if \"keypoints\" in dic:\n            self.draw_keypoints(dic[\"keypoints\"], dic[\"instances\"].to(\"cpu\"))\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw the segmentation masks\n        if \"segmentation_masks\" in dic:\n            self.draw_segmentation_masks(dic[\"segmentation_masks\"])\n\n        # Draw the keypoints\n        if \"keypoints\" in dic:\n            self.draw_keypoints(dic[\"keypoints\"])\n\n        # Draw the bounding boxes\n        if \"bbox\" in dic:\n            self.draw_bbox(dic[\"bbox\"])\n\n        # Draw the semantic segmentation\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"])\n\n        # Draw the panoptic segmentation\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"])\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        img = self.output.get_image()\n        annos = dic.get(\"annotations\", [])\n        if len(annos) > 0:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n                if len(masks) > 0:\n                    if isinstance(masks[0], list):\n                        masks = unflatten_segmentations(masks)\n                    if isinstance(masks[0], dict):\n                        masks = convert_format(masks, \"mask\", \"bitmask\")\n                    masks = [x.to(self.output.device) for x in masks]\n                    if len(masks) > 0:\n                        if isinstance(masks[0], torch.Tensor):\n                            masks = [x.squeeze(0) for x in masks]\n                        if isinstance(masks[0], np.ndarray):\n                            masks = [x.squeeze() for x in masks]\n                        self.draw_instance_masks(masks)\n        if \"sem_seg_file_name\" in dic:\n            sem_seg_file_name = dic[\"sem_seg_file_name\"]\n            sem_seg = Image.open(sem_seg_file_name)\n            sem_seg = sem_seg.resize(img.size, Image.NEAREST)\n            self.draw_sem_seg(sem_seg)\n        if \"pan_seg_file_name\" in dic:\n            pan_seg_file_name = dic[\"pan_seg_file_name\"]\n            pan_seg = Image.open(pan_seg_file_name)\n            pan_seg = pan_seg.resize(img.size, Image.NEAREST)\n            self.draw_panoptic(pan_seg)\n        if \"keypoints\" in dic:\n            kpts = dic[\"keypoints\"]\n            if len(kpts) > 0:\n                kpts = [x.to(self.output.device) for x in kpts]\n               "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # draw semantic segmentation\n        if \"sem_seg_file_name\" in dic:\n            sem_seg = np.asarray(Image.open(dic[\"sem_seg_file_name\"]))\n            sem_seg = sem_seg.astype(np.int32)\n            sem_seg = self.dataset_vis_transform(sem_seg)\n            sem_seg = sem_seg.argmax(dim=0).to(torch.uint8)\n            self.output.draw_sem_seg(sem_seg.cpu(), area_threshold=0)\n\n        # draw panoptic segmentation\n        if \"panoptic_seg_file_name\" in dic:\n            panoptic_seg, _ = load_panoptic_gt(\n                dic[\"file_name\"], dic[\"panoptic_seg_file_name\"]\n            )\n            panoptic_seg = panoptic_seg.astype(np.int32)\n            panoptic_seg = self.dataset_vis_transform(panoptic_seg)\n            panoptic_seg = panoptic_seg.argmax(dim=0).to(torch.uint8)\n            self.output.draw_panoptic(panoptic_seg.cpu(), dic[\"panoptic_category_names\"])\n\n        # draw bounding boxes\n        if \"annotations\" in dic:\n            # use instance masks if available (otherwise bounding boxes)\n            if \"segmentation_masks\" in dic:\n                masks = dic.get(\"segmentation_masks\", None)\n                masks = masks.to(self.output.output.device)\n                masks = masks.permute(0, 2, 3, 1).to(torch.uint8)\n                masks = masks[:, :, :, 0]\n                self.output.draw_instance_predictions(dic[\"instances\"].to(\"cpu\"))\n            else:\n                self.output.draw_instance_predictions(dic[\"instances\"].to(\"cpu\"))\n\n        # draw keypoints\n        if \"instances"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        img = self.output.get_image()\n        if \"annotations\" in dic:\n            for anno in dic[\"annotations\"]:\n                if not anno.get(\"iscrowd\", 0):\n                    self.draw_instance_coco(anno, keypoints=anno.get(\"keypoints\", None))\n        if \"sem_seg_file_name\" in dic:\n            sem_seg = Image.open(dic[\"sem_seg_file_name\"])\n            sem_seg = sem_seg.resize(img.size, Image.NEAREST)\n            self.overlay_sem_seg(sem_seg)\n        if \"panoptic_seg_file_name\" in dic:\n            panoptic_seg, segments_info = load_panoptic_gt(dic[\"panoptic_seg_file_name\"])\n            panoptic_seg = panoptic_seg.resize(img.size, Image.NEAREST)\n            segments_info = [\n                x for x in segments_info if x[\"iscrowd\"] == 0\n            ]  # filter crowd instances\n            self.overlay_panoptic_seg(panoptic_seg, segments_info)\n        return self.output\n\n\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw semantic segmentation\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"], dic.get(\"sem_seg_file_name\", None))\n        # Draw keypoints\n        if \"keypoints\" in dic:\n            self.draw_keypoints(dic[\"keypoints\"], dic.get(\"keypoint_colors\", None))\n        # Draw bounding boxes\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic[\"instances\"].to(\"cpu\"))\n        # Draw panoptic segmentation\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic(dic[\"panoptic_seg\"], dic.get(\"panoptic_seg_file_name\", None))\n        # Draw segmentation masks\n        if \"segmentation\" in dic:\n            self.draw_segmentation(dic[\"segmentation\"])\n        return self.output\n\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"instances\"].to(\"cpu\"))\n        if \"sem_seg_file_name\" in dic:\n            sem_seg = cv2.imread(dic.sem_seg_file_name, cv2.IMREAD_UNCHANGED)\n            if sem_seg.ndim == 3:\n                sem_seg = sem_seg[:, :, 0]\n            self.draw_sem_seg(sem_seg, dic.get(\"sem_seg_metadata\", None))\n        if \"panoptic_seg_file_name\" in dic:\n            panoptic_seg, segments_info = load_panoptic_gt(\n                dic[\"panoptic_seg_file_name\"]\n            )\n            self.draw_panoptic(panoptic_seg.squeeze(0).cpu(), segments_info)\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        img = self.get_image()\n        if \"annotations\" in dic:\n            for anno in dic[\"annotations\"]:\n                if \"segmentation\" in anno:\n                    if isinstance(anno[\"segmentation\"], list):\n                        for seg in anno[\"segmentation\"]:\n                            if len(seg) > 0:\n                                img = self.draw_segmentation(seg, color=\"green\")\n                    elif isinstance(anno[\"segmentation\"], dict):\n                        for seg in anno[\"segmentation\"].values():\n                            if len(seg) > 0:\n                                img = self.draw_segmentation(seg, color=\"green\")\n        if \"sem_seg_file_name\" in dic:\n            sem_seg = self.load_sem_seg(dic[\"sem_seg_file_name\"])\n            if len(sem_seg.shape) == 3:\n                img = self.draw_sem_seg(sem_seg)\n        if \"panoptic_seg_file_name\" in dic:\n            panoptic_seg, segments_info = self.load_panoptic_seg(\n                dic[\"panoptic_seg_file_name\"]\n            )\n            if len(panoptic_seg.shape) == 3:\n                img = self.draw_panoptic(panoptic_seg, segments_info)\n        return img\n\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = self.random_color()\n\n        if edge_color is None:\n            edge_color = color\n\n        # Convert the binary mask to a polygon mask\n        polygons = self.binary_mask_to_polygons(binary_mask)\n\n        # Draw the polygons on the image\n        self.draw_polygons(polygons, color=color, edge_color=edge_color, text=text, alpha=alpha, area_threshold=area_threshold)\n\n        return self"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = self._get_random_color()\n\n        if edge_color is None:\n            edge_color = color\n\n        binary_mask = binary_mask.astype(np.uint8)\n        contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n        for contour in contours:\n            area = cv2.contourArea(contour)\n            if area > area_threshold:\n                self.draw_polygon(contour.reshape(-1, 2), color=color, edge_color=edge_color, alpha=alpha)\n\n        if text is not None:\n            self.draw_text(text, binary_mask.shape[1] // 2, binary_mask.shape[0] // 2, color=color)\n\n        return self"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = np.random.rand(3)\n        if edge_color is None:\n            edge_color = color\n\n        # Convert the mask to a polygon mask\n        polygon_mask = mask_utils.binary_mask_to_polygon_mask(binary_mask)\n\n        # Draw the polygon mask on the image\n        image = self.draw_polygon_mask(polygon_mask, color=color, edge_color=edge_color, text=text, alpha=alpha, area_threshold=area_threshold)\n\n        return image"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # check if the input is a numpy array of shape (H, W)\n        if not isinstance(binary_mask, np.ndarray) or binary_mask.ndim != 2 or binary_mask.shape[0] == 0 or binary_mask.shape[1] == 0:\n            raise ValueError(\"binary_mask must be a numpy array of shape (H, W)\")\n\n        # check if the input is a numpy array of shape (H, W) and of type uint8\n        if not isinstance(binary_mask, np.ndarray) or binary_mask.ndim != 2 or binary_mask.shape[0] == 0 or binary_mask.shape[1] == 0 or binary_mask.dtype != np.uint8:\n            raise ValueError(\"binary_mask must be a numpy array of shape (H, W) and of type uint8\")\n\n        # check if the input is a numpy array of shape (H, W) and of type uint8\n        if not isinstance(binary_mask, np.ndarray) or binary_mask.ndim != 2 or binary_mask.shape[0] == 0 or binary_mask.shape[1] == 0 or binary_mask.dtype != np.uint8:\n            raise ValueError(\"binary_mask must be a numpy array of shape (H, W) and of type uint8\")\n\n        # check if the input is a numpy array of shape (H, W) and of type uint8\n        if not isinstance(binary_mask, np.ndarray) or binary_mask.ndim != 2 or binary_mask.shape[0] == 0 or binary_mask.shape[1] == 0 or binary_mask.dtype != np.uint8:\n            raise ValueError(\"binary_mask must be a numpy array of shape (H, W) and of type uint8\")\n\n        # check if the input is a numpy array of shape (H, W) and of type uint8\n        if not isinstance(binary_mask, np.ndarray) or binary_mask.ndim != 2 or binary_mask.shape[0] == 0 or binary"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if edge_color is None:\n            edge_color = color\n\n        if color is None:\n            color = self.get_random_color()\n\n        if text is not None:\n            text_color = color\n            text_color = self.get_text_color(text_color)\n\n        if self.mode == \"rgb\":\n            binary_mask = binary_mask.astype(np.uint8)\n\n        if self.mode == \"gray\":\n            binary_mask = binary_mask.astype(np.bool_)\n\n        if self.mode == \"rgb\":\n            binary_mask = binary_mask.astype(np.uint8)\n\n        if self.mode == \"gray\":\n            binary_mask = binary_mask.astype(np.bool_)\n\n        if self.mode == \"rgb\":\n            binary_mask = binary_mask.astype(np.uint8)\n\n        if self.mode == \"gray\":\n            binary_mask = binary_mask.astype(np.bool_)\n\n        if self.mode == \"rgb\":\n            binary_mask = binary_mask.astype(np.uint8)\n\n        if self.mode == \"gray\":\n            binary_mask = binary_mask.astype(np.bool_)\n\n        if self.mode == \"rgb\":\n            binary_mask = binary_mask.astype(np.uint8)\n\n        if self.mode == \"gray\":\n            binary_mask = binary_mask.astype(np.bool_)\n\n        if self.mode == \"rgb\":\n            binary_mask = binary_mask.astype(np.uint8)\n\n        if self.mode == \"gray\":\n            binary_mask = binary_mask.astype(np.bool_)\n\n        if self.mode == \"rgb\":\n            binary_mask = binary_mask.astype(np.uint8)\n\n        if self.mode == \"gray\":\n            binary_mask = binary_mask.astype(np.bool_)\n\n        if self.mode == \"rgb\":\n            binary_mask = binary_mask.ast"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # Check if the mask is a binary mask\n        if binary_mask.ndim != 2 or binary_mask.dtype != np.uint8:\n            raise ValueError(\"Input binary mask must be a 2D numpy array of dtype uint8\")\n\n        # Check if the mask is a binary mask with holes\n        if np.all(binary_mask == 0) or np.all(binary_mask == 1):\n            raise ValueError(\"Input binary mask must have at least one non-zero value\")\n\n        # Check if the mask is a binary mask with holes\n        if np.all(binary_mask == 0) or np.all(binary_mask == 1):\n            raise ValueError(\"Input binary mask must have at least one non-zero value\")\n\n        # Check if the mask is a binary mask with holes\n        if np.all(binary_mask == 0) or np.all(binary_mask == 1):\n            raise ValueError(\"Input binary mask must have at least one non-zero value\")\n\n        # Check if the mask is a binary mask with holes\n        if np.all(binary_mask == 0) or np.all(binary_mask == 1):\n            raise ValueError(\"Input binary mask must have at least one non-zero value\")\n\n        # Check if the mask is a binary mask with holes\n        if np.all(binary_mask == 0) or np.all(binary_mask == 1):\n            raise ValueError(\"Input binary mask must have at least one non-zero value\")\n\n        # Check if the mask is a binary mask with holes\n        if np.all(binary_mask == 0) or np.all(binary_mask == 1):\n            raise ValueError(\"Input binary mask must have at least one non-zero value\")\n\n        # Check if the mask is a binary mask with holes\n        if np.all(binary_mask == 0) or np.all(binary_mask == 1):\n            raise ValueError(\"Input binary mask must have at least one non-zero value\")\n\n        # Check if the mask is a binary mask with holes\n        if np.all(binary_mask == 0) or np.all(binary_mask == 1):"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # Create a copy of the image\n        img = self.img.copy()\n\n        # Convert the binary mask to a polygon mask\n        polygons = self._binary_mask_to_polygons(binary_mask)\n\n        # Draw the polygon mask on the image\n        img = self._draw_polygons(img, polygons, color=color, edge_color=edge_color, alpha=alpha)\n\n        # Draw the text on the image\n        if text is not None:\n            img = self._draw_text(img, text, color=color)\n\n        # Create a VisImage object from the image\n        vis_img = VisImage(img)\n\n        # Return the VisImage object\n        return vis_img"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # Convert the binary mask to a polygon mask\n        polygon_mask = binary_mask_to_polygon_mask(binary_mask)\n\n        # Draw the polygon mask\n        self.draw_polygon_mask(polygon_mask, color=color, edge_color=edge_color, text=text, alpha=alpha, area_threshold=area_threshold)\n\n        # Return the image object\n        return self\n\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random.choice(self.colors)\n\n        if edge_color is None:\n            edge_color = color\n\n        # Create a copy of the image to draw on\n        img_copy = self.img.copy()\n\n        # Convert the binary mask to a mask of polygons\n        polygons = binary_mask_to_polygons(binary_mask)\n\n        # Draw the polygons on the image\n        for polygon in polygons:\n            polygon = polygon.reshape((-1, 1, 2))\n            cv2.fillPoly(img_copy, [polygon], color)\n            cv2.polylines(img_copy, [polygon], True, edge_color, 2)\n\n        # Draw the text on the image\n        if text is not None:\n            cv2.putText(img_copy, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n\n        # Create a VisImage object and return it\n        return VisImage(img_copy, self.scale, self.output_format)"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = self.get_random_color()\n\n        if edge_color is None:\n            edge_color = color\n\n        if text is not None:\n            text_color = self.get_text_color(color)\n\n        if self.mode == \"rgb\":\n            binary_mask = binary_mask.astype(np.uint8)\n\n        contours, hierarchy = cv2.findContours(binary_mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n\n        for contour, hierarchy_item in zip(contours, hierarchy[0]):\n            if hierarchy_item[3] == -1 and cv2.contourArea(contour) > area_threshold:\n                cv2.drawContours(self.output_image, [contour], -1, color, -1)\n\n        if edge_color is not None:\n            for contour, hierarchy_item in zip(contours, hierarchy[0]):\n                if hierarchy_item[3] == -1 and cv2.contourArea(contour) > area_threshold:\n                    cv2.drawContours(self.output_image, [contour], -1, edge_color, 1)\n\n        if text is not None:\n            for contour, hierarchy_item in zip(contours, hierarchy[0]):\n                if hierarchy_item[3] == -1 and cv2.contourArea(contour) > area_threshold:\n                    x, y, w, h = cv2.boundingRect(contour)\n                    cv2.putText(\n                        self.output_image,\n                        text,\n                        (x, y + h // 2),\n                        cv2.FONT_HERSHEY_SIMPLEX,\n                        0.5,\n                        text_color,\n                        1,\n                    )\n\n        return self\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = np.random.randint(0, 255, size=(3,))\n\n        if edge_color is None:\n            edge_color = color\n\n        # Convert the mask to a mask with holes\n        binary_mask_with_holes = self._convert_binary_mask_to_mask_with_holes(binary_mask)\n\n        # Draw the mask with holes\n        self.draw_polygon(\n            binary_mask_with_holes,\n            color=color,\n            edge_color=edge_color,\n            text=text,\n            alpha=alpha,\n            area_threshold=area_threshold,\n        )\n\n        return self\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # get a random color if no color is specified\n        if color is None:\n            color = self.get_random_color()\n\n        # get a random color if no edge color is specified\n        if edge_color is None:\n            edge_color = self.get_random_color()\n\n        # get a random color if no text color is specified\n        if text is not None:\n            text_color = self.get_random_color()\n\n        # get the image to draw on\n        img = self.output.get_image()\n\n        # convert the binary mask to a polygon\n        polygon = mask_utils.binary_mask_to_polygon(binary_mask)\n\n        # draw the polygon on the image\n        img = cv2.fillPoly(img, [polygon], color=color)\n\n        # draw the polygon edges on the image\n        img = cv2.polylines(img, [polygon], True, edge_color, thickness=2)\n\n        # draw the text on the image\n        if text is not None:\n            cv2.putText(img, text, (polygon[0][0][0], polygon[0][0][1]), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 2)\n\n        # create a new VisImage object with the drawn mask\n        vis_img = VisImage(img, scale=self.output.scale)\n\n        return vis_img"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if self.output is None:\n            raise RuntimeError(\"No image is currently displayed. Call show() to show an image first.\")\n        if self.mode != \"image\":\n            raise ValueError(\"Expected mode 'image', but got '{}'\".format(self.mode))\n        if color is None:\n            color = self.get_color_palette()[0]\n        if edge_color is None:\n            edge_color = color\n\n        # Convert the binary mask to a polygon mask\n        polygons = binary_mask_to_polygons(binary_mask)\n\n        # Draw the polygon mask on the image\n        self.output.draw_polygons(polygons, color=color, edge_color=edge_color, alpha=alpha)\n\n        # Draw text on the mask if specified\n        if text is not None:\n            self.output.draw_text(text, polygons[0], color=color)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        from detectron2.utils.visualizer import ColorMode\n\n        if color is None:\n            color = self.get_random_color()\n\n        if edge_color is None:\n            edge_color = color\n\n        binary_mask = binary_mask.astype(np.uint8)\n\n        if self.output.mode == ColorMode.RGB:\n            binary_mask = binary_mask[..., np.newaxis]\n\n        if self.output.mode == ColorMode.BGR:\n            binary_mask = binary_mask[..., np.newaxis]\n\n        binary_mask = np.repeat(binary_mask, 3, axis=-1)\n\n        if self.output.mode == ColorMode.RGB:\n            binary_mask = binary_mask[..., ::-1]\n\n        if self.output.mode == ColorMode.BGR:\n            binary_mask = binary_mask[..., ::-1]\n\n        binary_mask = binary_mask.astype(np.float32)\n\n        binary_mask = cv2.cvtColor(binary_mask, cv2.COLOR_BGR2RGB)\n\n        binary_mask = (binary_mask * alpha).astype(np.uint8)\n\n        binary_mask = self.overlay_mask(binary_mask, color=color, alpha=alpha)\n\n        binary_mask = cv2.cvtColor(binary_mask, cv2.COLOR_RGB2BGR)\n\n        binary_mask = binary_mask.astype(np.uint8)\n\n        binary_mask = binary_mask[..., ::-1]\n\n        binary_mask = binary_mask.astype(np.float32)\n\n        binary_mask = cv2.cvtColor(binary_mask, cv2.COLOR_BGR2RGB)\n\n        binary_mask = binary_mask.astype(np.uint8)\n\n        binary_mask = binary_mask[..., ::-1]\n\n        binary_mask = binary_mask.astype(np.float32)\n\n       "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if not isinstance(binary_mask, np.ndarray):\n            raise TypeError(\"binary_mask must be a numpy array\")\n        if len(binary_mask.shape) != 2:\n            raise ValueError(\"binary_mask must be a 2D array\")\n        if binary_mask.dtype != np.uint8:\n            raise ValueError(\"binary_mask must be of dtype uint8\")\n\n        if color is None:\n            color = self._get_random_color()\n\n        if edge_color is None:\n            edge_color = self._get_random_color()\n\n        if text is None:\n            text = \"\"\n\n        if area_threshold < 0:\n            raise ValueError(\"area_threshold must be a positive integer\")\n\n        # Draw the mask as a polygon\n        polygons = self._get_polygons_from_binary_mask(binary_mask, area_threshold)\n        if len(polygons) == 0:\n            return self\n\n        for polygon in polygons:\n            self.draw_polygon(polygon, color=color, edge_color=edge_color, alpha=alpha)\n\n        # Draw the text on the mask\n        if text != \"\":\n            self.draw_text(text, (0, 0), color=color, alpha=alpha)\n\n        return self"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = self.get_random_color()\n\n        if edge_color is None:\n            edge_color = color\n\n        binary_mask = binary_mask.astype(np.uint8)\n        contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        polygons = [cnt for cnt in contours if cv2.contourArea(cnt) > area_threshold]\n\n        for poly in polygons:\n            poly = poly[:, 0, :]\n            poly = poly.astype(np.int32)\n            poly = np.concatenate([poly, [poly[0]]])\n            self.ax.fill(poly[:, 0], poly[:, 1], color=color, alpha=alpha)\n            self.ax.plot(poly[:, 0], poly[:, 1], color=edge_color, linewidth=2)\n\n        if text is not None:\n            text_x = np.average([poly[:, 0].min(), poly[:, 0].max()])\n            text_y = np.average([poly[:, 1].min(), poly[:, 1].max()])\n            self.ax.text(\n                text_x,\n                text_y,\n                text,\n                fontsize=10,\n                fontweight=\"bold\",\n                color=\"white\",\n                bbox=dict(facecolor=\"black\", alpha=0.5),\n            )\n\n        return self"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random.choice(self.colors)\n\n        if edge_color is None:\n            edge_color = color\n\n        if text is not None:\n            text_color = (0, 0, 0)\n            text_size = 16\n            text_thickness = 2\n            text_margin = 10\n\n        # Create a copy of the image to draw on\n        image = self.output.copy()\n\n        # Convert the binary mask to a polygon mask\n        polygons = binary_mask_to_polygons(binary_mask)\n\n        # Draw the polygons on the image\n        for polygon in polygons:\n            # Filter out small connected components\n            if cv2.contourArea(polygon) < area_threshold:\n                continue\n\n            # Draw the polygon\n            cv2.fillPoly(image, [polygon], color=color)\n            cv2.polylines(image, [polygon], True, edge_color, thickness=2)\n\n            # Draw the text on the mask\n            if text is not None:\n                text_position = (\n                    polygon[:, 0].min() - text_margin,\n                    polygon[:, 1].min() - text_margin,\n                )\n                cv2.putText(\n                    image,\n                    text,\n                    text_position,\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    text_size,\n                    text_color,\n                    text_thickness,\n                    cv2.LINE_AA,\n                )\n\n        # Create a VisImage object from the image and return it\n        return VisImage(image, self.scale)"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # Check if the input is a binary mask\n        if binary_mask.dtype != np.uint8:\n            raise ValueError(\"Input mask must be a binary mask (dtype=np.uint8)\")\n\n        # Get the image height and width\n        h, w = binary_mask.shape\n\n        # Create a new image object\n        vis_image = VisImage(self.image)\n\n        # Create a mask with the same size as the original image\n        mask = np.zeros((h, w), dtype=np.uint8)\n\n        # Get the connected components of the binary mask\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask)\n\n        # Iterate over the connected components\n        for label in range(1, num_labels):\n            # Get the bounding box of the connected component\n            x, y, w, h, area = stats[label]\n\n            # Check if the area of the connected component is greater than the threshold\n            if area < area_threshold:\n                continue\n\n            # Draw the bounding box on the mask\n            mask = cv2.rectangle(mask, (x, y), (x + w, y + h), 255, -1)\n\n        # Set the mask on the image object\n        vis_image.set_mask(mask)\n\n        # Draw the mask on the image\n        vis_image.draw_mask(color=color, edge_color=edge_color, alpha=alpha)\n\n        # Draw the text on the mask\n        if text is not None:\n            vis_image.draw_text(text, x=x, y=y, color=\"white\")\n\n        return vis_image"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if not isinstance(binary_mask, np.ndarray):\n            raise ValueError(\n                \"The input binary mask is not a numpy array. Please provide a numpy array of shape (H, W) representing the binary mask, where H is the image height and W is the image width. Each value in the array is either 0 or 1 of uint8 type.\"\n            )\n        if len(binary_mask.shape) != 2:\n            raise ValueError(\n                \"The input binary mask is not a 2D array. Please provide a numpy array of shape (H, W) representing the binary mask, where H is the image height and W is the image width. Each value in the array is either 0 or 1 of uint8 type.\"\n            )\n        if not isinstance(alpha, float):\n            raise ValueError(\n                \"The input alpha is not a float. Please provide a float value for the blending efficiency of the mask.\"\n            )\n        if not isinstance(area_threshold, float):\n            raise ValueError(\n                \"The input area_threshold is not a float. Please provide a float value for the minimum area of connected components to be shown.\"\n            )\n\n        if color is None:\n            color = self.random_color()\n        if edge_color is None:\n            edge_color = color\n\n        # Get the mask contours\n        contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        # Filter out small contours\n        contours = [cnt for cnt in contours if cv2.contourArea(cnt) > area_threshold]\n\n        # Draw the contours on the image\n        for contour in contours:\n            self.draw_polyline(contour.squeeze(1), color=color, edge_color=edge_color, alpha=alpha)\n\n        # Draw the text on the image\n        if text is not None:\n            self.draw_text(text, binary_mask.shape[0] // 2, binary_mask.shape[1"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if not isinstance(binary_mask, np.ndarray):\n            raise ValueError(\"binary_mask must be a numpy array\")\n        if binary_mask.ndim != 2:\n            raise ValueError(\"binary_mask must be a 2D array\")\n        if binary_mask.dtype != np.uint8:\n            raise ValueError(\"binary_mask must be of type uint8\")\n\n        if color is None:\n            color = random.choice(self.colors)\n\n        if edge_color is None:\n            edge_color = color\n\n        mask = binary_mask.copy()\n\n        # Connected components\n        labels, num_labels = ndimage.label(mask)\n        if num_labels > 1:\n            # Draw polygons on the mask\n            polygons = []\n            for label_num in range(1, num_labels + 1):\n                # Get the coordinates of the pixels belonging to the current label\n                y, x = np.where(labels == label_num)\n                # Get the coordinates of the pixels belonging to the current label\n                xy = np.stack([x, y], axis=1)\n                # Get the area of the polygon\n                area = Polygon(xy).area\n                # If the area is smaller than the threshold, skip it\n                if area < area_threshold:\n                    continue\n                # Draw the polygon\n                polygon = Polygon(xy, closed=True, edgecolor=edge_color, facecolor=color, alpha=alpha)\n                polygons.append(polygon)\n\n            # Add the polygons to the image\n            for polygon in polygons:\n                self.ax.add_patch(polygon)\n\n        # Draw the text on the mask\n        if text is not None:\n            self.ax.text(0.5, 0.5, text, ha=\"center\", va=\"center\", transform=self.ax.transAxes)\n\n        return self"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not isinstance(input, Instances):\n        raise ValueError(\"input is not an instance of Instances\")\n    if not isinstance(other, Instances):\n        raise ValueError(\"other is not an instance of Instances\")\n\n    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size), f\"{msg} image_size not equal\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size not equal\"\n\n    for k in input.fields():\n        if k == \"gt_masks\":\n            assert torch.allclose(input.gt_masks, other.gt_masks), f\"{msg} gt_masks not equal\"\n        elif k == \"gt_boxes\":\n            assert torch.allclose(input.gt_boxes, other.gt_boxes), f\"{msg} gt_boxes not equal\"\n        elif k == \"pred_masks\":\n            assert torch.allclose(input.pred_masks, other.pred_masks), f\"{msg} pred_masks not equal\"\n        elif k == \"pred_boxes\":\n            assert torch.allclose(input.pred_boxes, other.pred_boxes), f\"{msg} pred_boxes not equal\"\n        elif k == \"proposal_boxes\":\n            assert torch.allclose(input.proposal_boxes, other.proposal_boxes), f\"{msg} proposal_boxes not equal\"\n        elif k == \"gt_classes\":\n            assert torch.allclose(input.gt_classes, other.gt_classes), f\"{msg} gt_classes not equal\"\n        elif k == \"scores\":\n            assert torch.allclose(input.scores, other.scores), f\"{msg} scores not equal\"\n        elif k == \"gt_keypoints\":\n            assert torch.allclose(input.gt_keypoints, other.gt_keypoints), f\"{msg}"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg} Expected input to be Instances, got {type(input)}\"\n    assert isinstance(other, Instances), f\"{msg} Expected other to be Instances, got {type(other)}\"\n\n    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    for k, v in input.get_fields().items():\n        if k in other.get_fields():\n            if isinstance(v, (Boxes, ROIMasks)):\n                assert v.tensor.shape == other.get_fields()[k].tensor.shape, f\"{msg} {k} shape mismatch: {v.tensor.shape} vs {other.get_fields()[k].tensor.shape}\"\n                assert torch.allclose(v.tensor, other.get_fields()[k].tensor), f\"{msg} {k} mismatch: {v.tensor} vs {other.get_fields()[k].tensor}\"\n            elif isinstance(v, torch.Tensor):\n                assert v.shape == other.get_fields()[k].shape, f\"{msg} {k} shape mismatch: {v.shape} vs {other.get_fields()[k].shape}\"\n                assert torch.allclose(v, other.get_fields()[k], rtol=rtol), f\"{msg} {k} mismatch: {v} vs {other.get_fields()[k]}\"\n            elif isinstance(v, (list, tuple)):\n                assert v == other.get_fields()[k], f\"{msg} {k} mismatch: {v} vs {other.get_fields()[k]}\"\n            else:\n                raise ValueError(f\"{"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg}Expected input to be Instances, got {type(input)}\"\n    assert isinstance(other, Instances), f\"{msg}Expected other to be Instances, got {type(other)}\"\n    assert input.image_size == other.image_size, f\"{msg}Expected image_size to be {input.image_size}, got {other.image_size}\"\n    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size), f\"{msg}Expected image_size to be {input.image_size}, got {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Expected image_size to be {input.image_size}, got {other.image_size}\"\n    for field in input.fields():\n        assert field in other.fields(), f\"{msg}Expected field {field} not in other\"\n        if field == \"pred_boxes\":\n            assert isinstance(input.pred_boxes, Boxes), f\"{msg}Expected pred_boxes to be Boxes, got {type(input.pred_boxes)}\"\n            assert isinstance(other.pred_boxes, Boxes), f\"{msg}Expected pred_boxes to be Boxes, got {type(other.pred_boxes)}\"\n            assert input.pred_boxes.tensor.shape == other.pred_boxes.tensor.shape, f\"{msg}Expected pred_boxes.tensor.shape to be {input.pred_boxes.tensor.shape}, got {other.pred_boxes.tensor.shape}\"\n            assert torch.allclose(input.pred_boxes.tensor, other.pred_boxes.tensor), f\"{msg}Expected pred_boxes.tensor to be {input.pred_boxes.tensor}, got {other.pred_boxes.tensor}\"\n        elif field == \"pred_masks\":\n            assert isinstance(input.pred_masks, ROIMasks), f\"{msg}Expected pred_masks to be RO"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not isinstance(input, Instances):\n        raise ValueError(\"input is not an instance of Instances\")\n    if not isinstance(other, Instances):\n        raise ValueError(\"other is not an instance of Instances\")\n\n    # Check if the image sizes are the same\n    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size), msg + \"Image sizes are not equal\"\n    else:\n        assert input.image_size == other.image_size, msg + \"Image sizes are not equal\"\n\n    # Check if the fields are the same\n    for field in input.fields():\n        if field == \"gt_masks\":\n            assert torch.allclose(input.gt_masks, other.gt_masks), msg + f\"{field} are not equal\"\n        elif field == \"gt_boxes\":\n            assert torch.allclose(input.gt_boxes, other.gt_boxes), msg + f\"{field} are not equal\"\n        elif field == \"proposal_boxes\":\n            assert torch.allclose(input.proposal_boxes, other.proposal_boxes), msg + f\"{field} are not equal\"\n        elif field == \"pred_boxes\":\n            assert torch.allclose(input.pred_boxes, other.pred_boxes), msg + f\"{field} are not equal\"\n        elif field == \"scores\":\n            assert torch.allclose(input.scores, other.scores), msg + f\"{field} are not equal\"\n        elif field == \"pred_classes\":\n            assert torch.allclose(input.pred_classes, other.pred_classes), msg + f\"{field} are not equal\"\n        elif field == \"gt_classes\":\n            assert torch.allclose(input.gt_classes, other.gt_classes), msg + f\"{field} are not equal\"\n        elif field == \"gt_keypoints\":\n            assert torch.allclose(input.gt_keypoints, other.gt_keyp"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if the image sizes are the same\n    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size), f\"{msg}Image sizes do not match\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Image sizes do not match\"\n\n    # Check if the fields are the same or close\n    for field in input.fields():\n        if field in other.fields():\n            if isinstance(input.get(field), torch.Tensor) or isinstance(other.get(field), torch.Tensor):\n                assert torch.allclose(input.get(field), other.get(field), rtol=rtol), f\"{msg}Field {field} does not match\"\n            else:\n                assert input.get(field) == other.get(field), f\"{msg}Field {field} does not match\"\n        else:\n            raise ValueError(f\"{msg}Field {field} not found in other instance\")\n\n    # Check if the fields in other are present in input\n    for field in other.fields():\n        if field not in input.fields():\n            raise ValueError(f\"{msg}Field {field} not found in input instance\")\n\n    # If all checks pass, return without raising an error\n    return"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), \"input must be an Instances\"\n    assert isinstance(other, Instances), \"other must be an Instances\"\n\n    assert input.image_size == other.image_size, f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    for k, v in input.get_fields().items():\n        other_k = other.get_fields().get(k)\n        if other_k is None:\n            raise AssertionError(f\"{msg} other does not have field {k}\")\n        if isinstance(v, (Boxes, ROIMasks, torch.Tensor)):\n            if not torch.allclose(v, other_k, rtol=rtol):\n                raise AssertionError(f\"{msg} field {k} mismatch: {v} vs {other_k}\")\n        elif isinstance(v, dict):\n            assert_instances_allclose(v, other_k, msg=f\"{msg} field {k} \")\n        else:\n            if v != other_k:\n                raise AssertionError(f\"{msg} field {k} mismatch: {v} vs {other_k}\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), \"input must be an Instances\"\n    assert isinstance(other, Instances), \"other must be an Instances\"\n    assert input.image_size == other.image_size, f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    if size_as_tensor:\n        input_image_size = input.image_size\n        other_image_size = other.image_size\n        if not isinstance(input_image_size, torch.Tensor):\n            input_image_size = torch.tensor(input_image_size, dtype=torch.float32)\n        if not isinstance(other_image_size, torch.Tensor):\n            other_image_size = torch.tensor(other_image_size, dtype=torch.float32)\n        assert torch.allclose(input_image_size, other_image_size), f\"{msg} image_size mismatch: {input_image_size} vs {other_image_size}\"\n\n    for k in input.get_fields().keys():\n        if k == \"pred_boxes\":\n            assert_boxes_allclose(input.pred_boxes, other.pred_boxes, rtol=rtol, msg=msg)\n        elif k == \"gt_boxes\":\n            assert_boxes_allclose(input.gt_boxes, other.gt_boxes, rtol=rtol, msg=msg)\n        elif k == \"proposal_boxes\":\n            assert_boxes_allclose(input.proposal_boxes, other.proposal_boxes, rtol=rtol, msg=msg)\n        elif k == \"pred_masks\":\n            assert_roimasks_allclose(input.pred_masks, other.pred_masks, rtol=rtol, msg=msg)\n        elif k == \"gt_masks\":\n            assert_roimasks_allclose(input.gt_masks, other.gt_masks,"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if input.image_size != other.image_size:\n        if size_as_tensor:\n            input_size = input.image_size\n            other_size = other.image_size\n            if not (\n                torch.allclose(input_size[0], other_size[0])\n                and torch.allclose(input_size[1], other_size[1])\n            ):\n                raise AssertionError(\n                    f\"{msg}Expected input.image_size={input_size} but got {other_size}\"\n                )\n        else:\n            raise AssertionError(\n                f\"{msg}Expected input.image_size={input.image_size} but got {other.image_size}\"\n            )\n\n    for k in input.fields():\n        if k == \"gt_masks\":\n            if not torch.equal(input.gt_masks, other.gt_masks):\n                raise AssertionError(f\"{msg}Expected input.gt_masks={input.gt_masks} but got {other.gt_masks}\")\n        elif k == \"gt_boxes\":\n            if not Boxes.all_close(input.gt_boxes, other.gt_boxes, rtol=rtol):\n                raise AssertionError(f\"{msg}Expected input.gt_boxes={input.gt_boxes} but got {other.gt_boxes}\")\n        elif k == \"proposal_boxes\":\n            if not Boxes.all_close(input.proposal_boxes, other.proposal_boxes, rtol=rtol):\n                raise AssertionError(f\"{msg}Expected input.proposal_boxes={input.proposal_boxes} but got {other.proposal_boxes}\")\n        elif k == \"pred_boxes\":\n            if not Boxes.all_close(input.pred_boxes, other.pred_boxes, rtol=rtol):\n                raise AssertionError(f\"{msg}Expected input.pred_boxes="}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if input.image_size != other.image_size:\n        if size_as_tensor:\n            assert torch.allclose(\n                torch.tensor(input.image_size), torch.tensor(other.image_size)\n            ), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n        else:\n            assert (\n                input.image_size == other.image_size\n            ), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    for k, v in input.get_fields().items():\n        if k not in other.get_fields():\n            raise KeyError(f\"{msg} {k} field not found in other\")\n        other_v = other.get_fields()[k]\n        if isinstance(v, torch.Tensor) and isinstance(other_v, torch.Tensor):\n            assert torch.allclose(\n                v, other_v, rtol=rtol\n            ), f\"{msg} field {k} mismatch: {v} vs {other_v}\"\n        elif isinstance(v, (list, tuple)) and isinstance(other_v, (list, tuple)):\n            for v1, v2 in zip(v, other_v):\n                if isinstance(v1, torch.Tensor) and isinstance(v2, torch.Tensor):\n                    assert torch.allclose(\n                        v1, v2, rtol=rtol\n                    ), f\"{msg} field {k} mismatch: {v1} vs {v2}\"\n                else:\n                    assert (\n                        v1 == v2\n                    ), f\"{msg} field {k} mismatch: {v1} vs {v2}\"\n        elif isinstance(v, Boxes) and isinstance(other_v, Boxes):\n            assert v.tensor.shape == other_v.tensor.shape, (\n                f\"{msg} field {k} shape mismatch: \"\n                f\"{v.tensor.shape}"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg} Expected input to be Instances, but found {type(input)}\"\n    assert isinstance(other, Instances), f\"{msg} Expected other to be Instances, but found {type(other)}\"\n\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg} image_size does not match\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size does not match\"\n\n    for k, v in input.get_fields().items():\n        if k not in other.get_fields():\n            raise KeyError(f\"{msg} other does not have field {k}\")\n        other_v = other.get_fields()[k]\n\n        if isinstance(v, (Boxes, ROIMasks)):\n            assert torch.allclose(v.tensor, other_v.tensor), f\"{msg} {k} does not match\"\n        elif isinstance(v, torch.Tensor):\n            assert torch.allclose(v, other_v, rtol=rtol), f\"{msg} {k} does not match\"\n        elif isinstance(v, (tuple, list)):\n            assert v == other_v, f\"{msg} {k} does not match\"\n        else:\n            raise ValueError(f\"{msg} unknown type for {k}: {type(v)}\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if the image sizes are equal\n    if size_as_tensor:\n        assert torch.allclose(\n            input.image_size, other.image_size\n        ), f\"{msg}image_size not equal: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}image_size not equal: {input.image_size} vs {other.image_size}\"\n\n    # Check if the fields are equal\n    for field in input.fields():\n        if field == \"gt_masks\":\n            # Handle gt_masks field separately\n            assert torch.allclose(\n                input.gt_masks, other.gt_masks\n            ), f\"{msg}gt_masks not equal: {input.gt_masks} vs {other.gt_masks}\"\n        else:\n            # Check if the field is a tensor\n            if isinstance(input.get(field), torch.Tensor):\n                # Handle tensors\n                assert torch.allclose(\n                    input.get(field), other.get(field), rtol=rtol\n                ), f\"{msg}{field} not equal: {input.get(field)} vs {other.get(field)}\"\n            else:\n                # Handle other field types\n                assert input.get(field) == other.get(field), f\"{msg}{field} not equal: {input.get(field)} vs {other.get(field)}\"\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg} Expected input to be an Instances object, but found {type(input)}\"\n    assert isinstance(other, Instances), f\"{msg} Expected other to be an Instances object, but found {type(other)}\"\n\n    # Check image sizes\n    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    # Check fields\n    for k in input.fields():\n        if k == \"gt_masks\":\n            continue  # TODO: handle this field\n        elif k == \"gt_boxes\":\n            assert input.gt_boxes.tensor.shape == other.gt_boxes.tensor.shape, f\"{msg} gt_boxes tensor shape mismatch: {input.gt_boxes.tensor.shape} vs {other.gt_boxes.tensor.shape}\"\n            assert torch.allclose(input.gt_boxes.tensor, other.gt_boxes.tensor), f\"{msg} gt_boxes tensor mismatch\"\n        elif k == \"gt_classes\":\n            assert input.gt_classes == other.gt_classes, f\"{msg} gt_classes mismatch: {input.gt_classes} vs {other.gt_classes}\"\n        elif k == \"pred_boxes\":\n            assert input.pred_boxes.tensor.shape == other.pred_boxes.tensor.shape, f\"{msg} pred_boxes tensor shape mismatch: {input.pred_boxes.tensor.shape} vs {other.pred_boxes.tensor.shape}\"\n            assert torch.allclose(input.pred_boxes.tensor, other.pred_boxes.tensor, rtol=rtol), f\"{msg} pred_boxes"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), \"The input must be an instance of Instances.\"\n    assert isinstance(other, Instances), \"The other must be an instance of Instances.\"\n    assert len(input) == len(other), \"The input and other must have the same length.\"\n\n    for i in range(len(input)):\n        for k, v in input[i].items():\n            if k == \"image_size\":\n                if size_as_tensor:\n                    assert torch.allclose(\n                        input[i][k], other[i][k]\n                    ), f\"{msg} The image_size of the {i}th instance does not match.\"\n                else:\n                    assert input[i][k] == other[i][k], f\"{msg} The image_size of the {i}th instance does not match.\"\n            elif k == \"gt_masks\":\n                assert torch.allclose(\n                    input[i][k], other[i][k]\n                ), f\"{msg} The gt_masks of the {i}th instance does not match.\"\n            elif k == \"gt_boxes\":\n                assert torch.allclose(\n                    input[i][k].tensor, other[i][k].tensor\n                ), f\"{msg} The gt_boxes of the {i}th instance does not match.\"\n            elif k == \"proposal_boxes\":\n                assert torch.allclose(\n                    input[i][k].tensor, other[i][k].tensor\n                ), f\"{msg} The proposal_boxes of the {i}th instance does not match.\"\n            elif k == \"pred_boxes\":\n                assert torch.allclose(\n                    input[i][k].tensor, other[i][k].tensor\n                ), f\"{msg} The pred_boxes of the {i}th instance does not match.\"\n            elif k == \"scores\":\n                assert torch.allclose(\n                    input[i][k], other[i][k]\n                ), f\"{msg} The scores of the {i}th instance does not match.\"\n            elif"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if input.image_size != other.image_size:\n        raise AssertionError(f\"{msg}Image sizes do not match: {input.image_size} vs {other.image_size}\")\n\n    for k, v in input.get_fields().items():\n        other_v = other.get_fields()[k]\n        if k == \"gt_masks\":\n            if isinstance(v, ROIMasks):\n                if not isinstance(other_v, ROIMasks):\n                    raise ValueError(f\"{msg}Field {k} is ROIMasks, but other is not\")\n                if v.tensor.shape != other_v.tensor.shape:\n                    raise AssertionError(f\"{msg}Field {k} shape do not match: {v.tensor.shape} vs {other_v.tensor.shape}\")\n                if not torch.allclose(v.tensor, other_v.tensor, rtol=rtol):\n                    raise AssertionError(f\"{msg}Field {k} does not match: {v.tensor} vs {other_v.tensor}\")\n            else:\n                if not isinstance(other_v, Instances):\n                    raise ValueError(f\"{msg}Field {k} is not ROIMasks, but other is\")\n                assert_instances_allclose(v, other_v, rtol=rtol, msg=f\"{msg}{k}: \")\n        elif k == \"gt_boxes\":\n            if isinstance(v, Boxes):\n                if not isinstance(other_v, Boxes):\n                    raise ValueError(f\"{msg}Field {k} is Boxes, but other is not\")\n                if v.tensor.shape != other_v.tensor.shape:\n                    raise AssertionError(f\"{msg}Field {k} shape do not match: {v.tensor.shape} vs {other_v.tensor.shape}\")\n                if not torch.allclose(v.tensor, other_v.tensor, rtol=rtol):\n                    raise AssertionError(f\"{msg}Field {k} does not"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check that the input and other instances have the same image_size\n    if size_as_tensor:\n        assert (\n            torch.allclose(input.image_size, other.image_size)\n        ), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert (\n            input.image_size == other.image_size\n        ), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    # Iterate over all fields in the input instance\n    for field in input.fields():\n        # Check if the field exists in the other instance\n        if field not in other.fields():\n            raise AssertionError(f\"{msg} {field} not found in other instance\")\n\n        # Get the field from both instances\n        input_field = input.get(field)\n        other_field = other.get(field)\n\n        # Check if the field types match\n        if type(input_field) != type(other_field):\n            raise AssertionError(\n                f\"{msg} {field} type mismatch: {type(input_field)} vs {type(other_field)}\"\n            )\n\n        # Check if the field types are Boxes, ROIMasks, or torch.Tensor\n        if isinstance(input_field, Boxes) or isinstance(input_field, ROIMasks):\n            # Check if the fields are equal\n            assert (\n                input_field == other_field\n            ), f\"{msg} {field} mismatch: {input_field} vs {other_field}\"\n        elif isinstance(input_field, torch.Tensor):\n            # Check if the fields are close\n            assert torch.allclose(\n                input_field, other_field, rtol=rtol\n            ), f\"{msg} {field} mismatch: {input_field} vs {other_field}\"\n        else:\n            # Check if the fields are equal\n            assert (\n                input_field == other_field\n            ), f\"{msg} {"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), \"Input must be an instance of Instances.\"\n    assert isinstance(other, Instances), \"Other must be an instance of Instances.\"\n\n    # Check if the image sizes are equal\n    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size), f\"{msg} Image sizes must be equal.\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} Image sizes must be equal.\"\n\n    # Check if the fields are equal\n    for field_name in input.fields():\n        if field_name not in other.fields():\n            raise AssertionError(f\"{msg} Field '{field_name}' is present in input but not in other.\")\n\n        field_input = getattr(input, field_name)\n        field_other = getattr(other, field_name)\n\n        if isinstance(field_input, (Boxes, ROIMasks)):\n            assert field_input == field_other, f\"{msg} Field '{field_name}' must be equal.\"\n        elif isinstance(field_input, torch.Tensor):\n            assert torch.allclose(field_input, field_other, rtol=rtol), f\"{msg} Field '{field_name}' must be close.\"\n        else:\n            assert field_input == field_other, f\"{msg} Field '{field_name}' must be equal.\"\n\n    # Check if the fields in other are present in input\n    for field_name in other.fields():\n        if field_name not in input.fields():\n            raise AssertionError(f\"{msg} Field '{field_name}' is present in other but not in input.\")\n\n    # If all checks pass, return without raising an error\n    return\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check that the two instances have the same image_size\n    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size), f\"{msg} image_size does not match: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size does not match: {input.image_size} vs {other.image_size}\"\n\n    # Iterate over all fields in the instances\n    for k in input.fields():\n        # Check if the field exists in the other instance\n        if k not in other.fields():\n            raise AssertionError(f\"{msg} {k} not found in other\")\n\n        # Get the field values from the two instances\n        input_field = input.get(k)\n        other_field = other.get(k)\n\n        # Check if the field types match\n        if type(input_field) != type(other_field):\n            raise ValueError(f\"{msg} {k} has different types: {type(input_field)} vs {type(other_field)}\")\n\n        # Check if the field is a tensor\n        if isinstance(input_field, torch.Tensor):\n            # Check if the field is a Boxes or ROIMasks field\n            if isinstance(input_field, Boxes) or isinstance(input_field, ROIMasks):\n                # Compare the field values as tensors\n                assert torch.allclose(input_field.tensor, other_field.tensor), f\"{msg} {k} does not match: {input_field.tensor} vs {other_field.tensor}\"\n            else:\n                # Compare the field values as tensors with a custom relative tolerance\n                assert torch.allclose(input_field, other_field, rtol=rtol), f\"{msg} {k} does not match: {input_field} vs {other_field}\"\n        else:\n            # Compare the field values as other types\n            assert input_field == other_field, f\"{msg}"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if input is None:\n        if other is None:\n            return\n        else:\n            raise AssertionError(msg + \"Input is None but other is not.\")\n    if other is None:\n        raise AssertionError(msg + \"Other is None but input is not.\")\n\n    if input.image_size != other.image_size:\n        if size_as_tensor:\n            if not torch.allclose(input.image_size, other.image_size, rtol=rtol):\n                raise AssertionError(\n                    msg + f\"image_size {input.image_size} != {other.image_size}\"\n                )\n        else:\n            if input.image_size != other.image_size:\n                raise AssertionError(\n                    msg + f\"image_size {input.image_size} != {other.image_size}\"\n                )\n\n    for k, v in input.get_fields().items():\n        if k not in other.get_fields():\n            raise KeyError(\n                msg\n                + f\"{k} is in input but not in other. Keys in input: {list(input.get_fields().keys())}\"\n            )\n        other_v = other.get_fields()[k]\n        if isinstance(v, torch.Tensor):\n            if not torch.allclose(v, other_v, rtol=rtol):\n                raise AssertionError(\n                    msg + f\"{k} {v} != {other_v} ({type(v)} {type(other_v)})\"\n                )\n        elif isinstance(v, (list, tuple)):\n            if not isinstance(other_v, (list, tuple)):\n                raise ValueError(\n                    msg + f\"{k} is list/tuple in input but not in other: {type(v)} {type(other_v)}\"\n                )\n            if len(v) != len(other_v):\n                raise AssertionError(\n                    msg\n                    + f\"{k} len {len(v)} != {len(other_v"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check that the two instances have the same image size\n    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size), f\"{msg}image_size does not match\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}image_size does not match\"\n\n    # Get the fields of both instances\n    fields_input = set(input.get_fields())\n    fields_other = set(other.get_fields())\n\n    # Check that the fields are the same\n    assert fields_input == fields_other, f\"{msg}fields do not match\"\n\n    # Check that the values of each field are equal or close\n    for field in fields_input:\n        # Check if the field is a Boxes, ROIMasks, or torch.Tensor\n        if isinstance(input.get(field), Boxes):\n            assert input.get(field).tensor.allclose(other.get(field).tensor), f\"{msg}field {field} does not match\"\n        elif isinstance(input.get(field), ROIMasks):\n            assert input.get(field).tensor.allclose(other.get(field).tensor), f\"{msg}field {field} does not match\"\n        elif isinstance(input.get(field), torch.Tensor):\n            assert input.get(field).allclose(other.get(field)), f\"{msg}field {field} does not match\"\n        else:\n            # Check if the field is a list of Boxes\n            if isinstance(input.get(field), list) and isinstance(input.get(field)[0], Boxes):\n                for i in range(len(input.get(field))):\n                    assert input.get(field)[i].tensor.allclose(other.get(field)[i].tensor), f\"{msg}field {field} does not match\"\n            else:\n                assert input.get(field) == other.get(field), f\"{msg}field {field} does not match\""}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert input.image_size == other.image_size, f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    if size_as_tensor:\n        assert torch.equal(input.image_size, other.image_size), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    for k, v in input.get_fields().items():\n        other_v = other.get_fields()[k]\n        if isinstance(v, Boxes) or isinstance(v, ROIMasks):\n            assert v.tensor.shape == other_v.tensor.shape, f\"{msg} {k} shape mismatch: {v.tensor.shape} vs {other_v.tensor.shape}\"\n            assert torch.equal(v.tensor, other_v.tensor), f\"{msg} {k} mismatch: {v.tensor} vs {other_v.tensor}\"\n        elif isinstance(v, torch.Tensor):\n            assert torch.equal(v, other_v), f\"{msg} {k} mismatch: {v} vs {other_v}\"\n        else:\n            assert v == other_v, f\"{msg} {k} mismatch: {v} vs {other_v}\"\n\n    for k, v in other.get_fields().items():\n        if k not in input.get_fields():\n            raise ValueError(f\"{msg} {k} not in input\")\n\n    for k, v in input.get_fields().items():\n        if k not in other.get_fields():\n            raise ValueError(f\"{msg} {k} not in other\")\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.boxes[:, 2] * self.boxes[:, 3]\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return (self.widths * self.heights)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.widths * self.heights\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.widths * self.heights\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # Calculate the width and height of each box\n        width = self.boxes[:, 2] - self.boxes[:, 0]\n        height = self.boxes[:, 3] - self.boxes[:, 1]\n\n        # Compute the area of each box and return it\n        return width * height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # Compute the area of the rotated boxes\n        area = self.widths * self.heights\n\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return (self.widths * self.heights)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return (self.width * self.height)\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from .proposal_generator import build_proposal_generator\n        return build_proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from .proposal_generator import build_proposal_generator\n        return build_proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from .proposal_generator import build_proposal_generator\n        return build_proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from .proposal_generator import build_proposal_generator\n        return build_proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    from .proposal_generator import build_proposal_generator\n\n    return build_proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from .proposal_generator import build_proposal_generator\n        return build_proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from .proposal_generator import build_proposal_generator\n        return build_proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from .proposal_generator import build_proposal_generator\n        return build_proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from .proposal_generator import registry\n        return registry.get(cfg.MODEL.PROPOSAL_GENERATOR.NAME)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from detectron2.modeling.proposal_generator import build_proposal_generator\n        return build_proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    from detectron2.modeling import PROPOSAL_GENERATOR_REGISTRY\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(cfg.MODEL.PROPOSAL_GENERATOR.NAME)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    from .proposal_generator import build_proposal_generator\n    return build_proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from . import registry\n\n        return registry.PROPOSAL_GENERATORS[cfg.MODEL.PROPOSAL_GENERATOR.NAME](\n            cfg, input_shape\n        )"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from .proposal_generator import make_proposal_generator\n\n        return make_proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from maskrcnn_benchmark.modeling.roi_heads.proposal_generator import registry\n        return registry.ROI_PROPOSAL_GENERATORS[cfg.MODEL.PROPOSAL_GENERATOR.NAME](cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.RPN.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from . import registry\n        return registry.Registry.get_proposal_generator(cfg.MODEL.RPN.NAME)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from .proposal_generator import registry\n        proposal_generator = registry[cfg.MODEL.PROPOSAL_GENERATOR.NAME]()\n        proposal_generator.initialize(cfg, input_shape)\n        return proposal_generator"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from .proposal_generator import registry\n        func = registry.get(cfg.MODEL.PROPOSAL_GENERATOR.NAME)\n        return func(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    proposal_generator = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if proposal_generator == \"PrecomputedProposals\":\n        return None\n    else:\n        from maskrcnn_benchmark.modeling.proposal_generator import build_proposal_generator\n        return build_proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    from maskrcnn_benchmark.modeling.proposal_generator import build_proposal_generator as pg_builder\n    return pg_builder(cfg, input_shape)\n\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Unpack the predictions\n        scores, proposal_deltas = predictions\n\n        # Extract the ground truth boxes and classes from the proposals\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_classes = [x.gt_classes for x in proposals]\n\n        # Compute the classification loss\n        loss_cls = self.cls_loss(scores, gt_classes)\n\n        # Compute the box regression loss\n        loss_box_reg = self.box_reg_loss(proposal_deltas, gt_boxes, gt_classes)\n\n        # Scale the losses by their respective weights\n        losses = {\n            \"loss_cls\": self.loss_weight[\"loss_cls\"] * loss_cls,\n            \"loss_box_reg\": self.loss_weight[\"loss_box_reg\"] * loss_box_reg,\n        }\n\n        return losses"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Extract the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Extract the ground truth boxes and classes from the proposals\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_classes = [x.gt_classes for x in proposals]\n\n        # Calculate the classification loss\n        loss_cls = fast_rcnn_loss_cls(\n            scores, gt_classes, self.num_classes, self.loss_weight[\"loss_cls\"]\n        )\n\n        # Calculate the box regression loss\n        loss_box_reg = fast_rcnn_loss_box_reg(\n            proposal_deltas, gt_boxes, gt_classes, self.box2box_transform, self.loss_weight[\"loss_box_reg\"]\n        )\n\n        # Return the losses as a dictionary\n        losses = {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}\n        return losses"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Unpack the predictions tuple\n        scores, proposal_deltas = predictions\n\n        # Get the ground truth boxes and classes from the proposals\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_classes = [x.gt_classes for x in proposals]\n\n        # Compute the classification loss\n        loss_cls = self.loss_cls(scores, gt_classes)\n\n        # Compute the box regression loss\n        loss_box_reg = self.loss_box_reg(proposal_deltas, gt_boxes)\n\n        # Return the losses as a dictionary\n        losses = {\n            \"loss_cls\": self.loss_weight * loss_cls,\n            \"loss_box_reg\": self.loss_weight * loss_box_reg,\n        }\n        return losses"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Extract the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Calculate the classification loss\n        loss_cls = F.cross_entropy(scores, gt_classes)\n\n        # Calculate the box regression loss\n        loss_box_reg = F.smooth_l1_loss(proposal_deltas, gt_boxes, beta=1.0, reduction=\"sum\")\n\n        # Scale the losses by their respective weights\n        loss_cls = loss_cls * self.loss_weight[\"loss_cls\"]\n        loss_box_reg = loss_box_reg * self.loss_weight[\"loss_box_reg\"]\n\n        # Return a dictionary of losses\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Classification loss\n        scores, proposal_deltas = predictions\n        gt_labels = [x.gt_classes for x in proposals]\n        loss_cls = fast_rcnn_loss_cls(\n            scores,\n            gt_labels,\n            self.num_classes,\n            self.loss_weight[\"loss_cls\"],\n        )\n\n        # Box regression loss\n        if self.box_reg_loss_type == \"smooth_l1\":\n            loss_box_reg = fast_rcnn_loss_box_reg(\n                proposal_deltas,\n                [x.proposal_boxes for x in proposals],\n                [x.gt_boxes for x in proposals],\n                self.box_reg_loss_norm,\n                self.loss_weight[\"loss_box_reg\"],\n            )\n        elif self.box_reg_loss_type == \"giou\":\n            loss_box_reg = fast_rcnn_loss_box_reg_giou(\n                proposal_deltas,\n                [x.proposal_boxes for x in proposals],\n                [x.gt_boxes for x in proposals],\n                self.box_reg_loss_norm,\n                self.loss_weight[\"loss_box_reg\"],\n            )\n        else:\n            raise ValueError(\"Unknown box regression loss type: {}\".format(self.box_reg_loss_type))\n\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Extract the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Calculate the classification loss\n        loss_cls = F.cross_entropy(scores, proposals.gt_classes)\n\n        # Calculate the box regression loss\n        loss_box_reg = smooth_l1_loss(\n            proposal_deltas,\n            proposals.proposal_boxes.tensor,\n            beta=self.smooth_l1_beta,\n            reduction=\"none\",\n        )\n        loss_box_reg = loss_box_reg.mean(dim=[1, 2])\n\n        # Calculate the total loss by combining the classification and box regression losses\n        loss = (\n            self.loss_weight[\"loss_cls\"] * loss_cls\n            + self.loss_weight[\"loss_box_reg\"] * loss_box_reg\n        )\n\n        # Return a dictionary of losses\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg, \"loss\": loss}"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Extract the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Get the ground truth boxes and classes from the proposals\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_classes = [x.gt_classes for x in proposals]\n\n        # Calculate the classification loss\n        loss_cls = self.loss_cls(scores, gt_classes)\n        losses = {\"loss_cls\": loss_cls}\n\n        # Calculate the box regression loss\n        if self.box_reg_loss_type == \"smooth_l1\":\n            loss_box_reg = self.loss_box_reg(proposal_deltas, gt_boxes)\n        elif self.box_reg_loss_type == \"giou\":\n            loss_box_reg = self.loss_box_reg(proposal_deltas, gt_boxes)\n        else:\n            raise ValueError(\n                \"Unknown box regression loss type '{}'\".format(self.box_reg_loss_type)\n            )\n        losses[\"loss_box_reg\"] = loss_box_reg\n\n        # Scale the losses by the loss weights defined in self.loss_weight\n        for k, v in losses.items():\n            losses[k] = v * self.loss_weight[k]\n\n        return losses"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        num_classes = scores.shape[1]\n\n        # Select the indices of positive proposals.\n        # positive_inds = torch.where(torch.cat([proposal.gt_classes for proposal in proposals]) > 0)[0]\n        positive_inds = torch.where(torch.cat([proposal.gt_classes for proposal in proposals]) > 0)[0]\n        # positive_inds = torch.where(torch.cat([proposal.gt_classes for proposal in proposals]) != self.num_classes)[0]\n        # positive_inds = torch.where(torch.cat([proposal.gt_classes for proposal in proposals]) != self.num_classes)[0]\n        # positive_inds = torch.where(torch.cat([proposal.gt_classes for proposal in proposals]) != self.num_classes)[0]\n        # positive_inds = torch.where(torch.cat([proposal.gt_classes for proposal in proposals]) != self.num_classes)[0]\n        # positive_inds = torch.where(torch.cat([proposal.gt_classes for proposal in proposals]) != self.num_classes)[0]\n        # positive_inds = torch.where(torch.cat([proposal.gt_classes for proposal in proposals]) != self.num_classes)[0]\n        # positive_inds = torch.where(torch.cat([proposal.gt_classes for proposal in proposals]) != self.num_classes)[0]\n        # positive_inds = torch.where(torch.cat([proposal.gt_classes for proposal in proposals]) != self.num_classes)[0]\n        # positive_inds = torch.where(torch.cat([proposal.gt_classes for proposal in proposals]) != self.num_classes)[0]\n        # positive_inds = torch.where(torch.cat([proposal.gt_classes for proposal in proposals]) != self.num_classes"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Extract the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Calculate the classification loss\n        loss_cls = F.cross_entropy(\n            scores,\n            self.get_targets(proposals, self.dispatcher.get_name()),\n        )\n\n        # Calculate the box regression loss\n        loss_box_reg = smooth_l1_loss(\n            proposal_deltas,\n            self.get_targets(proposals, self.dispatcher.get_name()),\n            beta=1 / 9,\n            reduction=\"sum\",\n        )\n\n        # Return a dictionary of losses, scaled by their respective weights\n        return {\n            \"loss_cls\": loss_cls * self.loss_weight[\"loss_cls\"],\n            \"loss_box_reg\": loss_box_reg * self.loss_weight[\"loss_box_reg\"],\n        }"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # Select the foreground proposals\n        num_classes = scores.shape[1]\n        fg_inds = torch.nonzero((proposals.gt_classes >= 0) & (proposals.gt_classes < num_classes))[:, 0]\n        scores = scores[fg_inds]\n        proposal_deltas = proposal_deltas[fg_inds]\n        proposals = [proposals[i] for i in fg_inds]\n\n        # Calculate classification loss\n        loss_cls = F.cross_entropy(scores, torch.cat([p.gt_classes for p in proposals]))\n        loss_cls = self.loss_weight[0] * loss_cls\n\n        # Calculate box regression loss\n        gt_boxes = cat([p.gt_boxes for p in proposals], dim=0)\n        loss_box_reg = smooth_l1_loss(proposal_deltas, gt_boxes.sub(proposals.proposal_boxes).tensor(), beta=1.0 / 9)\n        loss_box_reg = self.loss_weight[1] * loss_box_reg\n\n        return dict(loss_cls=loss_cls, loss_box_reg=loss_box_reg)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # Select the indices of positive proposals.\n        # positive_inds = torch.where(torch.cat([proposal.get_field(\"gt_classes\") > 0 for proposal in proposals]))[0]\n        positive_inds = torch.where(torch.cat([proposal.gt_classes > 0 for proposal in proposals]))[0]\n        # positive_inds = torch.where(torch.cat([proposal.gt_classes > 0 for proposal in proposals]))[0]\n\n        # Select the corresponding scores and proposal deltas for the positive proposals.\n        # scores = scores[positive_inds]\n        # proposal_deltas = proposal_deltas[positive_inds]\n        scores = scores[positive_inds]\n        proposal_deltas = proposal_deltas[positive_inds]\n\n        # Compute the classification loss using the selected scores and ground truth classes.\n        # gt_classes = torch.cat([proposal.get_field(\"gt_classes\") for proposal in proposals])\n        gt_classes = torch.cat([proposal.gt_classes for proposal in proposals])\n        # gt_classes = torch.cat([proposal.gt_classes for proposal in proposals])\n        loss_cls = F.cross_entropy(scores, gt_classes)\n\n        # Compute the box regression loss using the selected proposal deltas and ground truth boxes.\n        # gt_boxes = torch.cat([proposal.get_field(\"gt_boxes\") for proposal in proposals])\n        gt_boxes = torch.cat([proposal.gt_boxes for proposal in proposals])\n        # gt_boxes = torch.cat([proposal.gt_boxes for proposal in proposals])\n        loss_box_reg = F.smooth_l1_loss(\n            proposal_deltas,\n            Box2BoxTransform.weights_to_deltas(\n                gt_boxes.tensor, proposal_deltas.tensor\n            ),\n           "}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Extract the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Get the ground truth boxes and classes from the proposals\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_classes = [x.gt_classes for x in proposals]\n\n        # Calculate the classification loss using the scores and ground truth classes\n        loss_cls = fast_rcnn_loss_cls(\n            scores, gt_classes, self.num_classes, self.loss_weight[\"loss_cls\"]\n        )\n\n        # Calculate the box regression loss using the proposal deltas and ground truth boxes\n        loss_box_reg = fast_rcnn_loss_box_reg(\n            proposal_deltas, gt_boxes, self.box2box_transform, self.loss_weight[\"loss_box_reg\"]\n        )\n\n        # Return a dictionary of losses, scaled by their respective weights\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}\n\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # Select only the foreground classes.\n        gt_classes = [\n            cat_inds.to(self.device)\n            for cat_inds in [x.gt_classes for x in proposals]\n        ]\n        # Select only the foreground boxes.\n        gt_boxes = [\n            x.gt_boxes.to(self.device)\n            for x in proposals\n            if x.is_thing\n        ]\n\n        # Compute classification loss.\n        loss_cls = self.cls_loss(scores, gt_classes)\n        # Compute box regression loss.\n        loss_box_reg = self.box_reg_loss(\n            proposal_deltas, gt_boxes, gt_classes\n        )\n\n        return dict(\n            loss_cls=loss_cls * self.loss_weight[\"loss_cls\"],\n            loss_box_reg=loss_box_reg * self.loss_weight[\"loss_box_reg\"],\n        )"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Get the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Get the ground truth boxes, classes, and indices from the proposals\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_classes = [x.gt_classes for x in proposals]\n        gt_inds = [x.gt_inds for x in proposals]\n\n        # Calculate the classification loss\n        loss_cls = self.loss_cls(scores, gt_classes)\n\n        # Calculate the box regression loss\n        loss_box_reg = self.loss_box_reg(proposal_deltas, gt_boxes, gt_inds)\n\n        # Return the losses as a dictionary\n        return dict(\n            loss_cls=loss_cls,\n            loss_box_reg=loss_box_reg,\n        )\n\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # Select the tensors corresponding to the current image.\n        num_images = len(proposals)\n        scores = scores.reshape(num_images, -1, self.cls_aggregator.num_classes)\n        proposal_deltas = proposal_deltas.reshape(num_images, -1, 4)\n\n        # Concatenate ground-truth boxes into the `gt_boxes` field so that they\n        # can be treated as object proposals.\n        for proposals_per_image, gt_boxes, gt_classes in zip(proposals, self.gt_boxes, self.gt_classes):\n            assert gt_boxes.device == gt_classes.device\n            gt_classes = gt_classes.type_as(proposals_per_image)\n            gt_boxes = Boxes(gt_boxes)\n            gt_boxes.tensor.set_shape(gt_classes.shape)\n            proposals_per_image.gt_classes = gt_classes\n            proposals_per_image.gt_boxes = gt_boxes\n\n        # Include a dummy background class. This allows sampling negative\n        # proposals during training, but it's not used in inference.\n        if self.cls_aggregator.cls_inputs_per_box == 1:\n            # single box prediction per class\n            num_classes = self.cls_aggregator.num_classes\n            scores = scores.reshape(num_images, -1, num_classes)\n            proposal_deltas = proposal_deltas.reshape(num_images, -1, 4)\n        else:\n            # concatenated box prediction per class\n            num_classes = self.cls_aggregator.num_classes + 1\n            scores = scores.reshape(num_images, -1, num_classes)\n            proposal_deltas = proposal_deltas.reshape(num_images, -1, 4)\n\n        # Classification/Regression Losses\n        loss_box_cls"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # predictions: (list[Instances], list[Instances])\n        # proposals: list[Instances]\n        # proposals[i] is the set of proposals for image i\n        # predictions[i] is the set of predicted boxes for image i\n        # predictions[i].pred_boxes is a Boxes object of size N x 4,\n        # where N is the number of predicted boxes\n        # predictions[i].pred_classes is a LongTensor of size N\n        # predictions[i].scores is a Tensor of size N\n\n        # TODO: Check if this is a correct implementation\n\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation\n        # TODO: Check if this is a correct implementation"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # In Fast R-CNN, each proposal is classified into one of two classes: background (0) or foreground (1).\n        # The classification loss is calculated using a binary cross-entropy loss.\n        # The box regression loss is calculated using a smooth L1 loss.\n\n        # Get the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Get the number of classes from the model\n        num_classes = scores.shape[1]\n\n        # Get the number of proposals from the proposals list\n        num_proposals = len(proposals)\n\n        # Calculate the classification loss using the scores and ground truth labels\n        # The ground truth labels are obtained from the proposals list\n        # The loss_weight is used to scale the loss\n        loss_cls = F.cross_entropy(\n            scores.reshape(-1, num_classes),\n            torch.cat([proposal.get_field(\"gt_classes\") for proposal in proposals]),\n        )\n        loss_cls = loss_cls * self.loss_weight\n\n        # Calculate the box regression loss using the proposal deltas and ground truth boxes\n        # The ground truth boxes are obtained from the proposals list\n        # The loss_weight is used to scale the loss\n        loss_box_reg = None\n        if self.box_reg_loss_type == \"smooth_l1\":\n            # Calculate the smooth L1 loss\n            loss_box_reg = smooth_l1_loss(\n                proposal_deltas,\n                torch.cat([proposal.get_field(\"gt_boxes\").tensor for proposal in proposals]),\n                beta=1.0 / 9,\n                reduction=\"sum\",\n            )\n        elif self.box_reg_loss_type == \"giou\":\n            # Calculate the GIoU loss\n            loss_box_reg = -torch.mean(\n                torch.sum(\n                    giou_loss(\n                        proposal_deltas,\n                        torch.cat(\n                            [proposal.get_field(\"gt_boxes\").tensor for proposal in proposals]"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # Select foreground proposals\n        num_fg = self.num_classes * self.fg_iou_thresh\n        num_proposals = scores.shape[1]\n        fg_inds = torch.nonzero(proposal_deltas.sum(dim=2).sum(dim=1) > 0).squeeze(1)\n        fg_inds = fg_inds[:num_fg]\n\n        # Select background proposals\n        bg_inds = torch.arange(num_proposals)\n        bg_inds = bg_inds[~torch.cat([fg_inds, bg_inds], dim=0)]\n        bg_inds = bg_inds[:num_fg]\n\n        # Concatenate foreground and background proposals\n        inds = torch.cat([fg_inds, bg_inds], dim=0)\n\n        # Compute classification loss\n        scores = scores[:, inds]\n        gt_labels = torch.cat([proposal.gt_classes[:num_fg], torch.zeros(num_fg)], dim=0)\n        loss_cls = F.cross_entropy(scores, gt_labels, reduction=\"none\")\n        loss_cls = loss_cls.mean(dim=1)\n        loss_cls = loss_cls[None].repeat(self.num_classes, 1)\n\n        # Compute box regression loss\n        proposal_deltas = proposal_deltas[:, inds]\n        gt_boxes = torch.cat([proposal.gt_boxes[:num_fg], proposal.gt_boxes[bg_inds]], dim=0)\n        loss_box_reg = smooth_l1_loss(proposal_deltas, gt_boxes, beta=1.0, reduction=\"none\")\n        loss_box_reg = loss_box_reg.mean(dim=1)\n\n        # Combine losses\n        loss_cls = loss_cls * self.loss_weight"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        # In the original implementation, the classification loss is calculated as follows:\n        # loss_cls = self.cls_loss(scores, gt_classes)\n        # However, in the current implementation, the classification loss is calculated as follows:\n        # loss_cls = F.cross_entropy(scores, gt_classes)\n        # The reason for this change is that the original implementation uses a softmax layer to calculate the classification loss, while the current implementation uses a cross-entropy loss.\n        # The softmax layer is not used in the current implementation because it is replaced by a cross-entropy loss, which is more efficient and accurate.\n        # Additionally, the original implementation uses a sigmoid layer to calculate the classification loss, which is not used in the current implementation.\n        # Therefore, the current implementation uses a cross-entropy loss to calculate the classification loss, which is more accurate and efficient.\n        # The cross-entropy loss is calculated by taking the negative log of the predicted probabilities for the correct class, and then averaging over the batch.\n        # The cross-entropy loss is more accurate than the softmax loss because it directly compares the predicted probabilities with the true labels, rather than using a softmax layer to calculate the probabilities.\n        # The cross-entropy loss is also more efficient than the sigmoid loss because it does not require a sigmoid layer to calculate the probabilities, which can be more computationally expensive.\n        # Therefore, the current implementation uses a cross-entropy loss to calculate the classification loss, which is more accurate and efficient.\n        # The cross-entropy loss is calculated by taking the negative log of the predicted probabilities for the correct class, and then averaging over the batch.\n        # The cross-entropy loss is more accurate than the softmax loss because it directly compares the predicted probabilities with the true labels, rather than using a softmax layer to calculate the probabilities.\n        # The cross-entropy loss is also more efficient than the sigmoid loss because it does not require a sigmoid layer to calculate the probabilities, which can be more computationally expensive.\n        # Therefore, the current implementation uses a cross-entropy loss to calculate the classification loss, which is more"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        num_classes = scores.shape[1]\n        # scores has shape (N, num_classes), so we split it into\n        # num_classes tensors along dim=1. In each case the\n        # batch_size of the tensor along dim=0 is the same.\n        scores_list = scores.split(1, dim=1)\n        # scores_list has shape [(N, 1), (N, 1), ...], each of shape (N, 1)\n        # Concat them with boxes\n        all_scores = torch.cat(scores_list, dim=2).squeeze(dim=-1)\n        # all_scores has shape (N, num_classes)\n        num_proposals = all_scores.shape[0]\n        # (N, num_classes) -> (N, num_classes, num_proposals)\n        all_scores = all_scores.transpose(0, 1)\n        all_proposal_deltas = proposal_deltas.transpose(0, 1)\n        # For each class, we have the following three pieces of info:\n        # (1) A set of N_i ground-truth boxes about that class (0 <= N_i <= N)\n        # (2) A set of N_i predicted boxes about that class (0 <= N_i <= N)\n        # (3) The matching between them (N_i of them)\n        #\n        # We expand all ground truth boxes, and all predicted boxes into\n        # flattened N * M tensors where M is the number of classes. The\n        # matching between them is a N-element vector where each element\n        # contains the matching between the corresponding ground-truth\n        # and predicted boxes.\n        #\n        # In the following, `gt` refers to the (flattened) ground-truth\n        # proposals and `pred` is the (flattened) predicted proposals.\n        #\n        # NOTE: The reason we need to expand the ground truth boxes and the\n        # predicted boxes into a single vector is because the matching"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACK.NAME\n    tracker_class = get_tracker_class(tracker_name)\n    tracker = tracker_class(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACK.NAME\n    tracker_class = get_tracker_class(tracker_name)\n    tracker = tracker_class(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.tracker_name\n    tracker_class = get_tracker_class(tracker_name)\n    tracker = tracker_class(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACK.NAME\n    tracker_class = get_tracker_class(tracker_name)\n    tracker = tracker_class(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACK.NAME\n    tracker_class = get_tracker_class(tracker_name)\n    tracker = tracker_class(cfg)\n    return tracker\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACK.NAME\n    tracker_class = get_tracker_class(tracker_name)\n\n    tracker = tracker_class(cfg)\n\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACK.NAME\n    tracker_class = tracker_registry.get(tracker_name)\n    tracker = tracker_class(cfg)\n\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACK.NAME\n    tracker_class = tracker_registry.get(tracker_name)\n\n    tracker = tracker_class(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.tracker_name\n    tracker_class = get_tracker_class(tracker_name)\n    tracker = tracker_class(cfg)\n    return tracker\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_class = tracker_registry[tracker_name]\n    tracker = tracker_class(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_class = tracker_registry[tracker_name]\n    tracker = tracker_class(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.name\n    tracker_class = get_tracker_class(tracker_name)\n\n    tracker = tracker_class(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_class = get_tracker_class(tracker_name)\n    tracker = tracker_class(cfg)\n    return tracker\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_class = trackers.get(tracker_name)\n    tracker_cfg = cfg.TRACKER\n    tracker_cfg.freeze()\n    return tracker_class(tracker_cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.tracker_name\n    tracker_class = get_tracker_class(tracker_name)\n    tracker = tracker_class(cfg)\n    return tracker\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACK.NAME\n    tracker_class = get_tracker_class(tracker_name)\n    tracker = tracker_class(cfg)\n    return tracker\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.tracker_name\n    tracker_class = get_tracker_class(tracker_name)\n    tracker = tracker_class(cfg)\n    return tracker\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.name\n    tracker_class = _TRACKER_REGISTRY.get(tracker_name)\n    if tracker_class is None:\n        raise KeyError(f\"No tracker named '{tracker_name}' has been registered\")\n\n    return tracker_class(cfg)\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACK.NAME\n    tracker_class = get_tracker_class(tracker_name)\n\n    if tracker_class is None:\n        raise ValueError(f'Tracker {tracker_name} not supported')\n\n    tracker_head = tracker_class(cfg)\n    return tracker_head\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACK.NAME\n    tracker_class = get_tracker_class(tracker_name)\n    tracker = tracker_class(cfg)\n\n    return tracker\n\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n        pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n        pred_w = np.exp(dw) * widths[:, np.newaxis]\n        pred_h = np.exp(dh) * heights[:, np.newaxis]\n\n        pred_boxes0 = pred_ctr_x - 0.5 * pred_w\n        pred_boxes1 = pred_ctr_y - 0.5 * pred_h\n        pred_boxes2 = pred_ctr_x + 0.5 * pred_w\n        pred_boxes3 = pred_ctr_y + 0.5 * pred_h\n\n        pred_boxes = np.stack((pred_boxes0, pred_boxes1, pred_boxes2, pred_boxes3), axis=1)\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Apply deltas to the boxes\n        boxes = boxes.to(deltas.dtype)\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Unpack the deltas into separate tensors for dx, dy, dw, dh\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        # Calculate the center coordinates of the boxes\n        ctr_x = boxes[:, 0] + 0.5 * boxes[:, 2]\n        ctr_y = boxes[:, 1] + 0.5 * boxes[:, 3]\n\n        # Calculate the predicted width and height of the boxes\n        pred_ctr_x = dx * boxes[:, 2] + ctr_x\n        pred_ctr_y = dy * boxes[:, 3] + ctr_y\n        pred_w = torch.exp(dw) * boxes[:, 2]\n        pred_h = torch.exp(dh) * boxes[:, 3]\n\n        # Calculate the new positions and sizes of the boxes\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_w\n        pred_boxes[:, 3::4] = pred_h\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Apply deltas\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        # Get widths and heights of boxes\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        # Apply deltas\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        # Convert predicted boxes to (x1, y1, x2, y2) format\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Calculate the widths and heights of the boxes\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n\n        # Calculate the center points of the boxes\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        # Calculate the deltas in the log space\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        # Calculate the predicted widths and heights\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        # Calculate the predicted bounding boxes\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Extract the widths and heights of the boxes\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n\n        # Extract the center coordinates of the boxes\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        # Extract the deltas for the center coordinates\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n\n        # Extract the deltas for the widths and heights\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        # Calculate the new center coordinates\n        pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n        pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n\n        # Calculate the new widths and heights\n        pred_w = np.exp(dw) * widths[:, np.newaxis]\n        pred_h = np.exp(dh) * heights[:, np.newaxis]\n\n        # Calculate the new bounding box coordinates\n        pred_boxes = np.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Extract the width and height of each box\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        # Extract the center point of each box\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        # Apply the transformation deltas to the center points and widths/heights\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        # Calculate the new center points and widths/heights\n        pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n        pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n        pred_w = np.exp(dw) * widths[:, np.newaxis]\n        pred_h = np.exp(dh) * heights[:, np.newaxis]\n\n        # Calculate the new bounding box coordinates\n        pred_boxes = np.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Convert the deltas to the format (x, y, w, h)\n        deltas = deltas.view(-1, 4)\n\n        # Extract the coordinates and sizes of the boxes\n        boxes = boxes.view(-1, 4)\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        # Apply the transformation deltas to the boxes\n        dx = deltas[:, 0]\n        dy = deltas[:, 1]\n        dw = deltas[:, 2]\n        dh = deltas[:, 3]\n\n        # Calculate the new coordinates and sizes of the boxes\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        # Convert the new coordinates and sizes to the format (x1, y1, x2, y2)\n        pred_x1 = pred_ctr_x - 0.5 * pred_w\n        pred_y1 = pred_ctr_y - 0.5 * pred_h\n        pred_x2 = pred_ctr_x + 0.5 * pred_w\n        pred_y2 = pred_ctr_y + 0.5 * pred_h\n\n        # Concatenate the new coordinates and sizes into a single tensor\n        pred_boxes = torch.stack((pred_x1, pred_y1, pred_x2, pred_y2), dim=1).view(-1, 4)\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Apply deltas to each corner of each box\n        # Shape: (N, 4, 4)\n        deltas = deltas.reshape(-1, 4)\n\n        # Shape: (N, 4)\n        bbox_corners = torch.stack(\n            [\n                -deltas[:, 0::4] * boxes[:, 2::4],\n                -deltas[:, 1::4] * boxes[:, 3::4],\n                deltas[:, 2::4] * boxes[:, 2::4],\n                deltas[:, 3::4] * boxes[:, 3::4],\n            ],\n            dim=1,\n        )\n\n        # Shape: (N, 4)\n        pred_boxes = torch.cat(\n            [\n                boxes[:, 0::4] + bbox_corners[:, 0::4],\n                boxes[:, 1::4] + bbox_corners[:, 1::4],\n                boxes[:, 2::4] + bbox_corners[:, 2::4],\n                boxes[:, 3::4] + bbox_corners[:, 3::4],\n            ],\n            dim=1,\n        )\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Apply deltas to each corner of each box\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        # Compute center of each box\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        # Compute the predicted left, top, right, and bottom coordinates\n        pred_boxes1 = pred_ctr_x - 0.5 * pred_w\n        pred_boxes2 = pred_ctr_y - 0.5 * pred_h\n        pred_boxes3 = pred_ctr_x + 0.5 * pred_w\n        pred_boxes4 = pred_ctr_y + 0.5 * pred_h\n\n        # Stack the predicted boxes into a single tensor\n        pred_boxes = torch.stack((pred_boxes1, pred_boxes2, pred_boxes3, pred_boxes4), dim=2).reshape(deltas.shape)\n\n        return pred_boxes\n\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Ensure that the deltas have the correct shape\n        deltas = deltas.reshape(-1, 4)\n\n        # Extract the width and height of each box\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n\n        # Extract the center points of each box\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        # Calculate the predicted width and height of the boxes\n        pred_ctr_x = deltas[:, 0] * widths + ctr_x\n        pred_ctr_y = deltas[:, 1] * heights + ctr_y\n        pred_w = torch.exp(deltas[:, 2]) * widths\n        pred_h = torch.exp(deltas[:, 3]) * heights\n\n        # Calculate the new top-left and bottom-right coordinates of the boxes\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Apply deltas to each corner of each box\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        # x1\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        # x2\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        # y2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Unpack the deltas and boxes tensors\n        deltas_x1y1x2y2 = deltas.view(-1, 4)\n        boxes_x1y1x2y2 = boxes.view(-1, 4)\n\n        # Calculate the width and height of the boxes\n        boxes_width_height = boxes_x1y1x2y2[:, 2:] - boxes_x1y1x2y2[:, :2]\n\n        # Calculate the center points of the boxes\n        boxes_center_xcyc = (boxes_x1y1x2y2[:, 2:] + boxes_x1y1x2y2[:, :2]) / 2.0\n\n        # Apply the deltas to the center points and width/height\n        deltas_width_height = deltas_x1y1x2y2[:, 2:] * boxes_width_height\n        deltas_center_xcyc = deltas_x1y1x2y2[:, :2] * boxes_width_height\n\n        # Calculate the new center points and width/height\n        new_center_xcyc = deltas_center_xcyc + boxes_center_xcyc\n        new_width_height = torch.exp(deltas_width_height) * boxes_width_height\n\n        # Convert the new center points and width/height back to x1y1x2y2 format\n        new_x1y1x2y2 = torch.cat([new_center_xcyc - new_width_height / 2.0,\n                                  new_center_xcyc + new_width_height / 2.0], dim=1)\n\n        # Clip the new boxes to the image boundaries\n        new_x1y1x2y2 = torch.clamp(new_x1y1x2y2, min=0, max=1)\n\n        # Return the new boxes in x1y1x2y2 format\n        return new_x1y1x2y2.view(deltas"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Note that the first two dimensions of the deltas are for the batch and the number of classes.\n        # The rest of the dimensions represent the deltas for each class.\n        # The first two dimensions of the boxes are for the batch and the number of boxes.\n        # The rest of the dimensions represent the coordinates of the boxes.\n        # The deltas are applied to the boxes in the following order:\n        # 1. The first two dimensions of the deltas are for the batch and the number of classes.\n        # 2. The rest of the dimensions represent the deltas for each class.\n        # 3. The first two dimensions of the boxes are for the batch and the number of boxes.\n        # 4. The rest of the dimensions represent the coordinates of the boxes.\n        # 5. The deltas are applied to the boxes in the following order:\n        # 6. The first two dimensions of the deltas are for the batch and the number of classes.\n        # 7. The rest of the dimensions represent the deltas for each class.\n        # 8. The first two dimensions of the boxes are for the batch and the number of boxes.\n        # 9. The rest of the dimensions represent the coordinates of the boxes.\n        # 10. The deltas are applied to the boxes in the following order:\n        # 11. The first two dimensions of the deltas are for the batch and the number of classes.\n        # 12. The rest of the dimensions represent the deltas for each class.\n        # 13. The first two dimensions of the boxes are for the batch and the number of boxes.\n        # 14. The rest of the dimensions represent the coordinates of the boxes.\n        # 15. The deltas are applied to the boxes in the following order:\n        # 16. The first two dimensions of the deltas are for the batch and the number of classes.\n        # 17. The rest of the dimensions represent the deltas for each class.\n        # 18. The first two dimensions of the boxes are for the batch and the number of boxes.\n        # 19. The rest of the dimensions"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Apply deltas to each box\n        for i in range(deltas.shape[0]):\n            boxes[i] = self.apply_single_delta(deltas[i], boxes[i])\n\n        return boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Get the number of classes\n        num_classes = deltas.size(1) // 4\n\n        # Get the width and height of each box\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n\n        # Get the center x and y of each box\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        # Get the deltas for each class\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        # Calculate the predicted center x and y of each box\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n\n        # Calculate the predicted width and height of each box\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        # Calculate the predicted x1, y1, x2, y2 coordinates of each box\n        pred_x1 = pred_ctr_x - 0.5 * pred_w\n        pred_y1 = pred_ctr_y - 0.5 * pred_h\n        pred_x2 = pred_ctr_x + 0.5 * pred_w\n        pred_y2 = pred_ctr_y + 0.5 * pred_h\n\n        # Stack the predictions into a single tensor\n        pred_boxes = torch.stack((pred_x1, pred_y1, pred_x2, pred_y2), dim=2)\n\n        # Reshape the predictions to match the input deltas\n        return pred_boxes.reshape(deltas.shape)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Convert the deltas from the format [dx, dy, dw, dh] to the format [x, y, w, h]\n        pred_ctr_x = (boxes[:, 0] + boxes[:, 2]) / 2\n        pred_ctr_y = (boxes[:, 1] + boxes[:, 3]) / 2\n        pred_w = boxes[:, 2] - boxes[:, 0] + 1.0\n        pred_h = boxes[:, 3] - boxes[:, 1] + 1.0\n\n        # Apply the deltas to the predictions\n        boxes_deltas = torch.cat([pred_ctr_x[:, None], pred_ctr_y[:, None], pred_w[:, None], pred_h[:, None]], dim=1)\n        boxes_deltas = boxes_deltas.repeat(1, deltas.size(1) // 4)\n        boxes_deltas = boxes_deltas.view(-1, 4)\n\n        # Apply the deltas to the predictions\n        boxes_deltas[:, 0] = deltas[:, 0::4] * pred_w + boxes_deltas[:, 0]\n        boxes_deltas[:, 1] = deltas[:, 1::4] * pred_h + boxes_deltas[:, 1]\n        boxes_deltas[:, 2] = torch.exp(deltas[:, 2::4]) * pred_w + boxes_deltas[:, 2]\n        boxes_deltas[:, 3] = torch.exp(deltas[:, 3::4]) * pred_h + boxes_deltas[:, 3]\n\n        # Convert the transformed boxes from the format [x, y, w, h] to the format [x1, y1, x2, y2]\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = boxes_deltas[:, 0] - boxes_"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Transform format is stored in the first two columns\n        # of the array. The rest is arbitrary.\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = deltas[:, 0::4].T\n        hx, hy, hw, hh = deltas[:, 1::4].T\n        tx, ty, tw, th = deltas[:, 2::4].T\n\n        # Prevent sending too large values into torch.exp()\n        # if boxes have area > 1e5\n        # TODO: Find a better way\n        area = widths * heights\n        too_large = torch.where(area > 1e5)[0]\n        if len(too_large) > 0:\n            # print(f\"Boxes with large area: {too_large}\")\n            # print(f\"Area: {area[too_large]}\")\n            # print(f\"Deltas: {deltas[too_large]}\")\n            # print(f\"Boxes: {boxes[too_large]}\")\n            # print(f\"Widths: {widths[too_large]}\")\n            # print(f\"Heights: {heights[too_large]}\")\n            # print(f\"CTR_X: {ctr_x[too_large]}\")\n            # print(f\"CTR_Y: {ctr_y[too_large]}\")\n            # print(f\"WX: {wx[too_large]}\")\n            # print(f\"WY: {wy[too_large]}\")\n            # print(f\"WW: {ww[too_large]}\")\n            # print(f\"WH: {wh[too_large]}\")\n            # print(f\"HX: {hx[too_large]}\")\n            # print(f\"HY:"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # boxes: (N, 4)\n        # deltas: (N, 4*k)\n        # deltas: (N, k, 4)\n        deltas = deltas.reshape(-1, 4)\n        deltas = deltas.to(boxes.dtype)\n\n        # Transform to center-size\n        # boxes: (N, 4)\n        # deltas: (N, 4)\n        # deltas: (N, 4)\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        # Apply deltas\n        # deltas: (N, 4)\n        # ctr_x: (N,)\n        # ctr_y: (N,)\n        # widths: (N,)\n        # heights: (N,)\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n        ctr_x = ctr_x[:, None] + dx * widths[:, None]\n        ctr_y = ctr_y[:, None] + dy * heights[:, None]\n        widths = torch.clamp(widths[:, None] * torch.exp(dw), min=1e-4)\n        heights = torch.clamp(heights[:, None] * torch.exp(dh), min=1e-4)\n\n        # Transform to corner-form\n        # ctr_x: (N,)\n        # ctr_y: (N,)\n        # widths: (N,)\n        # heights: (N,)\n        # boxes: (N, 4)\n        x1 = ctr_x - 0.5"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image\n        output = self.process(image)\n\n        # Filter the output based on the annotation type(s)\n        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            filtered_output = {}\n            for anno in anno_type:\n                if anno in output:\n                    filtered_output[anno] = output[anno]\n            if len(filtered_output) == 1:\n                return list(filtered_output.values())[0]\n            else:\n                return filtered_output\n        else:\n            return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image with the annotator\n        output = self.annotator.process_image(image)\n\n        # Filter the output based on the annotation type(s) specified\n        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            filtered_output = {}\n            for anno in anno_type:\n                if anno in output:\n                    filtered_output[anno] = output[anno]\n            if len(filtered_output) == 1:\n                return list(filtered_output.values())[0]\n            elif len(filtered_output) > 1:\n                return filtered_output\n            else:\n                return output\n        else:\n            return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.process(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            output = self.process(image)\n            anno_dict = {}\n            for anno in anno_type:\n                anno_dict[anno] = output[anno]\n            return anno_dict"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image using the annotator\n        output = self.annotator.run(image)\n\n        # Filter the output based on the annotation type(s) specified\n        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            anno_type = set(anno_type)\n            output = {k: v for k, v in output.items() if k in anno_type}\n\n        return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.process(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            anno_type = set(anno_type)\n            output = self.process(image)\n            if anno_type.issubset(set(output.keys())):\n                return {anno_type: output[anno_type] for anno_type in anno_type}\n            else:\n                return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image using the annotator\n        output = self.annotator.run(image)\n\n        # If no specific annotation type is requested, return the entire processed output\n        if anno_type is None:\n            return output\n\n        # If a specific annotation type is requested, filter the output based on the type\n        if isinstance(anno_type, str):\n            for anno in output:\n                if anno.type == anno_type:\n                    return anno\n        # If multiple annotation types are requested, filter the output based on the types\n        elif isinstance(anno_type, list):\n            anno_dict = {}\n            for anno in output:\n                if anno.type in anno_type:\n                    anno_dict[anno.type] = anno\n            return anno_dict\n\n        # If no annotations of the requested type are found, return None\n        return None"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image with the annotator\n        output = self.annotator.run(image)\n\n        # If an annotation type is specified, filter the output based on that type\n        if anno_type:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            output = {anno_type: output[anno_type]}\n\n        return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.process_image(image)\n        else:\n            if not isinstance(anno_type, list):\n                anno_type = [anno_type]\n\n            output = self.process_image(image)\n            filtered_output = {}\n            for anno in anno_type:\n                if anno in output:\n                    filtered_output[anno] = output[anno]\n            if len(filtered_output) == 1:\n                return list(filtered_output.values())[0]\n            else:\n                return filtered_output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.process(image)\n\n        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n\n        output = self.process(image)\n        annotations = {}\n\n        for anno_type_ in anno_type:\n            for anno in output:\n                if anno[\"type\"] == anno_type_:\n                    annotations[anno_type_] = anno\n\n        if len(annotations) == 1:\n            return list(annotations.values())[0]\n\n        return annotations"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        output = self.process(image)\n        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            anno_type = [anno_type]\n            if len(anno_type) == 1:\n                anno_type = anno_type[0]\n            if anno_type in output:\n                return output[anno_type]\n            else:\n                return None\n        else:\n            return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.process_image(image)\n\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n\n            output = self.process_image(image)\n            annotations = {}\n            for anno in anno_type:\n                if anno in output:\n                    annotations[anno] = output[anno]\n\n            if len(annotations) > 0:\n                return annotations\n            else:\n                return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image\n        output = self.process(image)\n\n        # Filter the output based on the requested annotation type(s)\n        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n\n            filtered_output = {}\n            for anno in anno_type:\n                if anno in output:\n                    filtered_output[anno] = output[anno]\n\n            if filtered_output:\n                return filtered_output\n            else:\n                return None\n\n        # Return the entire output if no specific annotation type is requested\n        return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image with the general annotation instruction\n        output = self.process(image)\n\n        # If an annotation type is specified, filter the output based on the type\n        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            anno_type = [anno.lower() for anno in anno_type]\n            anno_dict = {}\n            for anno in output:\n                if anno[\"type\"].lower() in anno_type:\n                    anno_dict[anno[\"type\"]] = anno\n            if len(anno_dict) == 1:\n                return list(anno_dict.values())[0]\n            else:\n                return anno_dict\n        else:\n            return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if self.annotator is None:\n            raise Exception(\"Annotator not initialized\")\n\n        if self.annotator.annotation_type is None:\n            raise Exception(\"Annotation type not set\")\n\n        if self.annotator.annotation_type not in self.annotator.supported_annotation_types:\n            raise Exception(\"Annotation type not supported\")\n\n        if self.annotator.annotation_type not in self.annotator.supported_annotation_types:\n            raise Exception(\"Annotation type not supported\")\n\n        if self.annotator.annotation_type not in self.annotator.supported_annotation_types:\n            raise Exception(\"Annotation type not supported\")\n\n        if self.annotator.annotation_type not in self.annotator.supported_annotation_types:\n            raise Exception(\"Annotation type not supported\")\n\n        if self.annotator.annotation_type not in self.annotator.supported_annotation_types:\n            raise Exception(\"Annotation type not supported\")\n\n        if self.annotator.annotation_type not in self.annotator.supported_annotation_types:\n            raise Exception(\"Annotation type not supported\")\n\n        if self.annotator.annotation_type not in self.annotator.supported_annotation_types:\n            raise Exception(\"Annotation type not supported\")\n\n        if self.annotator.annotation_type not in self.annotator.supported_annotation_types:\n            raise Exception(\"Annotation type not supported\")\n\n        if self.annotator.annotation_type not in self.annotator.supported_annotation_types:\n            raise Exception(\"Annotation type not supported\")\n\n        if self.annotator.annotation_type not in self.annotator.supported_annotation_types:\n            raise Exception(\"Annotation type not supported\")\n\n        if self.annotator.annotation_type not in self.annotator.supported_annotation_types:\n            raise Exception(\"Annotation type not supported\")\n\n        if self.annotator.annotation_type not in self.annotator.supported_annotation_types:\n            raise Exception(\"Annotation type not supported\")\n\n        if self.an"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image\n        output = self.process_image(image)\n\n        # Filter the output based on the annotation type(s) specified\n        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            filtered_output = {}\n            for anno_type_item in anno_type:\n                if anno_type_item in output:\n                    filtered_output[anno_type_item] = output[anno_type_item]\n            if len(filtered_output) == 1:\n                return list(filtered_output.values())[0]\n            elif len(filtered_output) > 1:\n                return filtered_output\n        return output\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        anno_type = self._validate_anno_type(anno_type)\n\n        output = self._process(image)\n        output = self._filter_output(output, anno_type)\n\n        return output\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.process(image)\n        elif isinstance(anno_type, str):\n            anno_type = [anno_type]\n        else:\n            assert isinstance(anno_type, list)\n\n        anno_dict = self.process(image)\n        anno_dict_filtered = {}\n        for anno_type_i in anno_type:\n            if anno_type_i in anno_dict:\n                anno_dict_filtered[anno_type_i] = anno_dict[anno_type_i]\n        if len(anno_dict_filtered) == 1:\n            return list(anno_dict_filtered.values())[0]\n        elif len(anno_dict_filtered) == 0:\n            return None\n        else:\n            return anno_dict_filtered\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n\n        # Run the annotator\n        output = self.annotator.run(image)\n\n        # Filter the output\n        if anno_type is not None:\n            filtered_output = {}\n            for anno_type_ in anno_type:\n                for anno in output:\n                    if anno_type_ == anno[\"type\"]:\n                        filtered_output[anno_type_] = anno\n            if len(filtered_output) == 1:\n                return list(filtered_output.values())[0]\n            elif len(filtered_output) > 1:\n                return filtered_output\n            else:\n                return None\n        else:\n            return output\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image\n        output = self.process(image)\n\n        # If no specific annotation type is requested, return the entire output\n        if anno_type is None:\n            return output\n\n        # If a specific annotation type is requested, filter the output based on that type\n        if isinstance(anno_type, str):\n            for anno in output:\n                if anno['type'] == anno_type:\n                    return anno\n            return None\n\n        # If multiple annotation types are requested, filter the output based on those types\n        if isinstance(anno_type, list):\n            anno_dict = {}\n            for anno in output:\n                if anno['type'] in anno_type:\n                    anno_dict[anno['type']] = anno\n            return anno_dict\n\n        # If an invalid annotation type is requested, raise an error\n        raise ValueError('Invalid annotation type')\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Check if the image is a valid image\n        if not isinstance(image, Image.Image):\n            raise ValueError(\"Image is not a valid image\")\n\n        # Check if the anno_type is a valid type\n        if anno_type is not None and not isinstance(anno_type, (str, list)):\n            raise ValueError(\"anno_type is not a valid type\")\n\n        # Check if the anno_type is a list and if it is, convert it to a string\n        if isinstance(anno_type, list):\n            anno_type = \",\".join(anno_type)\n\n        # Process the image\n        output = self.process(image)\n\n        # Check if the output is a valid output\n        if not isinstance(output, dict):\n            raise ValueError(\"Output is not a valid output\")\n\n        # Check if the anno_type is a valid type\n        if anno_type is not None and not isinstance(anno_type, (str, list)):\n            raise ValueError(\"anno_type is not a valid type\")\n\n        # Check if the anno_type is a list and if it is, convert it to a string\n        if isinstance(anno_type, list):\n            anno_type = \",\".join(anno_type)\n\n        # Check if the anno_type is a string\n        if anno_type is not None and isinstance(anno_type, str):\n            # Split the anno_type string into a list of anno_type\n            anno_type = anno_type.split(\",\")\n\n        # Check if the anno_type is a list\n        if anno_type is not None and isinstance(anno_type, list):\n            # Filter the output based on the anno_type\n            output = {k: v for k, v in output.items() if k in anno_type}\n\n        # Return the output\n        return output"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = normalize_query(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url, score in self.inverted_index[keyword].items():\n                    if url in scores:\n                        scores[url] += score\n                    else:\n                        scores[url] = score\n\n        # Return the scores as a dictionary\n        return scores"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = self.normalize(query)\n        keywords = self.split_into_keywords(query)\n        scores = self.calculate_bm25_score(keywords)\n        return scores"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = self.split_query(query)\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = self.calculate_scores(keywords)\n\n        # Aggregate the scores for each URL\n        aggregated_scores = self.aggregate_scores(scores)\n\n        # Return the aggregated scores as a dictionary\n        return aggregated_scores"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = self.split_keywords(query)\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = self.bm25.get_scores(keywords)\n\n        # Aggregate the scores for URLs\n        aggregated_scores = self.aggregate_scores(scores)\n\n        # Return the aggregated scores as a dictionary\n        return aggregated_scores"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query\n        query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Initialize a dictionary to store the scores for each URL\n        scores = {}\n\n        # Calculate the BM25 score for each keyword across URLs\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url in self.inverted_index[keyword]:\n                    scores[url] = scores.get(url, 0) + self.bm25(keyword, url)\n\n        # Aggregate the scores for each URL\n        aggregated_scores = {}\n        for url, score in scores.items():\n            aggregated_scores[url] = aggregated_scores.get(url, 0) + score\n\n        # Return the aggregated scores as a dictionary\n        return aggregated_scores"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = normalize(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 scores for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.index:\n                for url, score in self.index[keyword].items():\n                    if url not in scores:\n                        scores[url] = 0\n                    scores[url] += score\n\n        # Aggregate the scores for URLs\n        aggregated_scores = {}\n        for url, score in scores.items():\n            if url not in aggregated_scores:\n                aggregated_scores[url] = 0\n            aggregated_scores[url] += score\n\n        # Sort the aggregated scores in descending order\n        sorted_scores = sorted(aggregated_scores.items(), key=lambda x: x[1], reverse=True)\n\n        # Return the top 10 URLs with the highest scores\n        return {url: score for url, score in sorted_scores[:10]}"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query\n        query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.index:\n                for url in self.index[keyword]:\n                    scores[url] = scores.get(url, 0) + self.bm25.score(keyword, self.index[keyword][url])\n\n        # Aggregate the scores for each URL\n        aggregated_scores = {}\n        for url, score in scores.items():\n            aggregated_scores[url] = aggregated_scores.get(url, 0) + score\n\n        # Sort the URLs by their aggregated scores in descending order\n        sorted_urls = sorted(aggregated_scores.items(), key=lambda x: x[1], reverse=True)\n\n        # Return the top 10 URLs with the highest scores\n        return {url: score for url, score in sorted_urls[:10]}"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize_query(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = defaultdict(float)\n        for keyword in keywords:\n            for url, tf in self.inverted_index[keyword].items():\n                scores[url] += self.bm25.score(tf, self.doc_lengths[url], self.avg_doc_length, self.doc_freq[keyword])\n\n        # Aggregate the scores for URLs\n        return scores"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize_query(query)\n\n        # Split the query string into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            for url, score in self.bm25_scores[keyword].items():\n                scores[url] = scores.get(url, 0) + score\n\n        # Sort the URLs by their scores in descending order and return the top 10 URLs\n        return dict(sorted(scores.items(), key=lambda item: item[1], reverse=True)[:10])"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query_norm = self.normalize(query)\n        query_keywords = self.split_keywords(query_norm)\n        scores = {}\n        for keyword in query_keywords:\n            if keyword in self.inverted_index:\n                for url, score in self.inverted_index[keyword].items():\n                    scores[url] = scores.get(url, 0) + score\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query\n        query = self.normalize_query(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            for url, title, body in self.corpus:\n                scores.setdefault(url, 0)\n                scores[url] += self.bm25.score(keyword, title)\n                scores[url] += self.bm25.score(keyword, body)\n\n        # Return the aggregated scores as a dictionary\n        return scores"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = self.normalize(query)\n        keywords = self.tokenize(query)\n        scores = {}\n        for keyword in keywords:\n            for url, score in self.index.get(keyword, {}).items():\n                scores[url] = scores.get(url, 0) + score\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query\n        query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = self.tokenize(query)\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url, tf in self.inverted_index[keyword].items():\n                    scores[url] = scores.get(url, 0) + self.bm25_score(keyword, tf, len(self.urls[url]))\n\n        # Aggregate the scores for URLs and return the result\n        return scores\n\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize_query(query)\n\n        # Split the query into keywords\n        keywords = self.tokenizer.tokenize(query)\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url, score in self.inverted_index[keyword].items():\n                    if url not in scores:\n                        scores[url] = 0\n                    scores[url] += score\n\n        # Aggregate the scores for URLs\n        aggregated_scores = {}\n        for url, score in scores.items():\n            if url not in aggregated_scores:\n                aggregated_scores[url] = 0\n            aggregated_scores[url] += score\n\n        # Return the aggregated scores as a dictionary\n        return aggregated_scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = self.normalize(query)\n        keywords = self.split_keywords(query)\n\n        scores = {}\n        for keyword in keywords:\n            for url, tfidf_vector in self.tfidf_vectors.items():\n                if keyword in tfidf_vector:\n                    scores[url] = scores.get(url, 0) + tfidf_vector[keyword]\n\n        return scores\n\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query_norm = self.normalize(query)\n        keywords = query_norm.split()\n\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url, score in self.inverted_index[keyword].items():\n                    scores[url] = scores.get(url, 0) + score\n\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = self.normalize(query)\n        query_keywords = self.split(query)\n\n        scores = {}\n        for url in self.urls:\n            scores[url] = 0\n            for keyword in query_keywords:\n                if keyword in self.index[url]:\n                    scores[url] += self.bm25.score(keyword, self.index[url][keyword], self.doc_lengths[url])\n\n        return scores\n\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = self.normalize(query)\n        query_keywords = self.split_into_keywords(query)\n        scores = {}\n        for keyword in query_keywords:\n            keyword_scores = self.calculate_keyword_scores(keyword)\n            for url, score in keyword_scores.items():\n                if url not in scores:\n                    scores[url] = 0\n                scores[url] += score\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query\n        query = normalize_query(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url, tf in self.inverted_index[keyword].items():\n                    scores[url] = scores.get(url, 0) + self.bm25.score(keyword, tf, len(self.urls[url]))\n\n        # Aggregate the scores for each URL\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query\n        query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = self.split_query(query)\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url, score in self.inverted_index[keyword].items():\n                    scores[url] = scores.get(url, 0) + score\n\n        # Aggregate the scores for URLs\n        aggregated_scores = {}\n        for url, score in scores.items():\n            aggregated_scores[url] = score / len(keywords)\n\n        return aggregated_scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to (-180, 180]\n        self.tensor[:, 4] = torch.remainder(self.tensor[:, 4], 360.0)\n        self.tensor[:, 4] = torch.where(self.tensor[:, 4] > 180.0, self.tensor[:, 4] - 360.0, self.tensor[:, 4])\n\n        # Identify boxes that are nearly horizontal\n        is_horizontal = torch.abs(self.tensor[:, 4]) < clip_angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2\n\n        # Clamp x and y coordinates\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y2 = torch.clamp(y2, min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor[:, 0] = (x1 + x2) / 2\n        self.tensor[:, 1] = (y1 + y2) / 2\n        self.tensor[:, 2] = x2 - x1\n        self.tensor[:, 3] = y2 - y1\n\n        # Convert angles back to (-180, 180]\n        self.tensor[:, 4]"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        if self.tensor.shape[-1] == 5:\n            # Convert to (x1, y1, x2, y2) representation\n            x1 = self.tensor[..., 0] - self.tensor[..., 2] / 2\n            y1 = self.tensor[..., 1] - self.tensor[..., 3] / 2\n            x2 = self.tensor[..., 0] + self.tensor[..., 2] / 2\n            y2 = self.tensor[..., 1] + self.tensor[..., 3] / 2\n            # Clip x and y coordinates\n            x1 = torch.clamp(x1, min=0, max=box_size[1])\n            y1 = torch.clamp(y1, min=0, max=box_size[0])\n            x2 = torch.clamp(x2, min=0, max=box_size[1])\n            y2 = torch.clamp(y2, min=0, max=box_size[0])\n            # Convert back to (center x, center y, width, height, angle) representation\n            self.tensor[..., 0] = (x1 + x2) / 2\n            self.tensor[..., 1] = (y1 + y2) / 2\n            self.tensor[..., 2] = x2 - x1\n            self.tensor[..., 3] = y2 - y1\n        else:\n            # Clip x and y coordinates\n            self.tensor[..., 0] = torch.clamp(self.tensor[..., 0], min=0, max=box_size[1])\n            self.tensor[..., 1] = torch.clamp(self.tensor[..., 1], min=0, max=box_size[0])\n            self.tensor[..., 2] = torch.clamp(self.tensor[..., 2], min=0, max=box_size[1])\n            self.tensor[..., "}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles to be within the range (-180, 180] degrees\n        self.tensor[:, 4] = self.tensor[:, 4] % 360\n        self.tensor[:, 4] = np.where(self.tensor[:, 4] > 180, self.tensor[:, 4] - 360, self.tensor[:, 4])\n\n        # Identify the indices of the boxes that are nearly horizontal\n        angle_mask = np.abs(self.tensor[:, 4]) <= clip_angle_threshold\n        angle_mask = np.where(angle_mask)[0]\n\n        # Convert the representation from (center x, center y, width, height, angle) to (x1, y1, x2, y2)\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2\n\n        # Clamp the x and y coordinates to ensure they do not exceed the specified box_size limits\n        x1 = np.clip(x1, 0, box_size[1])\n        y1 = np.clip(y1, 0, box_size[0])\n        x2 = np.clip(x2, 0, box_size[1])\n        y2 = np.clip(y2, 0, box_size[0])\n\n        # Convert back to the original representation\n        self.tensor[:, 0] = (x1 + x2) / 2\n        self.tensor[:, 1] = (y1 + y2) / 2\n        self.tensor[:, 2] = x2 - x1\n        self.tensor[:, 3] = y2 - y1\n\n        # Clip the angles for the boxes that are nearly horizontal\n        self.tensor["}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles to be within (-180, 180] degrees\n        self.tensor[:, -1] = (self.tensor[:, -1] + 180) % 360 - 180\n\n        # Identify the indices of the boxes that are nearly horizontal\n        nearly_horizontal_indices = torch.abs(self.tensor[:, -1]) <= clip_angle_threshold\n\n        # Convert the representation from (center x, center y, width, height, angle) to (x1, y1, x2, y2)\n        # where (x1, y1) and (x2, y2) are the coordinates of the top-left and bottom-right corners, respectively\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2\n\n        # Clamp the x and y coordinates to ensure they do not exceed the specified box_size limits\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y2 = torch.clamp(y2, min=0, max=box_size[0])\n\n        # Convert the boxes back to their original representation\n        self.tensor[:, 0] = (x1 + x2) / 2\n        self.tensor[:, 1] = (y1 + y2) / 2\n        self.tensor[:, 2] = x2 - x1\n        self.tensor[:, 3] = y2 - y1\n\n        # Clip the angles of the boxes that are nearly horizontal\n       "}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to be within (-180, 180]\n        self.tensor[:, 4] = torch.remainder(self.tensor[:, 4], 360)\n        self.tensor[:, 4] = torch.where(self.tensor[:, 4] > 180, self.tensor[:, 4] - 360, self.tensor[:, 4])\n\n        # Identify indices of boxes that are nearly horizontal\n        angle_mask = torch.abs(self.tensor[:, 4]) < clip_angle_threshold\n        angle_mask = angle_mask.nonzero().squeeze(1)\n\n        # Convert boxes to (x1, y1, x2, y2) representation\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2\n\n        # Clamp x and y coordinates\n        x1 = torch.clamp(x1, min=0, max=box_size[1] - 1)\n        y1 = torch.clamp(y1, min=0, max=box_size[0] - 1)\n        x2 = torch.clamp(x2, min=0, max=box_size[1] - 1)\n        y2 = torch.clamp(y2, min=0, max=box_size[0] - 1)\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor[:, 0] = (x1 + x2) / 2\n        self.tensor[:, 1] = (y1 + y2) / 2\n        self.tensor[:, 2] = x2 - x1\n        self.tensor[:, 3]"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to (-180, 180]\n        self.normalize_angles()\n\n        # Identify indices of nearly horizontal boxes\n        angle_mask = torch.abs(self.angle) < clip_angle_threshold\n        angle_mask = angle_mask.nonzero().squeeze(1)\n\n        # Convert to (x1, y1, x2, y2) representation\n        boxes_xyxy = self.to_xyxy()\n\n        # Clamp x and y coordinates\n        boxes_xyxy[:, 0::2] = boxes_xyxy[:, 0::2].clamp(min=0, max=box_size[1])\n        boxes_xyxy[:, 1::2] = boxes_xyxy[:, 1::2].clamp(min=0, max=box_size[0])\n\n        # Convert back to original representation\n        self.from_xyxy(boxes_xyxy)"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles to be within the range (-180, 180] degrees\n        self.tensor[:, 4] = self.tensor[:, 4] % 360\n        self.tensor[:, 4] = np.where(self.tensor[:, 4] > 180, self.tensor[:, 4] - 360, self.tensor[:, 4])\n\n        # Identify the indices of the boxes that are nearly horizontal\n        horizontal_indices = np.abs(self.tensor[:, 4]) < clip_angle_threshold\n\n        # Convert the representation from (center x, center y, width, height, angle) to (x1, y1, x2, y2)\n        # for nearly horizontal boxes\n        self.tensor[horizontal_indices, :4] = self.to_xyxy(self.tensor[horizontal_indices, :5])\n\n        # Clamp the x and y coordinates to ensure they do not exceed the specified box_size limits\n        self.tensor[:, 0] = np.clip(self.tensor[:, 0], 0, box_size[1])\n        self.tensor[:, 1] = np.clip(self.tensor[:, 1], 0, box_size[0])\n        self.tensor[:, 2] = np.clip(self.tensor[:, 2], 0, box_size[1])\n        self.tensor[:, 3] = np.clip(self.tensor[:, 3], 0, box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor[horizontal_indices, :4] = self.to_xywha(self.tensor[horizontal_indices, :5])\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to (-180, 180]\n        self.tensor[:, -1] = torch.remainder(self.tensor[:, -1], 360.0)\n        self.tensor[:, -1] = torch.where(self.tensor[:, -1] > 180.0, self.tensor[:, -1] - 360.0, self.tensor[:, -1])\n\n        # Identify boxes that are nearly horizontal\n        near_horizontal_indices = torch.abs(self.tensor[:, -1]) < clip_angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation\n        x1y1 = self.tensor[:, :2] - self.tensor[:, 2:] / 2\n        x2y2 = self.tensor[:, :2] + self.tensor[:, 2:] / 2\n        boxes = torch.cat([x1y1, x2y2], dim=1)\n\n        # Clamp x and y coordinates\n        boxes[:, 0::2] = boxes[:, 0::2].clamp(min=0, max=box_size[1])\n        boxes[:, 1::2] = boxes[:, 1::2].clamp(min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        xcyc = (boxes[:, :2] + boxes[:, 2:]) / 2\n        wh = boxes[:, 2:] - boxes[:, :2]\n        self.tensor = torch.cat([xcyc, wh, self.tensor[:, -1:]], dim=1)\n\n        # Convert back to original representation\n        x1y1 = self.tensor[:, :2] - self.tensor[:, 2:] / 2\n        x2y2 = self.tensor[:, :2] + self.tensor[:, 2:] / 2\n        boxes = torch.cat([x1y1, x2y2], dim=1)"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to (-180, 180]\n        self.tensor[:, 4] = self.tensor[:, 4] % 360\n        self.tensor[:, 4][self.tensor[:, 4] > 180] -= 360\n\n        # Identify nearly horizontal boxes\n        nearly_horizontal_indices = np.abs(self.tensor[:, 4]) < clip_angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation\n        boxes = self.convert_to_xyxy()\n\n        # Clamp x and y coordinates to ensure they do not exceed the box_size limits\n        boxes[:, 0::2] = np.clip(boxes[:, 0::2], 0, box_size[1])\n        boxes[:, 1::2] = np.clip(boxes[:, 1::2], 0, box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor = self.convert_to_cxcywha(boxes)\n\n        # Ensure numerical stability by converting back to the original representation\n        self.tensor = self.convert_to_cxcywha(self.tensor)"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to (-180, 180] degrees\n        self.tensor[:, 4] = self.tensor[:, 4] % 360\n        self.tensor[:, 4] = np.where(self.tensor[:, 4] > 180, self.tensor[:, 4] - 360, self.tensor[:, 4])\n\n        # Identify indices of boxes that are nearly horizontal\n        angle_threshold = np.deg2rad(clip_angle_threshold)\n        angle_cos = np.cos(self.tensor[:, 4] * np.pi / 180)\n        angle_sin = np.sin(self.tensor[:, 4] * np.pi / 180)\n        is_horizontal = np.abs(angle_cos) < angle_threshold\n        is_vertical = np.abs(angle_sin) < angle_threshold\n        is_nearly_horizontal = np.logical_and(is_horizontal, np.logical_not(is_vertical))\n        nearly_horizontal_indices = np.where(is_nearly_horizontal)[0]\n\n        # Convert to (x1, y1, x2, y2) representation\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2\n\n        # Clip x and y coordinates\n        x1 = np.clip(x1, 0, box_size[1] - 1)\n        y1 = np.clip(y1, 0, box_size[0] - 1)\n        x2 = np.clip(x2, 0, box_size[1] - 1)\n        y2 = np.clip(y2, 0, box_size[0] - "}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to (-180, 180]\n        self.tensor[:, 4] = (self.tensor[:, 4] + 180) % 360 - 180\n\n        # Identify nearly horizontal boxes\n        horizontal_mask = torch.abs(self.tensor[:, 4]) < clip_angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation for horizontal boxes\n        x1y1 = torch.stack([self.tensor[:, 0] - self.tensor[:, 2] / 2, self.tensor[:, 1] - self.tensor[:, 3] / 2], dim=1)\n        x2y2 = torch.stack([self.tensor[:, 0] + self.tensor[:, 2] / 2, self.tensor[:, 1] + self.tensor[:, 3] / 2], dim=1)\n\n        # Clamp x and y coordinates\n        x1y1 = torch.min(torch.max(x1y1, torch.zeros_like(x1y1)), torch.tensor(box_size, device=self.tensor.device))\n        x2y2 = torch.min(torch.max(x2y2, torch.zeros_like(x2y2)), torch.tensor(box_size, device=self.tensor.device))\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor[:, 0] = (x1y1[:, 0] + x2y2[:, 0]) / 2\n        self.tensor[:, 1] = (x1y1[:, 1] + x2y2[:, 1]) / 2\n        self.tensor[:, 2] = x2y2[:, 0] - x1y1[:, 0]\n        self.tensor[:, 3] = x2y2[:, 1] - x1y1[:, 1]\n\n        # Ensure numerical stability by converting"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.tensor = self.normalize_angle()\n        angle_threshold = clip_angle_threshold * np.pi / 180.0\n        angle_mask = np.abs(self.tensor[:, 4]) < angle_threshold\n        if np.any(angle_mask):\n            self.tensor[angle_mask, :4] = self.tensor[angle_mask, :4].clip(min=0, max=box_size)\n            self.tensor[angle_mask, :4] = self.tensor[angle_mask, :4].clip(max=np.array(box_size) - 1)\n            self.tensor[angle_mask, :2] -= self.tensor[angle_mask, 2:4] / 2\n            self.tensor[angle_mask, 2:] += self.tensor[angle_mask, :2]\n        self.tensor = self.to_xyxy()"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles to be within the range (-180, 180] degrees\n        self.tensor[:, 4] = (self.tensor[:, 4] + 180) % 360 - 180\n\n        # Identify the indices of the boxes that are nearly horizontal\n        is_horizontal = np.abs(self.tensor[:, 4]) <= clip_angle_threshold\n\n        # Convert the boxes to (x1, y1, x2, y2) representation for clipping\n        x1y1 = np.stack([self.tensor[:, 0] - self.tensor[:, 2] / 2, self.tensor[:, 1] - self.tensor[:, 3] / 2], axis=1)\n        x2y2 = np.stack([self.tensor[:, 0] + self.tensor[:, 2] / 2, self.tensor[:, 1] + self.tensor[:, 3] / 2], axis=1)\n\n        # Clip the x and y coordinates\n        x1y1 = np.maximum(x1y1, np.zeros_like(x1y1))\n        x2y2 = np.minimum(x2y2, np.array(box_size))\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor[:, :2] = (x1y1 + x2y2) / 2\n        self.tensor[:, 2:] = x2y2 - x1y1\n\n        # Convert the angles back to their original representation\n        self.tensor[:, 4] = (self.tensor[:, 4] + 180) % 360 - 180\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor[:, :2] = (x1y1 + x2y2) / 2\n        self.tensor[:, 2:] = x2y2 - x1y1"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles of the boxes to be within the range (-180, 180] degrees\n        self.normalize_angles()\n\n        # Identify the indices of the boxes that are nearly horizontal based on the clip_angle_threshold\n        horizontal_indices = torch.abs(self.tensor[:, 4]) < clip_angle_threshold\n\n        # Convert the representation from (center x, center y, width, height, angle) to (x1, y1, x2, y2)\n        x1y1 = self.tensor[:, :2] - self.tensor[:, 2:4] / 2\n        x2y2 = self.tensor[:, :2] + self.tensor[:, 2:4] / 2\n        boxes = torch.cat([x1y1, x2y2], dim=1)\n\n        # Clamp the x and y coordinates to ensure they do not exceed the specified box_size limits\n        boxes[:, 0::2].clamp_(min=0, max=box_size[1])\n        boxes[:, 1::2].clamp_(min=0, max=box_size[0])\n\n        # Convert back to the original representation\n        x1y1 = boxes[:, :2]\n        x2y2 = boxes[:, 2:4]\n        self.tensor = torch.cat([(x1y1 + x2y2) / 2, x2y2 - x1y1, self.tensor[:, 4:5]], dim=1)\n\n        # Convert the representation from (x1, y1, x2, y2) to (center x, center y, width, height, angle)\n        self.tensor[:, 2:4] = x2y2 - x1y1\n        self.tensor[:, :2] = (x1y1 + x2y2) / 2\n\n        # Convert the angles back to the original range (-180, 180] degrees\n        self.tensor[:, 4] = self.tensor[:, 4] % 360\n        self.tensor[:"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to [-180, 180]\n        self.tensor[:, -1] = torch.remainder(self.tensor[:, -1], 360)\n        self.tensor[:, -1] = torch.where(self.tensor[:, -1] > 180, self.tensor[:, -1] - 360, self.tensor[:, -1])\n\n        # Identify indices of nearly horizontal boxes\n        nearly_horizontal_indices = torch.abs(self.tensor[:, -1]) < clip_angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation\n        x1y1 = self.tensor[:, :2] - self.tensor[:, 2:] / 2\n        x2y2 = self.tensor[:, :2] + self.tensor[:, 2:] / 2\n\n        # Clamp x and y coordinates to ensure they do not exceed box_size limits\n        x1y1 = torch.clamp(x1y1, min=0, max=torch.tensor([box_size[0], box_size[1]]))\n        x2y2 = torch.clamp(x2y2, min=0, max=torch.tensor([box_size[0], box_size[1]]))\n\n        # Convert back to original representation\n        self.tensor[:, :2] = (x1y1 + x2y2) / 2\n        self.tensor[:, 2:] = x2y2 - x1y1\n\n        # Convert angles back to original representation\n        self.tensor[:, -1] = torch.where(self.tensor[:, -1] > 180, self.tensor[:, -1] - 360, self.tensor[:, -1])\n        self.tensor[:, -1] = torch.where(self.tensor[:, -1] < -180, self.tensor[:, -1] + 360, self.tensor[:, -1])\n\n        # Ensure numerical errors do not increase box sizes\n        self."}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to be within (-180, 180]\n        angles = self.tensor[:, -1]\n        angles = (angles + 180) % 360 - 180\n\n        # Identify indices of nearly horizontal boxes\n        is_near_horizontal = torch.abs(angles) < clip_angle_threshold\n        indices = torch.nonzero(is_near_horizontal).squeeze(1)\n\n        # Convert boxes to (x1, y1, x2, y2) representation\n        boxes = self.tensor.clone()\n        boxes[:, 0] -= boxes[:, 2] / 2\n        boxes[:, 1] -= boxes[:, 3] / 2\n        boxes[:, 2] += boxes[:, 0]\n        boxes[:, 3] += boxes[:, 1]\n\n        # Clamp x and y coordinates to fit within the box size\n        boxes[:, 0].clamp_(min=0, max=box_size[1])\n        boxes[:, 1].clamp_(min=0, max=box_size[0])\n        boxes[:, 2].clamp_(min=boxes[:, 0] + 1)\n        boxes[:, 3].clamp_(min=boxes[:, 1] + 1)\n\n        # Convert boxes back to (center x, center y, width, height, angle) representation\n        boxes[:, 0] = (boxes[:, 0] + boxes[:, 2]) / 2\n        boxes[:, 1] = (boxes[:, 1] + boxes[:, 3]) / 2\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n\n        # Update tensor attribute with clipped boxes\n        self.tensor[indices] = boxes[indices]\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles to be within the range (-180, 180]\n        self.tensor[:, 4] = torch.fmod(self.tensor[:, 4] + 180, 360) - 180\n\n        # Identify the indices of the boxes that are nearly horizontal\n        nearly_horizontal_indices = torch.abs(self.tensor[:, 4]) <= clip_angle_threshold\n\n        # Convert the boxes to (x1, y1, x2, y2) representation for clipping\n        x1y1 = self.tensor[:, :2] - self.tensor[:, 2:] / 2\n        x2y2 = self.tensor[:, :2] + self.tensor[:, 2:] / 2\n\n        # Clamp the x and y coordinates to ensure they do not exceed the box_size limits\n        x1y1 = torch.min(torch.max(x1y1, torch.zeros_like(x1y1)), torch.tensor(box_size, device=x1y1.device) - 1)\n        x2y2 = torch.min(torch.max(x2y2, torch.zeros_like(x2y2)), torch.tensor(box_size, device=x2y2.device) - 1)\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor[:, :2] = (x1y1 + x2y2) / 2\n        self.tensor[:, 2:] = x2y2 - x1y1\n\n        # Convert back to the original representation\n        self.tensor[:, 2] = self.tensor[:, 2].clamp(min=0)\n        self.tensor[:, 3] = self.tensor[:, 3].clamp(min=0)\n\n        # Convert back to the original representation\n        self.tensor[:, :2] = self.tensor[:, :2].clamp(min=0, max=box_size[0] - 1)\n        self"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to (-180, 180] degrees\n        self.angle = torch.atan2(torch.sin(self.angle), torch.cos(self.angle))\n\n        # Identify boxes that are nearly horizontal\n        is_near_horizontal = torch.abs(self.angle) < clip_angle_threshold\n        is_near_horizontal = is_near_horizontal.view(-1, 1)\n\n        # Convert to (x1, y1, x2, y2) representation\n        x1 = self.center_x - self.width / 2\n        y1 = self.center_y - self.height / 2\n        x2 = self.center_x + self.width / 2\n        y2 = self.center_y + self.height / 2\n        boxes = torch.stack([x1, y1, x2, y2], dim=1)\n\n        # Clamp x and y coordinates\n        boxes[:, 0::2] = boxes[:, 0::2].clamp(min=0, max=box_size[1])\n        boxes[:, 1::2] = boxes[:, 1::2].clamp(min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.center_x = (boxes[:, 0] + boxes[:, 2]) / 2\n        self.center_y = (boxes[:, 1] + boxes[:, 3]) / 2\n        self.width = boxes[:, 2] - boxes[:, 0]\n        self.height = boxes[:, 3] - boxes[:, 1]\n        self.angle = torch.atan2(torch.sin(self.angle), torch.cos(self.angle))\n\n        # Convert back to original representation\n        self.angle = torch.atan2(torch.sin(self.angle), torch.cos(self.angle))\n        self.angle[is_near_horizontal[:, 0]] = torch.atan"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to be within (-180, 180] degrees\n        self.angle = torch.atan2(torch.sin(self.angle), torch.cos(self.angle))\n        self.angle = torch.remainder(self.angle, 2 * np.pi)\n\n        # Identify indices of boxes that are nearly horizontal\n        horizontal_indices = torch.abs(self.angle) <= clip_angle_threshold\n\n        # Convert boxes to (x1, y1, x2, y2) representation for clipping\n        x1, y1, x2, y2 = self.convert_to_xyxy()\n\n        # Clamp x and y coordinates to ensure they do not exceed box_size limits\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y2 = torch.clamp(y2, min=0, max=box_size[0])\n\n        # Convert boxes back to (center x, center y, width, height, angle) representation\n        self.x = (x1 + x2) / 2\n        self.y = (y1 + y2) / 2\n        self.width = x2 - x1\n        self.height = y2 - y1\n\n        # Set angles of horizontal boxes to 0\n        self.angle[horizontal_indices] = 0\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.convert_from_xyxy()\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angle()\n        indices = torch.abs(self.angle) < clip_angle_threshold\n        x1, y1, x2, y2 = self.convert_to_xyxy()\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y2 = torch.clamp(y2, min=0, max=box_size[0])\n        self.convert_to_cxcywha(x1=x1, y1=y1, x2=x2, y2=y2, indices=indices)\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary\n        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        # Iterate over the data and update the statistics\n        for item in self.data:\n            statistics[item['type']] += 1\n\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize a dictionary to store the statistics\n        statistics = {}\n\n        # Loop through the data and count the occurrences of each type\n        for item in self.data:\n            if item['type'] not in statistics:\n                statistics[item['type']] = 0\n            statistics[item['type']] += 1\n\n        # Return the statistics dictionary\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary with default values\n        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        # Iterate over the data and update the statistics\n        for item in self.data:\n            if 'type' in item:\n                statistics[item['type']] += 1\n\n        # Return the updated statistics\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary\n        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        # Loop through the data and update the statistics\n        for item in self.data:\n            statistics[item['type']] += 1\n\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for data in self.data:\n            if data['type'] in statistics:\n                statistics[data['type']] += 1\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary with default values\n        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        # Iterate through the data and update the statistics\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n\n        # Return the updated statistics\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize a dictionary to store the statistics\n        statistics = {}\n\n        # Loop through the types and update the statistics\n        for type in self.data:\n            if type in statistics:\n                statistics[type] += 1\n            else:\n                statistics[type] = 1\n\n        # Return the statistics\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary\n        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        # Iterate over the data\n        for i in self.data:\n\n            # Check if the type is in the statistics dictionary\n            if i['type'] in statistics:\n\n                # Increment the count for the type\n                statistics[i['type']] += 1\n\n        # Return the statistics dictionary\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Count the occurrences of each type\n        doc_count = sum(1 for item in self.data if item['type'] == 'doc')\n        gen_count = sum(1 for item in self.data if item['type'] == 'gen')\n        kno_count = sum(1 for item in self.data if item['type'] == 'kno')\n        num_count = sum(1 for item in self.data if item['type'] == 'num')\n\n        # Update the statistics\n        self.statistics = {\n            'doc': doc_count,\n            'gen': gen_count,\n            'kno': kno_count,\n            'num': num_count\n        }\n\n        return self.statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        for item in self.data:\n            statistics[item['type']] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize a dictionary to store the statistics\n        statistics = {}\n\n        # Iterate over the data and count the occurrences of each type\n        for item in self.data:\n            type = item['type']\n            if type in statistics:\n                statistics[type] += 1\n            else:\n                statistics[type] = 1\n\n        # Return the statistics dictionary\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary\n        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        # Iterate over the data and update the statistics\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n\n        # Return the statistics dictionary\n        return statistics\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary\n        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        # Count the occurrences of each type\n        for item in self.data:\n            if item['type'] == 'doc':\n                statistics['doc'] += 1\n            elif item['type'] == 'gen':\n                statistics['gen'] += 1\n            elif item['type'] == 'kno':\n                statistics['kno'] += 1\n            elif item['type'] == 'num':\n                statistics['num'] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary\n        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        # Iterate through the data and update the statistics\n        for item in self.data:\n            statistics[item['type']] += 1\n\n        # Return the statistics dictionary\n        return statistics\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {}\n        for i in self.data:\n            if i['type'] in statistics:\n                statistics[i['type']] += 1\n            else:\n                statistics[i['type']] = 1\n        return statistics\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary\n        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        # Iterate over the data and update the statistics\n        for item in self.data:\n            if item['type'] == 'doc':\n                statistics['doc'] += 1\n            elif item['type'] == 'gen':\n                statistics['gen'] += 1\n            elif item['type'] == 'kno':\n                statistics['kno'] += 1\n            elif item['type'] == 'num':\n                statistics['num'] += 1\n\n        # Return the statistics dictionary\n        return statistics\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        self.statistics = {}\n        for type_ in ['doc', 'gen', 'kno', 'num']:\n            self.statistics[type_] = len([i for i in self.data if i['type'] == type_])\n\n        return self.statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary with default values\n        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        # Iterate over the data and update the statistics dictionary\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n\n        return statistics\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {}\n        for d in self.data:\n            if d[\"type\"] in statistics:\n                statistics[d[\"type\"]] += 1\n            else:\n                statistics[d[\"type\"]] = 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Create a dictionary to store the statistics\n        stats = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        # Iterate over the data and update the statistics\n        for obj in self.data:\n            stats[obj['type']] += 1\n\n        # Return the statistics dictionary\n        return stats\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS:\n        return NECKS[cfg['type']](cfg)\n    else:\n        return MMDET_NECKS[cfg['type']](cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS:\n        return NECKS[cfg['type']](cfg)\n    else:\n        return MMDET_NECKS[cfg['type']](cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    neck_type = cfg['type']\n    if neck_type in NECKS:\n        neck = NECKS[neck_type](cfg)\n    else:\n        neck = MMDET_NECKS[neck_type](cfg)\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    neck_type = cfg['type']\n    if neck_type in NECKS:\n        neck = NECKS[neck_type](cfg)\n    else:\n        neck = MMDET_NECKS[neck_type](cfg)\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    neck_type = cfg['type']\n    if neck_type in NECKS:\n        neck = NECKS[neck_type](cfg)\n    else:\n        neck = MMDET_NECKS[neck_type](cfg)\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    neck_type = cfg['type']\n    if neck_type in NECKS:\n        return NECKS[neck_type](cfg)\n    elif neck_type in MMDET_NECKS:\n        return MMDET_NECKS[neck_type](cfg)\n    else:\n        raise NotImplementedError('Neck type {} is not supported'.format(neck_type))"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from .necks import NECKS, MMDET_NECKS\n    if cfg['type'] in NECKS:\n        neck = NECKS[cfg['type']](**cfg)\n    else:\n        neck = MMDET_NECKS[cfg['type']](**cfg)\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from .necks import NECKS, MMDET_NECKS\n\n    neck_type = cfg['type']\n\n    if neck_type in NECKS:\n        neck = NECKS[neck_type](cfg)\n    elif neck_type in MMDET_NECKS:\n        neck = MMDET_NECKS[neck_type](cfg)\n    else:\n        raise NotImplementedError(f\"Neck type {neck_type} is not supported.\")\n\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from .necks import NECKS, MMDET_NECKS\n\n    neck_type = cfg['type']\n    if neck_type in NECKS:\n        neck = NECKS[neck_type](cfg)\n    elif neck_type in MMDET_NECKS:\n        neck = MMDET_NECKS[neck_type](cfg)\n    else:\n        raise NotImplementedError\n\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from .necks import NECKS, MMDET_NECKS\n    neck_type = cfg.type\n    if neck_type in NECKS:\n        neck = NECKS[neck_type](cfg)\n    elif neck_type in MMDET_NECKS:\n        neck = MMDET_NECKS[neck_type](cfg)\n    else:\n        raise NotImplementedError('Neck type {} is not supported'.format(neck_type))\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from .necks import NECKS, MMDET_NECKS\n\n    neck_type = cfg.type\n    neck = None\n    if neck_type in NECKS:\n        neck = NECKS[neck_type](cfg)\n    elif neck_type in MMDET_NECKS:\n        neck = MMDET_NECKS[neck_type](cfg)\n    else:\n        raise NotImplementedError(f\"Neck {neck_type} not available\")\n\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from .mmdet_neck import MMDetNeck\n    from .necks import NECKS\n\n    neck_type = cfg.type\n    neck = None\n\n    if neck_type in NECKS:\n        neck = NECKS[neck_type](cfg)\n    else:\n        neck = MMDetNeck(cfg)\n\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from . import NECKS, MMDET_NECKS\n\n    neck_type = cfg.type\n    neck_cfg = cfg.copy()\n\n    if neck_type in NECKS:\n        neck = NECKS[neck_type](neck_cfg)\n    elif neck_type in MMDET_NECKS:\n        neck = MMDET_NECKS[neck_type](neck_cfg)\n    else:\n        raise NotImplementedError(f\"Neck type {neck_type} not found in either NECKS or MMDET_NECKS\")\n\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg.type in NECKS:\n        neck = NECKS[cfg.type](cfg)\n    else:\n        neck = MMDET_NECKS[cfg.type](cfg)\n\n    return neck\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from ..necks import NECKS, MMDET_NECKS\n\n    neck_type = cfg['type']\n\n    if neck_type in NECKS:\n        neck = NECKS[neck_type](cfg)\n    elif neck_type in MMDET_NECKS:\n        neck = MMDET_NECKS[neck_type](cfg)\n    else:\n        raise NotImplementedError(f'Neck type {neck_type} is not supported.')\n\n    return neck\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from mmcv.utils import build_from_cfg\n    from mmpose.models.necks import NECKS\n    from mmpose.models.necks.mmdet_necks import MMDET_NECKS\n    neck = cfg.neck\n    if neck['type'] in NECKS:\n        return build_from_cfg(neck, NECKS)\n    elif neck['type'] in MMDET_NECKS:\n        return build_from_cfg(neck, MMDET_NECKS)\n    else:\n        raise NotImplementedError(f'Neck {neck[\"type\"]} is not supported.')"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from mmcv.utils import build_from_cfg\n    from mmtrack.models.builder import NECKS, MMDET_NECKS\n\n    neck_cfg = cfg.copy()\n    neck_type = neck_cfg.pop('type')\n    if neck_type in NECKS:\n        return build_from_cfg(neck_cfg, NECKS, [MMDET_NECKS])\n    elif neck_type in MMDET_NECKS:\n        return build_from_cfg(neck_cfg, MMDET_NECKS)\n    else:\n        raise NotImplementedError(f'Unsupported neck type {neck_type}')"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    from mmpose.models.necks import NECKS, MMDET_NECKS\n    neck = cfg.neck\n    if neck['type'] in NECKS:\n        neck_cls = NECKS[neck['type']]\n    else:\n        neck_cls = MMDET_NECKS[neck['type']]\n    return neck_cls(**neck)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS:\n        neck = NECKS[cfg['type']](**cfg)\n    else:\n        from mmcv.runner import load_checkpoint\n        from mmcv.cnn import MODELS as MMCV_MODELS\n        from mmdet.models import MMDET_NECKS\n        neck = MMDET_NECKS.build(cfg)\n        if cfg.get('init_cfg', None):\n            neck = load_checkpoint(neck, cfg.init_cfg)\n    return neck"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    neck_type = cfg.get('type', None)\n    if neck_type is None:\n        raise ValueError(f\"The 'type' key must be specified in the neck configuration dictionary.\")\n\n    neck_type = neck_type.lower()\n    if neck_type in NECKS:\n        return NECKS[neck_type](cfg)\n    elif neck_type in MMDET_NECKS:\n        return MMDET_NECKS[neck_type](cfg)\n    else:\n        raise ValueError(f\"Neck type '{neck_type}' is not available.\")"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg['type']\n    if loss_type == 'CrossEntropyLoss':\n        return CrossEntropyLoss()\n    elif loss_type == 'BCEWithLogitsLoss':\n        return BCEWithLogitsLoss()\n    else:\n        raise NotImplementedError"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] == 'L2':\n        return L2Loss()\n    elif cfg['type'] == 'L1':\n        return L1Loss()\n    elif cfg['type'] == 'L2_L1':\n        return L2_L1Loss()\n    elif cfg['type'] == 'L2_L1_L2':\n        return L2_L1_L2Loss()\n    elif cfg['type'] == 'L2_L1_L2_L1':\n        return L2_L1_L2_L1Loss()\n    elif cfg['type'] == 'L2_L1_L2_L1_L2':\n        return L2_L1_L2_L1_L2Loss()\n    elif cfg['type'] == 'L2_L1_L2_L1_L2_L1':\n        return L2_L1_L2_L1_L2_L1Loss()\n    elif cfg['type'] == 'L2_L1_L2_L1_L2_L1_L2':\n        return L2_L1_L2_L1_L2_L1_L2Loss()\n    elif cfg['type'] == 'L2_L1_L2_L1_L2_L1_L2_L1':\n        return L2_L1_L2_L1_L2_L1_L2_L1Loss()\n    elif cfg['type'] == 'L2_L1_L2_L1_L2_L1_L2_L1_L2':\n        return L2_L1_L2_L1_L2_L1_L2_L1_L2Loss()\n    elif cfg['type'] == 'L2_L1_L2_L1_L2_L1_L2_L1_L2_L1':\n        return L2_L1_L2_L1_L2_L1_L2_L1_L2_L1Loss"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg[\"type\"] == \"L1\":\n        return L1Loss()\n    elif cfg[\"type\"] == \"L2\":\n        return L2Loss()\n    elif cfg[\"type\"] == \"L1L2\":\n        return L1L2Loss()\n    elif cfg[\"type\"] == \"L1L2_smooth\":\n        return L1L2SmoothLoss()\n    elif cfg[\"type\"] == \"L1L2_smooth_v2\":\n        return L1L2SmoothLossV2()\n    elif cfg[\"type\"] == \"L1L2_smooth_v3\":\n        return L1L2SmoothLossV3()\n    elif cfg[\"type\"] == \"L1L2_smooth_v4\":\n        return L1L2SmoothLossV4()\n    elif cfg[\"type\"] == \"L1L2_smooth_v5\":\n        return L1L2SmoothLossV5()\n    elif cfg[\"type\"] == \"L1L2_smooth_v6\":\n        return L1L2SmoothLossV6()\n    elif cfg[\"type\"] == \"L1L2_smooth_v7\":\n        return L1L2SmoothLossV7()\n    elif cfg[\"type\"] == \"L1L2_smooth_v8\":\n        return L1L2SmoothLossV8()\n    elif cfg[\"type\"] == \"L1L2_smooth_v9\":\n        return L1L2SmoothLossV9()\n    elif cfg[\"type\"] == \"L1L2_smooth_v10\":\n        return L1L2SmoothLossV10()\n    elif cfg[\"type\"] == \"L1L2_smooth_v11\":\n        return L1L2SmoothLossV11()\n    elif cfg[\"type\"] == \"L1L2_smooth_v12\":\n        return L1L2SmoothLossV12()\n    elif cfg[\"type\"] == \"L1L2_smooth_v13\":\n       "}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg['loss_type']\n\n    if loss_type == 'cross_entropy':\n        loss = nn.CrossEntropyLoss()\n    elif loss_type == 'mse':\n        loss = nn.MSELoss()\n    elif loss_type == 'l1':\n        loss = nn.L1Loss()\n    else:\n        raise NotImplementedError\n\n    return loss"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] == 'CrossEntropyLoss':\n        return torch.nn.CrossEntropyLoss()\n    elif cfg['type'] == 'BCEWithLogitsLoss':\n        return torch.nn.BCEWithLogitsLoss()\n    elif cfg['type'] == 'L1Loss':\n        return torch.nn.L1Loss()\n    elif cfg['type'] == 'MSELoss':\n        return torch.nn.MSELoss()\n    elif cfg['type'] == 'NLLLoss':\n        return torch.nn.NLLLoss()\n    elif cfg['type'] == 'KLDivLoss':\n        return torch.nn.KLDivLoss()\n    elif cfg['type'] == 'CTCLoss':\n        return torch.nn.CTCLoss()\n    elif cfg['type'] == 'BCELoss':\n        return torch.nn.BCELoss()\n    elif cfg['type'] == 'MarginRankingLoss':\n        return torch.nn.MarginRankingLoss()\n    elif cfg['type'] == 'HingeEmbeddingLoss':\n        return torch.nn.HingeEmbeddingLoss()\n    elif cfg['type'] == 'MultiLabelMarginLoss':\n        return torch.nn.MultiLabelMarginLoss()\n    elif cfg['type'] == 'MultiLabelSoftMarginLoss':\n        return torch.nn.MultiLabelSoftMarginLoss()\n    elif cfg['type'] == 'MultiMarginLoss':\n        return torch.nn.MultiMarginLoss()\n    elif cfg['type'] == 'NLLLoss2d':\n        return torch.nn.NLLLoss2d()\n    elif cfg['type'] == 'PoissonNLLLoss':\n        return torch.nn.PoissonNLLLoss()\n    elif cfg['type'] == 'SmoothL1Loss':\n        return torch.nn.SmoothL1Loss()\n    elif cfg['type']"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg[\"loss_type\"]\n    if loss_type == \"cross_entropy\":\n        return CrossEntropyLoss()\n    elif loss_type == \"focal_loss\":\n        return FocalLoss(alpha=cfg[\"alpha\"], gamma=cfg[\"gamma\"])\n    elif loss_type == \"dice_loss\":\n        return DiceLoss()\n    elif loss_type == \"tversky_loss\":\n        return TverskyLoss(alpha=cfg[\"alpha\"], beta=cfg[\"beta\"])\n    elif loss_type == \"combo_loss\":\n        return ComboLoss(alpha=cfg[\"alpha\"], beta=cfg[\"beta\"], gamma=cfg[\"gamma\"])\n    else:\n        raise ValueError(f\"Loss type {loss_type} not recognized.\")"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg['type']\n    loss_cfg = cfg.get('kwargs', {})\n    if loss_type == 'CrossEntropyLoss':\n        from torch.nn import CrossEntropyLoss\n        return CrossEntropyLoss(**loss_cfg)\n    elif loss_type == 'MSELoss':\n        from torch.nn import MSELoss\n        return MSELoss(**loss_cfg)\n    elif loss_type == 'L1Loss':\n        from torch.nn import L1Loss\n        return L1Loss(**loss_cfg)\n    elif loss_type == 'BCELoss':\n        from torch.nn import BCELoss\n        return BCELoss(**loss_cfg)\n    elif loss_type == 'BCEWithLogitsLoss':\n        from torch.nn import BCEWithLogitsLoss\n        return BCEWithLogitsLoss(**loss_cfg)\n    elif loss_type == 'NLLLoss':\n        from torch.nn import NLLLoss\n        return NLLLoss(**loss_cfg)\n    elif loss_type == 'CTCLoss':\n        from torch.nn import CTCLoss\n        return CTCLoss(**loss_cfg)\n    elif loss_type == 'NLLLoss2d':\n        from torch.nn import NLLLoss2d\n        return NLLLoss2d(**loss_cfg)\n    elif loss_type == 'PoissonNLLLoss':\n        from torch.nn import PoissonNLLLoss\n        return PoissonNLLLoss(**loss_cfg)\n    elif loss_type == 'GaussianNLLLoss':\n        from torch.nn import GaussianNLLLoss\n        return GaussianNLLLoss(**loss_cfg)\n    elif loss_type == 'KLDivLoss':\n        from torch.nn import KLDivLoss\n        return KLDivLoss(**loss_cfg)\n    elif loss_type == 'MultiLabelSoftMarginLoss':\n        from torch.nn import MultiLabelSoftMarginL"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg['loss_type']\n    if loss_type == 'mse':\n        from .mse import MSE\n        return MSE()\n    elif loss_type == 'l1':\n        from .l1 import L1\n        return L1()\n    elif loss_type == 'bce':\n        from .bce import BCE\n        return BCE()\n    elif loss_type == 'bce_logits':\n        from .bce_logits import BCELogits\n        return BCELogits()\n    elif loss_type == 'cross_entropy':\n        from .cross_entropy import CrossEntropy\n        return CrossEntropy()\n    elif loss_type == 'cross_entropy_logits':\n        from .cross_entropy_logits import CrossEntropyLogits\n        return CrossEntropyLogits()\n    elif loss_type == 'focal':\n        from .focal import Focal\n        return Focal()\n    elif loss_type == 'focal_logits':\n        from .focal_logits import FocalLogits\n        return FocalLogits()\n    elif loss_type == 'dice':\n        from .dice import Dice\n        return Dice()\n    elif loss_type == 'dice_logits':\n        from .dice_logits import DiceLogits\n        return DiceLogits()\n    elif loss_type == 'tversky':\n        from .tversky import Tversky\n        return Tversky()\n    elif loss_type == 'tversky_logits':\n        from .tversky_logits import TverskyLogits\n        return TverskyLogits()\n    elif loss_type == 'jaccard':\n        from .jaccard import Jaccard\n        return Jaccard()\n    elif loss_type == 'jaccard_logits':\n        from .jaccard_logits import JaccardLogits\n        return JaccardLogits()\n    elif loss_type == 'lovasz':\n        from .lovasz import Lovasz\n        return Lovasz()\n    elif"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] == 'CrossEntropyLoss':\n        return nn.CrossEntropyLoss()\n    elif cfg['type'] == 'MSELoss':\n        return nn.MSELoss()\n    elif cfg['type'] == 'BCEWithLogitsLoss':\n        return nn.BCEWithLogitsLoss()\n    elif cfg['type'] == 'L1Loss':\n        return nn.L1Loss()\n    elif cfg['type'] == 'SmoothL1Loss':\n        return nn.SmoothL1Loss()\n    elif cfg['type'] == 'CTCLoss':\n        return nn.CTCLoss()\n    elif cfg['type'] == 'NLLLoss':\n        return nn.NLLLoss()\n    elif cfg['type'] == 'NLLLoss2d':\n        return nn.NLLLoss2d()\n    elif cfg['type'] == 'PoissonNLLLoss':\n        return nn.PoissonNLLLoss()\n    elif cfg['type'] == 'MultiLabelMarginLoss':\n        return nn.MultiLabelMarginLoss()\n    elif cfg['type'] == 'MultiLabelSoftMarginLoss':\n        return nn.MultiLabelSoftMarginLoss()\n    elif cfg['type'] == 'MultiMarginLoss':\n        return nn.MultiMarginLoss()\n    elif cfg['type'] == 'MultiLabelSoftMarginLoss':\n        return nn.MultiLabelSoftMarginLoss()\n    elif cfg['type'] == 'SoftMarginLoss':\n        return nn.SoftMarginLoss()\n    elif cfg['type'] == 'TripletMarginLoss':\n        return nn.TripletMarginLoss()\n    elif cfg['type'] == 'TripletMarginWithDistanceLoss':\n        return nn.TripletMarginWithDistanceLoss()\n    elif cfg['type'] == 'HingeEmbeddingLoss':\n        return nn.HingeEmbeddingL"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg['type']\n    loss_args = cfg['args']\n    if loss_type == 'MSELoss':\n        return torch.nn.MSELoss(**loss_args)\n    elif loss_type == 'CrossEntropyLoss':\n        return torch.nn.CrossEntropyLoss(**loss_args)\n    elif loss_type == 'NLLLoss':\n        return torch.nn.NLLLoss(**loss_args)\n    elif loss_type == 'L1Loss':\n        return torch.nn.L1Loss(**loss_args)\n    elif loss_type == 'SmoothL1Loss':\n        return torch.nn.SmoothL1Loss(**loss_args)\n    elif loss_type == 'PoissonNLLLoss':\n        return torch.nn.PoissonNLLLoss(**loss_args)\n    elif loss_type == 'KLDivLoss':\n        return torch.nn.KLDivLoss(**loss_args)\n    elif loss_type == 'BCELoss':\n        return torch.nn.BCELoss(**loss_args)\n    elif loss_type == 'BCEWithLogitsLoss':\n        return torch.nn.BCEWithLogitsLoss(**loss_args)\n    elif loss_type == 'CTCLoss':\n        return torch.nn.CTCLoss(**loss_args)\n    elif loss_type == 'MarginRankingLoss':\n        return torch.nn.MarginRankingLoss(**loss_args)\n    elif loss_type == 'HingeEmbeddingLoss':\n        return torch.nn.HingeEmbeddingLoss(**loss_args)\n    elif loss_type == 'MultiLabelMarginLoss':\n        return torch.nn.MultiLabelMarginLoss(**loss_args)\n    elif loss_type == 'MultiMarginLoss':\n        return torch.nn.MultiMarginLoss(**loss_args)\n    elif loss_type == 'MultiLabel"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] == 'CrossEntropyLoss':\n        return CrossEntropyLoss(cfg['args'])\n    elif cfg['type'] == 'FocalLoss':\n        return FocalLoss(cfg['args'])\n    elif cfg['type'] == 'LabelSmoothingCrossEntropyLoss':\n        return LabelSmoothingCrossEntropyLoss(cfg['args'])\n    elif cfg['type'] == 'MultiLabelSoftMarginLoss':\n        return MultiLabelSoftMarginLoss(cfg['args'])\n    elif cfg['type'] == 'MultiLabelSoftMarginLoss':\n        return MultiLabelSoftMarginLoss(cfg['args'])\n    elif cfg['type'] == 'MultiMarginLoss':\n        return MultiMarginLoss(cfg['args'])\n    elif cfg['type'] == 'NLLLoss':\n        return NLLLoss(cfg['args'])\n    elif cfg['type'] == 'PoissonNLLLoss':\n        return PoissonNLLLoss(cfg['args'])\n    elif cfg['type'] == 'SmoothL1Loss':\n        return SmoothL1Loss(cfg['args'])\n    elif cfg['type'] == 'SoftMarginLoss':\n        return SoftMarginLoss(cfg['args'])\n    elif cfg['type'] == 'TripletMarginLoss':\n        return TripletMarginLoss(cfg['args'])\n    elif cfg['type'] == 'MarginRankingLoss':\n        return MarginRankingLoss(cfg['args'])\n    elif cfg['type'] == 'HingeEmbeddingLoss':\n        return HingeEmbeddingLoss(cfg['args'])\n    elif cfg['type'] == 'MultiLabelMarginLoss':\n        return MultiLabelMarginLoss(cfg['args'])\n    elif cfg['type'] == 'CosineEmbeddingLoss':\n        return CosineEmbeddingLoss(cfg['args'])\n    elif cfg['type'] == 'CTCLoss':\n        return CTCLoss(cfg['args'])\n    elif cfg['"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg['type']\n    if loss_type == 'CrossEntropyLoss':\n        return torch.nn.CrossEntropyLoss()\n    elif loss_type == 'FocalLoss':\n        return FocalLoss(cfg['alpha'], cfg['gamma'])\n    elif loss_type == 'MultiLabelSoftMarginLoss':\n        return torch.nn.MultiLabelSoftMarginLoss()\n    elif loss_type == 'MultiLabelSoftMarginLoss_weighted':\n        return torch.nn.MultiLabelSoftMarginLoss(weight=cfg['weight'])\n    elif loss_type == 'MultiLabelSoftMarginLoss_weighted_reduction':\n        return torch.nn.MultiLabelSoftMarginLoss(weight=cfg['weight'], reduction=cfg['reduction'])\n    elif loss_type == 'MultiLabelSoftMarginLoss_weighted_reduction_size_average':\n        return torch.nn.MultiLabelSoftMarginLoss(weight=cfg['weight'], reduction=cfg['reduction'], size_average=cfg['size_average'])\n    elif loss_type == 'MultiLabelSoftMarginLoss_weighted_reduction_size_average_ignore_index':\n        return torch.nn.MultiLabelSoftMarginLoss(weight=cfg['weight'], reduction=cfg['reduction'], size_average=cfg['size_average'], ignore_index=cfg['ignore_index'])\n    elif loss_type == 'MultiLabelSoftMarginLoss_weighted_reduction_size_average_ignore_index_reduction':\n        return torch.nn.MultiLabelSoftMarginLoss(weight=cfg['weight'], reduction=cfg['reduction'], size_average=cfg['size_average'], ignore_index=cfg['ignore_index'], reduction=cfg['reduction'])\n    elif loss_type == 'MultiLabelSoftMarginLoss_weighted_reduction_size_average_ignore_index_reduction_size_average':\n        return torch.nn.MultiLabelSoftMarginLoss(weight=cfg['weight'], reduction=cfg['"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] == 'cross_entropy':\n        return CrossEntropyLoss()\n    elif cfg['type'] == 'focal_loss':\n        return FocalLoss(cfg['alpha'], cfg['gamma'])\n    elif cfg['type'] == 'weighted_cross_entropy':\n        return WeightedCrossEntropyLoss()\n    elif cfg['type'] == 'weighted_focal_loss':\n        return WeightedFocalLoss(cfg['alpha'], cfg['gamma'])\n    else:\n        raise NotImplementedError(f\"Loss function {cfg['type']} not implemented.\")\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg['loss_type']\n    if loss_type == 'l1':\n        from .l1_loss import L1Loss\n        loss_function = L1Loss()\n    elif loss_type == 'l2':\n        from .l2_loss import L2Loss\n        loss_function = L2Loss()\n    elif loss_type == 'ssim':\n        from .ssim_loss import SSIMLoss\n        loss_function = SSIMLoss()\n    elif loss_type == 'charbonnier':\n        from .charbonnier_loss import CharbonnierLoss\n        loss_function = CharbonnierLoss()\n    elif loss_type == 'l1_l2':\n        from .l1_l2_loss import L1L2Loss\n        loss_function = L1L2Loss()\n    elif loss_type == 'l1_l2_ssim':\n        from .l1_l2_ssim_loss import L1L2SSIMLoss\n        loss_function = L1L2SSIMLoss()\n    elif loss_type == 'l1_ssim':\n        from .l1_ssim_loss import L1SSIMLoss\n        loss_function = L1SSIMLoss()\n    elif loss_type == 'l1_l2_ssim_charbonnier':\n        from .l1_l2_ssim_charbonnier_loss import L1L2SSIMCharbonnierLoss\n        loss_function = L1L2SSIMCharbonnierLoss()\n    else:\n        raise NotImplementedError('Loss type {} is not implemented'.format(loss_type))\n    return loss_function"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg['type']\n    if loss_type == 'cross_entropy':\n        from .cross_entropy_loss import CrossEntropyLoss\n        return CrossEntropyLoss(cfg)\n    elif loss_type == 'focal_loss':\n        from .focal_loss import FocalLoss\n        return FocalLoss(cfg)\n    elif loss_type == 'bce_loss':\n        from .bce_loss import BCELoss\n        return BCELoss(cfg)\n    elif loss_type == 'dice_loss':\n        from .dice_loss import DiceLoss\n        return DiceLoss(cfg)\n    elif loss_type == 'tversky_loss':\n        from .tversky_loss import TverskyLoss\n        return TverskyLoss(cfg)\n    elif loss_type == 'focal_tversky_loss':\n        from .focal_tversky_loss import FocalTverskyLoss\n        return FocalTverskyLoss(cfg)\n    else:\n        raise NotImplementedError\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg[\"type\"] == \"l1\":\n        return l1_loss\n    elif cfg[\"type\"] == \"l2\":\n        return l2_loss\n    elif cfg[\"type\"] == \"mse\":\n        return mse_loss\n    elif cfg[\"type\"] == \"bce\":\n        return bce_loss\n    elif cfg[\"type\"] == \"bce_logit\":\n        return bce_logit_loss\n    elif cfg[\"type\"] == \"cross_entropy\":\n        return cross_entropy_loss\n    elif cfg[\"type\"] == \"cross_entropy_logit\":\n        return cross_entropy_logit_loss\n    elif cfg[\"type\"] == \"focal_loss\":\n        return focal_loss\n    elif cfg[\"type\"] == \"focal_loss_logit\":\n        return focal_loss_logit\n    elif cfg[\"type\"] == \"dice\":\n        return dice_loss\n    elif cfg[\"type\"] == \"dice_logit\":\n        return dice_logit_loss\n    elif cfg[\"type\"] == \"tversky\":\n        return tversky_loss\n    elif cfg[\"type\"] == \"tversky_logit\":\n        return tversky_logit_loss\n    elif cfg[\"type\"] == \"dice_focal\":\n        return dice_focal_loss\n    elif cfg[\"type\"] == \"dice_focal_logit\":\n        return dice_focal_logit_loss\n    elif cfg[\"type\"] == \"tversky_focal\":\n        return tversky_focal_loss\n    elif cfg[\"type\"] == \"tversky_focal_logit\":\n        return tversky_focal_logit_loss\n    elif cfg[\"type\"] == \"combo\":\n        return combo_loss\n    elif cfg[\"type\"] == \"combo_logit\":\n        return combo_logit_loss\n    elif cfg[\"type\"] == \"combo_logit_focal\":\n        return combo_logit_focal_loss\n    elif cfg"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg['loss_type']\n    if loss_type == 'L1':\n        loss = torch.nn.L1Loss()\n    elif loss_type == 'L2':\n        loss = torch.nn.MSELoss()\n    elif loss_type == 'BCE':\n        loss = torch.nn.BCELoss()\n    elif loss_type == 'BCE_logit':\n        loss = torch.nn.BCEWithLogitsLoss()\n    elif loss_type == 'BCE_sigmoid':\n        loss = torch.nn.BCEWithLogitsLoss()\n    elif loss_type == 'CE':\n        loss = torch.nn.CrossEntropyLoss()\n    elif loss_type == 'NLL':\n        loss = torch.nn.NLLLoss()\n    elif loss_type == 'NLL_logit':\n        loss = torch.nn.NLLLoss()\n    elif loss_type == 'NLL_sigmoid':\n        loss = torch.nn.NLLLoss()\n    elif loss_type == 'NLL_softmax':\n        loss = torch.nn.NLLLoss()\n    elif loss_type == 'MSE':\n        loss = torch.nn.MSELoss()\n    elif loss_type == 'SmoothL1':\n        loss = torch.nn.SmoothL1Loss()\n    elif loss_type == 'SoftMargin':\n        loss = torch.nn.SoftMarginLoss()\n    elif loss_type == 'MultiLabelMargin':\n        loss = torch.nn.MultiLabelMarginLoss()\n    elif loss_type == 'MultiLabelSoftMargin':\n        loss = torch.nn.MultiLabelSoftMarginLoss()\n    elif loss_type == 'MultiLabelNLL':\n        loss = torch.nn.MultiLabelSoftMarginLoss()\n    elif loss_type == 'MultiLabelNLL_logit':\n        loss = torch.nn.MultiLabelSoftMarginLoss()\n    elif loss_type == 'MultiLabel"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg.loss_type\n    loss_cfg = cfg.loss_cfg\n    if loss_type == 'CrossEntropy':\n        return CrossEntropyLoss(loss_cfg)\n    elif loss_type == 'MSE':\n        return MSELoss(loss_cfg)\n    elif loss_type == 'L1':\n        return L1Loss(loss_cfg)\n    elif loss_type == 'SmoothL1':\n        return SmoothL1Loss(loss_cfg)\n    elif loss_type == 'KL':\n        return KLLoss(loss_cfg)\n    elif loss_type == 'Softmax':\n        return SoftmaxLoss(loss_cfg)\n    elif loss_type == 'CrossEntropyLoss':\n        return CrossEntropyLoss(loss_cfg)\n    elif loss_type == 'MSELoss':\n        return MSELoss(loss_cfg)\n    elif loss_type == 'L1Loss':\n        return L1Loss(loss_cfg)\n    elif loss_type == 'SmoothL1Loss':\n        return SmoothL1Loss(loss_cfg)\n    elif loss_type == 'KLLoss':\n        return KLLoss(loss_cfg)\n    elif loss_type == 'SoftmaxLoss':\n        return SoftmaxLoss(loss_cfg)\n    else:\n        raise NotImplementedError('Loss type {} is not implemented.'.format(loss_type))"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    from .losses import *\n    loss_type = cfg['type']\n    if loss_type == 'cross_entropy':\n        return CrossEntropyLoss(cfg['loss_params'])\n    elif loss_type == 'cross_entropy_weighted':\n        return CrossEntropyLossWeighted(cfg['loss_params'])\n    elif loss_type == 'cross_entropy_weighted_pos_neg':\n        return CrossEntropyLossWeightedPosNeg(cfg['loss_params'])\n    elif loss_type == 'cross_entropy_weighted_pos_neg_triplet':\n        return CrossEntropyLossWeightedPosNegTriplet(cfg['loss_params'])\n    elif loss_type == 'cross_entropy_weighted_pos_neg_triplet_semi':\n        return CrossEntropyLossWeightedPosNegTripletSemi(cfg['loss_params'])\n    elif loss_type == 'cross_entropy_weighted_pos_neg_triplet_semi_hard':\n        return CrossEntropyLossWeightedPosNegTripletSemiHard(cfg['loss_params'])\n    elif loss_type == 'cross_entropy_weighted_pos_neg_triplet_semi_hard_neg_margin':\n        return CrossEntropyLossWeightedPosNegTripletSemiHardNegMargin(cfg['loss_params'])\n    elif loss_type == 'cross_entropy_weighted_pos_neg_triplet_semi_hard_neg_margin_hard_mining':\n        return CrossEntropyLossWeightedPosNegTripletSemiHardNegMarginHardMining(cfg['loss_params'])\n    elif loss_type == 'cross_entropy_weighted_pos_neg_triplet_semi_hard_neg_margin_hard_mining_hard_mining_margin':\n        return CrossEntropyLossWeightedPosNegTripletSemiHardNegMarginHardMiningHardMiningMargin(cfg['loss_params'])\n    elif loss_type == 'cross_entropy_weighted_pos_neg"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg[\"loss_type\"]\n    if loss_type == \"bce\":\n        import torch.nn.functional as F\n        return F.binary_cross_entropy\n    elif loss_type == \"bce_logit\":\n        import torch.nn.functional as F\n        return lambda input, target: F.binary_cross_entropy_with_logits(input, target)\n    elif loss_type == \"mse\":\n        import torch.nn.functional as F\n        return F.mse_loss\n    elif loss_type == \"l1\":\n        import torch.nn.functional as F\n        return F.l1_loss\n    elif loss_type == \"cross_entropy\":\n        import torch.nn.functional as F\n        return F.cross_entropy\n    elif loss_type == \"cross_entropy_logit\":\n        import torch.nn.functional as F\n        return lambda input, target: F.cross_entropy(input, target)\n    elif loss_type == \"focal\":\n        from losses.focal_loss import FocalLoss\n        return FocalLoss(cfg)\n    elif loss_type == \"dice\":\n        from losses.dice_loss import DiceLoss\n        return DiceLoss(cfg)\n    elif loss_type == \"dice_logit\":\n        from losses.dice_loss import DiceLoss\n        return DiceLoss(cfg, logit=True)\n    elif loss_type == \"dice_logit_weighted\":\n        from losses.dice_loss import DiceLoss\n        return DiceLoss(cfg, logit=True, weighted=True)\n    elif loss_type == \"dice_logit_weighted_bce\":\n        from losses.dice_loss import DiceLoss\n        from losses.bce_logit_loss import BCELogitLoss\n        return DiceLoss(cfg, logit=True, weighted=True, bce_loss=True)\n    elif loss_type == \"dice_"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    head_type = cfg.MODEL.HEAD.TYPE\n    if head_type in HEADS:\n        return HEADS[head_type](cfg)\n    else:\n        return MMDET_HEADS[head_type](cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    head_type = cfg.MODEL.HEAD.TYPE\n    if head_type in HEADS:\n        return HEADS[head_type](cfg)\n    else:\n        return MMDET_HEADS[head_type](cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS:\n        return HEADS[cfg['type']](**cfg)\n    else:\n        return MMDET_HEADS[cfg['type']](**cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    head_type = cfg['type']\n    if head_type in HEADS:\n        return HEADS[head_type](**cfg)\n    else:\n        return MMDET_HEADS[head_type](**cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from mmcv.utils import build_from_cfg\n    from mmseg.models.builder import HEADS, MMDET_HEADS\n\n    if cfg['type'] in HEADS:\n        return build_from_cfg(cfg, HEADS)\n    else:\n        return build_from_cfg(cfg, MMDET_HEADS)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from .builder import HEADS, MMDET_HEADS\n    assert cfg['type'] in HEADS, f\"Head: {cfg['type']} is not registered in the registry\"\n    if cfg['type'] in HEADS:\n        return HEADS[cfg['type']](cfg)\n    else:\n        return MMDET_HEADS[cfg['type']](cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from mmcv.utils import build_from_cfg\n    from mmtrack.models.builder import HEADS, MMDET_HEADS\n\n    if cfg is None:\n        return None\n\n    if cfg.type in HEADS:\n        return build_from_cfg(cfg, HEADS)\n    else:\n        return build_from_cfg(cfg, MMDET_HEADS)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from .mmdet_heads import MMDET_HEADS\n    from .heads import HEADS\n\n    if cfg.type in HEADS:\n        return HEADS[cfg.type](cfg)\n    elif cfg.type in MMDET_HEADS:\n        return MMDET_HEADS[cfg.type](cfg)\n    else:\n        raise NotImplementedError"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS, MMDET_HEADS\n    head_type = cfg['type']\n    if head_type in HEADS:\n        return HEADS[head_type](cfg)\n    elif head_type in MMDET_HEADS:\n        return MMDET_HEADS[head_type](cfg)\n    else:\n        raise NotImplementedError\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from .builder import HEADS, MMDET_HEADS\n    from .registry import HEADS as HEADS_REGISTRY\n\n    if cfg.type in HEADS_REGISTRY:\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS, MMDET_HEADS\n\n    head_type = cfg.pop(\"type\")\n    if head_type in HEADS:\n        return HEADS[head_type](**cfg)\n    elif head_type in MMDET_HEADS:\n        return MMDET_HEADS[head_type](**cfg)\n    else:\n        raise NotImplementedError(f\"{head_type} is not implemented in the HEADS module\")"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from mmpose.models.heads.mmdet_heads import MMDetHead\n    from mmpose.models.heads import HEADS\n\n    head_type = cfg.type\n    if head_type in HEADS:\n        return HEADS[head_type](cfg)\n    else:\n        return MMDetHead(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from .builder import HEADS, MMDET_HEADS\n    if cfg.type in HEADS:\n        return HEADS[cfg.type](cfg)\n    elif cfg.type in MMDET_HEADS:\n        return MMDET_HEADS[cfg.type](cfg)\n    else:\n        raise NotImplementedError(\n            f'{cfg.type} is not supported by the current version.')"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from mmcv.utils import build_from_cfg\n\n    from mmseg.utils import HEADS, MMDET_HEADS\n\n    if cfg.type in HEADS:\n        return build_from_cfg(cfg, HEADS)\n    else:\n        return build_from_cfg(cfg, MMDET_HEADS)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from .builder import HEADS, MMDET_HEADS\n\n    if cfg[\"type\"] in HEADS:\n        return HEADS[cfg[\"type\"]](**cfg)\n    elif cfg[\"type\"] in MMDET_HEADS:\n        return MMDET_HEADS[cfg[\"type\"]](**cfg)\n    else:\n        raise NotImplementedError\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from mmpose.models.heads import HEADS, MMDET_HEADS\n\n    head_type = cfg.pop('type')\n\n    if head_type in HEADS:\n        head = HEADS[head_type](**cfg)\n    elif head_type in MMDET_HEADS:\n        head = MMDET_HEADS[head_type](**cfg)\n    else:\n        raise NotImplementedError(\n            f'{head_type} is not registered in '\n            'MMDET_HEADS or HEADS')\n\n    return head"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from .builder import HEADS, MMDET_HEADS\n\n    head_type = cfg.type\n    assert head_type in HEADS, f\"{head_type} is not in the registry\"\n    if head_type in HEADS:\n        head = HEADS[head_type](cfg)\n    else:\n        head = MMDET_HEADS[head_type](cfg)\n    return head"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS, MMDET_HEADS\n\n    head_type = cfg.pop(\"type\")\n    if head_type in HEADS:\n        head = HEADS[head_type](**cfg)\n    elif head_type in MMDET_HEADS:\n        head = MMDET_HEADS[head_type](**cfg)\n    else:\n        raise NotImplementedError(f\"{head_type} head is not supported\")\n    return head\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS, MMDET_HEADS\n\n    head = cfg.get(\"head\", None)\n    if head is not None:\n        if head in HEADS:\n            return HEADS[head](cfg)\n        elif head in MMDET_HEADS:\n            return MMDET_HEADS[head](cfg)\n        else:\n            raise NotImplementedError(\n                \"Head {} not implemented\".format(head)\n            )\n    return None"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from mmcv.utils import build_from_cfg\n    from mmdet.models.builder import HEADS, MMDET_HEADS\n    from mmdet.models.builder import build_neck\n\n    if cfg['type'] in HEADS:\n        return build_from_cfg(cfg, HEADS)\n    elif cfg['type'] in MMDET_HEADS:\n        return build_from_cfg(cfg, MMDET_HEADS)\n    elif cfg['type'] == 'FPNHead':\n        cfg['in_channels'] = build_neck(cfg['neck'])\n        return build_from_cfg(cfg, HEADS)\n    else:\n        raise NotImplementedError(f'{cfg[\"type\"]} is not implemented in this version')"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from .builder import build_segmentor\n    return build_segmentor(cfg, train_cfg, test_cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from .builder import build_segmentor\n    return build_segmentor(cfg, train_cfg, test_cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from .builder import build_segmentor\n    return build_segmentor(cfg, train_cfg, test_cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from .builder import build_segmentor\n    return build_segmentor(cfg, train_cfg, test_cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from mmdet.models import build_segmentor\n    return build_segmentor(cfg, train_cfg, test_cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from mmdet.models import build_segmentor\n    return build_segmentor(cfg, train_cfg, test_cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from mmdet.models import build_segmentor\n    return build_segmentor(cfg, train_cfg, test_cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from mmdet.models.builder import build_segmentor\n    return build_segmentor(cfg, train_cfg, test_cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from mmdet.models import build_segmentor\n    return build_segmentor(cfg, train_cfg, test_cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    cfg.train_cfg = train_cfg\n    cfg.test_cfg = test_cfg\n    return build_from_cfg(cfg, SEGMENTORS)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    cfg.train_cfg = train_cfg\n    cfg.test_cfg = test_cfg\n    model = MODELS.build(cfg)\n    return model"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from .builder import SEGMENTORS\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field ' \\\n        'DONOT EXCEPT THIS'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field ' \\\n        'DONOT EXCEPT THIS'\n    cfg.model.pretrained = None\n    model = SEGMENTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    return model"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg should be specified '\n            'in cfg', UserWarning)\n    if not isinstance(cfg, dict):\n        raise TypeError('cfg must be a dict')\n    cfg = cfg.copy()\n\n    # set cudnn_benchmark\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n\n    # build the model and load checkpoint\n    cfg.setdefault('train_cfg', {})\n    cfg.setdefault('test_cfg', {})\n    if train_cfg is not None:\n        cfg['train_cfg'] = train_cfg\n    if test_cfg is not None:\n        cfg['test_cfg'] = test_cfg\n    model = MODELS.build(cfg)\n    return model"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from .builder import build_segmentor\n    return build_segmentor(cfg, train_cfg, test_cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from mmdet.models import build_segmentor\n    return build_segmentor(cfg, train_cfg, test_cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from .builder import build_segmentor\n    return build_segmentor(cfg, train_cfg, test_cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model.train_cfg and '\n            'model.test_cfg')\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    cfg.train_cfg = train_cfg\n    cfg.test_cfg = test_cfg\n\n    model = MODELS.build(cfg)\n    logger = logging.getLogger(__name__)\n    logger.info('Model: {}'.format(model.__class__.__name__))\n    return model\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg should be specified '\n            'in cfg', UserWarning)\n    if not isinstance(cfg, dict):\n        raise TypeError('cfg must be a dict')\n    if 'train_cfg' in cfg or 'test_cfg' in cfg:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    cfg.setdefault('pretrained', None)\n    cfg.setdefault('train_cfg', train_cfg)\n    cfg.setdefault('test_cfg', test_cfg)\n    cfg.setdefault('init_cfg', None)\n\n    if cfg['type'] == 'EncoderDecoder':\n        cfg['pretrained'] = None\n    elif cfg['type'] == 'EncoderDecoder3Plus':\n        cfg['pretrained'] = None\n    elif cfg['type'] == 'EncoderDecoder3Plus_2':\n        cfg['pretrained'] = None\n    elif cfg['type'] == 'EncoderDecoder3Plus_3':\n        cfg['pretrained'] = None\n    elif cfg['type'] == 'EncoderDecoder3Plus_4':\n        cfg['pretrained'] = None\n    elif cfg['type'] == 'EncoderDecoder3Plus_5':\n        cfg['pretrained'] = None\n    elif cfg['type'] == 'EncoderDecoder3Plus_6':\n        cfg['pretrained'] = None\n    elif cfg['type'] == 'EncoderDecoder3Plus_7':\n        cfg['pretrained'] = None\n    elif cfg['type'] == 'EncoderDecoder3Plus_8':\n        cfg['pretrained'] = None\n    elif cfg['type'] == 'EncoderDecoder3Plus_9':\n        cfg['pretrained'] = None\n    elif cfg['type'] == 'EncoderDecoder3Plus_10':\n        cfg['pretrained'] = None\n   "}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified \"\n            \"in cfg not in args\")\n    if train_cfg is not None:\n        cfg.train_cfg = train_cfg\n    if test_cfg is not None:\n        cfg.test_cfg = test_cfg\n\n    segmentor_registry = MMSEG_BUILTIN_SEGMENTORS\n    segmentor_registry.register_module(cfg.type, cfg.module)\n    return segmentor_registry.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg should be specified '\n            'in cfg', UserWarning)\n    if not isinstance(cfg, dict):\n        raise TypeError('cfg must be a dict')\n    cfg = cfg.copy()\n\n    # build the base segmentor\n    base_segmentor = BASE_BUILDERS.get(cfg['type'])(**cfg)\n    # return early if the architecture doesn't need to be assembled\n    if cfg.get('__no_crop', False) or not hasattr(base_segmentor, 'forward_train'):\n        return base_segmentor\n\n    # assemble the (possibly mixed) architecture\n    segmentor = SEGMENTOR_BUILDERS.get(cfg['type'])(base_segmentor, **cfg)\n\n    # default arguments for training and testing\n    for task in ['seg_heads', 'auxiliary_heads']:\n        for head_cfg in cfg.get(task, []):\n            assert isinstance(head_cfg, dict) and 'type' in head_cfg\n            if head_cfg.get('train_cfg') is None:\n                head_cfg['train_cfg'] = dict(cfg['train_cfg'])\n            if head_cfg.get('test_cfg') is None:\n                head_cfg['test_cfg'] = dict(cfg['test_cfg'])\n\n    return segmentor"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field ' \\\n        f'{cfg.get(\"type\")}.'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field ' \\\n        f'{cfg.get(\"type\")}.'\n    if train_cfg is not None:\n        cfg['train_cfg'] = train_cfg\n    if test_cfg is not None:\n        cfg['test_cfg'] = test_cfg\n    return build_from_cfg(cfg, DETECTORS, MMDET_DETECTORS)"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified \"\n            \"in model, not in function arguments. \"\n            \"Please check whether you have missed any 'Model' fields.\"\n        )\n    assert (\n        \"train_cfg\" not in cfg or cfg[\"train_cfg\"] is None\n    ), \"train_cfg specified in both outer field and model field \"\n    assert (\n        \"test_cfg\" not in cfg or cfg[\"test_cfg\"] is None\n    ), \"test_cfg specified in both outer field and model field \"\n    cfg.setdefault(\"pretrained\", None)\n    if cfg[\"type\"] not in MMDET_DETECTORS:\n        raise KeyError(\n            \"Unrecognized detector type {}\".format(cfg[\"type\"])\n        )\n    else:\n        detector = MMDET_DETECTORS.get(cfg[\"type\"])(**cfg)\n\n    return detector"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified \"\n            \"in model, not in function arguments. \"\n            \"Please check whether you have forgotten to add them \"\n            \"in the model config file.\", UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and ' \\\n        'model field. Please check whether you have ' \\\n        'misplaced train_cfg in the config file.'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and ' \\\n        'model field. Please check whether you have ' \\\n        'misplaced test_cfg in the config file.'\n\n    if cfg['type'] in MMDET_DETECTORS:\n        return MMDET_DETECTORS.build(cfg)\n    else:\n        return DETECTORS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg are deprecated, \"\n            \"please specify them in model\", UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    if train_cfg is not None:\n        cfg['train_cfg'] = train_cfg\n    if test_cfg is not None:\n        cfg['test_cfg'] = test_cfg\n\n    if cfg['type'] in DETECTORS:\n        detector = DETECTORS.build(cfg)\n    elif cfg['type'] in MMDET_DETECTORS:\n        detector = MMDET_DETECTORS.build(cfg)\n    else:\n        raise NotImplementedError\n\n    return detector"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified \"\n            \"in model, not in function arguments. \"\n            \"Please check whether you have missed any 'Model' fields.\"\n        )\n    assert (\n        \"train_cfg\" not in cfg.keys() and \"test_cfg\" not in cfg.keys()\n    ), f\"train_cfg and test_cfg are not supposed to be specified in the yaml file, but in {cfg['type']}\"\n\n    if cfg[\"type\"] in DETECTORS.registered_names:\n        return build_from_cfg(cfg, DETECTORS, module_dict=DETECTORS)\n    elif cfg[\"type\"] in MMDET_DETECTORS.registered_names:\n        return build_from_cfg(cfg, MMDET_DETECTORS, module_dict=MMDET_DETECTORS)\n    else:\n        raise NotImplementedError(f\"Unkown detector type {cfg['type']}\")"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg should be specified '\n            'in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and ' \\\n        'model field. Please specify it in outer field.'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and ' \\\n        'model field. Please specify it in outer field.'\n    cfg_ = cfg.copy()\n    if train_cfg is not None:\n        cfg_['train_cfg'] = train_cfg\n    if test_cfg is not None:\n        cfg_['test_cfg'] = test_cfg\n    if cfg_['type'] in DETECTORS:\n        return build_from_cfg(cfg_, DETECTORS)\n    elif cfg_['type'] in MMDET_DETECTORS:\n        return build_from_cfg(cfg_, MMDET_DETECTORS)\n    else:\n        raise NotImplementedError(f'Unkown detector type {cfg_[\"type\"]}')"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified \"\n            \"in model, not in function arguments. \"\n            \"Please check whether you have missed any 'Model' fields.\"\n        )\n    assert (\n        \"train_cfg\" not in cfg.keys() and \"test_cfg\" not in cfg.keys()\n    ), f\"train_cfg and test_cfg are not supposed to be specified in cfg\"\n    if cfg[\"type\"] in DETECTORS:\n        return DETECTORS.build(cfg)\n    elif cfg[\"type\"] in MMDET_DETECTORS:\n        return MMDET_DETECTORS.build(cfg)\n    else:\n        raise TypeError(f\"cfg must have a type in {set(DETECTORS)}\")\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified in model, \"\n            \"but got {} in model_cfg and {} in train_cfg\".format(\n                cfg.model, train_cfg\n            )\n        )\n    assert (\n        cfg.get(\"train_cfg\", None) is None or train_cfg is None\n    ), \"train_cfg specified in both outer field and model_cfg\"\n    assert (\n        cfg.get(\"test_cfg\", None) is None or test_cfg is None\n    ), \"test_cfg specified in both outer field and model_cfg\"\n    if cfg.get(\"train_cfg\", None) is not None:\n        warnings.warn(\"train_cfg is deprecated, please specify it in model_cfg\")\n    if cfg.get(\"test_cfg\", None) is not None:\n        warnings.warn(\"test_cfg is deprecated, please specify it in model_cfg\")\n\n    if cfg[\"type\"] not in MMDET_DETECTORS:\n        raise KeyError(\n            \"{} is not in the registry table {}\".format(\n                cfg[\"type\"], list(MMDET_DETECTORS.keys())\n            )\n        )\n    else:\n        return MMDET_DETECTORS[cfg[\"type\"]](cfg, train_cfg, test_cfg)"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified in model \"\n            f\"{cfg.model}, not in built function {build_detector}\")\n\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model ' \\\n        f'{cfg.model}'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model ' \\\n        f'{cfg.model}'\n\n    if cfg.get('train_cfg') is not None:\n        cfg['model'] = cfg.pop('train_cfg')\n    elif train_cfg is not None:\n        cfg['model'] = train_cfg\n\n    if cfg.get('test_cfg') is not None:\n        cfg['model'].update(cfg.pop('test_cfg'))\n    elif test_cfg is not None:\n        cfg['model'].update(test_cfg)\n\n    return build_from_cfg(cfg, DETECTORS if cfg['type'] == 'Detector3D' else MMDET_DETECTORS)"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified in model \"\n            \"config file, but got \"\n            f\"train_cfg={train_cfg} and test_cfg={test_cfg} in \"\n            \"function arguments\"\n        )\n    assert (\n        \"train_cfg\" not in cfg.keys() and \"test_cfg\" not in cfg.keys()\n    ), f\"train_cfg and test_cfg are not supposed to be specified in {cfg['type']}\"\n\n    if cfg[\"type\"] in MMDET_DETECTORS.registry_names:\n        return MMDET_DETECTORS.build(cfg)\n    else:\n        return DETECTORS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified in model \"\n            \"configs. Now they are specified in outer field. This is \"\n            \"deprecated in MMDet v2.0.0 and will be removed in v2.1.0.\"\n        )\n    if \"train_cfg\" in cfg or \"test_cfg\" in cfg:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified in model \"\n            \"configs. Now they are specified in outer field. This is \"\n            \"deprecated in MMDet v2.0.0 and will be removed in v2.1.0.\"\n        )\n    assert (\n        \"train_cfg\" not in cfg or train_cfg is None\n    ), \"train_cfg specified in both outer field and model field. \"\n    assert (\n        \"test_cfg\" not in cfg or test_cfg is None\n    ), \"test_cfg specified in both outer field and model field. \"\n    if cfg[\"type\"] not in DETECTORS:\n        raise KeyError(f\"Unrecognized detector type {cfg['type']}\")\n    detector = DETECTORS.get(cfg[\"type\"])(cfg, train_cfg, test_cfg)\n    return detector"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg are deprecated, \"\n            \"please specify them in model\", UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field ' \\\n        'DONOT use this'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field ' \\\n        'DONOT use this'\n    if train_cfg is not None:\n        cfg['train_cfg'] = train_cfg\n    if test_cfg is not None:\n        cfg['test_cfg'] = test_cfg\n    if cfg['type'] == 'RetinaNet':\n        return build_retinanet(cfg)\n    elif cfg['type'] == 'FCOS':\n        return build_fcos(cfg)\n    elif cfg['type'] == 'GridRCNN':\n        return build_grid_rcnn(cfg)\n    elif cfg['type'] == 'SingleStageDetector':\n        return build_single_stage_detector(cfg)\n    elif cfg['type'] == 'TwoStageDetector':\n        return build_two_stage_detector(cfg)\n    elif cfg['type'] == 'RepPointsDetector':\n        return build_reppoints_detector(cfg)\n    elif cfg['type'] == 'ATSS':\n        return build_atss_detector(cfg)\n    elif cfg['type'] == 'FoveaBox':\n        return build_foveabox_detector(cfg)\n    elif cfg['type'] == 'FreeAnchor':\n        return build_free_anchor_detector(cfg)\n    elif cfg['type'] == 'GFL':\n        return build_gfl_detector(cfg)\n    elif cfg['type'] == 'RepPointsDetector':\n        return build_reppoints_detector(cfg)\n    elif cfg['type']"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified \"\n            \"in model, not in function arguments. \"\n            \"Please check whether you have wrongly set them in the \"\n            \"function arguments. \"\n            \"This will be deprecated in future versions.\"\n        )\n    if train_cfg is not None:\n        assert \"train_cfg\" not in cfg, \"test_cfg specified in both \" \\\n                                       \"outer field and as a model \" \\\n                                       \"input. Please check whether \" \\\n                                       \"you have wrongly set them.\"\n        cfg[\"train_cfg\"] = train_cfg\n    if test_cfg is not None:\n        assert \"test_cfg\" not in cfg, \"test_cfg specified in both \" \\\n                                      \"outer field and as a model \" \\\n                                      \"input. Please check whether \" \\\n                                      \"you have wrongly set them.\"\n        cfg[\"test_cfg\"] = test_cfg\n    if cfg[\"type\"] in DETECTORS:\n        detector = DETECTORS.build(cfg)\n    elif cfg[\"type\"] in MMDET_DETECTORS:\n        detector = MMDET_DETECTORS.build(cfg)\n    else:\n        raise NotImplementedError(f\"Unrecognized detector type {cfg['type']}\")\n    return detector"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified in model \"\n            \"configs. Now they are specified in outer \"\n            \"function which may lead to unexpected behaviour. Please \"\n            \"specify them in model configs.\", UserWarning)\n    assert 'type' in cfg, f\"cfg must have a type, but got {cfg}\"\n    cfg = cfg.copy()\n    if train_cfg is not None:\n        cfg[\"train_cfg\"] = train_cfg\n    if test_cfg is not None:\n        cfg[\"test_cfg\"] = test_cfg\n\n    if cfg[\"type\"] == \"SingleStageDetector\":\n        return SingleStageDetector(**cfg)\n    elif cfg[\"type\"] == \"TwoStageDetector\":\n        return TwoStageDetector(**cfg)\n    else:\n        raise NotImplementedError(f\"Unrecognized detector type {cfg['type']}\")\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field ' \\\n        f'{cfg.get(\"type\")}.'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field ' \\\n        f'{cfg.get(\"type\")}.'\n\n    if cfg.get('train_cfg') is not None:\n        warnings.warn(\n            'train_cfg is deprecated, '\n            'please specify it in model', UserWarning)\n    if cfg.get('test_cfg') is not None:\n        warnings.warn(\n            'test_cfg is deprecated, '\n            'please specify it in model', UserWarning)\n\n    if 'type' not in cfg:\n        raise KeyError('the cfg dict must contain the key \"type\"')\n    cfg_ = cfg.copy()\n\n    if cfg_['type'] not in DETECTORS:\n        raise KeyError(f'{cfg_[\"type\"]} is not registered in '\n                       'the registry')\n    else:\n        return DETECTORS.build(cfg_)\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified \"\n            \"in model, not in function arguments. \"\n            \"Please check whether you have forgotten to add them.\",\n            UserWarning,\n        )\n    assert (\n        \"train_cfg\" not in cfg.keys() or train_cfg is None\n    ), \"train_cfg specified in both outer field and model field. \" \\\n        \"Please check where train_cfg is specified.\"\n    assert (\n        \"test_cfg\" not in cfg.keys() or test_cfg is None\n    ), \"test_cfg specified in both outer field and model field. \" \\\n        \"Please check where test_cfg is specified.\"\n    if \"type\" not in cfg:\n        cfg[\"type\"] = \"BaseDetector\"\n    if isinstance(cfg[\"type\"], str):\n        model_type = cfg[\"type\"]\n    else:\n        raise TypeError(\n            \"model type should be a str, but got {}\".format(type(cfg[\"type\"]))\n        )\n    if model_type in MMDET_DETECTORS:\n        return MMDET_DETECTORS.build(cfg)\n    elif model_type in DETECTORS:\n        return DETECTORS.build(cfg)\n    else:\n        raise TypeError(\n            \"model type {} is not registered in both \"\n            \"DETECTORS and MMDET_DETECTORS\".format(model_type)\n        )"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified \"\n            \"in model since we already have train_cfg and \"\n            \"test_cfg in model_cfg\", UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and ' \\\n        'model field. Please specify it in outer field.'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and ' \\\n        'model field. Please specify it in outer field.'\n    if 'type' not in cfg:\n        raise KeyError(\"the cfg dict must contain the key 'type'\")\n    cfg_ = cfg.copy()\n    model_type = cfg_.pop('type')\n    if model_type in MMDET_DETECTORS:\n        return MMDET_DETECTORS.get(model_type)(**cfg_)\n    elif model_type in DETECTORS:\n        return DETECTORS.get(model_type)(**cfg_)\n    else:\n        raise NotImplementedError(\n            '{} is not currently supported in MMDetection'.format(model_type))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified in model, not in build_detector. \"\n            \"This behavior may change in the future.\"\n        )\n    assert (\n        \"train_cfg\" not in cfg or cfg[\"train_cfg\"] is None\n    ), \"train_cfg specified in outer field is deprecated, \" \"please specify it in model\"\n    assert (\n        \"test_cfg\" not in cfg or cfg[\"test_cfg\"] is None\n    ), \"test_cfg specified in outer field is deprecated, \" \"please specify it in model\"\n    if \"type\" not in cfg:\n        raise KeyError('the cfg dict must contain the key \"type\"')\n    cfg = cfg.copy()\n\n    if \"train_cfg\" not in cfg:\n        cfg[\"train_cfg\"] = train_cfg\n    if \"test_cfg\" not in cfg:\n        cfg[\"test_cfg\"] = test_cfg\n    if not cfg[\"train_cfg\"] is None:\n        assert (\n            cfg[\"train_cfg\"] is None or isinstance(cfg[\"train_cfg\"], dict)\n        ), \"train_cfg must be a dict\"\n    if not cfg[\"test_cfg\"] is None:\n        assert (\n            cfg[\"test_cfg\"] is None or isinstance(cfg[\"test_cfg\"], dict)\n        ), \"test_cfg must be a dict\"\n\n    if \"type\" not in cfg:\n        raise KeyError('the cfg dict must contain the key \"type\"')\n    if cfg[\"type\"] not in DETECTORS:\n        raise KeyError(f'{cfg[\"type\"]} is not registered in ' \"the registry\")\n    else:\n        return DETECTORS.get(cfg[\"type\"])(**cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified in model, \"\n            \"but got {}\".format(train_cfg)\n        )\n    assert cfg.get(\"train_cfg\") is None or train_cfg is None, \\\n        \"train_cfg specified in both outer field and model field \" \\\n        \"is not allowed.\"\n    assert cfg.get(\"test_cfg\") is None or test_cfg is None, \\\n        \"test_cfg specified in both outer field and model field \" \\\n        \"is not allowed.\"\n    if cfg.get(\"train_cfg\") is not None:\n        cfg[\"model\"][\"train_cfg\"] = cfg.pop(\"train_cfg\")\n    if cfg.get(\"test_cfg\") is not None:\n        cfg[\"model\"][\"test_cfg\"] = cfg.pop(\"test_cfg\")\n\n    if cfg[\"type\"] not in DETECTORS:\n        raise KeyError(\"{} is not in the registry\".format(cfg[\"type\"]))\n    else:\n        return DETECTORS.get(cfg[\"type\"])(**cfg[\"model\"])\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg should be specified in model '\n            f'{cfg.model}, not in root option. '\n            'The behavior may change in the future.')\n    if 'train_cfg' in cfg and train_cfg is None:\n        train_cfg = cfg.train_cfg\n    if 'test_cfg' in cfg and test_cfg is None:\n        test_cfg = cfg.test_cfg\n    if train_cfg is not None or test_cfg is not None:\n        assert cfg.get('custom_cfg') is None, 'custom_cfg exists in ' \\\n            'both train_cfg and model_cfg, which is not allowed.'\n    else:\n        cfg_ = cfg.copy()\n        # get model type\n        model_type = cfg_.pop('type')\n        if model_type in MMDET_DETECTORS:\n            model_cls = MMDET_DETECTORS.get(model_type)\n        elif model_type in DETECTORS:\n            warnings.warn('DeprecationWarning: The module of '\n                          f'{model_type} is deprecated, '\n                          'use mmcv.detection instead.')\n            model_cls = DETECTORS.get(model_type)\n        else:\n            raise KeyError(f'Unrecognized detector type {model_type}')\n        # add pretrained and resume_from config\n        if cfg_.get('load_from') is not None:\n            cfg_.pretrained = cfg_.pop('load_from')\n        if train_cfg is None and cfg_.get('train_from') is not None:\n            cfg_.resume_from = cfg_.pop('train_from')\n        # import modules from string type\n        if mmcv.is_str(model_cls):\n            model_cls = import_modules(model_cls)\n        # build the detector\n        model = model_cls(**cfg_)\n        if train_cfg is not None:\n            train_cfg"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .indoor_eval_utils import indoor_eval_utils\n    return indoor_eval_utils(gt_annos, dt_annos, metric, label2cat, logger, box_type_3d, box_mode_3d)"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .indoor_eval_utils import indoor_eval_utils\n    return indoor_eval_utils(gt_annos, dt_annos, metric, label2cat, logger, box_type_3d, box_mode_3d)"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    pass"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .indoor_eval_utils import get_official_eval_result\n    result_strings, result_dict = get_official_eval_result(\n        gt_annos,\n        dt_annos,\n        metric,\n        label2cat,\n        box_type_3d=box_type_3d,\n        box_mode_3d=box_mode_3d,\n        logger=logger)\n    return result_dict"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .indoor_eval_utils import indoor_eval_utils\n\n    return indoor_eval_utils.indoor_eval(gt_annos, dt_annos, metric, label2cat, logger, box_type_3d, box_mode_3d)"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .eval_detection import get_official_eval_result\n    from .eval_detection import print_iou_summary\n\n    ret_dict = get_official_eval_result(gt_annos, dt_annos, metric, label2cat)\n    print_iou_summary(ret_dict, logger=logger)\n    return ret_dict"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .eval_detection_3d import eval_detection_3d\n    eval_results = {}\n    for iou_thr in metric:\n        eval_results_per_class = eval_detection_3d(\n            gt_annos,\n            dt_annos,\n            iou_thr,\n            label2cat,\n            logger=logger,\n            box_type_3d=box_type_3d,\n            box_mode_3d=box_mode_3d)\n        eval_results.update(eval_results_per_class)\n    return eval_results"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    if box_type_3d is not None and box_mode_3d is not None:\n        from .indoor_eval_utils import get_box_type_and_mode\n        box_type_3d, box_mode_3d = get_box_type_and_mode(box_type_3d, box_mode_3d)\n\n    from .indoor_eval_utils import indoor_eval_utils\n    eval_results = indoor_eval_utils(gt_annos,\n                                     dt_annos,\n                                     metric,\n                                     label2cat,\n                                     logger=logger,\n                                     box_type_3d=box_type_3d,\n                                     box_mode_3d=box_mode_3d)\n\n    return eval_results"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    if not isinstance(gt_annos, list):\n        gt_annos = [gt_annos]\n    if not isinstance(dt_annos, list):\n        dt_annos = [dt_annos]\n\n    eval_results = []\n    for i in range(len(gt_annos)):\n        eval_result = indoor_class_eval(gt_annos[i],\n                                        dt_annos[i],\n                                        metric,\n                                        label2cat,\n                                        logger=logger,\n                                        box_type_3d=box_type_3d,\n                                        box_mode_3d=box_mode_3d)\n        eval_results.append(eval_result)\n\n    return eval_results\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .indoor_eval_utils import eval_detection\n\n    if box_mode_3d is None:\n        box_mode_3d = 'lidar'\n\n    if box_type_3d is None:\n        from pcdet.utils import common_utils\n        box_type_3d = common_utils.LidarBox3DMode\n\n    eval_dict = eval_detection(gt_annos,\n                               dt_annos,\n                               metric,\n                               label2cat,\n                               box_type_3d=box_type_3d,\n                               box_mode_3d=box_mode_3d)\n\n    if logger is not None:\n        logger.info('\\n' + eval_dict['msg'])\n\n    return eval_dict"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .eval import get_official_eval_result\n    from .indoor_eval_utils import get_official_eval_result_indoor\n\n    if box_type_3d is not None and box_mode_3d is not None:\n        gt_annos, dt_annos = box_mode_3d_converter(gt_annos,\n                                                   dt_annos,\n                                                   box_type_3d,\n                                                   box_mode_3d)\n\n    eval_result = get_official_eval_result_indoor(gt_annos,\n                                                  dt_annos,\n                                                  metric,\n                                                  label2cat,\n                                                  logger)\n\n    return eval_result\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .indoor_eval_utils import get_official_eval_result\n    result = get_official_eval_result(gt_annos, dt_annos, metric, label2cat, box_type_3d, box_mode_3d)\n    if logger is None:\n        logger = mmdet.utils.get_logger()\n    if isinstance(logger, str):\n        logger = mmdet.utils.get_logger(logger)\n\n    # format the table\n    table_header = [\n        'class', 'iou=0.5:0.95 | area=all | maxDets=100',\n        'iou=0.5  | area=all | maxDets=100',\n        'iou=0.75 | area=all | maxDets=100',\n        'iou=0.5:0.95 | area=small | maxDets=100',\n        'iou=0.5:0.95 | area=medium | maxDets=100',\n        'iou=0.5:0.95 | area=large | maxDets=100',\n        'AR@100', 'AR@300', 'AR@1000', 'AR_s@1000', 'AR_m@1000', 'AR_l@1000',\n        'AR_s@100', 'AR_m@100', 'AR_l@100', 'AR_s@300', 'AR_m@300', 'AR_l@300',\n        'AR_s@1000', 'AR_m@1000', 'AR_l@1000', 'AR_s@3000', 'AR_m@3000',\n        'AR_l@3000', 'AR_h@1000', 'AR_h@3000', 'AR_h@10000', 'AR_h@30000',\n        'AR_"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    if not isinstance(dt_annos, list):\n        dt_annos = [dt_annos]\n    if not isinstance(gt_annos, list):\n        gt_annos = [gt_annos]\n\n    eval_results = {}\n    print_format = []\n    print_format_comp = []\n    print_format_short = []\n    num_imgs = len(gt_annos)\n    print_format_short.append('\\n')\n    print_format_short.append('| {:<18s} | {:<9s} | {:<9s} |\\n'.format('class', 'AP', 'AR'))\n    print_format_short.append('| ----------------- | --------- | --------- |\\n')\n    for i in range(len(metric)):\n        print_format_short.append('| {:<18s} | {:<9.3f} | {:<9.3f} |\\n'.format(\n            str(metric[i]), 0, 0))\n\n    for i in range(len(metric)):\n        print_format.append('\\n')\n        print_format.append('| {:<18s} | {:<9s} | {:<9s} | {:<12s} | {:<12s} |\\n'.format(\n            'class', 'AP', 'AP50', 'AP75', 'APs', 'APm', 'AR', 'ARs', 'ARm', 'ARl'))\n        print_format.append('| ----------------- | --------- | --------- | ------------ | ------------ | --------- | --------- | --------- | --------- | --------- |\\n')\n        for j in range(len(label2cat)):\n            print_format.append('| {:<18s} | {:<9.3f} | {:<9.3f} | {:<12.3f} | {:<12.3f} | {:<9.3f} | {:<9.3f"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .indoor_eval_utils import eval_map_recall\n\n    if logger is None:\n        logger = logging.getLogger()\n\n    eval_results = {}\n    logger.info('\\n')\n    logger.info('Indoor evaluation results:')\n\n    for iou_thr in metric:\n        # iou_thr = metric\n        # print(iou_thr)\n        # print(type(iou_thr))\n        # print(type(metric))\n        # print(metric)\n        # print(type(metric[0]))\n\n        # print(iou_thr)\n        # print(type(iou_thr))\n        # print(type(metric))\n        # print(metric)\n        # print(type(metric[0]))\n        # print(iou_thr)\n        # print(type(iou_thr))\n        # print(type(metric))\n        # print(metric)\n        # print(type(metric[0]))\n        # print(iou_thr)\n        # print(type(iou_thr))\n        # print(type(metric))\n        # print(metric)\n        # print(type(metric[0]))\n        # print(iou_thr)\n        # print(type(iou_thr))\n        # print(type(metric))\n        # print(metric)\n        # print(type(metric[0]))\n        # print(iou_thr)\n        # print(type(iou_thr))\n        # print(type(metric))\n        # print(metric)\n        # print(type(metric[0]))\n        # print(iou_thr)\n        # print(type(iou_thr))\n        # print(type(metric))\n        # print(metric)\n        # print(type(metric[0]))\n        # print(iou_thr)\n        # print(type(iou_thr))\n        # print(type(metric))\n        # print(metric)\n        # print(type(metric[0]))\n        # print(iou_thr)\n        # print(type(iou_thr"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    if box_type_3d is not None:\n        from .indoor_eval_utils import convert_to_3d_box\n        for i, dt_anno in enumerate(dt_annos):\n            dt_annos[i]['bbox'] = convert_to_3d_box(dt_anno['bbox'], box_type_3d, box_mode_3d)\n        for i, gt_anno in enumerate(gt_annos):\n            gt_annos[i]['bbox'] = convert_to_3d_box(gt_anno['bbox'], box_type_3d, box_mode_3d)\n\n    eval_results = {}\n    for iou_thr in metric:\n        print('\\n\\n\\n')\n        print('IoU threshold: {}'.format(iou_thr))\n        print('\\n\\n\\n')\n        eval_results_iou = indoor_eval_iou(gt_annos, dt_annos, iou_thr, label2cat, logger)\n        eval_results.update(eval_results_iou)\n\n    mAP = eval_results['mAP']\n    mAR = eval_results['mAR']\n\n    return eval_results, mAP, mAR\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .indoor_eval_utils import eval_map_iou\n    from .indoor_eval_utils import print_map_summary\n    results = {}\n    for iou_thr in metric:\n        # print('\\n$>' + 'iou_thr=' + str(iou_thr))\n        class_recs, npos, bbox_results = eval_map_iou(\n            gt_annos,\n            dt_annos,\n            iou_thr,\n            label2cat,\n            box_type_3d=box_type_3d,\n            box_mode_3d=box_mode_3d)\n        map_iou_thr = bbox_results['map']\n        map_iou_thr_str = '{:.2f}'.format(iou_thr)\n        results[map_iou_thr_str] = map_iou_thr\n\n    map_summary = print_map_summary(\n        results,\n        npos,\n        label2cat,\n        map_thr=0.5,\n        logger=logger)\n    return map_summary\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .class_names import COCO_CLASSES\n    from .eval_detection_utils import (\n        eval_map_iou,\n        eval_recalls,\n        print_map_summary,\n        print_recall_summary,\n    )\n\n    if logger is None:\n        import logging\n\n        logger = logging.getLogger(__name__)\n    elif isinstance(logger, str):\n        logger = logging.getLogger(logger)\n\n    if box_type_3d is not None and box_mode_3d is not None:\n        from .bbox_3d import box_np_ops\n\n        gt_annos = box_np_ops.convert_box_type(\n            gt_annos, box_type_3d, box_mode_3d\n        )\n        dt_annos = box_np_ops.convert_box_type(\n            dt_annos, box_type_3d, box_mode_3d\n        )\n\n    recalls = {}\n    aps = {}\n    for iou_thr in metric:\n        if isinstance(iou_thr, float):\n            iou_thr = [iou_thr]\n        for label in range(len(COCO_CLASSES)):\n            recalls[label] = eval_recalls(\n                gt_annos,\n                dt_annos,\n                iou_thr,\n                label,\n                label2cat,\n            )\n            aps[label] = eval_map_iou(\n                gt_annos,\n                dt_annos,\n                iou_thr,\n                label,\n                label2cat,\n            )\n        mAP = sum([aps[l] for l in range(len(COCO_CLASSES))]) / len(\n            COCO_CLASSES\n        )\n        mAR = sum([recalls[l] for l in range(len(COCO_CLASSES))]) / len(\n            COCO_CLASSES\n        )\n        aps[\"all\"] = mAP"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    if logger is None:\n        logger = logging.getLogger(__name__)\n    else:\n        logger = get_logger(logger)\n\n    # Check if the input annotations are empty\n    if len(gt_annos) == 0:\n        logger.info('No ground truth annotations found.')\n        return {}\n\n    if len(dt_annos) == 0:\n        logger.info('No detection annotations found.')\n        return {}\n\n    # Convert the bounding boxes to the specified format\n    gt_annos = convert_to_box_mode(gt_annos, box_mode_3d)\n    dt_annos = convert_to_box_mode(dt_annos, box_mode_3d)\n\n    # Compute the mAP and mAR for each class\n    class_names = list(label2cat.values())\n    class_names.sort()\n    eval_results = {}\n    for iou_thr in metric:\n        print('\\n\\n*************iou_thr:', iou_thr)\n        rec, prec, ap = indoor_eval_map(\n            gt_annos,\n            dt_annos,\n            iou_thr,\n            class_names,\n            box_type_3d=box_type_3d)\n        eval_results[f'mAP-{iou_thr}'] = ap\n        eval_results[f'mAR-{iou_thr}'] = np.nanmean(rec)\n        print('mAP-{}, mAR-{}'.format(ap, np.nanmean(rec)))\n\n    # Compute the mAP and mAR for all classes\n    rec, prec, ap = indoor_eval_map(\n        gt_annos,\n        dt_annos,\n        metric,\n        class_names,\n        box_type_3d=box_type_3d)\n    eval_results['mAP'] = ap\n    eval_results['mAR'] = np.nanmean(rec)\n    print('mAP: {}, mAR: {}'.format(ap, np"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    if box_type_3d is not None and box_mode_3d is not None:\n        from .kitti_common import get_box_type, convert_eval_inputs\n        gt_annos, dt_annos = convert_eval_inputs(gt_annos, dt_annos,\n                                                 get_box_type(box_type_3d),\n                                                 box_mode_3d)\n\n    if isinstance(logger, str):\n        import logging\n        logger = logging.getLogger(logger)\n\n    class_names = list(label2cat.values())\n    num_classes = len(class_names)\n    mAP = np.zeros(len(metric), dtype=np.float32)\n    mAR = np.zeros(len(metric), dtype=np.float32)\n    summary = {}\n\n    for i, thr in enumerate(metric):\n        # print(f'{i}/{len(metric)} {thr}')\n        aps = []\n        ars = []\n        for cls_id in range(num_classes):\n            cls_name = class_names[cls_id]\n            rec, prec, ap = indoor_eval_map_recall.eval_map_recall(\n                gt_annos,\n                dt_annos,\n                cls_ids=[cls_id],\n                iou_thr=thr,\n                logger=logger,\n                label2cat=label2cat)\n            aps.append(ap)\n            ari = (rec[-1] - rec[0]) / np.sum(rec[0:-1] - rec[1:])\n            ari = ari * np.sum(prec[0:-1])\n            ari = max(0.0, ari)\n            ari = min(1.0, ari)\n            ari = round(ari, 6)\n            ari = float('%.6f' % ari)\n            ari = np.round(ari, 6)\n            ari = float('%.6f' % ari)\n           "}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .indoor_eval_utils import eval_detection_mAP\n\n    eval_dict = eval_detection_mAP(gt_annos,\n                                   dt_annos,\n                                   metric,\n                                   label2cat,\n                                   box_type_3d=box_type_3d,\n                                   box_mode_3d=box_mode_3d)\n\n    if logger is not None:\n        logger.info('\\n')\n        logger.info('|---------------------------------|')\n        logger.info('| Indoor results                  |')\n        logger.info('|---------------------------------|')\n        logger.info('| IoU threshold | mAP | mAR |')\n        logger.info('|---------------------------------|')\n        for i, metric in enumerate(metric):\n            logger.info('| {:^13} | {:^5.3f} | {:^5.3f} |'.format(\n                metric, eval_dict['mAP'][i], eval_dict['mAR'][i]))\n        logger.info('|---------------------------------|')\n        logger.info('| Overall | {:^5.3f} | {:^5.3f} |'.format(\n            eval_dict['mAP'].mean(), eval_dict['mAR'].mean()))\n        logger.info('|---------------------------------|')\n        logger.info('\\n')\n\n    return eval_dict\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_class = LiDARBox\n        mode = \"LiDAR\"\n    elif box_type == \"Camera\":\n        box_class = CameraBox\n        mode = \"Camera\"\n    elif box_type == \"Depth\":\n        box_class = DepthBox\n        mode = \"Depth\"\n    else:\n        raise ValueError(\"Unknown box type: {}\".format(box_type))\n\n    return box_class, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_class = LiDARBox3D\n        mode = \"LiDAR\"\n    elif box_type == \"Camera\":\n        box_class = CameraBox3D\n        mode = \"Camera\"\n    elif box_type == \"Depth\":\n        box_class = DepthBox3D\n        mode = \"Depth\"\n    else:\n        raise ValueError(f\"Unknown box type: {box_type}\")\n\n    return box_class, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    box_class = None\n    mode = None\n\n    if box_type == \"LiDAR\":\n        box_class = LiDARBox\n        mode = \"LiDAR\"\n    elif box_type == \"Camera\":\n        box_class = CameraBox\n        mode = \"Camera\"\n    elif box_type == \"Depth\":\n        box_class = DepthBox\n        mode = \"Depth\"\n    else:\n        raise ValueError(f\"Unknown box type: {box_type}\")\n\n    return box_class, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_class = LiDARBox\n        mode = \"LiDAR\"\n    elif box_type == \"Camera\":\n        box_class = CameraBox\n        mode = \"Camera\"\n    elif box_type == \"Depth\":\n        box_class = DepthBox\n        mode = \"Depth\"\n    else:\n        raise ValueError(\"Invalid box type: {}\".format(box_type))\n\n    return box_class, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        return LiDAR_Box, \"LiDAR\"\n    elif box_type == \"Camera\":\n        return Camera_Box, \"Camera\"\n    elif box_type == \"Depth\":\n        return Depth_Box, \"Depth\"\n    else:\n        raise ValueError(\"Invalid box type: {}\".format(box_type))\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_class = LiDARBox3D\n        mode = \"lidar\"\n    elif box_type == \"Camera\":\n        box_class = CameraBox3D\n        mode = \"camera\"\n    elif box_type == \"Depth\":\n        box_class = DepthBox3D\n        mode = \"depth\"\n    else:\n        raise ValueError(\"Invalid box type: {}\".format(box_type))\n\n    return box_class, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    box_type_to_class_and_mode = {\n        \"LiDAR\": (Box3D, \"LiDAR\"),\n        \"Camera\": (Box3D, \"Camera\"),\n        \"Depth\": (Box3D, \"Depth\"),\n    }\n\n    if box_type not in box_type_to_class_and_mode:\n        raise ValueError(f\"Unknown box type: {box_type}\")\n\n    return box_type_to_class_and_mode[box_type]"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    box_type_to_class_map = {\n        \"LiDAR\": (LiDARBox, \"LiDAR\"),\n        \"Camera\": (CameraBox, \"Camera\"),\n        \"Depth\": (DepthBox, \"Depth\"),\n    }\n\n    if box_type not in box_type_to_class_map:\n        raise ValueError(f\"Invalid box type: {box_type}\")\n\n    return box_type_to_class_map[box_type]"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        return LiDAR_Box3D, \"LiDAR\"\n    elif box_type == \"Camera\":\n        return Camera_Box3D, \"Camera\"\n    elif box_type == \"Depth\":\n        return Depth_Box3D, \"Depth\"\n    else:\n        raise ValueError(\"Invalid box type: {}\".format(box_type))"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_class = LiDARBox\n        mode = \"LiDAR\"\n    elif box_type == \"Camera\":\n        box_class = CameraBox\n        mode = \"Camera\"\n    elif box_type == \"Depth\":\n        box_class = DepthBox\n        mode = \"Depth\"\n    else:\n        raise ValueError(f\"Unknown box type: {box_type}\")\n\n    return box_class, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    # Mapping of box type to 3D box class and mode\n    box_type_mapping = {\n        \"LiDAR\": (LiDARBox, \"LiDAR\"),\n        \"Camera\": (CameraBox, \"Camera\"),\n        \"Depth\": (DepthBox, \"Depth\"),\n    }\n\n    # Check if the box type is valid\n    if box_type not in box_type_mapping:\n        raise ValueError(f\"Invalid box type: {box_type}\")\n\n    # Return the 3D box class and mode for the given box type\n    return box_type_mapping[box_type]\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    # Define the mapping from box type to class and mode\n    box_type_to_class = {\n        \"LiDAR\": (LiDARBox, \"LiDAR\"),\n        \"Camera\": (CameraBox, \"Camera\"),\n        \"Depth\": (DepthBox, \"Depth\"),\n    }\n\n    # Check if the box type is recognized\n    if box_type not in box_type_to_class:\n        raise ValueError(f\"Unknown box type: {box_type}\")\n\n    # Return the class and mode for the given box type\n    return box_type_to_class[box_type]"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_class = LiDAR_3D_Box\n        mode = \"LiDAR\"\n    elif box_type == \"Camera\":\n        box_class = Camera_3D_Box\n        mode = \"Camera\"\n    elif box_type == \"Depth\":\n        box_class = Depth_3D_Box\n        mode = \"Depth\"\n    else:\n        raise ValueError(\"Unrecognized box type: {}\".format(box_type))\n\n    return box_class, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    box_class_dict = {\n        \"LiDAR\": {\n            \"box_class\": \"LiDARBox\",\n            \"mode\": \"lidar\",\n        },\n        \"Camera\": {\n            \"box_class\": \"CameraBox\",\n            \"mode\": \"camera\",\n        },\n        \"Depth\": {\n            \"box_class\": \"DepthBox\",\n            \"mode\": \"depth\",\n        },\n    }\n\n    if box_type in box_class_dict:\n        return (\n            box_class_dict[box_type][\"box_class\"],\n            box_class_dict[box_type][\"mode\"],\n        )\n    else:\n        raise ValueError(f\"Unknown box type: {box_type}\")"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    # Mapping of box types to 3D box classes and modes\n    box_type_mapping = {\n        \"LiDAR\": (LiDARBox, \"LiDAR\"),\n        \"Camera\": (CameraBox, \"Camera\"),\n        \"Depth\": (DepthBox, \"Depth\")\n    }\n\n    # Get the 3D box class and mode based on the input box type\n    box_class, mode = box_type_mapping.get(box_type, None)\n\n    # If the box type is not recognized, raise an error\n    if box_class is None:\n        raise ValueError(f\"Invalid box type: {box_type}\")\n\n    # Return the 3D box class and mode\n    return box_class, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_class = LiDARBox\n        mode = \"LiDAR\"\n    elif box_type == \"Camera\":\n        box_class = CameraBox\n        mode = \"Camera\"\n    elif box_type == \"Depth\":\n        box_class = DepthBox\n        mode = \"Depth\"\n    else:\n        raise ValueError(f\"Unknown box type: {box_type}\")\n\n    return box_class, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    box_class_dict = {\n        \"LiDAR\": [\n            \"LiDAR_3D\",\n            \"LiDAR_3D_Depth\",\n            \"LiDAR_3D_Camera\",\n            \"LiDAR_3D_LiDAR\",\n            \"LiDAR_3D_Depth_Camera\",\n            \"LiDAR_3D_Depth_LiDAR\",\n            \"LiDAR_3D_Camera_LiDAR\",\n            \"LiDAR_3D_Depth_Camera_LiDAR\",\n        ],\n        \"Camera\": [\n            \"Camera_3D\",\n            \"Camera_3D_Depth\",\n            \"Camera_3D_Camera\",\n            \"Camera_3D_LiDAR\",\n            \"Camera_3D_Depth_Camera\",\n            \"Camera_3D_Depth_LiDAR\",\n            \"Camera_3D_Camera_LiDAR\",\n            \"Camera_3D_Depth_Camera_LiDAR\",\n        ],\n        \"Depth\": [\n            \"Depth_3D\",\n            \"Depth_3D_Depth\",\n            \"Depth_3D_Camera\",\n            \"Depth_3D_LiDAR\",\n            \"Depth_3D_Depth_Camera\",\n            \"Depth_3D_Depth_LiDAR\",\n            \"Depth_3D_Camera_LiDAR\",\n            \"Depth_3D_Depth_Camera_LiDAR\",\n        ],\n    }\n\n    if box_type in box_class_dict:\n        box_class = box_class_dict[box_type]\n    else:\n        raise ValueError(\"Box type not recognized\")\n\n    return box_class\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    box_type_map = {\"LiDAR\": \"lidar_box\", \"Camera\": \"camera_box\", \"Depth\": \"depth_box\"}\n    box_type_mode = {\"lidar_box\": \"lidar\", \"camera_box\": \"camera\", \"depth_box\": \"depth\"}\n\n    if box_type not in box_type_map:\n        raise ValueError(f\"Unrecognized box type: {box_type}. Supported box types are {list(box_type_map.keys())}\")\n\n    return box_type_map[box_type], box_type_mode[box_type_map[box_type]]"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        return LiDARBox, \"LiDAR\"\n    elif box_type == \"Camera\":\n        return CameraBox, \"Camera\"\n    elif box_type == \"Depth\":\n        return DepthBox, \"Depth\"\n    else:\n        raise ValueError(f\"Unknown box type: {box_type}\")\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    # Mapping from box type to 3D box class and mode\n    box_types = {\n        \"LiDAR\": (Box3D, \"LiDAR\"),\n        \"Camera\": (Box3D, \"Camera\"),\n        \"Depth\": (Box3D, \"Depth\"),\n    }\n\n    # Check if the box type is recognized\n    if box_type not in box_types:\n        raise ValueError(f\"Unrecognized box type: {box_type}\")\n\n    # Return the 3D box class and mode for the given box type\n    return box_types[box_type]\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model == '':\n      raise RequestError('Missing model')\n\n    if messages is None:\n      raise RequestError('Missing messages')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not all(isinstance(message, Message) or isinstance(message, dict) for message in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if any(message.get('role') not in ['system', 'user', 'assistant'] for message in messages):\n      raise RequestError('Missing role')\n\n    if any(message.get('content') is None for message in messages):\n      raise RequestError('Missing content')\n\n    if any(message.get('content') == '' for message in messages):\n      raise RequestError('Empty content')\n\n    if any(message.get('content') == '' for message in messages):\n      raise RequestError('Empty content')\n\n    if any(message.get('content') == '' for message in messages):\n      raise RequestError('Empty content')\n\n    if any(message.get('content') == '' for message in messages):\n      raise RequestError('Empty content')\n\n    if any(message.get('content') == '' for message in messages):\n      raise RequestError('Empty content')\n\n    if any(message.get('content') == '' for message in messages):\n      raise RequestError('Empty content')\n\n    if any(message.get('content') == '' for message in messages):\n      raise RequestError('Empty content')\n\n    if any(message.get('content') == '' for message in messages):\n      raise RequestError('Empty content')\n\n    if any(message.get('content') == '' for message in messages):\n      raise RequestError('Empty content')\n\n    if any(message.get('content') == '' for message in messages):\n      raise RequestError('Empty content')\n\n    if any(message.get('content') == '' for message in messages):\n      raise RequestError('Empty content')\n\n    if any(message.get('content') == '' for"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model == '':\n      raise RequestError('No model provided')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not all(isinstance(message, Message) or isinstance(message, dict) for message in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is not None:\n      options.keep_alive = keep_alive\n\n    if format == 'json':\n      options.format = 'json'\n\n    response = self._request(\n      method='POST',\n      path=f'/v1/chat/{model}',\n      json={\n        'messages': messages,\n        'options': options,\n      },\n      stream=stream,\n    )\n\n    if format == 'json':\n      return response\n\n    return ChatResponse(response)"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError(\n        'No model specified. Please provide a model identifier.'\n      )\n\n    if not messages:\n      raise RequestError('No messages provided. Please provide at least one message.')\n\n    if not isinstance(messages, list):\n      raise TypeError('Messages must be a list of Message or dict-like objects.')\n\n    if not all(isinstance(message, Message) for message in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects.')\n\n    if any(not isinstance(message, Message) for message in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects.')\n\n    if not all(isinstance(message, dict) for message in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects.')\n\n    if any(not isinstance(message, dict) for message in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects.')\n\n    if not all(isinstance(message.get('role'), str) for message in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects.')\n\n    if any(not isinstance(message.get('role'), str) for message in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects.')\n\n    if not all(isinstance(message.get('content'), str) for message in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects.')\n\n    if any(not isinstance(message.get('content'), str) for message in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects.')\n\n    if not all(isinstance(message.get('name'), str) for message in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects.')\n\n    if any(not isinstance(message.get('name'), str) for message in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects.')"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model == '':\n      raise RequestError('A model must be specified')\n    if messages is None:\n      raise RequestError('Messages must be provided')\n\n    if not isinstance(messages, list):\n      raise TypeError('Messages must be a list of Message or dict-like objects')\n\n    if not all(isinstance(message, Message) or isinstance(message, dict) for message in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects')\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is not None:\n      options.keep_alive = keep_alive\n\n    if format == 'json':\n      options.stream = False\n\n    if stream:\n      return self._chat_stream(model, messages, format, options)\n\n    return self._chat(model, messages, format, options)"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model == '':\n      raise RequestError('No model specified')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('Messages must be a list of Message or dict-like objects')\n\n    if not all(isinstance(message, (Message, dict)) for message in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects')\n\n    if not all(isinstance(message.get('role'), str) for message in messages):\n      raise RequestError('All messages must have a \"role\" field')\n\n    if not all(isinstance(message.get('content'), str) for message in messages):\n      raise RequestError('All messages must have a \"content\" field')\n\n    if 'images' in messages[0]:\n      messages = [\n        {\n          'role': message['role'],\n          'content': message['content'],\n          'images': [\n            {\n              'url': image['url'],\n              'alt': image.get('alt', ''),\n            }\n            for image in message['images']\n          ],\n        }\n        for message in messages\n      ]\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is not None:\n      options.keep_alive = keep_alive\n\n    if stream:\n      return self._stream_chat(\n        model=model,\n        messages=messages,\n        format=format,\n        options=options,\n      )\n    else:\n      return self._chat(\n        model=model,\n        messages=messages,\n        format=format,\n        options=options,\n      )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('No model provided')\n\n    if not messages:\n      messages = []\n    elif not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is not None:\n      options.keep_alive = keep_alive\n\n    if format == 'json':\n      options.stream = True\n\n    data = {\n      'model': model,\n      'messages': messages,\n      'options': options.to_dict(),\n    }\n\n    if format == 'json':\n      return self._request_json(\n        method='POST',\n        path='/chat',\n        data=data,\n        stream=stream,\n      )\n\n    return self._request_stream(\n      method='POST',\n      path='/chat',\n      data=data,\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model == '':\n      raise RequestError('Model must be specified')\n    if messages is None:\n      raise RequestError('Messages must be specified')\n    if not isinstance(messages, list) or not all(isinstance(m, Message) or isinstance(m, dict) for m in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects')\n    if not isinstance(stream, bool):\n      raise TypeError('Stream must be a bool')\n    if format not in ['', 'json']:\n      raise RequestError('Invalid format')\n    if options is not None and not isinstance(options, Options):\n      raise TypeError('Options must be an Options object')\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError('Keep-alive must be a float or str')\n\n    if stream:\n      yield from self._stream_chat(model, messages, format, options, keep_alive)\n    else:\n      yield self._chat(model, messages, format, options, keep_alive)\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model == '':\n      raise RequestError('No model provided')\n\n    if messages is None:\n      raise RequestError('No messages provided')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    for message in messages:\n      if not isinstance(message, Message) and not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if format not in ['', 'json']:\n      raise RequestError('Invalid format')\n\n    if options is None:\n      options = Options()\n\n    if not isinstance(options, Options):\n      raise TypeError('options must be an Options object')\n\n    if keep_alive is not None:\n      if not isinstance(keep_alive, (float, str)):\n        raise TypeError('keep_alive must be a float or str')\n\n    if format == 'json':\n      messages = [message.to_dict() for message in messages]\n\n    data = {\n      'model': model,\n      'messages': messages,\n      'stream': stream,\n      'options': options.to_dict(),\n    }\n\n    if keep_alive is not None:\n      data['keep_alive'] = keep_alive\n\n    response = self._request('POST', '/chat', data=data)\n\n    if response.status_code != 200:\n      raise ResponseError(response.status_code, response.text)\n\n    if format == 'json':\n      return response.json()\n\n    if stream:\n      return ChatResponse.from_json_stream(response.iter_lines())\n\n    return ChatResponse.from_json(response.json())"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError(\n        'No model specified. Please provide a model.',\n        'No model specified. Please provide a model.',\n      )\n\n    if messages:\n      messages = [\n        Message(**message) if isinstance(message, dict) else message\n        for message in messages\n      ]\n      for message in messages:\n        if not isinstance(message, Message):\n          raise TypeError(\n            'messages must be a list of Message or dict-like objects.'\n          )\n\n    if options:\n      options = Options(**options)\n\n    if keep_alive:\n      keep_alive = float(keep_alive)\n\n    return self._request(\n      'POST',\n      '/chat',\n      {\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options,\n        'keep_alive': keep_alive,\n      },\n      stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model == '':\n      raise RequestError('No model provided')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list) and not isinstance(messages, tuple):\n      raise TypeError('messages must be a list or tuple')\n\n    if not all(isinstance(m, Message) for m in messages):\n      raise TypeError('messages must be a list of Message objects')\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is not None:\n      options.keep_alive = keep_alive\n\n    if format == 'json':\n      options.format = format\n\n    if stream:\n      return self._stream_chat(model, messages, options)\n    else:\n      return self._chat(model, messages, options)\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model == '':\n      raise RequestError('No model provided')\n\n    if messages is None:\n      raise RequestError('No messages provided')\n\n    if not isinstance(messages, list) or not all(isinstance(message, (Message, dict)) for message in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects')\n\n    if format not in ['', 'json']:\n      raise RequestError(f'Invalid format: {format}')\n\n    if options is not None and not isinstance(options, Options):\n      raise TypeError('Options must be an instance of Options')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError('Keep-alive must be a float or string')\n\n    if format == 'json':\n      messages = [message.to_dict() for message in messages]\n\n    data = {\n      'model': model,\n      'messages': messages,\n      'options': options.to_dict() if options else None,\n      'keep_alive': keep_alive,\n    }\n\n    if stream:\n      return self.request(\n        'POST',\n        '/chat/stream',\n        data=data,\n        stream=True,\n        response_type=ChatResponse,\n      )\n\n    response = self.request(\n      'POST',\n      '/chat',\n      data=data,\n      response_type=ChatResponse,\n    )\n\n    return response"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('No model provided')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('Messages must be a list of Message or dict-like objects')\n\n    for message in messages:\n      if not isinstance(message, Message):\n        if not isinstance(message, dict):\n          raise TypeError('Messages must be a list of Message or dict-like objects')\n        message = Message(**message)\n\n      if 'images' in message:\n        message.images = [encode_image(image) for image in message.images]\n\n    if format not in ['', 'json']:\n      raise RequestError(f'Invalid format: {format}')\n\n    if options is None:\n      options = Options()\n\n    options.stream = stream\n\n    if keep_alive is not None:\n      options.keep_alive = keep_alive\n\n    return self._request(\n      method='POST',\n      url=f'{self.base_url}/chat/{model}',\n      headers=self._get_headers(),\n      data={\n        'messages': messages,\n        'options': options,\n      },\n      stream=stream,\n      format=format,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    # Validate input parameters\n    if model == '':\n      raise RequestError('No model specified')\n\n    if messages is None:\n      raise RequestError('No messages provided')\n\n    if not isinstance(messages, list):\n      raise TypeError('Messages must be a list of Message or dict-like objects')\n\n    if not all(isinstance(m, Message) or isinstance(m, dict) for m in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects')\n\n    if options is not None and not isinstance(options, Options):\n      raise TypeError('Options must be an instance of Options')\n\n    # Encode images in messages\n    for m in messages:\n      if isinstance(m, Message):\n        m.encode_images()\n      elif isinstance(m, dict):\n        m['content'] = encode_images(m.get('content', ''))\n\n    # Prepare request data\n    data = {\n      'model': model,\n      'messages': messages,\n      'stream': stream,\n      'format': format,\n      'options': options,\n    }\n\n    # Add keep_alive parameter if provided\n    if keep_alive is not None:\n      data['keep_alive'] = keep_alive\n\n    # Send request and handle response\n    response = self.request('POST', 'chat', data=data)\n\n    if stream:\n      return (ChatResponse(r) for r in response)\n    else:\n      return ChatResponse(response)"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model == '':\n      raise RequestError('Model not provided')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('Messages must be a list of Message or dict-like objects')\n\n    if not all(isinstance(message, Message) or isinstance(message, dict) for message in messages):\n      raise TypeError('Messages must be a list of Message or dict-like objects')\n\n    if any(isinstance(message, dict) for message in messages):\n      messages = [Message(**message) for message in messages]\n\n    if not all(message.role in ['system', 'user', 'assistant'] for message in messages):\n      raise RequestError('Messages must have a role of system, user, or assistant')\n\n    if not all(isinstance(message.content, str) for message in messages):\n      raise RequestError('Messages must have a content of type str')\n\n    if any(isinstance(message, Image) for message in messages):\n      for message in messages:\n        if isinstance(message, Image):\n          message.content = message.content.encode()\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is None:\n      keep_alive = self.keep_alive\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, str):\n        keep_alive = float(keep_alive)\n      if keep_alive < 0:\n        raise RequestError('Keep-alive must be a positive number')\n\n    if format not in ['', 'json']:\n      raise RequestError('Format must be either \"\", \"json\", or \"stream\"')\n\n    if format == 'stream':\n      raise RequestError('Format must be either \"\", \"json\", or \"stream\"')\n\n    if stream:\n      return self._stream_chat(\n        model=model,\n        messages=messages,\n        format=format,\n        options=options,\n        keep_alive=keep_alive,\n      )\n\n    return self._chat(\n      model=model,"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model == '':\n      raise RequestError('No model provided.')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not all(isinstance(message, (Message, dict)) for message in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    for message in messages:\n      if isinstance(message, Message):\n        message.encode()\n      else:\n        if 'role' not in message or 'content' not in message:\n          raise RequestError('Invalid message: missing role or content')\n\n    if options is None:\n      options = Options()\n\n    if not isinstance(options, Options):\n      raise TypeError('options must be an instance of Options')\n\n    if keep_alive is not None:\n      if not isinstance(keep_alive, (float, str)):\n        raise TypeError('keep_alive must be a float or a string')\n\n    if format not in ['', 'json']:\n      raise RequestError(f'Invalid format: {format}')\n\n    if format == 'json':\n      options.add_format(format)\n\n    if stream:\n      options.add_stream(stream)\n\n    if keep_alive is not None:\n      options.add_keep_alive(keep_alive)\n\n    return self._chat(model, messages, options)"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('No model specified')\n\n    if messages is None:\n      messages = []\n    elif not isinstance(messages, list):\n      raise TypeError('Messages must be a list of Message or dict-like objects')\n    else:\n      messages = [self.message(**message) for message in messages]\n\n    if keep_alive is not None:\n      options = options or Options()\n      options.keep_alive = keep_alive\n\n    if format:\n      options = options or Options()\n      options.format = format\n\n    if options:\n      options = options.to_dict()\n\n    if stream:\n      return self._stream(\n        method='POST',\n        path=f'/chat/{model}',\n        data={'messages': messages, 'options': options},\n        response_class=ChatResponse,\n      )\n\n    response = self._request(\n      method='POST',\n      path=f'/chat/{model}',\n      data={'messages': messages, 'options': options},\n      response_class=ChatResponse,\n    )\n\n    return response.to_dict()\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('No model specified')\n\n    if not messages:\n      raise RequestError('No messages specified')\n\n    if not isinstance(messages, (list, tuple)):\n      raise TypeError('Messages must be a list or tuple of Message or dict-like objects')\n\n    if not all(isinstance(message, (Message, dict)) for message in messages):\n      raise TypeError('Messages must be a list or tuple of Message or dict-like objects')\n\n    if format not in ['', 'json']:\n      raise RequestError('Invalid format specified')\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, str):\n        try:\n          keep_alive = float(keep_alive)\n        except ValueError:\n          raise RequestError('Invalid keep-alive value specified')\n\n      if not isinstance(keep_alive, float):\n        raise RequestError('Invalid keep-alive value specified')\n\n    if not stream:\n      return self._request(\n        'POST',\n        '/chat/completions',\n        {\n          'model': model,\n          'messages': messages,\n          'stream': stream,\n          'format': format,\n          'options': options.dict(),\n          'keep_alive': keep_alive,\n        },\n      )\n\n    yield from self._stream(\n      'POST',\n      '/chat/completions',\n      {\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options.dict(),\n        'keep_alive': keep_alive,\n      },\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model == '':\n      raise RequestError('model must be provided')\n\n    if messages is None:\n      raise RequestError('messages must be provided')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n\n    if not all(isinstance(message, Message) or isinstance(message, dict) for message in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is None:\n      keep_alive = self.keep_alive\n\n    if isinstance(keep_alive, str):\n      keep_alive = float(keep_alive)\n\n    if isinstance(keep_alive, float) and keep_alive <= 0:\n      raise ValueError('keep_alive must be a positive float or a string representing a positive float')\n\n    if format == '':\n      format = self.format\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be one of \\'\\' (default) or \\'json\\'')\n\n    if format == 'json':\n      messages = [message.to_dict() for message in messages]\n\n    data = {\n      'model': model,\n      'messages': messages,\n      'options': options.to_dict(),\n      'keep_alive': keep_alive,\n    }\n\n    if stream:\n      return self._stream_chat(data, format)\n\n    response = self._request(\n      method='POST',\n      path='chat',\n      data=data,\n      format=format,\n    )\n\n    return response\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('No model specified')\n\n    if not messages:\n      raise RequestError('No messages provided')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not all(isinstance(message, Message) or isinstance(message, dict) for message in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    messages = [message.to_dict() if isinstance(message, Message) else message for message in messages]\n\n    if any(message.get('role') not in ['system', 'user', 'assistant'] for message in messages):\n      raise RequestError('Invalid message role')\n\n    if any(message.get('role') == 'assistant' for message in messages):\n      raise RequestError('Assistant messages are not supported')\n\n    if any(isinstance(message.get('images'), list) for message in messages):\n      for message in messages:\n        if isinstance(message.get('images'), list):\n          for image in message.get('images', []):\n            if isinstance(image, Image):\n              image.encode()\n            else:\n              raise TypeError('images must be a list of Image or dict-like objects')\n\n    if options:\n      options = options.to_dict()\n\n    if keep_alive:\n      if isinstance(keep_alive, float):\n        keep_alive = str(keep_alive)\n      elif isinstance(keep_alive, str):\n        if not keep_alive.endswith('s'):\n          raise RequestError('keep_alive must be a string ending with \"s\"')\n      else:\n        raise TypeError('keep_alive must be a float or a string')\n\n    if stream:\n      return self._request_stream(\n        'POST',\n        '/v1/chat',\n        data={\n          'model': model,\n          'messages': messages,\n          'format': format,\n          'options': options,\n          'keep_alive': keep"}
{"namespace": "ollama._client.Client.chat", "completion": "    if model == '':\n      raise RequestError('No model specified')\n\n    if messages is None:\n      messages = []\n    if not isinstance(messages, list):\n      raise TypeError('Messages must be a list of Message or dict-like objects')\n    for message in messages:\n      if not isinstance(message, Message):\n        if not isinstance(message, dict):\n          raise TypeError('Messages must be a list of Message or dict-like objects')\n        message = Message(**message)\n      if message.role not in ('system', 'user', 'assistant'):\n        raise RequestError('Message must have a role of system, user, or assistant')\n      if message.role == 'user' and message.content == '':\n        raise RequestError('User message must have non-empty content')\n      if 'images' in message:\n        if not isinstance(message.images, list):\n          raise RequestError('Images must be a list of strings')\n        for image in message.images:\n          if not isinstance(image, str):\n            raise RequestError('Images must be a list of strings')\n          if not image.startswith('data:image/'):\n            raise RequestError('Images must be data URIs')\n\n    if format not in ('', 'json'):\n      raise RequestError('Format must be either \"\", \"json\", or \"stream_events\"')\n\n    if options is None:\n      options = Options()\n    if not isinstance(options, Options):\n      options = Options(**options)\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, float):\n        keep_alive = f'{keep_alive:.2f}'\n      if not isinstance(keep_alive, str):\n        raise TypeError('Keep-alive must be a float or a string')\n      if not keep_alive.isdigit():\n        raise RequestError('Keep-alive must be a positive number')\n      if float(keep_alive) < 0:\n        raise RequestError('Keep-alive must be a positive number')\n\n    if stream:\n      response = self"}
{"namespace": "ollama._client.Client.pull", "completion": "    if stream:\n      return self._stream(\n        method=\"POST\",\n        path=f\"/models/{model}/pull\",\n        insecure=insecure,\n        response_type=ProgressResponse,\n      )\n\n    response = self._request(\n      method=\"POST\",\n      path=f\"/models/{model}/pull\",\n      insecure=insecure,\n    )\n\n    return ProgressResponse(**response)"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = f\"{self.base_url}/models/{model}/pull\"\n    response = self.session.post(url, stream=stream, verify=not insecure)\n    if response.status_code != 200:\n      raise ResponseError(response)\n    if stream:\n      return self._iter_progress_response(response)\n    return self._progress_response(response)"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = f\"{self.base_url}/pull\"\n\n    data = {\n      \"model\": model,\n    }\n\n    response = self.session.post(\n      url,\n      json=data,\n      stream=stream,\n      verify=not insecure,\n    )\n\n    if response.status_code != 200:\n      raise ResponseError(response)\n\n    if stream:\n      return self._stream_response(response)\n\n    return ProgressResponse(response)"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = f\"{self.url}/pull\"\n    headers = {\"Content-Type\": \"application/json\"}\n    data = {\"model\": model}\n    response = self.session.post(url, headers=headers, json=data, verify=not insecure)\n\n    if response.status_code != 200:\n      raise ResponseError(response)\n\n    if stream:\n      return self._stream(response)\n\n    return response.json()"}
{"namespace": "ollama._client.Client.pull", "completion": "    if stream:\n      response = self.post(\n        f\"{self.server_url}/models/{model}/pull\",\n        insecure=insecure,\n        stream=True,\n      )\n      return map(\n        lambda x: ProgressResponse(**x),\n        response.iter_lines(decode_unicode=True),\n      )\n    else:\n      response = self.post(\n        f\"{self.server_url}/models/{model}/pull\",\n        insecure=insecure,\n      )\n      return ProgressResponse(**response)"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = self.build_url(model, \"pull\")\n    headers = {\"Content-Type\": \"application/json\"}\n    response = self.session.post(\n      url,\n      headers=headers,\n      json={\"insecure\": insecure},\n      stream=stream,\n    )\n    if response.status_code == 200:\n      if stream:\n        return self.parse_stream_response(response)\n      else:\n        return self.parse_response(response)\n    else:\n      raise ResponseError(response)"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = self.url + f\"/pull/{model}\"\n    headers = {\"Authorization\": f\"Bearer {self.token}\"}\n    response = requests.post(\n      url,\n      headers=headers,\n      stream=stream,\n      verify=not insecure,\n    )\n    if response.status_code != 200:\n      raise ResponseError(response)\n    if stream:\n      return map(ProgressResponse.from_json, response.iter_lines())\n    return ProgressResponse.from_json(response.content)"}
{"namespace": "ollama._client.Client.pull", "completion": "    try:\n      response = self._request(\n        method=\"POST\",\n        url=f\"{self._url}/pull/{model}\",\n        insecure=insecure,\n        stream=stream,\n      )\n\n      if stream:\n        return self._stream_response(response)\n      else:\n        return self._get_response(response)\n\n    except ResponseError as error:\n      raise error"}
{"namespace": "ollama._client.Client.pull", "completion": "    if stream:\n      return self._stream_pull(model=model, insecure=insecure)\n    else:\n      return self._pull(model=model, insecure=insecure)\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    # Send the request\n    response = self._send_request(\n      method=\"POST\",\n      endpoint=f\"pull/{model}\",\n      insecure=insecure,\n      stream=stream,\n    )\n\n    # If the request is not streamed, return the response\n    if not stream:\n      return response\n\n    # If the request is streamed, return an iterator of responses\n    return self._stream_response(response)"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = f\"{self.base_url}/pull\"\n\n    data = {\"model\": model}\n\n    if insecure:\n      data[\"insecure\"] = insecure\n\n    if stream:\n      data[\"stream\"] = stream\n\n    response = self.session.post(url, json=data)\n\n    if response.status_code == 200:\n      if stream:\n        return response.json(object_hook=ProgressResponse)\n      else:\n        return response.json()\n    else:\n      raise ResponseError(response.status_code, response.text)\n\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = f\"{self.url}/pull\"\n    params = {\"model\": model}\n\n    if stream:\n      params[\"stream\"] = \"true\"\n\n    if insecure:\n      params[\"insecure\"] = \"true\"\n\n    response = self.request(\"POST\", url, params=params)\n\n    if response.status_code != 200:\n      raise ResponseError(response.status_code, response.text)\n\n    if stream:\n      return (ProgressResponse(**response) for response in response.iter_lines())\n\n    return ProgressResponse(**response)"}
{"namespace": "ollama._client.Client.pull", "completion": "    # Check if the model name is valid\n    if not isinstance(model, str) or not model:\n      raise ValueError('Model name must be a non-empty string')\n\n    # Set the endpoint for the request\n    endpoint = f'/model/{model}/pull'\n\n    # Send the request and return the response\n    return self._request(\n      method='post',\n      endpoint=endpoint,\n      insecure=insecure,\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    if not isinstance(model, str):\n      raise TypeError(f\"Expected model to be a str, got {type(model)}\")\n\n    if not isinstance(insecure, bool):\n      raise TypeError(f\"Expected insecure to be a bool, got {type(insecure)}\")\n\n    if not isinstance(stream, bool):\n      raise TypeError(f\"Expected stream to be a bool, got {type(stream)}\")\n\n    url = f\"{self._base_url}/pull\"\n    params = {\"model\": model}\n    headers = {\"Accept\": \"application/json\"}\n\n    if insecure:\n      headers[\"X-Insecure\"] = \"true\"\n\n    response = self._session.post(\n      url,\n      params=params,\n      headers=headers,\n      stream=stream,\n    )\n\n    if response.status_code != 200:\n      raise ResponseError(response)\n\n    if stream:\n      return self._stream_response(response)\n    else:\n      return self._parse_response(response)"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = f\"{self.url}/pull\"\n    data = {\"model\": model}\n    if stream:\n      response = self.session.post(\n        url,\n        json=data,\n        stream=True,\n        verify=not insecure,\n      )\n      return (\n        ProgressResponse(\n          response=response,\n          stream=stream,\n        )\n        for response in response.iter_lines(\n          chunk_size=1024,\n          decode_unicode=True,\n        )\n      )\n    else:\n      response = self.session.post(\n        url,\n        json=data,\n        verify=not insecure,\n      )\n      return ProgressResponse(\n        response=response,\n        stream=stream,\n      )"}
{"namespace": "ollama._client.Client.pull", "completion": "    url = self.url + \"/pull\"\n\n    if not self.url.startswith(\"http\"):\n      url = \"http://\" + url\n\n    if insecure:\n      url = url.replace(\"http://\", \"http://127.0.0.1:\")\n\n    data = {\"model\": model}\n\n    if stream:\n      return self._stream_response(\n        method=\"post\",\n        url=url,\n        data=data,\n        response_type=ProgressResponse,\n      )\n    else:\n      return self._request(\n        method=\"post\",\n        url=url,\n        data=data,\n        response_type=ProgressResponse,\n      )"}
{"namespace": "ollama._client.Client.pull", "completion": "    if not model:\n      raise ValueError(\"Model name is required.\")\n\n    url = f\"{self._url}/pull\"\n    params = {\"model\": model}\n    headers = {\"Content-Type\": \"application/json\"}\n\n    if insecure:\n      headers[\"Authorization\"] = f\"Bearer {self._token}\"\n\n    if stream:\n      response = self._session.post(\n        url,\n        headers=headers,\n        params=params,\n        stream=True,\n      )\n      response.raise_for_status()\n      return self._stream_response(response)\n    else:\n      response = self._session.post(\n        url,\n        headers=headers,\n        params=params,\n      )\n      response.raise_for_status()\n      return self._get_response(response)"}
{"namespace": "ollama._client.Client.pull", "completion": "    if stream:\n      return self._pull_stream(model, insecure)\n    else:\n      return self._pull(model, insecure)\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    if not model:\n      raise ValueError(\"model must be specified\")\n\n    if not self.url:\n      raise ValueError(\"url must be specified\")\n\n    if not self.token:\n      raise ValueError(\"token must be specified\")\n\n    url = f\"{self.url}/pull\"\n\n    headers = {\"Authorization\": f\"Bearer {self.token}\"}\n\n    data = {\"model\": model}\n\n    if insecure:\n      data[\"insecure\"] = insecure\n\n    if stream:\n      data[\"stream\"] = stream\n\n    response = requests.post(url, headers=headers, json=data, stream=stream)\n\n    if response.status_code != 200:\n      raise ResponseError(response.status_code, response.text)\n\n    if stream:\n      return self._stream_response(response)\n\n    return response.json()\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    if not self._is_initialized:\n      raise Exception(\"Client is not initialized\")\n\n    if not model:\n      raise ValueError(\"model cannot be empty\")\n\n    url = f\"{self._url}/model/pull\"\n\n    try:\n      response = requests.post(\n        url,\n        json={\n          \"model\": model,\n        },\n        headers=self._headers,\n        verify=not insecure,\n        stream=stream,\n      )\n\n      if response.status_code == 200:\n        if stream:\n          return self._response_generator(response)\n        else:\n          return self._response(response)\n      else:\n        raise ResponseError(response.json())\n    except requests.exceptions.RequestException as e:\n      raise ResponseError(str(e))"}
{"namespace": "ollama._client.Client.generate", "completion": ""}
{"namespace": "ollama._client.Client.generate", "completion": ""}
{"namespace": "ollama._client.Client.generate", "completion": "    # Check if the model is provided\n    if not model:\n      raise ValueError('Model must be provided')\n\n    # Create the request data\n    request_data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    # Send the request and return the response\n    return self._send_request('generate', request_data)"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError(\"'model' must be provided\")\n\n    params = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    return self._request(\n      method='POST',\n      url='/generate',\n      params=params,\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    # Check if the model is provided\n    if not model:\n      raise ValueError('model must be provided')\n\n    # Create the request data\n    request_data = {\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context or [],\n      'raw': raw,\n      'format': format,\n      'images': images or [],\n      'options': options or {},\n      'keep_alive': keep_alive,\n    }\n\n    # Send the request and return the response\n    return self._request(\n      method='POST',\n      path=f'/v1/generate/{model}',\n      data=request_data,\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    # Check if the model is provided\n    if not model:\n      raise ValueError('Model is required')\n\n    # Create the request payload\n    payload = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context or [],\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images or [],\n      'options': options or {},\n      'keep_alive': keep_alive,\n    }\n\n    # Send the request and handle the response\n    response = self._request('POST', '/generate', json=payload)\n    if stream:\n      return self._stream_response(response)\n    else:\n      return self._parse_response(response)"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not prompt:\n      raise ValueError('prompt is required')\n\n    if not system:\n      raise ValueError('system is required')\n\n    if not template:\n      raise ValueError('template is required')\n\n    if not format:\n      raise ValueError('format is required')\n\n    if not isinstance(context, Sequence):\n      raise ValueError('context must be a sequence')\n\n    if not isinstance(images, Sequence):\n      raise ValueError('images must be a sequence')\n\n    if not isinstance(options, Options):\n      raise ValueError('options must be an Options object')\n\n    if not isinstance(keep_alive, (float, str, None)):\n      raise ValueError('keep_alive must be a float, str, or None')\n\n    if not isinstance(stream, bool):\n      raise ValueError('stream must be a bool')\n\n    if not isinstance(raw, bool):\n      raise ValueError('raw must be a bool')\n\n    if not isinstance(format, Literal['', 'json']):\n      raise ValueError('format must be either an empty string or \"json\"')\n\n    if not isinstance(model, str):\n      raise ValueError('model must be a string')\n\n    if not isinstance(prompt, str):\n      raise ValueError('prompt must be a string')\n\n    if not isinstance(system, str):\n      raise ValueError('system must be a string')\n\n    if not isinstance(template, str):\n      raise ValueError('template must be a string')\n\n    if not isinstance(format, Literal['', 'json']):\n      raise ValueError('format must be either an empty string or \"json\"')\n\n    if not isinstance(context, Sequence):\n      raise ValueError('context must be a sequence')\n\n    if not isinstance(images, Sequence):\n      raise ValueError('images must be a sequence')\n\n    if not isinstance(options, Options):\n      raise ValueError('options must be an Options object')\n\n    if not"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not prompt and not system:\n      raise ValueError('prompt or system is required')\n\n    if not format:\n      format = 'json'\n\n    if not options:\n      options = Options()\n\n    if not keep_alive:\n      keep_alive = None\n\n    if not context:\n      context = []\n\n    if not images:\n      images = []\n\n    if not isinstance(context, list):\n      context = list(context)\n\n    if not isinstance(images, list):\n      images = list(images)\n\n    if not isinstance(keep_alive, (float, str)):\n      keep_alive = None\n\n    if not isinstance(options, Options):\n      options = Options(**options)\n\n    if not isinstance(format, str):\n      format = ''\n\n    if not isinstance(raw, bool):\n      raw = False\n\n    if not isinstance(stream, bool):\n      stream = False\n\n    if not isinstance(system, str):\n      system = ''\n\n    if not isinstance(template, str):\n      template = ''\n\n    if not isinstance(prompt, str):\n      prompt = ''\n\n    if not isinstance(model, str):\n      model = ''\n\n    if not isinstance(format, str):\n      format = ''\n\n    if not isinstance(keep_alive, (float, str)):\n      keep_alive = None\n\n    if not isinstance(options, Options):\n      options = Options(**options)\n\n    if not isinstance(context, list):\n      context = list(context)\n\n    if not isinstance(images, list):\n      images = list(images)\n\n    if not isinstance(system, str):\n      system = ''\n\n    if not isinstance(template, str):\n      template = ''\n\n    if not isinstance(prompt, str):\n      prompt = ''\n\n    if not isinstance(model, str):\n      model = ''\n\n    if not isinstance(format"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not prompt and not system:\n      raise ValueError('prompt or system is required')\n\n    if stream:\n      return self._stream(\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        raw=raw,\n        format=format,\n        images=images,\n        options=options,\n        keep_alive=keep_alive,\n      )\n\n    response = self._request(\n      method='POST',\n      path='/v1/completions',\n      model=model,\n      prompt=prompt,\n      system=system,\n      template=template,\n      context=context,\n      raw=raw,\n      format=format,\n      images=images,\n      options=options,\n      keep_alive=keep_alive,\n    )\n\n    return response\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n    if not prompt and not system and not template:\n      raise ValueError('prompt, system, or template is required')\n\n    if context is None:\n      context = []\n\n    if images is None:\n      images = []\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is not None:\n      options.keep_alive = keep_alive\n\n    if format == 'json':\n      options.format = 'json'\n\n    if raw:\n      options.raw = True\n\n    if stream:\n      return self.stream(\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        images=images,\n        options=options,\n      )\n\n    return self.request(\n      model=model,\n      prompt=prompt,\n      system=system,\n      template=template,\n      context=context,\n      images=images,\n      options=options,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    # If the model is not provided, raise an exception\n    if not model:\n      raise ValueError('model is required')\n\n    # If the prompt is not provided, raise an exception\n    if not prompt:\n      raise ValueError('prompt is required')\n\n    # If the template is not provided, raise an exception\n    if not template:\n      raise ValueError('template is required')\n\n    # If the context is not provided, set it to an empty list\n    if context is None:\n      context = []\n\n    # If the images are not provided, set it to an empty list\n    if images is None:\n      images = []\n\n    # If the options are not provided, set it to an empty dictionary\n    if options is None:\n      options = {}\n\n    # If the keep_alive is not provided, set it to None\n    if keep_alive is None:\n      keep_alive = None\n\n    # If the keep_alive is a string, convert it to a float\n    if isinstance(keep_alive, str):\n      keep_alive = float(keep_alive)\n\n    # If the keep_alive is a float, convert it to a string\n    if isinstance(keep_alive, float):\n      keep_alive = str(keep_alive)\n\n    # If the keep_alive is not a string or a float, raise an exception\n    if not isinstance(keep_alive, (float, str, type(None))):\n      raise ValueError('keep_alive must be a float, a string, or None')\n\n    # If the format is not 'json', raise an exception\n    if format != 'json':\n      raise ValueError('format must be \"json\"')\n\n    # If the model is not supported, raise an exception\n    if model not in self.models:\n      raise ValueError(f'model \"{model}\" is not supported')\n\n    # If the template is not supported, raise an exception\n    if template not in self.templates:\n      raise ValueError(f'template \"{template}\" is not supported')\n\n    # If the keep_alive"}
{"namespace": "ollama._client.Client.generate", "completion": "    # Raise exception if model is not provided\n    if not model:\n      raise ValueError('model must be provided')\n\n    # Prepare the request data\n    data = {\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n    }\n\n    # Send the request\n    response = self.request(\n      method='POST',\n      url=f'{self.base_url}/{model}/generate',\n      data=data,\n      keep_alive=keep_alive,\n    )\n\n    # Return the response\n    return response\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model must be provided')\n\n    if not prompt and not system and not template:\n      raise ValueError('prompt, system, or template must be provided')\n\n    if not stream:\n      if raw:\n        if format:\n          raise ValueError('raw and format are mutually exclusive')\n        return self._request(\n          'POST',\n          f'/v1/engines/{model}/completions',\n          json={\n            'prompt': prompt,\n            'system': system,\n            'template': template,\n            'context': context,\n            'images': images,\n            'options': options,\n          },\n          stream=stream,\n          keep_alive=keep_alive,\n        )\n      else:\n        if format == 'json':\n          return self._request(\n            'POST',\n            f'/v1/engines/{model}/completions',\n            json={\n              'prompt': prompt,\n              'system': system,\n              'template': template,\n              'context': context,\n              'images': images,\n              'options': options,\n            },\n            stream=stream,\n            keep_alive=keep_alive,\n          )\n        else:\n          return self._request(\n            'POST',\n            f'/v1/engines/{model}/completions',\n            json={\n              'prompt': prompt,\n              'system': system,\n              'template': template,\n              'context': context,\n              'images': images,\n              'options': options,\n            },\n            stream=stream,\n            keep_alive=keep_alive,\n          )\n    else:\n      if raw:\n        if format:\n          raise ValueError('raw and format are mutually exclusive')\n        return self._stream(\n          'POST',\n          f'/v1/engines/{model}/completions',\n          json={\n            'prompt': prompt,\n            'system': system,\n            'template': template,\n            'context': context,\n            'images': images,\n            'options': options,"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError(\"Model must be provided\")\n\n    if not prompt and not system:\n      raise ValueError(\"Either prompt or system must be provided\")\n\n    if not format:\n      format = 'json'\n\n    if images is not None:\n      images = [image.read() for image in images]\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, str):\n        keep_alive = float(keep_alive)\n      else:\n        keep_alive = str(keep_alive)\n\n    if stream:\n      return self._stream_request(\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        raw=raw,\n        format=format,\n        images=images,\n        options=options,\n        keep_alive=keep_alive,\n      )\n\n    return self._request(\n      model=model,\n      prompt=prompt,\n      system=system,\n      template=template,\n      context=context,\n      raw=raw,\n      format=format,\n      images=images,\n      options=options,\n      keep_alive=keep_alive,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if model == '':\n      raise ValueError('Model is required')\n\n    if not prompt and not system:\n      raise ValueError('Either prompt or system must be provided')\n\n    if not template and not system:\n      raise ValueError('Either template or system must be provided')\n\n    if not format:\n      format = ''\n    elif format not in ['', 'json']:\n      raise ValueError('Invalid format')\n\n    if not keep_alive:\n      keep_alive = None\n    elif isinstance(keep_alive, float):\n      keep_alive = str(keep_alive)\n    elif not isinstance(keep_alive, str):\n      raise ValueError('Invalid keep_alive')\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    if stream:\n      return self._stream_request('generate', data)\n    else:\n      return self._request('generate', data)"}
{"namespace": "ollama._client.Client.generate", "completion": "    # Raise an exception if the model is not provided\n    if not model:\n      raise ValueError('model is required')\n\n    # Raise an exception if the request cannot be fulfilled\n    if not prompt and not system and not template and not context and not images:\n      raise ValueError('prompt, system, template, context, or images are required')\n\n    # Set the request data\n    request_data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    # Send the request and return the response\n    return self._send_request('generate', request_data)\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model must be specified')\n\n    if not prompt and not system and not template:\n      raise ValueError('prompt, system, or template must be specified')\n\n    if not prompt and system and not template:\n      raise ValueError('system must be specified with a prompt')\n\n    if not prompt and not system and template:\n      raise ValueError('template must be specified with a prompt')\n\n    if not prompt and system and template:\n      raise ValueError('system and template must be specified with a prompt')\n\n    if context is None:\n      context = []\n\n    if images is None:\n      images = []\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is None:\n      keep_alive = ''\n\n    if not isinstance(keep_alive, str):\n      keep_alive = str(keep_alive)\n\n    if not isinstance(stream, bool):\n      raise TypeError('stream must be a bool')\n\n    if not isinstance(raw, bool):\n      raise TypeError('raw must be a bool')\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be either \"\", \"json\", or None')\n\n    if not isinstance(context, (list, tuple)):\n      raise TypeError('context must be a list or tuple')\n\n    if not isinstance(images, (list, tuple)):\n      raise TypeError('images must be a list or tuple')\n\n    if not isinstance(options, Options):\n      raise TypeError('options must be an Options object')\n\n    if not isinstance(keep_alive, str):\n      raise TypeError('keep_alive must be a string or None')\n\n    if not isinstance(prompt, str):\n      raise TypeError('prompt must be a string')\n\n    if not isinstance(system, str):\n      raise TypeError('system must be a string')\n\n    if not isinstance(template, str):\n      raise TypeError('template must be a string')\n\n    if not isinstance(model, str):\n      raise Type"}
{"namespace": "ollama._client.Client.generate", "completion": "    if model == '':\n      raise ValueError('model must be provided')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise TypeError('keep_alive must be a float, a string, or None')\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be either an empty string or \"json\"')\n\n    if images is not None:\n      if not isinstance(images, Sequence):\n        raise TypeError('images must be a sequence')\n      for image in images:\n        if not isinstance(image, (str, bytes)):\n          raise TypeError('images must be a sequence of strings or bytes')\n\n    if context is not None:\n      if not isinstance(context, Sequence):\n        raise TypeError('context must be a sequence')\n      for item in context:\n        if not isinstance(item, int):\n          raise TypeError('context must be a sequence of integers')\n\n    if options is not None:\n      if not isinstance(options, Options):\n        raise TypeError('options must be an instance of Options')\n\n    if raw:\n      if format == '':\n        raise ValueError('format must be \"json\" if raw is True')\n\n    if stream:\n      return self._stream(\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        format=format,\n        images=images,\n        options=options,\n        keep_alive=keep_alive,\n      )\n\n    return self._request(\n      model=model,\n      prompt=prompt,\n      system=system,\n      template=template,\n      context=context,\n      format=format,\n      images=images,\n      options=options,\n      keep_alive=keep_alive,\n    )\n\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if model == '':\n      raise ValueError('model must be provided')\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, float):\n        keep_alive = str(keep_alive)\n      if isinstance(keep_alive, str):\n        if not keep_alive.isdigit():\n          raise ValueError('keep_alive must be a number')\n\n    if images is not None:\n      if not isinstance(images, (list, tuple)):\n        raise ValueError('images must be a list or tuple')\n      if not all(isinstance(image, (str, bytes, bytearray)) for image in images):\n        raise ValueError('images must be a list of strings, bytes, or bytearrays')\n\n    if options is not None:\n      if not isinstance(options, Options):\n        raise ValueError('options must be an instance of Options')\n\n    if format not in ('', 'json'):\n      raise ValueError('format must be either an empty string or \"json\"')\n\n    if stream:\n      if format == 'json':\n        raise ValueError('streaming JSON responses are not supported')\n      if raw:\n        raise ValueError('raw streaming responses are not supported')\n\n    if not raw:\n      if format == '':\n        format = 'json'\n\n    if format == 'json':\n      if stream:\n        raise ValueError('streaming JSON responses are not supported')\n\n    if keep_alive is not None:\n      if stream:\n        raise ValueError('keep_alive is not supported for streaming requests')\n\n    if context is not None:\n      if not isinstance(context, (list, tuple)):\n        raise ValueError('context must be a list or tuple')\n      if not all(isinstance(item, int) for item in context):\n        raise ValueError('context must be a list of integers')\n\n    if images is not None:\n      if not isinstance(images, (list, tuple)):\n        raise ValueError('images must be a list or tuple')\n      if not all(isinstance(image, (str, bytes,"}
{"namespace": "ollama._client.Client.generate", "completion": "    # Raise exception if model is not provided\n    if not model:\n      raise ValueError('model must be provided')\n\n    # Create request data\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context or [],\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images or [],\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    # Make request\n    response = self.request('POST', '/generate', data=data)\n\n    # Return response if not streamed\n    if not stream:\n      return response\n\n    # Return generator for streaming response\n    return self.stream(response)\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    params = {\n      'model': model,\n    }\n\n    return self._request(\n      method='POST',\n      path='/api/push',\n      params=params,\n      insecure=insecure,\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    ...\n"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self.stream(\n        method=\"POST\",\n        path=\"/api/push\",\n        data={\"model\": model},\n        insecure=insecure,\n      )\n\n    response = self.request(\n      method=\"POST\",\n      path=\"/api/push\",\n      data={\"model\": model},\n      insecure=insecure,\n    )\n\n    return ProgressResponse(**response)\n"}
{"namespace": "ollama._client.Client.push", "completion": "    if not stream:\n      return self._request(\n        method=\"POST\",\n        endpoint=\"/api/push\",\n        params={\"model\": model},\n        insecure=insecure,\n        response_type=ProgressResponse,\n      )\n\n    else:\n      return self._request(\n        method=\"POST\",\n        endpoint=\"/api/push\",\n        params={\"model\": model},\n        insecure=insecure,\n        response_type=ProgressResponse,\n        stream=True,\n      )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self._stream_request(\n        method='POST',\n        path='/api/push',\n        model=model,\n        insecure=insecure,\n        stream=stream,\n      )\n\n    response = self._request(\n      method='POST',\n      path='/api/push',\n      model=model,\n      insecure=insecure,\n    )\n\n    return ProgressResponse(**response)"}
{"namespace": "ollama._client.Client.push", "completion": "    url = self.url + \"/api/push\"\n    params = {\"model\": model}\n    headers = {\"Authorization\": \"Bearer \" + self.token}\n\n    if insecure:\n      response = requests.post(url, params=params, headers=headers, verify=False)\n    else:\n      response = requests.post(url, params=params, headers=headers)\n\n    if response.status_code != 200:\n      raise ResponseError(response.status_code, response.text)\n\n    if stream:\n      for line in response.iter_lines():\n        if line:\n          yield json.loads(line)\n    else:\n      return json.loads(response.text)"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self.stream(\n        method=\"POST\",\n        path=\"/api/push\",\n        data={\"model\": model},\n        insecure=insecure,\n        response_type=ProgressResponse,\n      )\n\n    response = self.request(\n      method=\"POST\",\n      path=\"/api/push\",\n      data={\"model\": model},\n      insecure=insecure,\n    )\n\n    if response.status_code != 200:\n      raise ResponseError(\n        response=response,\n        message=\"Failed to push model\",\n      )\n\n    return response.json()"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self._post_stream(\n        url=f\"{self._base_url}/api/push\",\n        data={\"model\": model},\n        insecure=insecure,\n      )\n    else:\n      return self._post(\n        url=f\"{self._base_url}/api/push\",\n        data={\"model\": model},\n        insecure=insecure,\n      )"}
{"namespace": "ollama._client.Client.push", "completion": "    params = {\"model\": model}\n    url = f\"{self.base_url}/api/push\"\n    return self._request(\n      method=\"POST\",\n      url=url,\n      params=params,\n      insecure=insecure,\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    url = f\"{self.base_url}/api/push\"\n    params = {\"model\": model}\n    headers = {\"Content-Type\": \"application/json\"}\n    response = self.request(\n      \"POST\",\n      url,\n      params=params,\n      headers=headers,\n      insecure=insecure,\n      stream=stream,\n    )\n    if stream:\n      return self._stream_response(response, ProgressResponse)\n    return self._response(response, ProgressResponse)"}
{"namespace": "ollama._client.Client.push", "completion": "    if stream:\n      return self._stream(\n        method=\"POST\",\n        endpoint=\"/api/push\",\n        model=model,\n        insecure=insecure,\n        response=ProgressResponse,\n      )\n\n    return self._send(\n      method=\"POST\",\n      endpoint=\"/api/push\",\n      model=model,\n      insecure=insecure,\n      response=ProgressResponse,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    url = f\"{self.url}/api/push\"\n\n    headers = {\"Content-Type\": \"application/json\"}\n\n    params = {\"model\": model}\n\n    if stream:\n      headers[\"Accept\"] = \"text/event-stream\"\n\n    response = self.session.post(\n      url,\n      headers=headers,\n      params=params,\n      verify=not insecure,\n      stream=stream,\n    )\n\n    if response.ok:\n      if stream:\n        return self._stream_response(response)\n      else:\n        return self._get_response(response)\n    else:\n      raise ResponseError(response)"}
{"namespace": "ollama._client.Client.push", "completion": "    url = self.url + \"/api/push\"\n    params = {\"model\": model}\n\n    if insecure:\n      params[\"insecure\"] = \"true\"\n\n    if stream:\n      response = requests.post(\n        url,\n        params=params,\n        headers=self.headers,\n        stream=True,\n      )\n      if response.status_code == 200:\n        return self._stream_response(response)\n      else:\n        raise ResponseError(response)\n    else:\n      response = requests.post(url, params=params, headers=self.headers)\n      if response.status_code == 200:\n        return response.json()\n      else:\n        raise ResponseError(response)"}
{"namespace": "ollama._client.Client.push", "completion": "    if not model:\n      raise ValueError(\"'model' must be specified\")\n\n    if stream:\n      return self._stream_post(\n        url=f\"{self.url}/api/push\",\n        data={\"model\": model},\n        insecure=insecure,\n        response_type=ProgressResponse,\n      )\n    else:\n      return self._post(\n        url=f\"{self.url}/api/push\",\n        data={\"model\": model},\n        insecure=insecure,\n        response_type=ProgressResponse,\n      )"}
{"namespace": "ollama._client.Client.push", "completion": "    # Check if the model exists\n    if model not in self.models:\n      raise ResponseError(\n        f\"Model '{model}' does not exist in the client\",\n        status_code=404,\n      )\n\n    # Make the request\n    response = self._request(\n      method=\"POST\",\n      endpoint=\"/api/push\",\n      json={\n        \"model\": model,\n        \"insecure\": insecure,\n      },\n      stream=stream,\n    )\n\n    # Return the response\n    if stream:\n      return response\n    else:\n      return response.json()"}
{"namespace": "ollama._client.Client.push", "completion": "    url = self._get_url(\"/api/push\")\n    params = {\"model\": model}\n    headers = self._get_headers()\n\n    response = self._request(\n      method=\"POST\",\n      url=url,\n      params=params,\n      headers=headers,\n      insecure=insecure,\n      stream=stream,\n    )\n\n    if response.status_code == 200:\n      if stream:\n        return self._stream_response(response, ProgressResponse)\n      else:\n        return self._get_response(response, ProgressResponse)\n    else:\n      raise ResponseError(response)"}
{"namespace": "ollama._client.Client.push", "completion": "    url = self._base_url + \"/api/push\"\n    params = {\"model\": model}\n    headers = self._get_headers()\n\n    if insecure:\n      url = url.replace(\"https\", \"http\")\n\n    if stream:\n      return self._stream_request(\n        method=\"POST\",\n        url=url,\n        params=params,\n        headers=headers,\n        response_cls=ProgressResponse,\n      )\n\n    response = self._request(\n      method=\"POST\",\n      url=url,\n      params=params,\n      headers=headers,\n      response_cls=ProgressResponse,\n    )\n\n    return response.json()"}
{"namespace": "ollama._client.Client.push", "completion": "    url = f\"{self.base_url}/api/push\"\n\n    if not model:\n      raise ValueError(\"Model name is required\")\n\n    data = {\"model\": model}\n\n    if stream:\n      return self._request(\n        \"POST\",\n        url,\n        json=data,\n        insecure=insecure,\n        stream=True,\n        response_hook=self._progress_response_hook,\n      )\n    else:\n      return self._request(\n        \"POST\",\n        url,\n        json=data,\n        insecure=insecure,\n        response_hook=self._progress_response_hook,\n      )"}
{"namespace": "ollama._client.Client.push", "completion": "    endpoint = self.endpoints.push\n    url = self.url + endpoint\n    headers = self.headers\n    params = {\"model\": model}\n    data = {}\n    response = self.requests.post(\n        url,\n        headers=headers,\n        params=params,\n        data=data,\n        insecure=insecure,\n        stream=stream,\n    )\n    if response.status_code == 200:\n      if not stream:\n        return ProgressResponse(**response.json())\n      else:\n        return (ProgressResponse(**r) for r in response.iter_lines())\n    else:\n      raise ResponseError(response)"}
{"namespace": "ollama._client.Client.push", "completion": "    # Get the API endpoint URL\n    url = self.get_url(\n      endpoint=\"push\",\n      model=model,\n    )\n\n    # Make the POST request\n    response = self.post(\n      url=url,\n      insecure=insecure,\n      stream=stream,\n    )\n\n    # Handle the response\n    if stream:\n      # If streaming, yield ProgressResponse objects from the generator\n      yield from response\n    else:\n      # If not streaming, return the single ProgressResponse\n      return response"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided\")\n\n    if path is not None:\n      modelfile = open(path, \"rb\").read()\n\n    if stream:\n      return self._stream_create(model, modelfile)\n    else:\n      return self._create(model, modelfile)"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError(\"Either `path` or `modelfile` is required\")\n\n    if not modelfile:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      yield from self._stream(\n        \"POST\",\n        f\"/models/{model}\",\n        data={\"modelfile\": modelfile},\n        response_type=ProgressResponse,\n      )\n    else:\n      return self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        data={\"modelfile\": modelfile},\n        response_type=ProgressResponse,\n      )"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    params = {\"model\": model}\n    headers = {\"Content-Type\": \"application/octet-stream\"}\n    response = self._request(\n      \"POST\",\n      \"/models\",\n      params=params,\n      headers=headers,\n      data=modelfile,\n      stream=stream,\n    )\n\n    if stream:\n      return response\n    else:\n      return ProgressResponse(response)"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided.\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      return self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=modelfile,\n        stream=True,\n      )\n\n    return self._request(\n      \"POST\",\n      f\"/models/{model}\",\n      data=modelfile,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError(\"Either path or modelfile must be provided\")\n\n    if not modelfile and path:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    response = self.post(\n      f\"/models/{model}\",\n      data={\"modelfile\": modelfile},\n      stream=stream,\n    )\n\n    if stream:\n      return response\n\n    return response.json()"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"One of path or modelfile must be provided\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      return self._stream_request(\n        \"POST\",\n        f\"/models/{model}\",\n        data={\"modelfile\": modelfile},\n        response_cls=ProgressResponse,\n      )\n\n    return self._request(\n      \"POST\",\n      f\"/models/{model}\",\n      data={\"modelfile\": modelfile},\n      response_cls=ProgressResponse,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\n        \"Either `path` or `modelfile` must be provided.\",\n        code=400,\n        status=\"Bad Request\",\n      )\n\n    if modelfile is None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    return self._request(\n      \"POST\",\n      f\"/models/{model}\",\n      data=modelfile,\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError(\n        \"Either `path` or `modelfile` must be provided to create a model\"\n      )\n\n    if modelfile:\n      data = {\"modelfile\": modelfile}\n    else:\n      data = {\"path\": path}\n\n    if stream:\n      return self._stream_request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=data,\n        response_type=ProgressResponse,\n      )\n    else:\n      return self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=data,\n        response_type=ProgressResponse,\n      )"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"one of path or modelfile must be provided\")\n\n    if modelfile is None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      return self._stream(\n        \"POST\",\n        f\"/models/{model}\",\n        headers={\"Content-Type\": \"application/octet-stream\"},\n        data=modelfile,\n      )\n\n    return self._request(\n      \"POST\",\n      f\"/models/{model}\",\n      headers={\"Content-Type\": \"application/octet-stream\"},\n      data=modelfile,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    params = {\n      \"model\": model,\n    }\n\n    if stream:\n      return self._stream(\"POST\", \"/models\", params, modelfile)\n\n    response = self._request(\"POST\", \"/models\", params, modelfile)\n\n    return response.json()"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\n        \"Either path or modelfile must be provided\",\n        \"create\",\n        \"path\" if path is None else \"modelfile\",\n      )\n\n    if modelfile is None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      yield from self._request(\n        \"POST\",\n        f\"/model/{model}\",\n        data={\"modelfile\": modelfile},\n        response_type=ProgressResponse,\n      )\n    else:\n      return self._request(\n        \"POST\",\n        f\"/model/{model}\",\n        data={\"modelfile\": modelfile},\n        response_type=ProgressResponse,\n      )"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError(\"Either path or modelfile must be provided.\")\n\n    if not path:\n      path = modelfile\n\n    if isinstance(path, str):\n      path = Path(path)\n\n    if not path.exists():\n      raise RequestError(f\"Path {path} does not exist.\")\n\n    if not path.is_file():\n      raise RequestError(f\"Path {path} is not a file.\")\n\n    with path.open(\"rb\") as f:\n      modelfile = f.read()\n\n    return self._create(model, modelfile, stream)"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided.\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      yield from self.stream(\n        \"POST\",\n        \"/api/models\",\n        data={\"model\": model},\n        files={\"modelfile\": modelfile},\n        response_type=ProgressResponse,\n      )\n    else:\n      return self.request(\n        \"POST\",\n        \"/api/models\",\n        data={\"model\": model},\n        files={\"modelfile\": modelfile},\n        response_type=ProgressResponse,\n      )"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either `path` or `modelfile` must be provided.\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      yield from self._stream_request(\n        \"POST\",\n        f\"/models/{model}\",\n        data={\"modelfile\": modelfile},\n        response_type=ProgressResponse,\n      )\n    else:\n      return self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        data={\"modelfile\": modelfile},\n        response_type=ProgressResponse,\n      )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided.\")\n\n    if path is not None and modelfile is not None:\n      raise RequestError(\"Only one of path or modelfile can be provided.\")\n\n    if path is not None:\n      modelfile = read_file(path)\n\n    url = f\"{self.base_url}/models\"\n\n    data = {\n      \"model\": model,\n      \"modelfile\": modelfile,\n    }\n\n    if stream:\n      return self._stream_request(\n        method=\"POST\",\n        url=url,\n        data=data,\n        response_type=ProgressResponse,\n      )\n\n    response = self._request(\n      method=\"POST\",\n      url=url,\n      data=data,\n      response_type=ProgressResponse,\n    )\n\n    return response"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    response = self.post(\n      f\"{self.base_url}/models\",\n      files={\"model\": modelfile},\n      params={\"model\": model},\n    )\n\n    if not stream:\n      return ProgressResponse.from_response(response)\n\n    yield from ProgressResponse.from_response_stream(response)"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided\")\n\n    if path is not None:\n      path = Path(path)\n      if not path.exists():\n        raise RequestError(f\"Path {path} does not exist\")\n      if not path.is_file():\n        raise RequestError(f\"Path {path} is not a file\")\n\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    url = f\"{self.base_url}/models\"\n    data = {\"model\": model}\n    files = {\"modelfile\": modelfile}\n\n    if stream:\n      yield from self._stream_request(\n        url=url,\n        method=\"POST\",\n        data=data,\n        files=files,\n        response_type=ProgressResponse,\n      )\n    else:\n      response = self._request(\n        url=url,\n        method=\"POST\",\n        data=data,\n        files=files,\n        response_type=ProgressResponse,\n      )\n      return response.data"}
{"namespace": "ollama._client.Client.create", "completion": "    if modelfile is None and path is None:\n      raise RequestError(\"One of `path` or `modelfile` must be provided\")\n\n    if modelfile is None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    response = self.post(\n      \"/v1/models\",\n      json={\n        \"name\": model,\n        \"modelfile\": modelfile,\n      },\n    )\n\n    if stream:\n      return self._iter_responses(response)\n\n    return response.json()"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError(\n        \"One of `path` or `modelfile` is required\"\n      )\n\n    if not path:\n      with open(modelfile, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      return self.request(\n        \"POST\",\n        f\"/models/{model}\",\n        data={\"model\": modelfile},\n        stream=True,\n      )\n\n    response = self.request(\n      \"POST\",\n      f\"/models/{model}\",\n      data={\"model\": modelfile},\n    )\n\n    return response\n\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\n        \"One of path or modelfile must be provided\",\n        status_code=400,\n        headers={},\n        body=None,\n      )\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    response = self._request(\n      \"POST\",\n      f\"/models/{model}\",\n      headers={\"Content-Type\": \"application/x-tar\"},\n      data=modelfile,\n    )\n\n    if stream:\n      return self._stream(response, ProgressResponse)\n\n    return self._response(response, ProgressResponse)\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Open the file in binary mode and calculate its SHA-256 checksum\n    with open(path, 'rb') as file:\n      file_bytes = file.read()\n      file_hash = hashlib.sha256(file_bytes).hexdigest()\n\n    # Check if the blob already exists on the server\n    response = requests.head(f'{self.url}/blobs/{file_hash}', headers=self.headers)\n\n    # If the blob does not exist, upload it\n    if response.status_code == 404:\n      response = requests.post(f'{self.url}/blobs/uploads/', headers=self.headers, data=file_bytes)\n      response.raise_for_status()\n\n    # Return the digest of the file\n    return f'sha256:{file_hash}'"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Calculate the SHA-256 checksum of the file\n    with open(path, 'rb') as f:\n      file_data = f.read()\n      checksum = hashlib.sha256(file_data).hexdigest()\n\n    # Check if the blob already exists on the server\n    response = requests.head(f'{self.url}/blobs/{checksum}')\n    if response.status_code == 200:\n      return f'sha256:{checksum}'\n\n    # Upload the blob to the server\n    response = requests.post(f'{self.url}/blobs/uploads/', data=file_data)\n    if response.status_code != 201:\n      raise Exception(f'Failed to upload blob: {response.text}')\n\n    # Return the digest of the blob\n    return f'sha256:{checksum}'"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, \"rb\") as f:\n      checksum = hashlib.sha256(f.read()).hexdigest()\n\n    try:\n      response = self.session.head(f\"{self.url}/blobs/{checksum}\")\n    except requests.exceptions.RequestException as e:\n      raise e\n\n    if response.status_code == 404:\n      with open(path, \"rb\") as f:\n        response = self.session.post(\n            f\"{self.url}/blobs/uploads/\",\n            data={\"digest\": f\"sha256:{checksum}\"},\n            files={\"file\": f},\n        )\n\n      if response.status_code != 201:\n        raise Exception(\n            f\"Failed to upload blob. Status code: {response.status_code}\"\n        )\n\n    return f\"sha256:{checksum}\""}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Calculate the SHA-256 checksum of the file\n    sha256 = self._calculate_sha256(path)\n\n    # Check if the blob already exists on the server\n    response = self._session.head(f'{self._base_url}/blobs/{sha256}', headers=self._headers)\n\n    # If the blob does not exist, upload it\n    if response.status_code == 404:\n      with open(path, 'rb') as f:\n        response = self._session.post(f'{self._base_url}/blobs/uploads/', headers=self._headers, data=f)\n      response.raise_for_status()\n\n    return f'sha256:{sha256}'\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Open the file and calculate its SHA-256 checksum\n    with open(path, 'rb') as f:\n      file_data = f.read()\n      file_sha256 = hashlib.sha256(file_data).hexdigest()\n\n    # Check if the blob already exists on the server\n    response = requests.head(f'{self.url}/blobs/{file_sha256}', headers=self.headers)\n\n    # If the blob does not exist, upload it\n    if response.status_code == 404:\n      response = requests.post(f'{self.url}/blobs/uploads/', headers=self.headers, files={'file': open(path, 'rb')})\n      response.raise_for_status()\n\n    # Return the SHA-256 digest of the file\n    return f'sha256:{file_sha256}'\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      blob_data = f.read()\n\n    blob_hash = hashlib.sha256(blob_data).hexdigest()\n    blob_digest = f'sha256:{blob_hash}'\n\n    blob_exists = self._check_blob_exists(blob_digest)\n    if blob_exists:\n      return blob_digest\n\n    headers = {'Content-Type': 'application/octet-stream'}\n    response = self.session.post(f'{self.api_url}/blobs/uploads/', headers=headers, data=blob_data)\n    response.raise_for_status()\n\n    return blob_digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Calculate the SHA-256 checksum of the file\n    checksum = self._get_checksum(path)\n\n    # Check if the blob already exists on the server\n    response = self._head_blob(checksum)\n\n    # If the blob does not exist on the server, upload it\n    if response.status_code == 404:\n      with open(path, 'rb') as f:\n        self._upload_blob(checksum, f)\n\n    return checksum\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Calculate the SHA-256 checksum of the file\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n      while True:\n        data = f.read(BLOCK_SIZE)\n        if not data:\n          break\n        sha256.update(data)\n\n    # Convert the SHA-256 checksum to a hexadecimal string\n    hexdigest = sha256.hexdigest()\n\n    # Construct the digest of the file in the format 'sha256:<hexdigest>'\n    digest = 'sha256:' + hexdigest\n\n    # Check if the blob already exists on the server\n    response = self._request(f'HEAD {digest}')\n    if response.status_code == 404:\n      # Upload the file as a new blob\n      with open(path, 'rb') as f:\n        data = f.read()\n      response = self._request(f'POST blobs/uploads/', data=data)\n      if response.status_code != 200:\n        raise ValueError(f'Failed to upload blob: {response.text}')\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Calculate the SHA-256 checksum of the file\n    sha256 = self._sha256(path)\n\n    # Check if a blob with the same checksum already exists on the server\n    response = self._session.head(f'{self._url}/blobs/{sha256}')\n\n    # If the blob already exists, return its digest\n    if response.status_code == 200:\n      return sha256\n\n    # If the blob does not exist, upload the file as a new blob\n    with open(path, 'rb') as f:\n      data = f.read()\n    response = self._session.post(f'{self._url}/blobs/uploads/', data=data)\n\n    # Return the digest of the file\n    return response.json()['digest']\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Open the file and calculate its SHA-256 checksum\n    with open(path, 'rb') as f:\n      file_data = f.read()\n      file_sha256 = hashlib.sha256(file_data).hexdigest()\n\n    # Check if the blob already exists on the server\n    headers = {'Accept': 'application/vnd.docker.distribution.manifest.v2+json'}\n    response = self.session.head(f'{self.url}/blobs/{file_sha256}', headers=headers)\n\n    # If the blob does not exist on the server, upload it\n    if response.status_code == 404:\n      headers = {'Content-Type': 'application/octet-stream', 'Content-Length': str(len(file_data))}\n      response = self.session.post(f'{self.url}/blobs/uploads/', headers=headers)\n      location = response.headers['Location']\n      response = self.session.put(location, data=file_data)\n\n    # Return the digest of the file\n    return f'sha256:{file_sha256}'\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Open the file in binary mode and calculate its SHA-256 checksum\n    with open(path, 'rb') as f:\n      file_hash = hashlib.sha256()\n      while True:\n        chunk = f.read(8192)\n        if not chunk:\n          break\n        file_hash.update(chunk)\n      file_hash = file_hash.hexdigest()\n\n    # Check if a blob with the same checksum already exists on the server\n    response = requests.head(f'{self.base_url}/blobs/{file_hash}')\n\n    # If the blob does not exist, upload it as a new blob\n    if response.status_code == 404:\n      with open(path, 'rb') as f:\n        response = requests.post(f'{self.base_url}/blobs/uploads/', data=f)\n\n    # Return the SHA-256 digest of the file\n    return response.json()['digest']\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Open the file in binary mode and calculate its SHA-256 checksum\n    with open(path, 'rb') as file:\n      file_hash = hashlib.sha256()\n      while chunk := file.read(8192):\n        file_hash.update(chunk)\n      file_digest = file_hash.hexdigest()\n\n    # Check if the file already exists on the server\n    try:\n      response = requests.head(f'{self.url}/blobs/{file_digest}', headers=self.headers)\n      response.raise_for_status()\n      return f'sha256:{file_digest}'\n    except requests.exceptions.HTTPError as e:\n      if e.response.status_code == 404:\n        # If the file does not exist, upload it\n        with open(path, 'rb') as file:\n          response = requests.post(f'{self.url}/blobs/uploads/', headers=self.headers, data=file)\n        response.raise_for_status()\n        return f'sha256:{file_digest}'\n      else:\n        raise e\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      data = f.read()\n      digest = hashlib.sha256(data).hexdigest()\n      checksum = f'sha256:{digest}'\n\n      try:\n        self._session.head(f'{self._url}/blobs/{checksum}')\n      except requests.exceptions.HTTPError as e:\n        if e.response.status_code == 404:\n          headers = {'Content-Type': 'application/octet-stream'}\n          response = self._session.post(f'{self._url}/blobs/uploads/', headers=headers, data=data)\n          response.raise_for_status()\n        else:\n          raise\n\n    return checksum\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Open the file and calculate its SHA-256 checksum\n    with open(path, 'rb') as f:\n        file_content = f.read()\n        file_hash = hashlib.sha256(file_content).hexdigest()\n\n    # Check if the blob already exists on the server\n    url = f'{self.base_url}/blobs/{file_hash}'\n    response = requests.head(url, headers=self.headers)\n\n    # If the blob does not exist, upload it\n    if response.status_code == 404:\n        with open(path, 'rb') as f:\n            data = f.read()\n            response = requests.post(url, data=data, headers=self.headers)\n            response.raise_for_status()\n\n    # Return the SHA-256 digest of the file\n    return f'sha256:{file_hash}'\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Open the file and calculate its SHA-256 checksum\n    with open(path, 'rb') as f:\n      data = f.read()\n      sha256_digest = hashlib.sha256(data).hexdigest()\n\n    # Check if the blob already exists on the server\n    response = self._request(\n      method='HEAD',\n      path=f'/v2/<name>/blobs/{sha256_digest}',\n    )\n    if response.status_code == 404:\n      # Upload the file as a new blob\n      response = self._request(\n        method='POST',\n        path=f'/v2/<name>/blobs/uploads/',\n        data=data,\n        headers={'Content-Type': 'application/octet-stream'},\n      )\n      response.raise_for_status()\n\n    return f'sha256:{sha256_digest}'\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Open the file and calculate its SHA-256 checksum\n    with open(path, 'rb') as file:\n      file_content = file.read()\n      checksum = hashlib.sha256(file_content).hexdigest()\n\n    # Check if the blob already exists on the server\n    response = requests.head(f'{self.url}/blobs/{checksum}')\n    if response.status_code == 404:\n\n      # Upload the file as a new blob\n      headers = {'Content-Type': 'application/octet-stream'}\n      response = requests.post(f'{self.url}/blobs/uploads/', headers=headers, data=file_content)\n      response.raise_for_status()\n\n    # Return the digest of the file\n    return f'sha256:{checksum}'\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Calculate the SHA-256 checksum of the file\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n      while True:\n        data = f.read(BUFFER_SIZE)\n        if not data:\n          break\n        sha256.update(data)\n    digest = sha256.hexdigest()\n\n    # Check if the blob already exists on the server\n    url = f'{self.base_url}/blobs/{digest}'\n    headers = {'Accept': 'application/vnd.docker.distribution.manifest.v2+json'}\n    response = requests.head(url, headers=headers)\n\n    if response.status_code == 404:\n      # Upload the file as a new blob\n      with open(path, 'rb') as f:\n        data = f.read()\n        headers = {'Content-Type': 'application/octet-stream'}\n        response = requests.post(url, data=data, headers=headers)\n        response.raise_for_status()\n\n    return f'sha256:{digest}'\n\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Calculate the SHA-256 checksum of the file\n    sha256_checksum = self._calculate_sha256_checksum(path)\n\n    # Check if the blob already exists on the server\n    response = self._client.head(f'{self._base_url}/blobs/{sha256_checksum}')\n\n    # If the blob already exists, return its digest\n    if response.status_code == 200:\n      return f'sha256:{sha256_checksum}'\n\n    # If the blob does not exist, upload it to the server\n    with open(path, 'rb') as f:\n      response = self._client.post(f'{self._base_url}/blobs/uploads/', data=f)\n\n    # Return the digest of the uploaded file\n    return response.json()['digest']\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as file:\n      file_bytes = file.read()\n      file_checksum = hashlib.sha256(file_bytes).hexdigest()\n      file_size = len(file_bytes)\n\n    blob_digest = f'sha256:{file_checksum}'\n    blob_url = f'{self.base_url}/blobs/{blob_digest}'\n\n    try:\n      response = requests.head(blob_url, headers=self.headers)\n      response.raise_for_status()\n      if response.status_code == requests.codes.ok:\n        return blob_digest\n    except requests.exceptions.HTTPError as err:\n      if err.response.status_code == requests.codes.not_found:\n        pass\n      else:\n        raise err\n\n    headers = self.headers.copy()\n    headers['Content-Type'] = 'application/octet-stream'\n    headers['Content-Length'] = str(file_size)\n\n    response = requests.post(blob_url, headers=headers, data=file_bytes)\n    response.raise_for_status()\n\n    return blob_digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      data = f.read()\n      sha256 = hashlib.sha256(data).hexdigest()\n\n    digest = f'sha256:{sha256}'\n    headers = {'Digest': digest}\n    try:\n      self._session.head(f'{self._base_url}/blobs/{digest}', headers=headers)\n      return digest\n    except requests.exceptions.HTTPError as e:\n      if e.response.status_code == 404:\n        files = {'file': (path.name, data)}\n        self._session.post(f'{self._base_url}/blobs/uploads/', headers=headers, files=files)\n        return digest\n      else:\n        raise e\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if model == '':\n      raise ValueError('model must be provided')\n\n    request = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    response = await self._request(request)\n\n    if stream:\n      return response\n\n    return response.json()"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not prompt:\n      raise ValueError('prompt is required')\n\n    if not system:\n      raise ValueError('system is required')\n\n    if not template:\n      raise ValueError('template is required')\n\n    if not context:\n      raise ValueError('context is required')\n\n    if not images:\n      raise ValueError('images is required')\n\n    if not options:\n      raise ValueError('options is required')\n\n    if not keep_alive:\n      raise ValueError('keep_alive is required')\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create a new instance of the AsyncClient class\n    client = AsyncClient()\n\n    # Create"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model must be provided')\n\n    if not prompt:\n      raise ValueError('prompt must be provided')\n\n    if not system:\n      raise ValueError('system must be provided')\n\n    if not template:\n      raise ValueError('template must be provided')\n\n    if not context:\n      raise ValueError('context must be provided')\n\n    if not images:\n      raise ValueError('images must be provided')\n\n    if not options:\n      raise ValueError('options must be provided')\n\n    if not keep_alive:\n      raise ValueError('keep_alive must be provided')\n\n    if not isinstance(model, str):\n      raise ValueError('model must be a string')\n\n    if not isinstance(prompt, str):\n      raise ValueError('prompt must be a string')\n\n    if not isinstance(system, str):\n      raise ValueError('system must be a string')\n\n    if not isinstance(template, str):\n      raise ValueError('template must be a string')\n\n    if not isinstance(context, (list, tuple)):\n      raise ValueError('context must be a list or tuple')\n\n    if not isinstance(images, (list, tuple)):\n      raise ValueError('images must be a list or tuple')\n\n    if not isinstance(options, Options):\n      raise ValueError('options must be an instance of Options')\n\n    if not isinstance(keep_alive, (float, str)):\n      raise ValueError('keep_alive must be a float or string')\n\n    if not all(isinstance(c, int) for c in context):\n      raise ValueError('context must be a list of integers')\n\n    if not all(isinstance(i, str) for i in images):\n      raise ValueError('images must be a list of strings')\n\n    if not isinstance(stream, bool):\n      raise ValueError('stream must be a boolean')\n\n    if not isinstance(raw, bool):\n      raise ValueError('raw must be a boolean')\n\n    if not isinstance(format, str):\n      raise"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model must be provided')\n\n    if not prompt and not system and not template and not context and not images:\n      raise ValueError('prompt, system, template, context, or images must be provided')\n\n    if not isinstance(keep_alive, (float, str)):\n      raise ValueError('keep_alive must be a float or string')\n\n    if not isinstance(options, Options):\n      raise ValueError('options must be an instance of Options')\n\n    if not isinstance(format, str) or format not in ('', 'json'):\n      raise ValueError('format must be an empty string or \"json\"')\n\n    if not isinstance(raw, bool):\n      raise ValueError('raw must be a boolean')\n\n    if not isinstance(stream, bool):\n      raise ValueError('stream must be a boolean')\n\n    if not isinstance(images, (list, tuple)):\n      raise ValueError('images must be a list or tuple')\n\n    if not isinstance(context, (list, tuple)):\n      raise ValueError('context must be a list or tuple')\n\n    if not isinstance(system, str):\n      raise ValueError('system must be a string')\n\n    if not isinstance(template, str):\n      raise ValueError('template must be a string')\n\n    if not isinstance(prompt, str):\n      raise ValueError('prompt must be a string')\n\n    if not isinstance(model, str):\n      raise ValueError('model must be a string')\n\n    if not isinstance(keep_alive, (float, str)):\n      raise ValueError('keep_alive must be a float or string')\n\n    if not isinstance(options, Options):\n      raise ValueError('options must be an instance of Options')\n\n    if not isinstance(format, str) or format not in ('', 'json'):\n      raise ValueError('format must be an empty string or \"json\"')\n\n    if not isinstance(raw, bool):\n      raise ValueError('raw must be a boolean')\n\n    if not isinstance(stream,"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model must be provided')\n\n    if not prompt and not system and not template and not images:\n      raise ValueError('prompt, system, template, or images must be provided')\n\n    if not prompt and not system and not template and images:\n      raise ValueError('prompt, system, template, or images must be provided')\n\n    if not prompt and system and not template and not images:\n      raise ValueError('prompt, system, template, or images must be provided')\n\n    if not prompt and system and template and not images:\n      raise ValueError('prompt, system, template, or images must be provided')\n\n    if not prompt and not system and template and not images:\n      raise ValueError('prompt, system, template, or images must be provided')\n\n    if not prompt and not system and not template and images:\n      raise ValueError('prompt, system, template, or images must be provided')\n\n    if prompt and system and not template and not images:\n      raise ValueError('prompt, system, template, or images must be provided')\n\n    if prompt and system and template and not images:\n      raise ValueError('prompt, system, template, or images must be provided')\n\n    if prompt and system and not template and images:\n      raise ValueError('prompt, system, template, or images must be provided')\n\n    if prompt and not system and template and images:\n      raise ValueError('prompt, system, template, or images must be provided')\n\n    if prompt and system and template and images:\n      raise ValueError('prompt, system, template, or images must be provided')\n\n    if not prompt and system and template and images:\n      raise ValueError('prompt, system, template, or images must be provided')\n\n    if not prompt and not system and template and images:\n      raise ValueError('prompt, system, template, or images must be provided')\n\n    if not prompt and not system and not template and images:\n      raise ValueError('prompt, system, template, or images must be provided')\n\n    if not prompt and system and not template and images:\n      raise ValueError"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if model == '':\n      raise ValueError('model is required')\n\n    if not prompt and not system:\n      raise ValueError('prompt or system is required')\n\n    if not prompt and system:\n      prompt = system\n\n    if not system:\n      system = ''\n\n    if not template:\n      template = ''\n\n    if not context:\n      context = []\n\n    if not images:\n      images = []\n\n    if not options:\n      options = Options()\n\n    if not keep_alive:\n      keep_alive = ''\n\n    request = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    if raw:\n      request['raw'] = raw\n\n    if format:\n      request['format'] = format\n\n    if stream:\n      async for response in self._request(request):\n        yield response\n    else:\n      response = await self._request(request)\n      return response"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    # Check if model is provided\n    if not model:\n      raise ValueError(\"Model must be provided\")\n\n    # Prepare request data\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n    }\n\n    # Send request to server\n    response = await self.request(\n      method='POST',\n      url='/generate',\n      json=data,\n      keep_alive=keep_alive,\n    )\n\n    # Process response\n    if stream:\n      return self.stream(response)\n    else:\n      return self.process(response)"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if model == '':\n      raise ValueError('model must be provided')\n\n    if prompt == '':\n      raise ValueError('prompt must be provided')\n\n    if stream:\n      return await self.stream_generate(\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        raw=raw,\n        format=format,\n        images=images,\n        options=options,\n        keep_alive=keep_alive,\n      )\n    else:\n      return await self.request(\n        method='POST',\n        path='/generate',\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        raw=raw,\n        format=format,\n        images=images,\n        options=options,\n        keep_alive=keep_alive,\n      )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    if stream:\n      async with self._session.post(self._url, data=data, stream=True) as response:\n        if response.status != 200:\n          raise HTTPError(response.status, response.reason, response.text)\n\n        async for line in response.content:\n          yield json.loads(line)\n\n    else:\n      async with self._session.post(self._url, data=data) as response:\n        if response.status != 200:\n          raise HTTPError(response.status, response.reason, response.text)\n\n        return await response.json()"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    # Check if model is provided\n    if not model:\n      raise ValueError(\"Model is required\")\n\n    # Prepare the request data\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options.dict() if options else None,\n    }\n\n    # Send the request and handle the response\n    try:\n      async with self.session.post(\n        self.base_url + '/generate',\n        json=data,\n        timeout=self.timeout,\n        keep_alive=keep_alive,\n      ) as response:\n        if stream:\n          async for chunk in response.content.iter_chunked(1024):\n            yield json.loads(chunk)\n        else:\n          return json.loads(await response.content.read())\n    except Exception as e:\n      raise e"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('No model provided')\n\n    if not prompt and not system:\n      raise ValueError('No prompt or system provided')\n\n    if not options:\n      options = Options()\n\n    if not options.stream:\n      options.stream = stream\n\n    if not options.raw:\n      options.raw = raw\n\n    if not options.format:\n      options.format = format\n\n    if not options.images:\n      options.images = images\n\n    if not options.keep_alive:\n      options.keep_alive = keep_alive\n\n    if not options.context:\n      options.context = context\n\n    if not options.system:\n      options.system = system\n\n    if not options.template:\n      options.template = template\n\n    if not options.prompt:\n      options.prompt = prompt\n\n    return await self.request(\n      method='POST',\n      url=f'/v1/engines/{model}/generate',\n      headers=self.headers,\n      json=options.to_dict(),\n    )\n\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not prompt and not system and not template:\n      raise ValueError('prompt, system, or template is required')\n\n    if not prompt and not system and template:\n      raise ValueError('prompt or system is required when template is provided')\n\n    if template and not template.startswith('<|im_start|>') and not template.endswith('<|im_end|>'):\n      raise ValueError('template must start with <|im_start|> and end with <|im_end|>')\n\n    if template and not system:\n      raise ValueError('system is required when template is provided')\n\n    if template and not prompt:\n      raise ValueError('prompt is required when template is provided')\n\n    if template and not context:\n      raise ValueError('context is required when template is provided')\n\n    if template and not isinstance(context, list):\n      raise ValueError('context must be a list when template is provided')\n\n    if template and not all(isinstance(c, int) for c in context):\n      raise ValueError('context must be a list of integers when template is provided')\n\n    if template and len(context) != template.count('<|im_|>'):\n      raise ValueError('context must have the same number of elements as template placeholders')\n\n    if template and len(images) != template.count('<|im_|>'):\n      raise ValueError('images must have the same number of elements as template placeholders')\n\n    if template and not all(isinstance(im, str) for im in images):\n      raise ValueError('images must be a list of strings when template is provided')\n\n    if template and not all(im.startswith('data:image/') for im in images):\n      raise ValueError('images must be encoded as data URIs when template is provided')\n\n    if template and not all(im.endswith(';base64') for im in images):\n      raise ValueError('images must be encoded as base64 when template is provided')\n\n    if template and"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if not prompt and not system and not template:\n      raise ValueError('prompt, system, or template is required')\n\n    if not format and not raw:\n      raise ValueError('format or raw is required')\n\n    if stream and raw:\n      raise ValueError('raw and stream cannot be used together')\n\n    if context and not isinstance(context, Sequence):\n      raise ValueError('context must be a sequence')\n\n    if images and not isinstance(images, Sequence):\n      raise ValueError('images must be a sequence')\n\n    if images and not all(isinstance(image, str) for image in images):\n      raise ValueError('images must be encoded as strings')\n\n    if options and not isinstance(options, Options):\n      raise ValueError('options must be an instance of Options')\n\n    if keep_alive and not isinstance(keep_alive, (float, str)):\n      raise ValueError('keep_alive must be a float or a string')\n\n    if stream:\n      async for response in self._request(\n        method='POST',\n        path='/generate',\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        stream=stream,\n        raw=raw,\n        format=format,\n        images=images,\n        options=options,\n        keep_alive=keep_alive,\n      ):\n        yield response\n    else:\n      response = await self._request(\n        method='POST',\n        path='/generate',\n        model=model,\n        prompt=prompt,\n        system=system,\n        template=template,\n        context=context,\n        stream=stream,\n        raw=raw,\n        format=format,\n        images=images,\n        options=options,\n        keep_alive=keep_alive,\n      )\n      return response\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    # Validate input arguments\n    if model == '':\n      raise ValueError('model is required')\n\n    # Construct the request payload\n    payload = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n    }\n\n    # Send the request and handle the response\n    async with self.session.post(\n      self.url + '/generate',\n      json=payload,\n      headers=self.headers,\n      timeout=self.timeout,\n      keep_alive=keep_alive,\n    ) as response:\n      if response.status != 200:\n        raise HTTPError(\n          f'Request failed with status code {response.status}',\n          response=response,\n        )\n\n      if stream:\n        async for chunk in response.content:\n          yield chunk\n      else:\n        return await response.json()\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    # Validate the model parameter\n    if not model:\n      raise ValueError('model must be provided')\n\n    # Create a dictionary to store the request data\n    data = {}\n\n    # Add the prompt to the request data if provided\n    if prompt:\n      data['prompt'] = prompt\n\n    # Add the system to the request data if provided\n    if system:\n      data['system'] = system\n\n    # Add the template to the request data if provided\n    if template:\n      data['template'] = template\n\n    # Add the context to the request data if provided\n    if context:\n      data['context'] = context\n\n    # Add the raw flag to the request data if provided\n    if raw:\n      data['raw'] = raw\n\n    # Add the format to the request data if provided\n    if format:\n      data['format'] = format\n\n    # Add the images to the request data if provided\n    if images:\n      data['images'] = images\n\n    # Add the options to the request data if provided\n    if options:\n      data['options'] = options\n\n    # Add the keep_alive parameter to the request data if provided\n    if keep_alive:\n      data['keep_alive'] = keep_alive\n\n    # Send the request to the server and get the response\n    response = await self.post(model=model, data=data)\n\n    # If the response is streamed, return an asynchronous generator\n    if stream:\n      return response\n\n    # If the response is not streamed, return the response as a dictionary\n    return response\n\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model must be provided')\n\n    if not prompt and not system:\n      raise ValueError('prompt or system must be provided')\n\n    if not format:\n      format = 'json'\n\n    if not options:\n      options = Options()\n\n    if not options.stream:\n      options.stream = stream\n\n    if not options.raw:\n      options.raw = raw\n\n    if not options.format:\n      options.format = format\n\n    if not options.images:\n      options.images = images\n\n    if not options.context:\n      options.context = context\n\n    if not options.system:\n      options.system = system\n\n    if not options.template:\n      options.template = template\n\n    if not options.keep_alive:\n      options.keep_alive = keep_alive\n\n    if not options.prompt:\n      options.prompt = prompt\n\n    request = self._request(\n      method='POST',\n      url=self.base_url + '/generate',\n      headers=self.headers,\n      json={\n        'model': model,\n        'options': options.dict(),\n      },\n    )\n\n    if options.stream:\n      return await self._stream_request(request)\n\n    response = await self._send_request(request)\n    return response.json()"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if model == '':\n      raise ValueError('model must be provided')\n\n    if not self.is_ready:\n      raise RuntimeError('client is not ready')\n\n    if self.is_closed:\n      raise RuntimeError('client is closed')\n\n    if self.is_busy:\n      raise RuntimeError('client is busy')\n\n    if self.is_paused:\n      raise RuntimeError('client is paused')\n\n    if not self.is_connected:\n      raise RuntimeError('client is not connected')\n\n    if self.is_disconnected:\n      raise RuntimeError('client is disconnected')\n\n    if self.is_disconnecting:\n      raise RuntimeError('client is disconnecting')\n\n    if self.is_connecting:\n      raise RuntimeError('client is connecting')\n\n    if self.is_reconnecting:\n      raise RuntimeError('client is reconnecting')\n\n    if self.is_reconnecting_delayed:\n      raise RuntimeError('client is reconnecting delayed')\n\n    if self.is_reconnecting_immediate:\n      raise RuntimeError('client is reconnecting immediate')\n\n    if self.is_reconnecting_failed:\n      raise RuntimeError('client is reconnecting failed')\n\n    if self.is_reconnecting_manual:\n      raise RuntimeError('client is reconnecting manual')\n\n    if self.is_reconnecting_scheduled:\n      raise RuntimeError('client is reconnecting scheduled')\n\n    if self.is_reconnecting_timeout:\n      raise RuntimeError('client is reconnecting timeout')\n\n    if self.is_reconnecting_unknown:\n      raise RuntimeError('client is reconnecting unknown')\n\n    if self.is_reconnecting_unexpected:\n      raise RuntimeError('client is reconnecting unexpected')\n\n    if self.is_reconnecting_user:\n      raise RuntimeError('client is reconnecting user')\n\n    if self.is_reconnecting_user_delayed:\n      raise RuntimeError('client is reconnecting user delayed')\n\n    if self.is"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if model == '':\n      raise ValueError('Model must be specified')\n\n    if prompt == '':\n      raise ValueError('Prompt must be specified')\n\n    if not context:\n      context = []\n\n    if not images:\n      images = []\n\n    if not options:\n      options = Options()\n\n    if keep_alive:\n      options.keep_alive = keep_alive\n\n    request = Request(\n      model=model,\n      prompt=prompt,\n      system=system,\n      template=template,\n      context=context,\n      stream=stream,\n      raw=raw,\n      format=format,\n      images=images,\n      options=options,\n    )\n\n    try:\n      async with self.session.post(self.url, json=request.dict()) as response:\n        if response.status == 404:\n          raise NotFoundError(response.reason)\n\n        if response.status == 429:\n          raise RateLimitError(response.reason)\n\n        if response.status == 500:\n          raise ServerError(response.reason)\n\n        if response.status == 503:\n          raise ServiceUnavailableError(response.reason)\n\n        if response.status != 200:\n          raise UnknownError(response.reason)\n\n        if stream:\n          async for chunk in response.content:\n            yield json.loads(chunk)\n        else:\n          return json.loads(await response.text())\n\n    except aiohttp.ClientError as e:\n      raise ConnectionError(e) from e\n\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if model == '':\n      raise ValueError('model is required')\n\n    if not prompt and not system:\n      raise ValueError('prompt or system is required')\n\n    if not format and raw:\n      raise ValueError('raw cannot be used with format')\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be one of: '', 'json'')\n\n    if images is not None and not isinstance(images, Sequence):\n      raise ValueError('images must be a sequence')\n\n    if images is not None and not all(isinstance(image, str) for image in images):\n      raise ValueError('images must be encoded as strings')\n\n    if options is not None and not isinstance(options, Options):\n      raise ValueError('options must be an instance of Options')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise ValueError('keep_alive must be a float or string')\n\n    if keep_alive is not None and not isinstance(keep_alive, str) and keep_alive <= 0:\n      raise ValueError('keep_alive must be greater than 0')\n\n    if keep_alive is not None and isinstance(keep_alive, str) and keep_alive.lower() not in ['true', 'false']:\n      raise ValueError('keep_alive must be a boolean or a string')\n\n    if keep_alive is not None and isinstance(keep_alive, str) and keep_alive.lower() == 'true':\n      keep_alive = True\n    elif keep_alive is not None and isinstance(keep_alive, str) and keep_alive.lower() == 'false':\n      keep_alive = False\n\n    if context is not None and not isinstance(context, Sequence):\n      raise ValueError('context must be a sequence')\n\n    if context is not None and not all(isinstance(c, int) for c in context):\n      raise ValueError('context must be integers')\n\n    if context is not None and len(context"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    # Check if model is provided\n    if model == '':\n      raise ValueError(\"Model is required\")\n\n    # Prepare request data\n    request_data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'stream': stream,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    # Send request and handle response\n    response = await self.request('POST', '/generate', json=request_data)\n    if response.status_code == 200:\n      if stream:\n        return self.stream_response(response)\n      else:\n        return response.json()\n    else:\n      raise RequestError(response)\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if not model:\n      raise ValueError(\"model cannot be empty\")\n\n    if not self.api_key:\n      raise ValueError(\"api_key cannot be empty\")\n\n    if not self.api_secret:\n      raise ValueError(\"api_secret cannot be empty\")\n\n    if not self.api_endpoint:\n      raise ValueError(\"api_endpoint cannot be empty\")\n\n    if not self.api_version:\n      raise ValueError(\"api_version cannot be empty\")\n\n    if not self.api_key_id:\n      raise ValueError(\"api_key_id cannot be empty\")\n\n    if not self.api_key_secret:\n      raise ValueError(\"api_key_secret cannot be empty\")\n\n    if not self.api_key_algorithm:\n      raise ValueError(\"api_key_algorithm cannot be empty\")\n\n    if not self.api_key_timestamp:\n      raise ValueError(\"api_key_timestamp cannot be empty\")\n\n    if not self.api_key_nonce:\n      raise ValueError(\"api_key_nonce cannot be empty\")\n\n    if not self.api_key_signature:\n      raise ValueError(\"api_key_signature cannot be empty\")\n\n    if not self.api_key_signature_method:\n      raise ValueError(\"api_key_signature_method cannot be empty\")\n\n    if not self.api_key_signature_headers:\n      raise ValueError(\"api_key_signature_headers cannot be empty\")\n\n    if not self.api_key_signature_version:\n      raise ValueError(\"api_key_signature_version cannot be empty\")\n\n    if not self.api_key_signature_key:\n      raise ValueError(\"api_key_signature_key cannot be empty\")\n\n    if not self.api_key_signature_key_id:\n      raise ValueError(\"api_key_signature_key_id cannot be empty\")\n\n    if not self.api_key_signature_key_id_algorithm:\n      raise ValueError(\"api_key_signature_key_id_algorithm cannot be empty\")\n\n    if not self.api_key"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    url = f\"{self.base_url}/pull/{model}\"\n    headers = self.headers.copy()\n    if insecure:\n      headers[\"Authorization\"] = f\"Bearer {self.token}\"\n    async with self.session.get(url, headers=headers, stream=stream) as response:\n      if response.status != 200:\n        raise ResponseError(response)\n      if stream:\n        async for chunk in response.content:\n          yield json.loads(chunk)\n      else:\n        return await response.json()\n\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    url = self.get_url(model)\n    params = {\"insecure\": insecure}\n    headers = self.get_headers()\n    response = await self.client.get(url, params=params, headers=headers, stream=stream)\n    if response.status_code != 200:\n      raise ResponseError(response)\n    if stream:\n      return self.stream_response(response)\n    return self.get_response(response)"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    url = self.url(f\"/pull/{model}\")\n    headers = {\n      \"Accept\": \"application/json\",\n    }\n    params = {\n      \"insecure\": insecure,\n      \"stream\": stream,\n    }\n    response = await self.request(\n      method=\"GET\",\n      url=url,\n      headers=headers,\n      params=params,\n    )\n    if response.status_code == 200:\n      return response.json()\n    raise ResponseError(response)\n\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    url = f\"{self.api_url}/pull/{model}\"\n    headers = {\n      \"Authorization\": f\"Bearer {self.token}\",\n      \"Content-Type\": \"application/json\",\n    }\n    if insecure:\n      headers[\"X-Insecure\"] = \"true\"\n\n    async with self.session.get(url, headers=headers, stream=stream) as response:\n      if response.status != 200:\n        raise ResponseError(response.status, response.reason, await response.text())\n\n      if stream:\n        async for line in response.content:\n          yield ProgressResponse.from_json(line.decode())\n      else:\n        return ProgressResponse.from_json(await response.text())"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if stream:\n      async for resp in self.get(\n        url=f\"/models/{model}/pull\",\n        insecure=insecure,\n        stream=True,\n      ):\n        yield resp\n    else:\n      resp = await self.get(\n        url=f\"/models/{model}/pull\",\n        insecure=insecure,\n      )\n      return resp\n\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if model not in self.models:\n      raise ResponseError(\n        f\"Model {model} does not exist.\",\n        status_code=404,\n      )\n\n    if insecure:\n      url = f\"{self.url}/{model}/pull?insecure=true\"\n    else:\n      url = f\"{self.url}/{model}/pull\"\n\n    if stream:\n      async with self.session.get(url, stream=True) as response:\n        if response.status != 200:\n          raise ResponseError(\n            f\"Failed to pull data for model {model}\",\n            status_code=response.status,\n          )\n\n        async for chunk in response.content:\n          yield chunk\n    else:\n      async with self.session.get(url) as response:\n        if response.status != 200:\n          raise ResponseError(\n            f\"Failed to pull data for model {model}\",\n            status_code=response.status,\n          )\n\n        return await response.json()"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if not model:\n      raise ValueError(\"model cannot be empty\")\n\n    url = f\"{self.base_url}/{model}/pull\"\n\n    headers = {\n      \"Authorization\": f\"Bearer {self.token}\",\n      \"Content-Type\": \"application/json\"\n    }\n\n    if insecure:\n      headers[\"Authorization\"] = f\"Bearer {self.token}?insecure=true\"\n\n    async with aiohttp.ClientSession() as session:\n      async with session.get(url, headers=headers, stream=stream) as response:\n        if response.status == 200:\n          if stream:\n            async for line in response.content:\n              yield json.loads(line)\n          else:\n            return json.loads(await response.text())\n        else:\n          raise ResponseError(response.status, await response.text())"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if not model:\n      raise ValueError(\"'model' is required\")\n\n    url = f\"{self.base_url}/pull\"\n    params = {\n      \"model\": model,\n    }\n\n    if insecure:\n      params[\"insecure\"] = \"true\"\n\n    async with self.session.get(url, params=params, stream=stream) as resp:\n      if resp.status == 200:\n        if stream:\n          async for chunk in resp.content:\n            yield json.loads(chunk)\n        else:\n          return await resp.json()\n      else:\n        raise ResponseError(resp.status, await resp.text())"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if not model:\n      raise ValueError(\"Model name cannot be empty.\")\n\n    if not self.api_url:\n      raise ValueError(\"API URL is not set.\")\n\n    if not self.token:\n      raise ValueError(\"Token is not set.\")\n\n    if not self.api_key:\n      raise ValueError(\"API key is not set.\")\n\n    if not self.api_secret:\n      raise ValueError(\"API secret is not set.\")\n\n    if not self.api_key_id:\n      raise ValueError(\"API key ID is not set.\")\n\n    if not self.api_key_secret:\n      raise ValueError(\"API key secret is not set.\")\n\n    if not self.api_key_algorithm:\n      raise ValueError(\"API key algorithm is not set.\")\n\n    if not self.api_key_headers:\n      raise ValueError(\"API key headers are not set.\")\n\n    if not self.api_key_signature:\n      raise ValueError(\"API key signature is not set.\")\n\n    if not self.api_key_signature_headers:\n      raise ValueError(\"API key signature headers are not set.\")\n\n    if not self.api_key_signature_algorithm:\n      raise ValueError(\"API key signature algorithm is not set.\")\n\n    if not self.api_key_signature_encoding:\n      raise ValueError(\"API key signature encoding is not set.\")\n\n    if not self.api_key_signature_timestamp:\n      raise ValueError(\"API key signature timestamp is not set.\")\n\n    if not self.api_key_signature_nonce:\n      raise ValueError(\"API key signature nonce is not set.\")\n\n    if not self.api_key_signature_key_id:\n      raise ValueError(\"API key signature key ID is not set.\")\n\n    if not self.api_key_signature_key_secret:\n      raise ValueError(\"API key signature key secret is not set.\")\n\n    if not self.api_key_signature_key_algorithm:\n      raise ValueError(\"API key signature key algorithm is not set.\")\n\n    if not self.api_key_signature_key_headers:\n     "}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if not model:\n      raise ValueError(\"model must be specified\")\n\n    url = f\"{self.base_url}/pull/{model}\"\n\n    headers = self._get_headers()\n\n    if insecure:\n      headers[\"Authorization\"] = \"insecure\"\n\n    if stream:\n      async with self.session.get(url, headers=headers, stream=True) as response:\n        if response.status != 200:\n          raise ResponseError(response)\n\n        async for line in response.content:\n          yield json.loads(line)\n    else:\n      async with self.session.get(url, headers=headers) as response:\n        if response.status != 200:\n          raise ResponseError(response)\n\n        return await response.json()\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    try:\n      async with self.session.get(\n        url=f\"{self.url}/pull/{model}\",\n        headers=self.headers,\n        ssl=not insecure,\n        stream=stream,\n      ) as response:\n        if response.status != 200:\n          raise ResponseError(\n            f\"Failed to pull data for model '{model}'\",\n            response.status,\n            await response.text(),\n          )\n        if stream:\n          async for line in response.content:\n            yield json.loads(line.decode(\"utf-8\"))\n        else:\n          return await response.json()\n    except Exception as e:\n      raise ResponseError(\n        f\"Failed to pull data for model '{model}'\",\n        status=500,\n        body=str(e),\n      ) from e"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    # Build the request URL\n    url = f\"{self.base_url}/pull/{model}\"\n\n    # Set the request headers\n    headers = {\n      \"Accept\": \"application/json\",\n      \"Content-Type\": \"application/json\",\n      \"Authorization\": f\"Bearer {self.access_token}\",\n    }\n\n    # Make the request\n    async with self.session.get(\n      url,\n      headers=headers,\n      params={\n        \"insecure\": insecure,\n        \"stream\": stream,\n      },\n    ) as response:\n\n      # Check the response status code\n      if response.status != 200:\n        raise ResponseError(response)\n\n      # Return the response content as a generator if streaming\n      if stream:\n        async for line in response.content:\n          yield json.loads(line)\n\n      # Return the response content as a single object otherwise\n      else:\n        return await response.json()"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if not self.is_connected:\n      raise ResponseError(\"Not connected to server.\")\n\n    if not self.is_authenticated:\n      raise ResponseError(\"Not authenticated.\")\n\n    if not model:\n      raise ResponseError(\"Model name is required.\")\n\n    if insecure:\n      url = f\"{self.url}/insecure/pull/{model}\"\n    else:\n      url = f\"{self.url}/pull/{model}\"\n\n    if stream:\n      async with self.session.get(url, stream=True) as response:\n        if response.status != 200:\n          raise ResponseError(\n            f\"Error: {response.status} {response.reason}\")\n\n        async for line in response.content:\n          yield json.loads(line)\n    else:\n      async with self.session.get(url) as response:\n        if response.status != 200:\n          raise ResponseError(\n            f\"Error: {response.status} {response.reason}\")\n\n        return await response.json()"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    # Validate model\n    if not isinstance(model, str):\n      raise TypeError(f\"model must be a string, not {type(model)}\")\n\n    # Validate insecure\n    if not isinstance(insecure, bool):\n      raise TypeError(f\"insecure must be a boolean, not {type(insecure)}\")\n\n    # Validate stream\n    if not isinstance(stream, bool):\n      raise TypeError(f\"stream must be a boolean, not {type(stream)}\")\n\n    # Validate model\n    if model not in self.models:\n      raise ValueError(f\"model {model} not found\")\n\n    # Validate insecure\n    if insecure and not self.insecure:\n      raise ValueError(\"insecure mode not enabled\")\n\n    # Validate stream\n    if stream and not self.stream:\n      raise ValueError(\"stream mode not enabled\")\n\n    # Build request\n    request = self.build_request(\n      method=\"GET\",\n      path=f\"/pull/{model}\",\n      headers={\"Accept\": \"application/json\"},\n      insecure=insecure,\n      stream=stream,\n    )\n\n    # Send request\n    response = await self.send_request(request)\n\n    # Handle response\n    if response.status_code == 200:\n      if stream:\n        return self.stream_response(response)\n      else:\n        return self.parse_response(response)\n    else:\n      raise ResponseError(response)"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    # Check if the model is valid\n    if not isinstance(model, str):\n      raise ValueError(\"Invalid model type, expected str\")\n\n    # Check if the model is valid\n    if not isinstance(insecure, bool):\n      raise ValueError(\"Invalid insecure type, expected bool\")\n\n    # Check if the stream is valid\n    if not isinstance(stream, bool):\n      raise ValueError(\"Invalid stream type, expected bool\")\n\n    # Build the URL for the request\n    url = f\"{self.base_url}/pull/{model}\"\n\n    # Create a dictionary to store the request headers\n    headers = {\"Content-Type\": \"application/json\"}\n\n    # Create a dictionary to store the request data\n    data = {}\n\n    # Create a dictionary to store the request params\n    params = {}\n\n    # Create a dictionary to store the request files\n    files = {}\n\n    # Create a dictionary to store the request auth\n    auth = None\n\n    # Create a dictionary to store the request timeout\n    timeout = None\n\n    # Create a dictionary to store the request verify\n    verify = None\n\n    # Create a dictionary to store the request cert\n    cert = None\n\n    # Create a dictionary to store the request proxies\n    proxies = None\n\n    # Create a dictionary to store the request allow_redirects\n    allow_redirects = None\n\n    # Create a dictionary to store the request hooks\n    hooks = None\n\n    # Create a dictionary to store the request stream\n    stream = stream\n\n    # Create a dictionary to store the request json\n    json = None\n\n    # Create a dictionary to store the request data\n    data = None\n\n    # Create a dictionary to store the request params\n    params = None\n\n    # Create a dictionary to store the request headers\n    headers = None\n\n    # Create a dictionary to store the request cookies\n    cookies = None\n\n    # Create a dictionary to store the request auth\n    auth = None\n\n    # Create a dictionary to store the request timeout\n    timeout = None\n\n    # Create a dictionary to store the request verify\n    verify = None\n\n    # Create a"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    url = self.base_url + f\"/pull/{model}\"\n    headers = {\"Accept\": \"application/json\"}\n\n    if insecure:\n      headers[\"X-Insecure\"] = \"true\"\n\n    async with self.session.get(url, headers=headers, stream=stream) as response:\n      if response.status != 200:\n        raise ResponseError(response.status, response.content)\n\n      if stream:\n        async for line in response.content.iter_lines():\n          yield json.loads(line)\n      else:\n        return await response.json()\n\n\n\n\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if not model:\n      raise ValueError(\"Model name is required\")\n\n    url = self.url + \"/pull\"\n    headers = {\"Content-Type\": \"application/json\"}\n    params = {\"model\": model}\n\n    if insecure:\n      url = url.replace(\"https://\", \"http://\")\n\n    try:\n      async with self.session.get(\n        url,\n        headers=headers,\n        params=params,\n        timeout=self.timeout,\n        verify=not insecure,\n      ) as response:\n        if response.status != 200:\n          raise ResponseError(response)\n\n        if stream:\n          async for line in response.content:\n            yield json.loads(line)\n        else:\n          return await response.json()\n    except asyncio.TimeoutError as e:\n      raise TimeoutError(f\"Timeout error: {e}\") from e\n    except aiohttp.ClientError as e:\n      raise ClientError(f\"Client error: {e}\") from e\n    except Exception as e:\n      raise Exception(f\"Unexpected error: {e}\") from e\n\n\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    # Check if model is provided\n    if model is None:\n      raise ValueError(\"Model is required.\")\n\n    # Construct the URL\n    url = f\"{self.api_url}/pull/{model}\"\n\n    # Set the headers\n    headers = {\"Content-Type\": \"application/json\"}\n\n    # Set the method\n    method = \"GET\"\n\n    # Set the params\n    params = {\"insecure\": insecure}\n\n    # Set the stream\n    stream = stream\n\n    # Make the request\n    async with self.session.request(method, url, headers=headers, params=params, stream=stream) as response:\n\n      # Check if the response is successful\n      if response.status != 200:\n        raise ResponseError(response.status, response.reason)\n\n      # Check if the response is streamed\n      if stream:\n        # Yield the response as a generator\n        async for chunk in response.content:\n          yield json.loads(chunk)\n      else:\n        # Return the response as a dictionary\n        return await response.json()\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    # Check if the model exists\n    if model not in self.models:\n      raise ModelNotFoundError(f\"Model {model} not found\")\n\n    # Get the model\n    model_info = self.models[model]\n\n    # Get the model's endpoint\n    endpoint = model_info[\"endpoint\"]\n\n    # Get the model's ID\n    model_id = model_info[\"id\"]\n\n    # Get the model's pull endpoint\n    pull_endpoint = f\"{endpoint}/{model_id}/pull\"\n\n    # Create the request headers\n    headers = {\n      \"Content-Type\": \"application/json\",\n      \"Authorization\": f\"Bearer {self.api_key}\",\n    }\n\n    # Create the request data\n    data = {\"model\": model}\n\n    # Send the request\n    async with self.session.post(pull_endpoint, headers=headers, json=data, ssl=not insecure) as response:\n      # Check if the request was successful\n      if response.status != 200:\n        # Raise a ResponseError with the response status code and text\n        raise ResponseError(response.status, await response.text())\n\n      # Check if the response should be streamed\n      if stream:\n        # Create an async generator to yield the response\n        async for response in response.aiter_lines():\n          # Try to parse the response as JSON\n          try:\n            # Yield the parsed response as a `ProgressResponse`\n            yield ProgressResponse(**json.loads(response))\n          except json.JSONDecodeError:\n            # If the response cannot be parsed as JSON, raise a ResponseError\n            raise ResponseError(response.status, response)\n      else:\n        # Parse the response as JSON\n        response_data = await response.json()\n\n        # Return the parsed response as a `ProgressResponse`\n        return ProgressResponse(**response_data)"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate the input parameters\n    if not model:\n      raise ValueError(\"'model' is required\")\n    if not messages:\n      raise ValueError(\"'messages' is required\")\n    if not isinstance(messages, list):\n      raise ValueError(\"'messages' should be a list\")\n    if not all(isinstance(message, dict) for message in messages):\n      raise ValueError(\"'messages' should be a list of dictionaries\")\n    if not all(set(message.keys()) == {'role', 'content', 'images'} for message in messages):\n      raise ValueError(\"'messages' should be a list of dictionaries with keys 'role', 'content', and 'images'\")\n    if not all(isinstance(message['role'], str) for message in messages):\n      raise ValueError(\"'messages' should be a list of dictionaries with 'role' as a string\")\n    if not all(isinstance(message['content'], str) for message in messages):\n      raise ValueError(\"'messages' should be a list of dictionaries with 'content' as a string\")\n    if not all(isinstance(message['images'], list) for message in messages):\n      raise ValueError(\"'messages' should be a list of dictionaries with 'images' as a list\")\n    if not all(isinstance(image, str) for message in messages for image in message['images']):\n      raise ValueError(\"'messages' should be a list of dictionaries with 'images' as a list of strings\")\n    if not all(isinstance(message['role'], str) for message in messages):\n      raise ValueError(\"'messages' should be a list of dictionaries with 'role' as a string\")\n    if not all(isinstance(message['content'], str) for message in messages):\n      raise ValueError(\"'messages' should be a list of dictionaries with 'content' as a string\")\n    if not all(isinstance(message['images'], list) for message in messages):\n      raise ValueError(\"'messages' should be a list of dictionaries with 'images' as a list\")\n    if not all(isinstance(image, str) for message in messages for image in message['images']"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if model == '':\n      raise ValueError('model must be specified')\n\n    if messages is None:\n      raise ValueError('messages must be specified')\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be either \\'\\' or \\'json\\'')\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, float):\n        options.keep_alive = keep_alive\n      elif isinstance(keep_alive, str):\n        options.keep_alive = float(keep_alive)\n      else:\n        raise ValueError('keep_alive must be either a float or a string')\n\n    if not stream:\n      return await self._chat(\n        model=model,\n        messages=messages,\n        format=format,\n        options=options,\n      )\n\n    return self._chat_stream(\n      model=model,\n      messages=messages,\n      format=format,\n      options=options,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate input parameters\n    if not model:\n      raise ValueError('model is required')\n    if messages is None:\n      raise ValueError('messages is required')\n\n    # Validate messages\n    for message in messages:\n      if not isinstance(message, dict):\n        raise ValueError('messages must be a list of dictionaries')\n      if 'role' not in message or 'content' not in message:\n        raise ValueError('messages must have \"role\" and \"content\" keys')\n\n    # Validate format\n    if format not in ['', 'json']:\n      raise ValueError('format must be either \"\", \"json\", or None')\n\n    # Validate options\n    if options is not None:\n      if not isinstance(options, dict):\n        raise ValueError('options must be a dictionary')\n      if 'temperature' in options:\n        temperature = options['temperature']\n        if not isinstance(temperature, (int, float)):\n          raise ValueError('temperature must be a number')\n        if temperature < 0 or temperature > 2:\n          raise ValueError('temperature must be between 0 and 2')\n      if 'top_p' in options:\n        top_p = options['top_p']\n        if not isinstance(top_p, (int, float)):\n          raise ValueError('top_p must be a number')\n        if top_p < 0 or top_p > 1:\n          raise ValueError('top_p must be between 0 and 1')\n      if 'n' in options:\n        n = options['n']\n        if not isinstance(n, int):\n          raise ValueError('n must be an integer')\n        if n < 1 or n > 10:\n          raise ValueError('n must be between 1 and 10')\n      if 'stream' in options:\n        stream = options['stream']\n        if not isinstance(stream, bool):\n          raise ValueError('stream must be a boolean')\n      if 'stop' in options:\n        stop = options['stop']\n        if not isinstance(stop, (list, str)):\n          raise ValueError('stop must be a list"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate the input parameters\n    if not model:\n      raise ValueError('model is required')\n\n    if messages is None:\n      raise ValueError('messages is required')\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be one of [\\'\\', \\'json\\']')\n\n    if options is not None and not isinstance(options, Options):\n      raise ValueError('options must be an instance of Options')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise ValueError('keep_alive must be a float or a string')\n\n    # Make the asynchronous request to generate a chat response\n    response = await self._request(\n      method='POST',\n      path='/chat',\n      model=model,\n      messages=messages,\n      stream=stream,\n      format=format,\n      options=options,\n      keep_alive=keep_alive,\n    )\n\n    # Return the response as a single ChatResponse if 'stream' is False\n    if not stream:\n      return response\n\n    # Otherwise, return an asynchronous generator of ChatResponse\n    return self._stream_response(response)"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate input parameters\n    if not model:\n      raise ValueError('model must be specified')\n    if messages is None:\n      raise ValueError('messages must be specified')\n    if format not in ['', 'json']:\n      raise ValueError(f'format must be one of \"\", \"json\", got {format}')\n    if options is not None and not isinstance(options, Options):\n      raise ValueError('options must be an instance of Options')\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise ValueError('keep_alive must be a float or a string')\n\n    # Validate messages\n    if not isinstance(messages, Sequence):\n      raise ValueError('messages must be a sequence')\n    for message in messages:\n      if not isinstance(message, Mapping):\n        raise ValueError('messages must be a sequence of mappings')\n      if 'role' not in message or 'content' not in message:\n        raise ValueError('each message must have \"role\" and \"content\" keys')\n      if 'images' in message and not isinstance(message['images'], Sequence):\n        raise ValueError('\"images\" must be a sequence')\n\n    # Validate options\n    if options is not None:\n      if options.temperature is not None and not isinstance(options.temperature, (float, int)):\n        raise ValueError('temperature must be a float or an int')\n      if options.top_p is not None and not isinstance(options.top_p, (float, int)):\n        raise ValueError('top_p must be a float or an int')\n      if options.n is not None and not isinstance(options.n, int):\n        raise ValueError('n must be an int')\n      if options.stream is not None and not isinstance(options.stream, bool):\n        raise ValueError('stream must be a bool')\n      if options.stop is not None and not isinstance(options.stop, (list, tuple)):\n        raise ValueError('stop must be a list or a tuple')\n      if options.max_tokens is not None"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if model == '':\n      raise ValueError('model is required')\n\n    if messages is None:\n      raise ValueError('messages is required')\n\n    if format not in ['', 'json']:\n      raise ValueError(f'format must be one of [\\'\\', \\'json\\'], got {format}')\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, float):\n        options.keep_alive = keep_alive\n      else:\n        options.keep_alive = float(keep_alive)\n\n    if stream:\n      return await self._chat_stream(model, messages, format, options)\n    else:\n      return await self._chat_single(model, messages, format, options)\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if model == '':\n      raise ValueError('model must be specified')\n    if messages is None:\n      raise ValueError('messages must be specified')\n    if not isinstance(messages, (list, tuple)):\n      raise ValueError('messages must be a list or tuple')\n    if len(messages) == 0:\n      raise ValueError('messages must not be empty')\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is not None:\n      options.keep_alive = keep_alive\n\n    if format == 'json':\n      options.format = 'json'\n\n    if format == 'json' and not stream:\n      raise ValueError('format=json is only supported for streaming')\n\n    if format == 'json' and stream:\n      raise ValueError('format=json is not supported for streaming')\n\n    if format == '':\n      options.format = ''\n\n    if format == '':\n      if stream:\n        raise ValueError('format='' is not supported for streaming')\n\n    if format == '':\n      if not stream:\n        raise ValueError('format='' is only supported for streaming')\n\n    if format == 'json':\n      if stream:\n        raise ValueError('format=json is not supported for streaming')\n\n    if format == 'json':\n      if not stream:\n        raise ValueError('format=json is only supported for streaming')\n\n    if format == '':\n      if stream:\n        raise ValueError('format='' is not supported for streaming')\n\n    if format == '':\n      if not stream:\n        raise ValueError('format='' is only supported for streaming')\n\n    if format == 'json':\n      if stream:\n        raise ValueError('format=json is not supported for streaming')\n\n    if format == 'json':\n      if not stream:\n        raise ValueError('format=json is only supported for streaming')\n\n    if format == '':\n      if stream:\n        raise ValueError('format='' is not supported for streaming')\n\n    if format == '':\n      if not stream:\n        raise ValueError('format='' is only"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate the input parameters\n    if not model:\n      raise ValueError('model must be specified')\n    if not messages:\n      raise ValueError('messages must be specified')\n    if format not in ['', 'json']:\n      raise ValueError('format must be either \\'\\' or \\'json\\'')\n    if not isinstance(messages, list):\n      raise ValueError('messages must be a list')\n    for message in messages:\n      if not isinstance(message, dict):\n        raise ValueError('messages must be a list of dictionaries')\n      if 'role' not in message or 'content' not in message:\n        raise ValueError('each message must have \\'role\\' and \\'content\\' keys')\n      if 'images' in message and not isinstance(message['images'], list):\n        raise ValueError('\\'images\\' must be a list')\n      for image in message.get('images', []):\n        if not isinstance(image, dict):\n          raise ValueError('\\'images\\' must be a list of dictionaries')\n        if 'url' not in image:\n          raise ValueError('each image must have a \\'url\\' key')\n        if 'alt' not in image:\n          raise ValueError('each image must have an \\'alt\\' key')\n\n    # Make the request to generate a chat response\n    if stream:\n      async for response in self._request(\n        'POST',\n        f'/v1/chat/{model}',\n        json={\n          'messages': messages,\n          'options': options,\n          'keep_alive': keep_alive,\n        },\n        stream=True,\n      ):\n        if format == 'json':\n          yield response.json()\n        else:\n          yield response.text\n    else:\n      response = await self._request(\n        'POST',\n        f'/v1/chat/{model}',\n        json={\n          'messages': messages,\n          'options': options,\n          'keep_alive': keep_alive,\n        },\n      )\n      if format == 'json':\n        return response.json()\n      else:\n        return"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate the input parameters\n    if model == '':\n      raise ValueError(\"Model cannot be an empty string\")\n    if messages is None:\n      raise ValueError(\"Messages cannot be None\")\n    if format not in ['', 'json']:\n      raise ValueError(\"Format must be either '', 'json', or None\")\n    if options is not None and not isinstance(options, Options):\n      raise ValueError(\"Options must be an instance of Options\")\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise ValueError(\"Keep-alive must be a float or string\")\n\n    # Make the asynchronous request\n    response = await self.async_request(\n      'POST',\n      '/v1/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options,\n        'keep_alive': keep_alive,\n      },\n    )\n\n    # Check for errors in the response\n    if response.status_code != 200:\n      raise Exception(f\"Error: {response.status_code} {response.text}\")\n\n    # If 'stream' is False, return a single ChatResponse\n    if not stream:\n      return response.json()\n\n    # If 'stream' is True, return an asynchronous generator of ChatResponse\n    async for chunk in response.iter_content(chunk_size=1024):\n      if chunk:\n        yield json.loads(chunk)"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError(\"Model must be specified\")\n\n    if not messages:\n      raise ValueError(\"Messages must be specified\")\n\n    if not isinstance(messages, list):\n      raise ValueError(\"Messages must be a list\")\n\n    if not all(isinstance(message, dict) for message in messages):\n      raise ValueError(\"Messages must be a list of dictionaries\")\n\n    if not all(isinstance(message.get('role'), str) for message in messages):\n      raise ValueError(\"Each message must have a 'role' field\")\n\n    if not all(isinstance(message.get('content'), str) for message in messages):\n      raise ValueError(\"Each message must have a 'content' field\")\n\n    if not all(\n      isinstance(message.get('images'), list) and\n      all(isinstance(image, str) for image in message.get('images', []))\n      for message in messages\n    ):\n      raise ValueError(\"Each message must have a 'images' field\")\n\n    if format not in ['', 'json']:\n      raise ValueError(\"Invalid format\")\n\n    if options and not isinstance(options, Options):\n      raise ValueError(\"Options must be an instance of Options\")\n\n    if keep_alive:\n      if isinstance(keep_alive, float):\n        if keep_alive < 0:\n          raise ValueError(\"Keep alive must be a positive number\")\n      elif isinstance(keep_alive, str):\n        if not keep_alive.isdigit():\n          raise ValueError(\"Keep alive must be a positive number\")\n      else:\n        raise ValueError(\"Keep alive must be a positive number\")\n\n    if stream:\n      return await self._chat_stream(model, messages, format, options, keep_alive)\n\n    return await self._chat(model, messages, format, options, keep_alive)"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate input parameters\n    if not model:\n      raise ValueError('Model parameter is required.')\n    if not messages:\n      raise ValueError('Messages parameter is required.')\n    if not isinstance(messages, (list, tuple)):\n      raise ValueError('Messages parameter must be a list or tuple.')\n    if not all(isinstance(message, dict) for message in messages):\n      raise ValueError('Messages parameter must be a list of dictionaries.')\n    if not all(\n      'role' in message and 'content' in message\n      for message in messages\n    ):\n      raise ValueError('Messages must contain \"role\" and \"content\" keys.')\n    if not all(\n      isinstance(message['role'], str) and isinstance(message['content'], str)\n      for message in messages\n    ):\n      raise ValueError(\n        'Message \"role\" and \"content\" values must be strings.'\n      )\n    if not all(\n      'images' in message\n      for message in messages\n    ):\n      raise ValueError('Messages must contain \"images\" key.')\n    if not all(\n      isinstance(message['images'], (list, tuple))\n      for message in messages\n    ):\n      raise ValueError('Message \"images\" value must be a list or tuple.')\n    if not all(\n      isinstance(image, str)\n      for message in messages\n      for image in message['images']\n    ):\n      raise ValueError('Message \"images\" value must be a list of strings.')\n    if format not in ['', 'json']:\n      raise ValueError('Invalid format parameter.')\n    if options and not isinstance(options, Options):\n      raise ValueError('Options parameter must be an instance of Options.')\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise ValueError('Invalid keep_alive parameter.')\n\n    # Make request to generate chat response\n    async with self.session.post(\n      f'{self.url}/chat',\n      json={\n        'model': model,\n        'messages':"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if model == '':\n      raise ValueError('model must be specified')\n    if not messages:\n      raise ValueError('messages must be specified')\n    if format not in ['', 'json']:\n      raise ValueError('format must be either \\'\\' or \\'json\\'')\n\n    messages = [self._validate_message(message) for message in messages]\n\n    if options:\n      options = self._validate_options(options)\n\n    if keep_alive:\n      if isinstance(keep_alive, str):\n        keep_alive = self._validate_keep_alive(keep_alive)\n      else:\n        keep_alive = self._validate_keep_alive(str(keep_alive))\n\n    if stream:\n      return await self._chat_stream(model, messages, format, options, keep_alive)\n\n    return await self._chat(model, messages, format, options, keep_alive)\n\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate the input parameters\n    if model == '':\n      raise ValueError(\"Model cannot be an empty string.\")\n    if not messages:\n      raise ValueError(\"Messages cannot be an empty sequence.\")\n    if format not in ['', 'json']:\n      raise ValueError(\"Format must be either '', 'json', or None.\")\n    if options and not isinstance(options, Options):\n      raise ValueError(\"Options must be an instance of Options.\")\n\n    # Create the request body\n    body = {\n      \"model\": model,\n      \"messages\": [message.dict() for message in messages],\n      \"stream\": stream,\n      \"format\": format,\n      \"options\": options.dict() if options else {},\n      \"keep_alive\": keep_alive,\n    }\n\n    # Make the request\n    async with self.session.post(self.base_url + '/chat', json=body) as response:\n      if response.status == 200:\n        if stream:\n          async for chunk in response.content.iter_chunks():\n            yield json.loads(chunk.decode('utf-8'))\n        else:\n          return await response.json()\n      else:\n        raise HTTPError(response.status, response.reason, await response.text())\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if model == '':\n      raise ValueError(\"'model' cannot be an empty string\")\n\n    if messages is None:\n      raise ValueError(\"'messages' cannot be None\")\n\n    if format not in ['', 'json']:\n      raise ValueError(\"'format' must be either '', 'json', or None\")\n\n    if options is not None and not isinstance(options, Options):\n      raise ValueError(\"'options' must be an instance of Options\")\n\n    if keep_alive is not None:\n      if not isinstance(keep_alive, (float, str)):\n        raise ValueError(\"'keep_alive' must be a float or a string\")\n\n    if not isinstance(messages, Sequence):\n      raise ValueError(\"'messages' must be a sequence\")\n\n    if len(messages) == 0:\n      raise ValueError(\"'messages' cannot be an empty sequence\")\n\n    if not all(isinstance(message, Message) for message in messages):\n      raise ValueError(\"All messages must be instances of Message\")\n\n    if not all(message.role in ['system', 'user', 'assistant'] for message in messages):\n      raise ValueError(\"All messages must have a valid role\")\n\n    if not all(message.content for message in messages):\n      raise ValueError(\"All messages must have content\")\n\n    if not all(isinstance(message.images, (list, tuple)) for message in messages):\n      raise ValueError(\"All messages must have images as a list or tuple\")\n\n    if not all(isinstance(image, str) for message in messages for image in message.images):\n      raise ValueError(\"All images must be strings\")\n\n    if not all(isinstance(image, str) for message in messages for image in message.images):\n      raise ValueError(\"All images must be strings\")\n\n    if not all(len(image) <= 2048 for message in messages for image in message.images):\n      raise ValueError(\"All images must be less than or equal to 2048 characters\")\n\n    if not all(image.startswith('data:image/') for message in messages for image"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('model must be specified')\n\n    if messages is None:\n      raise ValueError('messages must be specified')\n\n    if not isinstance(messages, list) or not all(isinstance(message, dict) for message in messages):\n      raise ValueError('messages must be a list of dictionaries')\n\n    if not isinstance(stream, bool):\n      raise ValueError('stream must be a boolean')\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be either an empty string or \"json\"')\n\n    if options is not None and not isinstance(options, Options):\n      raise ValueError('options must be an instance of Options')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise ValueError('keep_alive must be a float or a string')\n\n    if keep_alive is not None and isinstance(keep_alive, float) and keep_alive < 0:\n      raise ValueError('keep_alive must be a non-negative float')\n\n    if keep_alive is not None and isinstance(keep_alive, str) and not keep_alive.isdigit():\n      raise ValueError('keep_alive must be a string of digits')\n\n    if keep_alive is not None and isinstance(keep_alive, str) and float(keep_alive) < 0:\n      raise ValueError('keep_alive must be a non-negative float')\n\n    messages = [Message(**message) for message in messages]\n\n    if options is None:\n      options = Options()\n\n    if format == 'json':\n      options.stream = stream\n\n    if keep_alive is not None:\n      options.keep_alive = keep_alive\n\n    async with self.session.post(\n      self.url,\n      json={\n        'model': model,\n        'messages': [message.dict() for message in messages],\n        'options': options.dict(),\n      },\n    ) as response:\n      response.raise_"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate the input parameters\n    if not model:\n      raise ValueError('\"model\" is required')\n    if not messages:\n      raise ValueError('\"messages\" is required')\n    if format not in ['', 'json']:\n      raise ValueError('\"format\" must be one of \"\", \"json\"')\n\n    # Validate the messages\n    for message in messages:\n      if 'role' not in message:\n        raise ValueError('Each message must have a \"role\"')\n      if 'content' not in message:\n        raise ValueError('Each message must have a \"content\"')\n      if 'images' in message and not isinstance(message['images'], list):\n        raise ValueError('\"images\" must be a list')\n      if 'images' in message:\n        for image in message['images']:\n          if 'src' not in image:\n            raise ValueError('Each image must have a \"src\"')\n          if 'alt' not in image:\n            raise ValueError('Each image must have an \"alt\"')\n          if 'caption' not in image:\n            raise ValueError('Each image must have a \"caption\"')\n\n    # Create the request body\n    body = {\n      'model': model,\n      'messages': messages,\n      'stream': stream,\n      'format': format,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    # Make the request\n    async with self._session.post(\n      self._base_url + '/chat',\n      json=body,\n      headers=self._headers,\n      timeout=self._timeout,\n    ) as response:\n      # Handle the response\n      if response.status == 200:\n        if stream:\n          async for chunk in response.content.iter_chunked(1024):\n            yield json.loads(chunk.decode('utf-8'))\n        else:\n          return await response.json()\n      else:\n        raise HTTPError(response.status, await response.text())"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate the input parameters\n    if not model:\n      raise ValueError('Missing required parameter: model')\n\n    if not messages:\n      raise ValueError('Missing required parameter: messages')\n\n    if not isinstance(messages, Sequence):\n      raise ValueError('messages must be a sequence')\n\n    if not all(isinstance(message, dict) for message in messages):\n      raise ValueError('messages must be a sequence of dictionaries')\n\n    if format not in ['', 'json']:\n      raise ValueError('Invalid format value')\n\n    if options and not isinstance(options, Options):\n      raise ValueError('options must be an instance of Options')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise ValueError('keep_alive must be a float or a string')\n\n    # Create the request body\n    request_body = {\n      'model': model,\n      'messages': messages,\n      'format': format,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    # Make the request\n    response = await self._request('POST', '/chat', json=request_body)\n\n    # If the response is a single ChatResponse, return it\n    if not stream:\n      return response\n\n    # If the response is a stream of ChatResponse, return an asynchronous generator\n    async for response in response.aiter_lines(decode=True):\n      yield json.loads(response)"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise ValueError('Model must be specified.')\n\n    if not messages:\n      raise ValueError('Messages must be specified.')\n\n    if not isinstance(messages, Sequence):\n      raise ValueError('Messages must be a sequence.')\n\n    if len(messages) == 0:\n      raise ValueError('Messages must not be empty.')\n\n    if format not in ['', 'json']:\n      raise ValueError('Format must be either \"\", \"json\".')\n\n    if options:\n      if not isinstance(options, Options):\n        raise ValueError('Options must be of type Options.')\n\n    if keep_alive:\n      if not isinstance(keep_alive, (float, str)):\n        raise ValueError('Keep-alive must be a float or a string.')\n\n    for message in messages:\n      if not isinstance(message, dict):\n        raise ValueError('Messages must be dictionaries.')\n\n      if 'role' not in message:\n        raise ValueError('Messages must have a role.')\n\n      if 'content' not in message:\n        raise ValueError('Messages must have content.')\n\n      if not isinstance(message['role'], str):\n        raise ValueError('Message roles must be strings.')\n\n      if not isinstance(message['content'], str):\n        raise ValueError('Message content must be strings.')\n\n      if 'images' in message:\n        if not isinstance(message['images'], list):\n          raise ValueError('Message images must be a list.')\n\n        for image in message['images']:\n          if not isinstance(image, dict):\n            raise ValueError('Image must be a dictionary.')\n\n          if 'url' not in image:\n            raise ValueError('Image must have a url.')\n\n          if not isinstance(image['url'], str):\n            raise ValueError('Image url must be a string.')\n\n    if stream:\n      async for response in self._stream_chat(\n        model=model,\n        messages=messages,\n        format=format,\n        options=options,\n        keep_alive=keep"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate input parameters\n    if not model:\n      raise ValueError('model is required')\n    if not messages:\n      raise ValueError('messages is required')\n\n    # Validate each message in the messages list\n    for message in messages:\n      if not isinstance(message, dict):\n        raise ValueError('messages should be a list of dictionaries')\n      if 'role' not in message or 'content' not in message:\n        raise ValueError('each message should have \"role\" and \"content\" keys')\n      if 'images' in message:\n        if not isinstance(message['images'], list):\n          raise ValueError('\"images\" should be a list')\n        for image in message['images']:\n          if not isinstance(image, dict):\n            raise ValueError('\"images\" should be a list of dictionaries')\n          if 'url' not in image or 'alt' not in image:\n            raise ValueError('each image should have \"url\" and \"alt\" keys')\n\n    # Set up the request parameters\n    url = self.base_url + '/v1/chat'\n    headers = {'Authorization': f'Bearer {self.api_key}'}\n    params = {'model': model}\n    if format:\n      params['format'] = format\n    if options:\n      params['options'] = options.to_dict()\n    if keep_alive:\n      params['keep_alive'] = keep_alive\n\n    # Make the asynchronous request\n    async with self.session.post(url, headers=headers, params=params, json={'messages': messages}) as response:\n      response.raise_for_status()\n      if stream:\n        async for chunk in response.content.iter_chunks():\n          yield json.loads(chunk)\n      else:\n        return await response.json()"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate the input parameters\n    if not model:\n      raise ValueError(\"Model is required\")\n\n    if not messages:\n      raise ValueError(\"Messages are required\")\n\n    if not isinstance(messages, list):\n      raise ValueError(\"Messages must be a list\")\n\n    for message in messages:\n      if not isinstance(message, dict):\n        raise ValueError(\"Messages must be a list of dictionaries\")\n\n      if not message.get(\"role\"):\n        raise ValueError(\"Role is required for each message\")\n\n      if not message.get(\"content\"):\n        raise ValueError(\"Content is required for each message\")\n\n    if format not in ['', 'json']:\n      raise ValueError(\"Format must be one of '', 'json'\")\n\n    if options and not isinstance(options, Options):\n      raise ValueError(\"Options must be an instance of Options\")\n\n    if keep_alive and not isinstance(keep_alive, (float, str)):\n      raise ValueError(\"Keep-alive must be a float or a string\")\n\n    # Construct the request data\n    request_data = {\n      \"model\": model,\n      \"messages\": messages,\n      \"stream\": stream,\n      \"format\": format,\n      \"options\": options,\n      \"keep_alive\": keep_alive\n    }\n\n    # Make the request\n    response = await self.client.post(\n      '/chat',\n      json=request_data,\n      timeout=self.client.timeout,\n      headers=self.client.headers\n    )\n\n    # Handle the response\n    if response.status_code != 200:\n      raise HTTPError(\n        f\"Request failed with status code {response.status_code}\",\n        response=response\n      )\n\n    if not stream:\n      return response.json()\n\n    async for line in response.iter_lines():\n      if line:\n        yield json.loads(line)"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    ...\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    url = f\"/api/push\"\n\n    params = {\n      \"model\": model,\n      \"insecure\": insecure,\n      \"stream\": stream,\n    }\n\n    return await self.request(\n      \"POST\",\n      url,\n      params=params,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    request = PushRequest(\n      model=model,\n      insecure=insecure,\n    )\n\n    response = await self.post(\n      url='/api/push',\n      content=request,\n      response_model=ProgressResponse,\n      stream=stream,\n    )\n\n    return response"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    params = {}\n    params[\"model\"] = model\n    params[\"insecure\"] = insecure\n    params[\"stream\"] = stream\n\n    return await self._request(\n      method=\"POST\",\n      path=\"/api/push\",\n      params=params,\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    params = {}\n    if insecure:\n      params[\"insecure\"] = insecure\n\n    response = await self.post(\n      \"/api/push\",\n      params=params,\n      json={\"model\": model},\n      stream=stream,\n    )\n\n    if stream:\n      return response\n\n    return await response.json()"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._async_request(\n      method='POST',\n      path='/api/push',\n      insecure=insecure,\n      stream=stream,\n      data={'model': model},\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    response = await self._request(\n      method='POST',\n      url=f'/api/push',\n      params={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n    )\n\n    if stream:\n      async for item in response.aiter_lines():\n        yield ProgressResponse.parse_raw(item)\n    else:\n      return ProgressResponse.parse_raw(await response.aread())"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    url = \"/api/push\"\n    params = {\"model\": model}\n    if insecure:\n      params[\"insecure\"] = insecure\n    if stream:\n      params[\"stream\"] = stream\n    return await self._send_request(\n      \"POST\",\n      url,\n      params=params,\n      response_model=ProgressResponse,\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    endpoint = \"/api/push\"\n\n    params = {\n      \"model\": model,\n      \"insecure\": insecure,\n    }\n\n    response = await self.request(\n      \"POST\",\n      endpoint,\n      params=params,\n      stream=stream,\n    )\n\n    if stream:\n      return (ProgressResponse(**item) for item in response)\n    else:\n      return ProgressResponse(**response)"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    url = f\"{self.base_url}/api/push\"\n    headers = {\"accept\": \"application/json\"}\n    query_params = {\"model\": model}\n\n    async with self.client.post(\n      url,\n      headers=headers,\n      params=query_params,\n      insecure=insecure,\n      stream=stream,\n    ) as response:\n      if response.status == 200:\n        return await response.json()\n      else:\n        raise Exception(response.status, await response.text())"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    url = self.base_url.rstrip(\"/\") + \"/api/push\"\n    headers: Dict[str, str] = {}\n    params: Dict[str, Any] = {}\n    json: Dict[str, Any] = {}\n    data: Dict[str, Any] = {}\n    data[\"model\"] = model\n    response = await self._client.post(\n      url=url,\n      headers=headers,\n      params=params,\n      json=json,\n      data=data,\n      insecure=insecure,\n      stream=stream,\n    )\n    if response.status_code == 200:\n      return await response.json()\n    else:\n      raise HTTPError(response=response)"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    # Create the request payload\n    payload = {\"model\": model}\n\n    # Send the request and handle the response\n    if stream:\n      # If stream is True, return a generator of ProgressResponse objects\n      async for response in self.request(\n        \"POST\", \"/api/push\", json=payload, insecure=insecure\n      ):\n        yield response\n    else:\n      # If stream is False, return a single ProgressResponse object\n      response = await self.request(\n        \"POST\", \"/api/push\", json=payload, insecure=insecure\n      )\n      return response"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    response = await self._post(\n      url=f\"/api/push\",\n      json={\n        \"model\": model,\n        \"insecure\": insecure,\n        \"stream\": stream,\n      },\n      response_model=Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]],\n    )\n    return response\n\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    url = f'{self.base_url}/api/push'\n\n    headers = {\n      'Content-Type': 'application/json',\n    }\n\n    data = {\n      'model': model,\n    }\n\n    response = await self.session.post(\n      url,\n      headers=headers,\n      json=data,\n      verify=not insecure,\n    )\n\n    if stream:\n      async for chunk in response.content:\n        yield ProgressResponse.parse_raw(chunk)\n    else:\n      return ProgressResponse.parse_raw(await response.content.read())"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    url = f\"{self.base_url}/api/push\"\n    headers = {\"Content-Type\": \"application/json\"}\n    data = {\"model\": model}\n\n    if insecure:\n      headers[\"X-Insecure\"] = \"true\"\n\n    async with self.session.post(url, headers=headers, json=data, stream=stream) as response:\n      response.raise_for_status()\n      if stream:\n        async for chunk in response.content:\n          yield chunk\n      else:\n        return await response.json()\n\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    url = self.base_url + \"/api/push\"\n\n    headers = {}\n    if insecure:\n      headers[\"X-Insecure\"] = \"true\"\n\n    data = {\n      \"model\": model,\n    }\n\n    async with self.client.post(url, headers=headers, json=data, stream=stream) as response:\n      if response.status == 200:\n        if stream:\n          async for line in response.content:\n            yield ProgressResponse.parse_raw(line)\n        else:\n          return ProgressResponse.parse_raw(await response.read())\n      else:\n        raise Exception(await response.text())\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    # Create the request payload\n    payload = {\"model\": model}\n\n    # Make the POST request\n    response = await self._make_request(\n      method=\"POST\",\n      path=\"/api/push\",\n      payload=payload,\n      insecure=insecure,\n      stream=stream,\n    )\n\n    # Handle the response\n    if stream:\n      return self._stream_response(response, ProgressResponse)\n    else:\n      return self._get_response(response, ProgressResponse)"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    url = f\"{self.base_url}/api/push\"\n\n    headers = {\n      \"accept\": \"application/json\",\n      \"Content-Type\": \"application/json\",\n    }\n\n    data = {\n      \"model\": model,\n      \"insecure\": insecure,\n    }\n\n    response = await self.client.post(\n      url=url,\n      headers=headers,\n      json=data,\n      stream=stream,\n    )\n\n    if response.status_code == 200:\n      if stream:\n        return self.response(\n          response=response,\n          output=ProgressResponse,\n          stream=True,\n        )\n      return self.response(\n        response=response,\n        output=ProgressResponse,\n      )\n\n    raise HTTPException(\n      status_code=response.status_code,\n      detail=response.text,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    # Define the endpoint URL\n    endpoint = \"/api/push\"\n\n    # Define the request payload\n    payload = {\n      \"model\": model,\n    }\n\n    # Make the POST request\n    response = await self._request(\n      \"POST\",\n      endpoint,\n      payload=payload,\n      insecure=insecure,\n      stream=stream,\n    )\n\n    # Handle the response\n    if stream:\n      # If stream is True, return a generator of ProgressResponse objects\n      async for chunk in response.iter_text():\n        yield json.loads(chunk)\n    else:\n      # If stream is False, return a single ProgressResponse object\n      return await response.json()"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    url = self.base_url + \"/api/push\"\n    params: Dict[str, Any] = {\n        \"model\": model,\n        \"insecure\": insecure,\n        \"stream\": stream,\n    }\n    response = await self.request(\n        \"POST\",\n        url,\n        params=params,\n        json=None,\n        files=None,\n        headers=None,\n        timeout=None,\n    )\n    if response.status_code == 200:\n      if stream:\n        return self.response(response, \"ProgressResponse\", stream=True)\n      else:\n        return self.response(response, \"ProgressResponse\")\n    raise Exception(f\"Error: {response.status_code}\")"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = await self._calculate_checksum(path)\n\n    # Check if the blob already exists on the server\n    try:\n      await self.head_blob(checksum)\n      return checksum\n    except HTTPStatusError as e:\n      if e.status_code != 404:\n        raise e\n\n    # Upload the file to the server in chunks\n    async with aiofiles.open(path, \"rb\") as f:\n      while True:\n        chunk = await f.read(self.chunk_size)\n        if not chunk:\n          break\n        await self.post_blob(chunk)\n\n    return checksum\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = self._calculate_checksum(path)\n\n    # Check if the blob already exists on the server\n    async with self.session.head(f\"{self.base_url}/blobs/{checksum}\") as resp:\n      if resp.status == 404:\n        # Blob not found, upload the file\n        async with self.session.post(f\"{self.base_url}/blobs/uploads/\", data=await self._read_file(path)) as resp:\n          if resp.status == 202:\n            # Upload accepted, return the digest\n            return f\"sha256:{checksum}\"\n          else:\n            # Upload failed, raise an exception\n            raise Exception(f\"Upload failed with status code {resp.status}\")\n      elif resp.status == 200:\n        # Blob found, return the digest\n        return f\"sha256:{checksum}\"\n      else:\n        # Other status code, raise an exception\n        raise Exception(f\"HEAD request failed with status code {resp.status}\")\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = sha256(path.read_bytes()).hexdigest()\n\n    # Check if the blob already exists on the server\n    async with self.session.head(f\"{self.url}/blobs/{checksum}\") as response:\n      if response.status == 404:\n        # Upload the file to the server in chunks\n        async with self.session.post(f\"{self.url}/blobs/uploads/\", data=await self._read_file(path)) as response:\n          if response.status != 200:\n            raise ValueError(f\"Failed to upload file: {await response.text()}\")\n          return response.headers[\"Docker-Content-Digest\"]\n      else:\n        return response.headers[\"Docker-Content-Digest\"]\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    with open(path, \"rb\") as f:\n      checksum = hashlib.sha256(f.read()).hexdigest()\n    head_url = f\"{self.url}/blobs/{checksum}\"\n    head_response = await self.session.head(head_url)\n    if head_response.status == 404:\n      with open(path, \"rb\") as f:\n        async with aiohttp.ClientSession() as session:\n          async with session.post(self.url + \"/blobs/uploads/\", data={\"name\": path.name, \"digest\": f\"sha256:{checksum}\"}) as upload_response:\n            upload_location = upload_response.headers[\"Location\"]\n            async with session.put(upload_location, data=f) as upload_chunk_response:\n              if upload_chunk_response.status != 202:\n                raise ValueError(\"Failed to upload blob\")\n    return f\"sha256:{checksum}\""}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Read the file in binary mode and calculate its SHA256 checksum\n    with open(path, 'rb') as f:\n      file_content = f.read()\n    checksum = hashlib.sha256(file_content).hexdigest()\n\n    # Check if a blob with the calculated checksum already exists on the server\n    url = f'{self.base_url}/blobs/{checksum}'\n    response = await self.session.head(url)\n\n    # If the blob does not exist, upload the file to the server\n    if response.status == 404:\n      url = f'{self.base_url}/blobs/uploads/'\n      headers = {'Content-Type': 'application/octet-stream'}\n      data = file_content\n      response = await self.session.post(url, headers=headers, data=data)\n      response.raise_for_status()\n\n    # Return the digest of the file\n    return f'sha256:{checksum}'\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = await self._calculate_checksum(path)\n\n    # Check if the blob already exists on the server\n    response = await self.head(f\"{self.base_url}/blobs/{checksum}\")\n    if response.status_code == 200:\n      # The blob already exists, so return the digest\n      return f\"sha256:{checksum}\"\n\n    # The blob does not exist, so upload it to the server\n    with open(path, \"rb\") as f:\n      # Calculate the size of the file\n      size = f.seek(0, 2)\n      f.seek(0)\n\n      # Calculate the number of chunks to upload\n      num_chunks = math.ceil(size / self.chunk_size)\n\n      # Upload the file in chunks\n      for i in range(num_chunks):\n        # Calculate the offset and size of the current chunk\n        offset = i * self.chunk_size\n        chunk_size = min(self.chunk_size, size - offset)\n\n        # Read the chunk from the file\n        chunk = f.read(chunk_size)\n\n        # Upload the chunk to the server\n        response = await self.post(\n            f\"{self.base_url}/blobs/uploads/\",\n            headers={\"Content-Type\": \"application/octet-stream\"},\n            data=chunk,\n            params={\"digest\": f\"sha256:{checksum}\"},\n        )\n\n        # Check if the upload was successful\n        if response.status_code != 201:\n          raise Exception(\n              f\"Failed to upload chunk {i + 1} of {num_chunks} for file {path}: {response.status_code} {response.reason}\"\n          )\n\n    # Return the digest of the file\n    return f\"sha256:{checksum}\"\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = hashlib.sha256()\n    with open(path, 'rb') as f:\n      while True:\n        chunk = f.read(4096)\n        if not chunk:\n          break\n        checksum.update(chunk)\n    checksum = checksum.hexdigest()\n\n    # Check if a blob with the same checksum already exists on the server\n    try:\n      async with self.session.head(f'{self.url}/blobs/{checksum}', timeout=self.timeout) as resp:\n        if resp.status == 200:\n          # Blob already exists, return the digest\n          return f'sha256:{checksum}'\n    except aiohttp.ClientError as e:\n      raise RegistryError(f'Failed to check if blob exists: {e}') from e\n\n    # Blob does not exist, upload the file\n    try:\n      async with self.session.post(f'{self.url}/blobs/uploads/', data={'name': checksum}, timeout=self.timeout) as resp:\n        if resp.status != 200:\n          raise RegistryError(f'Failed to create upload: {await resp.text()}')\n        upload_url = (await resp.json())['location']\n    except aiohttp.ClientError as e:\n      raise RegistryError(f'Failed to create upload: {e}') from e\n\n    try:\n      with open(path, 'rb') as f:\n        while True:\n          chunk = f.read(4096)\n          if not chunk:\n            break\n          async with self.session.put(upload_url, data=chunk, timeout=self.timeout) as resp:\n            if resp.status != 200:\n              raise RegistryError(f'Failed to upload blob: {await resp.text()}')\n    except aiohttp.ClientError as e:\n      raise RegistryError(f'Failed to upload blob: {e}"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Read the file in binary mode and calculate its SHA256 checksum\n    with open(path, 'rb') as f:\n      file_hash = hashlib.sha256()\n      while True:\n        chunk = f.read(4096)\n        if not chunk:\n          break\n        file_hash.update(chunk)\n      file_checksum = file_hash.hexdigest()\n\n    # Check if a blob with the same checksum already exists on the server\n    headers = {'Accept': 'application/vnd.docker.distribution.manifest.v2+json'}\n    async with self.session.head(f'{self.url}/blobs/{file_checksum}', headers=headers) as resp:\n      if resp.status == 404:\n        # If the blob does not exist, upload the file to the server\n        async with self.session.post(f'{self.url}/blobs/uploads/', headers=headers) as resp:\n          if resp.status == 202:\n            upload_url = (await resp.json())['location']\n            async with self.session.post(upload_url, data=f.read()) as resp:\n              if resp.status == 202:\n                return f'sha256:{file_checksum}'\n              else:\n                raise Exception(f'Failed to upload blob: {resp.status} {resp.reason}')\n          else:\n            raise Exception(f'Failed to start blob upload: {resp.status} {resp.reason}')\n      elif resp.status == 200:\n        # If the blob already exists, return its digest\n        return f'sha256:{file_checksum}'\n      else:\n        raise Exception(f'Failed to check blob existence: {resp.status} {resp.reason}')\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n      while True:\n        data = f.read(1024)\n        if not data:\n          break\n        sha256.update(data)\n    checksum = sha256.hexdigest()\n\n    # Check if the blob already exists on the server\n    headers = {'Accept': 'application/vnd.docker.distribution.manifest.v2+json'}\n    response = await self.head(f'/v2/{self.repository}/blobs/{checksum}', headers=headers)\n    if response.status_code == 404:\n      # Upload the file to the server\n      headers = {'Content-Type': 'application/octet-stream'}\n      with open(path, 'rb') as f:\n        while True:\n          data = f.read(1024)\n          if not data:\n            break\n          response = await self.post(f'/v2/{self.repository}/blobs/uploads/', data=data, headers=headers)\n      # Get the digest of the file\n      digest = response.headers['Docker-Content-Digest']\n    else:\n      # Get the digest of the file\n      digest = response.headers['Docker-Content-Digest']\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = self._get_file_checksum(path)\n\n    # Check if a blob with the same checksum already exists on the server\n    try:\n      await self._client.head(f\"/blobs/{checksum}\")\n      return checksum\n    except HTTPStatusError as e:\n      if e.status_code != 404:\n        raise e\n\n    # Upload the file to the server in chunks\n    async with aiofiles.open(path, \"rb\") as f:\n      while True:\n        chunk = await f.read(self._chunk_size)\n        if not chunk:\n          break\n        await self._client.post(\"/blobs/uploads/\", data=chunk)\n\n    # Return the digest of the file\n    return checksum\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    with open(path, 'rb') as file:\n      checksum = hashlib.sha256(file.read()).hexdigest()\n\n    async with self.session.head(f'{self.url}/blobs/{checksum}') as response:\n      if response.status == 404:\n        async with self.session.post(f'{self.url}/blobs/uploads/', data={'digest': f'sha256:{checksum}'}) as response:\n          response.raise_for_status()\n          upload_url = response.headers['Location']\n\n        async with self.session.put(upload_url, data=await file.read()) as response:\n          response.raise_for_status()\n\n    return f'sha256:{checksum}'\n\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = self._calculate_checksum(path)\n\n    # Check if a blob with the same checksum already exists on the server\n    response = await self._head_blob(checksum)\n    if response.status_code == 200:\n      # Blob already exists, return its digest\n      return response.headers['Docker-Content-Digest']\n\n    # Blob does not exist, upload the file to the server\n    response = await self._upload_blob(path, checksum)\n    return response.headers['Docker-Content-Digest']\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = await self._calculate_sha256(path)\n\n    # Check if a blob with the same checksum already exists on the server\n    response = await self.head(f\"/blobs/{checksum}\")\n    if response.status_code == 404:\n      # The blob does not exist on the server, so upload the file\n      with open(path, \"rb\") as file:\n        # Upload the file in chunks of 1 MB\n        while True:\n          # Read the next chunk of the file\n          chunk = file.read(1024 * 1024)\n          if not chunk:\n            # If the end of the file is reached, break the loop\n            break\n          # Upload the chunk to the server\n          response = await self.post(\"/blobs/uploads/\", data=chunk)\n          # Check if the response is successful\n          response.raise_for_status()\n\n    # Return the digest of the file\n    return f\"sha256:{checksum}\"\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = self._calculate_checksum(path)\n\n    # Check if a blob with the same checksum already exists on the server\n    response = await self._head(f\"{self.base_url}/blobs/{checksum}\")\n    if response.status_code == 200:\n      # If the blob already exists, return its digest\n      return response.headers[\"Docker-Content-Digest\"]\n\n    # If the blob does not exist, upload it to the server\n    response = await self._post(f\"{self.base_url}/blobs/uploads/\", data={\"from\": checksum})\n    if response.status_code != 200:\n      raise DockerRegistryError(f\"Failed to create blob for {path}: {response.text}\")\n\n    # Get the upload URL from the response\n    upload_url = response.headers[\"Location\"]\n\n    # Open the file in binary mode and upload it in chunks\n    with open(path, \"rb\") as f:\n      while True:\n        data = f.read(self.chunk_size)\n        if not data:\n          break\n        response = await self._patch(upload_url, data=data)\n        if response.status_code != 202:\n          raise DockerRegistryError(f\"Failed to upload blob for {path}: {response.text}\")\n\n    # Complete the upload\n    response = await self._post(f\"{self.base_url}/blobs/uploads/?digest={checksum}\")\n    if response.status_code != 202:\n      raise DockerRegistryError(f\"Failed to complete upload for {path}: {response.text}\")\n\n    # Return the digest of the file\n    return response.headers[\"Docker-Content-Digest\"]\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = self._calculate_sha256(path)\n\n    # Check if a blob with the same checksum already exists\n    try:\n      await self._head(f\"/blobs/{checksum}\")\n      return f\"sha256:{checksum}\"\n    except NotFoundError:\n      pass\n\n    # Upload the file to the server in chunks\n    with open(path, \"rb\") as file:\n      await self._post(f\"/blobs/{checksum}\", data=file, chunked=True)\n\n    return f\"sha256:{checksum}\"\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    path = Path(path)\n    checksum = self._calculate_sha256(path)\n    async with self.session.head(f\"{self.url}/blobs/{checksum}\") as response:\n      if response.status == 404:\n        async with self.session.post(f\"{self.url}/blobs/uploads/\") as response:\n          upload_url = (await response.json())[\"url\"]\n          await self._upload_blob(path, upload_url)\n      else:\n        logger.info(f\"Blob {checksum} already exists on the server.\")\n    return checksum\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    with open(path, 'rb') as f:\n      checksum = hashlib.sha256(f.read()).hexdigest()\n\n    async with self.session.head(f'{self.url}/blobs/{checksum}') as response:\n      if response.status == 404:\n        async with self.session.post(f'{self.url}/blobs/uploads/', data={'digest': f'sha256:{checksum}'}) as response:\n          if response.status != 201:\n            raise ValueError(f'Failed to create blob: {response.status} {response.reason}')\n          upload_url = response.headers['Location']\n\n        async with self.session.post(upload_url, data=f.read(), headers={'Content-Type': 'application/octet-stream'}) as response:\n          if response.status != 200:\n            raise ValueError(f'Failed to upload blob: {response.status} {response.reason}')\n\n    return f'sha256:{checksum}'\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = hashlib.sha256(path.read_bytes()).hexdigest()\n\n    # Check if a blob with the same checksum already exists on the server\n    try:\n      await self.head(f'/blobs/{checksum}')\n      return f'sha256:{checksum}'\n    except HTTPError as e:\n      if e.status_code != 404:\n        raise e\n\n    # Upload the file to the server\n    with path.open('rb') as f:\n      async with self.stream('POST', f'/blobs/uploads/') as r:\n        await r.send(aiohttp.helpers.FormData({'name': checksum}, f))\n\n    # Return the digest of the file\n    return f'sha256:{checksum}'\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    sha256 = await self._get_sha256(path)\n\n    # Check if the blob already exists on the server\n    try:\n      await self._head(f'blobs/{sha256}')\n      return sha256\n    except HTTPError as e:\n      if e.response.status_code != 404:\n        raise e\n\n    # Upload the file in chunks\n    async with aiofiles.open(path, 'rb') as f:\n      while True:\n        chunk = await f.read(self.chunk_size)\n        if not chunk:\n          break\n        await self._post(f'blobs/uploads/{sha256}', data=chunk)\n\n    return sha256\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = await self._calculate_sha256(path)\n\n    # Make a HEAD request to check if the blob already exists\n    response = await self.head(\n      f\"/v2/{self._repository}/blobs/{checksum}\",\n      headers={\"Accept\": \"application/vnd.docker.distribution.manifest.v2+json\"}\n    )\n\n    # If the blob already exists, return its digest\n    if response.status_code == 200:\n      return f\"sha256:{checksum}\"\n\n    # If the blob does not exist, upload it to the server\n    with open(path, \"rb\") as f:\n      # Calculate the size of the file in bytes\n      size = os.fstat(f.fileno()).st_size\n\n      # Calculate the number of chunks to upload the file in\n      chunks = math.ceil(size / self._chunk_size)\n\n      # Initialize the offset and the length of the current chunk\n      offset = 0\n      length = self._chunk_size\n\n      # Initialize the list of hashes of the chunks\n      hashes = []\n\n      # Upload the file in chunks\n      for i in range(chunks):\n        # Read the next chunk of the file\n        data = f.read(length)\n\n        # Calculate the SHA256 hash of the chunk\n        hash = hashlib.sha256(data).hexdigest()\n\n        # Add the hash of the chunk to the list of hashes\n        hashes.append(hash)\n\n        # Increment the offset and the length of the current chunk\n        offset += length\n        length = min(self._chunk_size, size - offset)\n\n      # Create a dictionary with the hashes of the chunks\n      data = {\n        \"hashes\": [\n          {\"algorithm\": \"sha256\", \"hex\": hash}\n          for hash in hashes\n        ]\n      }\n\n      # Create the URL for the blob upload\n     "}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Write the user code and test code to temporary files\n        user_code_file = cls.write_to_temp_file(user_code)\n        test_code_file = cls.write_to_temp_file(test_code)\n\n        # Perform the type check using Pyright\n        type_check_result = cls.pyright_type_check(user_code_file, test_code_file)\n\n        # Remove the temporary files\n        os.remove(user_code_file)\n        os.remove(test_code_file)\n\n        return type_check_result"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code into a single string\n        combined_code = f\"{user_code}\\n{test_code}\"\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(combined_code)\n            file_path = f.name\n\n        # Run Pyright to perform the type check\n        pyright_output = subprocess.run(\n            [\"pyright\", file_path], capture_output=True, text=True\n        )\n\n        # Remove the temporary file\n        os.remove(file_path)\n\n        # Parse the Pyright output to extract the expected type errors\n        expected_errors = []\n        for line in pyright_output.stdout.splitlines():\n            if \"error:\" in line:\n                expected_errors.append(line)\n\n        # Check if the type check passed\n        if expected_errors:\n            # If type check failed, return a result indicating failure along with the expected type errors\n            return TypeCheckResult(\n                passed=False,\n                message=f\"Type check failed. Expected type errors:\\n{expected_errors}\",\n            )\n        else:\n            # If type check passed, return a result indicating success\n            return TypeCheckResult(passed=True, message=\"Type check passed.\")"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(\n            mode=\"w\", suffix=\".py\", delete=False\n        ) as f:\n            f.write(user_code + \"\\n\" + test_code)\n            f.close()\n            temp_file_path = f.name\n\n        # Run Pyright on the temporary file\n        result = subprocess.run(\n            [\"pyright\", temp_file_path],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n\n        # Remove the temporary file\n        os.remove(temp_file_path)\n\n        # Parse the Pyright output to extract the expected type errors\n        expected_errors = []\n        for line in result.stdout.decode().split(\"\\n\"):\n            if \"error:\" in line:\n                expected_errors.append(line)\n\n        # Check if the type check passed or failed\n        if result.returncode == 0:\n            passed = True\n            message = \"Type check passed.\"\n        else:\n            passed = False\n            message = \"Type check failed.\"\n\n        return TypeCheckResult(passed, message, expected_errors)"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        if not cls._pyright_installed:\n            raise RuntimeError(\n                \"Pyright is not installed. Please install Pyright to use this feature.\"\n            )\n\n        if not cls._pyright_config_file_path:\n            raise RuntimeError(\n                \"Pyright config file path is not set. Please set the pyright_config_file_path attribute to the path of the Pyright config file.\"\n            )\n\n        if not cls._pyright_config_file_path.exists():\n            raise RuntimeError(\n                f\"Pyright config file path does not exist: {cls._pyright_config_file_path}\"\n            )\n\n        if not cls._pyright_config_file_path.is_file():\n            raise RuntimeError(\n                f\"Pyright config file path is not a file: {cls._pyright_config_file_path}\"\n            )\n\n        if not cls._pyright_config_file_path.is_absolute():\n            raise RuntimeError(\n                f\"Pyright config file path is not absolute: {cls._pyright_config_file_path}\"\n            )\n\n        if not cls._pyright_config_file_path.exists():\n            raise RuntimeError(\n                f\"Pyright config file path does not exist: {cls._pyright_config_file_path}\"\n            )\n\n        if not cls._pyright_config_file_path.is_file():\n            raise RuntimeError(\n                f\"Pyright config file path is not a file: {cls._pyright_config_file_path}\"\n            )\n\n        if not cls._pyright_config_file_path.exists():\n            raise RuntimeError(\n                f\"Pyright config file path does not exist: {cls._pyright_config_file_path}\"\n            )\n\n        if not cls._pyright_config_file_path.is_file():\n            raise RuntimeError(\n                f\"Pyright config file path is not a file: {cls._pyright_config_file_path}\"\n            )\n\n        if not cls._pyright_config_file_path."}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Get the path to the pyright executable\n        pyright_path = cls._get_pyright_path()\n\n        # Create a temporary directory to store the test files\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            # Write the user code and test code to temporary files\n            user_file_path = os.path.join(tmp_dir, \"user.py\")\n            with open(user_file_path, \"w\") as f:\n                f.write(user_code)\n\n            test_file_path = os.path.join(tmp_dir, \"test.py\")\n            with open(test_file_path, \"w\") as f:\n                f.write(test_code)\n\n            # Run the type check using Pyright\n            result = subprocess.run(\n                [\n                    pyright_path,\n                    \"--lib\",\n                    \"--noEmit\",\n                    \"--noImplicitAny\",\n                    \"--noImplicitOverride\",\n                    \"--noImplicitReturns\",\n                    \"--noUnusedLocals\",\n                    \"--strictNullChecks\",\n                    \"--strictPropertyInitialization\",\n                    \"--strictFunctionTypes\",\n                    \"--strictBindCallApply\",\n                    \"--noUncheckedIndexedAccess\",\n                    \"--noFallthroughCasesInSwitch\",\n                    \"--noImplicitAny\",\n                    \"--noImplicitOverride\",\n                    \"--noImplicitReturns\",\n                    \"--noUnusedLocals\",\n                    \"--strictNullChecks\",\n                    \"--strictPropertyInitialization\",\n                    \"--strictFunctionTypes\",\n                    \"--strictBindCallApply\",\n                    \"--noUncheckedIndexedAccess\",\n                    \"--noFallthroughCasesInSwitch\",\n                    \"--noImplicitAny\",\n                    \"--noImplicitOverride\",\n                    \"--noImplicitReturns\",\n                    \"--noUnusedLocals\",\n                    \"--strictNullChecks\",\n                    \"--strictPropertyInitialization\",\n                    \"--strictFunctionTypes\",\n                    \"--strictBindCallApply\",\n                    \"--noUncheckedIndexedAccess\",\n                    \"--noFall"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.flush()\n            combined_file = f.name\n\n        # Run Pyright on the combined file\n        try:\n            result = subprocess.run(\n                [\"pyright\", combined_file],\n                capture_output=True,\n                text=True,\n            )\n        except FileNotFoundError:\n            return TypeCheckResult(\n                passed=False,\n                message=\"Pyright not found. Please install Pyright to use this feature.\",\n            )\n\n        # Parse the Pyright output and extract the expected type errors\n        expected_errors = []\n        for line in result.stdout.split(\"\\n\"):\n            if \"error:\" in line:\n                expected_errors.append(line)\n\n        # Remove the temporary file\n        os.remove(combined_file)\n\n        # Return the result of the type check\n        return TypeCheckResult(\n            passed=not expected_errors,\n            message=\"\\n\".join(expected_errors) if expected_errors else \"Type check passed\",\n        )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        combined_code = user_code + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(combined_code)\n            file_path = f.name\n\n        # Run the type check using Pyright\n        result = subprocess.run(\n            [\"pyright\", file_path],\n            capture_output=True,\n            text=True,\n            check=False,\n        )\n\n        # Remove the temporary file\n        os.remove(file_path)\n\n        # Check if the type check passed\n        if result.returncode == 0:\n            return TypeCheckResult(\n                passed=True,\n                message=\"Type check passed.\",\n            )\n\n        # Parse the output of the type check to extract the expected type errors\n        expected_errors = []\n        for line in result.stdout.splitlines():\n            match = re.match(\n                r\"^.*?\\.py:(?P<line>\\d+): error: Expected type '(?P<expected_type>.*)', found '(?P<actual_type>.*)'$\",\n                line,\n            )\n            if match:\n                expected_errors.append(\n                    ExpectedTypeError(\n                        line=int(match.group(\"line\")),\n                        expected_type=match.group(\"expected_type\"),\n                        actual_type=match.group(\"actual_type\"),\n                    )\n                )\n\n        # Return the result of the type check\n        return TypeCheckResult(\n            passed=False,\n            message=\"Type check failed.\",\n            expected_errors=expected_errors,\n        )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(\n            mode=\"w\", suffix=\".py\", delete=False\n        ) as combined_code_file:\n            combined_code_file.write(user_code)\n            combined_code_file.write(test_code)\n            combined_code_file.flush()\n\n            # Run Pyright on the combined code file\n            pyright_process = subprocess.Popen(\n                [\"pyright\", combined_code_file.name],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n            )\n            pyright_process.wait()\n\n            # Read the Pyright output\n            pyright_output, pyright_error = (\n                pyright_process.stdout.read().decode(),\n                pyright_process.stderr.read().decode(),\n            )\n\n            # Parse the Pyright output to extract the expected type errors\n            expected_type_errors = set()\n            for line in pyright_output.splitlines():\n                if \"error:\" in line:\n                    expected_type_errors.add(line)\n\n            # Check if the expected type errors were found\n            if expected_type_errors:\n                return TypeCheckResult(\n                    passed=False,\n                    message=f\"Expected type errors were found: {expected_type_errors}\",\n                )\n            else:\n                return TypeCheckResult(\n                    passed=True, message=\"Type check passed with no errors.\"\n                )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to store the combined user code and test code\n        with tempfile.NamedTemporaryFile(\n            mode=\"w\", suffix=\".py\", delete=False\n        ) as f:\n            f.write(user_code + \"\\n\" + test_code)\n            f.flush()\n            file_path = f.name\n\n        # Run Pyright on the combined file\n        result = subprocess.run(\n            [\"pyright\", file_path], capture_output=True, text=True\n        )\n\n        # Remove the temporary file\n        os.unlink(file_path)\n\n        # Extract the expected type errors from the Pyright output\n        expected_errors = cls.extract_expected_errors(result.stdout)\n\n        # Extract the actual type errors from the Pyright output\n        actual_errors = cls.extract_actual_errors(result.stdout)\n\n        # Determine if the type check passed or failed\n        passed = len(actual_errors) == 0\n\n        # Construct the result message\n        if passed:\n            message = \"Type check passed\"\n        else:\n            message = \"Type check failed\"\n\n        # Create a TypeCheckResult object and return it\n        return TypeCheckResult(\n            passed=passed,\n            message=message,\n            expected_errors=expected_errors,\n            actual_errors=actual_errors,\n        )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Construct the combined code by combining the user code and test code\n        combined_code = user_code + test_code\n\n        # Run the type check using Pyright\n        result = cls.run_pyright(combined_code)\n\n        # Extract the error lines from the type check result\n        error_lines = cls.extract_error_lines(result)\n\n        # Check if the type check passed\n        passed = len(error_lines) == 0\n\n        # Construct the message based on the result\n        message = (\n            f\"Type check passed.\"\n            if passed\n            else f\"Type check failed. Expected errors on lines {error_lines}.\"\n        )\n\n        # Return the result as a TypeCheckResult object\n        return TypeCheckResult(message, passed)"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        user_code = user_code.strip()\n        test_code = test_code.strip()\n\n        if not user_code:\n            return TypeCheckResult(\n                passed=False,\n                message=\"User code is empty. Please provide code to type check.\",\n            )\n\n        if not test_code:\n            return TypeCheckResult(\n                passed=False,\n                message=\"Test code is empty. Please provide code to type check against.\",\n            )\n\n        try:\n            result = cls._run_pyright(user_code, test_code)\n        except Exception as e:\n            return TypeCheckResult(\n                passed=False,\n                message=f\"Error running Pyright: {str(e)}\",\n            )\n\n        if result.exit_code != 0:\n            return TypeCheckResult(\n                passed=False,\n                message=result.stdout.strip(),\n            )\n\n        # Extract lines with expected type errors\n        expected_error_lines = [\n            line\n            for line in result.stdout.strip().split(\"\\n\")\n            if \"Expected type\" in line\n        ]\n\n        if not expected_error_lines:\n            return TypeCheckResult(\n                passed=True,\n                message=\"Type check passed without any expected type errors.\",\n            )\n\n        return TypeCheckResult(\n            passed=False,\n            message=f\"Type check failed with expected type errors:\\n{result.stdout.strip()}\",\n        )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        combined_code = user_code + \"\\n\" + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(suffix=\".py\") as f:\n            f.write(combined_code.encode())\n            f.flush()\n\n            # Run Pyright on the combined code\n            result = subprocess.run(\n                [\n                    \"pyright\",\n                    \"--outputjson\",\n                    f.name,\n                ],\n                capture_output=True,\n                text=True,\n            )\n\n        # Parse the JSON output of Pyright\n        try:\n            json_output = json.loads(result.stdout)\n        except json.JSONDecodeError:\n            return TypeCheckResult(\n                passed=False,\n                message=f\"Pyright failed to parse the JSON output: {result.stdout}\",\n            )\n\n        # Extract the line numbers of expected type errors from the JSON output\n        expected_error_line_numbers = set()\n        for diagnostic in json_output[\"diagnostics\"]:\n            if diagnostic[\"severity\"] == \"error\":\n                expected_error_line_numbers.add(diagnostic[\"range\"][\"start\"][\"line\"])\n\n        # Extract the line numbers of actual type errors from the JSON output\n        actual_error_line_numbers = set()\n        for diagnostic in json_output[\"diagnostics\"]:\n            if diagnostic[\"severity\"] == \"error\":\n                actual_error_line_numbers.add(diagnostic[\"range\"][\"start\"][\"line\"])\n\n        # Determine if the type check passed or failed\n        passed = expected_error_line_numbers == actual_error_line_numbers\n\n        # Create a message indicating the outcome of the type check\n        if passed:\n            message = \"Type check passed\"\n        else:\n            message = f\"Type check failed on lines {sorted(actual_error_line_numbers - expected_error_line_numbers)}\"\n\n        # Return the result of the type check\n       "}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Get the path to the Pyright executable\n        pyright_path = cls.get_pyright_executable_path()\n        # Create a temporary directory for the type check\n        with tempfile.TemporaryDirectory() as temp_dir:\n            # Create a temporary file for the user code\n            with tempfile.NamedTemporaryFile(\n                mode=\"w\",\n                suffix=\".py\",\n                dir=temp_dir,\n                encoding=\"utf-8\",\n                delete=False,\n            ) as user_file:\n                user_file.write(user_code)\n            # Create a temporary file for the test code\n            with tempfile.NamedTemporaryFile(\n                mode=\"w\",\n                suffix=\".py\",\n                dir=temp_dir,\n                encoding=\"utf-8\",\n                delete=False,\n            ) as test_file:\n                test_file.write(test_code)\n            # Create a temporary file for the combined code\n            with tempfile.NamedTemporaryFile(\n                mode=\"w\",\n                suffix=\".py\",\n                dir=temp_dir,\n                encoding=\"utf-8\",\n                delete=False,\n            ) as combined_file:\n                combined_file.write(user_code + test_code)\n            # Run Pyright to perform the type check\n            result = subprocess.run(\n                [\n                    pyright_path,\n                    \"--project\",\n                    temp_dir,\n                    \"--outputjson\",\n                    combined_file.name,\n                ],\n                capture_output=True,\n                text=True,\n            )\n            # Parse the Pyright output to extract the type errors\n            errors = []\n            if result.returncode == 0:\n                # No type errors found\n                return TypeCheckResult(\n                    passed=True,\n                    message=\"No type errors found.\",\n                )\n            else:\n                # Type errors found\n                with open(combined_file.name, \"r\") as f:\n                    combined_json = json.load(f)\n                for error in combined_json[\"errors\"]:\n                    if error[\"file\"] == user_file"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary directory to store the files\n        with tempfile.TemporaryDirectory() as temp_dir:\n            # Write the user code to a file\n            user_code_file = os.path.join(temp_dir, \"user_code.py\")\n            with open(user_code_file, \"w\") as f:\n                f.write(user_code)\n\n            # Write the test code to a file\n            test_code_file = os.path.join(temp_dir, \"test_code.py\")\n            with open(test_code_file, \"w\") as f:\n                f.write(test_code)\n\n            # Create a configuration file for Pyright\n            config_file = os.path.join(temp_dir, \"pyrightconfig.json\")\n            with open(config_file, \"w\") as f:\n                json.dump({\"include\": [user_code_file]}, f)\n\n            # Run Pyright\n            result = subprocess.run(\n                [\n                    \"pyright\",\n                    \"--config\",\n                    config_file,\n                    \"--output\",\n                    \"json\",\n                    \"--output-dir\",\n                    temp_dir,\n                ],\n                capture_output=True,\n                text=True,\n            )\n\n            # Parse the Pyright output\n            output = json.loads(result.stdout)\n\n            # Check for type errors\n            errors = []\n            for diagnostic in output.get(\"diagnostics\", []):\n                if diagnostic[\"severity\"] == \"error\":\n                    errors.append(\n                        f\"Line {diagnostic['range']['start']['line']}: {diagnostic['message']}\"\n                    )\n\n            # Check if the type check passed\n            if len(errors) > 0:\n                return TypeCheckResult(\n                    passed=False,\n                    message=\"Type check failed:\\n\" + \"\\n\".join(errors),\n                )\n            else:\n                return TypeCheckResult(\n                    passed=True,\n                    message=\"Type check passed.\",\n                )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary directory to store the code\n        with tempfile.TemporaryDirectory() as temp_dir:\n            # Write the user code and test code to a file\n            with open(os.path.join(temp_dir, \"user_code.py\"), \"w\") as f:\n                f.write(user_code)\n            with open(os.path.join(temp_dir, \"test_code.py\"), \"w\") as f:\n                f.write(test_code)\n\n            # Run the type checker on the combined code\n            result = subprocess.run(\n                [\"pyright\", \"--lib\", \"--ignore\", \"--pythonversion\", \"3.8\"],\n                cwd=temp_dir,\n                capture_output=True,\n            )\n\n            # Check if the type checker returned any errors\n            if result.returncode != 0:\n                # Extract the error messages from the output\n                error_lines = result.stdout.decode().split(\"\\n\")\n                error_lines = [\n                    line for line in error_lines if \"error:\" in line\n                ]\n                error_lines = [\n                    line.split(\"error:\")[1].strip() for line in error_lines\n                ]\n                error_lines = [\n                    line\n                    for line in error_lines\n                    if \"Expected type\" in line or \"Unexpected type\" in line\n                ]\n\n                # Create a TypeCheckResult object with the error messages\n                return TypeCheckResult(\n                    passed=False,\n                    message=\"Type check failed:\\n\" + \"\\n\".join(error_lines),\n                )\n\n            # If no errors were found, create a TypeCheckResult object with a success message\n            return TypeCheckResult(\n                passed=True,\n                message=\"Type check passed\",\n            )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Construct the full code to be type-checked\n        full_code = f\"{user_code}\\n{test_code}\"\n\n        # Create a temporary directory to store the code\n        with TemporaryDirectory() as temp_dir:\n            # Write the code to a file\n            code_file = os.path.join(temp_dir, \"code.py\")\n            with open(code_file, \"w\") as f:\n                f.write(full_code)\n\n            # Run Pyright to perform the type check\n            result = subprocess.run(\n                [\"pyright\", \"--outputjson\", code_file],\n                capture_output=True,\n                text=True,\n            )\n\n        # Parse the Pyright output to extract the type errors\n        errors = []\n        if result.returncode == 0:\n            # No type errors found\n            return TypeCheckResult(\n                passed=True,\n                message=\"Type check passed\",\n            )\n        else:\n            # Type errors found\n            errors = json.loads(result.stdout)\n\n        # Identify lines with expected type errors\n        expected_errors = []\n        for error in errors:\n            if error[\"kind\"] == \"type-error\":\n                expected_errors.append(error)\n\n        # Construct the result message\n        if len(expected_errors) > 0:\n            # Type errors found, but not expected\n            result_message = (\n                f\"Type check failed. Expected type errors:\\n{expected_errors}\"\n            )\n            passed = False\n        else:\n            # Type errors found, but not expected\n            result_message = \"Type check passed\"\n            passed = True\n\n        # Return the result\n        return TypeCheckResult(\n            passed=passed,\n            message=result_message,\n        )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Set up the Pyright configuration file\n        pyright_config_file = cls.pyright_config_file\n        pyright_config_file.write_text(\n            dedent(\n                f\"\"\"\n                {{\n                    \"include\": [\"{cls.user_code_file.name}\"],\n                    \"exclude\": [\"**/node_modules/**/*\", \"**/__pycache__/**/*\"]\n                }}\n                \"\"\"\n            )\n        )\n\n        # Run Pyright\n        result = subprocess.run(\n            [\n                \"pyright\",\n                \"--config\",\n                str(pyright_config_file),\n                \"--outputjson\",\n                str(cls.pyright_output_file),\n            ],\n            capture_output=True,\n            text=True,\n            cwd=cls.test_dir,\n        )\n\n        # Check if Pyright ran successfully\n        if result.returncode != 0:\n            return TypeCheckResult(\n                passed=False,\n                message=f\"Pyright failed with the following error: {result.stderr}\",\n            )\n\n        # Read the Pyright output file\n        with open(cls.pyright_output_file, \"r\") as f:\n            pyright_output = json.load(f)\n\n        # Extract the expected errors from the Pyright output\n        expected_errors = set()\n        for error in pyright_output[\"errors\"]:\n            if error[\"rule\"] == \"no-untyped-call\":\n                expected_errors.add(error[\"start\"])\n\n        # Extract the actual errors from the user code\n        actual_errors = set()\n        with open(cls.user_code_file, \"r\") as f:\n            lines = f.readlines()\n            for i, line in enumerate(lines):\n                if \"TypeError\" in line:\n                    actual_errors.add(i + 1)\n\n        # Check if the actual errors match the expected errors\n        passed = actual_errors == expected_errors\n        if passed:\n            message = \"Type check passed\"\n        else:\n            message = \"Type check failed\""}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        import os\n        from pathlib import Path\n        import subprocess\n        from typing import List, Optional\n\n        class TypeCheckResult:\n            \"\"\"\n            A class representing the result of a type check.\n\n            Attributes:\n                passed (bool): True if the type check passed, False otherwise.\n                message (str): A message describing the result of the type check.\n            \"\"\"\n\n            def __init__(self, passed: bool, message: str):\n                self.passed = passed\n                self.message = message\n\n        def get_pyright_config_path() -> Optional[str]:\n            \"\"\"\n            Get the path to the Pyright configuration file.\n\n            Returns:\n                str: The path to the Pyright configuration file.\n            \"\"\"\n            import os\n\n            return os.path.join(os.path.dirname(__file__), \"pyrightconfig.json\")\n\n        def get_pyright_config_content() -> str:\n            \"\"\"\n            Get the content of the Pyright configuration file.\n\n            Returns:\n                str: The content of the Pyright configuration file.\n            \"\"\"\n            import os\n\n            with open(get_pyright_config_path(), \"r\") as f:\n                return f.read()\n\n        def get_pyright_config_file() -> str:\n            \"\"\"\n            Get the path to the Pyright configuration file.\n\n            Returns:\n                str: The path to the Pyright configuration file.\n            \"\"\"\n            import os\n\n            return os.path.join(os.path.dirname(__file__), \"pyrightconfig.json\")\n\n        def create_pyright_config_file() -> None:\n            \"\"\"\n            Create the Pyright configuration file.\n\n            Raises:\n                FileExistsError: If the Pyright configuration file already exists.\n            \"\"\"\n            import os\n\n            if os.path.exists(get_pyright_config_path()):\n                raise FileExistsError(\"Pyright configuration file already exists\")\n            with open(get_pyright_config_path(), \"w\") as f:\n                f.write(get_pyright_config_content())"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        try:\n            # Combine user code and test code\n            combined_code = user_code + test_code\n\n            # Create a temporary file to store the combined code\n            with tempfile.NamedTemporaryFile(\n                mode=\"w\", suffix=\".py\", delete=False\n            ) as f:\n                f.write(combined_code)\n                file_path = f.name\n\n            # Run Pyright to perform type checking\n            pyright_output = subprocess.run(\n                [\"pyright\", file_path],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                text=True,\n            )\n\n            # Check if Pyright ran successfully\n            if pyright_output.returncode != 0:\n                return TypeCheckResult(\n                    passed=False,\n                    message=\"Pyright type check failed. See logs for details.\",\n                )\n\n            # Get the output of Pyright\n            pyright_output = pyright_output.stdout\n\n            # Get the line numbers of expected type errors\n            expected_error_lines = cls._get_expected_error_lines(\n                user_code, test_code\n            )\n\n            # Get the line numbers of actual type errors\n            actual_error_lines = cls._get_actual_error_lines(pyright_output)\n\n            # Check if expected type errors match actual type errors\n            if expected_error_lines == actual_error_lines:\n                return TypeCheckResult(\n                    passed=True,\n                    message=\"Type check passed with expected type errors.\",\n                )\n            else:\n                return TypeCheckResult(\n                    passed=False,\n                    message=\"Type check failed. See logs for details.\",\n                )\n\n        except Exception as e:\n            return TypeCheckResult(\n                passed=False,\n                message=f\"An error occurred during type checking: {str(e)}\",\n            )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary directory to store the files\n        with TemporaryDirectory() as tmp_dir:\n            # Write the user code and test code to separate files in the temporary directory\n            user_file = os.path.join(tmp_dir, \"user.py\")\n            test_file = os.path.join(tmp_dir, \"test.py\")\n            with open(user_file, \"w\") as f:\n                f.write(user_code)\n            with open(test_file, \"w\") as f:\n                f.write(test_code)\n\n            # Construct the command to run Pyright with the user code and test code\n            command = [\n                \"pyright\",\n                \"--lib\",\n                cls.pyright_lib_path,\n                \"--pythonversion\",\n                cls.python_version,\n                \"--project\",\n                tmp_dir,\n                \"--outputjson\",\n                \"-\",\n                user_file,\n                test_file,\n            ]\n\n            # Run the command and capture the output\n            result = subprocess.run(\n                command,\n                capture_output=True,\n                text=True,\n                check=False,\n            )\n\n            # Parse the JSON output from Pyright\n            output = json.loads(result.stdout)\n\n            # Extract the lines with expected type errors from the output\n            expected_lines = [\n                error[\"start\"][\"line\"]\n                for error in output\n                if error[\"rule\"] == \"Expected type\"\n            ]\n\n            # Determine whether the type check passed or failed\n            if expected_lines:\n                # If there are expected type errors, the type check failed\n                failed_lines = expected_lines\n                message = \"Type check failed\"\n                passed = False\n            else:\n                # If there are no expected type errors, the type check passed\n                failed_lines = []\n                message = \"Type check passed\"\n                passed = True\n\n        # Return a TypeCheckResult object with the result of the type check\n        return TypeCheckResult(\n            passed=passed,\n            message=message,\n            failed_lines=failed_"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        data = f.read()\n    else:\n      data = modelfile\n\n    if stream:\n      return await self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=data,\n        stream=True,\n      )\n    else:\n      return await self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=data,\n      )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be specified\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        data = f.read()\n    else:\n      data = modelfile\n\n    if stream:\n      async for resp in self.stream(\n        \"POST\",\n        f\"/models/{model}\",\n        data=data,\n      ):\n        yield resp\n    else:\n      resp = await self.request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=data,\n      )\n      return resp\n\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError(\"Either path or modelfile must be specified\")\n\n    if stream:\n      async for response in self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=modelfile,\n        stream=True,\n      ):\n        yield response\n    else:\n      response = await self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=modelfile,\n      )\n      return response\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not path and not modelfile:\n      raise RequestError(\n        \"Either path or modelfile must be specified\"\n      )\n\n    if path:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if not modelfile:\n      raise RequestError(\"modelfile must be specified\")\n\n    response = await self._request(\n      \"POST\",\n      f\"/models/{model}\",\n      data=modelfile,\n      stream=stream,\n    )\n\n    if stream:\n      return response\n\n    return await response.json()"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\n        \"Either path or modelfile must be specified\"\n      )\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        data = f.read()\n    else:\n      data = modelfile\n\n    if stream:\n      return self._stream_request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=data,\n        response_type=\"json\",\n      )\n    else:\n      return await self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=data,\n        response_type=\"json\",\n      )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError(\"One of path or modelfile must be specified\")\n\n    params = {\"model\": model}\n\n    if stream:\n      async for resp in self._request(\n        \"POST\",\n        \"/model\",\n        params=params,\n        data=modelfile,\n        stream=True,\n      ):\n        yield resp\n    else:\n      resp = await self._request(\n        \"POST\",\n        \"/model\",\n        params=params,\n        data=modelfile,\n      )\n      return resp\n\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError(\"Either 'path' or 'modelfile' must be specified\")\n\n    data = {\n      \"model\": model,\n      \"modelfile\": modelfile,\n    }\n\n    if stream:\n      async for resp in self._request(\"POST\", \"/models\", data=data):\n        yield resp\n    else:\n      resp = await self._request(\"POST\", \"/models\", data=data)\n      return resp\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be specified\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        data = f.read()\n    else:\n      data = modelfile\n\n    if stream:\n      async with self.session.post(\n        f\"{self.url}/models/{model}\", data=data, stream=True\n      ) as resp:\n        async for chunk in resp.content:\n          yield chunk\n    else:\n      async with self.session.post(\n        f\"{self.url}/models/{model}\", data=data\n      ) as resp:\n        return await resp.json()\n\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      path = Path(path)\n      if path.is_file():\n        with open(path, \"rb\") as f:\n          modelfile = f.read()\n      else:\n        raise FileNotFoundError(f\"File not found: {path}\")\n    elif modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided\")\n\n    if stream:\n      async for chunk in self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        headers={\"Content-Type\": \"application/octet-stream\"},\n        data=modelfile,\n      ):\n        yield chunk\n    else:\n      return await self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        headers={\"Content-Type\": \"application/octet-stream\"},\n        data=modelfile,\n      )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        data = f.read()\n    else:\n      data = modelfile\n\n    if stream:\n      async for chunk in self._post(\n        f\"/{self._base_path}/{model}\", data=data, stream=True\n      ):\n        yield chunk\n    else:\n      return await self._post(f\"/{self._base_path}/{model}\", data=data)\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided.\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        data = f.read()\n    else:\n      data = modelfile\n\n    response = await self._post(\n      f\"/models/{model}\",\n      data=data,\n      stream=stream,\n    )\n\n    if stream:\n      return response\n    else:\n      return await response.json()\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n    elif modelfile is None:\n      raise RequestError(\"either path or modelfile must be specified\")\n\n    data = {\"modelfile\": modelfile}\n    headers = {\"content-type\": \"application/json\"}\n    response = await self._request(\n      \"POST\",\n      f\"/models/{model}\",\n      data=data,\n      headers=headers,\n      stream=stream,\n    )\n\n    if stream:\n      return response\n\n    return response.json()"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      path = Path(path)\n      if not path.is_file():\n        raise FileNotFoundError(f\"File not found: {path}\")\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided.\")\n\n    if stream:\n      return await self._request(\n        method=\"POST\",\n        url=f\"{self.base_url}/{model}\",\n        data=modelfile,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n        stream=True,\n      )\n\n    response = await self._request(\n      method=\"POST\",\n      url=f\"{self.base_url}/{model}\",\n      data=modelfile,\n      headers={\"Content-Type\": \"application/octet-stream\"},\n    )\n\n    return response.json()\n\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not path and not modelfile:\n      raise RequestError(\"One of path or modelfile must be provided\")\n\n    if path:\n      with open(path, \"rb\") as f:\n        data = f.read()\n    else:\n      data = modelfile\n\n    url = self.url + \"/models/\" + model\n    headers = {\"Content-Type\": \"application/octet-stream\"}\n\n    if stream:\n      async with self.session.post(url, data=data, headers=headers, stream=True) as response:\n        yield await self._handle_response(response)\n    else:\n      async with self.session.post(url, data=data, headers=headers) as response:\n        return await self._handle_response(response)\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"either path or modelfile must be specified\")\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modeldata = f.read()\n    else:\n      modeldata = modelfile\n    if stream:\n      async for resp in self.request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=modeldata,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n        stream=True,\n      ):\n        yield resp\n    else:\n      resp = await self.request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=modeldata,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n      )\n      return resp\n\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError('Either path or modelfile must be specified')\n\n    params = {'model': model}\n    headers = {}\n    files = {'model': modelfile}\n    response = await self._post(\n      '/model/create',\n      params=params,\n      headers=headers,\n      files=files,\n      stream=stream,\n    )\n\n    if stream:\n      return response\n    else:\n      return await response.json()"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if modelfile is None:\n      raise RequestError(\"modelfile must be provided\")\n\n    if stream:\n      async for response in self._client.stream(\n        \"POST\",\n        f\"/models/{model}\",\n        data=modelfile,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n      ):\n        yield response\n    else:\n      response = await self._client.request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=modelfile,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n      )\n      return response\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be specified\")\n\n    if path is not None:\n      path = Path(path)\n      if not path.exists():\n        raise FileNotFoundError(f\"Path {path} does not exist\")\n\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    url = f\"{self.base_url}/models/{model}\"\n    headers = {\"Content-Type\": \"application/octet-stream\"}\n    data = modelfile\n\n    async with self.session.post(url, headers=headers, data=data) as resp:\n      if resp.status != 200:\n        raise ResponseError(resp.status, resp.reason)\n\n      if stream:\n        async for chunk in resp.content.iter_chunked(1024):\n          yield json.loads(chunk)\n      else:\n        return await resp.json()\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not path and not modelfile:\n      raise RequestError(\n        \"Either 'path' or 'modelfile' must be provided for model creation\"\n      )\n\n    if path:\n      with open(path, \"rb\") as f:\n        model_data = f.read()\n    else:\n      model_data = modelfile.encode(\"utf-8\")\n\n    if stream:\n      return await self._post(\n        f\"/{self.version}/models/{model}\",\n        data=model_data,\n        stream=True,\n      )\n\n    response = await self._post(\n      f\"/{self.version}/models/{model}\",\n      data=model_data,\n    )\n\n    if not response.ok:\n      raise ResponseError(response)\n\n    return response.json()\n\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n    if modelfile is None:\n      raise RequestError(\"modelfile must be provided if path is not provided\")\n\n    url = f\"{self.url}/{model}\"\n    headers = {\"Content-Type\": \"application/octet-stream\"}\n    response = await self.session.post(url, data=modelfile, headers=headers)\n    if response.status != 200:\n      raise ResponseError(response.status, response.reason, response.text)\n    if stream:\n      async for line in response.aiter_lines():\n        yield json.loads(line)\n    else:\n      return await response.json()\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module_printer(fn)\n    else:\n        return aot_function_printer(fn)"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return torch.compile(fn, mode=\"aot_module\")\n    else:\n        return torch.compile(fn, mode=\"aot_function\")"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module_compiler(fn)\n    else:\n        return aot_function_compiler(fn)"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return compile_module(fn)\n    else:\n        return compile_function(fn)"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_compile(fn.forward, backend=\"inductor\", example_inputs=(torch.randn(1, 3, 224, 224),))\n    else:\n        return aot_compile(fn, backend=\"inductor\", example_inputs=(torch.randn(1, 3, 224, 224),))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_compile(fn.forward, example_inputs=torch.randn(1, 3, 224, 224))\n    else:\n        return aot_compile(fn, example_inputs=torch.randn(1, 3, 224, 224))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        # Compile the module using a forward and backward compiler specific for modules\n        compiled_module = compile_module(fn)\n        return compiled_module\n    else:\n        # Compile the function using a forward and backward compiler specific for functions\n        compiled_function = compile_function(fn)\n        return compiled_function"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    import torch\n\n    if isinstance(fn, torch.nn.Module):\n        # Compile the module using a forward and backward compiler specific for modules\n        return fn\n    else:\n        # Compile the function using a forward and backward compiler specific for functions\n        return fn"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_compile(fn, example_inputs=torch.randn(1, 3, 224, 224))\n    else:\n        return aot_compile(fn, example_inputs=torch.randn(1, 3, 224, 224))\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_compile(fn.forward, aot_module=True)\n    else:\n        return aot_compile(fn, aot_module=False)\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_compile(fn.forward, aot_backward_compiler)\n    else:\n        return aot_compile(fn, aot_backward_compiler)\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    import torch\n    import torch.nn as nn\n    from aot_compilers.torch_compilers import torch_forward_compiler, torch_backward_compiler\n\n    if isinstance(fn, nn.Module):\n        return torch_forward_compiler(fn), torch_backward_compiler(fn)\n    else:\n        return torch_forward_compiler(fn), torch_backward_compiler(fn)"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    from aot_torch_compile import compile_module, compile_function\n\n    if isinstance(fn, torch.nn.Module):\n        return compile_module(fn)\n    else:\n        return compile_function(fn)"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_compile(fn.forward, aot_backward_compiler)\n    else:\n        return aot_compile(fn, aot_backward_compiler)\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_compile(fn.forward, aot_module_compiler, example_inputs=(torch.randn(1, 3, 224, 224),))\n    else:\n        return aot_compile(fn, aot_function_compiler, example_inputs=(torch.randn(1, 3, 224, 224),))\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    import torch\n    import torch.nn as nn\n    from torch.utils.cpp_extension import load\n\n    if isinstance(fn, nn.Module):\n        return load(\n            name=\"aot_printer\",\n            sources=[\"aot_printer.cpp\"],\n            extra_include_paths=[\".\"],\n            extra_cflags=[\"-O3\"],\n            is_python_module=False,\n        )(fn)\n    else:\n        return load(\n            name=\"aot_printer\",\n            sources=[\"aot_printer.cpp\"],\n            extra_include_paths=[\".\"],\n            extra_cflags=[\"-O3\"],\n            is_python_module=False,\n        )(fn)\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    import torch\n    from torch import nn\n    from torch.nn.parameter import Parameter\n\n    if isinstance(fn, nn.Module):\n        return fn.forward_compiler(fn)\n    elif isinstance(fn, Parameter):\n        return fn.forward_compiler(fn)\n    else:\n        return fn.forward_compiler(fn)\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return compile_module(fn)\n    else:\n        return compile_function(fn)\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    from torch.autograd import Variable\n    import numpy as np\n    import aotautograd\n    import aotbackward\n    import aotforward\n    import aotbackward_function\n    import aotforward_function\n\n    if isinstance(fn, nn.Module):\n        return aotforward.compile(fn), aotbackward.compile(fn)\n    elif callable(fn):\n        return aotforward_function.compile(fn), aotbackward_function.compile(fn)\n    else:\n        raise TypeError(\"Input must be a PyTorch neural network module or function.\")"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    from torch import nn\n    from torch.fx import GraphModule\n\n    if isinstance(fn, nn.Module):\n        return GraphModule(fn, fn.forward)\n    elif callable(fn):\n        return GraphModule(fn, fn)\n    else:\n        raise ValueError(f\"{fn} is not a PyTorch neural network module or function\")\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n    best_config = summary_df.iloc[0][\"config\"]\n    best_config_dict = json.loads(best_config)\n\n    config_path = os.path.join(trial_path, \"config.yaml\")\n    with open(config_path, \"r\") as f:\n        config_dict = yaml.safe_load(f)\n\n    best_config_dict.update(config_dict)\n\n    if output_path is not None:\n        with open(output_path, \"w\") as f:\n            yaml.safe_dump(best_config_dict, f)\n\n    return best_config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary file to get the best trial index\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n    best_trial_index = summary_df[\"trial_index\"].idxmin()\n\n    # Read the configuration file to get the best pipeline configuration\n    config_path = os.path.join(trial_path, \"config.yaml\")\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n    best_pipeline_config = config[\"pipeline\"][best_trial_index]\n\n    # Save the best pipeline configuration to a YAML file if output_path is provided\n    if output_path is not None:\n        with open(output_path, \"w\") as f:\n            yaml.safe_dump(best_pipeline_config, f)\n\n    return best_pipeline_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary file to get the best trial number\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n    best_trial_number = summary_df[\"trial_number\"].iloc[0]\n\n    # Read the configuration file to get the best pipeline configuration\n    config_path = os.path.join(trial_path, \"config.yaml\")\n    with open(config_path, \"r\") as f:\n        config_dict = yaml.safe_load(f)\n\n    # Extract the best pipeline configuration from the trial directory\n    best_config = config_dict[\"pipeline\"][\"steps\"][best_trial_number]\n\n    # Save the best pipeline configuration to a YAML file if output_path is provided\n    if output_path is not None:\n        with open(output_path, \"w\") as f:\n            yaml.safe_dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary file\n    summary_file = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_file)\n\n    # Get the best trial number\n    best_trial_num = summary_df[\"trial_num\"].values[0]\n\n    # Get the best trial directory\n    best_trial_dir = os.path.join(trial_path, f\"trial_{best_trial_num}\")\n\n    # Read the config file\n    config_file = os.path.join(best_trial_dir, \"config.yaml\")\n    with open(config_file, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    # Extract the best pipeline configuration\n    best_config = config[\"pipeline\"]\n\n    # Save the best pipeline configuration to a YAML file if output_path is provided\n    if output_path is not None:\n        with open(output_path, \"w\") as f:\n            yaml.safe_dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary.csv file to get the best trial id\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n    best_trial_id = summary_df.loc[summary_df[\"rank_test_score\"] == 0, \"trial_id\"].values[0]\n\n    # Read the config.json file to get the best pipeline configuration\n    config_path = os.path.join(trial_path, \"config.json\")\n    with open(config_path, \"r\") as f:\n        config_dict = json.load(f)\n\n    # Extract the best pipeline configuration from the config dictionary\n    best_config_dict = config_dict[str(best_trial_id)]\n\n    # Save the best pipeline configuration to a YAML file if output_path is provided\n    if output_path:\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config_dict, f)\n\n    return best_config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary.csv file from the trial directory\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n\n    # Extract the best configuration from the summary dataframe\n    best_config = summary_df.iloc[0][\"configuration\"]\n\n    # Load the best configuration into a dictionary\n    best_config_dict = json.loads(best_config)\n\n    # Save the best configuration to a YAML file if an output path is provided\n    if output_path is not None:\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config_dict, f)\n\n    return best_config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary.csv file to get the best trial ID\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n    best_trial_id = summary_df.loc[summary_df[\"rank_test_score\"] == 1, \"trial_id\"].values[0]\n\n    # Read the config.json file to get the best pipeline configuration\n    config_path = os.path.join(trial_path, \"config.json\")\n    with open(config_path, \"r\") as f:\n        config_dict = json.load(f)\n\n    # Extract the best pipeline configuration\n    best_config = config_dict[str(best_trial_id)]\n\n    # Save the best pipeline configuration to a YAML file if an output path is provided\n    if output_path:\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config, f)\n\n    return best_config\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary file to find the best trial\n    summary_file = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_file)\n    best_trial_number = summary_df.loc[summary_df[\"rank_test_score\"] == 1, \"trial_id\"].values[0]\n\n    # Read the configuration file to get the best pipeline configuration\n    config_file = os.path.join(trial_path, \"config.yaml\")\n    with open(config_file, \"r\") as f:\n        config_dict = yaml.safe_load(f)\n    best_config = config_dict[f\"trial_{best_trial_number}\"]\n\n    # Save the best configuration to a YAML file if output_path is specified\n    if output_path is not None:\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            output_path += \".yaml\"\n        with open(output_path, \"w\") as f:\n            yaml.safe_dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_path = os.path.join(trial_path, \"summary.csv\")\n    config_path = os.path.join(trial_path, \"config.yaml\")\n\n    summary = pd.read_csv(summary_path)\n    config = yaml.safe_load(open(config_path, \"r\"))\n\n    best_config = config[\"pipeline\"][0][\"steps\"][0]\n    best_config[\"parameters\"] = config[\"pipeline\"][0][\"steps\"][0][\"parameters\"]\n    best_config[\"parameters\"][\"estimator\"] = config[\"pipeline\"][0][\"steps\"][0][\"parameters\"][\"estimator\"][0]\n\n    if output_path:\n        with open(output_path, \"w\") as file:\n            yaml.dump(best_config, file)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary file to get the best trial\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary = pd.read_csv(summary_path)\n    best_trial_id = summary[\"trial_id\"].iloc[0]\n\n    # Read the configuration file for the best trial\n    config_path = os.path.join(trial_path, f\"{best_trial_id}.json\")\n    with open(config_path, \"r\") as f:\n        config_dict = json.load(f)\n\n    # Extract the best pipeline configuration from the dictionary\n    best_config = config_dict[\"pipeline_config\"]\n\n    # Save the best configuration to a YAML file if output_path is provided\n    if output_path is not None:\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_path = os.path.join(trial_path, \"summary.csv\")\n    if not os.path.exists(summary_path):\n        raise ValueError(f\"Summary file not found at {summary_path}\")\n\n    summary_df = pd.read_csv(summary_path)\n    best_config_row = summary_df.loc[summary_df[\"validation_score\"].idxmax()]\n\n    config_path = os.path.join(trial_path, \"config.yaml\")\n    with open(config_path, \"r\") as f:\n        config_dict = yaml.safe_load(f)\n\n    best_config_dict = {}\n    for key, value in config_dict.items():\n        if key in best_config_row.index:\n            best_config_dict[key] = value\n\n    if output_path:\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            raise ValueError(\n                \"Output path must have a .yaml or .yml extension if provided\"\n            )\n        with open(output_path, \"w\") as f:\n            yaml.safe_dump(best_config_dict, f)\n\n    return best_config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary.csv file to get the best trial\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n    best_trial_idx = summary_df[\"eval_score\"].idxmax()\n    best_trial = summary_df.iloc[best_trial_idx]\n\n    # Read the configuration file to get the best pipeline configuration\n    config_path = os.path.join(trial_path, \"config.yaml\")\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    # Extract the best pipeline configuration from the configuration file\n    best_config = {}\n    for key, value in config.items():\n        if key == \"pipeline\":\n            best_config[key] = value[best_trial[\"trial_id\"]]\n        else:\n            best_config[key] = value\n\n    # Save the best pipeline configuration to a YAML file if the output path is provided\n    if output_path:\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary = pd.read_csv(summary_path)\n    best_config = summary.iloc[0][\"config\"]\n    best_config = json.loads(best_config)\n    best_config = {\n        \"pipeline\": [\n            {\n                \"name\": task,\n                \"params\": params,\n            }\n            for task, params in best_config.items()\n        ]\n    }\n    if output_path:\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            raise ValueError(\n                \"The output file path must have a .yaml or .yml extension.\"\n            )\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config, f)\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_path = os.path.join(trial_path, \"summary.csv\")\n    config_path = os.path.join(trial_path, \"config.yaml\")\n\n    summary = pd.read_csv(summary_path)\n    best_config_idx = summary[\"mean_test_score\"].idxmax()\n    best_config = summary.iloc[best_config_idx]\n\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    best_pipeline_config = {}\n    for key, value in config.items():\n        if isinstance(value, dict):\n            best_pipeline_config[key] = {}\n            for subkey, subvalue in value.items():\n                if subkey == \"params\":\n                    best_pipeline_config[key][subkey] = {}\n                    for param, param_value in subvalue.items():\n                        if param == \"__pipeline__\":\n                            best_pipeline_config[key][subkey][param] = best_config[param_value]\n                        else:\n                            best_pipeline_config[key][subkey][param] = param_value\n                else:\n                    best_pipeline_config[key][subkey] = subvalue\n        else:\n            best_pipeline_config[key] = value\n\n    if output_path:\n        with open(output_path, \"w\") as f:\n            yaml.safe_dump(best_pipeline_config, f)\n\n    return best_pipeline_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary file to extract the best trial\n    summary_file = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_file)\n    best_trial = summary_df.loc[summary_df[\"mean_test_score\"].idxmax()]\n    best_trial_name = best_trial[\"trial_id\"]\n\n    # Read the configuration file for the best trial\n    config_file = os.path.join(trial_path, \"config\", f\"{best_trial_name}.yaml\")\n    with open(config_file, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    # Extract the best pipeline configuration from the config dictionary\n    best_config = config[\"pipeline\"]\n\n    # Save the best configuration to a YAML file if an output path is provided\n    if output_path is not None:\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            output_path += \".yaml\"\n        with open(output_path, \"w\") as f:\n            yaml.safe_dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Load the summary file\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n\n    # Get the best model\n    best_model = summary_df.loc[summary_df[\"rank_test_score\"] == 1, \"model\"].iloc[0]\n\n    # Get the best parameters\n    best_params = summary_df.loc[summary_df[\"rank_test_score\"] == 1, \"params\"].iloc[0]\n\n    # Convert the best parameters string to a dictionary\n    best_params_dict = ast.literal_eval(best_params)\n\n    # Get the best preprocessing steps\n    best_preprocessing = summary_df.loc[summary_df[\"rank_test_score\"] == 1, \"preprocessing\"].iloc[0]\n\n    # Convert the best preprocessing string to a list\n    best_preprocessing_list = ast.literal_eval(best_preprocessing)\n\n    # Get the best feature selection steps\n    best_feature_selection = summary_df.loc[summary_df[\"rank_test_score\"] == 1, \"feature_selection\"].iloc[0]\n\n    # Convert the best feature selection string to a list\n    best_feature_selection_list = ast.literal_eval(best_feature_selection)\n\n    # Get the best hyperparameter tuning steps\n    best_hyperparameter_tuning = summary_df.loc[summary_df[\"rank_test_score\"] == 1, \"hyperparameter_tuning\"].iloc[0]\n\n    # Convert the best hyperparameter tuning string to a list\n    best_hyperparameter_tuning_list = ast.literal_eval(best_hyperparameter_tuning)\n\n    # Get the best pipeline configuration\n    best_pipeline_config = {\n        \"model\": best_model,\n        \"params\": best_params_dict,\n        \"preprocessing\": best_preprocessing_list,\n        \"feature_selection\": best_feature_selection_list,\n        \"hyperparameter_tuning\": best_hyperparameter_tuning_list"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary.csv file to get the best trial id\n    summary_df = pd.read_csv(os.path.join(trial_path, \"summary.csv\"))\n    best_trial_id = summary_df.loc[summary_df[\"rank_test_score\"] == 1, \"trial_id\"].values[0]\n\n    # Read the configuration file for the best trial\n    config_file_path = os.path.join(trial_path, \"config.yaml\")\n    with open(config_file_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    # Extract the best pipeline configuration from the config dictionary\n    best_config = config[\"pipeline\"][best_trial_id]\n\n    # Save the best pipeline configuration to a YAML file if output_path is specified\n    if output_path is not None:\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            raise ValueError(\"The output path must have a .yaml or .yml extension.\")\n\n        with open(output_path, \"w\") as f:\n            yaml.safe_dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary file\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n\n    # Get the best configuration\n    best_config_idx = summary_df[\"mean_val_loss\"].idxmin()\n    best_config = summary_df.iloc[best_config_idx]\n\n    # Extract the configuration from the trial directory\n    config_path = os.path.join(trial_path, \"config.yaml\")\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    # Extract the pipeline configuration from the config\n    pipeline_config = config[\"pipeline\"]\n\n    # Add the best configuration to the pipeline configuration\n    pipeline_config[\"model\"][\"config\"] = best_config.to_dict()\n\n    # Save the pipeline configuration to a YAML file\n    if output_path is not None:\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            raise ValueError(\n                \"The output path must have a .yaml or .yml file extension.\"\n            )\n        with open(output_path, \"w\") as f:\n            yaml.safe_dump(pipeline_config, f)\n\n    return pipeline_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n\n    # Extract the best pipeline configuration from the summary DataFrame\n    best_config = summary_df.iloc[0, :].to_dict()\n\n    # Read the pipeline configuration file to get the pipeline configuration names\n    config_path = os.path.join(trial_path, \"config.yaml\")\n    with open(config_path, \"r\") as file:\n        config_dict = yaml.safe_load(file)\n\n    # Rename the pipeline configuration names to the original names\n    for key, value in config_dict.items():\n        if key in best_config:\n            best_config[key] = value\n\n    # Save the best pipeline configuration to a YAML file\n    if output_path is not None:\n        with open(output_path, \"w\") as file:\n            yaml.safe_dump(best_config, file)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n    best_config_index = summary_df[\"rank_test_score\"].idxmin()\n    best_config_path = os.path.join(trial_path, \"config.yaml\")\n    best_config = read_yaml(best_config_path)\n\n    if output_path is not None:\n        if not output_path.endswith(\".yaml\") and not output_path.endswith(\".yml\"):\n            raise ValueError(\"Output path must have a .yaml or .yml extension.\")\n        with open(output_path, \"w\") as f:\n            yaml.dump(best_config, f)\n\n    return best_config\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    from torch.utils._pytree import tree_map\n    from torch.utils._pytree import tree_unflatten\n    from torch.utils._pytree import tree_flatten\n    from torch.utils._pytree import tree_map\n    from torch.utils._pytree import tree_unflatten\n    from torch.utils._pytree import tree_flatten\n    from torch.utils._pytree import tree_map\n    from torch.utils._pytree import tree_unflatten\n    from torch.utils._pytree import tree_flatten\n    from torch.utils._pytree import tree_map\n    from torch.utils._pytree import tree_unflatten\n    from torch.utils._pytree import tree_flatten\n    from torch.utils._pytree import tree_map\n    from torch.utils._pytree import tree_unflatten\n    from torch.utils._pytree import tree_flatten\n    from torch.utils._pytree import tree_map\n    from torch.utils._pytree import tree_unflatten\n    from torch.utils._pytree import tree_flatten\n    from torch.utils._pytree import tree_map\n    from torch.utils._pytree import tree_unflatten\n    from torch.utils._pytree import tree_flatten\n    from torch.utils._pytree import tree_map\n    from torch.utils._pytree import tree_unflatten\n    from torch.utils._pytree import tree_flatten\n    from torch.utils._pytree import tree_map\n    from torch.utils._pytree import tree_unflatten\n    from torch.utils._pytree import tree_flatten\n    from torch.utils._pytree import tree_map\n    from torch.utils._pytree import tree_unflatten\n    from torch.utils._pytree import tree_flatten\n    from torch.utils._pytree import tree_map\n    from torch.utils._pytree import tree_unflatten\n    from torch.utils._pytree import tree_flatten"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.utils.data\n    import torch.utils.data.dataloader\n    import torch.utils.data.dataset\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n    import torch.utils.data.samplers\n    import torch.utils.data.sampler\n   "}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    import functools\n    import threading\n\n    cache = {}\n    lock = threading.Lock()\n\n    @functools.wraps(func)\n    def wrapped(*args, **kwargs):\n        key = (args, tuple(kwargs.items()))\n        with lock:\n            if key not in cache:\n                traced_func = torch.jit.trace(func, *args, **kwargs)\n                if ts_compiler is not None:\n                    traced_func = ts_compiler(traced_func, **kwargs_)\n                cache[key] = traced_func\n            return cache[key](*args, **kwargs)\n\n    return wrapped"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.utils.data\n    import torch.optim as optim\n    import torch.utils.data\n    import torch.utils.data.distributed\n    import torch.utils.data.dataloader\n    import torch.utils.data.dataset\n    import torch.utils.data.sampler\n    import torch.utils.data.dataloader\n    import torch.utils.data.dataset\n    import torch.utils.data.sampler\n    import torch.utils.data.dataloader\n    import torch.utils.data.dataset\n    import torch.utils.data.sampler\n    import torch.utils.data.dataloader\n    import torch.utils.data.dataset\n    import torch.utils.data.sampler\n    import torch.utils.data.dataloader\n    import torch.utils.data.dataset\n    import torch.utils.data.sampler\n    import torch.utils.data.dataloader\n    import torch.utils.data.dataset\n    import torch.utils.data.sampler\n    import torch.utils.data.dataloader\n    import torch.utils.data.dataset\n    import torch.utils.data.sampler\n    import torch.utils.data.dataloader\n    import torch.utils.data.dataset\n    import torch.utils.data.sampler\n    import torch.utils.data.dataloader\n    import torch.utils.data.dataset\n    import torch.utils.data.sampler\n    import torch.utils.data.dataloader\n    import torch.utils.data.dataset\n    import torch.utils.data.sampler\n    import torch.utils.data.dataloader\n    import torch.utils.data.dataset\n    import torch.utils.data.sampler\n    import torch.utils.data.dataloader\n    import torch.utils.data.dataset\n   "}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader\n    from torch.utils.data import TensorDataset\n    from torch.utils.data import random_split\n    import torch.optim as optim\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch\n    import torch.nn.functional as F\n    import torch.nn"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.utils.data\n    import torch.utils.data.dataloader\n    import torch.utils.data.sampler\n    import torch.utils.data.dataset\n    import torch.utils.data.distributed\n    import torch.utils.data.graph\n    import torch.utils.data.graph_settings\n    import torch.utils.data.graph_pipeline\n    import torch.utils.data.graph_executor\n    import torch.utils.data.graph_executor_internals\n    import torch.utils.data.graph_pipeline_internals\n    import torch.utils.data.graph_settings_internals\n    import torch.utils.data.graph_internals\n    import torch.utils.data.graph_pipeline_internals\n    import torch.utils.data.graph_settings_internals\n    import torch.utils.data.graph_internals\n    import torch.utils.data.graph_pipeline_internals\n    import torch.utils.data.graph_settings_internals\n    import torch.utils.data.graph_internals\n    import torch.utils.data.graph_pipeline_internals\n    import torch.utils.data.graph_settings_internals\n    import torch.utils.data.graph_internals\n    import torch.utils.data.graph_pipeline_internals\n    import torch.utils.data.graph_settings_internals\n    import torch.utils.data.graph_internals\n    import torch.utils.data.graph_pipeline_internals\n    import torch.utils.data.graph_settings_internals\n    import torch.utils.data.graph_internals\n    import torch.utils.data.graph_pipeline_internals\n    import torch.utils.data.graph_settings_internals\n    import torch.utils.data.graph_internals\n    import torch.utils.data.graph_pipeline_internals"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    import torch.nn as nn\n    import torch.utils.data\n    from torch.utils.data import DataLoader\n    import torch.utils.benchmark as benchmark\n    import torch.utils.data as data\n    import torch.utils.data.dataloader as dataloader\n    import torch.utils.data.dataset as dataset\n    import torch.utils.data.dataloader as dataloader\n    import torch.utils.data.dataset as dataset\n    import torch.utils.data.dataloader as dataloader\n    import torch.utils.data.dataset as dataset\n    import torch.utils.data.dataloader as dataloader\n    import torch.utils.data.dataset as dataset\n    import torch.utils.data.dataloader as dataloader\n    import torch.utils.data.dataset as dataset\n    import torch.utils.data.dataloader as dataloader\n    import torch.utils.data.dataset as dataset\n    import torch.utils.data.dataloader as dataloader\n    import torch.utils.data.dataset as dataset\n    import torch.utils.data.dataloader as dataloader\n    import torch.utils.data.dataset as dataset\n    import torch.utils.data.dataloader as dataloader\n    import torch.utils.data.dataset as dataset\n    import torch.utils.data.dataloader as dataloader\n    import torch.utils.data.dataset as dataset\n    import torch.utils.data.dataloader as dataloader\n    import torch.utils.data.dataset as dataset\n    import torch.utils.data.dataloader as dataloader\n    import torch.utils.data.dataset as dataset\n    import torch.utils.data.dataloader as dataloader\n    import torch.utils.data.dataset as dataset\n    import torch.utils.data.dataloader as dataloader\n    import torch"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    import functools\n    import threading\n\n    cache = {}\n    lock = threading.Lock()\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (args, frozenset(kwargs.items()))\n        with lock:\n            if key not in cache:\n                if isinstance(func, torch.nn.Module):\n                    traced_func = torch.jit.trace(func.forward, args, **kwargs_)\n                else:\n                    traced_func = torch.jit.trace(func, args, **kwargs_)\n                if ts_compiler is not None:\n                    traced_func = ts_compiler(traced_func, **kwargs_)\n                cache[key] = traced_func\n            return cache[key](*args, **kwargs)\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache = {}\n    lock = threading.Lock()\n\n    def wrapper(*args, **kwargs):\n        with lock:\n            key = (args, tuple(sorted(kwargs.items())))\n            if key not in cache:\n                cache[key] = torch.jit.trace(func, *args, **kwargs)\n            return cache[key]\n\n    if ts_compiler is not None:\n        wrapper = ts_compiler(wrapper)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    from functools import wraps\n    from torch import nn\n    from torch.utils.data import DataLoader\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torch.nn as nn\n    import torch.nn."}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    import threading\n\n    lock = threading.Lock()\n    cache = {}\n\n    def trace_and_cache(func, *args, **kwargs):\n        key = (func, args, tuple(kwargs.items()))\n        with lock:\n            if key not in cache:\n                cache[key] = torch.jit.trace(func, *args, **kwargs)\n            return cache[key]\n\n    def wrapper(*args, **kwargs):\n        return trace_and_cache(func, *args, **kwargs)\n\n    if isinstance(func, torch.nn.Module):\n        wrapper.forward = lambda self, *args, **kwargs: trace_and_cache(func.forward, *args, **kwargs)\n    else:\n        wrapper.forward = lambda *args, **kwargs: trace_and_cache(func, *args, **kwargs)\n\n    if ts_compiler is not None:\n        wrapper.forward = lambda self, *args, **kwargs: ts_compiler(trace_and_cache(func, *args, **kwargs), **kwargs_)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import functools\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    import torch.jit\n    import torch.utils.data\n    import torch.utils.data.dataloader\n    import torch.utils.data.dataset\n    import torch.utils.data.sampler\n    import torch.utils.data.distributed\n    import torch.utils.data.graph\n    import torch.utils.data.graph_settings\n    import torch.utils.data.datapipes\n    import torch.utils.data.datapipes.iter\n    import torch.utils.data.datapipes.iter.util\n    import torch.utils.data.datapipes.iter.util.common\n    import torch.utils.data.datapipes.iter.util.consumers\n    import torch.utils.data.datapipes.iter.util.consumers.common\n    import torch.utils.data.datapipes.iter.util.consumers.collate\n    import torch.utils.data.datapipes.iter.util.consumers.collate.common\n    import torch.utils.data.datapipes.iter.util.consumers.collate.collate\n    import torch.utils.data.datapipes.iter.util.consumers.collate.collate_utils\n    import torch.utils.data.datapipes.iter.util.consumers.collate.collate_utils.common\n    import torch.utils.data.datapipes.iter.util.consumers.collate.collate_utils.collate\n    import torch.utils.data.datapipes.iter.util.consumers.collate.collate_utils.collate_utils\n    import torch.utils.data.datapipes.iter.util.consumers.collate.collate_utils.collate_utils.common\n    import torch.utils.data.datapipes.iter.util.consumers.collate.collate_utils.collate_utils.collate\n    import"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Create a lock to synchronize access to the cache\n    import threading\n\n    lock = threading.Lock()\n\n    # Create a cache to store the traced modules\n    cache = {}\n\n    # Define the wrapper function\n    def wrapper(*args, **kwargs):\n        # Acquire the lock to prevent concurrent access\n        with lock:\n            # Check if the function or module is already in the cache\n            key = (id(func), args, tuple(kwargs.items()))\n            if key not in cache:\n                # Trace the function or module and store the result in the cache\n                traced_func = torch.jit.trace(func, *args, **kwargs)\n                if ts_compiler is not None:\n                    traced_func = ts_compiler(traced_func, **kwargs_)\n                cache[key] = traced_func\n            return cache[key]\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    from torch.utils._pytree import tree_map\n\n    cache = {}\n    lock = threading.Lock()\n\n    def wrapper(*args, **kwargs):\n        nonlocal cache\n\n        # Compute the hash of the input arguments\n        hash_ = hash(tuple(tree_map(id, args)) + tuple(sorted(kwargs.items())))\n\n        # Acquire the lock to ensure thread safety\n        with lock:\n            # Check if the hash is in the cache\n            if hash_ in cache:\n                return cache[hash_]\n\n        # Trace the function or module's forward method\n        traced_func = torch.jit.trace(func, args, **kwargs_)\n\n        # Apply the compiler if provided\n        if ts_compiler is not None:\n            traced_func = ts_compiler(traced_func, **kwargs_)\n\n        # Acquire the lock to ensure thread safety\n        with lock:\n            # Add the traced function to the cache\n            cache[hash_] = traced_func\n\n        # Return the traced function\n        return traced_func\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.utils.data as data\n    from torch.utils.data import DataLoader\n    import torch.nn.init as init\n    import torch.optim as optim\n    from torch.optim import lr_scheduler\n    import torch.backends.cudnn as cudnn\n    import torch.nn.parallel\n    import torch.distributed as dist\n    import torch.multiprocessing as mp\n    import torch.nn.parallel._functions as Func\n    import torch.nn.parallel.distributed as D\n    import torch.nn.parallel.replicated as R\n    import torch.nn.parallel.parallel_apply as P\n    import torch.nn.parallel.scatter_gather as S\n    import torch.nn.parallel.replicate as replicate\n    import torch.nn.parallel.convert_to_mpi as convert_to_mpi\n    import torch.nn.parallel.distributed_c10d as c10d\n    import torch.nn.parallel.distributed_c10d_cpu as c10d_cpu\n    import torch.nn.parallel.distributed_c10d_gpu as c10d_gpu\n    import torch.nn.parallel.distributed_c10d_init as c10d_init\n    import torch.nn.parallel.distributed_c10d_utils as c10d_utils\n    import torch.nn.parallel.distributed_c10d_cpu_utils as c10d_cpu_utils\n    import torch.nn.parallel.distributed_c10d_gpu_utils as c10d_gpu_utils\n    import torch.nn.parallel.distributed_c10d_cpu_comm as c10d_cpu_comm\n    import torch.nn.parallel.distributed_c10d_gpu_comm as c10d_gpu_comm\n    import torch.nn.parallel"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    from torch.utils.benchmark import _get_lock\n    from torch.utils.benchmark._utils.tracing import _get_traced_module\n\n    lock = _get_lock()\n\n    def wrapper(*args, **kwargs):\n        # Get the traced module for the given function or module\n        traced_module = _get_traced_module(func, *args, **kwargs)\n\n        # Acquire the lock to ensure thread safety\n        with lock:\n            # Check if the traced module is already in the cache\n            if traced_module in _TRACED_MODULES:\n                # If it is, return the cached version\n                return _TRACED_MODULES[traced_module]\n            else:\n                # If it's not in the cache, trace it and cache it\n                traced_module = _TRACED_MODULES[traced_module] = _trace(\n                    traced_module, ts_compiler=ts_compiler, **kwargs_\n                )\n                return traced_module\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.utils.data\n    import torch.utils.checkpoint\n    from torch.utils.data import DataLoader\n    import numpy as np\n    import os\n    import time\n    import inspect\n    import functools\n    import copy\n    import threading\n    import logging\n    import torch.onnx\n    import torch.jit\n    import torch.nn.parallel\n    import torch.distributed as dist\n    import torch.multiprocessing as mp\n    import torch.multiprocessing.spawn as mp_spawn\n    import torch.multiprocessing.reductions as mp_reductions\n    import torch.multiprocessing.queues as mp_queues\n    import torch.multiprocessing.shared_memory as mp_shared_memory\n    import torch.multiprocessing.connection as mp_connection\n    import torch.multiprocessing.dummy as mp_dummy\n    import torch.multiprocessing.pool as mp_pool\n    import torch.multiprocessing.util as mp_util\n    import torch.multiprocessing.queues as mp_queues\n    import torch.multiprocessing.reduction as mp_reduction\n    import torch.multiprocessing.shared_memory as mp_shared_memory\n    import torch.multiprocessing.connection as mp_connection\n    import torch.multiprocessing.dummy as mp_dummy\n    import torch.multiprocessing.pool as mp_pool\n    import torch.multiprocessing.util as mp_util\n    import torch.multiprocessing.queues as mp_queues\n    import torch.multiprocessing.reduction as mp_reduction\n    import torch.multiprocessing.shared_memory as mp_shared_memory\n    import torch.multiprocessing.connection as mp_connection"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    import torch.nn as nn\n    from torch.utils import hooks\n    import torch.jit as jit\n    from torch.utils.data import DataLoader\n    from typing import Callable\n    import functools\n    import time\n    import os\n    import threading\n\n    # Create a lock for thread-safe access to the cache\n    lock = threading.Lock()\n\n    # Create a cache to store traced modules\n    cache = {}\n\n    # Define a decorator function to trace and cache the function\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Acquire the lock to ensure thread-safe access to the cache\n            with lock:\n                # Check if the function's arguments are already in the cache\n                key = (args, tuple(kwargs.items()))\n                if key in cache:\n                    # If the arguments are in the cache, return the cached result\n                    return cache[key]\n\n            # If the arguments are not in the cache, trace the function and cache the result\n            traced_func = jit.trace(func, *args, **kwargs)\n            with lock:\n                cache[key] = traced_func\n            return traced_func\n\n        return wrapper\n\n    # Return the decorator function\n    return decorator(func)\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    from functools import wraps\n    import os\n    import hashlib\n    import inspect\n    import threading\n    from torch.utils.cpp_extension import load\n\n    # Get the current thread ID\n    thread_id = threading.get_ident()\n\n    # Define the cache directory and lock\n    cache_dir = os.path.join(os.path.expanduser('~'), '.cache', 'lazy_trace')\n    os.makedirs(cache_dir, exist_ok=True)\n    lock = threading.Lock()\n\n    # Define the tracing function\n    def trace_func(func, *args, **kwargs):\n        # Get the function signature and arguments\n        sig = inspect.signature(func)\n        bound_args = sig.bind(*args, **kwargs)\n        bound_args.apply_defaults()\n\n        # Get the hash of the function signature\n        hash_str = hashlib.sha256(str(sig).encode()).hexdigest()\n\n        # Define the path to the cached module\n        cached_path = os.path.join(cache_dir, hash_str + '.pt')\n\n        # Load the cached module if it exists\n        if os.path.exists(cached_path):\n            module = torch.jit.load(cached_path)\n        else:\n            # Trace the function and cache the module\n            module = torch.jit.trace(func, *args, **kwargs)\n            torch.jit.save(module, cached_path)\n\n        # Compile the module if a compiler function is provided\n        if ts_compiler is not None:\n            module = ts_compiler(module, *args, **kwargs)\n\n        return module\n\n    # Define the wrapper function\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        # Acquire the lock\n        with lock:\n            # Get the hash of the function signature\n            sig = inspect.signature(func)\n            bound_args = sig.bind(*args, **kwargs)\n            bound_args.apply_defaults()\n            hash_str = hashlib"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import functools\n    import os\n    import torch.jit as jit\n\n    from .utils import get_module_name\n\n    # Define a lock to synchronize access to the cache\n    cache_lock = threading.Lock()\n\n    # Define the cache of traced modules\n    cache = {}\n\n    # Define a decorator function that wraps the original function or module's forward method\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Acquire the lock to ensure exclusive access to the cache\n            with cache_lock:\n                # Generate a unique key based on the function name and arguments\n                key = get_module_name(func) + str(args) + str(kwargs)\n\n                # Check if the key is already in the cache\n                if key in cache:\n                    # If the key is in the cache, return the cached trace\n                    return cache[key]\n\n                # If the key is not in the cache, trace the function or module's forward method\n                if isinstance(func, nn.Module):\n                    # If the function is a PyTorch module, trace its forward method\n                    trace_func = func.forward\n                else:\n                    # If the function is not a PyTorch module, use it directly\n                    trace_func = func\n\n                # Trace the function or module's forward method with the given arguments\n                trace = torch.jit.trace(trace_func, args, **kwargs)\n\n                # Apply any compiler enhancements if provided\n                if ts_compiler is not None:\n                    trace = ts_compiler(trace, **kwargs)\n\n                # Cache the traced module for future calls\n                cache[key] = trace\n\n                # Return the traced module\n                return trace\n\n        # Return the wrapped function\n        return wrapper\n\n    # Apply the decorator to the input function or module\n    return decorator(func)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Extract the best configuration from the trial folder\n        best_config = cls.extract_best_config(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_dir)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Extract the best configuration from the trial folder\n        config = cls.extract_best_config(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.abspath(os.path.join(trial_path, os.pardir))\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(config, project_dir)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = cls.extract_config_from_trial(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config_path = os.path.join(trial_path, 'config.json')\n        with open(config_path, 'r') as f:\n            config = json.load(f)\n        project_path = os.path.dirname(trial_path)\n        return cls(config, project_path)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Get the best configuration from the trial folder\n        best_config = Evaluator.get_best_config(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        runner = cls(best_config, project_dir)\n\n        return runner"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        trial_path = Path(trial_path)\n        best_config = extract_best_config(trial_path)\n        project_dir = trial_path.parent\n        return cls(best_config, project_dir)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Extract the best configuration from the trial folder\n        best_config = cls.get_best_config(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.abspath(os.path.join(trial_path, os.pardir))\n\n        # Initialize the Runner with the best configuration and project directory\n        runner = cls(best_config, project_dir)\n\n        return runner"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        trial_path = Path(trial_path)\n\n        with open(trial_path / \"best_config.json\", \"r\") as f:\n            best_config = json.load(f)\n\n        return cls(best_config=best_config, project_dir=trial_path.parent)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Get the best configuration from the trial folder\n        best_config = cls.get_best_config(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        trial_path = os.path.abspath(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        trial_name = os.path.basename(trial_path)\n\n        best_config_path = os.path.join(trial_path, \"best_config.json\")\n        with open(best_config_path, \"r\") as f:\n            best_config = json.load(f)\n\n        return cls(best_config, project_dir, trial_name)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        trial_path = os.path.abspath(trial_path)\n        project_path = os.path.dirname(trial_path)\n\n        # Extract the best configuration from the trial folder\n        config_path = os.path.join(trial_path, \"config.json\")\n        with open(config_path, \"r\") as f:\n            config = json.load(f)\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(config, project_path)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = cls.extract_config(trial_path)\n        project_path = os.path.abspath(os.path.join(trial_path, os.pardir))\n        return cls(config, project_path)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        trial_path = os.path.abspath(trial_path)\n\n        # Get the best configuration from the trial folder\n        best_config = get_best_config(trial_path)\n\n        # Get the project directory from the trial folder\n        project_dir = os.path.abspath(os.path.join(trial_path, os.pardir))\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_dir=project_dir)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Get the best configuration from the trial folder\n        best_config = cls.get_best_config(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = cls._extract_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir)\n\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Get the best configuration from the trial folder\n        best_config = cls.get_best_config(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_dir)\n\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        trial_folder = os.path.abspath(trial_path)\n        project_directory = os.path.abspath(os.path.join(trial_folder, os.pardir))\n        trial_config_path = os.path.join(trial_folder, 'trial_config.json')\n        with open(trial_config_path, 'r') as file:\n            trial_config = json.load(file)\n        return cls(trial_config['best_config'], project_directory)\n\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = cls.extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Get the best configuration from the trial folder\n        config = cls.get_best_config(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(config, project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        trial_path = Path(trial_path)\n        project_dir = trial_path.parent\n\n        config = cls._extract_config_from_trial_folder(trial_path)\n\n        return cls(project_dir=project_dir, config=config)\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Evaluate and select the best retrieval node result\n    best_result = evaluate_and_select_retrieval_node(modules=modules,\n                                                     module_params=module_params,\n                                                     previous_result=previous_result,\n                                                     node_line_dir=node_line_dir,\n                                                     strategies=strategies)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Evaluate and select the best retrieval node result\n    best_result, execution_times, evaluation_metrics = evaluate_and_select_best_retrieval_node(\n        modules=modules,\n        module_params=module_params,\n        previous_result=previous_result,\n        node_line_dir=node_line_dir,\n        strategies=strategies,\n    )\n\n    # Save the results and summaries to disk\n    save_results_and_summaries(\n        best_result=best_result,\n        execution_times=execution_times,\n        evaluation_metrics=evaluation_metrics,\n        node_line_dir=node_line_dir,\n    )\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Evaluate and select the best retrieval node result\n    result = evaluate_retrieval_node(modules, module_params, previous_result, strategies)\n\n    # Save the result and summary to disk\n    save_retrieval_node_result(result, node_line_dir)\n\n    # Combine the previous result columns with the selected retrieval node's result columns\n    return pd.concat([previous_result, result], axis=1)\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Evaluate and select the best module among retrieval node results\n    result = run_retrieval_node_modules(modules, module_params, previous_result, node_line_dir, strategies)\n\n    # Combine the previous result columns with the selected retrieval node's result columns\n    result = pd.concat([previous_result, result], axis=1)\n\n    return result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create a directory for this node line\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize a dataframe to store the results\n    result_df = pd.DataFrame()\n\n    # Initialize a dictionary to store the execution times\n    execution_times = {}\n\n    # Initialize a dictionary to store the evaluation metrics\n    evaluation_metrics = {}\n\n    # Loop through each retrieval module\n    for module, params in zip(modules, module_params):\n\n        # Run the module and get the result\n        result = module(**params)\n\n        # Get the execution time\n        execution_time = result['execution_time']\n\n        # Get the evaluation metrics\n        evaluation_metrics[module.__name__] = result['evaluation_metrics']\n\n        # Add the execution time to the dictionary\n        execution_times[module.__name__] = execution_time\n\n        # Add the result to the dataframe\n        result_df = pd.concat([result_df, result['result']], axis=0)\n\n    # Apply the strategies to select the best result\n    best_result = select_best_result(result_df, strategies)\n\n    # Add the previous result columns to the best result\n    best_result = pd.concat([best_result, previous_result], axis=1)\n\n    # Save the best result to disk\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n\n    # Save the execution times to disk\n    with open(os.path.join(node_line_dir, 'execution_times.json'), 'w') as f:\n        json.dump(execution_times, f)\n\n    # Save the evaluation metrics to disk\n    with open(os.path.join(node_line_dir, 'evaluation_metrics.json'), 'w') as f:\n        json.dump(evaluation_metrics, f)\n\n    # Return the best result\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create a directory for this node line if it does not exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Initialize a list to store the results from each module\n    results = []\n\n    # Iterate through each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n\n        # Run the module with the given parameters\n        result = module(**params)\n\n        # Save the result to a file\n        result.to_csv(os.path.join(node_line_dir, f\"{module.__name__}.csv\"), index=False)\n\n        # Append the result to the list of results\n        results.append(result)\n\n    # Calculate the execution times for each module\n    execution_times = [time.time() - start_time for start_time in module_start_times]\n\n    # Save the execution times to a file\n    with open(os.path.join(node_line_dir, \"execution_times.txt\"), \"w\") as f:\n        f.write(str(execution_times))\n\n    # Evaluate the results using the specified strategies\n    evaluator = Evaluator(strategies)\n    scores = evaluator.evaluate(results)\n\n    # Save the scores to a file\n    with open(os.path.join(node_line_dir, \"scores.txt\"), \"w\") as f:\n        f.write(str(scores))\n\n    # Select the best result using the specified strategies\n    best_result = evaluator.select_best(results, scores)\n\n    # Save the best result to a file\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    # Combine the best result with the previous result\n    combined_result = pd.concat([previous_result, best_result], axis=1)\n\n    # Return the combined result\n    return combined_result"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the output directory if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a list to store the results from each module\n    results = []\n\n    # Create a list to store the execution times of each module\n    execution_times = []\n\n    # Create a list to store the evaluation metrics of each module\n    evaluation_metrics = []\n\n    # Iterate over the modules and their parameters\n    for module, params in zip(modules, module_params):\n\n        # Run the module with the given parameters\n        start_time = time.time()\n        result = module(**params)\n        execution_time = time.time() - start_time\n\n        # Save the result to the output directory\n        result.to_csv(os.path.join(node_line_dir, f\"{module.__name__}.csv\"), index=False)\n\n        # Add the result to the list of results\n        results.append(result)\n\n        # Add the execution time to the list of execution times\n        execution_times.append(execution_time)\n\n        # Evaluate the result using the evaluation metrics in the strategies\n        metrics = {}\n        for metric in strategies[\"metrics\"]:\n            metrics[metric] = metric(result, previous_result)\n\n        # Add the evaluation metrics to the list of evaluation metrics\n        evaluation_metrics.append(metrics)\n\n    # Save the evaluation metrics to the output directory\n    evaluation_metrics_df = pd.DataFrame(evaluation_metrics)\n    evaluation_metrics_df.to_csv(os.path.join(node_line_dir, \"evaluation_metrics.csv\"), index=False)\n\n    # Save the execution times to the output directory\n    execution_times_df = pd.DataFrame(execution_times, columns=[\"execution_time\"])\n    execution_times_df.to_csv(os.path.join(node_line_dir, \"execution_times.csv\"), index=False)\n\n    # Select the best result based on the specified strategies\n    best_result = None\n    best"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize the result dataframe with the previous result columns\n    result = previous_result.copy()\n\n    # Initialize the metrics dataframe\n    metrics = pd.DataFrame()\n\n    # Initialize the execution times dataframe\n    execution_times = pd.DataFrame()\n\n    # Iterate over each retrieval module and its parameters\n    for module, params in zip(modules, module_params):\n\n        # Run the retrieval module with the given parameters\n        module_result = module(**params)\n\n        # Get the execution time of the retrieval module\n        execution_time = module_result['execution_time']\n\n        # Get the evaluation metrics of the retrieval module\n        module_metrics = module_result['metrics']\n\n        # Add the execution time and evaluation metrics to their respective dataframes\n        execution_times = execution_times.append(execution_time, ignore_index=True)\n        metrics = metrics.append(module_metrics, ignore_index=True)\n\n        # Add the retrieval module's result columns to the result dataframe\n        result = pd.concat([result, module_result['result']], axis=1)\n\n    # Save the results and summaries to disk\n    result.to_parquet(os.path.join(node_line_dir, 'result.parquet'))\n    metrics.to_parquet(os.path.join(node_line_dir, 'metrics.parquet'))\n    execution_times.to_parquet(os.path.join(node_line_dir, 'execution_times.parquet'))\n\n    # Evaluate and select the best retrieval node result\n    best_result = evaluate_and_select_best_result(result, metrics, strategies)\n\n    # Return the best result dataframe\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create a directory for this node line if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a list to store the results of each module\n    module_results = []\n\n    # Create a list to store the execution times of each module\n    module_execution_times = []\n\n    # Create a list to store the evaluation metrics of each module\n    module_metrics = []\n\n    # Iterate through each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Create a unique name for this module\n        module_name = f\"{module.__name__}_{str(uuid.uuid4())[:8]}\"\n\n        # Create a directory for this module\n        module_dir = os.path.join(node_line_dir, module_name)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Start the timer\n        start_time = time.time()\n\n        # Run the module with the given parameters\n        result = module(**params)\n\n        # Stop the timer\n        end_time = time.time()\n\n        # Calculate the execution time\n        execution_time = end_time - start_time\n\n        # Add the result, execution time, and evaluation metrics to the lists\n        module_results.append(result)\n        module_execution_times.append(execution_time)\n        module_metrics.append(evaluate_retrieval_node_result(result, previous_result, strategies))\n\n        # Save the result to disk\n        result.to_csv(os.path.join(module_dir, \"result.csv\"), index=False)\n\n    # Create a dataframe from the list of results\n    results = pd.concat(module_results)\n\n    # Create a dataframe from the list of execution times\n    execution_times = pd.DataFrame(module_execution_times, columns=[\"execution_time\"])\n\n    # Create a dataframe from the list of evaluation metrics\n    metrics = pd.DataFrame(module_metrics)\n\n    # Save the results"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the node line directory if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Initialize the results and execution times lists\n    results = []\n    execution_times = []\n\n    # Iterate through each module and its parameters\n    for module, params in zip(modules, module_params):\n\n        # Run the module with the given parameters\n        start_time = time.time()\n        result = module(**params)\n        end_time = time.time()\n\n        # Calculate the execution time\n        execution_time = end_time - start_time\n\n        # Add the result and execution time to the lists\n        results.append(result)\n        execution_times.append(execution_time)\n\n        # Save the result to disk\n        result_file_path = os.path.join(node_line_dir, f\"{module.__name__}.csv\")\n        result.to_csv(result_file_path, index=False)\n\n    # Calculate the evaluation metrics for each result\n    metrics = {}\n    for result in results:\n        metric_name = result.get('metric_name', '')\n        metric_value = result.get('metric_value', 0)\n        metrics[metric_name] = metric_value\n\n    # Apply the strategies to select the best result\n    best_result = None\n    for strategy_name, strategy_params in strategies.items():\n        strategy = strategies[strategy_name]\n        best_result = strategy(results, metrics, execution_times, strategy_params)\n\n    # Combine the previous result columns with the selected retrieval node's result columns\n    combined_result = pd.concat([previous_result, best_result], axis=1)\n\n    # Save the combined result to disk\n    combined_result_file_path = os.path.join(node_line_dir, \"combined_result.csv\")\n    combined_result.to_csv(combined_result_file_path, index=False)\n\n    # Save the execution times and metrics to disk"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create a directory for this node line\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize a list to store the results of each module\n    results = []\n\n    # Initialize a dictionary to store the execution times of each module\n    execution_times = {}\n\n    # Iterate over the modules and their parameters\n    for module, params in zip(modules, module_params):\n\n        # Initialize a dictionary to store the execution time for this module\n        module_execution_time = {}\n\n        # Get the name of the module\n        module_name = module.__name__\n\n        # Get the start time\n        start_time = time.time()\n\n        # Run the module with the given parameters\n        result = module(previous_result, **params)\n\n        # Get the end time\n        end_time = time.time()\n\n        # Calculate the execution time\n        execution_time = end_time - start_time\n\n        # Add the execution time to the dictionary\n        module_execution_time[module_name] = execution_time\n\n        # Add the result to the list\n        results.append(result)\n\n    # Save the results to a CSV file\n    result_path = os.path.join(node_line_dir, 'results.csv')\n    pd.concat(results, axis=1).to_csv(result_path, index=False)\n\n    # Save the execution times to a JSON file\n    execution_time_path = os.path.join(node_line_dir, 'execution_times.json')\n    with open(execution_time_path, 'w') as f:\n        json.dump(module_execution_time, f)\n\n    # Evaluate the results and select the best one\n    best_result = evaluate_and_select_best_result(results, strategies)\n\n    # Save the best result to a CSV file\n    best_result_path = os.path.join(node_line_dir, 'best_result.csv')\n    best_result.to_csv(best_result_path, index=False)\n\n   "}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the directory for this node line if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Create a list to store the results of each retrieval module\n    results = []\n\n    # Create a list to store the execution times of each retrieval module\n    execution_times = []\n\n    # Create a list to store the evaluation metrics of each retrieval module\n    evaluation_metrics = []\n\n    # Iterate over the retrieval modules and their parameters\n    for module, params in zip(modules, module_params):\n\n        # Get the module name\n        module_name = module.__name__\n\n        # Get the module parameters\n        module_params = params\n\n        # Create a directory for this module if it doesn't exist\n        module_dir = os.path.join(node_line_dir, module_name)\n        if not os.path.exists(module_dir):\n            os.makedirs(module_dir)\n\n        # Create a start time\n        start_time = time.time()\n\n        # Run the retrieval module with the given parameters\n        result = module(previous_result, **module_params)\n\n        # Create an end time\n        end_time = time.time()\n\n        # Calculate the execution time\n        execution_time = end_time - start_time\n\n        # Save the result to disk\n        result_file = os.path.join(module_dir, 'result.csv')\n        result.to_csv(result_file, index=False)\n\n        # Add the result to the results list\n        results.append(result)\n\n        # Add the execution time to the execution times list\n        execution_times.append(execution_time)\n\n        # Calculate the evaluation metrics for this module\n        evaluation_metrics.append(calculate_evaluation_metrics(result, previous_result))\n\n        # Save the evaluation metrics to disk\n        evaluation_metrics_file = os.path.join(module_dir, 'evaluation_metrics.csv')\n        pd.DataFrame"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create a directory for this node line if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Run each retrieval module with given parameters\n    results = []\n    for module, params in zip(modules, module_params):\n        result = module(**params)\n        results.append(result)\n\n    # Save the results to disk\n    for i, result in enumerate(results):\n        result.to_csv(os.path.join(node_line_dir, f\"result_{i}.csv\"), index=False)\n\n    # Measure the execution times of each retrieval module\n    execution_times = [result.execution_time for result in results]\n\n    # Evaluate the results using the specified metrics\n    metrics = strategies[\"metrics\"]\n    metric_values = []\n    for metric in metrics:\n        metric_value = metric(results)\n        metric_values.append(metric_value)\n\n    # Select the best result based on the specified strategies\n    best_result = None\n    if strategies[\"strategy\"] == \"max\":\n        # Select the result with the maximum metric value\n        best_result = results[metric_values.index(max(metric_values))]\n    elif strategies[\"strategy\"] == \"min\":\n        # Select the result with the minimum metric value\n        best_result = results[metric_values.index(min(metric_values))]\n    elif strategies[\"strategy\"] == \"threshold\":\n        # Select the result with the execution time within the specified speed threshold\n        speed_threshold = strategies[\"speed_threshold\"]\n        best_result = results[execution_times.index(speed_threshold)]\n\n    # Combine the previous result columns with the selected retrieval node's result columns\n    best_result = pd.concat([previous_result, best_result], axis=1)\n\n    # Save the best result to disk\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    # Create a summary"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Check if the input arguments are valid\n    if not isinstance(modules, list) or not isinstance(module_params, list) or not isinstance(previous_result, pd.DataFrame) or not isinstance(node_line_dir, str) or not isinstance(strategies, dict):\n        raise ValueError(\"Invalid input arguments. Please provide a list of modules, a list of module parameters, a DataFrame for previous result, a node line directory, and a dictionary of strategies.\")\n\n    # Check if the length of the input arguments match\n    if len(modules) != len(module_params):\n        raise ValueError(\"The length of the modules and module_params lists must match.\")\n\n    # Check if the previous_result DataFrame has a 'document_id' column\n    if 'document_id' not in previous_result.columns:\n        raise ValueError(\"The previous_result DataFrame must have a 'document_id' column.\")\n\n    # Check if the node_line_dir is valid\n    if not os.path.exists(node_line_dir):\n        raise ValueError(\"The node_line_dir does not exist.\")\n\n    # Check if the strategies are valid\n    if not isinstance(strategies, dict):\n        raise ValueError(\"The strategies must be a dictionary.\")\n\n    # Check if the strategies have the required keys\n    required_keys = ['metric', 'threshold', 'strategy']\n    for key in required_keys:\n        if key not in strategies:\n            raise ValueError(f\"The strategies dictionary must have a '{key}' key.\")\n\n    # Check if the strategies have the correct values\n    if strategies['strategy'] not in ['min', 'max', 'mean']:\n        raise ValueError(\"The strategy value must be 'min', 'max', or 'mean'.\")\n\n    # Check if the strategies have the correct types\n    if not isinstance(strategies['metric'], str):\n        raise ValueError(\"The metric value must be a string.\")\n\n    if not isinstance(strategies['threshold'], float):\n        raise ValueError(\"The threshold value must be a float.\")\n\n    # Check if the strategies"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the node line directory if it does not exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a list of dataframes to store the results\n    result_dfs = []\n\n    # Create a list of dataframes to store the summaries\n    summary_dfs = []\n\n    # Create a list of dataframes to store the evaluation metrics\n    eval_dfs = []\n\n    # Iterate through the modules and their parameters\n    for module, params in zip(modules, module_params):\n        # Create a dictionary to store the execution times\n        exec_times = {}\n\n        # Create a dictionary to store the evaluation metrics\n        eval_metrics = {}\n\n        # Create a dictionary to store the evaluation metrics\n        eval_metrics = {}\n\n        # Run the retrieval module with the given parameters\n        result_df = module(previous_result, **params)\n\n        # Add the result dataframe to the list of dataframes\n        result_dfs.append(result_df)\n\n        # Add the result dataframe to the list of dataframes\n        result_dfs.append(result_df)\n\n        # Add the result dataframe to the list of dataframes\n        result_dfs.append(result_df)\n\n        # Save the result dataframe to disk\n        result_df.to_csv(os.path.join(node_line_dir, f\"{module.__name__}.csv\"), index=False)\n\n        # Save the result dataframe to disk\n        result_df.to_csv(os.path.join(node_line_dir, f\"{module.__name__}.csv\"), index=False)\n\n        # Save the result dataframe to disk\n        result_df.to_csv(os.path.join(node_line_dir, f\"{module.__name__}.csv\"), index=False)\n\n        # Save the result dataframe to disk\n        result_df.to_csv(os.path.join(node_line_dir, f\"{module.__name__}.csv\"), index=False)\n\n        # Save the result dataframe to"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Check if the input arguments are valid\n    if not isinstance(modules, list) or not isinstance(module_params, list):\n        raise ValueError(\"'modules' and 'module_params' must be lists.\")\n    if len(modules) != len(module_params):\n        raise ValueError(\"'modules' and 'module_params' must have the same length.\")\n    if not isinstance(strategies, dict):\n        raise ValueError(\"'strategies' must be a dictionary.\")\n    if not isinstance(previous_result, pd.DataFrame):\n        raise ValueError(\"'previous_result' must be a pandas DataFrame.\")\n\n    # Initialize the best result dataframe\n    best_result = None\n\n    # Create a directory for this node line\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Loop through each retrieval module and its parameters\n    for module, params in zip(modules, module_params):\n\n        # Get the module name\n        module_name = module.__name__\n\n        # Create a directory for this module\n        module_dir = os.path.join(node_line_dir, module_name)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Run the module with the given parameters\n        module_result = module(**params)\n\n        # Save the module result to disk\n        module_result_path = os.path.join(module_dir, f\"{module_name}.csv\")\n        module_result.to_csv(module_result_path, index=False)\n\n        # Evaluate the module result\n        evaluation_metrics = evaluate_module_result(module_result, previous_result, strategies[\"metrics\"])\n\n        # Save the evaluation metrics to disk\n        evaluation_metrics_path = os.path.join(module_dir, f\"{module_name}_evaluation_metrics.json\")\n        with open(evaluation_metrics_path, \"w\") as f:\n            json.dump(evaluation_metrics, f)\n\n        # Apply the evaluation strategies"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create a directory for this node line if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Create a list of results and execution times for each retrieval module\n    results = []\n    execution_times = []\n\n    # Run each retrieval module with its parameters and save the results and execution times\n    for module, params in zip(modules, module_params):\n        start_time = time.time()\n        result = module(**params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n        results.append(result)\n        execution_times.append(execution_time)\n\n    # Create a dataframe with the execution times and save it to disk\n    execution_df = pd.DataFrame(\n        {'module': [module.__name__ for module in modules],\n         'execution_time': execution_times})\n    execution_df.to_csv(os.path.join(node_line_dir, 'execution_times.csv'), index=False)\n\n    # Evaluate the results using the specified strategies\n    evaluation_results = evaluate_results(results, strategies)\n\n    # Select the best result using the specified strategies\n    best_result = select_best_result(evaluation_results, strategies)\n\n    # Combine the previous result columns with the selected retrieval node's result columns\n    result_columns = [col for col in best_result.columns if col not in previous_result.columns]\n    combined_result = pd.concat([previous_result, best_result[result_columns]], axis=1)\n\n    # Save the combined result to disk\n    combined_result.to_csv(os.path.join(node_line_dir, 'result.csv'), index=False)\n\n    return combined_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Evaluate and select the best module among retrieval node results\n    results = []\n    metrics = []\n    for module, params in zip(modules, module_params):\n        result = module(previous_result, **params)\n        metric = evaluate_module(result, previous_result, strategies)\n        results.append(result)\n        metrics.append(metric)\n    best_result, best_metric = select_best_module(results, metrics, strategies)\n\n    # Save the results and a summary of the execution times and evaluation metrics to disk\n    save_results(results, metrics, best_result, best_metric, node_line_dir)\n\n    # Combine the previous result columns with the selected retrieval node's result columns\n    combined_result = combine_results(previous_result, best_result)\n\n    return combined_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize variables\n    best_result = None\n    best_result_score = 0\n    best_result_module = None\n    best_result_params = None\n    best_result_time = None\n    best_result_metrics = None\n    best_result_speed = None\n    best_result_speed_threshold = None\n    best_result_speed_threshold_met = False\n\n    # Create a directory for this node line\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Create a summary file for this node line\n    summary_file = os.path.join(node_line_dir, \"summary.txt\")\n    with open(summary_file, \"w\") as f:\n        f.write(\"Module,Params,Time,Metrics,Speed,Speed Threshold,Speed Threshold Met\\n\")\n\n    # Loop over each module and its parameters\n    for module, params in zip(modules, module_params):\n\n        # Initialize variables for this module\n        result = None\n        result_score = 0\n        result_time = None\n        result_metrics = None\n        result_speed = None\n        result_speed_threshold = None\n        result_speed_threshold_met = False\n\n        # Run the module with the given parameters\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n        result_time = end_time - start_time\n\n        # Evaluate the result using the specified metrics\n        result_metrics = evaluate_result(result, previous_result, strategies[\"metrics\"])\n        result_score = strategies[\"scoring_function\"](result_metrics)\n\n        # Check if the result speed is below the speed threshold\n        if strategies[\"speed_threshold\"] is not None:\n            result_speed_threshold = strategies[\"speed_threshold\"]\n            result_speed = result_time / result_speed_threshold\n            if result_speed < 1:\n                result_speed_threshold_met = True"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize the result and summary dataframes\n    result_df = pd.DataFrame()\n    summary_df = pd.DataFrame()\n\n    # Initialize the execution time list\n    execution_times = []\n\n    # Initialize the evaluation metrics list\n    evaluation_metrics = []\n\n    # Loop through each retrieval module\n    for module, params in zip(modules, module_params):\n        # Initialize the start time\n        start_time = time.time()\n\n        # Run the retrieval module with the given parameters\n        module_result = module(**params)\n\n        # Calculate the execution time\n        execution_time = time.time() - start_time\n\n        # Save the execution time to the list\n        execution_times.append(execution_time)\n\n        # Initialize the evaluation metric dictionary\n        evaluation_metric = {}\n\n        # Evaluate the retrieval module's results using the given strategies\n        for strategy_name, strategy_params in strategies.items():\n            # Get the evaluation metric function and parameters\n            metric_func = strategy_params['metric_func']\n            metric_params = strategy_params['metric_params']\n\n            # Evaluate the retrieval module's results using the metric function\n            evaluation_metric[strategy_name] = metric_func(module_result, previous_result, **metric_params)\n\n        # Save the evaluation metric dictionary to the list\n        evaluation_metrics.append(evaluation_metric)\n\n        # Concatenate the previous result and the module result\n        result_df = pd.concat([previous_result, module_result], axis=1)\n\n        # Save the result dataframe to disk\n        result_df.to_parquet(os.path.join(node_line_dir, 'result.parquet'))\n\n        # Save the execution time and evaluation metric to the summary dataframe\n        summary_df = summary_df.append({\n            'execution_time': execution_time,\n            'evaluation_metrics': evaluation_metric\n        }, ignore_index=True)\n\n        # Save the summary dataframe to disk\n        summary_df"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # create directory for node line\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # create directory for node line results\n    node_line_results_dir = os.path.join(node_line_dir, 'results')\n    os.makedirs(node_line_results_dir, exist_ok=True)\n\n    # create directory for node line summaries\n    node_line_summaries_dir = os.path.join(node_line_dir, 'summaries')\n    os.makedirs(node_line_summaries_dir, exist_ok=True)\n\n    # create directory for node line best results\n    node_line_best_results_dir = os.path.join(node_line_dir, 'best_results')\n    os.makedirs(node_line_best_results_dir, exist_ok=True)\n\n    # create directory for node line best summaries\n    node_line_best_summaries_dir = os.path.join(node_line_dir, 'best_summaries')\n    os.makedirs(node_line_best_summaries_dir, exist_ok=True)\n\n    # create directory for node line best summaries\n    node_line_best_summaries_dir = os.path.join(node_line_dir, 'best_summaries')\n    os.makedirs(node_line_best_summaries_dir, exist_ok=True)\n\n    # create directory for node line best summaries\n    node_line_best_summaries_dir = os.path.join(node_line_dir, 'best_summaries')\n    os.makedirs(node_line_best_summaries_dir, exist_ok=True)\n\n    # create directory for node line best summaries\n    node_line_best_summaries_dir = os.path.join(node_line_dir, 'best_summaries')\n    os.makedirs(node_line_best_summaries_dir, exist_ok=True)"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a directory for the current node if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Initialize an empty list to store the results from each module\n    results = []\n\n    # Initialize a dictionary to store the execution times for each module\n    execution_times = {}\n\n    # Initialize a dictionary to store the evaluation metrics for each module\n    evaluation_metrics = {}\n\n    # Iterate over the modules and their corresponding parameters\n    for module, params in zip(modules, module_params):\n        # Create a subdirectory for the current module\n        module_dir = os.path.join(node_line_dir, module.__name__)\n        if not os.path.exists(module_dir):\n            os.makedirs(module_dir)\n\n        # Run the module with the given parameters and the previous result\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n\n        # Calculate the execution time for the module\n        execution_time = end_time - start_time\n        execution_times[module.__name__] = execution_time\n\n        # Evaluate the performance of the module using the specified strategies\n        evaluation_metrics[module.__name__] = evaluate_module(result, strategies)\n\n        # Save the result and the evaluation metrics to the module directory\n        result.to_csv(os.path.join(module_dir, \"result.csv\"), index=False)\n        with open(os.path.join(module_dir, \"evaluation_metrics.json\"), \"w\") as f:\n            json.dump(evaluation_metrics[module.__name__], f)\n\n        # Append the result to the list of results\n        results.append(result)\n\n    # Create a dataframe from the list of results\n    results_df = pd.concat(results)\n\n    # Save the results dataframe to the node directory\n    results_df.to_csv(os.path.join(node_line_dir,"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a list to store the results of each query expansion module\n    results = []\n\n    # Create a list to store the execution times of each query expansion module\n    execution_times = []\n\n    # Loop through each query expansion module and its corresponding parameters\n    for module, module_param in zip(modules, module_params):\n\n        # Run the query expansion module with the given parameters\n        result, execution_time = module(previous_result, **module_param)\n\n        # Append the result and execution time to the corresponding lists\n        results.append(result)\n        execution_times.append(execution_time)\n\n    # Create a dataframe to store the results of each query expansion module\n    results_df = pd.DataFrame(results)\n\n    # Create a dataframe to store the execution times of each query expansion module\n    execution_times_df = pd.DataFrame(execution_times, columns=[\"execution_time\"])\n\n    # Concatenate the results and execution times dataframes\n    results_df = pd.concat([results_df, execution_times_df], axis=1)\n\n    # Evaluate the results of each query expansion module based on the specified strategies\n    evaluation_df = evaluate_query_expansion_node(results_df, strategies)\n\n    # Select the best result based on the evaluation\n    best_result = select_best_query_expansion_node(evaluation_df)\n\n    # Save the results of each query expansion module to a file\n    save_query_expansion_node_results(results_df, node_line_dir)\n\n    # Save the evaluation of each query expansion module to a file\n    save_query_expansion_node_evaluation(evaluation_df, node_line_dir)\n\n    # Save the best result to a file\n    save_query_expansion_node_best_result(best_result, node_line_dir)\n\n    # Return the best result\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the output directory if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Initialize a list to store the results from each module\n    results = []\n\n    # Initialize a dictionary to store the execution times for each module\n    execution_times = {}\n\n    # Initialize a dictionary to store the evaluation metrics for each module\n    evaluation_metrics = {}\n\n    # Initialize a dictionary to store the evaluation scores for each module\n    evaluation_scores = {}\n\n    # Iterate over the modules and their corresponding parameters\n    for module, params in zip(modules, module_params):\n\n        # Initialize a list to store the results from the current module\n        current_results = []\n\n        # Initialize a dictionary to store the execution times for the current module\n        current_execution_times = {}\n\n        # Initialize a dictionary to store the evaluation metrics for the current module\n        current_evaluation_metrics = {}\n\n        # Initialize a dictionary to store the evaluation scores for the current module\n        current_evaluation_scores = {}\n\n        # Iterate over the previous results\n        for i, previous_result_i in enumerate(previous_result):\n\n            # Run the current module with the current parameters and the current previous result\n            result_i, execution_time_i, evaluation_metrics_i, evaluation_score_i = module(previous_result_i, **params)\n\n            # Append the current result to the list of results from the current module\n            current_results.append(result_i)\n\n            # Append the current execution time to the dictionary of execution times for the current module\n            current_execution_times[i] = execution_time_i\n\n            # Append the current evaluation metrics to the dictionary of evaluation metrics for the current module\n            current_evaluation_metrics[i] = evaluation_metrics_i\n\n            # Append the current evaluation score to the dictionary of evaluation scores for the current module\n            current_evaluation_scores[i] = evaluation_score_i\n\n        # Append the list"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a directory for the node line if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize a dictionary to store the results of each module\n    results = {}\n\n    # Initialize a dictionary to store the execution times of each module\n    execution_times = {}\n\n    # Initialize a dictionary to store the evaluation metrics of each module\n    evaluation_metrics = {}\n\n    # Iterate over each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n\n        # Execute the module with the given parameters\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n\n        # Calculate the execution time for the module\n        execution_time = end_time - start_time\n\n        # Save the result and execution time for the module\n        results[module.__name__] = result\n        execution_times[module.__name__] = execution_time\n\n        # Evaluate the module's performance using the specified strategies\n        evaluation_metrics[module.__name__] = evaluate_query_expansion_node(result, strategies)\n\n    # Create a dataframe from the evaluation metrics dictionary\n    evaluation_df = pd.DataFrame(evaluation_metrics).T\n\n    # Save the evaluation metrics dataframe to a CSV file\n    evaluation_df.to_csv(os.path.join(node_line_dir, \"evaluation_metrics.csv\"))\n\n    # Save the execution times dataframe to a CSV file\n    pd.DataFrame(execution_times, index=[\"execution_time\"]).T.to_csv(os.path.join(node_line_dir, \"execution_times.csv\"))\n\n    # Save the results of each module to separate CSV files\n    for module_name, result in results.items():\n        result.to_csv(os.path.join(node_line_dir, f\"{module_name}.csv\"), index=False)\n\n    # Select the best module based on the specified strategies\n    best_module"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Evaluate and select the best module among query expansion node results\n    results = []\n    for module, params in zip(modules, module_params):\n        # Run each module with given parameters\n        result = module(previous_result, **params)\n        # Measure execution time\n        result['execution_time'] = time.time() - result['execution_time']\n        results.append(result)\n\n    # Evaluate performance based on specified strategies\n    metrics = strategies['metrics']\n    speed_thresholds = strategies['speed_thresholds']\n    other_criteria = strategies['other_criteria']\n\n    # Save the results and a summary, including execution times and evaluation metrics, to the specified directory\n    result_df = pd.concat(results)\n    result_df.to_csv(os.path.join(node_line_dir, 'results.csv'), index=False)\n    summary_df = result_df.groupby('module').agg({'execution_time': 'mean', **metrics})\n    summary_df.to_csv(os.path.join(node_line_dir, 'summary.csv'))\n\n    # Select and save the best result based on the evaluation\n    best_result = result_df.copy()\n    for metric in metrics:\n        best_result = best_result.loc[best_result.groupby('module')[metric].idxmax()]\n    for criteria, threshold in speed_thresholds.items():\n        best_result = best_result.loc[best_result['execution_time'] <= threshold]\n    for criteria, value in other_criteria.items():\n        best_result = best_result.loc[best_result[criteria] == value]\n\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Initialize variables for storing results and evaluation metrics\n    results = []\n    metrics = []\n    execution_times = []\n\n    # Loop through each query expansion module and its parameters\n    for module, params in zip(modules, module_params):\n        # Run the query expansion module with the given parameters\n        result = module(previous_result, **params)\n\n        # Measure the execution time of the query expansion module\n        start_time = time.time()\n        result = module(previous_result, **params)\n        execution_time = time.time() - start_time\n\n        # Evaluate the performance of the query expansion module using the specified strategies\n        metric = evaluate_result(result, strategies)\n\n        # Store the results, metrics, and execution times\n        results.append(result)\n        metrics.append(metric)\n        execution_times.append(execution_time)\n\n        # Save the result to the specified directory\n        result.to_csv(os.path.join(node_line_dir, f\"{module.__name__}.csv\"), index=False)\n\n    # Save the summary of the evaluation metrics and execution times to the specified directory\n    summary = pd.DataFrame({\n        \"module\": [module.__name__ for module in modules],\n        \"metrics\": metrics,\n        \"execution_time\": execution_times\n    })\n    summary.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    # Select the best result based on the evaluation metrics\n    best_result = results[np.argmin(metrics)]\n\n    # Save the best result to the specified directory\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    # Return the best result\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the output directory if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize the best result and best result index\n    best_result = None\n    best_result_index = None\n\n    # Initialize the execution times dictionary\n    execution_times = {}\n\n    # Loop through each module and its corresponding parameters\n    for module, module_param in zip(modules, module_params):\n\n        # Create a directory for the module\n        module_dir = os.path.join(node_line_dir, module.__name__)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Run the module with the given parameters\n        start_time = time.time()\n        result = module(previous_result, **module_param)\n        end_time = time.time()\n\n        # Calculate the execution time\n        execution_time = end_time - start_time\n\n        # Save the result to a file\n        result_file = os.path.join(module_dir, 'result.csv')\n        result.to_csv(result_file, index=False)\n\n        # Save the execution time to the execution times dictionary\n        execution_times[module.__name__] = execution_time\n\n        # Evaluate the result based on the strategies\n        evaluation_metrics = evaluate_result(result, strategies)\n\n        # Save the evaluation metrics to a file\n        evaluation_file = os.path.join(module_dir, 'evaluation.json')\n        with open(evaluation_file, 'w') as f:\n            json.dump(evaluation_metrics, f)\n\n        # Save the evaluation metrics to a summary file\n        summary_file = os.path.join(node_line_dir, 'summary.json')\n        with open(summary_file, 'w') as f:\n            json.dump(evaluation_metrics, f)\n\n        # Update the best result and best result index if the result is better than the current best\n        if best_result is None or evaluation_metrics['f1_score'] > best"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize empty lists to store results, execution times, and evaluation metrics\n    results = []\n    execution_times = []\n    evaluation_metrics = []\n\n    # Loop through each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Initialize an empty list to store the results for this module\n        module_results = []\n\n        # Initialize a timer to measure the execution time of the module\n        start_time = time.time()\n\n        # Run the module with the given parameters and previous result\n        module_result = module(previous_result, **params)\n\n        # Measure the execution time of the module\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Evaluate the performance of the module using the specified strategies\n        metrics = evaluate_module(module_result, strategies)\n\n        # Append the results, execution time, and evaluation metrics for this module to the respective lists\n        results.append(module_result)\n        execution_times.append(execution_time)\n        evaluation_metrics.append(metrics)\n\n    # Create a dataframe to store the results, execution times, and evaluation metrics\n    results_df = pd.DataFrame(\n        {\n            \"module\": [module.__name__ for module in modules],\n            \"results\": results,\n            \"execution_time\": execution_times,\n            \"metrics\": evaluation_metrics,\n        }\n    )\n\n    # Save the results dataframe to a file\n    results_df.to_csv(f\"{node_line_dir}/results.csv\", index=False)\n\n    # Create a summary dataframe to store the execution times and evaluation metrics\n    summary_df = pd.DataFrame(\n        {\n            \"module\": [module.__name__ for module in modules],\n            \"execution_time\": execution_times,\n            \"metrics\": evaluation_metrics,\n        }\n    )\n\n    # Save the summary dataframe to a file\n    summary_df.to_csv(f\"{node_line_dir}/summary.csv\", index=False)"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the node line directory if it does not exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize a list to store the results from each module\n    results = []\n\n    # Initialize a dictionary to store the execution times for each module\n    execution_times = {}\n\n    # Loop through each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n\n        # Run the module with the given parameters\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n\n        # Calculate the execution time for the module\n        execution_time = end_time - start_time\n\n        # Store the result and execution time in the corresponding lists\n        results.append(result)\n        execution_times[module.__name__] = execution_time\n\n    # Create a dictionary to store the evaluation metrics for each module\n    evaluation_metrics = {}\n\n    # Loop through each module and its corresponding result\n    for module, result in zip(modules, results):\n\n        # Evaluate the module's performance using the specified strategies\n        evaluation_metrics[module.__name__] = evaluate_performance(result, strategies)\n\n    # Create a summary dataframe to store the evaluation metrics and execution times\n    summary_df = pd.DataFrame(evaluation_metrics, index=['Evaluation Metrics'])\n    summary_df['Execution Time'] = pd.Series(execution_times)\n\n    # Save the summary dataframe to a CSV file\n    summary_file_path = os.path.join(node_line_dir, 'summary.csv')\n    summary_df.to_csv(summary_file_path)\n\n    # Select the best result based on the specified strategies\n    best_result = select_best_result(results, strategies)\n\n    # Save the best result to a CSV file\n    best_result_file_path = os.path.join(node_line_dir, 'best_result.csv')\n    best_result.to_csv(best_result_file"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Evaluate and select the best module among query expansion node results\n    result = []\n    for module, params in zip(modules, module_params):\n        # Run the query expansion module with the given parameters\n        module_result = module(previous_result, **params)\n        # Measure the execution time of the query expansion module\n        module_result['execution_time'] = time.time() - module_result['execution_time']\n        # Evaluate the performance of the query expansion module based on the specified strategies\n        evaluation = evaluate_query_expansion_node(module_result, strategies)\n        # Save the query expansion module result and its evaluation to the specified directory\n        module_result.to_csv(os.path.join(node_line_dir, f'{module.__name__}.csv'), index=False)\n        evaluation.to_csv(os.path.join(node_line_dir, f'{module.__name__}_evaluation.csv'), index=False)\n        # Append the evaluation to the list of results\n        result.append(evaluation)\n    # Convert the list of results to a dataframe\n    result = pd.DataFrame(result)\n    # Save the summary of all query expansion module results and their evaluations to the specified directory\n    result.to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n    # Select and save the best result based on the evaluation\n    best_result = result.sort_values(by=strategies['sort_by'], ascending=strategies['sort_ascending']).iloc[0]\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the output directory if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Run each module with the given parameters and measure its execution time\n    execution_times = {}\n    for module, module_param in zip(modules, module_params):\n        start_time = time.time()\n        result = module(previous_result, **module_param)\n        execution_time = time.time() - start_time\n        execution_times[module.__name__] = execution_time\n\n        # Save the result\n        result_file = os.path.join(node_line_dir, f\"{module.__name__}.csv\")\n        result.to_csv(result_file, index=False)\n\n    # Evaluate the results and select the best module\n    best_module = None\n    best_result = None\n    best_score = float(\"-inf\")\n\n    for module, result in zip(modules, results):\n        # Evaluate the result based on the specified strategies\n        score = evaluate_result(result, strategies)\n\n        # Update the best module and result if the score is higher\n        if score > best_score:\n            best_module = module\n            best_result = result\n            best_score = score\n\n    # Save the best result\n    best_result_file = os.path.join(node_line_dir, f\"{best_module.__name__}.csv\")\n    best_result.to_csv(best_result_file, index=False)\n\n    # Save the execution times and evaluation metrics\n    summary_file = os.path.join(node_line_dir, \"summary.txt\")\n    with open(summary_file, \"w\") as f:\n        for module, execution_time in execution_times.items():\n            f.write(f\"{module}: {execution_time:.2f} seconds\\n\")\n        f.write(f\"Best module: {best_module.__name__}\\n\")\n        f.write(f\"Best score: {best_score:.2f}\\n\")\n\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a dictionary to store the results from each module\n    module_results = {}\n\n    # Create a dictionary to store the execution times for each module\n    module_exec_times = {}\n\n    # Create a dictionary to store the evaluation metrics for each module\n    module_eval_metrics = {}\n\n    # Iterate over the modules and their parameters\n    for module, params in zip(modules, module_params):\n\n        # Execute the module with the given parameters\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n\n        # Calculate the execution time for the module\n        exec_time = end_time - start_time\n\n        # Store the result and execution time in the dictionaries\n        module_results[module.__name__] = result\n        module_exec_times[module.__name__] = exec_time\n\n        # Calculate the evaluation metrics for the module\n        eval_metrics = evaluate_module_performance(result, strategies)\n        module_eval_metrics[module.__name__] = eval_metrics\n\n    # Create a dataframe to store the evaluation metrics for each module\n    eval_metrics_df = pd.DataFrame(module_eval_metrics).T\n\n    # Save the evaluation metrics to a file\n    eval_metrics_df.to_csv(os.path.join(node_line_dir, 'eval_metrics.csv'))\n\n    # Create a dataframe to store the execution times for each module\n    exec_times_df = pd.DataFrame(module_exec_times, index=['exec_time']).T\n\n    # Save the execution times to a file\n    exec_times_df.to_csv(os.path.join(node_line_dir, 'exec_times.csv'))\n\n    # Select the best module based on the evaluation metrics\n    best_module = eval_metrics_df.idxmax(axis=0)\n\n    # Get the best result from the best module\n    best_result = module_results[best_module]\n\n    # Save the best"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Evaluate and select the best module among query expansion node results\n    # Save the results and a summary, including execution times and evaluation metrics, to the specified directory\n    # Select and save the best result based on the evaluation\n\n    # Create a directory for the node line results if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Initialize a list to store the execution times of each module\n    execution_times = []\n\n    # Initialize a list to store the evaluation metrics of each module\n    evaluation_metrics = []\n\n    # Initialize a list to store the results of each module\n    results = []\n\n    # Loop over the modules and their corresponding parameters\n    for module, params in zip(modules, module_params):\n        # Run the module with the given parameters\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n\n        # Calculate the execution time of the module\n        execution_time = end_time - start_time\n\n        # Evaluate the performance of the module using the specified strategies\n        evaluation_metric = evaluate_module(result, strategies)\n\n        # Save the results, execution time, and evaluation metric of the module to the specified directory\n        results.append(result)\n        execution_times.append(execution_time)\n        evaluation_metrics.append(evaluation_metric)\n\n        # Save the results, execution time, and evaluation metric of the module to the specified directory\n        result.to_csv(f\"{node_line_dir}/result_{module.__name__}.csv\", index=False)\n        with open(f\"{node_line_dir}/execution_times_{module.__name__}.txt\", \"w\") as f:\n            f.write(str(execution_time))\n        with open(f\"{node_line_dir}/evaluation_metrics_{module.__name__}.txt\", \"w\") as f:\n            f.write(str(evaluation_metric))\n\n    # Create a dataframe to store the execution times"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create directory for the node if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Execute each module with the given parameters\n    results = []\n    for module, params in zip(modules, module_params):\n        start_time = time.time()\n        result = module(previous_result, **params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n        results.append(result)\n\n    # Save results and summaries for each module\n    for i, result in enumerate(results):\n        result_file = os.path.join(node_line_dir, f\"result_{i}.csv\")\n        result.to_csv(result_file, index=False)\n\n        summary_file = os.path.join(node_line_dir, f\"summary_{i}.json\")\n        summary = {\n            \"execution_time\": execution_time,\n            \"metrics\": {}\n        }\n        with open(summary_file, \"w\") as f:\n            json.dump(summary, f)\n\n    # Evaluate and select the best result\n    best_result = None\n    best_score = -np.inf\n    for i, result in enumerate(results):\n        summary_file = os.path.join(node_line_dir, f\"summary_{i}.json\")\n        with open(summary_file, \"r\") as f:\n            summary = json.load(f)\n\n        # Evaluate the result based on the specified strategies\n        score = 0\n        for metric, threshold in strategies[\"metrics\"].items():\n            metric_value = summary[\"metrics\"].get(metric, None)\n            if metric_value is not None:\n                score += metric_value\n\n        if strategies[\"speed_thresholds\"] is not None:\n            speed_threshold = strategies[\"speed_thresholds\"].get(i, None)\n            if speed_threshold is not None:\n                score -= max(0, summary[\"execution_time\"] - speed_"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory for the node if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a dictionary to store the results for each module\n    results = {}\n\n    # Loop through each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n\n        # Create a directory for the module\n        module_dir = os.path.join(node_line_dir, module.__name__)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Run the module with the specified parameters and the previous result\n        result = module(previous_result, **params)\n\n        # Save the result to the module directory\n        result.to_csv(os.path.join(module_dir, 'result.csv'), index=False)\n\n        # Save the result to the results dictionary\n        results[module.__name__] = result\n\n    # Create a dataframe to store the summary of the results\n    summary = pd.DataFrame(columns=['module', 'execution_time', 'speed', 'accuracy', 'recall', 'precision', 'f1'])\n\n    # Loop through each module and its corresponding result\n    for module, result in results.items():\n\n        # Calculate the execution time of the module\n        execution_time = time.time() - start_time\n\n        # Calculate the speed of the module\n        speed = result.shape[0] / execution_time\n\n        # Calculate the accuracy of the module\n        accuracy = accuracy_score(result['label'], result['prediction'])\n\n        # Calculate the recall of the module\n        recall = recall_score(result['label'], result['prediction'], zero_division=0)\n\n        # Calculate the precision of the module\n        precision = precision_score(result['label'], result['prediction'], zero_division=0)\n\n        # Calculate the F1 score of the module\n        f1 = f1_score(result['label'], result['prediction'], zero_division=0)\n\n        # Append the summary for the module to the dataframe"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory for the node if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Initialize the results list\n    results = []\n\n    # Initialize the summary list\n    summary = []\n\n    # Initialize the best result\n    best_result = None\n\n    # Iterate over the modules and their parameters\n    for module, module_param in zip(modules, module_params):\n\n        # Initialize the module execution time\n        module_execution_time = 0\n\n        # Initialize the module execution start time\n        module_execution_start_time = time.time()\n\n        # Run the module with the given parameters\n        result = module(previous_result, **module_param)\n\n        # Calculate the module execution time\n        module_execution_time = time.time() - module_execution_start_time\n\n        # Append the module execution time to the summary\n        summary.append(module_execution_time)\n\n        # Evaluate the result using the specified strategies\n        result_evaluation = evaluate_result(result, strategies)\n\n        # Append the result evaluation to the summary\n        summary.append(result_evaluation)\n\n        # Append the result and summary to the results list\n        results.append((result, summary))\n\n        # Reset the summary list\n        summary = []\n\n        # Check if the result is better than the best result\n        if best_result is None or result_evaluation > best_result[1]:\n            best_result = (result, result_evaluation)\n\n    # Save the results to the node directory\n    for i, (result, summary) in enumerate(results):\n        result.to_csv(os.path.join(node_line_dir, f\"result_{i}.csv\"), index=False)\n\n    # Save the summary to the node directory\n    summary_df = pd.DataFrame(results, columns=[\"result\", \"summary\"])\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize a dictionary to store the results from each module\n    results_dict = {}\n\n    # Initialize a dictionary to store the execution times for each module\n    execution_times = {}\n\n    # Initialize a dictionary to store the evaluation metrics for each module\n    evaluation_metrics = {}\n\n    # Loop through each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Initialize a dictionary to store the results from the current module\n        module_results = {}\n\n        # Initialize a dictionary to store the execution times for the current module\n        module_execution_times = {}\n\n        # Initialize a dictionary to store the evaluation metrics for the current module\n        module_evaluation_metrics = {}\n\n        # Run the current module with the given parameters\n        start_time = time.time()\n        module_results[module.__name__] = module(previous_result, **params)\n        end_time = time.time()\n\n        # Calculate the execution time for the current module\n        execution_time = end_time - start_time\n        module_execution_times[module.__name__] = execution_time\n\n        # Calculate the evaluation metrics for the current module\n        module_evaluation_metrics[module.__name__] = evaluate_node_results(module_results[module.__name__],\n                                                                           strategies)\n\n        # Add the results, execution times, and evaluation metrics to the dictionaries\n        results_dict[module.__name__] = module_results\n        execution_times[module.__name__] = module_execution_times\n        evaluation_metrics[module.__name__] = module_evaluation_metrics\n\n        # Save the results, execution times, and evaluation metrics to files\n        save_node_results(results_dict, execution_times, evaluation_metrics, node_line_dir)\n\n    # Select the best module based on the evaluation metrics\n    best_module = select_best_node_module(evaluation_metrics, strategies)"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the node directory if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Initialize the best result and summary\n    best_result = None\n    best_summary = None\n\n    # Initialize a dictionary to store the execution times for each module\n    execution_times = {}\n\n    # Initialize a dictionary to store the evaluation metrics for each module\n    evaluation_metrics = {}\n\n    # Iterate through each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n        # Get the module name\n        module_name = module.__name__\n\n        # Get the start time for the module execution\n        start_time = time.time()\n\n        # Run the module with the given parameters and the previous result\n        result = module(previous_result, **params)\n\n        # Get the end time for the module execution\n        end_time = time.time()\n\n        # Calculate the execution time for the module\n        execution_time = end_time - start_time\n\n        # Save the execution time for the module\n        execution_times[module_name] = execution_time\n\n        # Evaluate the performance of the module\n        evaluation_metrics[module_name] = evaluate_performance(result, strategies)\n\n        # Save the module result and summary to the node directory\n        result.to_csv(os.path.join(node_line_dir, f\"{module_name}_result.csv\"), index=False)\n        summary = pd.DataFrame(\n            [execution_times, evaluation_metrics],\n            index=[\"execution_times\", \"evaluation_metrics\"]\n        )\n        summary.to_csv(os.path.join(node_line_dir, f\"{module_name}_summary.csv\"))\n\n        # If the current module result is better than the best result, update the best result and summary\n        if best_result is None or evaluation_metrics[module_name] > evaluation_metrics[best_result.name]:\n            best_result = result\n            best_summary"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a directory for the node line results\n    node_line_dir = os.path.join(node_line_dir, \"node_line_result\")\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Evaluate each module and save the results\n    results = []\n    for i, (module, params) in enumerate(zip(modules, module_params)):\n        print(f\"Executing module {i + 1}/{len(modules)}: {module.__name__}\")\n        module_result = module(previous_result, **params)\n        results.append(module_result)\n        module_result.to_csv(os.path.join(node_line_dir, f\"module_{i + 1}.csv\"), index=False)\n\n    # Evaluate the results and select the best one\n    best_result = select_best_result(results, strategies)\n\n    # Save the best result to a file\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'results'), exist_ok=True)\n\n    # Run each prompt maker module and evaluate its performance\n    results = []\n    for module, module_param in zip(modules, module_params):\n        result = module(**module_param)\n        results.append(result)\n\n    # Select the best prompt maker module based on specified strategies\n    best_result = select_best_prompt_maker(results, strategies)\n\n    # Combine the results of the previous operation and the best prompt maker's output\n    combined_result = pd.concat([previous_result, best_result], axis=0)\n\n    # Save the results and a summary\n    combined_result.to_csv(os.path.join(node_line_dir, 'results', 'combined_result.csv'), index=False)\n    summary = create_summary(results, strategies)\n    summary.to_csv(os.path.join(node_line_dir, 'results', 'summary.csv'), index=False)\n\n    return combined_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    create_directories([node_line_dir])\n\n    # Run prompt maker modules\n    results = run_modules(modules, module_params, node_line_dir, strategies)\n\n    # Select best prompt maker\n    best_prompt_maker_result = select_best_prompt_maker(results, strategies)\n\n    # Combine results\n    combined_result = combine_results(previous_result, best_prompt_maker_result)\n\n    # Save results\n    save_results(combined_result, node_line_dir)\n\n    return combined_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Check if the node_line_dir exists, if not create it\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Create a list to store the results of each module\n    results = []\n\n    # Iterate through each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n        # Create a directory for the current module\n        module_dir = os.path.join(node_line_dir, module.__name__)\n        if not os.path.exists(module_dir):\n            os.makedirs(module_dir)\n\n        # Run the module with the given parameters\n        start_time = time.time()\n        result = module(**params)\n        end_time = time.time()\n\n        # Save the result to a file\n        result_file = os.path.join(module_dir, 'result.json')\n        with open(result_file, 'w') as f:\n            json.dump(result, f)\n\n        # Save the execution time to a file\n        execution_time_file = os.path.join(module_dir, 'execution_time.txt')\n        with open(execution_time_file, 'w') as f:\n            f.write(str(end_time - start_time))\n\n        # Append the result to the list\n        results.append(result)\n\n    # Evaluate the results using the specified strategies\n    best_result = evaluate_results(results, strategies)\n\n    # Combine the best result with the previous result\n    combined_result = combine_results(best_result, previous_result)\n\n    # Save the combined result to a file\n    combined_result_file = os.path.join(node_line_dir, 'combined_result.json')\n    with open(combined_result_file, 'w') as f:\n        json.dump(combined_result, f)\n\n    # Return the combined result\n    return combined_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create a directory for this node if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the summary of this node if it doesn't exist\n    summary_dir = os.path.join(node_line_dir, 'summary')\n    os.makedirs(summary_dir, exist_ok=True)\n\n    # Create a directory for the results of this node if it doesn't exist\n    result_dir = os.path.join(node_line_dir, 'result')\n    os.makedirs(result_dir, exist_ok=True)\n\n    # Create a directory for the logs of this node if it doesn't exist\n    log_dir = os.path.join(node_line_dir, 'log')\n    os.makedirs(log_dir, exist_ok=True)\n\n    # Create a directory for the artifacts of this node if it doesn't exist\n    artifacts_dir = os.path.join(node_line_dir, 'artifacts')\n    os.makedirs(artifacts_dir, exist_ok=True)\n\n    # Create a directory for the metrics of this node if it doesn't exist\n    metrics_dir = os.path.join(node_line_dir, 'metrics')\n    os.makedirs(metrics_dir, exist_ok=True)\n\n    # Create a directory for the figures of this node if it doesn't exist\n    figures_dir = os.path.join(node_line_dir, 'figures')\n    os.makedirs(figures_dir, exist_ok=True)\n\n    # Create a directory for the logs of this node if it doesn't exist\n    logs_dir = os.path.join(node_line_dir, 'logs')\n    os.makedirs(logs_dir, exist_ok=True)\n\n    # Create a directory for the artifacts of this node if it doesn't exist\n    artifacts_dir = os.path.join(node_line_dir, 'artifacts')\n    os.makedirs"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker_results'), exist_ok=True)\n\n    # Run prompt maker modules\n    for module, params in zip(modules, module_params):\n        module_name = module.__name__.split('.')[-1]\n        module_dir = os.path.join(node_line_dir, 'prompt_maker_results', module_name)\n        os.makedirs(module_dir, exist_ok=True)\n        module_result = module(**params)\n\n        # Save module results\n        module_result.to_csv(os.path.join(module_dir, 'module_result.csv'), index=False)\n\n        # Evaluate module performance\n        evaluation_result = evaluate_prompt_maker(module_result, strategies['generator_module'])\n\n        # Save evaluation results\n        evaluation_result.to_csv(os.path.join(module_dir, 'evaluation_result.csv'), index=False)\n\n    # Select best prompt maker module\n    best_module_name, best_module_result = select_best_prompt_maker(node_line_dir, strategies)\n\n    # Save best module results\n    best_module_result.to_csv(os.path.join(node_line_dir, 'best_module_result.csv'), index=False)\n\n    # Combine results\n    combined_result = combine_results(previous_result, best_module_result)\n\n    # Save combined results\n    combined_result.to_csv(os.path.join(node_line_dir, 'combined_result.csv'), index=False)\n\n    return combined_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, \"output\"), exist_ok=True)\n\n    # Initialize a dictionary to store the results of each prompt maker module\n    results = {}\n\n    # Initialize a dictionary to store the execution times of each prompt maker module\n    execution_times = {}\n\n    # Initialize a dictionary to store the evaluation metrics of each prompt maker module\n    evaluation_metrics = {}\n\n    # Initialize a dictionary to store the best prompt maker module based on specified strategies\n    best_prompt_maker = {}\n\n    # Initialize a dictionary to store the best prompt maker's parameters\n    best_prompt_maker_params = {}\n\n    # Initialize a dictionary to store the best prompt maker's evaluation metrics\n    best_prompt_maker_evaluation_metrics = {}\n\n    # Initialize a dictionary to store the best prompt maker's execution time\n    best_prompt_maker_execution_time = {}\n\n    # Initialize a dictionary to store the best prompt maker's output\n    best_prompt_maker_output = {}\n\n    # Iterate over each prompt maker module\n    for module, module_param in zip(modules, module_params):\n        # Initialize a dictionary to store the results of the current prompt maker module\n        results[module.__name__] = {}\n\n        # Initialize a dictionary to store the evaluation metrics of the current prompt maker module\n        evaluation_metrics[module.__name__] = {}\n\n        # Initialize a dictionary to store the execution times of the current prompt maker module\n        execution_times[module.__name__] = {}\n\n        # Initialize a dictionary to store the best prompt maker module based on specified strategies for the current prompt maker module\n        best_prompt_maker[module.__name__] = {}\n\n        # Initialize a dictionary to store the best prompt maker's parameters for the current prompt maker module\n        best_prompt_maker_params[module.__name__] = {}\n\n        # Initialize a dictionary to store the best prompt maker'"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker_results'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker_results', 'summary'), exist_ok=True)\n\n    # Create a dictionary to store the results of each prompt maker module\n    results = {}\n\n    # Create a dictionary to store the execution times of each prompt maker module\n    execution_times = {}\n\n    # Create a dictionary to store the evaluation metrics of each prompt maker module\n    evaluation_metrics = {}\n\n    # Iterate through each prompt maker module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n        # Get the name of the prompt maker module\n        module_name = module.__name__\n\n        # Run the prompt maker module with the given parameters\n        start_time = time.time()\n        result = module(**params)\n        execution_time = time.time() - start_time\n\n        # Store the result in the results dictionary\n        results[module_name] = result\n\n        # Store the execution time in the execution_times dictionary\n        execution_times[module_name] = execution_time\n\n        # Evaluate the prompt maker module's performance using the specified strategies\n        evaluation_metrics[module_name] = evaluate_prompt_maker_performance(result, strategies)\n\n    # Select the best prompt maker module based on the specified strategies\n    best_prompt_maker = select_best_prompt_maker(evaluation_metrics, strategies)\n\n    # Save the results of the best prompt maker module to a file\n    save_results(results[best_prompt_maker], os.path.join(node_line_dir, 'prompt_maker_results', 'best_prompt_maker_results.csv'))\n\n    # Save the execution times and evaluation metrics to a file\n    save_summary(execution_times, evaluation_metrics, os.path.join(node_line_"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Run each prompt maker module and evaluate its performance\n    results = []\n    for module, params in zip(modules, module_params):\n        module_result = module(**params)\n        results.append(module_result)\n\n    # Select the best prompt maker module based on the specified strategies\n    best_module, best_result = select_best_module(results, strategies)\n\n    # Combine the results of the previous operation and the best prompt maker's output\n    combined_result = pd.concat([previous_result, best_result])\n\n    # Save the results and a summary to the specified directory\n    combined_result.to_csv(os.path.join(node_line_dir, \"combined_result.csv\"), index=False)\n    summary = create_summary(results, strategies)\n    summary.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    return combined_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    output_dir = os.path.join(node_line_dir, \"output\")\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Run each prompt maker module with given parameters\n    results = []\n    for module, params in zip(modules, module_params):\n        try:\n            result = module(**params)\n            results.append(result)\n        except Exception as e:\n            print(f\"Error occurred while running {module.__name__}: {str(e)}\")\n\n    # Evaluate the results and select the best prompt maker module based on specified strategies\n    best_module = None\n    best_result = None\n    for result in results:\n        if strategies[\"evaluation\"] == \"speed\":\n            if result[\"speed\"] < strategies[\"speed_threshold\"]:\n                best_module = result[\"module\"]\n                best_result = result\n        elif strategies[\"evaluation\"] == \"accuracy\":\n            if result[\"accuracy\"] > strategies[\"accuracy_threshold\"]:\n                best_module = result[\"module\"]\n                best_result = result\n\n    # Combine the results of the previous operation and the best prompt maker's output\n    combined_result = pd.concat([previous_result, best_result], axis=0)\n\n    # Save the combined result to the specified directory\n    combined_result.to_csv(os.path.join(output_dir, \"combined_result.csv\"), index=False)\n\n    # Save the summary of the best prompt maker module to the specified directory\n    summary = {\n        \"module\": best_module.__name__,\n        \"parameters\": params,\n        \"execution_time\": best_result[\"execution_time\"],\n        \"accuracy\": best_result[\"accuracy\"],\n        \"speed\": best_result[\"speed\"],\n    }\n    with open(os.path.join(output_dir, \"summary.json\"), \"w\") as f:\n        json.dump(summary, f)\n\n    return combined_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    create_necessary_dirs(node_line_dir)\n\n    # Get the default generator module\n    default_generator_module = strategies['generator_module']\n\n    # Initialize a list to store the results of all prompt maker modules\n    results = []\n\n    # Iterate over the prompt maker modules and their parameters\n    for module, params in zip(modules, module_params):\n        # Run the prompt maker module with the specified parameters\n        result = module(**params)\n\n        # Evaluate the performance of the prompt maker module using the default generator module\n        result = evaluate_module_performance(result, default_generator_module)\n\n        # Append the result to the list of results\n        results.append(result)\n\n    # Select the best prompt maker module based on the specified strategies\n    best_result = select_best_prompt_maker_module(results, strategies)\n\n    # Combine the results of the previous operation and the best prompt maker module\n    combined_result = combine_results(previous_result, best_result)\n\n    # Save the combined results to the specified directory\n    combined_result.to_csv(os.path.join(node_line_dir, 'combined_result.csv'), index=False)\n\n    # Save the summary of the execution to the specified directory\n    save_summary(results, node_line_dir)\n\n    return combined_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'results'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'summary'), exist_ok=True)\n\n    # Create a dictionary to store the results of each module\n    results = {}\n\n    # Create a dictionary to store the execution times of each module\n    execution_times = {}\n\n    # Create a dictionary to store the evaluation metrics of each module\n    evaluation_metrics = {}\n\n    # Create a dictionary to store the evaluation metrics of each module\n    evaluation_metrics = {}\n\n    # Iterate over each module and its parameters\n    for module, module_param in zip(modules, module_params):\n        # Get the module name\n        module_name = module.__name__\n\n        # Run the module\n        start_time = time.time()\n        result = module(**module_param)\n        end_time = time.time()\n\n        # Save the result\n        results[module_name] = result\n\n        # Save the execution time\n        execution_times[module_name] = end_time - start_time\n\n        # Evaluate the result using the specified strategy\n        evaluation_metrics[module_name] = evaluate_result(result, strategies['generator_module'])\n\n    # Select the best module based on the specified strategy\n    best_module = select_best_module(evaluation_metrics, strategies['strategy'])\n\n    # Combine the results of the previous operation and the best module\n    combined_result = combine_results(previous_result, results[best_module])\n\n    # Save the combined result\n    combined_result.to_csv(os.path.join(node_line_dir, 'results', 'combined_result.csv'), index=False)\n\n    # Create a summary of the execution times and evaluation metrics\n    summary = pd.DataFrame({'Execution Time (s)': execution_times, 'Evaluation Metrics': evaluation_metrics})"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    create_directory(node_line_dir)\n    create_directory(os.path.join(node_line_dir, \"prompt_maker\"))\n    create_directory(os.path.join(node_line_dir, \"generator\"))\n    create_directory(os.path.join(node_line_dir, \"evaluation\"))\n\n    # Run prompt maker modules\n    prompt_maker_results = []\n    for module, params in zip(modules, module_params):\n        prompt_maker_results.append(module(**params))\n\n    # Evaluate prompt maker modules\n    evaluation_results = []\n    for module, params, result in zip(modules, module_params, prompt_maker_results):\n        evaluation_results.append(evaluate_prompt_maker(module, params, result, strategies))\n\n    # Select best prompt maker module\n    best_prompt_maker_index = select_best_prompt_maker(evaluation_results, strategies)\n\n    # Save results\n    save_results(prompt_maker_results, node_line_dir, \"prompt_maker\")\n\n    # Save evaluation results\n    save_results(evaluation_results, node_line_dir, \"evaluation\")\n\n    # Save best prompt maker result\n    save_results([prompt_maker_results[best_prompt_maker_index]], node_line_dir, \"prompt_maker\", \"best\")\n\n    # Combine previous result and best prompt maker result\n    combined_result = combine_results(previous_result, prompt_maker_results[best_prompt_maker_index])\n\n    # Save combined result\n    save_results([combined_result], node_line_dir, \"combined\")\n\n    # Run generator module with best prompt maker result\n    generator_module = strategies.get(\"generator_module\", default_generator_module)\n    generator_params = strategies.get(\"generator_params\", default_generator_params)\n    generator_result = generator_module(combined_result, **generator_params)\n\n    # Save generator result\n    save_results([generator_result], node_"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    node_line_dir = node_line_dir + '/prompt_maker'\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(node_line_dir + '/output', exist_ok=True)\n    os.makedirs(node_line_dir + '/summary', exist_ok=True)\n\n    # Initialize variables\n    best_module = None\n    best_module_params = None\n    best_result = None\n    best_time = None\n    best_metrics = None\n    best_generator = None\n    best_generator_params = None\n    best_generator_result = None\n    best_generator_time = None\n    best_generator_metrics = None\n\n    # Run prompt maker modules\n    for module, params in zip(modules, module_params):\n        # Run module\n        start_time = time.time()\n        result = module(**params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Evaluate module\n        metrics = evaluate_module(result, strategies['metrics'])\n\n        # Check if module is better than the current best\n        if best_module is None or metrics['score'] > best_metrics['score']:\n            best_module = module\n            best_module_params = params\n            best_result = result\n            best_time = execution_time\n            best_metrics = metrics\n\n    # Run generator module with best prompt maker module's parameters\n    if strategies['generator'] is not None:\n        generator, generator_params = strategies['generator']\n        # Run generator module\n        start_time = time.time()\n        generator_result = generator(best_result, **generator_params)\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        # Evaluate generator module\n        generator_metrics = evaluate_module(generator_result, strategies['metrics'])\n\n        # Check if generator module is better than the current best\n        if best_generator is None or generator_metrics"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create a subdirectory for the current node\n    node_dir = os.path.join(node_line_dir, \"node_\" + str(len(os.listdir(node_line_dir))))\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Run the prompt maker modules and evaluate their performance\n    results = []\n    for module, params in zip(modules, module_params):\n        result = module(**params)\n        results.append(result)\n\n    # Select the best prompt maker module based on the specified strategies\n    best_module, best_result = select_best_prompt_maker(results, strategies)\n\n    # Combine the results of the previous operation and the best prompt maker's result\n    combined_result = pd.concat([previous_result, best_result])\n\n    # Save the combined result to the node's output directory\n    combined_result.to_csv(os.path.join(node_dir, \"combined_result.csv\"), index=False)\n\n    # Save the summary of the execution to the node's output directory\n    summary = create_summary(results, strategies)\n    summary.to_csv(os.path.join(node_dir, \"summary.csv\"), index=False)\n\n    return combined_result\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    node_line_dir = Path(node_line_dir)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n\n    # Get default generator module\n    default_generator_module = strategies.get(\"default_generator_module\", \"default_generator_module\")\n\n    # Initialize a list to store the results of each prompt maker module\n    results = []\n\n    # Iterate over each prompt maker module and its parameters\n    for module, params in zip(modules, module_params):\n        # Create a subdirectory for the current prompt maker module\n        module_dir = node_line_dir / module.__name__\n        module_dir.mkdir(parents=True, exist_ok=True)\n\n        # Execute the prompt maker module with the given parameters\n        result = module(**params)\n\n        # Evaluate the performance of the prompt maker module\n        eval_result = evaluate_prompt_maker_module(result, default_generator_module, strategies)\n\n        # Save the evaluation results to a file\n        eval_result_file = module_dir / \"eval_result.json\"\n        with open(eval_result_file, \"w\") as f:\n            json.dump(eval_result, f)\n\n        # Append the evaluation result to the list of results\n        results.append(eval_result)\n\n    # Select the best prompt maker module based on the specified strategies\n    best_result = select_best_prompt_maker_module(results, strategies)\n\n    # Combine the best result with the previous result\n    combined_result = pd.concat([previous_result, best_result])\n\n    # Save the combined result to a file\n    combined_result_file = node_line_dir / \"combined_result.csv\"\n    combined_result.to_csv(combined_result_file, index=False)\n\n    # Return the combined result\n    return combined_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker'), exist_ok=True)\n\n    # Initialize variables\n    best_module = None\n    best_module_params = None\n    best_module_result = None\n    best_module_summary = None\n    best_module_time = None\n    best_module_metric = None\n\n    # Initialize default generator module if not specified\n    if 'generator_module' not in strategies:\n        strategies['generator_module'] = 'default'\n\n    # Run each prompt maker module with specified parameters\n    for module, module_param in zip(modules, module_params):\n        module_result, module_summary, module_time = module(**module_param)\n        module_result = pd.DataFrame(module_result)\n\n        # Evaluate the performance of the prompt maker module\n        module_metric = evaluate_prompt_maker(module_result,\n                                              strategies['generator_module'])\n\n        # Update the best prompt maker module if necessary\n        if best_module is None or module_metric < best_module_metric:\n            best_module = module\n            best_module_params = module_param\n            best_module_result = module_result\n            best_module_summary = module_summary\n            best_module_time = module_time\n            best_module_metric = module_metric\n\n    # Save the best prompt maker module's results\n    best_module_result.to_csv(os.path.join(node_line_dir,\n                                           'prompt_maker',\n                                           'best_module_result.csv'),\n                              index=False)\n\n    # Save the best prompt maker module's summary\n    with open(os.path.join(node_line_dir,\n                           'prompt_maker',\n                           'best_module_summary.json'), 'w') as f:\n        json.dump(best_module_summary, f)\n\n    # Combine the best prompt maker module's result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    create_dir(node_line_dir)\n    create_dir(f\"{node_line_dir}/prompt_maker_results\")\n\n    # Initialize variables for storing results and evaluation metrics\n    best_module_idx = None\n    best_module_result = None\n    best_module_metrics = None\n\n    # Loop through each prompt maker module\n    for module_idx, (module, params) in enumerate(zip(modules, module_params)):\n        print(f\"Executing prompt maker module {module_idx + 1}\")\n\n        # Execute the prompt maker module\n        module_result = module(**params)\n\n        # Save the module result to a file\n        module_result.to_csv(f\"{node_line_dir}/prompt_maker_results/prompt_maker_module_{module_idx + 1}.csv\", index=False)\n\n        # Evaluate the module's performance using the specified strategies\n        module_metrics = evaluate_module_performance(module_result, strategies)\n\n        # Save the module's evaluation metrics to a file\n        with open(f\"{node_line_dir}/prompt_maker_results/prompt_maker_module_{module_idx + 1}_metrics.json\", \"w\") as f:\n            json.dump(module_metrics, f)\n\n        # Update the best module if necessary\n        if best_module_idx is None or module_metrics[\"performance\"] > best_module_metrics[\"performance\"]:\n            best_module_idx = module_idx\n            best_module_result = module_result\n            best_module_metrics = module_metrics\n\n    # Combine the best module's result with the previous result\n    combined_result = pd.concat([previous_result, best_module_result])\n\n    # Save the combined result to a file\n    combined_result.to_csv(f\"{node_line_dir}/prompt_maker_results/combined_result.csv\", index=False)\n\n    # Save the best module's result to a file\n    best_module_"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'results'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'summaries'), exist_ok=True)\n\n    # Run each prompt maker module and evaluate its performance\n    results = []\n    for module, params in zip(modules, module_params):\n        # Run the module and get the result\n        result = module(**params)\n\n        # Evaluate the module's performance\n        if strategies.get('evaluation_metrics'):\n            evaluation_metrics = strategies.get('evaluation_metrics')\n            evaluation_result = evaluate_prompt_maker(result, evaluation_metrics)\n        else:\n            evaluation_result = {}\n\n        # Save the module's result and evaluation summary\n        result_file_path = os.path.join(node_line_dir, 'results', f'{module.__name__}.csv')\n        result.to_csv(result_file_path, index=False)\n\n        summary = {\n            'module_name': module.__name__,\n            'params': params,\n            'evaluation_metrics': evaluation_metrics,\n            'evaluation_result': evaluation_result,\n            'execution_time': execution_time,\n        }\n        summary_file_path = os.path.join(node_line_dir, 'summaries', f'{module.__name__}.json')\n        with open(summary_file_path, 'w') as f:\n            json.dump(summary, f)\n\n        # Add the module's result and evaluation summary to the list of results\n        results.append({\n            'module_name': module.__name__,\n            'params': params,\n            'evaluation_metrics': evaluation_metrics,\n            'evaluation_result': evaluation_result,\n            'execution_time': execution_time,\n            'result': result\n        })\n\n    # Select the best"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Initialize the result dataframe\n    result = pd.DataFrame()\n\n    # Initialize the generator module\n    generator_module = default_generator_module\n\n    # Initialize the generator module parameters\n    generator_module_params = default_generator_module_params\n\n    # Get the generator module from the strategies dictionary if it exists\n    if 'generator_module' in strategies:\n        generator_module = strategies['generator_module']\n\n    # Get the generator module parameters from the strategies dictionary if it exists\n    if 'generator_module_params' in strategies:\n        generator_module_params = strategies['generator_module_params']\n\n    # Create the necessary directories\n    create_necessary_directories(node_line_dir)\n\n    # Iterate through the modules and module parameters\n    for module, module_param in zip(modules, module_params):\n\n        # Get the module name\n        module_name = module.__name__\n\n        # Get the module parameters\n        module_params = module_param\n\n        # Run the module\n        module_result = module(module_params)\n\n        # Save the module result to a file\n        save_module_result_to_file(module_result, node_line_dir, module_name)\n\n        # Evaluate the module result using the generator module\n        module_result_evaluation = generator_module(module_result, generator_module_params)\n\n        # Save the module result evaluation to a file\n        save_module_result_evaluation_to_file(module_result_evaluation, node_line_dir, module_name)\n\n        # Append the module result to the result dataframe\n        result = pd.concat([result, module_result])\n\n        # Save the result dataframe to a file\n        save_result_to_file(result, node_line_dir)\n\n    # Combine the previous result and the best prompt maker's result\n    result = pd.concat([previous_result, result])\n\n    # Save the combined result to a file\n    save_result_to_file(result, node_line_dir)\n\n    # Return the combined result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # create necessary directories\n    for dir_name in [\"data\", \"log\", \"result\"]:\n        os.makedirs(os.path.join(node_line_dir, dir_name), exist_ok=True)\n\n    # run all prompt maker modules\n    result_list = []\n    for module, param in zip(modules, module_params):\n        result_list.append(module(**param))\n\n    # evaluate the performance of each prompt maker module\n    eval_result_list = []\n    for result, module, param in zip(result_list, modules, module_params):\n        eval_result_list.append(evaluate_prompt_maker_node(result, module, param, strategies))\n\n    # select the best prompt maker module based on the specified strategies\n    best_eval_result = select_best_prompt_maker(eval_result_list, strategies)\n\n    # save the best prompt maker module's output and evaluation results\n    save_prompt_maker_node(best_eval_result, node_line_dir)\n\n    # combine the best prompt maker module's output with the previous operation's results\n    combined_result = combine_prompt_maker_results(previous_result, best_eval_result)\n\n    return combined_result\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            for module_param in module.module_params:\n                if module_param.key == key:\n                    values.append(module_param.value)\n\n    return list(set(values))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n\n    for node in nodes:\n        for module in node.modules:\n            for module_param in module.module_params:\n                if module_param.key == key:\n                    values.append(module_param.value)\n\n    return list(set(values))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n\n    for node in nodes:\n        for module in node.modules:\n            for module_param in module.module_params:\n                if module_param.key == key:\n                    values.append(module_param.value)\n\n    return list(set(values))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        if node.module_params is not None:\n            for module_param in node.module_params:\n                if module_param.key == key:\n                    values.append(module_param.value)\n\n    return list(set(values))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            if module.module_params is not None:\n                for module_param in module.module_params:\n                    if module_param.key == key:\n                        values.append(module_param.value)\n    return list(set(values))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            if key in module.module_params:\n                values.append(module.module_params[key])\n\n    return list(set(values))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    # Extract values from nodes\n    values = []\n    for node in nodes:\n        if node.module_params:\n            for module_param in node.module_params:\n                if module_param.key == key:\n                    values.append(module_param.value)\n\n    # Remove duplicates\n    values = list(set(values))\n\n    return values"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n\n    for node in nodes:\n        for module_param in node.module_params:\n            if module_param.key == key:\n                values.append(module_param.value)\n\n    return list(set(values))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            for module_param in module.module_params:\n                if module_param.key == key:\n                    values.append(module_param.value)\n\n    return list(set(values))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module_param in node.module_params:\n            if module_param.key == key:\n                values.append(module_param.value)\n\n    return list(set(values))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            for module_param in module.module_params:\n                if module_param.key == key:\n                    values.append(module_param.value)\n    return list(set(values))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            for module_param in module.module_params:\n                if module_param.key == key:\n                    values.append(module_param.value)\n    return list(set(values))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        if node.module_params is not None:\n            for module_param in node.module_params:\n                if module_param.key == key:\n                    if module_param.value not in values:\n                        values.append(module_param.value)\n    return values\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            for module_param in module.module_params:\n                if module_param.key == key:\n                    values.append(module_param.value)\n    return list(set(values))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            for module_param in module.module_params:\n                if module_param.key == key:\n                    values.append(module_param.value)\n    return list(set(values))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            for module_param in module.module_params:\n                if module_param.key == key:\n                    values.append(module_param.value)\n    return list(set(values))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module_param in node.module_params:\n            if module_param.key == key:\n                values.append(module_param.value)\n    return list(set(values))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        if key in node.module_params:\n            values.append(node.module_params[key])\n\n    return list(set(values))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            for module_param in module.module_params:\n                if module_param.key == key:\n                    values.append(module_param.value)\n    return list(set(values))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            if module.name == \"module_params\":\n                for param in module.params:\n                    if param.name == key:\n                        values.append(param.value)\n    return list(set(values))\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n\n    pred_embedding = embedding_model.encode(pred)\n    gt_embeddings = embedding_model.encode(generation_gt)\n\n    cosine_similarities = cosine_similarity(pred_embedding, gt_embeddings)\n    return cosine_similarities.max()"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n    pred_embedding = embedding_model.encode(pred)\n    max_score = 0\n    for gt in generation_gt:\n        gt_embedding = embedding_model.encode(gt)\n        score = cosine_similarity(pred_embedding, gt_embedding)\n        if score > max_score:\n            max_score = score\n    return max_score"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    embedding_model = embedding_model or get_embedding_model()\n    pred_embedding = embedding_model.encode(pred)\n    gt_embeddings = embedding_model.encode(generation_gt)\n    return max(cosine_similarity([pred_embedding], gt_embeddings))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    # If no embedding model is provided, use the default model\n    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n\n    # Convert the input strings into embeddings\n    embeddings = embedding_model.encode(generation_gt)\n\n    # Compute the cosine similarity between the predicted string and each ground truth string\n    pred_embedding = embedding_model.encode(pred)\n    cosine_similarities = cosine_similarity(pred_embedding, embeddings)\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return cosine_similarities.max()"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n\n    # Compute embeddings for the ground truth strings\n    embeddings_gt = embedding_model.encode(generation_gt)\n\n    # Compute the embedding for the predicted string\n    embedding_pred = embedding_model.encode([pred])\n\n    # Compute cosine similarity between the predicted string and each ground truth string\n    cosine_similarities = cosine_similarity(embedding_pred, embeddings_gt)\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return cosine_similarities.max()"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n    embedding_model.eval()\n    with torch.no_grad():\n        pred_embedding = embedding_model.encode(pred)\n        gt_embeddings = embedding_model.encode(generation_gt)\n\n    return max([cosine_similarity(pred_embedding, gt_embedding) for gt_embedding in gt_embeddings])"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n\n    # Convert the ground truth strings into embeddings\n    generation_gt_embeddings = embedding_model.encode(generation_gt)\n\n    # Convert the predicted string into an embedding\n    pred_embedding = embedding_model.encode(pred)\n\n    # Compute the cosine similarity between the predicted embedding and the ground truth embeddings\n    cosine_similarities = cosine_similarity(pred_embedding.reshape(1, -1), generation_gt_embeddings)\n\n    # Find the maximum cosine similarity\n    max_cosine_similarity = np.max(cosine_similarities)\n\n    return max_cosine_similarity"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n\n    # Compute embeddings for the ground truth strings\n    ground_truth_embeddings = embedding_model.encode(generation_gt)\n\n    # Compute the embedding for the predicted string\n    pred_embedding = embedding_model.encode([pred])\n\n    # Compute cosine similarity between the predicted string and each ground truth string\n    cosine_similarities = cosine_similarity(pred_embedding, ground_truth_embeddings)\n\n    # Return the maximum cosine similarity\n    return np.max(cosine_similarities)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n\n    # Convert the prediction and ground truth strings into embeddings\n    pred_embedding = embedding_model.encode(pred)\n    gt_embeddings = embedding_model.encode(generation_gt)\n\n    # Compute the cosine similarity between the prediction and each ground truth string\n    cosine_similarities = cosine_similarity(pred_embedding.reshape(1, -1), gt_embeddings)\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return float(np.max(cosine_similarities))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    # convert ground truth to embeddings\n    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n    generation_gt_embeddings = embedding_model.encode(generation_gt)\n\n    # convert prediction to embedding\n    pred_embedding = embedding_model.encode([pred])\n\n    # calculate cosine similarity\n    cosine_similarities = cosine_similarity(pred_embedding, generation_gt_embeddings)\n\n    # return maximum cosine similarity\n    return np.max(cosine_similarities)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n\n    pred_emb = embedding_model.encode(pred)\n    gt_embs = embedding_model.encode(generation_gt)\n\n    return np.max([cosine_similarity([pred_emb], [gt_emb])[0][0] for gt_emb in gt_embs])\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    # Check if the embedding model is provided. If not, use a default model\n    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n\n    # Convert the input strings into embeddings\n    embedding_pred = embedding_model.encode(pred)\n    embedding_gt = embedding_model.encode(generation_gt)\n\n    # Calculate the cosine similarity between the predicted string and each ground truth string\n    cosine_sim = cosine_similarity(embedding_pred, embedding_gt)\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return cosine_sim.max()"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n\n    embedding_pred = embedding_model.encode(pred)\n    embedding_gt = embedding_model.encode(generation_gt)\n\n    # Compute cosine similarity\n    cosine_similarities = cosine_similarity([embedding_pred], embedding_gt)\n    max_cosine_similarity = np.max(cosine_similarities)\n\n    return max_cosine_similarity\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    # If no embedding model is provided, use the default model\n    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n\n    # Convert the ground truth strings and the predicted string into embeddings\n    embedding_gt = embedding_model.encode(generation_gt)\n    embedding_pred = embedding_model.encode([pred])\n\n    # Compute the cosine similarity between the predicted string and each ground truth string\n    cosine_sim = cosine_similarity(embedding_pred, embedding_gt)[0]\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return np.max(cosine_sim)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.encode(pred)\n    gt_embeddings = embedding_model.encode(generation_gt)\n\n    cosine_similarities = cosine_similarity(pred_embedding.reshape(1, -1), gt_embeddings)\n    return np.max(cosine_similarities)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n    embeddings_gt = embedding_model.encode(generation_gt)\n    embedding_pred = embedding_model.encode(pred)\n    return np.max([cosine_similarity([embedding_pred], [embedding])[0][0] for embedding in embeddings_gt])\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n    pred_embedding = embedding_model.encode(pred)\n    gt_embeddings = embedding_model.encode(generation_gt)\n    max_score = max([cosine_similarity([pred_embedding], [gt_emb])[0] for gt_emb in gt_embeddings])\n    return max_score\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    # Get the embedding model\n    if embedding_model is None:\n        embedding_model = get_embedding_model(model_name=\"all-mpnet-base-v2\")\n\n    # Convert the prediction into an embedding\n    pred_embedding = embedding_model.encode(pred)\n\n    # Convert the ground truth strings into embeddings\n    gt_embeddings = embedding_model.encode(generation_gt)\n\n    # Compute the cosine similarity between the prediction and each ground truth string\n    cosine_similarities = cosine_similarity(pred_embedding, gt_embeddings)\n\n    # Return the maximum cosine similarity\n    return max(cosine_similarities)\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    # Check if the embedding model is provided. If not, use the default model.\n    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n\n    # Convert the prediction string and ground truth strings to embeddings using the embedding model.\n    pred_embedding = embedding_model.encode(pred)\n    generation_gt_embedding = embedding_model.encode(generation_gt)\n\n    # Compute the cosine similarity between the prediction and ground truth embeddings.\n    cosine_similarity = np.dot(pred_embedding, generation_gt_embedding) / (np.linalg.norm(pred_embedding) * np.linalg.norm(generation_gt_embedding))\n\n    # Return the maximum cosine similarity as the semantic similarity score.\n    return np.max(cosine_similarity)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n\n    # Compute the embeddings for the ground truth strings and the predicted string\n    generation_gt_embeddings = embedding_model.encode(generation_gt)\n    pred_embedding = embedding_model.encode([pred])\n\n    # Compute the cosine similarity between the predicted string and each ground truth string\n    similarities = cosine_similarity(pred_embedding, generation_gt_embeddings)\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return similarities.max()\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    # Import the GFPGAN face restorer\n    try:\n        from gfpgan import GFPGANer\n    except ImportError:\n        logger.warning(\"GFPGAN face restorer not set up. Skipping face restoration.\")\n        return np_image\n\n    # Set up the GFPGAN face restorer\n    gfpgan = GFPGANer(\n        model_path=\"gfpgan/weights/GFPGANCleanv1-NoCE-C2.pth\",\n        upscale=2,\n        arch=\"clean\",\n        channel_multiplier=2,\n        bg_upsampler=None,\n    )\n\n    # Restore faces in the image\n    _, restored_faces, restored_img = gfpgan.enhance(\n        np_image,\n        paste_back=True,\n        weight=0.1,\n    )\n\n    # Return the restored image\n    return restored_img"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        from gfpgan import GFPGANer\n    except:\n        print(\"GFPGAN face restorer is not set up. Skipping face restoration.\")\n        return np_image\n\n    gfpganer = GFPGANer(\n        model_path='https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth',\n        upscale=2,\n        arch='clean',\n        channel_multiplier=2,\n        bg_upsampler=None)\n\n    _, _, restored_faces = gfpganer.enhance(np_image,\n                                            paste_back=True,\n                                            weight=0.1)\n\n    return restored_faces"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        from gfpgan import GFPGANer\n        from basicsr.utils import img2tensor, tensor2img\n\n        gfpganer = GFPGANer(\n            model_path=\"https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth\",\n            upscale=2,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n\n        np_image = img2tensor(np_image)\n        np_image = np_image.unsqueeze(0)\n        np_image = gfpganer.enhance(np_image, paste_back=True)\n        np_image = tensor2img(np_image)\n    except Exception as e:\n        print(\"GFPGAN face restoration failed:\", e)\n\n    return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    # Import the GFPGAN face restorer.\n    from gfpgan import GFPGANer\n\n    # Check if the GFPGAN face restorer is set up.\n    if GFPGANer is None:\n        print(\"GFPGANer is not set up. Skipping face restoration.\")\n        return np_image\n\n    # Set up the GFPGAN face restorer.\n    gfpganer = GFPGANer(\n        model_path=\"https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth\",\n        upscale=2,\n        arch=\"clean\",\n        channel_multiplier=2,\n        bg_upsampler=None,\n    )\n\n    # Restore faces in the image.\n    _, restored_image = gfpganer.enhance(np_image, paste_back=True)\n\n    # Return the image.\n    return restored_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        from gfpgan import GFPGANer\n        from basicsr.archs.rrdbnet_arch import RRDBNet\n        from realesrgan import RealESRGANer\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n        import torch\n\n        # Initialize the GFPGAN face restorer\n        gfpgan = GFPGANer(\n            model_path=\"https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth\",\n            upscale=2,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None)\n\n        # Initialize the Real-ESRGAN upscaler\n        sr_backend = 'realesrgan'\n        codec = 'video'\n        if sr_backend == 'realesrgan':\n            model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n        else:\n            model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=2, act_type='prelu')\n        upsampler = RealESRGANer(\n            scale=2,\n            model_path='https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth',\n            model=model,\n            tile_pad=10,\n            pre_pad=0,\n            half=True)\n\n        # Restore faces in the image\n        _, _, restored_img = gfpgan.enhance(np_image,\n                                            has_aligned=False,\n                                            only_center_face=False,\n                                            paste_back="}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        from gfpgan import GFPGANer\n        from basicsr.utils.download_util import load_file_from_url\n        import cv2\n        import numpy as np\n        import torch\n\n        # Load the GFPGAN face restorer\n        gfpgan_model_path = load_file_from_url(\n            url='https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth',\n            model_dir='gfpgan/weights',\n            progress=True,\n            file_name=None)\n        gfpgan = GFPGANer(\n            model_path=gfpgan_model_path,\n            upscale=2,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None)\n\n        # Restore faces in the image\n        _, _, restored_image = gfpgan.enhance(\n            np_image,\n            has_aligned=False,\n            only_center_face=False,\n            paste_back=True,\n            weight=0.1)\n        return restored_image\n\n    except Exception as e:\n        print(f'Error in gfpgan_fix_faces: {e}')\n        return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        import cv2\n        import numpy as np\n        import torch\n        from basicsr.archs.rrdbnet_arch import RRDBNet\n        from gfpgan import GFPGANer\n        from realesrgan import RealESRGANer\n\n        # Initialize the GFPGAN face restorer\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n        model_path = 'https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth'\n        upscale = 2\n        arch = 'rrdbnet_arch'\n        channel_multiplier = 2\n        model_path = model_path if model_path.startswith('http') else './experiments/pretrained_models/GFPGANCleanv1-NoCE-C2.pth'\n        bg_upsampler = None\n        model_path = model_path if model_path.startswith('http') else './experiments/pretrained_models/RealESRGAN_x4plus.pth'\n        upsampler = RealESRGANer(\n            scale=upscale,\n            model_path=model_path,\n            model='realesr-animevideov3',\n            tile=0,\n            tile_pad=10,\n            pre_pad=0,\n            half=True)\n        restorer = GFPGANer(\n            model,\n            model_path,\n            upscale,\n            arch,\n            channel_multiplier,\n            bg_upsampler)\n\n        # Restore faces in the image\n        _, _, restored_image = restorer.enhance(np_image, has_aligned=False, only_center_face=False, paste_back"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        from gfpgan import GFPGANer\n        from basicsr.utils.download_util import load_file_from_url\n        from basicsr.utils.img_util import img2tensor, imwrite, tensor2img\n\n        gfpganer = GFPGANer(\n            model_path=load_file_from_url(\n                \"https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth\"\n            ),\n            upscale=2,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n\n        _, _, height, width = np_image.shape\n        np_image = img2tensor(np_image, bgr2rgb=True, float32=True)\n        _, _, restored_image = gfpganer.enhance(\n            np_image,\n            paste_back=True,\n            weight=4,\n            transparent=False,\n        )\n        restored_image = tensor2img(restored_image)\n        return restored_image\n\n    except Exception as e:\n        print(f\"Error while restoring faces using GFPGAN: {e}\")\n        return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    import numpy as np\n    from PIL import Image\n    import cv2\n    import torch\n    import gfpgan\n    from basicsr.utils import img2tensor, tensor2img, imwrite\n\n    try:\n        # Load the GFPGAN face restorer\n        gfpgan_restorer = gfpgan.GFPGANer(\n            model_path=\"https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth\",\n            upscale=2,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None)\n\n        # Convert the image to a PIL image\n        pil_image = Image.fromarray(np_image)\n\n        # Convert the PIL image to a tensor\n        tensor_image = img2tensor(pil_image)\n\n        # Perform face restoration\n        _, _, tensor_restored = gfpgan_restorer.enhance(tensor_image,\n                                                        paste_back=True,\n                                                        weight_path=None)\n\n        # Convert the tensor back to a NumPy array\n        np_restored = tensor2img(tensor_restored)\n\n        # Return the restored image\n        return np_restored\n\n    except Exception as e:\n        print(f\"Error restoring faces: {e}\")\n        return np_image\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    # Import the necessary libraries\n    import numpy as np\n    import torch\n    from basicsr.utils import img2tensor, tensor2img, imwrite\n    from gfpgan import GFPGANer\n    from gfpgan.archs.arch_util import GFPGANv1Generator\n    from gfpgan.archs.arch_util import GFPGANStyleGANv1Generator\n\n    # If the GFPGAN face restorer is not set up, log a warning and return the original image\n    if not gfpgan_restorer.is_initialized:\n        logger.warning(\"GFPGAN face restorer is not initialized. Skipping face restoration.\")\n        return np_image\n\n    # Convert the NumPy image to a PyTorch tensor\n    image_tensor = img2tensor(np_image)\n\n    # Restore the faces in the image using the GFPGAN face restorer\n    restored_image_tensor = gfpgan_restorer.enhance(image_tensor, paste_back=True)\n\n    # Convert the restored image tensor back to a NumPy array\n    restored_image_np = tensor2img(restored_image_tensor)\n\n    # Return the restored image as a NumPy array\n    return restored_image_np"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        import gfpgan\n        import numpy as np\n        from basicsr.archs.rrdbnet_arch import RRDBNet\n        from realesrgan import RealESRGANer\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n\n        model_path = 'https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth'\n        bg_upsampler = None\n        device = 'cuda'\n\n        # initialize the GFPGAN face restorer\n        gfpgan_face_restorer = gfpgan.GFPGAN(\n            model_path=model_path,\n            upscale=2,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=bg_upsampler\n        )\n\n        # initialize the RealESRGAN upscaler\n        sr_backend = 'ncnn'\n        codec = 'video'\n        model_path = 'RealESRGAN_x4plus.pth'\n        upsampler = RealESRGANer(\n            scale=4,\n            model_path=model_path,\n            model='realesr-animevideov3',\n            tile=0,\n            tile_pad=10,\n            pre_pad=0,\n            half=True,\n        )\n\n        # restore faces in the image\n        _, _, restored_faces, restored_img = gfpgan_face_restorer.enhance(np_image, has_aligned=False, only_center_face=False, paste_back=True, weight=0.1)\n\n        # return the restored image\n        return restored_img\n    except Exception as e:\n        # log the error\n        print(f'Error: {e}')\n\n        # return the original image\n        return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        from gfpgan import GFPGANer\n        from basicsr.archs.rrdbnet_arch import RRDBNet\n        from realesrgan import RealESRGANer\n        from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n        from gfpgan.utils.face_restorer import FaceRestorer\n        from gfpgan.utils.restore_face import RestoreFace\n\n        gfpganer = GFPGANer(\n            det_model=\"gfpgan/weights/parsing_sim.pth\",\n            sr_backbone=RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=1),\n            codec=\"pt\",\n            upscale=4,\n            suffix=\"RestoreFormer\",\n            bg_upsampler=RealESRGANer(\n                scale=2,\n                model_path=\"realesr-general-x4v3/RealESRGAN_x4plus_anime_6B.pth\",\n                model=SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=4, act_type=\"prelu\"),\n                tile_pad=10,\n                pre_pad=0,\n                half=True,\n            ),\n        )\n        face_restorer = FaceRestorer(gfpganer, sr_scale=4)\n\n        return face_restorer.enhance(np_image)\n    except Exception as e:\n        print(f\"Error in gfpgan_fix_faces: {e}\")\n        return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    import numpy as np\n    import logging\n    import cv2\n    from gfpgan import GFPGANer\n    from basicsr.utils import img2tensor, tensor2img, imwrite\n\n    # Initialize the GFPGAN face restorer if it is not set up\n    if not hasattr(gfpgan_fix_faces, 'gfpganer'):\n        try:\n            gfpgan_fix_faces.gfpganer = GFPGANer(\n                model_path='gfpgan/experiments/pretrained_models/GFPGANCleanv1-NoCE-C2.pth',\n                upscale=2,\n                arch='clean',\n                channel_multiplier=2,\n                bg_upsampler=None\n            )\n        except Exception as e:\n            logging.warning(f'Failed to initialize GFPGAN face restorer: {e}')\n            return np_image\n\n    # Convert the image to a PIL image\n    pil_image = cv2.cvtColor(np_image, cv2.COLOR_BGR2RGB)\n\n    # Restore the faces in the image\n    try:\n        _, _, output = gfpgan_fix_faces.gfpganer.enhance(pil_image, paste_back=True)\n    except Exception as e:\n        logging.warning(f'Failed to restore faces in image: {e}')\n        return np_image\n\n    # Convert the restored image to a NumPy array\n    np_image = np.array(output)\n\n    # Return the restored image\n    return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        import gfpgan\n        from basicsr.archs.rrdbnet_arch import RRDBNet\n        import torch\n        from basicsr.utils.download_util import load_file_from_url\n        from basicsr.utils.img_util import img2tensor, tensor2img\n        from basicsr.utils.registry import ARCH_REGISTRY\n        from basicsr.utils.download_util import load_file_from_url\n        from basicsr.utils.download_util import load_file_from_url\n        from basicsr.utils.download_util import load_file_from_url\n\n        ARCH_REGISTRY.register_module(RRDBNet)\n\n        device = torch.device('cuda')\n        gfpgan_model = gfpgan.archs.GFPGANv1(\n            out_size=(np_image.shape[0], np_image.shape[1]),\n            num_style_feat=512,\n            channel_multiplier=2,\n            decoder_load_path=load_file_from_url(\n                'https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth',\n                progress=True,\n                path='gfpgan/weights/GFPGANCleanv1-NoCE-C2.pth',\n                backend='requests',\n                rank=0,\n            ),\n            encoder_load_path=load_file_from_url(\n                'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/encoder-512-simple-79043421.pth',\n                progress=True,\n                path='gfpgan/weights/encoder-512-simple-79043421.pth',\n                backend='requests',\n                rank=0,\n            ),\n        )\n\n        gfpgan_model.eval()\n       "}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    # Import the GFPGAN face restorer and the required libraries\n    import cv2\n    import numpy as np\n    import os\n    from basicsr.archs.rrdbnet_arch import RRDBNet\n    from realesrgan import RealESRGANer\n    from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n    from gfpgan import GFPGANer\n    import warnings\n\n    # Check if the GFPGAN face restorer is set up\n    if os.path.exists(\"gfpgan/experiments/pretrained_models/GFPGANCleanv1-NoCE-C2.pth\"):\n        # Initialize the GFPGAN face restorer\n        gfpgan = GFPGANer(\n            model_path=\"gfpgan/experiments/pretrained_models/GFPGANCleanv1-NoCE-C2.pth\",\n            upscale=2,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None\n        )\n\n        # Restore faces in the image\n        _, _, restored_img = gfpgan.enhance(np_image, paste_back=True, weight=0.1)\n\n        # Return the restored image\n        return restored_img\n\n    # Log a warning if the GFPGAN face restorer is not set up\n    else:\n        warnings.warn(\"GFPGAN face restorer not set up. Skipping face restoration.\")\n\n        # Return the original image\n        return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    import numpy as np\n    import cv2\n    from gfpgan import GFPGANer\n    from basicsr.archs.rrdbnet_arch import RRDBNet\n    from realesrgan import RealESRGANer\n\n    # Load the GFPGAN face restorer\n    try:\n        gfpgan = GFPGANer(\n            model_path='gfpgan/experiments/pretrained_models/GFPGANCleanv1-NoCE-C2.pth',\n            upscale=2,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=RealESRGANer(\n                scale=2,\n                model_path='realesrgan/experiments/pretrained_models/RealESRGAN_x2plus.pth',\n                model=RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2),\n            )\n        )\n    except Exception as e:\n        print(f'GFPGAN face restorer could not be loaded: {e}')\n        return np_image\n\n    # Restore faces in the image\n    try:\n        _, _, restored_faces, restored_masks = gfpgan.enhance(np_image, paste_back=True, align=True)\n        restored_faces = restored_faces.astype(np.uint8)\n        restored_masks = restored_masks.astype(np.uint8)\n        restored_faces = cv2.cvtColor(restored_faces, cv2.COLOR_RGB2BGR)\n        restored_masks = cv2.cvtColor(restored_masks, cv2.COLOR_RGB2BGR)\n    except Exception as e:\n        print(f'GFPGAN face restorer could not be used: {e}')\n        return np_image\n\n    # Return the restored image\n    return restored_faces\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        import gfpgan\n        import numpy as np\n        import cv2\n        import torch\n        from basicsr.archs.rrdbnet_arch import RRDBNet\n        from realesrgan import RealESRGANer\n\n        gfpgan_model = gfpgan.GFPGAN(\n            det_model=\"weights/gfpgan/detection_model.pth\",\n            upscale=4,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        gfpgan_model.to(device)\n        gfpgan_model.eval()\n        gfpgan_model.first_run = False\n\n        # Load RealESRGAN model\n        model = RRDBNet(\n            num_in_ch=3,\n            num_out_ch=3,\n            num_feat=64,\n            num_block=23,\n            num_grow_ch=32,\n            scale=4,\n        )\n        model_path = \"weights/realesrgan/RealESRGAN_x4plus.pth\"\n        upsampler = RealESRGANer(\n            scale=4,\n            model_path=model_path,\n            model=model,\n            tile=0,\n            tile_pad=10,\n            pre_pad=0,\n            half=True,\n        )\n\n        # Run GFPGAN face restoration\n        _, _, restored_faces = gfpgan_model.enhance(\n            np_image,\n            has_aligned=False,\n            only_center_face=False,\n            paste_back=True,\n            weight=4.0,\n        )\n\n        # Upsample the image\n        restored_faces = upsampler.enhance(restored_faces, outscale=4)[0]\n\n        # Convert the image to a NumPy array\n        np_image = restored_faces.astype"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        from gfpgan import GFPGANer\n        from basicsr.utils import img2tensor, tensor2img\n        import cv2\n        import numpy as np\n        from PIL import Image\n        import torch\n        import warnings\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n\n        # Initialize the GFPGAN face restorer\n        model_path = \"https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth\"\n        gfpgan = GFPGANer(\n            model_path=model_path,\n            upscale=2,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None)\n\n        # Restore faces in the image\n        _, _, restored_faces = gfpgan.enhance(\n            img2tensor(np_image),\n            paste_back=True,\n            weight=0.1)\n\n        # Convert the restored faces back to a NumPy array\n        restored_faces = tensor2img(restored_faces)\n\n        # Convert the restored faces to a PIL image\n        restored_faces = Image.fromarray(restored_faces)\n\n        # Convert the PIL image to a NumPy array\n        restored_faces = np.array(restored_faces)\n\n        # Convert the NumPy array to a BGR image\n        restored_faces = cv2.cvtColor(restored_faces, cv2.COLOR_RGB2BGR)\n\n        # Return the restored faces\n        return restored_faces\n    except:\n        # If the GFPGAN face restorer is not set up, log a warning and return the original image\n        print(\"GFPGAN face restorer not set up. Returning original image.\")\n        return np_image\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    import numpy as np\n    import warnings\n    from PIL import Image\n    import cv2\n\n    # Check if GFPGAN face restorer is set up\n    try:\n        from basicsr.archs.rrdbnet_arch import RRDBNet\n        from realesrgan import RealESRGANer\n    except ImportError:\n        warnings.warn(\"GFPGAN face restorer is not set up. Skipping face restoration.\")\n        return np_image\n\n    # Set up GFPGAN face restorer\n    gfpgan = None\n    try:\n        gfpgan = GFPGANer(\n            discriminator=dict(type='PConvDiscriminator'),\n            channel_multiplier=2,\n            bg_upsampler=dict(type='nearest', scale=2)\n        )\n    except Exception as e:\n        warnings.warn(\"GFPGAN face restorer is not set up. Skipping face restoration.\")\n        return np_image\n\n    # Restore faces in the image\n    try:\n        # Convert the NumPy array to a PIL image\n        image = Image.fromarray(np_image)\n\n        # Convert the PIL image to a cv2 image\n        cv2_image = np.array(image)\n\n        # Restore faces in the cv2 image\n        restored_cv2_image = gfpgan.enhance(cv2_image, paste_back=True)\n\n        # Convert the cv2 image to a PIL image\n        restored_image = Image.fromarray(restored_cv2_image)\n\n        # Convert the PIL image to a NumPy array\n        restored_np_image = np.array(restored_image)\n\n        # Return the restored image\n        return restored_np_image\n    except Exception as e:\n        warnings.warn(\"GFPGAN face restorer is not set up. Skipping face restoration.\")\n        return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    import numpy as np\n    import cv2\n    import torch\n    from basicsr.archs.rrdbnet_arch import RRDBNet\n    from realesrgan import RealESRGANer\n    from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n    from gfpgan import GFPGANer\n    from gfpgan.archs.stylegan2_arch import Discriminator\n    from gfpgan.archs.stylegan2_arch import Generator\n    from gfpgan.utils.ops import resize_tensor\n\n    # Check if the GFPGAN face restorer is set up\n    if not hasattr(gfpgan_face_restorer, \"restorer\"):\n        # Log a warning and return the original image\n        print(\"GFPGAN face restorer is not set up. Skipping face restoration.\")\n        return np_image\n\n    # Convert the image to a PyTorch tensor\n    image = torch.from_numpy(np_image).unsqueeze(0).permute(0, 3, 1, 2).float()\n\n    # Resize the image to the size required by the GFPGAN face restorer\n    _, _, h, w = image.shape\n    h_pad = (h // 8 + 1) * 8 - h\n    w_pad = (w // 8 + 1) * 8 - w\n    if h_pad > 0 or w_pad > 0:\n        image = torch.nn.functional.pad(image, (0, w_pad, 0, h_pad), \"reflect\")\n\n    # Use the GFPGAN face restorer to restore faces in the image\n    try:\n        with torch.no_grad():\n            output, _ = gfpgan_face_restorer.restorer.enhance(image, has_aligned=False, only_center_face=False, paste_back=True)\n    except RuntimeError as e:\n        # Log a warning and return the original image\n        print(\"GFPGAN face restorer failed:\", e)\n        return np_"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from face_restorer_codeformer import FaceRestorerCodeFormer\n        global face_restorers\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        print(e)"}
{"namespace": "codeformer_model.setup_model", "completion": "    from face_restorer.codeformer import FaceRestorerCodeFormer\n    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        print(e)"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from face_restorer.face_restorer_codeformer import FaceRestorerCodeFormer\n        from face_restorer.face_restorer_codeformer import face_restorer_codeformer_list\n        face_restorer_codeformer_list.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        print(e)"}
{"namespace": "codeformer_model.setup_model", "completion": "    from .face_restorer_codeformer import FaceRestorerCodeFormer\n\n    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        print(f\"Error setting up model: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from facexlib.detectors import FaceBoxes\n        from facexlib.detectors import RetinaFace\n        from facexlib.detectors import S3FD\n        from facexlib.detectors import MTCNN\n        from facexlib.detectors import PS3FD\n        from facexlib.detectors import PFLD\n        from facexlib.detectors import FaceDetector\n        from facexlib.detectors import FaceDetector2\n        from facexlib.detectors import FaceDetector3\n        from facexlib.detectors import FaceDetector4\n        from facexlib.detectors import FaceDetector5\n        from facexlib.detectors import FaceDetector6\n        from facexlib.detectors import FaceDetector7\n        from facexlib.detectors import FaceDetector8\n        from facexlib.detectors import FaceDetector9\n        from facexlib.detectors import FaceDetector10\n        from facexlib.detectors import FaceDetector11\n        from facexlib.detectors import FaceDetector12\n        from facexlib.detectors import FaceDetector13\n        from facexlib.detectors import FaceDetector14\n        from facexlib.detectors import FaceDetector15\n        from facexlib.detectors import FaceDetector16\n        from facexlib.detectors import FaceDetector17\n        from facexlib.detectors import FaceDetector18\n        from facexlib.detectors import FaceDetector19\n        from facexlib.detectors import FaceDetector20\n        from facexlib.detectors import FaceDetector21\n        from facexlib.detectors import FaceDetector22\n        from facexlib.detectors import FaceDetector23\n        from facexlib.detectors import FaceDetector24\n        from facexlib.detectors import FaceDetector25\n        from facexlib.detectors import FaceDetector26\n        from facex"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from facexlib.restorer import FaceRestorer\n        face_restorer = FaceRestorer(dirname, device='cuda')\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        print('Failed to initialize FaceRestorerCodeFormer:', e)"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from facexlib.code_formers import FaceRestorerCodeFormer\n\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        print(e)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    from face_restorer.codeformer import CodeFormer\n\n    try:\n        global face_restorers\n        face_restorers.append(CodeFormer(dirname))\n    except Exception as e:\n        print(f\"Error setting up CodeFormer: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    import os\n    import sys\n    import torch\n    from facexlib.utils.face_restorer_codeformer import FaceRestorerCodeFormer\n    from facexlib.utils.face_restorer_codeformer import face_restorers\n\n    try:\n        model = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(model)\n    except Exception as e:\n        print(f'{e}')\n        sys.exit(1)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n\n        from .face_restorer_codeformer import FaceRestorerCodeFormer\n\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n\n    except Exception as e:\n\n        print(f\"Failed to setup model {dirname}\")\n        print(e)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from face_restorer import FaceRestorerCodeFormer\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        print(f\"Error setting up model: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from facexlib.code_formers.face_restorer_codeformer import FaceRestorerCodeFormer\n        from facexlib.code_formers.face_restorer_codeformer_utils import CodeFormerConfig\n        from facexlib.code_formers.face_restorer_codeformer_utils import CodeFormerData\n        from facexlib.code_formers.face_restorer_codeformer_utils import CodeFormerDataLoader\n        from facexlib.code_formers.face_restorer_codeformer_utils import CodeFormerModel\n        from facexlib.code_formers.face_restorer_codeformer_utils import CodeFormerUtils\n        from facexlib.code_formers.face_restorer_codeformer_utils import CodeFormerUtils as CFU\n        from facexlib.code_formers.face_restorer_codeformer_utils import get_device\n        from facexlib.code_formers.face_restorer_codeformer_utils import get_device_id\n        from facexlib.code_formers.face_restorer_codeformer_utils import get_face_aligner\n        from facexlib.code_formers.face_restorer_codeformer_utils import get_face_detector\n        from facexlib.code_formers.face_restorer_codeformer_utils import get_face_detector_2\n        from facexlib.code_formers.face_restorer_codeformer_utils import get_face_embedder\n        from facexlib.code_formers.face_restorer_codeformer_utils import get_face_embedder_2\n        from facexlib.code_formers.face_restorer_codeformer_utils import get_face_embedder_3\n        from facexlib.code_formers.face_restorer_codeformer_utils import get_face_embedder_4\n        from facexlib.code_formers.face_restorer_codeformer_utils import get_face_embedder_5\n        from facexlib.code_formers.face_restorer_codeformer_utils import get"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorer_list.append(face_restorer)\n    except Exception as e:\n        print(e)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        print(f\"Failed to setup face restorer {dirname}: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from facexlib.codeformer import FaceRestorerCodeFormer\n\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        print(f\"Failed to setup model: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from face_restorer import FaceRestorerCodeFormer\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        print(f\"Error setting up model {dirname}: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from facexlib.code_formers.face_restorer_codeformer import FaceRestorerCodeFormer\n        from facexlib.code_formers.face_restorer_codeformer_config import CodeFormerConfig\n\n        cfg = CodeFormerConfig()\n        cfg.update(dirname)\n        face_restorer = FaceRestorerCodeFormer(cfg)\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        print(f\"Error in setup_model: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    from facexlib.code_formers.face_restorer import FaceRestorerCodeFormer\n\n    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n        print(f\"Successfully setup face restorer: {dirname}\")\n    except Exception as e:\n        print(f\"Failed to setup face restorer: {dirname}\")\n        print(e)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n\n        from facexlib.code_formers import FaceRestorerCodeFormer\n        from facexlib.code_formers.code_former_utils import code_former_models\n\n        model_path = code_former_models[dirname]\n        face_restorer = FaceRestorerCodeFormer(model_path)\n\n        face_restorers.append(face_restorer)\n\n    except Exception as e:\n        print(f\"Error in setup_model: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    import os\n    import sys\n    from face_restorer import FaceRestorerCodeFormer\n    from face_restorer.utils.options import Options\n    from face_restorer.utils.logger import setup_logger\n\n    global face_restorers\n\n    # setup logger\n    logger = setup_logger()\n\n    # setup model\n    if dirname is not None:\n        if os.path.isdir(dirname):\n            # load config\n            config_path = os.path.join(dirname, 'config.yaml')\n            if not os.path.isfile(config_path):\n                logger.error(f'Config file not found: {config_path}')\n                sys.exit(1)\n            opt = Options(config_path)\n            # setup face restorer\n            face_restorer = FaceRestorerCodeFormer(opt)\n            # load pretrained model\n            pretrained_path = os.path.join(dirname, 'model.pth')\n            if not os.path.isfile(pretrained_path):\n                logger.error(f'Pretrained model not found: {pretrained_path}')\n                sys.exit(1)\n            face_restorer.load_state_dict(torch.load(pretrained_path, map_location='cpu'))\n            face_restorers.append(face_restorer)\n        else:\n            logger.error(f'Directory not found: {dirname}')\n            sys.exit(1)\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        from facexlib.utils.face_restorer import FaceRestorer\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRest"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        from facexlib.utils.face_restorer import FaceRestorer\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as FRH\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper as"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import os\n    import sys\n    from facexlib.utils.face_restorer import FaceRestorer\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n    from facexlib.utils.face_restorer_"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import sys\n    import os\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_restorer_helper import FaceRestoreHelper\n    from facexlib.utils.face_"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        import facexlib\n        import sys\n        import os\n        import cv2\n        import numpy as np\n        from facexlib.detection import FaceDetector\n        from facexlib.utils import load_file_from_url\n        from facexlib.utils.face_restorer import FaceRestorer\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n       "}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        import facexlib\n        from facexlib.detection import RetinaFace\n        from facexlib.utils import load_file_from_url\n        from facexlib.utils.face_restorer import GFPGANer\n\n        facexlib.set_face_detector(RetinaFace())\n\n        model_url = 'https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth'\n        model_path = load_file_from_url(url=model_url, model_dir=dirname, progress=True, file_name=None)\n        face_restorer = GFPGANer(\n            upscale=2,\n            model_path=model_path,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None\n        )\n        facexlib.set_face_restorer(face_restorer)\n    except Exception as e:\n        print(f'Error setting up model: {e}')"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import sys\n    import os\n    import facexlib.detection.scrfd\n    from facexlib.detection.scrfd import SCRFD\n    from facexlib.detection.scrfd import SCRFDConfig\n    from facexlib.detection.scrfd import SCRFDDetector\n    from facexlib.detection.scrfd import SCRFDModel\n    from facexlib.detection.scrfd import SCRFDPredictor\n    from facexlib.detection.scrfd import SCRFDPostProcess\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFaceList\n    from facexlib.detection.scrfd import SCRFDFaceList\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection.scrfd import SCRFDFace\n    from facexlib.detection."}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        import sys\n        sys.path.append(dirname)\n\n        from facexlib.utils import load_file_from_url\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n\n        FaceRestorerHelper.init_restorer(\n            load_file_from_url(\n                \"https://github.com/xinntao/facexlib/releases/download/v0.1.0/GFPGANv1.3.pth\",\n                model_dir=dirname,\n                progress=True,\n                file_name=None\n            )\n        )\n    except Exception as e:\n        print(\"GFPGAN setup error: \", e)"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n\n        from facexlib.utils.face_restorer import FaceRestorer\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n\n        from gfpgan.utils.restore_face import RestoreFace\n\n        FaceRestorerHelper.patch_facexlib(dirname)\n\n        restorer = RestoreFace(\n            upscale=2,\n            model_path=f\"{dirname}/GFPGAN/GFPGANCleanv1-NoCE-C2.pth\",\n            bg_upsampler=None,\n            device=\"cuda\",\n            tile_pad=10,\n            tile_size=512,\n            pre_pad=0,\n            half=True,\n        )\n\n        FaceRestorer.register_restorer(\"gfpgan\", restorer)\n\n    except Exception as e:\n        print(f\"Failed to setup GFPGAN: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import os\n    import sys\n    import traceback\n\n    from facexlib.detection import RetinaFace\n    from facexlib.utils.face_restorer import GFPGANer\n\n    try:\n        sys.path.append(os.path.join(dirname, \"facexlib\"))\n        RetinaFace.setup_keys(dirname)\n        GFPGANer.setup_gfpgan(dirname)\n    except Exception:\n        traceback.print_exc()\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        import sys\n        import os\n        sys.path.append(dirname)\n        import facexlib\n        facexlib.DIR_MODELS = dirname\n        from facexlib.detection import RetinaFace\n        from facexlib.utils import load_file_from_url\n        from facexlib.utils import load_pretrained_pytorch_weights\n        from facexlib.utils import load_pretrained_torch_weights\n        from facexlib.utils import load_pretrained_tf_weights\n        from facexlib.utils import load_pretrained_tf_weights_from_url\n        from facexlib.utils import load_pretrained_weights\n        from facexlib.utils import load_pretrained_weights_from_url\n        from facexlib.utils import load_pretrained_weights_from_url_with_name\n        from facexlib.utils import load_pretrained_weights_from_url_with_name_and_ext\n        from facexlib.utils import load_pretrained_weights_from_url_with_name_and_ext_and_key\n        from facexlib.utils import load_pretrained_weights_from_url_with_name_and_ext_and_key_and_name\n        from facexlib.utils import load_pretrained_weights_from_url_with_name_and_ext_and_key_and_name_and_ext\n        from facexlib.utils import load_pretrained_weights_from_url_with_name_and_ext_and_key_and_name_and_ext_and_key\n        from facexlib.utils import load_pretrained_weights_from_url_with_name_and_ext_and_key_and_name_and_ext_and_key_and_name\n        from facexlib.utils import load_pretrained_weights_from_url_with_name_and_ext_and_key_and_name_and_ext_and_key_and_name_and_ext\n        from facexlib.utils import"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        import sys\n        import os\n        import facexlib\n        import cv2\n        import numpy as np\n        from facexlib.utils.face_restorer import FaceRestorer\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n        from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n\n        sys.path.append(dirname)\n        from gfpgan import GFPGANer\n\n        facexlib.set_face_restorer(FaceRestorer())\n        facexlib.set_face_restorer_helper(FaceRestorerHelper())\n        facexlib.set_face_restorer_helper(FaceRestorerHelper())\n\n        gfpganer = GFPGANer(\n            upscale=2,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n            model_path=os.path.join(dirname, \"experiments/pretrained_models/GFPGANCleanv1-NoCE-C2.pth\"),\n            bg_upsampler_dir=\"\",\n        )\n        facexlib.set_face_restorer(gfpganer)\n    except Exception as e:\n        print(e)"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n\n        import facexlib\n        import sys\n        import os\n\n        sys.path.append(dirname)\n\n        facexlib.initialize()\n\n        from gfpgan import GFPGANer\n\n        gfpganer = GFPGANer(\n            model_path=os.path.join(dirname, \"GFPGANv1.3.pth\"),\n            upscale=2,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n\n        gfpganer.face_restorer.face_helper.face_detector.model_path = os.path.join(\n            dirname, \"detection_Resnet50_Final.pth\"\n        )\n\n        gfpganer.face_restorer.face_helper.face_detector.det_threshold = 0.5\n        gfpganer.face_restorer.face_helper.face_detector.device = \"cuda\"\n\n    except Exception as e:\n        print(f\"Error setting up model: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n\n        import sys\n        sys.path.append(dirname)\n        from gfpgan import GFPGANer\n        from basicsr.utils.download_util import load_file_from_url\n        from basicsr.utils.registry import MODEL_REGISTRY\n\n        @MODEL_REGISTRY.register_module()\n        class GFPGANClean(GFPGANer):\n\n            \"\"\"\n            A subclass of GFPGANer that overrides the _load_pretrained_model method to load the model from a specified directory.\n\n            Input-Output Arguments\n            :param dirname: str, The directory path where the GFPGAN model is located. This directory is used to load the model.\n            :return: No return values.\n            \"\"\"\n\n            def _load_pretrained_model(self, path):\n                param_dict = load_file_from_url(\n                    url=f'https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANCleanv1-NoCE-C2.pth',\n                    model_dir=path,\n                    progress=True,\n                    file_name=None)\n                self.load_state_dict(param_dict, strict=True)\n\n        GFPGANClean(\n            num_style_feat=512,\n            num_mlp=8,\n            channel_multiplier=2,\n            restorator_path=dirname,\n            fix_backend=True,\n            backend='ncnn',\n            verbose=False,\n            device='cuda'\n        )\n\n    except Exception as e:\n        print(e)"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import os\n    import sys\n    import torch\n    import facexlib\n    from facexlib.detection import build_detector\n    from facexlib.utils.face_restorer import FaceRestorer\n\n    try:\n        # Patch facexlib\n        sys.path.append(dirname)\n        facexlib.utils.face_restorer.FaceRestorer = FaceRestorer\n\n        # Init face restorer\n        face_detector = build_detector(\n            name='RetinaFace',\n            opt={\n                'det_name': 'Resnet',\n                'det_backbone': 'ResNet',\n                'det_head': 'MobileFaceNet',\n                'load_path': os.path.join(dirname, 'weights/det_Resnet50_Final.pth'),\n                'input_size': 640,\n                'vis_thresh': 0.5,\n                'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n            }\n        )\n        face_restorer = FaceRestorer(\n            name='GFPGAN',\n            opt={\n                'model_path': os.path.join(dirname, 'experiments/pretrained_models/GFPGANCleanv1-NoCE-C2.pth'),\n                'bg_upsampler': None,\n                'face_shrink': 0.25,\n                'face_threshold': 0.7,\n                'half_precision': False,\n                'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n            }\n        )\n        face_restorer.face_detector = face_detector\n\n        print('GFPGAN model setup successfully.')\n    except Exception as e:\n        print(f'Error setting up GFPGAN model: {e}')\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        import facexlib\n        import facexlib.detection.face_detection\n        import facexlib.detection.face_restoration\n        import facexlib.utils.face_restoration_helper\n        import gfpgan\n        import gfpgan.archs\n        import gfpgan.data\n        import gfpgan.utils\n\n        facexlib.detection.face_detection.FaceDetector = facexlib.detection.face_detection.build_detector(\n            name=\"RetinaFace\",\n            max_resolution=1024,\n            device=\"cuda\",\n            run_mode=\"python\",\n            det_model_dir=f\"{dirname}/models/detection_Resnet50_Final.pth\",\n            run_times=10,\n            return_heatmap=False,\n            box_score_thresh=0.9,\n            box_top_k=-1,\n            box_predictor_path=f\"{dirname}/models/detection_res50_pfpld.pth\",\n        )\n\n        facexlib.detection.face_restoration.FaceRestoreModel = facexlib.detection.face_restoration.build_face_restorer(\n            name=\"GFPGAN\",\n            model_path=f\"{dirname}/models/GFPGANCleanv1-NoCE-C2.pth\",\n            upscale=2,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n\n        facexlib.utils.face_restoration_helper.FaceRestoreHelper = facexlib.utils.face_restoration_helper.FaceRestoreHelper(\n            face_size=512,\n            face_restorer=facexlib.detection.face_restoration.FaceRestoreModel,\n            matting=None,\n            upscale=2,\n            device=\"cuda\",\n            bg_upsampler=None,\n        )\n\n        gfpgan.archs.arch_util.GF"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import os\n    import sys\n    import torch\n    from facexlib.utils.face_restorer import FaceRestorer\n    from gfpgan import GFPGANer\n    from basicsr.utils import scandir\n\n    try:\n        sys.path.append(\"./\")\n        sys.path.append(os.path.abspath(os.path.join(os.path.abspath(\"./\"), \"../\")))\n        sys.path.append(os.path.abspath(os.path.join(os.path.abspath(\"./\"), \"../../\")))\n        sys.path.append(os.path.abspath(os.path.join(os.path.abspath(\"./\"), \"../../facexlib\")))\n        from facexlib.utils import setup_facexlib\n        setup_facexlib(dirname)\n    except Exception as e:\n        print(e)\n        print(\"Please install facexlib first: \\n\\thttps://github.com/xinntao/facexlib\")\n        sys.exit(1)\n\n    try:\n        model_path = os.path.join(dirname, \"GFPGANv1.4.pth\")\n        model_path = scandir(model_path, (\"pth\",), recursive=True)[0]\n        model = GFPGANer(\n            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n            scale=2,\n            model_path=model_path,\n            upscale=2,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n        FaceRestorer(model, 1024, \"cuda\")\n    except Exception as e:\n        print(e)\n        print(\"Please download the model from https://github.com/TencentARC/GFPGAN/releases and save it to experiments/pretrained_models/GFPGAN/GFPGANv1.4.pth\")\n        sys.exit(1)\n\n    print"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import os\n    import sys\n    import torch\n    from facexlib.detection import FaceXLib\n    from facexlib.utils import load_file_from_url\n    from gfpgan import GFPGANer\n\n    try:\n        # Patch facexlib\n        sys.path.append(dirname)\n        face_detector = FaceXLib()\n        face_detector.detect_from_patched_model()\n\n        # Initialize the GFPGAN face restorer\n        model_name = \"GFPGANv1.3\"\n        model_path = os.path.join(dirname, \"experiments/pretrained_models\", model_name + \".pth\")\n        if not os.path.isfile(model_path):\n            load_file_from_url(\n                url=\"https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANv1.3.pth\",\n                model_dir=dirname + \"/experiments/pretrained_models\",\n                model_name=model_name + \".pth\",\n                progress=True,\n                file_name=None\n            )\n        restorer = GFPGANer(\n            model_path=model_path,\n            upscale=2,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None\n        )\n        if torch.cuda.is_available():\n            restorer.cuda()\n\n    except Exception as e:\n        print(e)\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        from facexlib.detection import RetinaFace\n        from facexlib.utils.face_restorer import FaceRestorer\n        import facexlib\n\n        facexlib.WEIGHTS_ROOT = dirname\n        facexlib.WEIGHTS_DIR = dirname\n        facexlib.WEIGHTS_DETECTION = dirname\n        facexlib.WEIGHTS_RESTORE = dirname\n        facexlib.WEIGHTS_RESTORE_G = dirname\n\n        RetinaFace.setup_keys(dirname)\n        FaceRestorer.setup_keys(dirname)\n\n    except Exception as e:\n        print(\"Error while setting up model:\", e)\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    import os\n    import sys\n    import logging\n    import torch\n    from facexlib.utils.face_restorer import FaceRestorer\n    from facexlib.utils.face_restorer_helper import FaceRestorerHelper\n\n    logger = logging.getLogger()\n\n    try:\n        # patch facexlib\n        sys.path.append(dirname)\n        import facexlib\n\n        facexlib.DIR_FACES = dirname\n        facexlib.DIR_MODELS = dirname\n\n        # initialize face restorer\n        face_restorer = FaceRestorer(\n            upscale=2,\n            face_size=512,\n            channel_multiplier=2,\n            model_path=os.path.join(dirname, \"GFPGANv1.4.pth\"),\n            model=\"GFPGANv1.4\",\n            tile_pad=10,\n            tile_pad_bottom=10,\n            pre_pad=0,\n            half=True,\n        )\n        FaceRestorerHelper.face_restorer = face_restorer\n\n        logger.info(f\"GFPGAN model loaded from {dirname}\")\n    except Exception as e:\n        logger.error(f\"Error setting up GFPGAN model: {e}\")\n        raise e\n\n"}
{"namespace": "quaternion.rotate", "completion": "  q_v = [0] + list(v)\n  q_conj = [q[0]] + [-n for n in q[1:]]\n  return qmult(qmult(q, q_v), q_conj)[1:]"}
{"namespace": "quaternion.rotate", "completion": "  qv = np.zeros(4)\n  qv[0] = 0\n  qv[1:4] = v\n  qv = qv * q\n  qv = qv * np.array([1, -1, -1, -1])\n  qv = qv * np.array([1, 1, 1, -1])\n  qv = qv * q\n  return qv[1:4]"}
{"namespace": "quaternion.rotate", "completion": "  q = np.array(q)\n  v = np.array(v)\n  qv = np.array([0] + list(v))\n  q_conj = q * np.array([1, -1, -1, -1])\n  return (q * qv * q_conj)[1:]"}
{"namespace": "quaternion.rotate", "completion": "  qv = [0] * 4\n  qv[0] = 0\n  qv[1:4] = v[:]\n  qv = multiply(q, qv)\n  qv = multiply(qv, conjugate(q))\n  return qv[1:4]\n"}
{"namespace": "quaternion.rotate", "completion": "  q = np.asarray(q)\n  v = np.asarray(v)\n\n  # Convert the vector to a quaternion\n  q_v = np.concatenate([[0], v])\n\n  # Apply the rotation\n  q_rotated = q * q_v * q.conjugate()\n\n  # Convert the rotated quaternion back to a vector\n  v_rotated = q_rotated[1:]\n\n  return v_rotated"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  q_v = [0] + list(v)\n\n  # Apply the rotation using the quaternion multiplication rules\n  q_rotated = multiply(multiply(q, q_v), conjugate(q))\n\n  # Convert the rotated quaternion back to a vector format\n  return q_rotated[1:]\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  qv = [0] + list(v)\n\n  # Apply the rotation using the quaternion\n  q_rotated = multiply(q, multiply(qv, conjugate(q)))\n\n  # Convert the rotated quaternion back to a vector\n  v_rotated = q_rotated[1:]\n\n  return v_rotated\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion\n  v_q = [0] + list(v)\n\n  # Apply the rotation\n  q_rotated = multiply(q, multiply(v_q, conjugate(q)))\n\n  # Convert the rotated quaternion back to a vector\n  v_rotated = q_rotated[1:]\n\n  return v_rotated\n"}
{"namespace": "quaternion.rotate", "completion": "  qv = [0] * 4\n  qv[:3] = v\n  qv[3] = 0\n  qv = multiply(multiply(q, qv), conjugate(q))\n  return qv[:3]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  q_v = [0] * 4\n  q_v[0] = 0\n  q_v[1:4] = v\n  q_conj = [q[0], -q[1], -q[2], -q[3]]\n  q_rotated = multiply(q, q_v)\n  q_rotated = multiply(q_rotated, q_conj)\n  return q_rotated[1:4]\n"}
{"namespace": "quaternion.rotate", "completion": "  q = np.array(q, dtype=np.float64)\n  v = np.array(v, dtype=np.float64)\n  qv = np.array([0.0] + list(v), dtype=np.float64)\n  q_conj = np.array([q[0]] + [-n for n in q[1:]])\n  return qv * q * q_conj\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  v_quat = [0, v[0], v[1], v[2]]\n\n  # Apply the rotation using the quaternion multiplication rules\n  q_v_quat = multiply(multiply(q, v_quat), conjugate(q))\n\n  # Convert the rotated quaternion back to a vector format\n  v_rotated = [q_v_quat[1], q_v_quat[2], q_v_quat[3]]\n\n  return v_rotated\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion\n  qv = [0] + list(v)\n\n  # Apply the rotation\n  qr = multiply(multiply(q, qv), conjugate(q))\n\n  # Convert the rotated quaternion back to a vector\n  return qr[1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion\n  v_q = [0] + list(v)\n\n  # Apply the rotation to the vector using the quaternion multiplication\n  q_v = multiply(q, v_q)\n  q_v = multiply(q_v, conjugate(q))\n\n  # Convert the rotated vector back to a 3D vector\n  return q_v[1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  q = np.asarray(q)\n  v = np.asarray(v)\n  qv = np.zeros(4)\n  qv[:3] = v\n  qv[3] = 0\n  qv = q * qv\n  qv = np.array([-qv[1], qv[0], -qv[3], qv[2]])\n  qv = q * qv\n  v[:] = qv[:3]\n  return v\n"}
{"namespace": "quaternion.rotate", "completion": "  qv = [0] * 4\n  qv[:3] = v\n  qv[3] = 0\n\n  q_conj = [-x for x in q]\n\n  v_rot = quaternion_multiply(quaternion_multiply(q, qv), q_conj)\n  return v_rot[:3]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion\n  v_quat = [0, v[0], v[1], v[2]]\n\n  # Apply the rotation to the vector\n  rotated_quat = multiply(multiply(q, v_quat), conjugate(q))\n\n  # Convert the rotated quaternion back to a vector\n  rotated_vector = rotated_quat[1:]\n\n  return rotated_vector\n\n"}
{"namespace": "quaternion.rotate", "completion": "  q = np.array(q)\n  v = np.array(v)\n\n  v_quat = np.array([0, v[0], v[1], v[2]])\n\n  # Convert quaternion to rotation matrix\n  R = np.array([[1 - 2 * q[2]**2 - 2 * q[3]**2, 2 * q[1] * q[2] - 2 * q[0] * q[3], 2 * q[1] * q[3] + 2 * q[0] * q[2]],\n                [2 * q[1] * q[2] + 2 * q[0] * q[3], 1 - 2 * q[1]**2 - 2 * q[3]**2, 2 * q[2] * q[3] - 2 * q[0] * q[1]],\n                [2 * q[1] * q[3] - 2 * q[0] * q[2], 2 * q[2] * q[3] + 2 * q[0] * q[1], 1 - 2 * q[1]**2 - 2 * q[2]**2]])\n\n  # Apply rotation to vector\n  v_rot = np.dot(R, v_quat)\n\n  return v_rot[1:]\n"}
{"namespace": "quaternion.rotate", "completion": "  qv = np.append(0, v)\n  qv = np.dot(q, qv)\n  qv = np.dot(qv, np.conjugate(q))\n  return qv[1:]\n"}
{"namespace": "quaternion.rotate", "completion": "  q = np.array(q)\n  v = np.array(v)\n\n  if len(v.shape) == 1:\n    v = np.array([v[0], v[1], v[2], 0])\n  else:\n    v = np.append(v, np.zeros((v.shape[0], 1)), axis=1)\n\n  q_conj = np.array([-q[0], -q[1], -q[2], q[3]])\n  q_v = np.dot(q, v)\n  q_v_conj = np.dot(q_conj, q_v)\n  return q_v_conj[:3]\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n  axis_angle = axis_angle / (angle + eps)\n  angle = angle * 0.5\n  return jnp.concatenate([jnp.cos(angle), jnp.sin(angle) * axis_angle])"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Normalize the axis vector\n  axis = axis_angle / jnp.linalg.norm(axis_angle)\n\n  # Calculate the quaternion components\n  q = jnp.concatenate([jnp.cos(0.5 * jnp.linalg.norm(axis_angle)),\n                       jnp.sin(0.5 * jnp.linalg.norm(axis_angle)) * axis])\n\n  # Return the quaternion\n  return q"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Normalize the axis vector\n  axis = jnp.array(axis_angle) / jnp.linalg.norm(axis_angle)\n\n  # Calculate the quaternion components\n  q = jnp.zeros(4)\n  q = q.at[0].set(jnp.cos(jnp.linalg.norm(axis_angle) / 2.0))\n  q = q.at[1:].set(jnp.sin(jnp.linalg.norm(axis_angle) / 2.0) * axis)\n\n  # Ensure numerical stability for small angles by using a small epsilon value\n  q = q * jnp.where(jnp.linalg.norm(axis_angle) > eps, 1.0, eps)\n\n  return q"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle)\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n  axis_angle = axis_angle / (angle + eps)\n  angle = jnp.clip(angle, -1.0, 1.0)\n  angle = jnp.arccos(angle)\n  quaternion = jnp.concatenate([jnp.cos(angle / 2.0), axis_angle * jnp.sin(angle / 2.0)])\n  return quaternion"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Convert axis-angle to quaternion\n  axis = jnp.array(axis_angle[:3], dtype = jnp.float32)\n  angle = jnp.array(axis_angle[3], dtype = jnp.float32)\n  axis = axis / jnp.linalg.norm(axis)\n  quaternion = jnp.array([\n    jnp.cos(angle / 2),\n    *(jnp.sin(angle / 2) * axis)\n  ])\n\n  # Ensure numerical stability for small angles\n  quaternion = quaternion / (jnp.linalg.norm(quaternion) + eps)\n\n  return quaternion"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Normalize the axis vector\n  axis = jnp.array(axis_angle) / jnp.linalg.norm(axis_angle)\n\n  # Calculate the quaternion components\n  q = jnp.concatenate([jnp.cos(jnp.linalg.norm(axis_angle) / 2.0),\n                       jnp.sin(jnp.linalg.norm(axis_angle) / 2.0) * axis])\n\n  # Ensure numerical stability for small angles\n  q = q / jnp.linalg.norm(q)\n\n  return q\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle)\n  axis_angle = jnp.reshape(axis_angle, (3,))\n\n  axis = axis_angle[:3]\n  angle = axis_angle[3]\n\n  axis = axis / jnp.linalg.norm(axis)\n\n  angle = jnp.clip(angle, -1.0, 1.0)\n  angle = jnp.arccos(angle)\n\n  angle = jnp.where(angle < eps, eps, angle)\n\n  axis = axis * angle\n\n  return jnp.hstack([jnp.cos(angle / 2.0), axis * jnp.sin(angle / 2.0)])"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Normalize the axis vector\n  axis = axis_angle / jnp.linalg.norm(axis_angle)\n\n  # Calculate the quaternion components\n  quat = jnp.zeros(4)\n  quat = quat.at[0].set(jnp.cos(0.5 * jnp.linalg.norm(axis_angle)))\n  quat = quat.at[1:].set(jnp.sin(0.5 * jnp.linalg.norm(axis_angle)) * axis)\n\n  # Ensure numerical stability for small angles\n  quat = quat.at[0].set(jnp.where(jnp.linalg.norm(axis_angle) < eps, 1.0, quat[0]))\n  quat = quat.at[1:].set(jnp.where(jnp.linalg.norm(axis_angle) < eps, 0.0, quat[1:]))\n\n  return quat\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis = jnp.array(axis_angle[:3])\n  angle = jnp.array(axis_angle[3])\n  axis_norm = jnp.linalg.norm(axis)\n  if axis_norm < eps:\n    return jnp.array([1.0, 0.0, 0.0, 0.0])\n  axis = axis / axis_norm\n  angle = angle / 2.0\n  return jnp.array([jnp.cos(angle), jnp.sin(angle) * axis[0], jnp.sin(angle) * axis[1], jnp.sin(angle) * axis[2]])\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle, dtype=jnp.float32)\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n\n  return jnp.array([jnp.cos(jnp.linalg.norm(axis_angle) / 2.0), *(jnp.sin(jnp.linalg.norm(axis_angle) / 2.0) * axis_angle)], dtype=jnp.float32)\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Convert the axis-angle representation to a 4-vector\n  axis_angle = jnp.array(axis_angle)\n  axis_angle = jnp.concatenate([jnp.zeros(1), axis_angle])\n\n  # Normalize the axis-angle vector\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n\n  # Calculate the quaternion\n  quaternion = jnp.concatenate([jnp.cos(jnp.linalg.norm(axis_angle) / 2), axis_angle * jnp.sin(jnp.linalg.norm(axis_angle) / 2)])\n\n  # Ensure numerical stability for small angles\n  quaternion = quaternion / (jnp.linalg.norm(quaternion) + eps)\n\n  return quaternion\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Ensure that the input axis_angle is a 3-vector\n  axis_angle = jnp.array(axis_angle, dtype=jnp.float32)\n  if axis_angle.shape != (3,):\n    raise ValueError(\"Input axis_angle must be a 3-vector.\")\n\n  # Normalize the axis of rotation\n  axis = axis_angle / jnp.linalg.norm(axis_angle)\n\n  # Calculate the quaternion components\n  angle = jnp.linalg.norm(axis_angle)\n  q_vec = jnp.sin(angle / 2) * axis\n  q_w = jnp.cos(angle / 2)\n\n  # Ensure numerical stability for small angles by using a small epsilon value\n  q_vec = jnp.where(jnp.abs(q_vec) < eps, eps * jnp.sign(q_vec), q_vec)\n  q_w = jnp.where(jnp.abs(q_w) < eps, eps * jnp.sign(q_w), q_w)\n\n  # Return the quaternion as a 4-vector\n  return jnp.array([q_w, *q_vec])"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Convert the axis-angle representation to a quaternion\n  axis = jnp.array(axis_angle[:3])\n  angle = jnp.array(axis_angle[3])\n\n  # Normalize the axis vector\n  axis = axis / jnp.linalg.norm(axis)\n\n  # Calculate the quaternion components\n  q = jnp.zeros(4)\n  q = q.at[0].set(jnp.cos(angle / 2))\n  q = q.at[1:].set(axis * jnp.sin(angle / 2))\n\n  # Return the quaternion\n  return q\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Convert the input axis-angle vector to a normalized vector\n  axis_angle = jnp.array(axis_angle)\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n\n  # Compute the quaternion components\n  q = jnp.zeros(4)\n  q = q.at[0].set(jnp.cos(jnp.linalg.norm(axis_angle) / 2.0))\n  q = q.at[1:].set(axis_angle * jnp.sin(jnp.linalg.norm(axis_angle) / 2.0))\n\n  # Ensure numerical stability for small angles by using a small epsilon value\n  q = q / (jnp.linalg.norm(q) + eps)\n\n  return q\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n  theta = jnp.linalg.norm(axis_angle)\n  theta = jnp.where(theta < eps, eps, theta)\n  axis_angle = axis_angle / theta\n  theta = theta / 2.0\n  return jnp.array([jnp.cos(theta), axis_angle[0] * jnp.sin(theta), axis_angle[1] * jnp.sin(theta), axis_angle[2] * jnp.sin(theta)])\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Extract the axis and angle from the input vector\n  axis = axis_angle[:3]\n  angle = axis_angle[3]\n\n  # Normalize the axis vector\n  axis = axis / jnp.linalg.norm(axis)\n\n  # Calculate the quaternion components\n  qx, qy, qz = jnp.sin(angle / 2) * axis\n  qw = jnp.cos(angle / 2)\n\n  # Return the quaternion as a 4-vector\n  return jnp.array([qw, qx, qy, qz])\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis = jnp.array(axis_angle) / jnp.linalg.norm(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n  return jnp.array([jnp.cos(angle / 2), *(jnp.sin(angle / 2) * axis)])\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle, dtype = jnp.float32)\n  axis = axis_angle[:3]\n  angle = axis_angle[3]\n  axis = axis / jnp.linalg.norm(axis)\n  angle = angle / 2\n\n  return jnp.array([jnp.cos(angle), *(jnp.sin(angle) * axis)])\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Check if the input axis_angle is a 3-vector\n  if axis_angle.shape != (3,):\n    raise ValueError(\"Input axis_angle must be a 3-vector.\")\n\n  # Normalize the axis of rotation\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n\n  # Calculate the quaternion components\n  theta = jnp.linalg.norm(axis_angle)\n  w = jnp.cos(theta / 2)\n  x, y, z = jnp.sin(theta / 2) * axis_angle\n\n  # Construct the quaternion\n  quat = jnp.array([w, x, y, z])\n\n  # Ensure numerical stability for small angles\n  quat = quat / jnp.linalg.norm(quat)\n\n  return quat\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle, dtype=jnp.float32)\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n  theta = jnp.linalg.norm(axis_angle)\n\n  # Check if the angle is within a certain threshold\n  if theta < eps:\n    return jnp.array([1.0, 0.0, 0.0, 0.0])\n\n  axis_angle = axis_angle * jnp.sin(theta / 2)\n  return jnp.array([jnp.cos(theta / 2), *axis_angle])\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk = model.get_topk(prefix, k=k)\n    # get indices\n    indices = topk[0]\n    # get logprobs\n    logprobs = topk[1]\n    # get target index\n    target_idx = indices[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob\n    target_logprob = logprobs[idx]\n    # get target logprob"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top k\n    # adjust bias\n    # get top"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    # get topk\n    topk = model.topk(prefix, k=k, high=high)\n    # get topk indices\n    topk_indices = [i[0] for i in topk]\n    # get topk log probs\n    topk_logprobs = [i[1] for i in topk]\n    # get topk words\n    topk_words = [i[2] for i in topk]\n    # get topk log probs\n    topk_logprobs = [i[1] for i in topk]\n    # get topk words\n    topk_words = [i[2] for i in topk]\n    # get topk log probs\n    topk_logprobs = [i[1] for i in topk]\n    # get topk words\n    topk_words = [i[2] for i in topk]\n    # get topk log probs\n    topk_logprobs = [i[1] for i in topk]\n    # get topk words\n    topk_words = [i[2] for i in topk]\n    # get topk log probs\n    topk_logprobs = [i[1] for i in topk]\n    # get topk words\n    topk_words = [i[2] for i in topk]\n    # get topk log probs\n    topk_logprobs = [i[1] for i in topk]\n    # get topk words\n    topk_words = [i[2] for i in topk]\n    # get topk log probs\n    topk_logprobs = [i[1] for i in topk]\n    # get topk words\n    topk_words = [i[2] for i in topk]\n    # get topk log probs\n    topk_logprobs = [i[1] for i in topk]\n    # get topk words\n    topk_words = [i[2] for i in topk]\n    # get topk log probs"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk = model.topk(prefix, k=k)\n    # get topk indices\n    topk_indices = topk[0]\n    # get topk probabilities\n    topk_probs = topk[1]\n    # get topk words\n    topk_words = topk[2]\n    # get topk logprobs\n    topk_logprobs = topk[3]\n    # get topk logprobs\n    topk_logprobs = topk[3]\n\n    # get topk indices\n    topk_indices = topk[0]\n    # get topk probabilities\n    topk_probs = topk[1]\n    # get topk words\n    topk_words = topk[2]\n    # get topk logprobs\n    topk_logprobs = topk[3]\n\n    # get topk indices\n    topk_indices = topk[0]\n    # get topk probabilities\n    topk_probs = topk[1]\n    # get topk words\n    topk_words = topk[2]\n    # get topk logprobs\n    topk_logprobs = topk[3]\n\n    # get topk indices\n    topk_indices = topk[0]\n    # get topk probabilities\n    topk_probs = topk[1]\n    # get topk words\n    topk_words = topk[2]\n    # get topk logprobs\n    topk_logprobs = topk[3]\n\n    # get topk indices\n    topk_indices = topk[0]\n    # get topk probabilities\n    topk_probs = topk[1]\n    # get topk words\n    topk_words = topk[2]\n    # get topk logprobs\n    topk_logprobs = topk[3]\n\n    # get topk indices\n    topk_indices = topk[0]\n    # get topk probabilities\n    topk_probs = topk[1"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk = model.topk(prefix, k=k, high=high)\n\n    # get idx\n    idx = topk[0][idx]\n\n    # get log prob\n    log_prob = topk[1][idx]\n\n    # get calls\n    calls = model.calls\n\n    return log_prob, calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk\n    raw_topk = model.get_topk(prefix, k=k)\n\n    # get topk indices\n    topk_idx = [i[0] for i in raw_topk]\n\n    # get topk log probs\n    topk_log_probs = [i[1] for i in raw_topk]\n\n    # get target index log prob\n    target_log_prob = topk_log_probs[idx]\n\n    # get target index\n    target_idx = topk_idx[idx]\n\n    # get target index log prob\n    target_log_prob = topk_log_probs[idx]\n\n    # get target index\n    target_idx = topk_idx[idx]\n\n    # get target index log prob\n    target_log_prob = topk_log_probs[idx]\n\n    # get target index\n    target_idx = topk_idx[idx]\n\n    # get target index log prob\n    target_log_prob = topk_log_probs[idx]\n\n    # get target index\n    target_idx = topk_idx[idx]\n\n    # get target index log prob\n    target_log_prob = topk_log_probs[idx]\n\n    # get target index\n    target_idx = topk_idx[idx]\n\n    # get target index log prob\n    target_log_prob = topk_log_probs[idx]\n\n    # get target index\n    target_idx = topk_idx[idx]\n\n    # get target index log prob\n    target_log_prob = topk_log_probs[idx]\n\n    # get target index\n    target_idx = topk_idx[idx]\n\n    # get target index log prob\n    target_log_prob = topk_log_probs[idx]\n\n    # get target index\n    target_idx = topk_idx[idx]\n\n    # get target index log prob\n    target_log_prob = topk_log_probs[idx]\n\n    # get target index"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # TODO: check if prefix is in the model\n    # TODO: check if idx is in the model\n    # TODO: check if k is in the model\n    # TODO: check if high is in the model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the model is a model\n    # TODO: check if the"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # adjust high bias\n    model.set_bias(idx, high)\n\n    # get topk\n    topk = model.get_topk(prefix, k=k)\n\n    # get log prob of topk\n    log_prob = model.log_prob(prefix, topk)\n\n    # get idx\n    idx = topk[idx]\n\n    # get log prob of idx\n    log_prob_idx = log_prob[idx]\n\n    # get log prob of topk\n    log_prob_topk = log_prob[topk]\n\n    # get log prob of topk\n    log_prob_topk = log_prob[topk]\n\n    # get log prob of topk\n    log_prob_topk = log_prob[topk]\n\n    # get log prob of topk\n    log_prob_topk = log_prob[topk]\n\n    # get log prob of topk\n    log_prob_topk = log_prob[topk]\n\n    # get log prob of topk\n    log_prob_topk = log_prob[topk]\n\n    # get log prob of topk\n    log_prob_topk = log_prob[topk]\n\n    # get log prob of topk\n    log_prob_topk = log_prob[topk]\n\n    # get log prob of topk\n    log_prob_topk = log_prob[topk]\n\n    # get log prob of topk\n    log_prob_topk = log_prob[topk]\n\n    # get log prob of topk\n    log_prob_topk = log_prob[topk]\n\n    # get log prob of topk\n    log_prob_topk = log_prob[topk]\n\n    # get log prob of topk\n    log_prob_topk = log_prob[topk]\n\n    # get log prob of topk\n    log_prob_topk = log_prob[topk]\n\n    # get log prob of topk\n    log_prob_topk = log_prob[topk]"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    calls = 0\n    low = 0\n    while high - low > 1:\n        mid = (high + low) // 2\n        calls, topk = model.topk(prefix, k, bias=mid)\n        if topk[idx] > topk[0]:\n            low = mid\n        else:\n            high = mid\n    calls, topk = model.topk(prefix, k, bias=low)\n    return topk[idx] / topk[0], calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get the topk\n    topk = model.get_topk(prefix, k=k)\n    # get the index of the target\n    target_idx = topk[0].index(idx)\n    # set the high bias\n    model.set_high_bias(target_idx, high)\n    # get the log probability of the target index being the top result\n    logp = model.logp(prefix)\n    # reset the high bias\n    model.set_high_bias(target_idx, 0)\n    # return the log probability and the number of calls made to the model\n    return logp, model.calls"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk\n    topk = model.topk(prefix, k=k, high=high)\n    # get idx\n    idx = topk[0][idx]\n    # get log prob\n    log_prob = topk[1][idx]\n    # get number of calls\n    num_calls = model.num_calls\n    # return log prob and num calls\n    return log_prob, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    calls = 0\n    low = 0\n    while high - low > 1:\n        mid = (low + high) // 2\n        topk, topk_idx = model.topk(prefix, k=k, bias=mid)\n        calls += 1\n        if idx in topk_idx:\n            low = mid\n        else:\n            high = mid\n    return math.log(1 / (high - low)), calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk = model.topk(prefix, k=k)\n    # get the index of the target index\n    target_idx = topk[0].index(idx)\n    # get the log probability of the target index\n    target_log_prob = topk[1][target_idx]\n    # get the number of calls made to the model\n    num_calls = model.num_calls\n    # adjust the search bias\n    while target_idx != 0:\n        # adjust the search bias\n        model.search_bias = high\n        # get topk\n        topk = model.topk(prefix, k=k)\n        # get the index of the target index\n        target_idx = topk[0].index(idx)\n        # get the log probability of the target index\n        target_log_prob = topk[1][target_idx]\n        # get the number of calls made to the model\n        num_calls = model.num_calls\n        # adjust the high bias\n        high *= 2\n    # return the log probability and the number of calls\n    return target_log_prob, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk, could be done outside and passed in\n    # topk = model.get_topk(prefix, k=k)\n\n    # get topk indices, could be done outside and passed in\n    # topk_idx = topk[1]\n\n    # get target index, could be done outside and passed in\n    # target_idx = topk_idx[idx]\n\n    # get target log prob, could be done outside and passed in\n    # target_logprob = topk[0][idx]\n\n    # get target log prob, could be done outside and passed in\n    # target_logprob = model.get_logprob(prefix, idx)\n\n    # get target log prob, could be done outside and passed in\n    # target_logprob = model.get_logprob(prefix, idx)\n\n    # get target log prob, could be done outside and passed in\n    # target_logprob = model.get_logprob(prefix, idx)\n\n    # get target log prob, could be done outside and passed in\n    # target_logprob = model.get_logprob(prefix, idx)\n\n    # get target log prob, could be done outside and passed in\n    # target_logprob = model.get_logprob(prefix, idx)\n\n    # get target log prob, could be done outside and passed in\n    # target_logprob = model.get_logprob(prefix, idx)\n\n    # get target log prob, could be done outside and passed in\n    # target_logprob = model.get_logprob(prefix, idx)\n\n    # get target log prob, could be done outside and passed in\n    # target_logprob = model.get_logprob(prefix, idx)\n\n    # get target log prob, could be done outside and passed in\n    # target_logprob = model.get_logprob(prefix, idx)\n\n    # get target log prob, could be done outside and passed in\n    # target_logprob = model.get_logprob(prefix, idx)\n\n    # get target log prob, could be done outside and passed in\n    # target_logprob = model.get"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk = model.topk(prefix, k=k)\n\n    # get the index of the target index\n    idx = topk.indices[idx]\n\n    # get the log probability of the target index\n    logp = topk.logprobs[idx]\n\n    # get the number of calls made to the model\n    n_calls = model.n_calls\n\n    # adjust the search bias until the target index is the most probable\n    while True:\n        # get the adjusted topk\n        topk = model.topk(prefix, k=k, high=high)\n\n        # get the index of the target index\n        idx = topk.indices[idx]\n\n        # get the log probability of the target index\n        logp = topk.logprobs[idx]\n\n        # get the number of calls made to the model\n        n_calls = model.n_calls\n\n        # if the target index is the most probable, break the loop\n        if idx == 0:\n            break\n\n        # adjust the high bias value for the next iteration\n        high = high * 2\n\n    # return the log probability of the target index being the top result and the number of calls made to the model\n    return logp, n_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    calls = 0\n    while True:\n        calls += 1\n        topk = model.topk(prefix, k=k, high=high)\n        topk_idx = topk.indices[0]\n        if topk_idx == idx:\n            return topk.logits[0].item(), calls\n        high = topk.values[0].item()\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk, could be done outside and passed in\n    topk = model.topk(prefix, k=k)\n    # get the index of the target word\n    target_idx = topk[0].index(idx)\n    # get the log probability of the target word\n    target_logp = topk[1][target_idx]\n    # get the number of calls made to the model\n    calls = model.calls\n    # adjust the search bias until the target index is the most probable\n    while target_idx != 0:\n        # adjust the search bias\n        model.set_search_bias(target_idx, high)\n        # get the new topk\n        topk = model.topk(prefix, k=k)\n        # get the new index of the target word\n        target_idx = topk[0].index(idx)\n        # get the new log probability of the target word\n        target_logp = topk[1][target_idx]\n        # get the new number of calls made to the model\n        calls = model.calls\n    # return the log probability of the target index being the top result and the number of calls made to the model\n    return target_logp, calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk\n    topk = model.topk(prefix, k=k, high=high)\n    # get index of target\n    target_idx = topk[0][idx]\n    # get log prob of target\n    target_log_prob = topk[1][idx]\n    # get number of calls\n    num_calls = model.num_calls\n    # return log prob and num calls\n    return target_log_prob, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk, could be done outside and passed in\n    # k = 1\n    # high = 40\n    # idx = 0\n\n    # get topk\n    # topk = model.topk(prefix, k, high)\n    # topk = model.topk(prefix, k, high)\n    topk = model.topk(prefix, k, high)\n\n    # get the log probability of the target index\n    # log_prob = topk[0][idx]\n    log_prob = topk[0][idx]\n\n    # count the number of calls made to the model\n    # num_calls = model.num_calls\n    num_calls = model.num_calls\n\n    return log_prob, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk, could be done outside and passed in\n    # print(f\"{prefix}\")\n    topk, _ = model.topk(prefix, k=k, high=high)\n    # print(f\"{topk}\")\n    # print(f\"{idx}\")\n    # print(f\"{topk[idx]}\")\n\n    # find the log probability of the target index being the top result\n    logprob = model.logp(prefix, topk[idx])\n    # print(f\"{logprob}\")\n\n    # return logprob, number of calls made to the model\n    return logprob, model.calls\n\n"}
{"namespace": "resample.resample_3d", "completion": "  if method == 'TRILINEAR':\n    return resample_3d_trilinear(data, locations, edge_behavior, constant_values, coordinate_order, half_pixel_center)\n  elif method == 'NEAREST':\n    return resample_3d_nearest(data, locations, edge_behavior, constant_values, coordinate_order, half_pixel_center)\n  else:\n    raise ValueError('Invalid interpolation method.')\n"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order == 'xyz':\n    locations = tf.transpose(locations, [0, 3, 1, 2])\n  elif coordinate_order != 'zyx':\n    raise ValueError('Unsupported coordinate_order: {}'.format(coordinate_order))\n\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  if method == 'TRILINEAR':\n    return tf.image.resize3d(\n        data,\n        locations.shape[1:],\n        method='trilinear',\n        align_corners=True,\n        half_pixel_centers=half_pixel_center,\n    )\n  elif method == 'NEAREST':\n    return tf.gather_nd(data, tf.cast(locations, tf.int32))\n  else:\n    raise ValueError('Unsupported method: {}'.format(method))"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order == 'xyz':\n    locations = tf.transpose(locations, [0, 3, 1, 2])\n  elif coordinate_order != 'zyx':\n    raise ValueError('Invalid coordinate_order')\n\n  if half_pixel_center:\n    locations = locations + tf.constant(0.5, dtype=locations.dtype)\n\n  if method == 'TRILINEAR':\n    return tf.nn.trilinear_interpolation(data, locations, edge_mode=edge_behavior, name='resample_3d')\n  elif method == 'NEAREST':\n    return tf.nn.pool(data, window_shape=[1, 1, 1], strides=[1, 1, 1], padding='VALID', pooling_type='MAX', name='resample_3d')\n  else:\n    raise ValueError('Invalid method')"}
{"namespace": "resample.resample_3d", "completion": "  if half_pixel_center:\n    locations = locations - 0.5\n\n  if method == 'TRILINEAR':\n    return tf.image.resize3d(\n        data,\n        locations.shape[1:],\n        method,\n        edge_behavior,\n        constant_values,\n        coordinate_order,\n    )\n  elif method == 'NEAREST':\n    locations = tf.cast(locations, tf.int32)\n    return tf.gather_nd(data, locations)\n  else:\n    raise ValueError(\n        f'Invalid method {method}. Must be either \"TRILINEAR\" or \"NEAREST\".'\n    )"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order == 'xyz':\n    locations = tf.transpose(locations, [0, 3, 1, 2])\n  elif coordinate_order != 'zyx':\n    raise ValueError('Unknown coordinate_order: {}'.format(coordinate_order))\n\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  if method == 'TRILINEAR':\n    return tf.image.resize3d(\n        data,\n        locations.shape[1:],\n        method='trilinear',\n        align_corners=True,\n        name='resample_3d',\n    )\n  elif method == 'NEAREST':\n    return tf.image.resize3d(\n        data,\n        locations.shape[1:],\n        method='nearest',\n        align_corners=True,\n        name='resample_3d',\n    )\n  else:\n    raise ValueError('Unknown method: {}'.format(method))"}
{"namespace": "resample.resample_3d", "completion": "  # Check if input data is 3D\n  if len(data.shape) != 4:\n    raise ValueError(\"Input data must be 3D.\")\n\n  # Check if locations are in xyz or zyx order\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError(\"Coordinate order must be either 'xyz' or 'zyx'.\")\n\n  # Check if method is either trilinear or nearest neighbor\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError(\"Method must be either 'TRILINEAR' or 'NEAREST'.\")\n\n  # Check if edge behavior is either constant outside or clamp\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(\"Edge behavior must be either 'CONSTANT_OUTSIDE' or 'CLAMP'.\")\n\n  # Check if data and locations have the same number of dimensions\n  if len(data.shape) != len(locations.shape):\n    raise ValueError(\"Data and locations must have the same number of dimensions.\")\n\n  # Check if data and locations have the same number of channels\n  if data.shape[-1] != locations.shape[-1]:\n    raise ValueError(\"Data and locations must have the same number of channels.\")\n\n  # Check if data and locations have the same number of dimensions\n  if len(data.shape) != len(locations.shape):\n    raise ValueError(\"Data and locations must have the same number of dimensions.\")\n\n  # Check if data and locations have the same number of channels\n  if data.shape[-1] != locations.shape[-1]:\n    raise ValueError(\"Data and locations must have the same number of channels.\")\n\n  # Check if data and locations have the same number of dimensions\n  if len(data.shape) != len(locations.shape):\n    raise ValueError(\"Data and locations must have the same number of dimensions.\")\n\n  # Check if data and locations have the same number of channels\n  if data.shape[-1] != locations.shape[-1]:\n    raise Value"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order == 'xyz':\n    locations = tf.transpose(locations, [0, 3, 1, 2])\n  elif coordinate_order == 'zyx':\n    locations = tf.transpose(locations, [0, 3, 2, 1])\n  else:\n    raise ValueError('Invalid coordinate_order: {}'.format(coordinate_order))\n\n  if half_pixel_center:\n    locations = locations - 0.5\n\n  if method == 'TRILINEAR':\n    return tf.image.resize3d(\n        data,\n        locations.shape[1:],\n        method=tf.image.ResizeMethod.BILINEAR,\n        align_corners=True,\n        half_pixel_centers=half_pixel_center,\n    )\n  elif method == 'NEAREST':\n    return tf.gather_nd(data, tf.cast(locations, tf.int32))\n  else:\n    raise ValueError('Invalid method: {}'.format(method))\n\n"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order == 'xyz':\n    locations = locations\n  elif coordinate_order == 'zyx':\n    locations = tf.reverse(locations, axis=[-1])\n  else:\n    raise ValueError('coordinate_order must be either xyz or zyx')\n\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  if method == 'TRILINEAR':\n    if edge_behavior == 'CONSTANT_OUTSIDE':\n      return tf.contrib.resampler.resampler(\n          data,\n          locations,\n          edge_behavior=edge_behavior,\n          constant_values=constant_values,\n          method='TRILINEAR',\n      )\n    elif edge_behavior == 'CLAMP':\n      return tf.contrib.resampler.resampler(\n          data, locations, edge_behavior=edge_behavior, method='TRILINEAR'\n      )\n    else:\n      raise ValueError('edge_behavior must be either CONSTANT_OUTSIDE or CLAMP')\n  elif method == 'NEAREST':\n    return tf.contrib.resampler.resampler(\n        data, locations, edge_behavior=edge_behavior, method='NEAREST'\n    )\n  else:\n    raise ValueError('method must be either TRILINEAR or NEAREST')"}
{"namespace": "resample.resample_3d", "completion": "  if half_pixel_center:\n    locations = locations - 0.5\n\n  if coordinate_order == 'xyz':\n    locations = tf.reverse(locations, axis=[-1])\n\n  if method == 'TRILINEAR':\n    return tf.raw_ops.Resampler(\n        input=data,\n        grid=locations,\n        edge_mode=edge_behavior,\n        constant_values=constant_values,\n        name='Resampler')\n\n  elif method == 'NEAREST':\n    return tf.raw_ops.Resampler(\n        input=data,\n        grid=locations,\n        edge_mode=edge_behavior,\n        constant_values=constant_values,\n        method='nearest',\n        name='Resampler')\n\n  else:\n    raise ValueError('Unknown interpolation method: {}'.format(method))"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order == 'xyz':\n    locations = locations.permute(0, 4, 1, 2, 3)\n  elif coordinate_order != 'zyx':\n    raise ValueError(f'Unsupported coordinate order: {coordinate_order}')\n\n  if half_pixel_center:\n    locations = locations - 0.5\n\n  if method == 'TRILINEAR':\n    return torch.nn.functional.grid_sample(data, locations, padding_mode=edge_behavior, align_corners=False, mode='trilinear',\n                                           border_value=constant_values)\n  elif method == 'NEAREST':\n    return torch.nn.functional.grid_sample(data, locations, padding_mode=edge_behavior, align_corners=False, mode='nearest',\n                                           border_value=constant_values)\n  else:\n    raise ValueError(f'Unsupported interpolation method: {method}')"}
{"namespace": "resample.resample_3d", "completion": "  # Check if the input data is a 4D tensor\n  if len(data.shape) != 4:\n    raise ValueError('Input data must be a 4D tensor.')\n\n  # Check if the locations are a 3D tensor\n  if len(locations.shape) != 3:\n    raise ValueError('Locations must be a 3D tensor.')\n\n  # Check if the number of dimensions in the input data and locations match\n  if data.shape[0] != locations.shape[0]:\n    raise ValueError('Number of dimensions in input data and locations must match.')\n\n  # Check if the number of dimensions in the input data and locations match\n  if data.shape[1] != locations.shape[1]:\n    raise ValueError('Number of dimensions in input data and locations must match.')\n\n  # Check if the number of dimensions in the input data and locations match\n  if data.shape[2] != locations.shape[2]:\n    raise ValueError('Number of dimensions in input data and locations must match.')\n\n  # Check if the number of dimensions in the input data and locations match\n  if data.shape[3] != locations.shape[3]:\n    raise ValueError('Number of dimensions in input data and locations must match.')\n\n  # Check if the coordinate order is valid\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Invalid coordinate order. Must be either \"xyz\" or \"zyx\".')\n\n  # Check if the interpolation method is valid\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Invalid interpolation method. Must be either \"TRILINEAR\" or \"NEAREST\".')\n\n  # Check if the edge behavior is valid\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('Invalid edge behavior. Must be either \"CONSTANT_OUTSIDE\" or \"CLAMP\".')\n\n  # Check if the data type is valid\n  if data.dtype not in [tf.float32, tf.float6"}
{"namespace": "resample.resample_3d", "completion": "  if half_pixel_center:\n    if coordinate_order == 'xyz':\n      locations = locations - 0.5\n    else:\n      locations = locations - 0.5\n\n  if method == 'TRILINEAR':\n    return tf.compat.v1.image.resize_trilinear(\n        data,\n        tf.shape(locations)[1:],\n        align_corners=True,\n        half_pixel_centers=half_pixel_center,\n        name='resample_3d_trilinear',\n    )\n  elif method == 'NEAREST':\n    return tf.compat.v1.image.resize_nearest_neighbor(\n        data,\n        tf.shape(locations)[1:],\n        align_corners=True,\n        half_pixel_centers=half_pixel_center,\n        name='resample_3d_nearest',\n    )\n  else:\n    raise ValueError('Unsupported resampling method: {}'.format(method))"}
{"namespace": "resample.resample_3d", "completion": "  import tensorflow as tf\n\n  # Check if the input data is a 4D tensor with shape [D, H, W, C]\n  if len(data.shape) != 4:\n    raise ValueError(\n        'Input data must be a 4D tensor with shape [D, H, W, C].')\n\n  # Check if the input locations is a 3D tensor with shape [D, ..., 3]\n  if len(locations.shape) != 3:\n    raise ValueError(\n        'Input locations must be a 3D tensor with shape [D, ..., 3].')\n\n  # Check if the input edge_behavior is a string\n  if not isinstance(edge_behavior, str):\n    raise ValueError('Input edge_behavior must be a string.')\n\n  # Check if the input coordinate_order is a string\n  if not isinstance(coordinate_order, str):\n    raise ValueError('Input coordinate_order must be a string.')\n\n  # Check if the input method is a string\n  if not isinstance(method, str):\n    raise ValueError('Input method must be a string.')\n\n  # Check if the input half_pixel_center is a boolean\n  if not isinstance(half_pixel_center, bool):\n    raise ValueError('Input half_pixel_center must be a boolean.')\n\n  # Check if the input constant_values is a float\n  if not isinstance(constant_values, float):\n    raise ValueError('Input constant_values must be a float.')\n\n  # Check if the input edge_behavior is valid\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(\n        'Input edge_behavior must be either \"CONSTANT_OUTSIDE\" or \"CLAMP\".')\n\n  # Check if the input coordinate_order is valid\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Input coordinate_order must be either \"xyz\" or \"zyx\".')\n\n  # Check if the input method is"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order == 'xyz':\n    x, y, z = tf.split(locations, 3, axis=-1)\n  elif coordinate_order == 'zyx':\n    z, y, x = tf.split(locations, 3, axis=-1)\n  else:\n    raise ValueError('Unsupported coordinate_order: {}'.format(coordinate_order))\n\n  if half_pixel_center:\n    x = x + 0.5\n    y = y + 0.5\n    z = z + 0.5\n\n  if method == 'TRILINEAR':\n    return tf.image.interpolate_trilinear(data, [x, y, z], align_corners=True, half_pixel_centers=half_pixel_center, name='resample_3d_trilinear')\n  elif method == 'NEAREST':\n    return tf.image.resize_nearest_neighbor(data, tf.shape(locations)[1:-1], align_corners=True, name='resample_3d_nearest')\n  else:\n    raise ValueError('Unsupported interpolation method: {}'.format(method))\n\n"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order == 'xyz':\n    locations = locations.permute(0, 4, 1, 2, 3)\n  else:\n    locations = locations.permute(0, 4, 3, 1, 2)\n\n  if half_pixel_center:\n    locations = locations - 0.5\n\n  if method == 'TRILINEAR':\n    resampled_data = F.grid_sample(\n        data,\n        locations,\n        padding_mode=edge_behavior,\n        align_corners=False,\n        mode='trilinear',\n        )\n  elif method == 'NEAREST':\n    resampled_data = F.grid_sample(\n        data,\n        locations,\n        padding_mode=edge_behavior,\n        align_corners=False,\n        mode='nearest',\n        )\n  else:\n    raise ValueError(f'Invalid interpolation method: {method}')\n\n  if coordinate_order == 'xyz':\n    resampled_data = resampled_data.permute(0, 2, 3, 4, 1)\n  else:\n    resampled_data = resampled_data.permute(0, 2, 4, 1, 3)\n\n  return resampled_data\n\n"}
{"namespace": "resample.resample_3d", "completion": "  # Check if the input data is a 4D tensor\n  if data.shape.ndims != 4:\n    raise ValueError('Input data must be a 4D tensor')\n\n  # Check if the locations are a 3D tensor\n  if locations.shape.ndims != 3:\n    raise ValueError('Locations must be a 3D tensor')\n\n  # Check if the coordinate order is valid\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Invalid coordinate order')\n\n  # Check if the interpolation method is valid\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Invalid interpolation method')\n\n  # Check if the edge behavior is valid\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('Invalid edge behavior')\n\n  # Check if the locations are in the correct order\n  if coordinate_order == 'xyz':\n    if locations.shape[2] != 3:\n      raise ValueError('Locations must be in xyz order')\n  else:\n    if locations.shape[1] != 3:\n      raise ValueError('Locations must be in zyx order')\n\n  # Check if the data and locations have the same number of dimensions\n  if data.shape[0] != locations.shape[0]:\n    raise ValueError('Data and locations must have the same number of dimensions')\n\n  # Check if the data and locations have the same number of channels\n  if data.shape[3] != locations.shape[2]:\n    raise ValueError('Data and locations must have the same number of channels')\n\n  # Check if the locations are within the bounds of the data\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    if locations.shape[0] != data.shape[0]:\n      raise ValueError('Locations must be within the bounds of the data')\n    if locations.shape[1] != data.shape[1]:\n      raise ValueError('Locations must be within the bounds of the data')\n    if locations"}
{"namespace": "resample.resample_3d", "completion": "  if half_pixel_center:\n    locations = locations + 0.5\n\n  if coordinate_order == 'zyx':\n    locations = tf.reverse(locations, axis=[-1])\n\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = tf.pad(data, [[0, 0], [1, 1], [1, 1], [1, 1], [0, 0]], mode='CONSTANT', constant_values=constant_values)\n    locations = locations + 1.0\n\n  if method == 'TRILINEAR':\n    locations = locations * tf.constant([1.0, 1.0, 1.0, 0.0], dtype=locations.dtype)\n    locations = tf.concat([locations, tf.ones_like(locations[..., :1])], axis=-1)\n    locations = tf.transpose(locations, [0, 4, 1, 2, 3])\n\n    output = tf.map_fn(lambda x: tf.image.sample_distorted_bounding_box(x[0], x[1], x[2], x[3], x[4], x[5]),\n                       [data, tf.shape(data)[1:], tf.shape(data)[1:], locations, tf.shape(data)[1:], tf.shape(data)[1:]],\n                       dtype=data.dtype)\n\n  elif method == 'NEAREST':\n    locations = tf.cast(locations, tf.int32)\n    output = tf.gather_nd(data, locations)\n\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    output = output[..., 0]\n\n  if coordinate_order == 'zyx':\n    output = tf.reverse(output, axis=[-1])\n\n  return output\n\n"}
{"namespace": "resample.resample_3d", "completion": "  # Check the input and output shapes\n  assert data.shape[0] == locations.shape[0]\n  assert data.shape[3] == locations.shape[locations.ndim - 1]\n\n  # Determine the coordinate order\n  if coordinate_order == 'xyz':\n    locations = locations.transpose(0, 3, 1, 2)\n  elif coordinate_order == 'zyx':\n    locations = locations.transpose(0, 2, 3, 1)\n  else:\n    raise ValueError('Invalid coordinate_order. Must be either xyz or zyx.')\n\n  # Determine the edge behavior\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    locations = tf.clip_by_value(locations, 0, data.shape[1] - 1)\n    locations = tf.pad(locations, [[0, 0], [1, 1], [1, 1], [1, 1]], mode='CONSTANT', constant_values=constant_values)\n  elif edge_behavior == 'CLAMP':\n    locations = tf.clip_by_value(locations, 0, data.shape[1] - 1)\n  else:\n    raise ValueError('Invalid edge_behavior. Must be either CONSTANT_OUTSIDE or CLAMP.')\n\n  # Determine the half-pixel centering\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  # Perform the resampling\n  if method == 'TRILINEAR':\n    output = tf.nn.trilinear_interpolation(data, locations, name='resample_3d')\n  elif method == 'NEAREST':\n    output = tf.nn.pool(data, locations, 'MAX', 'VALID', 'SAME', name='resample_3d')\n  else:\n    raise ValueError('Invalid method. Must be either TRILINEAR or NEAREST.')\n\n  # Return the output\n  return output"}
{"namespace": "resample.resample_3d", "completion": "  import tensorflow as tf\n  from tensorflow.python.ops import gen_image_ops\n\n  # Convert locations to a tensor if it is a list\n  if isinstance(locations, list):\n    locations = tf.convert_to_tensor(locations)\n\n  # Add a batch dimension to locations if it is 2D\n  if locations.shape.ndims == 2:\n    locations = tf.expand_dims(locations, 0)\n\n  # Check if locations are in zyx order\n  if coordinate_order == 'zyx':\n    locations = tf.reverse(locations, [2])\n\n  # Check if half-pixel centering is used\n  if half_pixel_center:\n    locations = locations - 0.5\n\n  # Check if edge_behavior is set to 'CONSTANT_OUTSIDE'\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    # Pad the input volume with a constant value\n    data = tf.pad(data, [[0, 0], [1, 1], [1, 1], [1, 1], [0, 0]], constant_values=constant_values)\n\n  # Check if the interpolation method is set to 'TRILINEAR'\n  if method == 'TRILINEAR':\n    # Resample the input volume using trilinear interpolation\n    output = gen_image_ops.resampler(data, locations, 'trilinear')\n  else:\n    # Resample the input volume using nearest neighbor interpolation\n    output = gen_image_ops.resampler(data, locations, 'nearest')\n\n  # Check if edge_behavior is set to 'CONSTANT_OUTSIDE'\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    # Remove the padding from the output\n    output = output[:, 1:-1, 1:-1, 1:-1, :]\n\n  # Check if the output is 2D\n  if output.shape.ndims == 4:\n    # Remove the batch dimension from the output\n   "}
{"namespace": "resample.resample_3d", "completion": "  # Convert locations to xyz order if necessary\n  if coordinate_order == 'zyx':\n    locations = tf.transpose(locations, [0, 3, 1, 2])\n\n  # Add half-pixel offset if necessary\n  if half_pixel_center:\n    locations = locations + tf.constant(0.5, dtype=locations.dtype)\n\n  # Handle edge behavior\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data_shape = tf.shape(data)\n    data = tf.pad(data, [[0, 0], [1, 1], [1, 1], [1, 1], [0, 0]], mode='CONSTANT', constant_values=constant_values)\n\n  # Perform trilinear interpolation\n  if method == 'TRILINEAR':\n    # Calculate the fractional offsets from the integer coordinates\n    fractional_offsets = locations - tf.floor(locations)\n\n    # Calculate the integer coordinates\n    integer_coords = tf.floor(locations)\n\n    # Clamp the integer coordinates to the valid range\n    integer_coords = tf.clip_by_value(integer_coords, 0, tf.cast(data_shape[1:4], integer_coords.dtype) - 1)\n\n    # Calculate the trilinear weights\n    weights = (1 - fractional_offsets) * (1 - fractional_offsets) * (1 - fractional_offsets)\n    weights += fractional_offsets * (1 - fractional_offsets) * (1 - fractional_offsets)\n    weights += fractional_offsets * fractional_offsets * (1 - fractional_offsets)\n    weights += fractional_offsets * fractional_offsets * fractional_offsets\n\n    # Gather the values from the input volume at the integer coordinates\n    gathered_values = tf.gather_nd(data, tf.cast(integer_coords, tf.int32))"}
{"namespace": "math.plus_eps", "completion": "  eps = 1e-12\n  if x < eps:\n    return eps\n  else:\n    return x\n"}
{"namespace": "math.plus_eps", "completion": "  eps = 1e-16\n  if x < eps:\n    return eps\n  else:\n    return x"}
{"namespace": "math.plus_eps", "completion": "  eps = 1e-10\n  if x < eps:\n    return eps\n  else:\n    return x\n\n"}
{"namespace": "math.plus_eps", "completion": "  eps = 1e-15\n  if x < eps:\n    x = eps\n  else:\n    x = nextafter(x, inf)\n  return x"}
{"namespace": "math.plus_eps", "completion": "  eps = 1e-20\n  if x < eps:\n    return eps\n  else:\n    return x\n"}
{"namespace": "math.plus_eps", "completion": "  if x < 1e-20:\n    return 1e-20\n  else:\n    return nextafter(x, inf)"}
{"namespace": "math.plus_eps", "completion": "  eps = 1e-12\n  if x < eps:\n    return eps\n  else:\n    return nextafter(x, inf)\n"}
{"namespace": "math.plus_eps", "completion": "  eps = 1e-14\n  if x < eps:\n    return eps\n  return x\n\n"}
{"namespace": "math.plus_eps", "completion": "  eps = 1e-12\n  if x < eps:\n    return eps\n  else:\n    return nextafter(x, inf)\n"}
{"namespace": "math.plus_eps", "completion": "  import numpy as np\n  eps = np.finfo(x.dtype).eps\n  return np.maximum(x, eps)\n"}
{"namespace": "math.plus_eps", "completion": "  eps = 1e-6\n  if x < eps:\n    return eps\n  return nextafter(x, inf)\n"}
{"namespace": "math.plus_eps", "completion": "  # Set the threshold to a very small value\n  threshold = 1e-10\n\n  # Check if x is smaller than the threshold\n  if x < threshold:\n    # If x is smaller, return the threshold\n    return threshold\n  else:\n    # If x is larger, compute the next representable floating-point value\n    return np.nextafter(x, np.inf)\n"}
{"namespace": "math.plus_eps", "completion": "  # Check if x is smaller than a tiny threshold\n  if abs(x) < 1e-16:\n    return 1e-16\n  else:\n    # Compute the next representable floating-point value\n    return float('nextafter', x, float('inf'))\n\n"}
{"namespace": "math.plus_eps", "completion": "  import numpy as np\n\n  # Define the tiny threshold value\n  eps = np.finfo(x.dtype).tiny\n\n  # Adjust x if it is smaller than the threshold\n  x = np.maximum(x, eps)\n\n  # Compute the next representable floating-point value if necessary\n  if np.any(x < eps):\n    x = np.nextafter(x, np.inf)\n\n  return x\n"}
{"namespace": "math.plus_eps", "completion": "  eps = np.finfo(x.dtype).eps\n  if x < eps:\n    return eps\n  else:\n    return np.nextafter(x, np.inf)\n\n"}
{"namespace": "math.plus_eps", "completion": "  # Define a tiny threshold value\n  eps = 1e-15\n\n  # Check if x is smaller than the threshold\n  if x < eps:\n    # If x is smaller, return the threshold value\n    return eps\n  else:\n    # If x is larger or equal to the threshold, return the next representable floating-point value\n    return nextafter(x, np.inf)\n"}
{"namespace": "math.plus_eps", "completion": "  eps = 1e-15\n  if x < eps:\n    x = eps\n  else:\n    x = nextafter(x, inf)\n  return x\n\n"}
{"namespace": "math.plus_eps", "completion": "  eps = 1e-12\n  if x < eps:\n    return eps\n  else:\n    return nextafter(x, np.inf)\n"}
{"namespace": "math.plus_eps", "completion": "  # Initialize the threshold value\n  threshold = 1e-15\n\n  # Check if x is smaller than the threshold\n  if x < threshold:\n    return threshold\n\n  # Compute the next representable floating-point value\n  return x + np.nextafter(0, 1)\n\n"}
{"namespace": "math.plus_eps", "completion": "  import numpy as np\n\n  if x == 0.0:\n    return 1e-10\n  elif x < 1e-10:\n    return 1e-10\n  else:\n    return np.nextafter(x, np.inf)\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x > tiny_val:\n    return x - tiny_val\n  else:\n    return -tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x > tiny_val:\n    return x - tiny_val\n  else:\n    return -tiny_val\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-20\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x > tiny_val:\n    return x - tiny_val\n  else:\n    return -tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-12\n\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  return x - tiny_val if x > tiny_val else -tiny_val\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x > tiny_val:\n    return x - tiny_val\n  else:\n    return -tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  # Define a small value to be used for comparison and adjustment\n  tiny_val = 1e-10\n\n  # Check if the input value is smaller than the tiny value\n  if x < tiny_val:\n    # If the input value is smaller, return the negative tiny value\n    return -tiny_val\n  else:\n    # If the input value is not smaller, return the next smaller floating-point number towards negative infinity\n    return x - tiny_val"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x > tiny_val:\n    return x - tiny_val\n  else:\n    return -tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-15\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-12\n  if x > tiny_val:\n    return x - tiny_val\n  else:\n    return -tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n\n  if x > tiny_val:\n    return x - tiny_val\n  else:\n    return -tiny_val\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-12\n  return x if x > tiny_val else -tiny_val\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  return max(x - tiny_val, -tiny_val)\n\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-12\n  return x if x > tiny_val else -tiny_val\n"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    \"\"\"\n    The helper function computes the safe exponential function for a given input x. It limits the input range to prevent overflow issues.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function.\n    :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    if x > 100:\n      return np.exp(100)\n    elif x < -100:\n      return np.exp(-100)\n    else:\n      return np.exp(x)\n\n  def safe_exp_grad(x):\n    \"\"\"\n    The gradient function for the safe exponential function. It computes the gradient of the safe exponential function with respect to its input x.\n\n    Input-Output Arguments\n    :param x: The input value for which the gradient of the safe exponential function is computed. It is used as the argument for the gradient computation.\n    :return: The gradient of the safe exponential function with respect to its input x. This is used for backpropagation in automatic differentiation contexts.\n    \"\"\"\n    return safe_exp_helper(x)\n\n  return safe_exp_helper(x), safe_exp_grad(x)\n"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    \"\"\"\n    The helper function computes the safe exponential function by limiting the input range to avoid overflow issues.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function.\n    :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    if x > 100:\n      return np.exp(100)\n    elif x < -100:\n      return np.exp(-100)\n    else:\n      return np.exp(x)\n\n  def safe_exp_grad(x):\n    \"\"\"\n    The custom gradient function for the safe exponential function.\n\n    Input-Output Arguments\n    :param x: The input value for which the gradient of the safe exponential function is computed. It is used as the argument for the gradient computation.\n    :return: The gradient of the safe exponential function with respect to the input x. This is used for backpropagation in automatic differentiation contexts.\n    \"\"\"\n    return safe_exp_helper(x)\n\n  return safe_exp_helper(x), safe_exp_grad(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  import tensorflow as tf\n  from tensorflow.python.framework import ops\n  def safe_exp_helper(x):\n    return tf.exp(tf.minimum(x, 100))\n  safe_exp_op = ops.CustomOp(\"SafeExp\", \"safe_exp\", grad_func=lambda op, grad: grad * safe_exp_helper(op.inputs[0]))\n  return safe_exp_op(x)"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    \"\"\"\n    The helper function computes the safe exponential function using a custom gradient function.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function and its custom gradient.\n    :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    return np.exp(np.clip(x, -100, 100))\n\n  return safe_exp_helper(x)\n"}
{"namespace": "math.safe_exp", "completion": "  # Define the range of input values for which the safe exponential function is valid\n  safe_range = (-10, 10)\n\n  # Define the safe exponential function\n  def safe_exp_func(x):\n    if x < safe_range[0]:\n      return np.exp(safe_range[0])\n    elif x > safe_range[1]:\n      return np.exp(safe_range[1])\n    else:\n      return np.exp(x)\n\n  # Define the custom gradient function for the safe exponential function\n  def safe_exp_grad(x):\n    if x < safe_range[0]:\n      return 0\n    elif x > safe_range[1]:\n      return 0\n    else:\n      return np.exp(x)\n\n  # Apply the safe exponential function to the input x and return the result\n  return safe_exp_func(x), safe_exp_grad(x)\n"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    \"\"\"\n    The helper function computes the safe exponential function by limiting the input range to [-100, 100]. It ensures that the exponential function is computed only within this range, preventing overflow issues.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function.\n    :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    return np.exp(np.clip(x, -100, 100))\n\n  return safe_exp_helper(x)\n"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    \"\"\"\n    The helper function computes the safe exponential function for a given input x. It avoids overflow issues by limiting the input range to [-100, 100].\n\n    Input-Output Arguments\n    :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function.\n    :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    if x < -100:\n      return np.exp(-100)\n    elif x > 100:\n      return np.exp(100)\n    else:\n      return np.exp(x)\n\n  # Define the custom gradient function for the safe exponential function\n  def safe_exp_grad(x):\n    \"\"\"\n    The custom gradient function for the safe exponential function. It computes the gradient of the safe exponential function with respect to the input x.\n\n    Input-Output Arguments\n    :param x: The input value for which the gradient of the safe exponential function is computed. It is used as the argument for the gradient computation.\n    :return: The gradient of the safe exponential function with respect to the input x. This is used for backpropagation in automatic differentiation contexts.\n    \"\"\"\n    return safe_exp_helper(x)\n\n  # Create the safe exponential function using the helper function and the custom gradient function\n  safe_exp_func = lambda x: safe_exp_helper(x)\n  safe_exp_func.gradient = safe_exp_grad\n\n  return safe_exp_func(x)"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    \"\"\"\n    The helper function computes the safe exponential function by limiting the input range. It ensures that the input x is within a specified range to prevent overflow errors.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function.\n    :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    if x > 10:\n      return np.exp(10)\n    elif x < -10:\n      return 0\n    else:\n      return np.exp(x)\n\n  def safe_exp_grad(x):\n    \"\"\"\n    The custom gradient function for the safe exponential function. It computes the derivative of the safe exponential function with respect to the input x.\n\n    Input-Output Arguments\n    :param x: The input value for which the custom gradient of the safe exponential function is computed. It is used as the argument for the gradient computation.\n    :return: The result of applying the custom gradient of the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    if x > 10:\n      return np.exp(10)\n    elif x < -10:\n      return 0\n    else:\n      return np.exp(x)\n\n  return safe_exp_helper(x), safe_exp_grad(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    \"\"\"\n    Helper function to compute the safe exponential function.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function and its custom gradient.\n    :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    return np.exp(x) if x < 100 else np.exp(100)\n\n  def safe_exp_grad(x):\n    \"\"\"\n    Custom gradient function for the safe exponential function.\n\n    Input-Output Arguments\n    :param x: The input value for which the custom gradient of the safe exponential function is computed.\n    :return: The gradient of the safe exponential function at the input x.\n    \"\"\"\n    return safe_exp_helper(x)\n\n  return safe_exp_helper(x), safe_exp_grad\n"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    \"\"\"\n    The function computes the safe exponential function by limiting the input range. It uses a custom gradient function for backpropagation in automatic differentiation contexts.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function and its custom gradient.\n    :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    x = tf.where(x > 100, 100, x)\n    x = tf.where(x < -100, -100, x)\n    return tf.exp(x)\n\n  @tf.custom_gradient\n  def safe_exp_custom_gradient(x):\n    \"\"\"\n    The function computes the safe exponential function by limiting the input range and provides a custom gradient function for backpropagation in automatic differentiation contexts.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function and its custom gradient.\n    :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    y = safe_exp_helper(x)\n    def grad(dy):\n      return dy * y\n    return y, grad\n\n  return safe_exp_custom_gradient(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  # Define the maximum and minimum values for the input x\n  min_x = -1000\n  max_x = 1000\n\n  # Define the helper function to generate the safe exponential function\n  def safe_exp_helper(x):\n    if x < min_x:\n      return 0\n    elif x > max_x:\n      return 1\n    else:\n      return np.exp(x)\n\n  # Define the custom gradient function for the safe exponential function\n  def safe_exp_grad(x):\n    if x < min_x:\n      return 0\n    elif x > max_x:\n      return 0\n    else:\n      return np.exp(x)\n\n  # Apply the safe exponential function to the input x\n  y = safe_exp_helper(x)\n\n  # Apply the custom gradient function to the input x\n  dy_dx = safe_exp_grad(x)\n\n  return y, dy_dx"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    \"\"\"\n    The helper function is used to compute the safe exponential function. It takes a single input x and returns the result of applying the safe exponential function to x.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function.\n    :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    if x > 100:\n      return 1e30\n    elif x < -100:\n      return 1e-30\n    else:\n      return np.exp(x)\n\n  def safe_exp_grad(x):\n    \"\"\"\n    The gradient function is used to compute the gradient of the safe exponential function with respect to its input x. It takes a single input x and returns the gradient of the safe exponential function with respect to x.\n\n    Input-Output Arguments\n    :param x: The input value for which the gradient of the safe exponential function is computed. It is used as the argument for the gradient computation.\n    :return: The gradient of the safe exponential function with respect to the input x. This is used for backpropagation in automatic differentiation contexts.\n    \"\"\"\n    return safe_exp_helper(x)\n\n  return safe_exp_helper(x), safe_exp_grad(x)"}
{"namespace": "math.safe_exp", "completion": "  # Define a helper function to generate the safe exponential function\n  def safe_exp_helper(x):\n    # Define the maximum value for the exponential function\n    max_exp = 709.782712893384\n    # Define the minimum value for the exponential function\n    min_exp = -708.3964185322641\n    # Clip the input to the specified range\n    x = np.clip(x, min_exp, max_exp)\n    # Compute the exponential function within the specified range\n    return np.exp(x)\n  # Define the custom gradient function for the safe exponential function\n  def safe_exp_gradient(x):\n    # Compute the gradient of the safe exponential function\n    return safe_exp_helper(x)\n  # Apply the safe exponential function to the input x\n  return safe_exp_helper(x), safe_exp_gradient(x)\n"}
{"namespace": "math.safe_exp", "completion": "  # Define the safe exponential function\n  def safe_exp_helper(x):\n    # Define the maximum value for the exponential function\n    max_exp = 100\n    # Clip the input to the maximum value\n    x = np.minimum(x, max_exp)\n    # Compute the exponential function\n    return np.exp(x)\n\n  # Define the custom gradient function for the safe exponential function\n  def safe_exp_grad(x):\n    # Compute the gradient of the safe exponential function\n    return safe_exp_helper(x)\n\n  # Return the safe exponential function and its custom gradient\n  return safe_exp_helper(x), safe_exp_grad\n"}
{"namespace": "math.safe_exp", "completion": "  return safe_exp_helper(x, 10000, 1e-5)\n"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    \"\"\"\n    The helper function computes the exponential function while limiting the input range. It ensures that the input is within a specified range to prevent overflow errors.\n\n    Input-Output Arguments\n    :param x: The input value for which the exponential function is computed. It is used as the argument for the exponential function.\n    :return: The result of applying the exponential function to the input x, ensuring that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    if x > 100:\n      return np.exp(100)\n    elif x < -100:\n      return np.exp(-100)\n    else:\n      return np.exp(x)\n\n  def safe_exp_grad(x):\n    \"\"\"\n    The custom gradient function for the safe exponential function. It computes the gradient of the safe exponential function with respect to its input.\n\n    Input-Output Arguments\n    :param x: The input value for which the gradient of the safe exponential function is computed. It is used as the argument for the gradient computation.\n    :return: The gradient of the safe exponential function with respect to its input x. This gradient is used for backpropagation in automatic differentiation contexts.\n    \"\"\"\n    return safe_exp_helper(x)\n\n  return safe_exp_helper(x), safe_exp_grad(x)\n"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    \"\"\"\n    The helper function calculates the safe exponential function using a custom gradient function for backpropagation in automatic differentiation contexts.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function and its custom gradient.\n    :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    if x >= 100:\n      return np.exp(100)\n    elif x <= -100:\n      return np.exp(-100)\n    else:\n      return np.exp(x)\n\n  def safe_exp_grad(x):\n    \"\"\"\n    The custom gradient function for the safe exponential function.\n\n    Input-Output Arguments\n    :param x: The input value for which the gradient of the safe exponential function is calculated. It is used as the argument for the gradient of the exponential function.\n    :return: The gradient of the safe exponential function with respect to the input x. This is calculated using the chain rule and the gradient of the exponential function.\n    \"\"\"\n    return safe_exp_helper(x)\n\n  return safe_exp_helper(x), safe_exp_grad\n"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    \"\"\"\n    The helper function is used to compute the safe exponential function. It takes a single input x and returns the result of applying the exponential function to x.\n\n    Input-Output Arguments\n    :param x: The input value for which the exponential function is computed.\n    :return: The result of applying the exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    return np.exp(x) if x < 0 else np.exp(x)\n  return safe_exp_helper(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  # Define the helper function to compute the safe exponential function\n  def safe_exp_helper(x):\n    # Set the maximum value for the exponent\n    max_exp = 1000\n    # Compute the safe exponential function\n    return np.exp(np.minimum(x, max_exp))\n\n  # Define the custom gradient function for the safe exponential function\n  def safe_exp_grad(x):\n    # Compute the gradient of the safe exponential function\n    return safe_exp_helper(x)\n\n  # Apply the helper function to the input x and return the result\n  return safe_exp_helper(x), safe_exp_grad\n"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    \"\"\"\n    The helper function computes the safe exponential function for a given input x. It applies a custom gradient function for backpropagation in automatic differentiation contexts.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function and its custom gradient.\n    :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    # Define the maximum value for the exponential function\n    max_exp = 1000\n\n    # Compute the safe exponential function\n    exp_x = np.exp(np.minimum(x, max_exp))\n\n    # Define the custom gradient function for the safe exponential function\n    def safe_exp_grad(dy):\n      \"\"\"\n      The custom gradient function computes the gradient of the safe exponential function with respect to its input x. It is used for backpropagation in automatic differentiation contexts.\n\n      Input-Output Arguments\n      :param dy: The gradient of the output with respect to the input x. It is used to compute the gradient of the safe exponential function.\n      :return: The gradient of the safe exponential function with respect to the input x. This is computed using the chain rule and the gradient of the exponential function.\n      \"\"\"\n      return dy * exp_x\n\n    # Return the safe exponential function and its custom gradient function\n    return exp_x, safe_exp_grad\n\n  # Return the safe exponential function and its custom gradient function\n  return safe_exp_helper(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log)(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, x)\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log)(x)\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log)(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log, jnp.log,"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, x, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, "}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, x)\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, x, 0, jnp.inf, 1e-15, 1e-15)"}
{"namespace": "math.safe_log", "completion": "  from jax import jit\n  from jax import numpy as jnp\n  from jax.scipy.special import logsumexp\n\n  def safe_log_fn(x):\n    return jnp.log(x)\n\n  safe_log_fn = jit(safe_log_fn)\n\n  return safe_log_fn(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, x, 1e-10, 1e10, 1e-10)\n"}
{"namespace": "math.safe_log", "completion": "  import jax.numpy as jnp\n  from jax import jit\n  from jax.scipy.special import logsumexp\n  from jax.tree_util import tree_map\n  from jax.numpy import isfinite\n  from jax.numpy import log\n  from jax.numpy import logaddexp\n  from jax.numpy import logaddexp2\n  from jax.numpy import log2\n  from jax.numpy import log1p\n  from jax.numpy import log10\n  from jax.numpy import log1p\n  from jax.numpy import log2\n  from jax.numpy import log10\n  from jax.numpy import log1p\n  from jax.numpy import log2\n  from jax.numpy import log10\n  from jax.numpy import log1p\n  from jax.numpy import log2\n  from jax.numpy import log10\n  from jax.numpy import log1p\n  from jax.numpy import log2\n  from jax.numpy import log10\n  from jax.numpy import log1p\n  from jax.numpy import log2\n  from jax.numpy import log10\n  from jax.numpy import log1p\n  from jax.numpy import log2\n  from jax.numpy import log10\n  from jax.numpy import log1p\n  from jax.numpy import log2\n  from jax.numpy import log10\n  from jax.numpy import log1p\n  from jax.numpy import log2\n  from jax.numpy import log10\n  from jax.numpy import log1p\n  from jax.numpy import log2\n  from jax.numpy import log10\n  from jax.numpy import log1p\n  from jax"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, x, bounds=(0, None), derivative_bounds=(0, None), derivative_fn=jnp.log)"}
{"namespace": "math.safe_log", "completion": "  import jax.numpy as jnp\n  from jax import custom_jvp, custom_vjp\n\n  @custom_jvp\n  def safe_log(x):\n      return jnp.log(x)\n\n  @safe_log.defjvp\n  def safe_log_jvp(primals, tangents):\n      x, = primals\n      x_dot, = tangents\n      y = safe_log(x)\n      y_dot = x_dot / x\n      return (y,), (y_dot,)\n\n  @custom_vjp\n  def safe_log_vjp(x):\n      return safe_log(x), lambda _, v: (v / x,)\n\n  safe_log.defvjp(safe_log_vjp)\n\n  return safe_log(x)"}
{"namespace": "math.safe_log", "completion": "  from jax import numpy as jnp\n  from jax.scipy.special import logsumexp\n  from jax.numpy import log, exp, log1p, logaddexp, logaddexp2, logaddexp0\n  from jax.numpy import log1p, logaddexp, logaddexp2, logaddexp0\n  from jax.numpy import log1p, logaddexp, logaddexp2, logaddexp0\n  from jax.numpy import log1p, logaddexp, logaddexp2, logaddexp0\n  from jax.numpy import log1p, logaddexp, logaddexp2, logaddexp0\n  from jax.numpy import log1p, logaddexp, logaddexp2, logaddexp0\n  from jax.numpy import log1p, logaddexp, logaddexp2, logaddexp0\n  from jax.numpy import log1p, logaddexp, logaddexp2, logaddexp0\n  from jax.numpy import log1p, logaddexp, logaddexp2, logaddexp0\n  from jax.numpy import log1p, logaddexp, logaddexp2, logaddexp0\n  from jax.numpy import log1p, logaddexp, logaddexp2, logaddexp0\n  from jax.numpy import log1p, logaddexp, logaddexp2, logaddexp0\n  from jax.numpy import log1p, logaddexp, logaddexp2, logaddexp0\n  from jax.numpy import log1p, logaddexp, logaddexp2, logaddexp0\n  from jax.nump"}
{"namespace": "math.safe_log", "completion": "  import jax.numpy as jnp\n  import jax.numpy.linalg as jla\n  from jax import jit, vmap\n  from jax.scipy.special import logsumexp\n  from jax.scipy.stats import norm\n  from jax.tree_util import tree_map\n  from jax.numpy import exp, log, log1p, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2, logaddexp, logaddexp2"}
{"namespace": "math.safe_log", "completion": "  from jax import jit, lax\n  from jax.numpy import log, isfinite, isinf, isfinite\n  from jax.scipy.special import logsumexp\n  from jax.scipy.special import logit\n  from jax.scipy.special import expit\n  from jax.scipy.special import log_softmax\n  from jax.scipy.special import softmax\n  from jax.scipy.special import log_sigmoid\n  from jax.scipy.special import sigmoid\n  from jax.scipy.special import log_cosh\n  from jax.scipy.special import cosh\n  from jax.scipy.special import log_gamma\n  from jax.scipy.special import gamma\n  from jax.scipy.special import log_beta\n  from jax.scipy.special import beta\n  from jax.scipy.special import log_digamma\n  from jax.scipy.special import digamma\n  from jax.scipy.special import log_gammaln\n  from jax.scipy.special import gammaln\n  from jax.scipy.special import log_ndtr\n  from jax.scipy.special import ndtr\n  from jax.scipy.special import log_ndtri\n  from jax.scipy.special import ndtri\n  from jax.scipy.special import log_erf\n  from jax.scipy.special import erf\n  from jax.scipy.special import log_erfc\n  from jax.scipy.special import erfc\n  from jax.scipy.special import log_erfcx\n  from jax.scipy.special import erfcx\n  from jax.scipy.special import log_erfinv\n  from jax.scipy.special import erfinv\n  from jax.scipy.special import log_erfcinv\n  from jax.scipy."}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, x)\n"}
{"namespace": "math.safe_log", "completion": "  from jax import jit, vmap\n  from jax import numpy as jnp\n  from jax.scipy.special import logsumexp\n  from jax.scipy.special import xlogy\n\n  @jit\n  def _safe_log(x):\n    return jnp.log(x)\n\n  return _safe_log(x)\n"}
{"namespace": "math.safe_log", "completion": "  # Define the safe logarithm function using `generate_safe_fn`\n  safe_log_fn = generate_safe_fn(jnp.log)\n\n  # Apply the safe logarithm function to `x`\n  result = safe_log_fn(x)\n\n  # Return the result of the safe logarithm function\n  return result\n"}
{"namespace": "math.safe_sqrt", "completion": "  max_value = 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"}
{"namespace": "math.safe_sqrt", "completion": "  # Define the maximum value for the input x\n  max_value = 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"}
{"namespace": "math.safe_sqrt", "completion": "  def safe_sqrt_helper(x):\n    \"\"\"\n    This helper function computes the safe square root of the input x. It ensures that the input is clamped between 0 and a maximum value to avoid invalid inputs like negative numbers.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n    :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n    \"\"\"\n    max_value = 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"}
{"namespace": "math.safe_sqrt", "completion": "  def safe_sqrt_func(x):\n    \"\"\"\n    This helper function computes the safe square root of a given input value x. It ensures that the input is clamped between 0 and a maximum value to avoid invalid inputs like negative numbers.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function.\n    :return: The result of applying the safe square root function to the input x.\n    \"\"\"\n    return tf.clip_by_value(tf.sqrt(x), 0, 10)\n\n  def safe_sqrt_grad(op, grad):\n    \"\"\"\n    This helper function computes the gradient of the safe square root function with respect to its input x. It uses the chain rule to combine the gradients of the square root function and the clamping operation.\n\n    Input-Output Arguments\n    :param op: The operation object representing the safe square root function.\n    :param grad: The gradient with respect to the output of the safe square root function.\n    :return: The gradient with respect to the input x of the safe square root function.\n    \"\"\"\n    x = op.inputs[0]\n    return grad / (2 * safe_sqrt_func(x))\n\n  return tf.custom_gradient(safe_sqrt_func, safe_sqrt_grad)"}
{"namespace": "math.safe_sqrt", "completion": "  def safe_sqrt_func(x):\n    \"\"\"\n    This function computes the safe square root of a given input value x. It ensures that the input is clamped between 0 and a maximum value to avoid invalid inputs like negative numbers.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n    :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n    \"\"\"\n    max_val = 1e8\n    x = tf.clip_by_value(x, 0, max_val)\n    return tf.sqrt(x)\n\n  return safe_sqrt_func(x)"}
{"namespace": "math.safe_sqrt", "completion": "  import tensorflow as tf\n  def safe_sqrt_helper(x):\n    \"\"\"\n    This helper function computes the safe square root of the input x. It clamps the input between 0 and a maximum value to handle edge cases like negative numbers. The resulting value is then used as the input for the square root function.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function.\n    :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n    \"\"\"\n    x = tf.clip_by_value(x, 0, 1e10)\n    return tf.sqrt(x)\n\n  return tf.custom_gradient(safe_sqrt_helper)(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  # Define the maximum value for the input\n  max_value = 1e10\n\n  # Clamp the input to be between 0 and the maximum value\n  x = tf.clip_by_value(x, 0, max_value)\n\n  # Define the safe square root function\n  def safe_sqrt_fn(x):\n    return tf.sqrt(x)\n\n  # Define the custom gradient function for the safe square root\n  def safe_sqrt_grad(dy):\n    return dy / (2 * safe_sqrt_fn(x))\n\n  # Apply the safe square root function with the custom gradient\n  return tf.custom_gradient(safe_sqrt_fn, safe_sqrt_grad)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  # Define the maximum value for the input\n  max_value = 100000\n  # Clamp the input between 0 and the maximum value\n  x = tf.clip_by_value(x, 0, max_value)\n  # Define the custom gradient function for the square root function\n  def safe_sqrt_grad(dy):\n    # Compute the gradient of the square root function with respect to the input\n    dx = dy / (2 * tf.sqrt(x))\n    # Clamp the gradient to avoid exploding gradients\n    dx = tf.clip_by_value(dx, -1, 1)\n    return dx\n  # Apply the safe square root function to the input\n  y = tf.sqrt(x)\n  # Apply the custom gradient function to the output\n  y = y + tf.stop_gradient(safe_sqrt_grad(y))\n  return y"}
{"namespace": "math.safe_sqrt", "completion": "  def safe_sqrt_grad(dy):\n    \"\"\"\n    This function computes the gradient of the safe square root function with respect to the input x. It is used for backpropagation in automatic differentiation contexts.\n\n    Input-Output Arguments\n    :param dy: The derivative of the output with respect to the input x. It is used to compute the gradient of the safe square root function.\n    :return: The gradient of the safe square root function with respect to the input x. This is computed using the chain rule and the derivative of the square root function with respect to the input x.\n    \"\"\"\n    return dy / (2 * safe_sqrt(x))\n\n  def safe_sqrt(x):\n    \"\"\"\n    This function computes the safe square root of a given input x. It is used to handle edge cases like negative numbers more gracefully.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n    :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n    \"\"\"\n    return jax.custom_jvp(lambda x: jnp.sqrt(jnp.clip(x, 0, max_value)), x, safe_sqrt_grad)\n\n  return safe_sqrt(x)"}
{"namespace": "math.safe_sqrt", "completion": "  def safe_sqrt_helper(x):\n    \"\"\"\n    This helper function computes the safe square root of the input x. It clamps the input between 0 and a maximum value to avoid invalid inputs like negative numbers. The result is then used as the argument for the square root function and its custom gradient computation.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n    :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n    \"\"\"\n    x = tf.clip_by_value(x, 0, max_value)\n    return tf.sqrt(x)\n\n  # Define the custom gradient function for the safe square root function\n  def safe_sqrt_grad(dy):\n    \"\"\"\n    This custom gradient function computes the gradient of the safe square root function with respect to its input x. It uses the chain rule to combine the gradients of the square root function and the clamping operation. The result is then returned as the gradient for the safe square root function.\n\n    Input-Output Arguments\n    :param dy: The gradient of the output of the safe square root function with respect to the input x. It is used to compute the gradient of the square root function and the clamping operation.\n    :return: The gradient of the safe square root function with respect to its input x. This includes the combination of the gradients of the square root function and the clamping operation.\n    \"\"\"\n    return dy * 0.5 / safe_sqrt_helper(x)\n\n  # Register the custom gradient function for the safe square root function\n  safe_sqrt_helper.def_custom_gradient(safe_sqrt_grad)\n\n  # Return the result of applying the safe square root function to the input x\n  return safe_sqrt_helper(x)"}
{"namespace": "math.safe_sqrt", "completion": "  # Define the maximum value for x\n  max_value = 1000000\n\n  # Define the custom gradient function\n  def custom_gradient(x):\n    # Define the custom gradient function\n    def custom_gradient_function(dy):\n      # Compute the gradient of the square root function\n      dx = 0.5 * dy / safe_sqrt(x)\n      return dx\n\n    # Return the custom gradient function\n    return custom_gradient_function\n\n  # Define the safe square root function\n  def safe_sqrt_function(x):\n    # Clamp the input to be between 0 and the maximum value\n    x = np.clip(x, 0, max_value)\n    # Compute the square root of the clamped input\n    return np.sqrt(x)\n\n  # Return the safe square root function with the custom gradient\n  return jax.custom_jvp(safe_sqrt_function, x)(custom_gradient=custom_gradient)"}
{"namespace": "math.safe_sqrt", "completion": "  # Define a helper function to generate the safe square root function\n  def safe_sqrt_helper(x):\n    # Define a custom gradient function for automatic differentiation\n    def custom_gradient(op, grad):\n      x = op.inputs[0]\n      return grad / (2 * safe_sqrt(x))\n\n    # Apply the safe square root function and attach the custom gradient\n    return tf.sqrt(tf.clip_by_value(x, 0, max_value))\n\n  # Define the maximum value to clip the input\n  max_value = 1000000\n\n  # Apply the helper function to generate the safe square root function\n  return tf.py_function(safe_sqrt_helper, [x], Tout=tf.float32)"}
{"namespace": "math.safe_sqrt", "completion": "  def safe_sqrt_helper(x):\n    \"\"\"\n    This helper function computes the safe square root of a given input value x. It is used within the safe_sqrt function to ensure that the input is clamped between 0 and a maximum value to avoid invalid inputs like negative numbers.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n    :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n    \"\"\"\n    if x < 0:\n      x = 0\n    elif x > max_val:\n      x = max_val\n    return np.sqrt(x)\n\n  def safe_sqrt_grad(x, dy):\n    \"\"\"\n    This helper function computes the gradient of the safe square root function with respect to its input x. It is used within the safe_sqrt function to compute the gradient of the square root function for automatic differentiation purposes.\n\n    Input-Output Arguments\n    :param x: The input value for which the gradient of the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n    :param dy: The derivative of the output of the safe square root function with respect to the input x. This is used to compute the gradient of the safe square root function.\n    :return: The gradient of the safe square root function with respect to its input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n    \"\"\"\n    return dy * 0.5 / safe_sqrt_helper(x)\n\n  return safe_sqrt_helper(x), safe_sqrt_grad\n"}
{"namespace": "math.safe_sqrt", "completion": "  def safe_sqrt_helper(x):\n    \"\"\"\n    This helper function computes the safe square root of a given input value x. It clamps the input between 0 and a maximum value to avoid invalid inputs like negative numbers.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function.\n    :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n    \"\"\"\n    # Define the maximum value to clamp the input between\n    max_value = 1e10\n\n    # Clamp the input between 0 and the maximum value\n    x = tf.clip_by_value(x, 0, max_value)\n\n    # Compute the square root of the input\n    sqrt_x = tf.sqrt(x)\n\n    # Define the custom gradient function for automatic differentiation purposes\n    @tf.custom_gradient\n    def custom_gradient(x):\n      \"\"\"\n      This custom gradient function computes the gradient of the safe square root function with respect to its input x. It is used for automatic differentiation purposes.\n\n      Input-Output Arguments\n      :param x: The input value for which the gradient of the safe square root function will be computed. It is used as the argument for the gradient computation.\n      :return: The gradient of the safe square root function with respect to its input x. This includes the computation of the gradient of the square root function and its chain rule application.\n      \"\"\"\n      # Compute the gradient of the square root function with respect to its input x\n      grad_sqrt_x = tf.gradients(sqrt_x, x)[0]\n\n      # Compute the gradient of the safe square root function with respect to its input x\n      grad_safe_sqrt_x = grad_sqrt_x / (2 * sqrt_x)\n\n      # Return the gradient of the safe square root function with respect to its input x\n      return grad_safe_sqrt_x\n\n    # Return the result of applying the custom gradient function to the square root function\n    return sqrt_"}
{"namespace": "math.safe_sqrt", "completion": "  def safe_sqrt_helper(x):\n    \"\"\"\n    This helper function computes the safe square root of a given input x, ensuring that the input is clamped between 0 and a maximum value. It also computes the custom gradient for backpropagation in automatic differentiation contexts.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n    :return: A tuple containing the result of applying the safe square root function to the input x, and the custom gradient for backpropagation in automatic differentiation contexts.\n    \"\"\"\n    # Compute the square root of x, ensuring that the input is clamped between 0 and a maximum value\n    x = jnp.clip(x, 0, 1e10)\n    y = jnp.sqrt(x)\n\n    # Compute the custom gradient for backpropagation in automatic differentiation contexts\n    def safe_sqrt_grad(dy):\n      \"\"\"\n      This function computes the custom gradient for backpropagation in automatic differentiation contexts. It is used to compute the gradient of the safe square root function with respect to its input x.\n\n      Input-Output Arguments\n      :param dy: The gradient of the output of the safe square root function with respect to its input x.\n      :return: The gradient of the safe square root function with respect to its input x, computed using the custom gradient.\n      \"\"\"\n      return dy / (2 * y)\n\n    return y, safe_sqrt_grad\n\n  # Apply the safe square root function to the input x\n  return jax.custom_vjp(safe_sqrt_helper, x)\n"}
{"namespace": "math.safe_sqrt", "completion": "  def safe_sqrt_helper(x):\n    \"\"\"\n    This helper function computes the safe square root of the input x, ensuring that the input is clamped between 0 and a maximum value. It also computes the custom gradient for backpropagation in automatic differentiation contexts.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n    :return: A tuple containing the result of applying the safe square root function to the input x, and the custom gradient function for backpropagation in automatic differentiation contexts.\n    \"\"\"\n    # Compute the safe square root of x\n    x = jnp.clip(x, 0, 1e10)\n    y = jnp.sqrt(x)\n\n    # Compute the custom gradient for backpropagation\n    @jax.custom_jvp\n    def safe_sqrt_grad(x):\n      \"\"\"\n      This function computes the custom gradient for the safe square root function. It is used in automatic differentiation contexts to compute the gradient of the output with respect to the input.\n\n      Input-Output Arguments\n      :param x: The input value for which the custom gradient will be computed. It is used as the argument for the safe square root function and its gradient computation.\n      :return: The custom gradient of the safe square root function with respect to the input x.\n      \"\"\"\n      return 0.5 * jnp.power(jnp.maximum(x, 0), -0.5)\n\n    return y, safe_sqrt_grad\n\n  # Compute the safe square root of x using the helper function\n  y, safe_sqrt_grad = safe_sqrt_helper(x)\n\n  # Return the result and the custom gradient function\n  return y, safe_sqrt_grad"}
{"namespace": "math.safe_sqrt", "completion": "  # Define a maximum value for the input\n  max_value = 1000000\n\n  # Clamp the input between 0 and the maximum value\n  x = tf.clip_by_value(x, 0, max_value)\n\n  # Define a helper function to compute the safe square root\n  def safe_sqrt_helper(x):\n    # Compute the square root of the input\n    y = tf.sqrt(x)\n\n    # Define a custom gradient for the square root function\n    def safe_sqrt_grad(dy):\n      # Compute the gradient of the square root function\n      dx = dy / (2 * y)\n\n      # Clamp the gradient to a maximum value\n      dx = tf.clip_by_value(dx, -max_value, max_value)\n\n      return dx\n\n    # Return the square root function and its custom gradient\n    return y, safe_sqrt_grad\n\n  # Apply the safe square root function to the input\n  return safe_sqrt_helper(x)\n"}
{"namespace": "math.safe_sqrt", "completion": "  def safe_sqrt_helper(x):\n    \"\"\"\n    This helper function computes the safe square root of a given input x. It clamps the input between 0 and a maximum value to avoid invalid inputs like negative numbers. It also computes the gradient of the safe square root function with respect to the input x, which is used for automatic differentiation purposes.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n    :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n    \"\"\"\n    # Clamp the input between 0 and a maximum value to avoid invalid inputs\n    x = tf.clip_by_value(x, 0, max_value)\n\n    # Compute the safe square root\n    y = tf.sqrt(x)\n\n    # Compute the custom gradient for the safe square root function\n    def custom_gradient(dy):\n      \"\"\"\n      This custom gradient function computes the gradient of the safe square root function with respect to the input x. It includes the chain rule to account for the derivative of the square root function with respect to the input x.\n\n      Input-Output Arguments\n      :param dy: The gradient of the output y with respect to the output of the safe square root function. It is used to compute the gradient of the safe square root function with respect to the input x.\n      :return: The gradient of the safe square root function with respect to the input x. This includes the chain rule to account for the derivative of the square root function with respect to the input x.\n      \"\"\"\n      return dy / (2 * y)\n\n    # Register the custom gradient function with the safe square root function\n    y.set_shape(x.get_shape())\n    y.custom_gradient = custom_gradient\n\n    return y\n\n  # Apply the safe square root function to the input x\n  return safe_sqrt_helper(x)"}
{"namespace": "math.safe_sqrt", "completion": "  def safe_sqrt_func(x):\n    \"\"\"\n    This helper function computes the safe square root of a given input value x. It ensures that the input is clamped between 0 and a maximum value to avoid invalid inputs like negative numbers.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n    :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n    \"\"\"\n    # Clamp the input to a maximum value\n    x = jnp.clip(x, 0, max_value)\n\n    # Compute the square root using JAX's jnp.sqrt function\n    y = jnp.sqrt(x)\n\n    # Define a custom gradient function for the square root operation\n    def safe_sqrt_grad(dy):\n      \"\"\"\n      This function computes the gradient of the safe square root operation with respect to the input x. It uses the chain rule to propagate the gradient through the square root operation.\n\n      Input-Output Arguments\n      :param dy: The gradient with respect to the output of the safe square root operation.\n      :return: The gradient with respect to the input x of the safe square root operation.\n      \"\"\"\n      # Compute the gradient of the square root operation using the chain rule\n      dx = 0.5 * dy / y\n\n      # Return the gradient with respect to the input x\n      return dx\n\n    # Return the result of applying the safe square root function to the input x\n    # along with the custom gradient function\n    return y, safe_sqrt_grad\n\n  # Define the maximum value for the input x\n  max_value = 1e10\n\n  # Compute the safe square root of the input x using the helper function\n  y, safe_sqrt_grad = safe_sqrt_func(x)\n\n  # Return the result of applying the safe square root function to the input x\n  return y\n"}
{"namespace": "math.safe_sqrt", "completion": "  max_value = 1000000000\n  return tf.sqrt(tf.maximum(x, max_value))\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p == 1:\n    return 1\n  elif p == 2:\n    return 2\n  elif p == 3:\n    return 4\n  elif p == 4:\n    return 8\n  elif p == 5:\n    return 16\n  elif p == 6:\n    return 32\n  elif p == 7:\n    return 64\n  elif p == 8:\n    return 128\n  elif p == 9:\n    return 256\n  elif p == 10:\n    return 512\n  elif p == 11:\n    return 1024\n  elif p == 12:\n    return 2048\n  elif p == 13:\n    return 4096\n  elif p == 14:\n    return 8192\n  elif p == 15:\n    return 16384\n  elif p == 16:\n    return 32768\n  elif p == 17:\n    return 65536\n  elif p == 18:\n    return 131072\n  elif p == 19:\n    return 262144\n  elif p == 20:\n    return 524288\n  elif p == 21:\n    return 1048576\n  elif p == 22:\n    return 2097152\n  elif p == 23:\n    return 4194304\n  elif p == 24:\n    return 8388608\n  elif p == 25:\n    return 16777216\n  elif p == 26:\n    return 33554432\n  elif p == 27:\n    return 67108864\n  elif p == 28:\n    return 134217728\n  elif p == 29:\n    return 268435456\n  elif p == 30:\n   "}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p == 1:\n    return float('inf')\n  else:\n    return float('inf')"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p < 0:\n    return 1\n  elif p > 0:\n    return 'infinity'\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 0:\n    return \"Power must be positive\"\n  elif p == 0:\n    return \"Power must be positive\"\n  elif p == 1:\n    return 1\n  elif p > 1:\n    return \"Infinity\""}
{"namespace": "math.power_ladder_max_output", "completion": "  if p > 0:\n    return 'infinity'\n  elif p == 0:\n    return '1'\n  else:\n    return '0'\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  # Initialize the output variable to 0\n  output = 0\n\n  # Use conditions to determine the output based on the value of p\n  if p == 0:\n    # If p is 0, the output is 1\n    output = 1\n  elif p == 1:\n    # If p is 1, the output is infinity\n    output = float('inf')\n  elif p < 0:\n    # If p is negative, the output is 0\n    output = 0\n  else:\n    # If p is positive and not equal to 0 or 1, the output is infinity\n    output = float('inf')\n\n  # Return the output\n  return output"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 0:\n    return 0\n  elif p == 0:\n    return 1\n  elif p == 1:\n    return 2\n  else:\n    return 2**p\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p == 1:\n    return 1\n  elif p > 1:\n    return 1000\n  elif p < 1:\n    return 0\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 0:\n    return \"error\"\n  elif p == 0:\n    return \"infinity\"\n  elif p == 1:\n    return \"infinity\"\n  else:\n    return \"infinity\"\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p > 0:\n    return 1\n  elif p < 0:\n    return 0\n  else:\n    return 1 / 2\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 1:\n    return float('inf')\n  elif p == 1:\n    return 1\n  else:\n    return float('inf')\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  # Check if p is greater than 1\n  if p > 1:\n    return float(\"inf\")\n\n  # Check if p is less than or equal to 1\n  elif p <= 1:\n    return 1\n\n  # Check if p is equal to 1\n  elif p == 1:\n    return 1\n\n  # If none of the conditions are met, return None\n  else:\n    return None\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p == 1:\n    return float('inf')\n  else:\n    return float('inf')\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  # Check if p is less than or equal to 0\n  if p <= 0:\n    # If p is less than or equal to 0, return 0\n    return 0\n  # Check if p is greater than 0 and less than 1\n  elif 0 < p < 1:\n    # If p is greater than 0 and less than 1, return 1\n    return 1\n  # Check if p is equal to 1\n  elif p == 1:\n    # If p is equal to 1, return 2\n    return 2\n  # Check if p is greater than 1\n  elif p > 1:\n    # If p is greater than 1, return 2^p\n    return 2**p\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 0:\n    return 0\n  elif p == 0:\n    return 1\n  elif p == 1:\n    return float('inf')\n  elif p > 1:\n    return float('inf')\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 0:\n    return \"Error: p must be greater than or equal to 0.\"\n  elif p == 0:\n    return 1\n  elif p == 1:\n    return \"Infinity\"\n  else:\n    return \"Infinity\"\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 0:\n    return 0\n  elif p == 0:\n    return 1\n  elif p > 0:\n    return float(\"inf\")\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 1\n  elif p > 1:\n    return 'inf'\n  elif p < 1:\n    return 0\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return \"infinity\"\n  elif p == 0:\n    return 0\n  else:\n    return \"infinity\"\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 1\n  elif p == 0:\n    return 0\n  elif p > 1:\n    return \"infinity\"\n  elif p < 0:\n    return \"infinity\"\n\n"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == \"tetrahedron\":\n    basis = np.array(\n      [\n        [0.57735027, 0.57735027, 0.57735027],\n        [-0.57735027, 0.57735027, 0.57735027],\n        [-0.57735027, -0.57735027, 0.57735027],\n        [0.57735027, -0.57735027, 0.57735027],\n      ]\n    )\n  elif base_shape == \"icosahedron\":\n    basis = np.array(\n      [\n        [0.52573111, 0.52573111, 0.52573111],\n        [0.52573111, 0.52573111, -0.52573111],\n        [0.52573111, -0.52573111, 0.52573111],\n        [0.52573111, -0.52573111, -0.52573111],\n        [-0.52573111, 0.52573111, 0.52573111],\n        [-0.52573111, 0.52573111, -0.52573111],\n        [-0.52573111, -0.52573111, 0.52573111],\n        [-0.52573111, -0.52573111,"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    from .tetrahedron import tetrahedron\n    return tetrahedron(angular_tesselation, remove_symmetries, eps)\n  elif base_shape == 'icosahedron':\n    from .icosahedron import icosahedron\n    return icosahedron(angular_tesselation, remove_symmetries, eps)\n  elif base_shape == 'octahedron':\n    from .octahedron import octahedron\n    return octahedron(angular_tesselation, remove_symmetries, eps)\n  else:\n    raise ValueError(f'Invalid base_shape: {base_shape}')"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == \"tetrahedron\":\n    basis = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [-1, 0, 0], [0, -1, 0], [0, 0, -1]])\n  elif base_shape == \"icosahedron\":\n    basis = np.array(\n        [\n            [0, 0, 1],\n            [0, 1, 0],\n            [1, 0, 0],\n            [0, 0, -1],\n            [0, -1, 0],\n            [-1, 0, 0],\n            [0.57735027, 0.57735027, 0.57735027],\n            [-0.57735027, 0.57735027, 0.57735027],\n            [-0.57735027, -0.57735027, 0.57735027],\n            [0.57735027, -0.57735027, 0.57735027],\n            [0.57735027, 0.57735027, -0.57735027],\n            [-0.57735027, 0.57735027, -0.57735027],\n            [-0.57735027, -0.57735027, -0.57735027],\n            [0.57735027, -0.57735027, -0.57735027],\n        ]\n    )\n  elif base_shape == \"octahedron\":\n    basis = np.array("}
{"namespace": "geopoly.generate_basis", "completion": "  # Check if the base_shape is valid\n  if base_shape not in [\"tetrahedron\", \"icosahedron\", \"octahedron\"]:\n    raise ValueError(\n        \"Invalid base_shape. It must be either 'tetrahedron', 'icosahedron', or 'octahedron'.\"\n    )\n\n  # Define the vertices of the initial polyhedron\n  if base_shape == \"tetrahedron\":\n    vertices = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n  elif base_shape == \"icosahedron\":\n    vertices = np.array(\n        [\n            [0, 0, 1],\n            [0, 0.951056516, -0.309016994],\n            [0, -0.951056516, -0.309016994],\n            [0.809016994, 0, 0.587785252],\n            [-0.809016994, 0, 0.587785252],\n            [0.809016994, 0, -0.587785252],\n            [-0.809016994, 0, -0.587785252],\n            [0.587785252, 0.809016994, 0],\n            [-0.587785252, 0.809016994, 0],\n            [0.587785252, -0.809016994, 0],\n            [-0.587785252, -0.809016994, 0],"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [-1 / 2, -np.sqrt(3) / 2, 0],\n                     [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]])\n  elif base_shape == 'icosahedron':\n    base = np.array([[0, 0, 1], [0, 1, 0], [-1, 0, 0], [0, 0, -1],\n                     [np.sqrt(5) / 2, 0, np.sqrt(5) / 2],\n                     [np.sqrt(5) / 2, 0, -np.sqrt(5) / 2],\n                     [-np.sqrt(5) / 2, 0, np.sqrt(5) / 2],\n                     [-np.sqrt(5) / 2, 0, -np.sqrt(5) / 2],\n                     [0, np.sqrt(5) / 2, np.sqrt(5) / 2],\n                     [0, np.sqrt(5) / 2, -np.sqrt(5) / 2],\n                     [0, -np.sqrt(5) / 2, np.sqrt(5) / 2],\n                     [0, -np.sqrt(5) / 2, -np.sqrt(5) / 2],\n                     [np.sqrt(5) / 2, np.sqrt(5) / 2, 0],\n                     [np.sqrt(5) / 2, -np.sqrt(5) / 2, 0],\n                     [-np.sqrt(5) / 2, np.sqrt(5) / 2, 0],\n                     [-np.sqrt(5) / 2, -np.sqrt(5) / 2, 0]])\n  elif base_shape == 'octahedron':\n    base ="}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == \"tetrahedron\":\n    vertices = np.array([\n      [0.0, 0.0, 1.0],\n      [1.0, 0.0, 0.0],\n      [0.0, 1.0, 0.0],\n      [0.0, 0.0, -1.0]\n    ])\n  elif base_shape == \"icosahedron\":\n    vertices = np.array([\n      [0.0, 0.0, 1.0],\n      [0.0, 0.0, -1.0],\n      [1.0, 0.0, 0.0],\n      [-1.0, 0.0, 0.0],\n      [0.0, 1.0, 0.0],\n      [0.0, -1.0, 0.0],\n      [0.5773502691896257, 0.5773502691896257, 0.5773502691896257],\n      [-0.5773502691896257, 0.5773502691896257, 0.5773502691896257],\n      [0.5773502691896257, -0.5773502691896257, 0.5773502691896257],\n      [-0.5773502691896257, -0.5773502691896257, 0.5773502691896257],\n      [0.5773502691896257, 0.577350269"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base = np.array([[1, 1, 1], [-1, 1, 1], [1, -1, 1], [1, 1, -1]]) / np.sqrt(3)\n  elif base_shape == 'icosahedron':\n    t = (1 + np.sqrt(5)) / 2\n    base = np.array(\n        [\n            [-1, t, 0],\n            [1, t, 0],\n            [-1, -t, 0],\n            [1, -t, 0],\n            [0, -1, t],\n            [0, 1, t],\n            [0, -1, -t],\n            [0, 1, -t],\n            [t, 0, -1],\n            [t, 0, 1],\n            [-t, 0, -1],\n            [-t, 0, 1],\n        ]\n    )\n    base = base / np.sqrt(t**2 + 1)\n  elif base_shape == 'octahedron':\n    base = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [-1, 0, 0], [0, -1, 0], [0, 0, -1]])\n  else:\n    raise ValueError('Invalid base_shape. Must be either \"tetrahedron\", \"icosahedron\", or \"octahedron\".')\n\n  for _ in range(angular_tesselation):\n    new_base = []\n    for i in range(base.shape[0]):\n      for j in range(i + 1, base.shape[0]):\n        v1 = base[i]\n        v2 = base[j]\n        v3 = (v1 + v2) / np.sqrt(2)\n        new_base.append(v1)\n        new_base.append(v2)\n        new_base.append(v3)\n    base = np"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  from scipy.spatial import ConvexHull\n  from scipy.spatial.transform import Rotation\n\n  # Define the vertices of the base shape\n  if base_shape == \"tetrahedron\":\n    vertices = np.array(\n        [\n            [0.0, 0.0, 1.0],\n            [0.0, 1.0, 0.0],\n            [1.0, 0.0, 0.0],\n            [0.0, -1.0, 1.0],\n            [0.0, 1.0, -1.0],\n            [-1.0, 0.0, 0.0],\n        ]\n    )\n  elif base_shape == \"icosahedron\":\n    vertices = np.array(\n        [\n            [0.0, 0.0, 1.0],\n            [0.0, 0.9510565162951535, -0.30901699437494745],\n            [0.0, -0.9510565162951535, -0.30901699437494745],\n            [0.0, 0.9510565162951535, 0.30901699437494745],\n            [0.0, -0.9510565162951535, 0.30901699437494745],\n            [0.8506508083520399, 0.0, 0.5257311121191336],\n            [-0.8506508083520399, 0.0, 0.5257311121"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == \"tetrahedron\":\n    basis = np.array(\n      [\n        [1, 1, 1],\n        [-1, 1, 1],\n        [1, -1, 1],\n        [1, 1, -1],\n      ]\n    )\n  elif base_shape == \"icosahedron\":\n    basis = np.array(\n      [\n        [0, 0, 1],\n        [0, 1, 0],\n        [1, 0, 0],\n        [0, 0, -1],\n        [0, -1, 0],\n        [-1, 0, 0],\n        [1, 1, 1],\n        [-1, 1, 1],\n        [1, -1, 1],\n        [1, 1, -1],\n        [-1, -1, 1],\n        [-1, 1, -1],\n        [1, -1, -1],\n        [-1, -1, -1],\n      ]\n    )\n  elif base_shape == \"octahedron\":\n    basis = np.array(\n      [\n        [1, 0, 0],\n        [-1, 0, 0],\n        [0, 1, 0],\n        [0, -1, 0],\n        [0, 0, 1],\n        [0, 0, -1],\n      ]\n    )\n  else:\n    raise ValueError(\"Invalid base shape. Must be 'tetrahedron', 'icosahedron', or 'octahedron'.\")\n\n  for _ in range(angular_tesselation):\n    basis = tessellate_basis(basis)\n\n  if remove_symmetries:\n    basis = remove_symmetry(basis, eps)\n\n  return basis\n\n"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    basis = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [-1, -1, -1]])\n  elif base_shape == 'icosahedron':\n    basis = np.array([\n      [0, 0, 1],\n      [0, 1, 0],\n      [1, 0, 0],\n      [-1, 0, 0],\n      [0, -1, 0],\n      [0, 0, -1],\n      [1, 1, 1],\n      [-1, 1, 1],\n      [1, -1, 1],\n      [1, 1, -1],\n      [-1, -1, 1],\n      [-1, 1, -1],\n      [1, -1, -1],\n      [-1, -1, -1],\n    ])\n  elif base_shape == 'octahedron':\n    basis = np.array([\n      [0, 0, 1],\n      [0, 1, 0],\n      [1, 0, 0],\n      [-1, 0, 0],\n      [0, -1, 0],\n      [0, 0, -1],\n    ])\n  else:\n    raise ValueError('Invalid base shape.')\n\n  for _ in range(angular_tesselation):\n    basis = tessellate(basis)\n\n  if remove_symmetries:\n    basis = remove_symmetry(basis, eps)\n\n  return basis\n\n"}
{"namespace": "geopoly.generate_basis", "completion": "  # Check if base_shape is valid\n  if base_shape not in [\"tetrahedron\", \"icosahedron\", \"octahedron\"]:\n    raise ValueError(\"Invalid base_shape. Must be either 'tetrahedron', 'icosahedron', or 'octahedron'.\")\n\n  # Check if angular_tesselation is a positive integer\n  if not isinstance(angular_tesselation, int) or angular_tesselation < 1:\n    raise ValueError(\"angular_tesselation must be a positive integer.\")\n\n  # Check if remove_symmetries is a boolean\n  if not isinstance(remove_symmetries, bool):\n    raise ValueError(\"remove_symmetries must be a boolean.\")\n\n  # Check if eps is a positive float\n  if not isinstance(eps, float) or eps <= 0:\n    raise ValueError(\"eps must be a positive float.\")\n\n  # Generate the initial polyhedron\n  if base_shape == \"tetrahedron\":\n    basis = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [-1, -1, -1]])\n  elif base_shape == \"icosahedron\":\n    basis = np.array([\n      [0, 0, 1],\n      [0.95105652, 0, -0.30901699],\n      [0.58778525, 0.80901699, 0],\n      [-0.58778525, 0.80901699, 0],\n      [-0.95105652, 0, -0.30901699],\n      [0, 0.5, 0.8660254],\n      [0.80901699, 0.5, 0.58778525],\n      [-0.80901699"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  import math\n\n  # Define the vertices of the starting polyhedron\n  if base_shape == 'tetrahedron':\n    vertices = np.array([[0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, -1]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + math.sqrt(5)) / 2\n    vertices = np.array([\n        [0, 1, phi], [0, 1, -phi], [0, -1, phi], [0, -1, -phi], [1, phi, 0],\n        [1, -phi, 0], [-1, phi, 0], [-1, -phi, 0], [phi, 0, 1], [-phi, 0, 1],\n        [phi, 0, -1], [-phi, 0, -1]\n    ])\n  elif base_shape == 'octahedron':\n    vertices = np.array([[1, 0, 0], [-1, 0, 0], [0, 1, 0], [0, -1, 0], [0, 0, 1],\n                         [0, 0, -1]])\n  else:\n    raise ValueError(\n        f\"Invalid base_shape: {base_shape}. Supported shapes are 'tetrahedron', 'icosahedron', and 'octahedron'.\"\n    )\n\n  # Tessellate the polyhedron\n  for _ in range(angular_tesselation):\n    new_vertices = []\n    for i in range(len(vertices)):\n      for j in range(i + 1, len(vertices)):\n        midpoint = (vertices[i] + vertices[j]) / 2\n        new_vertices.append(midpoint)\n    vertices = np.vstack((vertices, new_vertices))\n\n  # Remove symmetric basis columns\n  if remove_symmetries:\n   "}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n\n  # Define the vertices of the initial polyhedron\n  if base_shape == \"tetrahedron\":\n    vertices = np.array(\n        [\n            [0, 0, 1],\n            [0, 1, 0],\n            [1, 0, 0],\n            [0, -1 / np.sqrt(2), -1 / np.sqrt(2)],\n        ]\n    )\n  elif base_shape == \"icosahedron\":\n    vertices = np.array(\n        [\n            [0, 0, 1],\n            [0, 1, 0],\n            [1, 0, 0],\n            [0, -1, 0],\n            [0, 0, -1],\n            [0, -1, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, 0, -1],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 1, 0],\n            [1, 0, 0],\n            [0, -1, 0],\n            [0, 0, -1],\n            [0, -1, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, 0, -1],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 1, 0],\n            [1, 0, 0],\n            [0, -1, 0],\n            [0, 0, -1],\n            [0, -1, 0],\n        ]\n    )\n  elif base_shape == \"octahedron\":\n    vertices = np.array(\n        [\n            [0, 0, 1],\n            [0, 1, 0],\n            [1, 0, 0],\n            [0, -1, 0],\n            [0, 0"}
{"namespace": "geopoly.generate_basis", "completion": "  # Check if the base_shape is valid\n  if base_shape not in [\"tetrahedron\", \"icosahedron\", \"octahedron\"]:\n    raise ValueError(\n        \"Invalid base_shape. Please choose either 'tetrahedron', 'icosahedron', or 'octahedron'.\"\n    )\n\n  # Define the vertices and faces for the base shape\n  if base_shape == \"tetrahedron\":\n    vertices = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n    faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == \"icosahedron\":\n    vertices = np.array(\n        [\n            [0, 0, 1],\n            [0, 1, 0],\n            [1, 0, 0],\n            [0, -1, 0],\n            [-1, 0, 0],\n            [0, 0, -1],\n            [0.5, 0.5, 0.5],\n            [-0.5, 0.5, 0.5],\n            [-0.5, -0.5, 0.5],\n            [0.5, -0.5, 0.5],\n            [0.5, 0.5, -0.5],\n            [0.5, -0.5, -0.5],\n            [-0.5, 0.5, -0.5],\n            [-0.5, -0.5, -0.5],\n        ]\n    )\n    faces = np.array(\n        [\n            [0, 1, 2],\n            [0, 1, 4],\n            [0, 2, 5],\n            [0, 4, 5],\n            [1, 2, 6],\n            [1"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n\n  # Check if the base_shape is valid\n  if base_shape not in [\"tetrahedron\", \"icosahedron\", \"octahedron\"]:\n    raise ValueError(\"Invalid base_shape. Must be 'tetrahedron', 'icosahedron', or 'octahedron'.\")\n\n  # Generate the initial shape\n  if base_shape == \"tetrahedron\":\n    basis = np.array([[1, 1, 1], [-1, 1, 1], [-1, -1, 1], [1, -1, 1]]).T\n  elif base_shape == \"icosahedron\":\n    basis = np.array(\n      [\n        [0, 0, 1],\n        [0, 0.942809041582063, 0.333333333333333],\n        [0, -0.942809041582063, 0.333333333333333],\n        [0.816496580927726, 0, 0.577350269189626],\n        [-0.816496580927726, 0, 0.577350269189626],\n        [0.816496580927726, 0, -0.577350269189626],\n        [-0.816496580927726, 0, -0.577350269189626],\n        [0.422618261740699, 0.766044443118978, 0.4830096887152"}
{"namespace": "geopoly.generate_basis", "completion": "  # Check if the base_shape is valid\n  if base_shape not in ['tetrahedron', 'icosahedron', 'octahedron']:\n    raise ValueError(\"Invalid base_shape. Must be one of 'tetrahedron', 'icosahedron', or 'octahedron'.\")\n\n  # Generate the initial polyhedron\n  if base_shape == 'tetrahedron':\n    vertices = np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, -1, -1]])\n  elif base_shape == 'icosahedron':\n    vertices = np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, -1, -1],\n                         [0, 0, -1], [-1, 0, 0], [0, 1, 1], [0, -1, 1],\n                         [1, 0, 1], [-1, 0, -1], [1, 0, -1], [1, 1, 0],\n                         [1, -1, 0], [-1, 1, 0], [-1, -1, 0], [1, 1, -1],\n                         [1, -1, -1], [-1, 1, -1], [-1, -1, -1]])\n  else:\n    vertices = np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, -1, -1],\n                         [0, 0, -1], [-1, 0, 0], [0, 1, 1], [0, -1, 1]])\n\n  # Tessellate the polyhedron\n  for _ in range(angular_tesselation):\n    vertices = tessellate_polyhedron(vertices)\n\n  # Remove symmetries\n  if remove_symmetries:\n    vertices = remove_symmetries_"}
{"namespace": "geopoly.generate_basis", "completion": "  # Check if the input base_shape is valid\n  if base_shape not in [\"tetrahedron\", \"icosahedron\", \"octahedron\"]:\n    raise ValueError(\"Invalid base_shape. Please choose from 'tetrahedron', 'icosahedron', or 'octahedron'.\")\n\n  # Generate the initial basis based on the base_shape\n  if base_shape == \"tetrahedron\":\n    basis = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [-1, -1, -1]])\n  elif base_shape == \"icosahedron\":\n    basis = np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, -1], [0, -1, 0], [-1, 0, 0], [0, 1, 1], [0, -1, 1], [1, 0, 1], [1, 1, 0], [1, -1, 0], [-1, 0, 1], [0, 1, -1], [0, -1, -1], [-1, 0, -1], [-1, 1, 0], [-1, -1, 0], [1, -1, 0], [1, 0, -1], [1, 1, 0], [0, -1, 1], [0, 1, 1], [0, -1, -1], [0, 1, -1], [-1, 0, -1], [-1, 1, 0], [-1, -1, 0], [1, -1, 0], [1, 0, -1], [1, 1, 0]])\n  elif base_shape == \"octahedron\":\n    basis = np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, -1], ["}
{"namespace": "geopoly.generate_basis", "completion": "  assert base_shape in [\n      \"tetrahedron\",\n      \"icosahedron\",\n      \"octahedron\",\n  ], \"base_shape must be either 'tetrahedron', 'icosahedron', or 'octahedron'\"\n  assert (\n      angular_tesselation >= 1\n  ), \"angular_tesselation must be a non-negative integer\"\n  assert isinstance(remove_symmetries, bool), \"remove_symmetries must be a boolean\"\n  assert isinstance(eps, float), \"eps must be a float\"\n\n  if base_shape == \"tetrahedron\":\n    base_vertices = np.array(\n        [\n            [0.0, 0.0, 1.0],\n            [0.0, 1.0, 0.0],\n            [1.0, 0.0, 0.0],\n            [0.0, -1.0, -1.0],\n        ]\n    )\n  elif base_shape == \"icosahedron\":\n    base_vertices = np.array(\n        [\n            [0.0, 0.0, 1.0],\n            [0.0, 1.0, 0.0],\n            [1.0, 0.0, 0.0],\n            [0.0, -1.0, -1.0],\n            [0.0, 1.0, -1.0],\n            [1.0, 0.0, -1.0],\n            [1.0, -1.0, 0.0],\n            [0.0, -1.0, 1.0],\n            [-1.0, 0.0, 1.0],\n            [-1.0, 1.0, 0.0],\n            [-1.0, 0.0, -1.0],\n            [1.0, -1.0, -1.0],\n            [-1.0, 1.0, -1.0],\n            [0.0, 1.0, -"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  from . import tessellate\n  from . import remove_symmetries\n\n  # Generate the initial polyhedron based on the base_shape\n  if base_shape == \"tetrahedron\":\n    vertices = np.array([[1, 1, 1], [-1, 1, -1], [1, -1, -1], [-1, -1, 1]])\n  elif base_shape == \"icosahedron\":\n    vertices = np.array(\n        [\n            [0, 0, 1],\n            [0, 0.942809, 0.333333],\n            [0, -0.471405, -0.881921],\n            [0.816497, 0, 0.57735],\n            [-0.408248, 0.942809, 0.333333],\n            [-0.408248, -0.471405, -0.881921],\n            [0.729847, 0.404509, 0.564273],\n            [-0.264923, 0.942809, 0.211325],\n            [-0.264923, -0.471405, -0.72265],\n            [0.5, 0.309017, -0.801784],\n            [0.264923, 0.942809, -0.211325],\n            [0.264923, -0.471405, 0.72265],\n            [0.5, -0.309017, 0.801784],\n            [-0.5, 0.30"}
{"namespace": "geopoly.generate_basis", "completion": "  import numpy as np\n  from .tessellation import tessellate_polyhedron\n\n  # Tessellate the polyhedron\n  vertices, faces = tessellate_polyhedron(base_shape, angular_tesselation)\n\n  # Compute the basis\n  basis = vertices\n\n  # Remove symmetric basis columns\n  if remove_symmetries:\n    basis = remove_symmetric_columns(basis, eps)\n\n  return basis\n\n"}
{"namespace": "math.safe_log1p", "completion": "  import torch\n  from torch.autograd import Function\n  from torch.autograd.function import once_differentiable\n\n  class SafeLog1p(Function):\n    @staticmethod\n    @once_differentiable\n    def forward(ctx, x):\n      ctx.save_for_backward(x)\n      return torch.log1p(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n      x, = ctx.saved_tensors\n      grad_input = grad_output / (x + 1)\n      return grad_input\n\n  return SafeLog1p.apply(x)"}
{"namespace": "math.safe_log1p", "completion": "  import numpy as np\n  from torch.autograd import Function\n  from torch.autograd.function import once_differentiable\n\n  class SafeLog1p(Function):\n    @staticmethod\n    @once_differentiable\n    def forward(ctx, x):\n      ctx.save_for_backward(x)\n      return np.log1p(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n      x, = ctx.saved_tensors\n      grad_input = grad_output / (1 + x)\n      return grad_input\n\n  return SafeLog1p.apply(x)"}
{"namespace": "math.safe_log1p", "completion": "  import torch\n  from torch.autograd import Function\n  from torch.autograd.function import once_differentiable\n\n  class safe_log1p(Function):\n    @staticmethod\n    @once_differentiable\n    def forward(ctx, x):\n      ctx.save_for_backward(x)\n      return torch.log1p(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n      x = ctx.saved_tensors[0]\n      return grad_output / (1 + x)\n\n  return safe_log1p.apply(x)"}
{"namespace": "math.safe_log1p", "completion": "  from jax import numpy as jnp\n  from jax import custom_jvp\n\n  @custom_jvp\n  def safe_log1p(x):\n    return jnp.log1p(x)\n\n  @safe_log1p.defjvp\n  def safe_log1p_jvp(primals, tangents):\n    x, = primals\n    dx, = tangents\n    return (safe_log1p(x), dx / (1 + x))\n\n  return safe_log1p(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  import torch\n  x = torch.clamp(x, min=-1 + 1e-8, max=1 - 1e-8)\n  return torch.log1p(x)\n"}
{"namespace": "math.safe_log1p", "completion": "  import numpy as np\n  from jax import custom_jvp\n\n  @custom_jvp\n  def safe_log1p_fn(x):\n    return np.log1p(x)\n\n  @safe_log1p_fn.defjvp\n  def safe_log1p_jvp(primals, tangents):\n    x, = primals\n    dx, = tangents\n    return (safe_log1p_fn(x), (1.0 + np.exp(x)) * dx)\n\n  return safe_log1p_fn(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  import jax.numpy as jnp\n  from jax import custom_jvp\n\n  @custom_jvp\n  def safe_log1p_op(x):\n    return jnp.log1p(x)\n\n  @safe_log1p_op.defjvp\n  def safe_log1p_jvp(primals, tangents):\n    x, = primals\n    dx, = tangents\n    return (safe_log1p_op(x), dx / (1 + x))\n\n  return safe_log1p_op(x)\n"}
{"namespace": "math.safe_log1p", "completion": "  import numpy as np\n  from torch.autograd import Function\n\n  class SafeLog1p(Function):\n    @staticmethod\n    def forward(ctx, x):\n      ctx.save_for_backward(x)\n      return np.log1p(x.data)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n      x, = ctx.saved_tensors\n      grad_x = grad_output / (1 + x.data)\n      return grad_x\n\n  return SafeLog1p.apply(x)"}
{"namespace": "math.safe_log1p", "completion": "  import torch\n  import torch.nn as nn\n  import torch.autograd as autograd\n  import torch.nn.functional as F\n  from torch.autograd import Function\n\n  class SafeLog1p(Function):\n    @staticmethod\n    def forward(ctx, x):\n      ctx.save_for_backward(x)\n      return torch.log1p(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n      x, = ctx.saved_tensors\n      grad_x = grad_output / (1 + x)\n      return grad_x\n\n  return SafeLog1p.apply(x)"}
{"namespace": "math.safe_log1p", "completion": "  import torch\n  import torch.nn.functional as F\n  x = torch.clamp(x, min=-1 + 1e-6, max=1 - 1e-6)\n  return torch.log1p(x)\n"}
{"namespace": "math.safe_log1p", "completion": "  import numpy as np\n  # Define the safe range for x\n  safe_range = np.logical_and(x >= -1, x <= 1)\n  # Compute the safe log1p operation for values within the safe range\n  safe_log1p = np.where(safe_range, np.log1p(x), x)\n  # Compute the derivative of the safe log1p operation for values within the safe range\n  safe_log1p_derivative = np.where(safe_range, 1 / (1 + x), 1)\n  return safe_log1p, safe_log1p_derivative"}
{"namespace": "math.safe_log1p", "completion": "  import numpy as np\n  from jax import custom_jvp\n  from jax import jit\n  from jax import lax\n\n  @custom_jvp\n  def log1p(x):\n    return np.log1p(x)\n\n  @log1p.defjvp\n  def log1p_jvp(primals, tangents):\n    x, = primals\n    x_dot, = tangents\n    y = np.log1p(x)\n    y_dot = x_dot / (1 + x)\n    return (y,), (y_dot,)\n\n  return jit(log1p)(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  import tensorflow as tf\n  from tensorflow.python.ops.gen_math_ops import _log1p\n  from tensorflow.python.framework import dtypes\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import array_ops\n  from tensorflow.python.framework import ops\n  from tensorflow.python.ops import control_flow_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import array_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import control_flow_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import array_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import control_flow_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import array_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import control_flow_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import array_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import control_flow_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import array_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import control_flow_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import array_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import control_flow_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import array_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import control_flow_ops\n  from tensorflow.python.ops import math_ops\n  from tensorflow.python.ops import array_ops\n  from"}
{"namespace": "math.safe_log1p", "completion": "  # Define the safe range for x\n  safe_min = -1e-6\n  safe_max = 1 - 1e-6\n\n  # Clip x to the safe range\n  x = jnp.clip(x, safe_min, safe_max)\n\n  # Compute the result of the safe log1p operation\n  result = jnp.log1p(x)\n\n  # Define the derivative of the safe log1p operation\n  def safe_log1p_derivative(x):\n    return 1 / (1 + x)\n\n  # Return the result and the derivative of the safe log1p operation\n  return result, safe_log1p_derivative(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  from jax import numpy as jnp\n  from jax import custom_jvp\n  from jax import custom_vjp\n\n  @custom_jvp\n  def safe_log1p(x):\n    return jnp.log1p(x)\n\n  def safe_log1p_fwd(x):\n    return safe_log1p(x), x\n\n  @safe_log1p.defjvp\n  def safe_log1p_jvp(primals, tangents):\n    x, = primals\n    dx, = tangents\n    return (safe_log1p(x),), (jnp.exp(x) * dx,)\n\n  @custom_vjp\n  def safe_log1p_vjp(res, tangents):\n    y, = res\n    dy, = tangents\n    return (jnp.exp(y) * dy,)\n\n  safe_log1p.defvjp(safe_log1p_vjp, safe_log1p_fwd)\n\n  return safe_log1p(x)\n"}
{"namespace": "math.safe_log1p", "completion": "  import numpy as np\n  from numba import njit\n\n  @njit\n  def safe_log1p_numba(x):\n    \"\"\"\n    This function calculates the natural logarithm of 1 plus the input value, x. It ensures that the input value is within a safe range to avoid numerical errors or undefined behavior. The function also specifies how the derivative of this operation should be computed for automatic differentiation.\n\n    Input-Output Arguments\n    :param x: The input value for which the natural logarithm of 1 plus x is computed. \n    :return: The result of the safe log1p operation on the input value x.\n    \"\"\"\n    if x > 0:\n      return np.log1p(x)\n    elif x == 0:\n      return 0\n    else:\n      return np.log1p(x) - np.log(1 - x)\n\n  return safe_log1p_numba(x)\n"}
{"namespace": "math.safe_log1p", "completion": "  import numpy as np\n  from ml_collections import config_dict\n  cfg = config_dict.ConfigDict()\n  cfg.epsilon = 1e-8\n  cfg.min_value = 1e-10\n  cfg.max_value = 1e10\n\n  x = np.clip(x, cfg.min_value, cfg.max_value)\n  return np.log1p(x)\n"}
{"namespace": "math.safe_log1p", "completion": "  import numpy as np\n  from . import safe_exp\n\n  # Define the safe range for the input value\n  safe_range = np.array([-100, 100])\n\n  # Clip the input value to the safe range\n  x = np.clip(x, safe_range[0], safe_range[1])\n\n  # Compute the safe log1p operation\n  result = np.log1p(x)\n\n  # Define the derivative of the safe log1p operation\n  def safe_log1p_derivative(x):\n    return 1 / (x + 1)\n\n  # Compute the derivative of the safe log1p operation\n  derivative = safe_log1p_derivative(x)\n\n  # Return the result and the derivative of the safe log1p operation\n  return result, derivative\n\n"}
{"namespace": "math.safe_log1p", "completion": "  import numpy as np\n  import tensorflow as tf\n\n  def log1p(x):\n    return tf.math.log1p(x)\n\n  def log1p_derivative(x):\n    return 1 / (1 + x)\n\n  # Define a safe range for the input values\n  safe_range = (-1, 1)\n\n  # Clip the input values to the safe range\n  x = tf.clip_by_value(x, *safe_range)\n\n  # Compute the result of the safe log1p operation\n  result = log1p(x)\n\n  # Compute the derivative of the safe log1p operation\n  derivative = log1p_derivative(x)\n\n  # Return the result and the derivative of the safe log1p operation\n  return result, derivative\n"}
{"namespace": "math.safe_log1p", "completion": "  # Check if the input value is within a safe range\n  if x < -1:\n    # If the input value is less than -1, return a large negative value\n    return -1e10\n  elif x > 1:\n    # If the input value is greater than 1, return a large positive value\n    return 1e10\n  else:\n    # If the input value is within the safe range, calculate the natural logarithm of 1 plus x\n    return jnp.log1p(x)\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = np.log(x)\n  elif p == -np.inf:\n    y = np.log10(x)\n  elif p == np.inf:\n    y = np.sqrt(x)\n  else:\n    y = np.power(x, p)\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    x = np.log10(x)\n  elif p == 0:\n    x = np.log2(x)\n  elif p == -np.inf:\n    x = np.log(x)\n  elif p == np.inf:\n    x = np.sqrt(x)\n  else:\n    x = np.power(x, p)\n\n  if postmult is not None:\n    x = x * postmult\n\n  return x"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = np.log10(x)\n  elif p == -np.inf:\n    y = np.log(x)\n  elif p == np.inf:\n    y = np.sqrt(x)\n  else:\n    y = np.sign(x) * np.abs(x) ** p\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n\n  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = np.log(x)\n  elif p == 0:\n    y = np.sign(x) * np.log(np.abs(x) + 1)\n  elif p == -np.inf:\n    y = np.sign(x) * np.log(np.abs(x))\n  elif p == np.inf:\n    y = np.sign(x) * np.log(np.abs(x))\n  else:\n    y = np.sign(x) * (np.abs(x) ** p - 1) / p\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y"}
{"namespace": "math.power_ladder", "completion": "  # Check if p is 1, 0, -inf, or inf\n  if p == 1:\n    return x\n  elif p == 0:\n    return np.log(x)\n  elif np.isinf(p):\n    return np.sign(x) * np.log(np.abs(x))\n\n  # Apply pre-multiplication if provided\n  if premult is not None:\n    x *= premult\n\n  # Apply Tukey's power ladder transformation\n  if p > 0:\n    x = np.sign(x) * np.power(np.abs(x), p)\n  else:\n    x = np.sign(x) * np.power(np.abs(x), -p)\n\n  # Apply post-multiplication if provided\n  if postmult is not None:\n    x *= postmult\n\n  return x"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n\n  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = np.log(x)\n  elif p == -np.inf:\n    y = np.log(x)\n    y[y < 0] = np.nan\n  elif p == np.inf:\n    y = np.exp(x)\n  else:\n    y = np.sign(x) * np.abs(x) ** p\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n\n  # Handle special cases for p\n  if p == 1:\n    return x\n  elif p == 0:\n    return np.log(x)\n  elif p == -np.inf:\n    return np.log(x) ** -1\n  elif p == np.inf:\n    return np.exp(x)\n\n  # Apply pre-multiplication if provided\n  if premult is not None:\n    x = x * premult\n\n  # Apply Tukey's power ladder transformation\n  if p > 0:\n    x = np.sign(x) * (np.abs(x) ** p)\n  else:\n    x = np.sign(x) * (np.abs(x) ** (-p))\n\n  # Apply post-multiplication if provided\n  if postmult is not None:\n    x = x * postmult\n\n  return x"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n\n  # Apply pre-multiplication if provided\n  if premult is not None:\n    x = x * premult\n\n  # Handle special cases for p\n  if p == 1:\n    # No transformation for p = 1\n    pass\n  elif p == 0:\n    # Transform to log scale for p = 0\n    x = np.log(x)\n  elif p == -np.inf:\n    # Transform to reciprocal scale for p = -inf\n    x = 1 / x\n  elif p == np.inf:\n    # Transform to square root scale for p = inf\n    x = np.sqrt(x)\n  else:\n    # Apply Tukey's power ladder transformation for general p values\n    x = np.sign(x) * np.abs(x) ** p\n\n  # Apply post-multiplication if provided\n  if postmult is not None:\n    x = x * postmult\n\n  return x"}
{"namespace": "math.power_ladder", "completion": "  # Special cases for p\n  if p == 1:\n    return x\n  elif p == 0:\n    return np.log(x)\n  elif np.isinf(p):\n    return np.sign(x) * np.log(np.abs(x))\n\n  # Apply pre-multiplication if provided\n  if premult is not None:\n    x = x * premult\n\n  # Apply Tukey's power ladder transformation\n  if p > 0:\n    x = np.sign(x) * (np.abs(x) ** p)\n  else:\n    x = np.sign(x) * (np.abs(x) ** (-p))\n\n  # Apply post-multiplication if provided\n  if postmult is not None:\n    x = x * postmult\n\n  return x"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n\n  # Check if p is 1, 0, -inf, or inf\n  if p == 1:\n    return x\n  elif p == 0:\n    return np.log(x)\n  elif p == -np.inf:\n    return np.log(x) / np.log(np.min(x))\n  elif p == np.inf:\n    return np.log(x) / np.log(np.max(x))\n\n  # Apply pre-multiplication if provided\n  if premult is not None:\n    x = x * premult\n\n  # Apply Tukey's power ladder transformation\n  if p > 0:\n    x = np.power(x, p)\n  elif p < 0:\n    x = np.power(x, 1 / p)\n\n  # Apply post-multiplication if provided\n  if postmult is not None:\n    x = x * postmult\n\n  return x"}
{"namespace": "math.power_ladder", "completion": "  # Handle special cases for p\n  if p == 1:\n    return x\n  elif p == 0:\n    return x ** 0\n  elif p == -float('inf'):\n    return x ** -float('inf')\n  elif p == float('inf'):\n    return x ** float('inf')\n\n  # Apply pre-multiplication if provided\n  if premult is not None:\n    x = x * premult\n\n  # Apply Tukey's power ladder transformation\n  if p > 0:\n    x = x ** p\n  elif p < 0:\n    x = x ** -p\n\n  # Apply post-multiplication if provided\n  if postmult is not None:\n    x = x * postmult\n\n  return x"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    # no-op\n    pass\n  elif p == 0:\n    # log\n    x = np.log(x)\n  elif p == -np.inf:\n    # reciprocal\n    x = 1.0 / x\n  elif p == np.inf:\n    # square root\n    x = np.sqrt(x)\n  else:\n    # general power\n    x = np.power(x, p)\n\n  if postmult is not None:\n    x = x * postmult\n\n  return x"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n\n  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = np.log(x)\n  elif p == -np.inf:\n    y = np.exp(x)\n  elif p == np.inf:\n    y = np.sign(x) * np.abs(x) ** (1 / x)\n  else:\n    y = x ** p\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y\n"}
{"namespace": "math.power_ladder", "completion": "  # Handle special cases for p\n  if p == 1:\n    return x\n  elif p == 0:\n    return np.log(x)\n  elif p == -np.inf:\n    return np.log(x)\n  elif p == np.inf:\n    return np.exp(x)\n\n  # Apply pre-multiplication if provided\n  if premult is not None:\n    x = x * premult\n\n  # Apply Tukey's power ladder transformation\n  if p > 0:\n    x = np.power(x, p)\n  elif p < 0:\n    x = np.power(x, p)\n\n  # Apply post-multiplication if provided\n  if postmult is not None:\n    x = x * postmult\n\n  return x\n\n"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n  from scipy.special import gamma\n\n  # Apply pre-multiplication if provided\n  if premult is not None:\n    x *= premult\n\n  # Handle special cases for p\n  if p == 1:\n    # Identity transformation\n    pass\n  elif p == 0:\n    # Logarithmic transformation\n    x = np.log(x)\n  elif p == -np.inf:\n    # Inverse transformation\n    x = 1 / x\n  elif p == np.inf:\n    # Square root transformation\n    x = np.sqrt(x)\n  else:\n    # General power transformation\n    x = np.power(x, p)\n\n  # Apply post-multiplication if provided\n  if postmult is not None:\n    x *= postmult\n\n  return x"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n\n  if p == 1:\n    # Handle the special case of p=1\n    x = np.sqrt(x)\n  elif p == 0:\n    # Handle the special case of p=0\n    x = np.log(x)\n  elif p == -np.inf:\n    # Handle the special case of p=-inf\n    x = 1 / x\n  elif p == np.inf:\n    # Handle the special case of p=inf\n    x = np.exp(x)\n  else:\n    # Apply the standard power ladder transformation\n    x = np.sign(x) * np.abs(x) ** p\n\n  if postmult is not None:\n    x *= postmult\n\n  return x"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    # Special case for p = 1: no transformation is required\n    pass\n  elif p == 0:\n    # Special case for p = 0: the transformation is log(x)\n    x = np.log(x)\n  elif np.isinf(p):\n    # Special case for p = inf: the transformation is 1/x\n    x = 1 / x\n  else:\n    # For general cases, apply Tukey's power ladder transformation\n    x = np.sign(x) * np.power(np.abs(x), p)\n\n  if postmult is not None:\n    x = x * postmult\n\n  return x\n\n"}
{"namespace": "math.power_ladder", "completion": "  import numpy as np\n  from scipy.special import gamma\n\n  # Handle special cases for p values\n  if p == 1:\n    return np.log(x) if premult is None else np.log(premult * x)\n  elif p == 0:\n    return x if premult is None else premult * x\n  elif np.isinf(p):\n    return np.power(x, p) if premult is None else np.power(premult * x, p)\n  else:\n    # Apply Tukey's power ladder transformation\n    if premult is not None:\n      x = premult * x\n    if postmult is not None:\n      return postmult * np.power(x, p)\n    else:\n      return np.power(x, p)\n\n"}
{"namespace": "math.power_ladder", "completion": "  # Handle special cases for p values\n  if p == 1:\n    # Power ladder with p = 1 is equivalent to taking the square root of x\n    return np.sqrt(x)\n  elif p == 0:\n    # Power ladder with p = 0 is equivalent to taking the absolute value of x\n    return np.abs(x)\n  elif p == -np.inf:\n    # Power ladder with p = -inf is equivalent to taking the reciprocal of x\n    return 1 / x\n  elif p == np.inf:\n    # Power ladder with p = inf is equivalent to taking the reciprocal of the square root of x\n    return 1 / np.sqrt(x)\n\n  # Apply pre-multiplication if provided\n  if premult is not None:\n    x = x * premult\n\n  # Apply Tukey's power ladder transformation\n  if p > 0:\n    # Positive power\n    x = np.sign(x) * np.power(np.abs(x), p)\n  else:\n    # Negative power\n    x = np.sign(x) * np.power(np.abs(x), -p)\n\n  # Apply post-multiplication if provided\n  if postmult is not None:\n    x = x * postmult\n\n  return x\n\n"}
{"namespace": "math.power_ladder", "completion": "  # If the input x is a single value, convert it to a list\n  if not isinstance(x, list):\n    x = [x]\n\n  # If pre-multiplication is specified, apply it to the input data\n  if premult is not None:\n    x = [premult * val for val in x]\n\n  # Handle special cases for the power parameter p\n  if p == 1:\n    # For p = 1, the transformation is just the identity function\n    y = x\n  elif p == 0:\n    # For p = 0, the transformation is log(x)\n    y = [np.log(val) for val in x]\n  elif p == -np.inf:\n    # For p = -inf, the transformation is 1 / x\n    y = [1 / val for val in x]\n  elif p == np.inf:\n    # For p = inf, the transformation is 1 / (1 - x)\n    y = [1 / (1 - val) for val in x]\n  else:\n    # For other values of p, apply the Tukey's power ladder transformation\n    y = [np.sign(val) * (abs(val) ** p) for val in x]\n\n  # If post-multiplication is specified, apply it to the transformed data\n  if postmult is not None:\n    y = [postmult * val for val in y]\n\n  # If the input x was a single value, return a single value\n  if len(x) == 1:\n    return y[0]\n  else:\n    # Otherwise, return the transformed data as a list\n    return y"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  if postmult is not None:\n    y = y * postmult\n  if p == 0:\n    return y\n  elif p == 1:\n    return 1 / y\n  elif p == 2:\n    return y ** -2\n  elif p == 3:\n    return 1 / (y ** 3)\n  elif p == 4:\n    return y ** -4\n  elif p == 5:\n    return 1 / (y ** 5)\n  elif p == 6:\n    return y ** -6\n  elif p == 7:\n    return 1 / (y ** 7)\n  elif p == 8:\n    return y ** -8\n  elif p == 9:\n    return 1 / (y ** 9)\n  elif p == 10:\n    return y ** -10\n  elif p == 11:\n    return 1 / (y ** 11)\n  elif p == 12:\n    return y ** -12\n  elif p == 13:\n    return 1 / (y ** 13)\n  elif p == 14:\n    return y ** -14\n  elif p == 15:\n    return 1 / (y ** 15)\n  elif p == 16:\n    return y ** -16\n  elif p == 17:\n    return 1 / (y ** 17)\n  elif p == 18:\n    return y ** -18\n  elif p == 19:\n    return 1 / (y ** 19)\n  elif p == 20:\n    return y ** -20\n  elif p == 21:\n    return 1 / (y ** 21)\n  elif p == 22:\n    return y ** -22\n  elif p == 23:\n    return 1 / (y ** 23)\n  elif p == 24:\n    return y ** -24\n  elif p == 25:\n    return 1 / (y ** 25)\n  elif p == 26:\n    return y **"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y *= premult\n  if postmult is not None:\n    y *= postmult\n\n  if p == 1:\n    return y\n  elif p == 2:\n    return y**2\n  elif p == 3:\n    return y**3\n  elif p == 4:\n    return y**4\n  elif p == 5:\n    return y**5\n  elif p == 6:\n    return y**6\n  elif p == 7:\n    return y**7\n  elif p == 8:\n    return y**8\n  elif p == 9:\n    return y**9\n  elif p == 10:\n    return y**10\n  elif p == 11:\n    return y**11\n  elif p == 12:\n    return y**12\n  elif p == 13:\n    return y**13\n  elif p == 14:\n    return y**14\n  elif p == 15:\n    return y**15\n  elif p == 16:\n    return y**16\n  elif p == 17:\n    return y**17\n  elif p == 18:\n    return y**18\n  elif p == 19:\n    return y**19\n  elif p == 20:\n    return y**20\n  elif p == 21:\n    return y**21\n  elif p == 22:\n    return y**22\n  elif p == 23:\n    return y**23\n  elif p == 24:\n    return y**24\n  elif p == 25:\n    return y**25\n  elif p == 26:\n    return y**26\n  elif p == 27:\n    return y**27\n  elif p == 28:\n    return y**28\n  elif p == 29:\n    return y**29\n  elif p == 30:\n    return y**30\n  elif p == 31:\n    return y**31\n  elif p == 32:\n    return y**32\n  elif p == 33"}
{"namespace": "math.inv_power_ladder", "completion": "  if postmult is not None:\n    y = y * postmult\n  if p == 2:\n    y = y ** (1.0 / 2.0)\n  elif p == 3:\n    y = y ** (1.0 / 3.0)\n  elif p == 4:\n    y = y ** (1.0 / 4.0)\n  elif p == 5:\n    y = y ** (1.0 / 5.0)\n  elif p == 6:\n    y = y ** (1.0 / 6.0)\n  elif p == 7:\n    y = y ** (1.0 / 7.0)\n  elif p == 8:\n    y = y ** (1.0 / 8.0)\n  elif p == 9:\n    y = y ** (1.0 / 9.0)\n  elif p == 10:\n    y = y ** (1.0 / 10.0)\n  elif p == 11:\n    y = y ** (1.0 / 11.0)\n  elif p == 12:\n    y = y ** (1.0 / 12.0)\n  elif p == 13:\n    y = y ** (1.0 / 13.0)\n  elif p == 14:\n    y = y ** (1.0 / 14.0)\n  elif p == 15:\n    y = y ** (1.0 / 15.0)\n  elif p == 16:\n    y = y ** (1.0 / 16.0)\n  elif p == 17:\n    y = y ** (1.0 / 17.0)\n  elif p == 18:\n    y = y ** (1.0 / 18.0)\n  elif p == 19:\n    y = y ** (1.0 / 19.0)\n  elif p == 20:\n    y = y ** (1.0 / 20.0)\n  elif p == 21:\n    y = y ** (1.0 / 21.0"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  if postmult is not None:\n    y = y / postmult\n  if p == 0:\n    return y\n  elif p == 1:\n    return 1 / y\n  elif p == 2:\n    return y**-2\n  elif p == 3:\n    return 1 / (y**3)\n  elif p == 4:\n    return y**-4\n  elif p == 5:\n    return 1 / (y**5)\n  elif p == 6:\n    return y**-6\n  elif p == 7:\n    return 1 / (y**7)\n  elif p == 8:\n    return y**-8\n  elif p == 9:\n    return 1 / (y**9)\n  elif p == 10:\n    return y**-10\n  elif p == 11:\n    return 1 / (y**11)\n  elif p == 12:\n    return y**-12\n  elif p == 13:\n    return 1 / (y**13)\n  elif p == 14:\n    return y**-14\n  elif p == 15:\n    return 1 / (y**15)\n  elif p == 16:\n    return y**-16\n  elif p == 17:\n    return 1 / (y**17)\n  elif p == 18:\n    return y**-18\n  elif p == 19:\n    return 1 / (y**19)\n  elif p == 20:\n    return y**-20\n  elif p == 21:\n    return 1 / (y**21)\n  elif p == 22:\n    return y**-22\n  elif p == 23:\n    return 1 / (y**23)\n  elif p == 24:\n    return y**-24\n  elif p == 25:\n    return 1 / (y**25)\n  elif p == 26:\n    return y**-26\n  elif p == 27:"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 0:\n    return y\n  elif p == 1:\n    return y**(1/p)\n  elif p == 2:\n    return y**(1/p)\n  elif p == 3:\n    return y**(1/p)\n  elif p == 4:\n    return y**(1/p)\n  elif p == 5:\n    return y**(1/p)\n  elif p == 6:\n    return y**(1/p)\n  elif p == 7:\n    return y**(1/p)\n  elif p == 8:\n    return y**(1/p)\n  elif p == 9:\n    return y**(1/p)\n  elif p == 10:\n    return y**(1/p)\n  elif p == 11:\n    return y**(1/p)\n  elif p == 12:\n    return y**(1/p)\n  elif p == 13:\n    return y**(1/p)\n  elif p == 14:\n    return y**(1/p)\n  elif p == 15:\n    return y**(1/p)\n  elif p == 16:\n    return y**(1/p)\n  elif p == 17:\n    return y**(1/p)\n  elif p == 18:\n    return y**(1/p)\n  elif p == 19:\n    return y**(1/p)\n  elif p == 20:\n    return y**(1/p)\n  elif p == 21:\n    return y**(1/p)\n  elif p == 22:\n    return y**(1/p)\n  elif p == 23:\n    return y**(1/p)\n  elif p == 24:\n    return y**(1/p)\n  elif p == 25:\n    return y**(1/p)\n  elif p == 26:\n    return y**(1/p)\n  elif p == 27:\n    return y**(1/p)\n  elif p == 28:\n    return y"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  if postmult is not None:\n    y = y * postmult\n\n  if p == 0:\n    return y\n  elif p == 1:\n    return 1.0 / y\n  elif p == 2:\n    return y ** -2\n  elif p == 3:\n    return 1.0 / y ** 3\n  elif p == 4:\n    return y ** -4\n  elif p == 5:\n    return 1.0 / y ** 5\n  elif p == 6:\n    return y ** -6\n  elif p == 7:\n    return 1.0 / y ** 7\n  elif p == 8:\n    return y ** -8\n  elif p == 9:\n    return 1.0 / y ** 9\n  elif p == 10:\n    return y ** -10\n  elif p == 11:\n    return 1.0 / y ** 11\n  elif p == 12:\n    return y ** -12\n  elif p == 13:\n    return 1.0 / y ** 13\n  elif p == 14:\n    return y ** -14\n  elif p == 15:\n    return 1.0 / y ** 15\n  elif p == 16:\n    return y ** -16\n  elif p == 17:\n    return 1.0 / y ** 17\n  elif p == 18:\n    return y ** -18\n  elif p == 19:\n    return 1.0 / y ** 19\n  elif p == 20:\n    return y ** -20\n  elif p == 21:\n    return 1.0 / y ** 21\n  elif p == 22:\n    return y ** -22\n  elif p == 23:\n    return 1.0 / y ** 23\n  elif p == 24:\n    return y ** -24\n  elif p == 25:\n    return 1.0 / y ** 25\n  elif p == 26:"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  if postmult is not None:\n    y = y / postmult\n\n  if p == 1:\n    return y\n  elif p == -1:\n    return 1 / y\n  elif p == 0:\n    return 1\n  elif p == 2:\n    return y * y\n  elif p == -2:\n    return 1 / (y * y)\n  elif p == 3:\n    return y * y * y\n  elif p == -3:\n    return 1 / (y * y * y)\n  elif p == 4:\n    return y * y * y * y\n  elif p == -4:\n    return 1 / (y * y * y * y)\n  elif p == 5:\n    return y * y * y * y * y\n  elif p == -5:\n    return 1 / (y * y * y * y * y)\n  elif p == 6:\n    return y * y * y * y * y * y\n  elif p == -6:\n    return 1 / (y * y * y * y * y * y)\n  elif p == 7:\n    return y * y * y * y * y * y * y\n  elif p == -7:\n    return 1 / (y * y * y * y * y * y * y)\n  elif p == 8:\n    return y * y * y * y * y * y * y * y\n  elif p == -8:\n    return 1 / (y * y * y * y * y * y * y * y)\n  elif p == 9:\n    return y * y * y * y * y * y * y * y * y\n  elif p == -9:\n    return 1 / (y * y * y * y * y * y * y * y * y)\n  elif p == 10:\n    return y * y * y * y * y * y * y * y * y * y\n  elif p == -10:\n    return 1 / (y * y * y * y * y * y * y"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 0:\n    return y\n  if p > 0:\n    if premult is not None:\n      y = y * premult\n    return (y ** (1 / p))\n  if p < 0:\n    if postmult is not None:\n      y = y * postmult\n    return (y ** (-p))"}
{"namespace": "math.inv_power_ladder", "completion": "  if p == 1:\n    return y\n  elif p == 0:\n    return 1.0 / y\n  elif p == -1:\n    return 1.0 / y\n  elif p == 2:\n    return y ** 2\n  elif p == -2:\n    return 1.0 / (y ** 2)\n  elif p == 3:\n    return y ** 3\n  elif p == -3:\n    return 1.0 / (y ** 3)\n  elif p == 4:\n    return y ** 4\n  elif p == -4:\n    return 1.0 / (y ** 4)\n  elif p == 5:\n    return y ** 5\n  elif p == -5:\n    return 1.0 / (y ** 5)\n  elif p == 6:\n    return y ** 6\n  elif p == -6:\n    return 1.0 / (y ** 6)\n  elif p == 7:\n    return y ** 7\n  elif p == -7:\n    return 1.0 / (y ** 7)\n  elif p == 8:\n    return y ** 8\n  elif p == -8:\n    return 1.0 / (y ** 8)\n  elif p == 9:\n    return y ** 9\n  elif p == -9:\n    return 1.0 / (y ** 9)\n  elif p == 10:\n    return y ** 10\n  elif p == -10:\n    return 1.0 / (y ** 10)\n  elif p == 11:\n    return y ** 11\n  elif p == -11:\n    return 1.0 / (y ** 11)\n  elif p == 12:\n    return y ** 12\n  elif p == -12:\n    return 1.0 / (y ** 12)\n  elif p == 13:\n    return y ** 13\n  elif p == -13:\n    return 1.0 / (y ** 13)\n  elif p == 14:"}
{"namespace": "math.inv_power_ladder", "completion": "  if postmult is not None:\n    y = y / postmult\n  if p == 1:\n    y = y\n  elif p == 2:\n    y = np.sqrt(y)\n  elif p == 3:\n    y = np.cbrt(y)\n  elif p == 4:\n    y = np.sqrt(y)\n  elif p == 5:\n    y = np.power(y, 0.2)\n  elif p == 6:\n    y = np.power(y, 0.3333333333333333)\n  elif p == 7:\n    y = np.power(y, 0.42857142857142855)\n  elif p == 8:\n    y = np.power(y, 0.5)\n  elif p == 9:\n    y = np.power(y, 0.5555555555555556)\n  elif p == 10:\n    y = np.power(y, 0.6)\n  elif p == 11:\n    y = np.power(y, 0.6363636363636364)\n  elif p == 12:\n    y = np.power(y, 0.6666666666666666)\n  elif p == 13:\n    y = np.power(y, 0.6923076923076923)\n  elif p == 14:\n    y = np.power(y, 0.7142857142857143)\n  elif p == 15:\n    y = np.power(y, 0.7333333333333333)\n  elif p == 16:\n    y = np.power(y, 0.75)\n  elif p == 17:\n    y = np.power(y, 0.7692"}
{"namespace": "math.inv_power_ladder", "completion": "  if postmult is not None:\n    y = y * postmult\n\n  if p == 0:\n    if premult is not None:\n      y = y * premult\n    return y\n\n  if p == 1:\n    if premult is not None:\n      y = y * premult\n    return 1 / y\n\n  if p == 2:\n    if premult is not None:\n      y = y * premult\n    return y ** -2\n\n  if p == 3:\n    if premult is not None:\n      y = y * premult\n    return 1 / (y ** 3)\n\n  if p == 4:\n    if premult is not None:\n      y = y * premult\n    return y ** -4\n\n  if p == 5:\n    if premult is not None:\n      y = y * premult\n    return 1 / (y ** 5)\n\n  if p == 6:\n    if premult is not None:\n      y = y * premult\n    return y ** -6\n\n  if p == 7:\n    if premult is not None:\n      y = y * premult\n    return 1 / (y ** 7)\n\n  if p == 8:\n    if premult is not None:\n      y = y * premult\n    return y ** -8\n\n  if p == 9:\n    if premult is not None:\n      y = y * premult\n    return 1 / (y ** 9)\n\n  if p == 10:\n    if premult is not None:\n      y = y * premult\n    return y ** -10\n\n  if p == 11:\n    if premult is not None:\n      y = y * premult\n    return 1 / (y ** 11)\n\n  if p == 12:\n    if premult is not None:\n      y = y * premult\n    return y ** -12\n\n  if p == 13:\n    if premult is not None:\n      y = y * premult\n    return 1 / (y ** 13"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is None:\n    premult = 1.0\n  if postmult is None:\n    postmult = 1.0\n  if p == 0:\n    return premult * postmult * y\n  elif p == 1:\n    return premult * postmult * 1.0 / y\n  elif p == 2:\n    return premult * postmult * y ** -0.5\n  elif p == 3:\n    return premult * postmult * y ** -0.3333333333333333\n  elif p == 4:\n    return premult * postmult * y ** -0.25\n  elif p == 5:\n    return premult * postmult * y ** -0.2\n  elif p == 6:\n    return premult * postmult * y ** -0.16666666666666666\n  elif p == 7:\n    return premult * postmult * y ** -0.14285714285714285\n  elif p == 8:\n    return premult * postmult * y ** -0.125\n  elif p == 9:\n    return premult * postmult * y ** -0.1111111111111111\n  elif p == 10:\n    return premult * postmult * y ** -0.1\n  elif p == 11:\n    return premult * postmult * y ** -0.09090909090909091\n  elif p == 12:\n    return premult * postmult * y ** -0.08333333333333333\n  elif p == 13:\n    return premult * postmult * y ** -0.07692307692307693\n  elif p == 14:\n    return premult * postmult * y ** -0.07142857142857142\n  elif p == 15:"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = premult * y\n  if postmult is not None:\n    y = y * postmult\n  if p == 1:\n    return y\n  elif p == -1:\n    return 1.0 / y\n  elif p > 1:\n    return y ** (1.0 / p)\n  else:\n    return 1.0 / (y ** (-p))"}
{"namespace": "math.inv_power_ladder", "completion": "  # Apply pre-multiplication if provided\n  if premult is not None:\n    y *= premult\n\n  # Apply post-multiplication if provided\n  if postmult is not None:\n    y /= postmult\n\n  # Apply the inverse power ladder transformation based on the value of p\n  if p == 2:\n    y = y ** (1 / 2)\n  elif p == 3:\n    y = y ** (1 / 3)\n  elif p == 4:\n    y = y ** (1 / 4)\n  elif p == 5:\n    y = y ** (1 / 5)\n  elif p == 6:\n    y = y ** (1 / 6)\n  elif p == 7:\n    y = y ** (1 / 7)\n  elif p == 8:\n    y = y ** (1 / 8)\n  elif p == 9:\n    y = y ** (1 / 9)\n  elif p == 10:\n    y = y ** (1 / 10)\n  elif p == 11:\n    y = y ** (1 / 11)\n  elif p == 12:\n    y = y ** (1 / 12)\n  elif p == 13:\n    y = y ** (1 / 13)\n  elif p == 14:\n    y = y ** (1 / 14)\n  elif p == 15:\n    y = y ** (1 / 15)\n  elif p == 16:\n    y = y ** (1 / 16)\n  elif p == 17:\n    y = y ** (1 / 17)\n  elif p == 18:\n    y = y ** (1 / 18)\n  elif p == 19:\n    y = y ** (1 / 19)\n  elif p == 20:\n    y = y ** (1 / 20)\n  elif p == 21:\n    y = y ** (1 / 21)\n  elif p == 22:\n    y = y ** (1 / 22)\n  elif p == "}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  if postmult is not None:\n    y = y * postmult\n\n  if p == 1:\n    return y\n  elif p == 2:\n    return y ** (1 / 2)\n  elif p == 3:\n    return y ** (1 / 3)\n  elif p == 4:\n    return y ** (1 / 4)\n  elif p == 5:\n    return y ** (1 / 5)\n  elif p == 6:\n    return y ** (1 / 6)\n  elif p == 7:\n    return y ** (1 / 7)\n  elif p == 8:\n    return y ** (1 / 8)\n  elif p == 9:\n    return y ** (1 / 9)\n  elif p == 10:\n    return y ** (1 / 10)\n  else:\n    raise ValueError(f\"Invalid power value {p}\")\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # If premult is not None, apply premult to y\n  if premult is not None:\n    y = premult * y\n\n  # If postmult is not None, apply postmult to y\n  if postmult is not None:\n    y = y * postmult\n\n  # If p is 0, return y\n  if p == 0:\n    return y\n\n  # If p is 1, return 1/y\n  if p == 1:\n    return 1 / y\n\n  # If p is 2, return 1/y^2\n  if p == 2:\n    return 1 / (y * y)\n\n  # If p is 3, return 1/y^3\n  if p == 3:\n    return 1 / (y * y * y)\n\n  # If p is 4, return 1/y^4\n  if p == 4:\n    return 1 / (y * y * y * y)\n\n  # If p is 5, return 1/y^5\n  if p == 5:\n    return 1 / (y * y * y * y * y)\n\n  # If p is 6, return 1/y^6\n  if p == 6:\n    return 1 / (y * y * y * y * y * y)\n\n  # If p is 7, return 1/y^7\n  if p == 7:\n    return 1 / (y * y * y * y * y * y * y)\n\n  # If p is 8, return 1/y^8\n  if p == 8:\n    return 1 / (y * y * y * y * y * y * y * y)\n\n  # If p is 9, return 1/y^9\n  if p == 9:\n    return 1 / (y * y * y * y * y * y * y * y * y)\n\n  # If p is 10, return 1/y^10\n  if p == 10:\n    return 1 / (y * y * y * y * y * y *"}
{"namespace": "math.inv_power_ladder", "completion": "  # Define a dictionary of inverse power ladder functions based on the value of p\n  inverse_power_ladder_functions = {\n      2: lambda y: y**(1/2),\n      3: lambda y: y**(1/3),\n      4: lambda y: y**(1/4),\n      5: lambda y: y**(1/5),\n      6: lambda y: y**(1/6),\n      7: lambda y: y**(1/7),\n      8: lambda y: y**(1/8),\n      9: lambda y: y**(1/9),\n      10: lambda y: y**(1/10),\n      11: lambda y: y**(1/11),\n      12: lambda y: y**(1/12),\n      13: lambda y: y**(1/13),\n      14: lambda y: y**(1/14),\n      15: lambda y: y**(1/15),\n      16: lambda y: y**(1/16),\n      17: lambda y: y**(1/17),\n      18: lambda y: y**(1/18),\n      19: lambda y: y**(1/19),\n      20: lambda y: y**(1/20),\n      21: lambda y: y**(1/21),\n      22: lambda y: y**(1/22),\n      23: lambda y: y**(1/23),\n      24: lambda y: y**(1/24),\n      25: lambda y: y**(1/25),\n      26: lambda y: y**(1/26),\n      27: lambda y: y**(1/27),\n      28: lambda y: y**(1/28),\n      29: lambda y: y**(1/29),\n      30: lambda y: y**"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  if postmult is not None:\n    y = y * postmult\n\n  if p == 0:\n    return y\n  elif p == 1:\n    return 1 / y\n  elif p == -1:\n    return 1 / np.sqrt(y)\n  elif p == -2:\n    return 1 / np.sqrt(np.sqrt(y))\n  elif p == -3:\n    return 1 / np.sqrt(np.sqrt(np.sqrt(y)))\n  elif p == -4:\n    return 1 / np.sqrt(np.sqrt(np.sqrt(np.sqrt(y))))\n  elif p == -5:\n    return 1 / np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(y)))))\n  elif p == -6:\n    return 1 / np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(y))))))\n  elif p == -7:\n    return 1 / np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(y)))))))\n  elif p == -8:\n    return 1 / np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(y))))))))\n  elif p == -9:\n    return 1 / np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(y)))))))))\n  elif p == -10:\n    return 1 / np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(y))))))))))\n  else:\n    raise ValueError(\"Invalid power value for inverse power ladder transformation\")"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  if postmult is not None:\n    y = y / postmult\n\n  if p == 1:\n    return y\n  elif p == -1:\n    return 1 / y\n  elif p == 0:\n    return 1 / np.sqrt(y)\n  elif p == -2:\n    return 1 / np.sqrt(y)\n  elif p == -3:\n    return 1 / np.cbrt(y)\n  elif p == -4:\n    return 1 / np.sqrt(np.sqrt(y))\n  elif p == -5:\n    return 1 / np.power(y, 1 / 5)\n  elif p == -6:\n    return 1 / np.power(y, 1 / 6)\n  elif p == -7:\n    return 1 / np.power(y, 1 / 7)\n  elif p == -8:\n    return 1 / np.power(y, 1 / 8)\n  elif p == -9:\n    return 1 / np.power(y, 1 / 9)\n  elif p == -10:\n    return 1 / np.power(y, 1 / 10)\n  elif p == -11:\n    return 1 / np.power(y, 1 / 11)\n  elif p == -12:\n    return 1 / np.power(y, 1 / 12)\n  elif p == -13:\n    return 1 / np.power(y, 1 / 13)\n  elif p == -14:\n    return 1 / np.power(y, 1 / 14)\n  elif p == -15:\n    return 1 / np.power(y, 1 / 15)\n  elif p == -16:\n    return 1 / np.power(y, 1 / 16)\n  elif p == -17:\n    return 1 / np.power(y, 1 / 17)"}
{"namespace": "math.inv_power_ladder", "completion": "  # check if premult and postmult are None\n  if premult is None and postmult is None:\n    if p == 2:\n      return y ** (1 / 2)\n    elif p == 3:\n      return y ** (1 / 3)\n    elif p == 4:\n      return y ** (1 / 4)\n    elif p == 5:\n      return y ** (1 / 5)\n    elif p == 6:\n      return y ** (1 / 6)\n    elif p == 7:\n      return y ** (1 / 7)\n    elif p == 8:\n      return y ** (1 / 8)\n    elif p == 9:\n      return y ** (1 / 9)\n    elif p == 10:\n      return y ** (1 / 10)\n    elif p == 11:\n      return y ** (1 / 11)\n    elif p == 12:\n      return y ** (1 / 12)\n    elif p == 13:\n      return y ** (1 / 13)\n    elif p == 14:\n      return y ** (1 / 14)\n    elif p == 15:\n      return y ** (1 / 15)\n    elif p == 16:\n      return y ** (1 / 16)\n    elif p == 17:\n      return y ** (1 / 17)\n    elif p == 18:\n      return y ** (1 / 18)\n    elif p == 19:\n      return y ** (1 / 19)\n    elif p == 20:\n      return y ** (1 / 20)\n    elif p == 21:\n      return y ** (1 / 21)\n    elif p == 22:\n      return y ** (1 / 22)\n    elif p == 23:\n      return y ** (1 / 23)\n    elif p == 24:\n      return y ** (1 / 24)\n    elif p == 25:\n      return y ** (1 / 25)\n    elif p == 26:\n      return"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = (lr_init / lr_final) ** (1 / lr_delay_steps)\n\n  if step < lr_delay_steps:\n    alpha = step / lr_delay_steps\n    lr = lr_init * lr_delay_mult ** alpha\n  else:\n    alpha = (step - lr_delay_steps) / (max_steps - lr_delay_steps)\n    lr = lr_final * alpha + lr_init * (1 - alpha)\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n\n  if step < lr_delay_steps:\n    lr = lr_init\n  else:\n    lr = lr_final + (lr_init - lr_final) * (\n        1 - math.exp(-1.0 * (step - lr_delay_steps) / max_steps)\n    )\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_delay_mult = 1\n\n  if step <= lr_delay_steps:\n    lr = lr_init * step / lr_delay_steps\n  elif step >= max_steps:\n    lr = lr_final\n  else:\n    lr = lr_final + (lr_init - lr_final) * (\n      1 - (step - lr_delay_steps) / (max_steps - lr_delay_steps)\n    )\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_delay_mult = 1.0\n\n  if step < lr_delay_steps:\n    return lr_init\n\n  return lr_final + (lr_init - lr_final) * (1 - (step - lr_delay_steps) / max_steps)"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult *= (lr_init / lr_final) ** (1 / lr_delay_steps)\n    lr_delay_steps = max(lr_delay_steps - step, 0)\n    if lr_delay_steps > 0:\n      return lr_init * lr_delay_mult ** lr_delay_steps\n\n  return lr_final + (lr_init - lr_final) * (1 - step / max_steps) ** 0.25"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_delay_mult = 1\n\n  if step <= lr_delay_steps:\n    lr = lr_init * (step / lr_delay_steps)\n  else:\n    lr = (\n        lr_final\n        + (lr_init - lr_final)\n        * (1 - math.exp(-1 * (step - lr_delay_steps) / max_steps))\n    )\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = (lr_init / lr_final) ** (1 / lr_delay_steps)\n    lr_init *= lr_delay_mult\n\n  if step < lr_delay_steps:\n    lr = lr_init * lr_delay_mult ** step\n  else:\n    lr = lr_final + (lr_init - lr_final) * (\n        1 - (1 + math.exp(0 - (step - lr_delay_steps) / max_steps)) ** (-1)\n    )\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_delay_mult = (lr_init / lr_final) ** (1 / lr_delay_steps)\n\n  return lr_init * lr_delay_mult ** max(step - lr_delay_steps, 0)\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_delay_mult = 1.0\n\n  if step < lr_delay_steps:\n    return lr_init * (step / lr_delay_steps) * lr_delay_mult\n\n  return lr_final + (lr_init - lr_final) * (1 - (step - lr_delay_steps) / max_steps)"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the learning rate based on the current step\n  if lr_delay_steps > 0:\n    if step < lr_delay_steps:\n      lr = lr_init * lr_delay_mult - (lr_init * lr_delay_mult - lr_final) * (\n          step / lr_delay_steps\n      )\n    else:\n      lr = lr_final + (lr_init * lr_delay_mult - lr_final) * (\n          (max_steps - step) / (max_steps - lr_delay_steps)\n      )\n  else:\n    lr = lr_final + (lr_init - lr_final) * (1 - step / max_steps)\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_delay_mult = 1.0 / lr_delay_mult\n\n  if step <= lr_delay_steps:\n    pct_remaining = 1 - (step / lr_delay_steps)\n    decayed_lr = lr_init * pct_remaining**lr_delay_mult\n  else:\n    pct_remaining = 1 - (step - lr_delay_steps) / (max_steps - lr_delay_steps)\n    decayed_lr = lr_final + (lr_init - lr_final) * pct_remaining\n\n  return decayed_lr"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the learning rate decay\n  if lr_delay_steps > 0:\n    if step < lr_delay_steps:\n      lr = lr_init * lr_delay_mult - (\n          lr_init * lr_delay_mult - lr_final\n      ) * (step / lr_delay_steps)\n    else:\n      lr = lr_final + (lr_init * lr_delay_mult - lr_final) * (\n          (max_steps - step) / (max_steps - lr_delay_steps)\n      )\n  else:\n    lr = lr_final + (lr_init - lr_final) * (1 - step / max_steps)\n\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1.0 / lr_delay_mult\n\n  if step < lr_delay_steps:\n    # Apply delay\n    alpha = step / lr_delay_steps\n    lr = lr_init * (lr_delay_mult + alpha * (1 - lr_delay_mult))\n  else:\n    # Apply normal decay\n    alpha = (step - lr_delay_steps) / (max_steps - lr_delay_steps)\n    lr = lr_final + (lr_init - lr_final) * (1 - alpha)\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_delay_mult = 1.0\n\n  if step <= lr_delay_steps:\n    lr = lr_init * step / lr_delay_steps\n  else:\n    lr = lr_final + (lr_init - lr_final) * (1 - step / max_steps) ** 3\n\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n\n  if step < lr_delay_steps:\n    pct_remaining = 1 - (step / lr_delay_steps)\n    decayed_lr = lr_init * pct_remaining\n  else:\n    pct_remaining = (max_steps - step) / (max_steps - lr_delay_steps)\n    decayed_lr = lr_final + (lr_init - lr_final) * pct_remaining\n\n  return decayed_lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the learning rate based on the current step\n  if lr_delay_steps > 0:\n    # Apply the delay multiplier to the initial learning rate\n    lr_init *= lr_delay_mult\n    # Calculate the learning rate during the delay period\n    lr = lr_init * (1 - step / max(1, lr_delay_steps))\n  else:\n    lr = lr_init\n\n  # Calculate the learning rate decay\n  lr *= (lr_final / lr_init) ** (step / max_steps)\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_delay_mult = 1.0\n\n  if step < lr_delay_steps:\n    # Apply the delay learning rate\n    lr = (lr_init - lr_final) / lr_delay_steps * step + lr_final\n  else:\n    # Apply the normal learning rate decay\n    pct_remaining = 1 - (step - lr_delay_steps) / (max_steps - lr_delay_steps)\n    lr = lr_init * pct_remaining ** 0.9\n\n  # Apply the multiplier\n  lr = lr * lr_delay_mult\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_delay_mult = 1.0 / lr_delay_mult\n\n  lr = lr_init * lr_delay_mult * (\n      (1 - step / max_steps) ** (math.log(lr_init / lr_final) / math.log(0.5))\n  )\n\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the learning rate decay\n  if lr_delay_steps > 0:\n    if step < lr_delay_steps:\n      # Apply the delay multiplier to the initial learning rate\n      lr = lr_init * lr_delay_mult\n    else:\n      # Calculate the learning rate decay after the delay period\n      lr = lr_final + (lr_init - lr_final) * (\n        1 - math.exp(-1.0 * (step - lr_delay_steps) / (max_steps - lr_delay_steps))\n      )\n  else:\n    # Calculate the learning rate decay without a delay\n    lr = lr_final + (lr_init - lr_final) * (\n      1 - math.exp(-1.0 * step / max_steps)\n    )\n\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the progression of the optimization step\n  p = step / max_steps\n\n  # Calculate the learning rate decay using log-linear interpolation\n  lr = lr_final + 0.5 * (lr_init - lr_final) * (1 + np.cos(np.pi * p))\n\n  # If a delay is specified, apply the delay and scale the learning rate\n  if lr_delay_steps > 0:\n    if step < lr_delay_steps:\n      lr = lr_init * lr_delay_mult ** (step / lr_delay_steps)\n    else:\n      lr = lr_init * lr_delay_mult ** (lr_delay_steps / max_steps)\n\n  return lr\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  from .generate_random_rays import generate_random_rays\n\n  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  from .generate_random_rays import generate_random_rays\n\n  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  from .generate_random_rays import generate_random_rays\n\n  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  from .generate_random_rays import generate_random_rays\n  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  from .generate_random_rays import generate_random_rays\n  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(include_exposure_idx, include_exposure_values, include_device_idx)"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(include_exposure_idx, include_exposure_values, include_device_idx)"}
{"namespace": "utils.dummy_rays", "completion": "  from .generate_random_rays import generate_random_rays\n  return generate_random_rays(include_exposure_idx, include_exposure_values, include_device_idx)"}
{"namespace": "utils.dummy_rays", "completion": "  from . import generate_random_rays\n  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  # TODO: Implement this\n  raise NotImplementedError"}
{"namespace": "utils.dummy_rays", "completion": "  def generate_random_rays(\n      include_exposure_idx = False,\n      include_exposure_values = False,\n      include_device_idx = False,\n  ):\n\n    \"\"\"\n    Generates random rays with specified options for including exposure index, exposure values, and device index.\n\n    Input-Output Arguments\n    :param include_exposure_idx: Bool, optional. Determines whether to include the exposure index in the generated rays.\n    :param include_exposure_values: Bool, optional. Specifies if the exposure values should be included in the generated rays.\n    :param include_device_idx: Bool, optional. Indicates whether to include the device index in the generated rays.\n    :return: The result from `generate_random_rays`. The type and structure of the return value depend on the implementation of `generate_random_rays`, typically a collection of generated rays with specified properties and optional information.\n    \"\"\"\n\n    return None\n\n  return generate_random_rays(\n      include_exposure_idx = include_exposure_idx,\n      include_exposure_values = include_exposure_values,\n      include_device_idx = include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  import numpy as np\n\n  # Generate random rays\n  rays = generate_random_rays()\n\n  # Add exposure index\n  if include_exposure_idx:\n    rays['exposure_idx'] = np.random.randint(0, 10, len(rays['ray_id']))\n\n  # Add exposure values\n  if include_exposure_values:\n    rays['exposure_values'] = np.random.rand(len(rays['ray_id']), 10)\n\n  # Add device index\n  if include_device_idx:\n    rays['device_idx'] = np.random.randint(0, 10, len(rays['ray_id']))\n\n  return rays"}
{"namespace": "utils.dummy_rays", "completion": "  def generate_random_rays(\n      include_exposure_idx = False,\n      include_exposure_values = False,\n      include_device_idx = False,\n  ):\n\n    \"\"\"\n    Generates a set of random rays with specified parameters and options for including exposure index, exposure values, and device index.\n\n    Input-Output Arguments\n    :param include_exposure_idx: Bool, optional. Determines whether to include the exposure index in the generated rays.\n    :param include_exposure_values: Bool, optional. Specifies if the exposure values should be included in the generated rays.\n    :param include_device_idx: Bool, optional. Indicates whether to include the device index in the generated rays.\n    :return: A dictionary containing the generated rays. The dictionary has the following structure:\n    - `rays`: A list of dictionaries, each representing a ray. Each ray dictionary contains the following keys:\n      - `ray_origin`: A list of three floats representing the origin of the ray.\n      - `ray_direction`: A list of three floats representing the direction of the ray.\n      - `exposure_idx` (optional): An integer representing the exposure index of the ray.\n      - `exposure_values` (optional): A list of floats representing the exposure values of the ray.\n      - `device_idx` (optional): An integer representing the device index of the ray.\n    \"\"\"\n\n    return {\n        \"rays\": [\n            {\n                \"ray_origin\": [0.0, 0.0, 0.0],\n                \"ray_direction\": [1.0, 0.0, 0.0],\n                \"exposure_idx\": 0 if include_exposure_idx else None,\n                \"exposure_values\": [1.0, 2.0, 3.0] if include_exposure_values else None,\n                \"device_idx\": 0 if include_device_idx else None,\n            },\n            {\n                \"ray_origin\": [1.0, 1.0, 1.0],"}
{"namespace": "utils.dummy_rays", "completion": "  import numpy as np\n  import torch\n\n  from .generate_random_rays import generate_random_rays\n\n  # Generate random rays\n  rays = generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )\n\n  # Set the number of rays\n  num_rays = 1000\n\n  # Set the number of pixels\n  num_pixels = 1000\n\n  # Set the number of channels\n  num_channels = 3\n\n  # Set the number of exposures\n  num_exposures = 1\n\n  # Set the number of devices\n  num_devices = 1\n\n  # Set the number of samples\n  num_samples = 1\n\n  # Set the number of rays per pixel\n  num_rays_per_pixel = 10\n\n  # Set the number of rays per exposure\n  num_rays_per_exposure = 10\n\n  # Set the number of rays per device\n  num_rays_per_device = 10\n\n  # Set the number of rays per sample\n  num_rays_per_sample = 10\n\n  # Set the number of rays per batch\n  num_rays_per_batch = 10\n\n  # Set the number of batches\n  num_batches = 10\n\n  # Set the number of rays per batch per device\n  num_rays_per_batch_per_device = 10\n\n  # Set the number of rays per batch per sample\n  num_rays_per_batch_per_sample = 10\n\n  # Set the number of rays per batch per exposure\n  num_rays_per_batch_per_exposure = 10\n\n  # Set the number of rays per batch per pixel\n  num_rays_per_batch_per_pixel = 10\n\n  # Set the number of rays per batch per channel\n  num_rays_per_batch_per_channel = 10\n\n  # Set the"}
{"namespace": "utils.dummy_rays", "completion": "  import numpy as np\n  import math\n  from . import generate_random_rays\n\n  # Generate random rays\n  rays = generate_random_rays()\n\n  # Add exposure index\n  if include_exposure_idx:\n    rays['exposure_idx'] = np.random.randint(0, 100, size=(rays['ray_origins'].shape[0],))\n\n  # Add exposure values\n  if include_exposure_values:\n    rays['exposure_values'] = np.random.rand(rays['ray_origins'].shape[0],)\n\n  # Add device index\n  if include_device_idx:\n    rays['device_idx'] = np.random.randint(0, 100, size=(rays['ray_origins'].shape[0],))\n\n  return rays\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  if distortion_params is not None:\n    points = undistort_points(points, distortion_params, xnp=xnp)\n\n  coordinates = project_points(points, pixtocams, camtoworlds, camtype, xnp=xnp)\n\n  if camtype == ProjectionType.PERSPECTIVE:\n    depth = coordinates[..., 2]\n    coordinates = coordinates[..., :2] / depth[..., None]\n  else:\n    raise NotImplementedError\n\n  return coordinates, depth"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Check if the input arrays are vectorized over the leading dimensions\n  is_vectorized = points.ndim > 1 and points.shape[0] > 1\n\n  # If the input arrays are vectorized, calculate the number of points and cameras\n  if is_vectorized:\n    num_points = points.shape[1]\n    num_cams = points.shape[0]\n  else:\n    num_points = points.shape[0]\n    num_cams = 1\n\n  # Check if the input arrays are batched over the leading dimensions\n  is_batched = pixtocams.ndim > 2 and pixtocams.shape[0] > 1\n\n  # If the input arrays are batched, calculate the batch size and number of cameras\n  if is_batched:\n    batch_size = pixtocams.shape[0]\n    num_cams = pixtocams.shape[1]\n  else:\n    batch_size = 1\n\n  # Check if the input arrays are batched over the leading dimensions\n  is_batched = camtoworlds.ndim > 2 and camtoworlds.shape[0] > 1\n\n  # If the input arrays are batched, calculate the batch size and number of cameras\n  if is_batched:\n    batch_size = camtoworlds.shape[0]\n    num_cams = camtoworlds.shape[1]\n  else:\n    batch_size = 1\n\n  # Check if the input arrays are batched over the leading dimensions\n  is_batched = distortion_params.ndim > 2 and distortion_params.shape[0] > 1\n\n  # If the input arrays are batched, calculate the batch size and number of cameras\n  if is_batched:\n    batch_size = distortion_params.shape[0]\n    num_cams = distortion_params.shape[1]\n  else:\n    batch_size = 1\n\n  # Check if the input arrays are batched over the leading dimensions\n  is_batched = camtype.ndim > 2 and camtype.shape[0] > 1\n\n  #"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert points to homogenous coordinates\n  points_hom = xnp.concatenate([points, xnp.ones_like(points[..., :1])], axis=-1)\n\n  # Transform points to camera coordinates\n  points_cam = xnp.einsum(\"...ij,...j->...i\", pixtocams, points_hom)\n\n  # Transform points to world coordinates\n  points_world = xnp.einsum(\"...ij,...j->...i\", camtoworlds, points_cam)\n\n  # Compute depth values\n  depth = xnp.linalg.norm(points_world, axis=-1)\n\n  # Apply lens distortion if distortion_params are provided\n  if distortion_params is not None:\n    x = points_world[..., 0] / depth\n    y = points_world[..., 1] / depth\n    r2 = x**2 + y**2\n    x = x * (1 + distortion_params[\"k1\"] * r2 + distortion_params[\"k2\"] * r2**2)\n    y = y * (1 + distortion_params[\"k1\"] * r2 + distortion_params[\"k2\"] * r2**2)\n    points_world[..., 0] = x * depth\n    points_world[..., 1] = y * depth\n\n  # Project points to pixel coordinates\n  if camtype == ProjectionType.PERSPECTIVE:\n    coordinates = points_world[..., :2] / points_world[..., 2:3]\n  else:\n    raise ValueError(f\"Unsupported projection type: {camtype}\")\n\n  return coordinates, depth"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    # Compute the 3D camera coordinates of the points\n    points_cam = xnp.einsum(\"...ij,...j->...i\", camtoworlds, points)\n\n    # Compute the 2D pixel coordinates of the points\n    coordinates = xnp.einsum(\"...ij,...j->...i\", pixtocams, points_cam)\n\n    # Compute the depth values of the points\n    depth = points_cam[..., 2]\n\n    # Apply distortion correction if distortion parameters are provided\n    if distortion_params is not None:\n      # Compute the radial and tangential distortion parameters\n      radial_distortion = distortion_params[\"radial\"]\n      tangential_distortion = distortion_params[\"tangential\"]\n\n      # Compute the radial distortion correction\n      r2 = xnp.sum(points_cam[..., :2] ** 2, axis=-1)\n      radial_correction = (\n        radial_distortion[0] * r2 + radial_distortion[1] * r2 ** 2 + radial_distortion[2] * r2 ** 3\n      )\n\n      # Compute the tangential distortion correction\n      tangential_correction = (\n        tangential_distortion[0] * points_cam[..., 1]\n        + tangential_distortion[1] * points_cam[..., 0]\n      )\n\n      # Apply the distortion correction to the pixel coordinates\n      coordinates[..., 0] += tangential_correction[..., 0]\n      coordinates[..., 1] += tangential_correction[..., 1]\n      coordinates[..., 0] += radial_correction[..., 0] * coordinates[..., 0]\n      coordinates[..., 1] += radial_correction[..., 1] * coordinates[..., 1]\n\n  else:\n    raise ValueError(\"Only perspective projection is supported.\")\n\n  return coordinates, depth"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Check if the input arrays have the correct shape\n  assert points.shape[-1] == 3, \"points must have shape (..., 3)\"\n  assert pixtocams.shape[-1] == 3, \"pixtocams must have shape (..., 3, 3)\"\n  assert camtoworlds.shape[-1] == 4, \"camtoworlds must have shape (..., 4, 4)\"\n\n  # Extract the camera intrinsics and extrinsics matrices\n  pixtocam = pixtocams[..., :3, :3]\n  camtoworld = camtoworlds[..., :3, :4]\n\n  # Transform the points from world coordinates to camera coordinates\n  points_cam = xnp.einsum(\"...ij,...j->...i\", camtoworld, xnp.concatenate([points, xnp.ones_like(points[..., :1])], axis=-1))\n\n  # Project the points onto the image plane\n  if camtype == ProjectionType.PERSPECTIVE:\n    coordinates = xnp.einsum(\"...ij,...j->...i\", pixtocam, points_cam[..., :3])\n    depth = points_cam[..., 2]\n  else:\n    raise ValueError(\"Unsupported camera type\")\n\n  # Apply lens distortion if distortion_params are provided\n  if distortion_params is not None:\n    coordinates = apply_lens_distortion(coordinates, distortion_params)\n\n  # Return the pixel coordinates and depth values\n  return coordinates, depth\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Check that the input arrays have the correct shape and type\n  assert points.shape[-1] == 3, \"points must be a 3D array\"\n  assert pixtocams.shape[-2:] == (3, 3), \"pixtocams must be a 3x3 matrix\"\n  assert camtoworlds.shape[-2:] == (3, 4), \"camtoworlds must be a 3x4 matrix\"\n  assert points.ndim >= 2, \"points must have at least 2 dimensions\"\n  assert pixtocams.ndim >= 2, \"pixtocams must have at least 2 dimensions\"\n  assert camtoworlds.ndim >= 2, \"camtoworlds must have at least 2 dimensions\"\n  assert points.dtype == np.float32, \"points must be a float32 array\"\n  assert pixtocams.dtype == np.float32, \"pixtocams must be a float32 array\"\n  assert camtoworlds.dtype == np.float32, \"camtoworlds must be a float32 array\"\n\n  # Check that the distortion parameters are either None or a dict of floats or float arrays\n  if distortion_params is not None:\n    assert isinstance(distortion_params, dict), \"distortion_params must be a dict\"\n    assert all(\n      isinstance(v, (float, np.float32)) for v in distortion_params.values()\n    ), \"distortion_params must be a dict of floats or float32 arrays\"\n\n  # Check that the camera type is valid\n  assert (\n    camtype in ProjectionType\n  ), \"camtype must be one of the ProjectionType enum values\"\n\n  # Check that the input arrays have the same leading dimensions\n  assert points.shape[:-1] == pixtocams.shape[:-2], \"points and pixtocams must have the same leading dimensions\"\n  assert points.shape[:-1] == camtoworlds.shape[:-2], \"points and camtoworlds must have the same leading dimensions\"\n\n  # Convert the input arrays to"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  if distortion_params is not None:\n    if \"k1\" in distortion_params:\n      k1 = distortion_params[\"k1\"]\n      k2 = distortion_params[\"k2\"]\n      p1 = distortion_params[\"p1\"]\n      p2 = distortion_params[\"p2\"]\n    else:\n      k1 = distortion_params\n      k2 = distortion_params\n      p1 = distortion_params\n      p2 = distortion_params\n  else:\n    k1 = k2 = p1 = p2 = 0.0\n\n  # Convert points to homogenous coordinates\n  points_h = xnp.concatenate(\n      [points, xnp.ones((points.shape[0], 1), dtype=points.dtype)], axis=1\n  )\n\n  # Transform points to camera coordinates\n  points_c = xnp.einsum(\"...ij,...jk->...ik\", points_h, camtoworlds)\n\n  # Transform points to pixel coordinates\n  points_p = xnp.einsum(\"...ij,...jk->...ik\", points_c, pixtocams)\n\n  # Apply distortion model\n  r2 = xnp.sum(points_p ** 2, axis=-1, keepdims=True)\n  r4 = r2 ** 2\n  r6 = r4 * r2\n  r8 = r6 * r2\n  r10 = r8 * r2\n  r12 = r10 * r2\n  r14 = r12 * r2\n  r16 = r14 * r2\n  r18 = r16 * r2\n  r20 = r18 * r2\n  r22 = r20 * r2\n  r24 = r22 * r2\n  r26 = r24 * r2\n  r28 = r26 * r2\n  r30 = r28 * r2\n  r32 = r30 * r2\n  r34 = r32 * r2"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Check the input shapes and dimensions\n  assert points.shape[-1] == 3, \"points must have shape (..., 3)\"\n  assert pixtocams.shape[-2:] == (3, 3), \"pixtocams must have shape (..., 3, 3)\"\n  assert camtoworlds.shape[-2:] == (3, 4), \"camtoworlds must have shape (..., 3, 4)\"\n  assert (\n      points.ndim == pixtocams.ndim == camtoworlds.ndim\n  ), \"points, pixtocams, and camtoworlds must have the same number of dimensions\"\n  assert (\n      points.shape[:-1] == pixtocams.shape[:-2] == camtoworlds.shape[:-2]\n  ), \"points, pixtocams, and camtoworlds must have matching leading dimensions\"\n\n  # Convert points to camera coordinates\n  points = xnp.einsum(\"...ij,...j->...i\", camtoworlds[..., :3, :3], points) + camtoworlds[\n    ..., :3, 3\n  ]\n\n  # Apply distortion if provided\n  if distortion_params is not None:\n    k1, k2, k3, p1, p2 = distortion_params\n    r2 = xnp.sum(points ** 2, axis=-1, keepdims=True)\n    r4 = r2 ** 2\n    r6 = r2 ** 3\n    radial_distortion = 1 + k1 * r2 + k2 * r4 + k3 * r6\n    tangential_distortion_x = 2 * p1 * points[..., 0] * points[..., 1] + p2 * (\n      r2 + 2 * points[..., 0] ** 2\n    )\n    tangential_distortion_y = p1 * (r2 + 2 * points[..., 1] ** 2) + 2 * p2 * points[\n      ..., 0\n    ] * points[\n      ..., 1\n   "}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Check if the input arrays are valid\n  if points.ndim != 3:\n    raise ValueError(\"points must be a 3D array\")\n  if pixtocams.ndim != 3:\n    raise ValueError(\"pixtocams must be a 3D array\")\n  if camtoworlds.ndim != 3:\n    raise ValueError(\"camtoworlds must be a 3D array\")\n  if points.shape[0] != pixtocams.shape[0]:\n    raise ValueError(\"points and pixtocams must have the same leading dimension\")\n  if points.shape[0] != camtoworlds.shape[0]:\n    raise ValueError(\"points and camtoworlds must have the same leading dimension\")\n\n  # Convert the input arrays to the correct data types\n  points = xnp.asarray(points, dtype=xnp.float32)\n  pixtocams = xnp.asarray(pixtocams, dtype=xnp.float32)\n  camtoworlds = xnp.asarray(camtoworlds, dtype=xnp.float32)\n\n  # Apply camera extrinsics to transform points to camera coordinates\n  points_cam = xnp.einsum(\"bij,bj->bi\", camtoworlds, points)\n\n  # Apply camera intrinsics to transform points to pixel coordinates\n  points_pix = xnp.einsum(\"bij,bj->bi\", pixtocams, points_cam)\n\n  # Apply lens distortion to pixel coordinates\n  if distortion_params is not None:\n    k1, k2, k3, k4, k5, k6, p1, p2 = distortion_params\n    r2 = xnp.sum(points_pix[:, :2] ** 2, axis=-1)\n    radial_distortion = 1.0 + k1 * r2 + k2 * r2 ** 2 + k3 * r2 ** 3 + k4 * r2 ** 4 + k5 * r2 ** 5 + k6 * r2 **"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Check if the input arrays have the correct shapes\n  assert points.shape[-1] == 3, \"points must have shape (..., 3)\"\n  assert pixtocams.shape[-2:] == (3, 3), \"pixtocams must have shape (..., 3, 3)\"\n  assert camtoworlds.shape[-2:] == (3, 4), \"camtoworlds must have shape (..., 3, 4)\"\n\n  # Get the batch dimensions\n  batch_dims = points.shape[:-1]\n\n  # Reshape the input arrays to match the batch dimensions\n  points = xnp.reshape(points, (-1, 3))\n  pixtocams = xnp.reshape(pixtocams, (-1, 3, 3))\n  camtoworlds = xnp.reshape(camtoworlds, (-1, 3, 4))\n\n  # Transform points from world to camera coordinates\n  points = xnp.matmul(camtoworlds, xnp.concatenate([points, xnp.ones((points.shape[0], 1))], axis=1).T).T\n\n  # Project points onto the image plane using the camera intrinsics\n  if camtype == ProjectionType.PERSPECTIVE:\n    points = xnp.matmul(pixtocams, points.T).T\n    points = points[:, :2] / points[:, 2:3]\n  else:\n    raise ValueError(\"Unsupported camera type: {}\".format(camtype))\n\n  # Apply distortion correction if distortion_params are provided\n  if distortion_params is not None:\n    k1 = distortion_params.get(\"k1\", 0.0)\n    k2 = distortion_params.get(\"k2\", 0.0)\n    k3 = distortion_params.get(\"k3\", 0.0)\n    p1 = distortion_params.get(\"p1\", 0.0)\n    p2 = distortion_params.get(\"p2\", 0.0)\n    kc ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  if distortion_params is None:\n    distortion_params = {}\n\n  # Get the number of points to process\n  num_points = points.shape[0]\n\n  # Reshape the input arrays to 2D\n  points = points.reshape(-1, 3)\n  pixtocams = pixtocams.reshape(-1, 3, 3)\n  camtoworlds = camtoworlds.reshape(-1, 4, 4)\n\n  # Calculate the camera coordinates of the points\n  points_cam = xnp.einsum(\"ij,kj->ki\", camtoworlds, xnp.concatenate([points, xnp.ones((num_points, 1))], axis=1))\n  points_cam = points_cam[:, :3] / points_cam[:, 3:4]\n\n  # Apply lens distortion to the camera coordinates\n  if \"radial\" in distortion_params:\n    points_cam = lens_distortion(points_cam, distortion_params[\"radial\"], camtype)\n  if \"tangential\" in distortion_params:\n    points_cam = lens_distortion(points_cam, distortion_params[\"tangential\"], camtype)\n\n  # Project the camera coordinates onto the image plane\n  coordinates = xnp.einsum(\"ij,kj->ki\", pixtocams, points_cam)\n\n  # Calculate the depth values\n  depth = xnp.linalg.norm(points - xnp.einsum(\"ij,kj->ki\", camtoworlds, xnp.concatenate([points, xnp.ones((num_points, 1))], axis=1))[:, :3], axis=1)\n\n  # Reshape the output arrays to the original shape\n  coordinates = coordinates.reshape(num_points, -1)\n  depth = depth.reshape(num_points, -1)\n\n  return coordinates, depth"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Check if the input arrays have the correct shape and dtype\n  assert points.shape[-1] == 3\n  assert points.dtype == np.float32\n  assert pixtocams.shape == (4, 4)\n  assert pixtocams.dtype == np.float32\n  assert camtoworlds.shape == (4, 4)\n  assert camtoworlds.dtype == np.float32\n\n  # Convert the input arrays to jax.numpy arrays if xnp is jax.numpy\n  if xnp is jnp:\n    points = jnp.array(points)\n    pixtocams = jnp.array(pixtocams)\n    camtoworlds = jnp.array(camtoworlds)\n\n  # Calculate the camera coordinates of the input points\n  camtoworlds = jnp.transpose(camtoworlds, (1, 0))\n  points = jnp.matmul(camtoworlds, jnp.concatenate((points, jnp.ones((1, points.shape[1]))), axis=0))\n  points = points[0:3, :] / points[3, :]\n\n  # Apply the distortion model to the camera coordinates\n  if distortion_params is not None:\n    k1, k2, p1, p2, k3 = distortion_params\n    r2 = points[0, :] ** 2 + points[1, :] ** 2\n    radial_distortion = 1 + k1 * r2 + k2 * r2 ** 2 + k3 * r2 ** 3\n    tangential_distortion_x = 2 * p1 * points[0, :] * points[1, :] + p2 * (r2 + 2 * points[0, :] ** 2)\n    tangential_distortion_y = p1 * (r2 + 2 * points[1, :] ** 2) + 2 * p2 * points[0, :] * points[1, :]\n    points[0, :] = points[0, :] * radial_distortion"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Check if the input arrays have the correct shape\n  assert points.shape[-1] == 3, \"Points must be a 3D array\"\n  assert pixtocams.shape[-1] == 9, \"Pixtocams must be a 9D array\"\n  assert camtoworlds.shape[-1] == 16, \"Camtoworlds must be a 16D array\"\n\n  # Convert the input arrays to the correct data type\n  points = xnp.array(points, dtype=xnp.float32)\n  pixtocams = xnp.array(pixtocams, dtype=xnp.float32)\n  camtoworlds = xnp.array(camtoworlds, dtype=xnp.float32)\n\n  # Calculate the camera coordinates of the points\n  coordinates = xnp.einsum(\"...ij,...jk->...ik\", pixtocams, points)\n\n  # Calculate the depth values of the points\n  depth = xnp.einsum(\"...ij,...jk->...ik\", camtoworlds[..., :3, 3], points)\n\n  # Apply distortion correction if distortion parameters are provided\n  if distortion_params is not None:\n    # Get the distortion parameters\n    k1 = distortion_params[\"k1\"]\n    k2 = distortion_params[\"k2\"]\n    p1 = distortion_params[\"p1\"]\n    p2 = distortion_params[\"p2\"]\n\n    # Calculate the radial distortion\n    r2 = xnp.sum(coordinates ** 2, axis=-1, keepdims=True)\n    radial_distortion = 1 + k1 * r2 + k2 * r2 ** 2\n\n    # Calculate the tangential distortion\n    tangential_distortion = (\n        2 * p1 * coordinates[..., 0] * coordinates[..., 1]\n        + p2 * (r2 + 2 * coordinates[..., 0] ** 2)\n    )\n\n    # Apply distortion correction\n    coordinates = (\n        coordinates\n        * radial"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Check that the input arrays have the correct shape\n  assert points.shape[-1] == 3, \"points should be a (..., 3) array\"\n  assert pixtocams.shape[-2:] == (3, 3), \"pixtocams should be a (..., 3, 3) array\"\n  assert camtoworlds.shape[-2:] == (3, 4), \"camtoworlds should be a (..., 3, 4) array\"\n\n  # Extract the camera intrinsics matrices from pixtocams\n  pixtocam_matrices = pixtocams[..., :3, :3]\n\n  # Extract the camera extrinsics matrices from camtoworlds\n  camtoworld_matrices = camtoworlds[..., :3, :4]\n\n  # Convert the points from world coordinates to camera coordinates\n  points_cam = xnp.einsum(\"...ij,...j->...i\", camtoworld_matrices, points)\n\n  # Project the points onto the image plane using the camera intrinsics\n  if camtype == ProjectionType.PERSPECTIVE:\n    coordinates = xnp.einsum(\"...ij,...j->...i\", pixtocam_matrices, points_cam)\n    depth = coordinates[..., 2]\n    coordinates = coordinates[..., :2] / coordinates[..., 2:3]\n\n  # Apply distortion correction to the coordinates\n  if distortion_params is not None:\n    coordinates = distortion_correction(\n      coordinates,\n      distortion_params,\n      xnp=xnp,\n    )\n\n  return coordinates, depth\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Check if the input arrays have the correct shape and rank\n  assert points.ndim == 3\n  assert pixtocams.ndim == 4\n  assert camtoworlds.ndim == 4\n  assert points.shape[-1] == 3\n  assert pixtocams.shape[1:] == camtoworlds.shape[1:]\n  assert pixtocams.shape[0] == camtoworlds.shape[0]\n\n  # Check if the distortion parameters are provided and have the correct shape\n  if distortion_params is not None:\n    assert distortion_params[\"radial\"] is not None\n    assert distortion_params[\"radial\"].ndim == 1\n    assert distortion_params[\"radial\"].shape[0] == 2\n    assert distortion_params[\"tangential\"] is not None\n    assert distortion_params[\"tangential\"].ndim == 1\n    assert distortion_params[\"tangential\"].shape[0] == 2\n\n  # Check if the camera type is valid\n  assert camtype in [ProjectionType.PERSPECTIVE]\n\n  # Convert the input arrays to the specified device (CPU or GPU/TPU)\n  points = xnp.asarray(points)\n  pixtocams = xnp.asarray(pixtocams)\n  camtoworlds = xnp.asarray(camtoworlds)\n  if distortion_params is not None:\n    distortion_params[\"radial\"] = xnp.asarray(distortion_params[\"radial\"])\n    distortion_params[\"tangential\"] = xnp.asarray(distortion_params[\"tangential\"])\n\n  # Compute the camera coordinates of the input points\n  camtoworlds = xnp.transpose(camtoworlds, (0, 2, 3, 1))\n  points = xnp.expand_dims(points, axis=-2)\n  points = xnp.matmul(camtoworlds, points)\n  points = xnp.squeeze(points, axis=-2)\n\n  # Apply the distortion model to the camera coordinates\n  if"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Check input shapes\n  assert points.shape[-1] == 3, \"points should be of shape (..., 3)\"\n  assert pixtocams.shape[-2:] == (3, 3), \"pixtocams should be of shape (..., 3, 3)\"\n  assert camtoworlds.shape[-2:] == (3, 4), \"camtoworlds should be of shape (..., 3, 4)\"\n\n  # Check input types\n  assert isinstance(points, xnp.ndarray), \"points should be a numpy array\"\n  assert isinstance(pixtocams, xnp.ndarray), \"pixtocams should be a numpy array\"\n  assert isinstance(camtoworlds, xnp.ndarray), \"camtoworlds should be a numpy array\"\n\n  # Check distortion_params type\n  if distortion_params is not None:\n    assert isinstance(distortion_params, dict), \"distortion_params should be a dictionary\"\n\n  # Check camtype\n  assert camtype in [\n    ProjectionType.PERSPECTIVE\n  ], \"Only perspective projection is supported for now\"\n\n  # Check if pixtocams is a batch of matrices\n  if pixtocams.ndim > points.ndim:\n    assert pixtocams.shape[0] == points.shape[0], \"pixtocams and points should have the same leading dimension\"\n    pixtocams = pixtocams[None]\n\n  # Check if camtoworlds is a batch of matrices\n  if camtoworlds.ndim > points.ndim:\n    assert camtoworlds.shape[0] == points.shape[0], \"camtoworlds and points should have the same leading dimension\"\n    camtoworlds = camtoworlds[None]\n\n  # Check if distortion_params is a batch of parameters\n  if distortion_params is not None:\n    if \"k\" in distortion_params:\n      assert distortion_params[\"k\"].shape[-2:] == (\n        1,\n        5,\n      ), \"distortion_params['"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  if distortion_params is not None:\n    k1 = distortion_params[\"k1\"]\n    k2 = distortion_params[\"k2\"]\n    p1 = distortion_params[\"p1\"]\n    p2 = distortion_params[\"p2\"]\n\n  # Project 3D points to 2D using camera intrinsics and extrinsics\n  points = xnp.concatenate([points, xnp.ones_like(points[..., :1])], axis=-1)\n  points = xnp.einsum(\"...ij,...j->...i\", pixtocams, points)\n  points = xnp.einsum(\"...ij,...j->...i\", camtoworlds, points)\n\n  # Apply radial distortion if distortion parameters are provided\n  if distortion_params is not None:\n    r2 = xnp.sum(points[..., :2] ** 2, axis=-1)\n    radial_distortion = 1.0 + k1 * r2 + k2 * r2 ** 2\n    tangential_distortion_x = 2.0 * p1 * points[..., 0] * points[..., 1] + p2 * (\n      r2 + 2.0 * points[..., 0] ** 2\n    )\n    tangential_distortion_y = p1 * (r2 + 2.0 * points[..., 1] ** 2) + 2.0 * p2 * points[..., 0] * points[..., 1]\n    points[..., 0] = (\n      points[..., 0] * radial_distortion + tangential_distortion_x\n    )\n    points[..., 1] = (\n      points[..., 1] * radial_distortion + tangential_distortion_y\n    )\n\n  # Convert points from camera coordinates to pixel coordinates\n  coordinates = xnp.zeros_like(points)\n  coordinates[..., 0] = points[..., 0] / points[..., 2]\n  coordinates[..., 1] = points[..., "}
{"namespace": "camera_utils.points_to_pixels", "completion": "  if distortion_params is not None:\n    if \"radial\" in distortion_params:\n      radial = distortion_params[\"radial\"]\n    else:\n      radial = xnp.zeros(1)\n    if \"tangential\" in distortion_params:\n      tangential = distortion_params[\"tangential\"]\n    else:\n      tangential = xnp.zeros(2)\n    if \"center\" in distortion_params:\n      center = distortion_params[\"center\"]\n    else:\n      center = xnp.zeros(2)\n  else:\n    radial = None\n    tangential = None\n    center = None\n\n  # Convert points to homogeneous coordinates\n  points_homogeneous = xnp.concatenate(\n      (points, xnp.ones((points.shape[0], 1))), axis=1\n  )\n\n  # Transform points from world to camera coordinates\n  points_camera = xnp.einsum(\"...ij,...jk->...ik\", camtoworlds, points_homogeneous)\n\n  # Project points onto the image plane\n  if camtype == ProjectionType.PERSPECTIVE:\n    # Apply perspective projection\n    points_camera_homogeneous = xnp.concatenate(\n        (points_camera, xnp.ones((points_camera.shape[0], 1))), axis=1\n    )\n    points_image = xnp.einsum(\"...ij,...jk->...ik\", pixtocams, points_camera_homogeneous)\n    points_image = points_image[..., :2] / points_image[..., 2:3]\n  else:\n    raise NotImplementedError(\"Only perspective projection is supported.\")\n\n  # Apply lens distortion\n  if radial is not None:\n    # Calculate radial distortion\n    r2 = xnp.sum(points_image**2, axis=-1, keepdims=True)\n    r4 = r2**2\n    r6 = r4 * r2\n    radial_distortion = (\n        radial[0] * r2 + radial"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Validate input arguments\n  assert len(pixtocams.shape) == 4\n  assert len(camtoworlds.shape) == 4\n  assert points.shape[-1] == 3\n  assert points.shape[:-1] == pixtocams.shape[:-2]\n  assert points.shape[:-1] == camtoworlds.shape[:-2]\n\n  # Compute the coordinates in the camera coordinate system\n  coordinates = xnp.einsum(\"...ij,...j->...i\", pixtocams, points)\n\n  # Compute the depth values\n  depth = xnp.linalg.norm(coordinates, axis=-1)\n\n  # Apply camera distortion\n  if distortion_params is not None:\n    if \"radial\" in distortion_params:\n      coordinates = radial_distortion(\n          coordinates, distortion_params[\"radial\"], xnp=xnp\n      )\n    if \"tangential\" in distortion_params:\n      coordinates = tangential_distortion(\n          coordinates, distortion_params[\"tangential\"], xnp=xnp\n      )\n\n  # Apply camera projection\n  if camtype == ProjectionType.PERSPECTIVE:\n    coordinates = perspective_projection(coordinates, xnp=xnp)\n\n  # Apply camera extrinsics\n  coordinates = xnp.einsum(\"...ij,...j->...i\", camtoworlds, coordinates)\n\n  return coordinates, depth\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Check if input arrays are of correct shape and type\n  assert points.shape[-1] == 3, \"points must be a 3D coordinate\"\n  assert pixtocams.shape[-2:] == (3, 3), \"pixtocams must be a 3x3 matrix\"\n  assert camtoworlds.shape[-2:] == (3, 4), \"camtoworlds must be a 3x4 matrix\"\n  assert isinstance(\n    distortion_params, dict\n  ), \"distortion_params must be a dictionary of floats or arrays\"\n\n  # Convert input arrays to numpy arrays if not already\n  points = xnp.asarray(points)\n  pixtocams = xnp.asarray(pixtocams)\n  camtoworlds = xnp.asarray(camtoworlds)\n\n  # Calculate the camera coordinates of the points\n  camtoworlds = xnp.transpose(camtoworlds, (0, 2, 1))\n  points = xnp.matmul(camtoworlds, xnp.concatenate((points, xnp.ones((*points.shape[:-1], 1))), axis=-1)[..., None])[..., 0]\n\n  # Apply lens distortion to the camera coordinates\n  if distortion_params is not None:\n    points = lens_distortion(points, distortion_params)\n\n  # Project the points onto the image plane using the camera intrinsics\n  points = xnp.matmul(pixtocams, points[..., None])[..., 0]\n\n  # Compute the depth values\n  depth = points[..., -1]\n  points = points[..., :-1] / depth[..., None]\n\n  return points, depth\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4)\n  else:\n    w = screw_axis[:3] / theta\n    v = screw_axis[3:]\n    w_hat = jnp.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]])\n    return jnp.eye(4) + jnp.sin(theta) * w_hat + (1 - jnp.cos(theta)) * w_hat @ w_hat\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4)\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:]\n\n  w_hat = jnp.array([\n      [0, -w[2], w[1]],\n      [w[2], 0, -w[0]],\n      [-w[1], w[0], 0],\n  ])\n  v_hat = jnp.array([\n      [0, -v[2], v[1]],\n      [v[2], 0, -v[0]],\n      [-v[1], v[0], 0],\n  ])\n\n  return jnp.eye(4) + jnp.sin(theta) * w_hat + (1 - jnp.cos(theta)) * w_hat @ w_hat + (\n      theta - jnp.sin(theta)\n  ) * w_hat @ v_hat\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the angle-axis rotation and translation components from the screw axis\n  theta = jnp.linalg.norm(screw_axis[:3])\n  w = screw_axis[:3] / (theta + eps)\n  v = screw_axis[3:]\n\n  # Compute the skew-symmetric matrix of w\n  wx = jnp.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(theta) * wx + (1 - jnp.cos(theta)) * wx @ wx\n\n  # Compute the translation matrix\n  t = jnp.eye(3) @ v.reshape(3, 1)\n\n  # Construct the homogeneous transformation matrix\n  T = jnp.block([[R, t], [jnp.zeros((1, 4))]])\n\n  return T"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4)\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:]\n  w_hat = jnp.array([\n      [0, -w[2], w[1]],\n      [w[2], 0, -w[0]],\n      [-w[1], w[0], 0]\n  ])\n  return jnp.eye(4) + jnp.sin(theta) * w_hat + (1 - jnp.cos(theta)) * jnp.matmul(w_hat, w_hat) + (theta - jnp.sin(theta)) * jnp.outer(w, v)\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  w = screw_axis[:3] / (theta + eps)\n  v = screw_axis[3:]\n\n  w_skew = jnp.array([[0, -w[2], w[1]],\n                      [w[2], 0, -w[0]],\n                      [-w[1], w[0], 0]])\n\n  R = jnp.eye(3) + jnp.sin(theta) * w_skew + (1 - jnp.cos(theta)) * jnp.matmul(\n      w_skew, w_skew)\n  p = jnp.matmul(jnp.eye(3) * theta + (1 - jnp.cos(theta)) * w_skew +\n                 (theta - jnp.sin(theta)) * jnp.matmul(w_skew, w_skew), v)\n\n  T = jnp.block([[R, p[:, jnp.newaxis]], [jnp.array([0, 0, 0, 1])]])\n\n  return T\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4)\n  else:\n    w = screw_axis[:3] / theta\n    v = screw_axis[3:]\n    w_hat = jnp.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]])\n    exp_w = jnp.eye(3) + jnp.sin(theta) * w_hat + (1 - jnp.cos(theta)) * jnp.matmul(w_hat, w_hat)\n    exp_v = jnp.matmul(jnp.eye(3) * theta, v)\n    exp_se3 = jnp.block([[exp_w, exp_v], [jnp.zeros((1, 3)), 1]])\n    return exp_se3\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4)\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:]\n  w_hat = jnp.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]])\n  return jnp.array([[jnp.cos(theta), -jnp.sin(theta) * w_hat, v],\n                    [jnp.sin(theta) * w_hat, jnp.cos(theta) * jnp.eye(3),\n                     jnp.zeros(3)], [0, 0, 1]])\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the angle-axis rotation and translation from the screw axis\n  theta = jnp.linalg.norm(screw_axis[:3])\n  w = screw_axis[:3] / (theta + eps)\n  v = screw_axis[3:]\n\n  # Compute the skew-symmetric matrix of w\n  wx = jnp.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]])\n\n  # Compute the rotation matrix and translation vector\n  R = jnp.eye(3) + jnp.sin(theta) * wx + (1 - jnp.cos(theta)) * wx @ wx\n  p = (jnp.eye(3) - R) @ v\n\n  # Construct the homogeneous transformation matrix\n  T = jnp.block([[R, p[:, None]], [jnp.zeros((1, 4))]])\n  T = jnp.vstack((T, [0, 0, 0, 1]))\n\n  return T\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n\n  if theta < eps:\n    return jnp.eye(4)\n\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:]\n\n  w_hat = jnp.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]])\n\n  return jnp.eye(4) + jnp.sin(theta) * w_hat + (1 - jnp.cos(theta)) * jnp.dot(w_hat, w_hat)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4)\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:]\n  w_hat = jnp.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]])\n  return jnp.eye(4) + jnp.sin(theta) * w_hat + (1 - jnp.cos(theta)) * w_hat @ w_hat\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4)\n\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:]\n\n  w_skew = jnp.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]])\n\n  exp_w = jnp.eye(3) + jnp.sin(theta) * w_skew + (1 - jnp.cos(theta)) * w_skew @ w_skew\n\n  exp_se3 = jnp.block([[exp_w, v[:, None]], [jnp.zeros(3), 1]])\n\n  return exp_se3\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4)\n\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:] / theta\n  w_hat = so3_hat(w)\n\n  return jnp.eye(4) + jnp.sin(theta) * w_hat + (1 - jnp.cos(theta)) * w_hat @ w_hat\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  w = screw_axis[:3] / (theta + eps)\n  v = screw_axis[3:]\n\n  # Calculate the skew-symmetric matrix of w\n  wx = jnp.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]])\n\n  # Calculate the rotation matrix\n  R = jnp.eye(3) + wx * jnp.sin(theta) + jnp.linalg.matrix_power(wx, 2) * (1 - jnp.cos(theta))\n\n  # Calculate the translation vector\n  t = jnp.dot(jnp.linalg.inv(R), v)\n\n  # Create the homogeneous transformation matrix\n  T = jnp.block([[R, t[:, None]], [jnp.zeros((1, 4))]])\n  T = T.at[3, 3].set(1)\n\n  return T\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the angle-axis rotation and translation from the screw axis\n  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4)\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:]\n\n  # Compute the rotation matrix\n  w_hat = jnp.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]])\n  R = jnp.eye(3) + jnp.sin(theta) * w_hat + (1 - jnp.cos(theta)) * jnp.dot(\n      w_hat, w_hat\n  )\n\n  # Compute the translation matrix\n  t = jnp.dot(jnp.eye(3) - R, v)\n\n  # Create the homogeneous transformation matrix\n  T = jnp.zeros((4, 4))\n  T[:3, :3] = R\n  T[:3, 3] = t\n  T[3, 3] = 1\n\n  return T\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta > eps:\n    w = screw_axis[:3] / theta\n    v = screw_axis[3:]\n    w_hat = jnp.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]])\n    exp_w = jnp.eye(3) + jnp.sin(theta) * w_hat + (\n        1 - jnp.cos(theta)\n    ) * jnp.linalg.matrix_power(w_hat, 2)\n    exp_v = (jnp.eye(3) - exp_w) @ v[:, None]\n    exp_v = exp_v.squeeze()\n  else:\n    exp_w = jnp.eye(3)\n    exp_v = screw_axis[3:]\n\n  exp_se3 = jnp.block([[exp_w, exp_v[:, None]], [jnp.zeros((1, 4))]])\n  exp_se3 = jnp.concatenate([exp_se3, jnp.array([[0, 0, 0, 1]])], axis=0)\n  return exp_se3\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the angle-axis rotation and translation from the screw axis\n  w, v = screw_axis[:3], screw_axis[3:]\n\n  # Compute the magnitude of the angle-axis rotation\n  theta = jnp.linalg.norm(w)\n\n  # Check if the magnitude of the angle-axis rotation is zero\n  if theta < eps:\n    # If the magnitude is zero, return the identity matrix\n    return jnp.eye(4)\n\n  # Compute the normalized angle-axis rotation\n  w = w / theta\n\n  # Compute the skew-symmetric matrix of the angle-axis rotation\n  wx = jnp.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(theta) * wx + (1 - jnp.cos(theta)) * jnp.matmul(wx, wx)\n\n  # Compute the translation matrix\n  t = jnp.matmul(jnp.eye(3) * theta, v)\n\n  # Create the homogeneous transformation matrix\n  T = jnp.zeros((4, 4))\n  T[:3, :3] = R\n  T[:3, 3] = t\n  T[3, 3] = 1\n\n  return T"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  w = screw_axis[:3] / (theta + eps)\n  v = screw_axis[3:]\n\n  w_skew = jnp.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]])\n  R = jnp.eye(3) + jnp.sin(theta) * w_skew + (1 - jnp.cos(theta)) * jnp.matmul(\n      w_skew, w_skew\n  )\n  p = jnp.matmul(jnp.eye(3) * theta + (1 - jnp.cos(theta)) * w_skew + (\n      theta - jnp.sin(theta)\n  ) * jnp.matmul(w_skew, w_skew), v\n  )\n\n  T = jnp.eye(4)\n  T[:3, :3] = R\n  T[:3, 3] = p\n  return T\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the angle-axis rotation and translation from the screw axis\n  w = screw_axis[:3]\n  v = screw_axis[3:]\n\n  # Compute the magnitude of the angle-axis rotation\n  theta = jnp.linalg.norm(w)\n\n  # If the magnitude of the angle-axis rotation is small, use a Taylor series approximation\n  if theta < eps:\n    R = jnp.eye(3) + (jnp.sin(theta) / theta) * jnp.cross(jnp.eye(3), w) + (\n        (1 - jnp.cos(theta)) / (theta**2)) * (jnp.cross(jnp.eye(3), w)\n                                              @ jnp.cross(jnp.eye(3), w))\n    t = v\n\n  # Otherwise, use the standard formula for the exponential map\n  else:\n    R = jnp.eye(3) + jnp.sin(theta) / theta * jnp.cross(jnp.eye(3), w) + (\n        1 - jnp.cos(theta)) / (theta**2) * jnp.cross(jnp.eye(3), w) @ jnp.cross(\n            jnp.eye(3), w)\n    t = (1 - jnp.cos(theta)) / theta * jnp.cross(jnp.eye(3), w) @ v + (\n        jnp.sin(theta) / theta) * v\n\n  # Construct the homogeneous transformation matrix\n  T = jnp.eye(4)\n  T[:3, :3] = R\n  T[:3, 3] = t\n\n  return T"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta > eps:\n    w = screw_axis[:3] / theta\n    v = screw_axis[3:]\n    wx = skew_symmetric(w)\n    R = jnp.eye(3) + jnp.sin(theta) * wx + (1 - jnp.cos(theta)) * wx @ wx\n    p = (jnp.eye(3) - R) @ w @ v\n    return jnp.block([[R, p[:, None]], [jnp.zeros((1, 4))]])\n  else:\n    return jnp.eye(4) + skew_symmetric(screw_axis)\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Split the screw axis into rotation and translation components\n  w, v = screw_axis[:3], screw_axis[3:]\n\n  # Calculate the magnitude of the rotation component\n  theta = jnp.linalg.norm(w)\n\n  # Check if the rotation component is small enough to be considered zero\n  if theta < eps:\n    # If the rotation component is zero, use a small rotation matrix\n    R = jnp.eye(3)\n  else:\n    # If the rotation component is non-zero, use a rotation matrix\n    w_hat = so3_hat(w)\n    R = jnp.eye(3) + jnp.sin(theta) / theta * w_hat + (1 - jnp.cos(theta)) / theta**2 * jnp.matmul(w_hat, w_hat)\n\n  # Create the homogeneous transformation matrix\n  T = jnp.eye(4)\n  T[:3, :3] = R\n  T[:3, 3] = v\n\n  return T\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  if theta < eps:\n    return jnp.eye(3)\n  k = axis_angle / theta\n  K = jnp.array([[0, -k[2], k[1]], [k[2], 0, -k[0]], [-k[1], k[0], 0]])\n  R = jnp.eye(3) + jnp.sin(theta) * K + (1 - jnp.cos(theta)) * K @ K\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  k = axis_angle / (theta + eps)\n  K = jnp.array([[0, -k[2], k[1]], [k[2], 0, -k[0]], [-k[1], k[0], 0]])\n  R = jnp.eye(3) + jnp.sin(theta) * K + (1 - jnp.cos(theta)) * K @ K\n  return R\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  if theta < eps:\n    return jnp.eye(3)\n  else:\n    k = axis_angle / theta\n    kx = jnp.array([[0, -k[2], k[1]], [k[2], 0, -k[0]], [-k[1], k[0], 0]])\n    return jnp.eye(3) + jnp.sin(theta) * kx + (1 - jnp.cos(theta)) * kx @ kx\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the axis and angle of rotation\n  axis = axis_angle[:3]\n  angle = jnp.linalg.norm(axis_angle[:3])\n\n  # Check if the angle is sufficiently small to avoid division by zero\n  if angle < eps:\n    # If the angle is small, return the identity matrix as the rotation matrix\n    return jnp.eye(3)\n\n  # Normalize the axis of rotation\n  axis = axis / angle\n\n  # Compute the rotation matrix using Rodrigues' formula\n  K = jnp.array([[0, -axis[2], axis[1]],\n                 [axis[2], 0, -axis[0]],\n                 [-axis[1], axis[0], 0]])\n  R = jnp.eye(3) + jnp.sin(angle) * K + (1 - jnp.cos(angle)) * jnp.matmul(K, K)\n\n  # Return the rotation matrix\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the axis and angle from the input\n  axis = axis_angle[:3]\n  angle = jnp.linalg.norm(axis_angle[:3])\n\n  # Compute the skew-symmetric matrix of the axis\n  k = jnp.array([[0, -axis[2], axis[1]], [axis[2], 0, -axis[0]], [-axis[1], axis[0], 0]])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(angle) * k + (1 - jnp.cos(angle)) * jnp.matmul(k, k)\n\n  # Return the rotation matrix\n  return R\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the angle and axis from the axis-angle representation\n  angle = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (angle + eps)\n\n  # Compute the rotation matrix using Rodrigues' formula\n  kx, ky, kz = jnp.sin(angle) * axis\n  c = jnp.cos(angle)\n  s = jnp.sin(angle)\n  R = jnp.array(\n      [\n          [c + kx * kx * (1 - c), kx * ky * (1 - c) - kz * s, kx * kz * (1 - c) + ky * s],\n          [ky * kx * (1 - c) + kz * s, c + ky * ky * (1 - c), ky * kz * (1 - c) - kx * s],\n          [kz * kx * (1 - c) - ky * s, kz * ky * (1 - c) + kx * s, c + kz * kz * (1 - c)],\n      ]\n  )\n\n  return R\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  if theta < eps:\n    return jnp.eye(3)\n  axis_angle /= theta\n  K = jnp.array([[0, -axis_angle[2], axis_angle[1]],\n                 [axis_angle[2], 0, -axis_angle[0]],\n                 [-axis_angle[1], axis_angle[0], 0]])\n  return jnp.eye(3) + jnp.sin(theta) * K + (1 - jnp.cos(theta)) * jnp.matmul(K, K)\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the norm of the axis-angle vector\n  theta = jnp.linalg.norm(axis_angle)\n\n  # Check if the norm is close to zero\n  if theta < eps:\n    # If the norm is close to zero, return the identity matrix\n    return jnp.eye(3)\n\n  # Normalize the axis-angle vector\n  axis = axis_angle / theta\n\n  # Compute the skew-symmetric matrix of the axis\n  w = jnp.array([[0, -axis[2], axis[1]],\n                 [axis[2], 0, -axis[0]],\n                 [-axis[1], axis[0], 0]])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(theta) * w + (1 - jnp.cos(theta)) * jnp.dot(w, w)\n\n  return R\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the axis and angle of rotation from the input axis_angle\n  axis = axis_angle[:3]\n  angle = jnp.linalg.norm(axis)\n\n  # Check if the angle of rotation is sufficiently small to avoid numerical instability\n  if angle < eps:\n    return jnp.eye(3)\n\n  # Normalize the axis of rotation\n  axis = axis / angle\n\n  # Compute the rotation matrix using Rodrigues' formula\n  K = jnp.array([[0, -axis[2], axis[1]],\n                 [axis[2], 0, -axis[0]],\n                 [-axis[1], axis[0], 0]])\n  R = jnp.eye(3) + jnp.sin(angle) * K + (1 - jnp.cos(angle)) * K @ K\n\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the axis and angle of rotation from the input axis-angle vector\n  axis = axis_angle[:3]\n  angle = jnp.linalg.norm(axis_angle[:3])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  if angle < eps:\n    # If the angle of rotation is small, use the identity matrix as the rotation matrix\n    return jnp.eye(3)\n  else:\n    # Normalize the axis of rotation\n    axis = axis / angle\n\n    # Compute the skew-symmetric matrix from the axis of rotation\n    wx = jnp.array([[0, -axis[2], axis[1]],\n                    [axis[2], 0, -axis[0]],\n                    [-axis[1], axis[0], 0]])\n\n    # Compute the rotation matrix using Rodrigues' formula\n    R = jnp.eye(3) + jnp.sin(angle) * wx + (1 - jnp.cos(angle)) * jnp.dot(wx, wx)\n\n    return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the axis and angle from the input axis-angle representation\n  axis = axis_angle[:3]\n  angle = jnp.linalg.norm(axis_angle[:3])\n\n  # Check if the angle is close to zero\n  if angle < eps:\n    # If the angle is close to zero, return the identity matrix\n    return jnp.eye(3)\n\n  # Normalize the axis\n  axis = axis / angle\n\n  # Compute the rotation matrix using Rodrigues' formula\n  rotation_matrix = jnp.eye(3) + jnp.sin(angle) * so3.hat(axis) + (1 - jnp.cos(angle)) * jnp.linalg.matrix_power(so3.hat(axis), 2)\n\n  # Return the rotation matrix\n  return rotation_matrix\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  k = axis_angle / (theta + eps)\n  kx = jnp.array([[0, -k[2], k[1]], [k[2], 0, -k[0]], [-k[1], k[0], 0]])\n  R = jnp.eye(3) + jnp.sin(theta) * kx + (1 - jnp.cos(theta)) * kx @ kx\n  return R\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the axis and angle of rotation from the input axis-angle representation\n  axis = axis_angle[:3]\n  angle = jnp.linalg.norm(axis)\n\n  # Compute the skew-symmetric matrix from the axis of rotation\n  skew_symmetric_matrix = jnp.array([[0, -axis[2], axis[1]],\n                                     [axis[2], 0, -axis[0]],\n                                     [-axis[1], axis[0], 0]])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  rotation_matrix = jnp.eye(3) + jnp.sin(angle) * skew_symmetric_matrix + (1 - jnp.cos(angle)) * jnp.matmul(skew_symmetric_matrix, skew_symmetric_matrix)\n\n  # Add a small epsilon value to the angle of rotation to avoid division by zero or very small values that could lead to numerical instability\n  angle += eps\n\n  # Compute the scaled rotation matrix\n  scaled_rotation_matrix = jnp.array([[0, -axis[2], axis[1]],\n                                      [axis[2], 0, -axis[0]],\n                                      [-axis[1], axis[0], 0]]) / angle\n\n  # Compute the exponential map from the Lie algebra so3 to the Lie group SO3\n  exponential_map = jnp.eye(3) + jnp.sin(angle) * scaled_rotation_matrix + (1 - jnp.cos(angle)) * jnp.matmul(scaled_rotation_matrix, scaled_rotation_matrix)\n\n  return exponential_map"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  k = axis_angle / (theta + eps)\n  k = jnp.expand_dims(k, 1)\n  K = jnp.dot(k, jnp.transpose(k))\n\n  R = jnp.eye(3) + jnp.sin(theta) * K + (1 - jnp.cos(theta)) * jnp.dot(K, K)\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.array(axis_angle)\n\n  # Check if the input is a 3-vector\n  if axis_angle.shape != (3,):\n    raise ValueError(\"Input must be a 3-vector\")\n\n  # Normalize the axis of rotation\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n\n  # Compute the angle of rotation\n  theta = jnp.linalg.norm(axis_angle)\n\n  # Compute the skew-symmetric matrix from the axis of rotation\n  wx = jnp.array([\n      [0, -axis_angle[2], axis_angle[1]],\n      [axis_angle[2], 0, -axis_angle[0]],\n      [-axis_angle[1], axis_angle[0], 0]\n  ])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(theta) * wx + (1 - jnp.cos(theta)) * jnp.matmul(wx, wx)\n\n  # Add a small epsilon to the diagonal elements of the rotation matrix to avoid numerical instability\n  R = R + jnp.eye(3) * eps\n\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the magnitude of the axis-angle vector\n  theta = jnp.linalg.norm(axis_angle)\n\n  # If the magnitude is close to zero, return the identity matrix\n  if theta < eps:\n    return jnp.eye(3)\n\n  # Normalize the axis-angle vector\n  axis = axis_angle / theta\n\n  # Compute the skew-symmetric matrix of the axis vector\n  wx = jnp.array([\n    [0, -axis[2], axis[1]],\n    [axis[2], 0, -axis[0]],\n    [-axis[1], axis[0], 0]\n  ])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(theta) * wx + (1 - jnp.cos(theta)) * jnp.matmul(wx, wx)\n\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  axis_angle = axis_angle / (theta + eps)\n\n  theta = jnp.clip(theta, 0, jnp.pi)\n\n  axis_angle_hat = jnp.array([\n      [0, -axis_angle[2], axis_angle[1]],\n      [axis_angle[2], 0, -axis_angle[0]],\n      [-axis_angle[1], axis_angle[0], 0]\n  ])\n\n  return jnp.eye(3) + jnp.sin(theta) * axis_angle_hat + (\n      1 - jnp.cos(theta)) * axis_angle_hat @ axis_angle_hat\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the axis of rotation and the magnitude of rotation from the input\n  axis = axis_angle[:3]\n  theta = jnp.linalg.norm(axis)\n\n  # Check if the angle of rotation is sufficiently small\n  if theta < eps:\n    # If the angle is small, use the Rodrigues formula to compute the rotation matrix\n    return jnp.eye(3) + jnp.outer(axis, axis) * (1 - jnp.cos(theta)) / (theta**2) + jnp.array([[0, -axis[2], axis[1]], [axis[2], 0, -axis[0]], [-axis[1], axis[0], 0]]) * jnp.sin(theta) / theta\n  else:\n    # If the angle is large, use the Rodrigues formula to compute the rotation matrix\n    return jnp.eye(3) + jnp.sin(theta) / theta * jnp.array([[0, -axis[2], axis[1]], [axis[2], 0, -axis[0]], [-axis[1], axis[0], 0]]) + (1 - jnp.cos(theta)) / (theta**2) * jnp.outer(axis, axis)\n\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n\n  if theta < eps:\n    return jnp.eye(3)\n\n  axis_angle = axis_angle / theta\n\n  theta_sq = theta * theta\n  theta_sq_2 = theta_sq * theta_sq\n\n  theta_sq_2_5 = theta_sq_2 * theta_sq * theta\n\n  axis_angle_cross = jnp.cross(axis_angle, axis_angle)\n\n  return jnp.eye(3) + jnp.sin(theta) * axis_angle_cross + (1 - jnp.cos(theta)) * axis_angle_cross * axis_angle_cross\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n  theta = jnp.linalg.norm(axis_angle)\n\n  if theta < eps:\n    return jnp.eye(3)\n\n  axis = axis_angle / theta\n\n  x, y, z = axis\n  s = jnp.sin(theta)\n  c = jnp.cos(theta)\n  C = 1 - c\n\n  xs = x * s\n  ys = y * s\n  zs = z * s\n  xC = x * C\n  yC = y * C\n  zC = z * C\n  xyC = x * yC\n  yzC = y * zC\n  zxC = z * xC\n\n  return jnp.array([[x * xC + c, xyC - zs, zxC + ys],\n                    [xyC + zs, y * yC + c, yzC - xs],\n                    [zxC - ys, yzC + xs, z * zC + c]])\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  d = d / jnp.linalg.norm(d)\n  mean = (t1 - t0) * d\n  if diag:\n    cov = (t1 - t0) * base_radius ** 2 * jnp.eye(3)\n  else:\n    cov = (t1 - t0) * base_radius ** 2 * jnp.outer(d, d)\n  return mean, cov"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  d = d / jnp.linalg.norm(d)\n  mu = t0 * d\n  sigma = t0 * t1 / (t1 - t0) * base_radius * d\n  if diag:\n    cov = sigma * sigma\n  else:\n    cov = jnp.outer(sigma, sigma)\n  return mu, cov"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  d = d / jnp.linalg.norm(d)\n  mean = (t0 + t1) * d / 2\n  r0 = base_radius * t0\n  r1 = base_radius * t1\n  cov = jnp.zeros((3, 3))\n  cov = cov.at[0, 0].set(jnp.square(r0 + r1) / 4)\n  cov = cov.at[1, 1].set(jnp.square(r0 + r1) / 4)\n  cov = cov.at[2, 2].set(jnp.square(r0 + r1) / 4)\n  cov = cov.at[0, 1].set(r0 * r1 * d[0] * d[1] / 4)\n  cov = cov.at[0, 2].set(r0 * r1 * d[0] * d[2] / 4)\n  cov = cov.at[1, 2].set(r0 * r1 * d[1] * d[2] / 4)\n  cov = cov.at[1, 0].set(cov[0, 1])\n  cov = cov.at[2, 0].set(cov[0, 2])\n  cov = cov.at[2, 1].set(cov[1, 2])\n  if diag:\n    cov = jnp.diag(jnp.diag(cov))\n  return mean, cov\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  d = d / jnp.linalg.norm(d)\n  t0 = t0\n  t1 = t1\n  base_radius = base_radius\n  diag = diag\n\n  # Calculate the mean of the Gaussian distribution\n  mean = (t0 * d + t1 * d) / (t0 + t1)\n\n  # Calculate the covariance matrix of the Gaussian distribution\n  if diag:\n    cov = (t0 + t1) * (t0 * t1 / (t0 + t1) ** 2) * (d * d.T)\n  else:\n    cov = (t0 + t1) * (t0 * t1 / (t0 + t1) ** 2) * jnp.outer(d, d)\n\n  return mean, cov"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Normalize d\n  d = d / jnp.linalg.norm(d)\n\n  # Compute the direction of the frustum\n  d_hat = d\n\n  # Compute the mean of the Gaussian distribution\n  mean = (t0 + t1) / 2 * d_hat\n\n  # Compute the covariance of the Gaussian distribution\n  if diag:\n    cov = jnp.diag((t1 - t0) * d_hat * d_hat * base_radius)\n  else:\n    cov = jnp.outer(d_hat, d_hat) * (t1 - t0) * base_radius\n\n  return mean, cov"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  d = jnp.array(d, dtype=jnp.float32)\n  d = d / jnp.linalg.norm(d)\n  t0 = jnp.float32(t0)\n  t1 = jnp.float32(t1)\n  base_radius = jnp.float32(base_radius)\n\n  mean = d * (t0 + t1) / 2.0\n  cov = (t1 - t0) * (base_radius ** 2) / 4.0 * jnp.eye(3)\n  if diag:\n    cov = jnp.diag(jnp.diag(cov))\n\n  return mean, cov"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  d = d / jnp.linalg.norm(d)\n  t0 = t0 / jnp.linalg.norm(d)\n  t1 = t1 / jnp.linalg.norm(d)\n  mean = jnp.array([0.0, 0.0, 0.0])\n  cov = jnp.zeros((3, 3))\n\n  if diag:\n    cov = jnp.array([[base_radius ** 2 * (t1 ** 2 - t0 ** 2), 0.0, 0.0],\n                     [0.0, base_radius ** 2 * (t1 ** 2 - t0 ** 2), 0.0],\n                     [0.0, 0.0, base_radius ** 2 * (t1 ** 2 - t0 ** 2)]])\n  else:\n    cov = jnp.array([[base_radius ** 2 * (t1 ** 2 - t0 ** 2), 0.0, 0.0],\n                     [0.0, base_radius ** 2 * (t1 ** 2 - t0 ** 2), 0.0],\n                     [0.0, 0.0, base_radius ** 2 * (t1 ** 2 - t0 ** 2)]])\n\n  return mean, cov\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Normalize the direction vector\n  d = d / jnp.linalg.norm(d)\n\n  # Calculate the angle between the direction vector and the x-axis\n  theta = jnp.arccos(d[0])\n\n  # Calculate the rotation matrix to align the cone with the x-axis\n  R = jnp.array([[1, 0, 0], [0, jnp.cos(theta), -jnp.sin(theta)], [0, jnp.sin(theta), jnp.cos(theta)]])\n\n  # Rotate the direction vector and the starting and ending distances to align with the x-axis\n  d_rot = jnp.dot(R, d)\n  t0_rot = jnp.dot(R, jnp.array([t0, 0, 0]))\n  t1_rot = jnp.dot(R, jnp.array([t1, 0, 0]))\n\n  # Calculate the cone's height and base radius\n  h = t1_rot[0] - t0_rot[0]\n  r0 = base_radius * t0_rot[0]\n  r1 = base_radius * t1_rot[0]\n\n  # Calculate the Gaussian distribution's mean\n  mean = jnp.array([(r0 + r1) / 2, 0, 0])\n\n  # Calculate the Gaussian distribution's covariance\n  if diag:\n    cov = jnp.array([[(r1 - r0) ** 2 / 12, 0, 0], [0, h ** 2 / 12, 0], [0, 0, (r1 + r0) ** 2 / 4]])\n  else:\n    cov = jnp.array([[(r1 - r0) ** 2 / 3, 0, 0], [0, h ** 2 / 3, 0], [0, 0, (r1 + r0) ** 2 / 6]])\n\n  return mean, cov\n\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  d = d / jnp.linalg.norm(d)\n  t0 = jnp.array([t0])\n  t1 = jnp.array([t1])\n  base_radius = jnp.array([base_radius])\n  mu = (t0 * d + t1 * d) / 2\n  r = (t0 + t1) / 2 * base_radius\n  if diag:\n    cov = jnp.diag(jnp.square(r))\n  else:\n    cov = jnp.outer(r, r)\n  return mu, cov\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Compute the direction of the frustum\n  d = d / jnp.linalg.norm(d)\n\n  # Compute the mean of the Gaussian distribution\n  mu = d * (t0 + t1) / 2\n\n  # Compute the covariance of the Gaussian distribution\n  if diag:\n    cov = jnp.diag(jnp.array([base_radius * (t1 - t0) / 2, base_radius * (t1 - t0) / 2, base_radius * (t1 - t0) / 2]))\n  else:\n    cov = jnp.array([[base_radius * (t1 - t0) / 2, 0, 0],\n                     [0, base_radius * (t1 - t0) / 2, 0],\n                     [0, 0, base_radius * (t1 - t0) / 2]])\n\n  return mu, cov\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the direction of the cone\n  d = d / jnp.linalg.norm(d)\n\n  # Calculate the angle between the cone axis and the x-axis\n  theta = jnp.arccos(d[0])\n\n  # Calculate the rotation matrix that aligns the cone axis with the x-axis\n  R = jnp.array([[jnp.cos(theta), -jnp.sin(theta), 0],\n                 [jnp.sin(theta), jnp.cos(theta), 0],\n                 [0, 0, 1]])\n\n  # Transform the cone axis and the starting and ending distances into the new coordinate system\n  d = jnp.dot(R, d)\n  t0 = jnp.dot(R, jnp.array([t0, 0, 0]))\n  t1 = jnp.dot(R, jnp.array([t1, 0, 0]))\n\n  # Calculate the mean of the approximated Gaussian distribution\n  mu = (t1 - t0) / 2\n\n  # Calculate the covariance of the approximated Gaussian distribution\n  if diag:\n    sigma = jnp.array([[1 / 12 * (t1[0] - t0[0]) ** 2, 0, 0],\n                       [0, 1 / 12 * (t1[0] - t0[0]) ** 2 * (1 + d[2] ** 2), 0],\n                       [0, 0, 1 / 12 * (t1[0] - t0[0]) ** 2 * (1 + d[2] ** 2) * (1 - d[2] ** 2)]])\n  else:\n    sigma = jnp.array([[1 / 12 * (t1[0] - t0[0]) ** 2, 0, 0, 0, 0, 0],\n                       [0, 1 / 12 * (t1[0] - t0[0]) ** 2 * (1 + d[2] ** 2),"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the angle between the axis and the line segment connecting the origin to the frustum.\n  angle = jnp.arccos(jnp.dot(d, jnp.array([0., 0., 1.])) / jnp.linalg.norm(d))\n\n  # Calculate the radius of the frustum at the starting distance.\n  r0 = base_radius * t0\n\n  # Calculate the radius of the frustum at the ending distance.\n  r1 = base_radius * t1\n\n  # Calculate the length of the line segment connecting the origin to the frustum.\n  l = t1 - t0\n\n  # Calculate the direction of the frustum.\n  d = d / jnp.linalg.norm(d)\n\n  # Calculate the mean of the Gaussian distribution.\n  mean = jnp.array([0., 0., 0.])\n\n  # Calculate the covariance of the Gaussian distribution.\n  if diag:\n    cov = jnp.array([[r1**2, 0., 0.], [0., r1**2, 0.], [0., 0., r1**2]])\n  else:\n    cov = jnp.array([[r1**2, 0., 0.], [0., r1**2, 0.], [0., 0., r1**2]])\n\n  # Return the mean and covariance of the Gaussian distribution.\n  return mean, cov\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the direction of the cone\n  d = jnp.array(d) / jnp.linalg.norm(d)\n\n  # Calculate the angle between the cone axis and the x-axis\n  phi = jnp.arccos(d[0])\n\n  # Calculate the rotation matrix to align the cone axis with the x-axis\n  R = jnp.array([[jnp.cos(phi), -jnp.sin(phi), 0],\n                 [jnp.sin(phi), jnp.cos(phi), 0],\n                 [0, 0, 1]])\n\n  # Rotate the cone axis and the starting and ending distances to align with the x-axis\n  d = jnp.dot(R, d)\n  t0 = jnp.dot(R, t0 * d)\n  t1 = jnp.dot(R, t1 * d)\n\n  # Calculate the radius of the frustum at the starting and ending distances\n  r0 = base_radius * t0\n  r1 = base_radius * t1\n\n  # Calculate the mean of the Gaussian distribution\n  mean = (r0 * t0 + r1 * t1) / (t0 + t1)\n\n  # Calculate the covariance of the Gaussian distribution\n  if diag:\n    cov = jnp.array([[mean * (r0 + r1) / (t0 + t1), 0, 0],\n                     [0, mean * (r0 + r1) / (t0 + t1), 0],\n                     [0, 0, mean * (r0 + r1) / (t0 + t1)]])\n  else:\n    cov = jnp.array([[mean * (r0 + r1) / (t0 + t1), 0, 0],\n                     [0, mean * (r0 + r1) / (t0 + t1), 0],\n                     [0, 0, mean * (r0 + r1) / (t0 + t1)]])\n\n  return mean, cov\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Normalize d\n  d = d / jnp.linalg.norm(d)\n\n  # Calculate the direction of the frustum\n  d_hat = jnp.array([d[0], d[1], d[2]])\n\n  # Calculate the radius of the frustum at the starting and ending distances\n  r0 = base_radius * t0\n  r1 = base_radius * t1\n\n  # Calculate the height of the frustum\n  h = r0 - r1\n\n  # Calculate the mean of the frustum\n  mean = d_hat * (r0 + r1) / 2\n\n  # Calculate the covariance of the frustum\n  if diag:\n    cov = jnp.diag(jnp.array([h * h / 3, h * h / 12, h * h / 12]))\n  else:\n    cov = jnp.array([[h * h / 3, 0, 0], [0, h * h / 12, 0], [0, 0, h * h / 12]])\n\n  return mean, cov\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Normalize the axis of the cone\n  d = d / jnp.linalg.norm(d)\n\n  # Calculate the direction of the frustum\n  dir = d\n\n  # Calculate the height of the frustum\n  h = t1 - t0\n\n  # Calculate the radius of the frustum at the starting and ending points\n  r0 = base_radius * t0\n  r1 = base_radius * t1\n\n  # Calculate the center of the frustum\n  c = dir * (t0 + t1) / 2\n\n  # Calculate the angle between the axis of the cone and the direction of the frustum\n  angle = jnp.arccos(jnp.dot(d, dir))\n\n  # Calculate the rotation matrix to rotate the frustum to the xy-plane\n  R = jnp.array([[jnp.cos(angle), -jnp.sin(angle)], [jnp.sin(angle), jnp.cos(angle)]])\n\n  # Calculate the rotation matrix to rotate the frustum to the x-axis\n  R_inv = jnp.array([[jnp.cos(angle), jnp.sin(angle)], [-jnp.sin(angle), jnp.cos(angle)]])\n\n  # Calculate the rotation matrix to rotate the frustum to the z-axis\n  R_z = jnp.array([[jnp.cos(angle), -jnp.sin(angle)], [jnp.sin(angle), jnp.cos(angle)]])\n\n  # Calculate the rotation matrix to rotate the frustum to the xy-plane\n  R_z_inv = jnp.array([[jnp.cos(angle), jnp.sin(angle)], [-jnp.sin(angle), jnp.cos(angle)]])\n\n  # Calculate the radius of the frustum at the starting and ending points in the xy-plane\n  r0_xy = r0 * jnp.cos(angle)\n  r1_xy = r1 * jnp.cos(angle)\n\n  # Calcul"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Normalize the direction vector\n  d = d / jnp.linalg.norm(d)\n\n  # Calculate the radius at the start and end of the frustum\n  r0 = base_radius * t0\n  r1 = base_radius * t1\n\n  # Calculate the direction of the frustum\n  d = jnp.array([d[0], d[1], d[2]])\n\n  # Calculate the mean of the Gaussian distribution\n  mean = (r0 * d + r1 * d) / 2\n\n  # Calculate the covariance matrix\n  if diag:\n    # Calculate the diagonal of the covariance matrix\n    diag = jnp.array([(r0 + r1) / 2, (r0 + r1) / 2, (r0 + r1) / 2])\n\n    # Create the covariance matrix\n    cov = jnp.diag(diag)\n  else:\n    # Calculate the off-diagonal elements of the covariance matrix\n    cov_off_diag = jnp.array([\n      r0 * d[1] * d[2],\n      r0 * d[2] * d[0],\n      r0 * d[0] * d[1],\n      r1 * d[1] * d[2],\n      r1 * d[2] * d[0],\n      r1 * d[0] * d[1]\n    ])\n\n    # Create the covariance matrix\n    cov = jnp.diag(diag) + jnp.diag(cov_off_diag, k=1) + jnp.diag(cov_off_diag, k=-1)\n\n  return mean, cov"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Normalize the axis of the cone\n  d = d / jnp.linalg.norm(d)\n\n  # Calculate the direction of the frustum\n  direction = d\n\n  # Calculate the starting and ending radii of the frustum\n  r0 = base_radius * t0\n  r1 = base_radius * t1\n\n  # Calculate the directional cosine of the frustum\n  cos_theta = r0 / t0\n\n  # Calculate the normal vector to the plane of the frustum\n  normal = jnp.cross(direction, jnp.array([0.0, 0.0, 1.0]))\n\n  # Calculate the normal vector to the plane of the frustum\n  normal = normal / jnp.linalg.norm(normal)\n\n  # Calculate the angle between the normal vector and the direction of the frustum\n  angle = jnp.arccos(jnp.dot(normal, direction))\n\n  # Calculate the rotation matrix to rotate the normal vector to the direction of the frustum\n  rotation_matrix = jnp.array([[jnp.cos(angle), -jnp.sin(angle), 0.0],\n                               [jnp.sin(angle), jnp.cos(angle), 0.0],\n                               [0.0, 0.0, 1.0]])\n\n  # Rotate the normal vector to the direction of the frustum\n  normal = jnp.dot(rotation_matrix, normal)\n\n  # Calculate the mean of the Gaussian distribution\n  mean = jnp.array([0.0, 0.0, 0.0])\n\n  # Calculate the covariance of the Gaussian distribution\n  if diag:\n    cov = jnp.array([[r0 * r0, 0.0, 0.0],\n                     [0.0, r1 * r1, 0.0],\n                     [0.0, 0.0, 1.0]])\n  else:\n    cov = jnp.array([[r0 * r0, 0.0, "}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the unit vector in the direction of the cone axis\n  d = d / jnp.linalg.norm(d)\n\n  # Calculate the radius of the frustum at the starting distance\n  r0 = base_radius * t0\n\n  # Calculate the radius of the frustum at the ending distance\n  r1 = base_radius * t1\n\n  # Calculate the angle between the cone axis and the line connecting the starting and ending distances\n  alpha = jnp.arccos(jnp.dot(d, jnp.array([1., 0., 0.])))\n\n  # Calculate the angle between the cone axis and the line connecting the starting and ending distances, rotated by 90 degrees\n  beta = jnp.arccos(jnp.dot(d, jnp.array([0., 1., 0.])))\n\n  # Calculate the height of the frustum\n  h = t1 - t0\n\n  # Calculate the mean of the Gaussian distribution\n  mean = jnp.array([0., 0., 0.])\n\n  # Calculate the covariance matrix of the Gaussian distribution\n  if diag:\n    cov = jnp.diag(jnp.array([r0 ** 2, h ** 2, r1 ** 2]))\n  else:\n    cov = jnp.array([[r0 ** 2, 0., 0.],\n                     [0., h ** 2, 0.],\n                     [0., 0., r1 ** 2]])\n\n  # Rotate the covariance matrix to match the cone axis\n  cov = jnp.matmul(jnp.array([[jnp.cos(alpha), -jnp.sin(alpha), 0.],\n                              [jnp.sin(alpha), jnp.cos(alpha), 0.],\n                              [0., 0., 1.]]),\n                   jnp.matmul(cov, jnp.array([[jnp.cos(beta), 0., jnp.sin(beta)],\n                                              [0., 1., 0.],\n                                              [-jnp."}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the direction of the cone\n  d = d / jnp.linalg.norm(d)\n\n  # Calculate the angle of the cone\n  theta = jnp.arccos(d[2])\n\n  # Calculate the radius of the frustum at the starting and ending distances\n  r0 = base_radius * t0\n  r1 = base_radius * t1\n\n  # Calculate the direction of the frustum\n  dir = (r1 * d - r0 * d) / (t1 - t0)\n\n  # Calculate the mean of the Gaussian distribution\n  mean = r0 * d + (t1 - t0) * dir / 2\n\n  # Calculate the covariance matrix of the Gaussian distribution\n  if diag:\n    # Calculate the diagonal elements of the covariance matrix\n    cov_diag = jnp.array([base_radius ** 2 * t0 ** 2 * jnp.sin(theta) ** 2,\n                          base_radius ** 2 * t1 ** 2 * jnp.sin(theta) ** 2,\n                          (r1 ** 2 - r0 ** 2) / (2 * (t1 - t0))])\n\n    # Create the covariance matrix\n    cov = jnp.diag(cov_diag)\n  else:\n    # Calculate the full covariance matrix\n    cov = jnp.array([[base_radius ** 2 * t0 ** 2 * jnp.sin(theta) ** 2,\n                      base_radius ** 2 * t0 ** 2 * jnp.sin(theta) * jnp.cos(theta),\n                      base_radius ** 2 * t0 * jnp.sin(theta) ** 2 * dir[0] / 2],\n                     [base_radius ** 2 * t0 ** 2 * jnp.sin(theta) * jnp.cos(theta),\n                      base_radius ** 2 * t0 ** 2 * jnp.cos(theta) ** 2,\n                      base_radius ** 2 * t0 * jnp.sin(theta) ** 2 * dir[1] / "}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  d = jnp.array(d)\n\n  # Compute the direction of the conical frustum\n  d = d / jnp.linalg.norm(d)\n\n  # Compute the angle between the cone axis and the x-axis\n  theta = jnp.arccos(d[0])\n\n  # Compute the rotation matrix to align the cone axis with the x-axis\n  rot = jnp.array([[jnp.cos(theta), -jnp.sin(theta), 0],\n                   [jnp.sin(theta), jnp.cos(theta), 0],\n                   [0, 0, 1]])\n\n  # Rotate the cone axis and the starting and ending distances\n  d = jnp.matmul(rot, d)\n  t0 = jnp.matmul(rot, jnp.array([t0, 0, 0]))\n  t1 = jnp.matmul(rot, jnp.array([t1, 0, 0]))\n\n  # Compute the mean of the Gaussian distribution\n  mean = (t0 + t1) / 2\n\n  # Compute the covariance matrix\n  if diag:\n    cov = jnp.diag(jnp.array([(t1 - t0) ** 2, (t1 - t0) ** 2, (t1 - t0) ** 2]))\n  else:\n    cov = jnp.array([[(t1 - t0) ** 2, 0, 0],\n                     [0, (t1 - t0) ** 2, 0],\n                     [0, 0, (t1 - t0) ** 2]])\n\n  # Compute the scale of the radius as a function of distance\n  radius_scale = base_radius + jnp.array([0, 0, 0])\n\n  # Compute the radius at the starting and ending distances\n  r0 = radius_scale[0]\n  r1 = radius_scale[0]\n\n  # Compute the covariance matrix\n  if diag:\n    cov = jnp.diag(jnp"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mean = jnp.array([t0, t1, 0.0])\n  cov = jnp.array([[radius, 0.0, 0.0],\n                   [0.0, radius, 0.0],\n                   [0.0, 0.0, 0.0]])\n  return lift_gaussian(mean, cov, d, diag)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mean = jnp.array([t0, t1, 0.0])\n  if diag:\n    cov = jnp.array([radius, radius, 1.0])\n  else:\n    cov = jnp.array([[radius, 0.0, 0.0], [0.0, radius, 0.0], [0.0, 0.0, 1.0]])\n  return lift_gaussian(mean, cov, d)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mean = jnp.array([t0, t1, 0])\n  if diag:\n    cov = jnp.array([radius, radius, 0])\n  else:\n    cov = jnp.array([[radius, 0, 0], [0, radius, 0], [0, 0, 0]])\n  return lift_gaussian(mean, cov)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mean = jnp.array([t0, t1, 0.0])\n  var = jnp.array([radius ** 2, radius ** 2, 0.0])\n  cov = jnp.diag(var) if diag else jnp.outer(var, var)\n  return lift_gaussian(mean, cov, d)"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mean = jnp.array([t0, t0, t0])\n  var = jnp.array([t1 - t0, t1 - t0, t1 - t0])\n  return lift_gaussian(mean, var, d, diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mu = (t0 + t1) / 2.0\n  var = (t1 - t0) ** 2 / 12.0 + radius ** 2\n  return lift_gaussian(mu, var, d, diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  d = jnp.array(d)\n  d = d / jnp.linalg.norm(d)\n  mean = (t0 + t1) / 2 * d\n  variance = (t1 - t0) ** 2 / 12 * jnp.dot(d, d) + radius ** 2 / 4\n  return lift_gaussian(mean, variance, diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mean = jnp.array([t0, t1, 0.0])\n  if diag:\n    cov = jnp.array([radius, radius, 0.0])\n  else:\n    cov = jnp.array([[radius, 0.0, 0.0], [0.0, radius, 0.0], [0.0, 0.0, 0.0]])\n  return lift_gaussian(mean, cov, d)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mean = jnp.array([t0, t1, 0.0])\n  cov = jnp.array([[radius, 0.0, 0.0], [0.0, radius, 0.0], [0.0, 0.0, 0.0]])\n  return lift_gaussian(mean, cov, d, diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mean = jnp.array([t0, t1, 0.0])\n  cov = jnp.array([[radius, 0.0, 0.0], [0.0, radius, 0.0], [0.0, 0.0, 0.0]])\n  return lift_gaussian(mean, cov, d, diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mean = jnp.array([t0, t1, 0.0])\n  var = jnp.array([radius**2, radius**2, 0.0])\n  return lift_gaussian(mean, var, d, diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mean = jnp.array([t0, t1, 0.0])\n  var = jnp.array([radius**2, radius**2, 0.0])\n  return lift_gaussian(mean, var, d, diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # Calculate the mean of the Gaussian\n  mu = t0 * d\n\n  # Calculate the variance of the Gaussian\n  var = (t1 - t0) ** 2 * (d ** 2) + radius ** 2\n\n  # Call the lift_gaussian function to convert the cylinder to a Gaussian\n  return lift_gaussian(mu, var, diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mean = jnp.array([t0, t1, 0.0])\n  cov = jnp.array([[radius, 0.0, 0.0], [0.0, radius, 0.0], [0.0, 0.0, 0.0]])\n  return lift_gaussian(mean, cov, d, diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # Calculate the mean of the Gaussian distribution\n  mean = t0 * d + t1 * d\n\n  # Calculate the variance of the Gaussian distribution\n  var = (t1 - t0) ** 2 * jnp.dot(d, d) + 2 * (t1 - t0) * radius * jnp.abs(d) + radius ** 2\n\n  # Return the mean and covariance of the Gaussian distribution\n  return lift_gaussian(mean, var, diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mu = (t0 + t1) / 2\n  var = (t1 - t0)**2 / 12\n  return lift_gaussian(mu, var, radius, d, diag)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mean = jnp.array([t0, t1, 0.0])\n  cov = jnp.array([[radius, 0.0, 0.0], [0.0, radius, 0.0], [0.0, 0.0, 0.0]])\n  return lift_gaussian(mean, cov, d, diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # Calculate the mean and variance of the Gaussian\n  mean = (t0 + t1) / 2.0\n  variance = (t1 - t0) ** 2.0 / 12.0 + radius ** 2.0\n\n  # Call the lift_gaussian function to convert the cylinder to a Gaussian\n  return lift_gaussian(d, mean, variance, diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mu = (t0 + t1) / 2.0\n  sigma = (t1 - t0) / 2.0\n  return lift_gaussian(mu, sigma, d, radius, diag)\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  m = (t0 + t1) / 2.0\n  v = (t1 - t0) * (t1 - t0) / 12.0\n  return lift_gaussian(d, m, v, radius, diag)\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates\n  pix_x_cam = (pix_x_int + 0.5) * pixtocams[:, 0, 0] + pixtocams[:, 0, 2]\n  pix_y_cam = (pix_y_int + 0.5) * pixtocams[:, 1, 1] + pixtocams[:, 1, 2]\n  pix_z_cam = xnp.ones_like(pix_x_cam)\n  pix_cam = xnp.stack([pix_x_cam, pix_y_cam, pix_z_cam], axis=-1)\n\n  # Apply lens distortion correction, if necessary\n  if distortion_params is not None:\n    pix_cam = lens_distortion(pix_cam, distortion_params)\n\n  # Convert camera coordinates to world coordinates\n  pix_world = xnp.einsum(\"...ij,...j->...i\", camtoworlds[:, :3, :3], pix_cam) + camtoworlds[:, :3, 3]\n\n  # Compute ray origins and directions\n  origins = camtoworlds[:, :3, 3]\n  directions = pix_world - origins\n\n  # Normalize directions\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute differential radii\n  radii = xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute image plane coordinates\n  imageplane = pix_cam\n\n  # Convert to NDC space, if necessary\n  if pixtocam_ndc is not None:\n    origins = xnp.einsum(\"...ij,...j->...i\", pixtocam_ndc, origins)\n    directions = xnp.einsum(\"...ij,...j->...i\", pixtocam_ndc, directions)\n    viewdirs = xnp.einsum(\"...ij,..."}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates\n  if camtype == ProjectionType.PERSPECTIVE:\n    pix_x_cam = (pix_x_int - pixtocams[..., 0, 2]) / pixtocams[..., 0, 0]\n    pix_y_cam = (pix_y_int - pixtocams[..., 1, 2]) / pixtocams[..., 1, 1]\n  elif camtype == ProjectionType.FISHEYE:\n    pix_x_cam = (pix_x_int - pixtocams[..., 0, 2]) / pixtocams[..., 0, 0]\n    pix_y_cam = (pix_y_int - pixtocams[..., 1, 2]) / pixtocams[..., 1, 1]\n    pix_x_cam = pix_x_cam / xnp.sqrt(pix_x_cam ** 2 + pix_y_cam ** 2 + 1)\n    pix_y_cam = pix_y_cam / xnp.sqrt(pix_x_cam ** 2 + pix_y_cam ** 2 + 1)\n  elif camtype == ProjectionType.PANORAMIC:\n    pix_x_cam = (pix_x_int - pixtocams[..., 0, 2]) / pixtocams[..., 0, 0]\n    pix_y_cam = (pix_y_int - pixtocams[..., 1, 2]) / pixtocams[..., 1, 1]\n    pix_x_cam = pix_x_cam / xnp.sqrt(pix_x_cam ** 2 + pix_y_cam ** 2)\n    pix_y_cam = pix_y_cam / xnp.sqrt(pix_x_cam ** 2 + pix_y_cam ** 2)\n  else:\n    raise ValueError(\"Invalid camera projection type.\")\n\n  # Apply distortion correction, if applicable\n  if"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates\n  cam_x_int = (pix_x_int - pixtocams[..., 0, 2]) * pixtocams[..., 0, 0]\n  cam_y_int = (pix_y_int - pixtocams[..., 1, 2]) * pixtocams[..., 1, 1]\n\n  # Compute ray origins and directions\n  if camtype == ProjectionType.PERSPECTIVE:\n    # Perspective projection\n    origins = xnp.stack([cam_x_int, cam_y_int, xnp.ones_like(cam_x_int)], -1)\n    directions = xnp.stack([cam_x_int, cam_y_int, -xnp.ones_like(cam_x_int)], -1)\n  elif camtype == ProjectionType.FISHEYE:\n    # Fisheye projection\n    directions = xnp.stack([cam_x_int, cam_y_int, xnp.zeros_like(cam_x_int)], -1)\n    origins = xnp.zeros_like(directions)\n  elif camtype == ProjectionType.PANORAMIC:\n    # Panoramic projection\n    directions = xnp.stack([cam_x_int, cam_y_int, xnp.zeros_like(cam_x_int)], -1)\n    origins = xnp.zeros_like(directions)\n  else:\n    raise ValueError(f\"Unknown camera type: {camtype}\")\n\n  # Normalize directions\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Apply distortion correction\n  if distortion_params is not None:\n    directions = correct_distortion(directions, distortion_params)\n\n  # Transform directions to world coordinates\n  directions = xnp.einsum(\"...ij,...j->...i\", camtoworlds[..., :3, :"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  pix_x_cam = (pix_x_int + 0.5) * pixtocams[..., 0, 0] + pixtocams[..., 0, 2]\n  pix_y_cam = (pix_y_int + 0.5) * pixtocams[..., 1, 1] + pixtocams[..., 1, 2]\n\n  # Compute ray origins and directions.\n  origins = xnp.stack([pix_x_cam, pix_y_cam, xnp.ones_like(pix_x_cam)], axis=-1)\n  directions = xnp.stack(\n      [pix_x_cam, pix_y_cam, xnp.ones_like(pix_x_cam)], axis=-1\n  )\n  origins = origins[..., None, :]\n  directions = directions[..., None, :]\n\n  # Compute ray origins and directions in world coordinates.\n  origins = xnp.matmul(camtoworlds[..., :3, :3], origins) + camtoworlds[\n      ..., :3, 3:\n  ]\n  directions = xnp.matmul(camtoworlds[..., :3, :3], directions)\n\n  # Compute ray origins and directions in NDC space.\n  if pixtocam_ndc is not None:\n    origins = xnp.matmul(pixtocam_ndc, origins)\n    directions = xnp.matmul(pixtocam_ndc, directions)\n\n  # Compute ray origins and directions in camera coordinates.\n  origins = xnp.squeeze(origins, axis=-2)\n  directions = xnp.squeeze(directions, axis=-2)\n\n  # Compute ray origins and directions in world coordinates.\n  origins = xnp.matmul(camtoworlds[..., :3, :3].T, origins) + camtow"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Compute ray origins and directions in camera coordinates\n  pix_x = xnp.array(pix_x_int, dtype=xnp.float32)\n  pix_y = xnp.array(pix_y_int, dtype=xnp.float32)\n  camtoworlds = xnp.array(camtoworlds, dtype=xnp.float32)\n  pixtocams = xnp.array(pixtocams, dtype=xnp.float32)\n  origins = xnp.einsum(\"...ij,...j->...i\", camtoworlds, xnp.array([0, 0, 0, 1.0]))\n  directions = xnp.einsum(\"...ij,...j->...i\", pixtocams, xnp.array([pix_x, pix_y, 1.0]))\n\n  # Compute ray origins and directions in world coordinates\n  origins = xnp.einsum(\"...ij,...j->...i\", camtoworlds, xnp.array([0, 0, 0, 1.0]))\n  directions = xnp.einsum(\"...ij,...j->...i\", pixtocams, xnp.array([pix_x, pix_y, 1.0]))\n\n  # Compute ray origins and directions in NDC space\n  if pixtocam_ndc is not None:\n    origins = xnp.einsum(\"...ij,...j->...i\", pixtocam_ndc, xnp.array([0, 0, 0, 1.0]))\n    directions = xnp.einsum(\"...ij,...j->...i\", pixtocam_ndc, xnp.array([pix_x, pix_y, 1.0]))\n\n  # Compute ray origins and directions in NDC space\n  if pixtocam_ndc is not None:\n    origins = xnp.einsum(\"...ij,...j->...i\", pixtocam_ndc, xnp.array(["}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates\n  cam_x_int = (pix_x_int - pixtocams[..., 0, 2]) / pixtocams[..., 0, 0]\n  cam_y_int = (pix_y_int - pixtocams[..., 1, 2]) / pixtocams[..., 1, 1]\n\n  # Apply lens distortion correction\n  if distortion_params is not None:\n    cam_x_int, cam_y_int = distortion.apply_distortion(\n        cam_x_int,\n        cam_y_int,\n        distortion_params,\n        xnp=xnp,\n    )\n\n  # Compute ray origins and directions\n  origins = xnp.zeros(list(pix_x_int.shape) + [3], dtype=xnp.float32)\n  directions = xnp.zeros(list(pix_x_int.shape) + [3], dtype=xnp.float32)\n\n  if camtype == ProjectionType.PERSPECTIVE:\n    origins[..., 0] = cam_x_int\n    origins[..., 1] = cam_y_int\n    origins[..., 2] = 1.0\n    directions[..., 0] = cam_x_int\n    directions[..., 1] = cam_y_int\n    directions[..., 2] = -1.0\n  elif camtype == ProjectionType.FISHEYE:\n    origins[..., 0] = cam_x_int\n    origins[..., 1] = cam_y_int\n    origins[..., 2] = 1.0\n    directions[..., 0] = cam_x_int\n    directions[..., 1] = cam_y_int\n    directions[..., 2] = -1.0\n    directions = xnp.linalg.norm(directions, axis=-1, keepdims=True) * directions\n  elif cam"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  pix_x_cam = (pix_x_int - pixtocams[..., 0, 2]) / pixtocams[..., 0, 0]\n  pix_y_cam = (pix_y_int - pixtocams[..., 1, 2]) / pixtocams[..., 1, 1]\n  pix_z_cam = xnp.ones_like(pix_x_cam)\n\n  # Convert camera coordinates to world coordinates.\n  pix_x_world = pix_x_cam[..., None] * camtoworlds[..., 0, :3] + \\\n                pix_y_cam[..., None] * camtoworlds[..., 1, :3] + \\\n                pix_z_cam[..., None] * camtoworlds[..., 2, :3] + \\\n                camtoworlds[..., 3, :3]\n  pix_y_world = pix_x_cam[..., None] * camtoworlds[..., 0, 3:] + \\\n                pix_y_cam[..., None] * camtoworlds[..., 1, 3:] + \\\n                pix_z_cam[..., None] * camtoworlds[..., 2, 3:] + \\\n                camtoworlds[..., 3, 3:]\n\n  # Compute ray origins and directions.\n  origins = pix_x_world\n  directions = pix_y_world - origins\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute ray differential radii.\n  if camtype == ProjectionType.PERSPECTIVE:\n    # For perspective cameras, the differential radii are the same as the z-coordinates of the camera rays.\n    radii = pix_z_cam\n  elif camtype == ProjectionType.FISHEYE:\n    # For fisheye cameras, the differential radii are the same as the x"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates\n  pix_x = xnp.array(pix_x_int, dtype=xnp.float32)\n  pix_y = xnp.array(pix_y_int, dtype=xnp.float32)\n  pix_x = pix_x[..., None]\n  pix_y = pix_y[..., None]\n\n  if pixtocam_ndc is not None:\n    pix_x = pix_x / pixtocam_ndc[0, 0] * 2 - 1\n    pix_y = pix_y / pixtocam_ndc[1, 1] * 2 - 1\n\n  pix_z = xnp.ones_like(pix_x)\n  pix_x_cam = pix_x * pixtocams[..., 0, 0] + pix_y * pixtocams[..., 0, 1] + pixtocams[..., 0, 2]\n  pix_y_cam = pix_x * pixtocams[..., 1, 0] + pix_y * pixtocams[..., 1, 1] + pixtocams[..., 1, 2]\n  pix_z_cam = pix_x * pixtocams[..., 2, 0] + pix_y * pixtocams[..., 2, 1] + pixtocams[..., 2, 2]\n\n  # Convert camera coordinates to world coordinates\n  origins = xnp.stack([pix_x_cam, pix_y_cam, pix_z_cam], axis=-1)\n  origins = origins[..., None, :]\n  origins = xnp.matmul(origins, camtoworlds[..., :3, :3].transpose(..., 0, 2, 1))\n  origins = origins[..., 0, :]\n\n  # Calculate ray directions\n  directions = xnp.stack([pix_x_cam, pix_y_"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Compute ray origins\n  origins = xnp.zeros((pix_x_int.shape[0], 3), dtype=xnp.float32)\n  origins = origins.at[:, 0].set(xnp.ones_like(pix_x_int))\n  origins = origins.at[:, 1].set(pix_x_int.astype(xnp.float32))\n  origins = origins.at[:, 2].set(pix_y_int.astype(xnp.float32))\n  origins = xnp.einsum(\"...ij,...j->...i\", pixtocams, origins)\n\n  # Compute ray directions\n  directions = xnp.zeros((pix_x_int.shape[0], 3), dtype=xnp.float32)\n  directions = directions.at[:, 0].set(xnp.ones_like(pix_x_int))\n  directions = directions.at[:, 1].set(pix_x_int.astype(xnp.float32))\n  directions = directions.at[:, 2].set(pix_y_int.astype(xnp.float32))\n  directions = xnp.einsum(\"...ij,...j->...i\", pixtocams, directions)\n  directions = directions - origins\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute ray view directions\n  viewdirs = directions\n\n  # Compute ray differential radii\n  radii = xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute image plane coordinates\n  imageplane = xnp.stack([pix_x_int, pix_y_int], axis=-1)\n\n  # Convert ray origins and directions to NDC space\n  if pixtocam_ndc is not None:\n    origins = xnp.einsum(\"...ij"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates\n  pix_x = xnp.asarray(pix_x_int, dtype=xnp.float32)\n  pix_y = xnp.asarray(pix_y_int, dtype=xnp.float32)\n  pix_x = pix_x[..., None]\n  pix_y = pix_y[..., None]\n  pix_x_cam = xnp.matmul(pixtocams, xnp.concatenate([pix_x, pix_y, xnp.ones_like(pix_x)], axis=-1))\n  pix_x_cam = pix_x_cam[..., :2] / pix_x_cam[..., 2:]\n\n  # Correct for lens distortion if distortion_params are provided\n  if distortion_params is not None:\n    k1, k2, k3, p1, p2 = distortion_params[\"k1\"], distortion_params[\"k2\"], distortion_params[\"k3\"], distortion_params[\"p1\"], distortion_params[\"p2\"]\n    r2 = xnp.sum(pix_x_cam ** 2, axis=-1, keepdims=True)\n    r4 = r2 ** 2\n    r6 = r2 * r4\n    pix_x_cam = pix_x_cam * (1 + k1 * r2 + k2 * r4 + k3 * r6) + 2 * p1 * pix_x_cam * pix_y_cam + p2 * (r2 + 2 * pix_x_cam ** 2)\n\n  # Convert camera coordinates to world coordinates\n  pix_x_world = xnp.matmul(camtoworlds[:, :3, :3], pix_x_cam)\n  pix_y_world = xnp.matmul(camtoworlds[:, :3, :3], pix_y_cam)\n  pix_z_world = xnp.matmul(camtoworlds[:, :3, :3], xnp.ones_like(pix"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates\n  pix_x_cam, pix_y_cam, pix_z_cam = cam_utils.pixels_to_camera(\n    pix_x_int,\n    pix_y_int,\n    pixtocams,\n    distortion_params=distortion_params,\n    xnp=xnp,\n  )\n\n  # Convert camera coordinates to world coordinates\n  origins, directions = cam_utils.camera_to_world(\n    pix_x_cam,\n    pix_y_cam,\n    pix_z_cam,\n    camtoworlds,\n    xnp=xnp,\n  )\n\n  # Normalize direction vectors\n  viewdirs = xnp.linalg.norm(directions, axis=-1, keepdims=True)\n  directions = directions / viewdirs\n\n  # Compute differential radii\n  if camtype == ProjectionType.PERSPECTIVE:\n    radii = pix_z_cam / viewdirs\n  elif camtype == ProjectionType.FISHEYE:\n    radii = xnp.ones_like(viewdirs)\n  else:\n    raise ValueError(f\"Unknown camera type: {camtype}\")\n\n  # Compute image plane coordinates\n  if pixtocam_ndc is not None:\n    imageplane = cam_utils.camera_to_imageplane(\n      pix_x_cam,\n      pix_y_cam,\n      pixtocam_ndc,\n      xnp=xnp,\n    )\n  else:\n    imageplane = xnp.stack([pix_x_cam, pix_y_cam], axis=-1)\n\n  return origins, directions, viewdirs, radii, imageplane"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates\n  if camtype == ProjectionType.PERSPECTIVE:\n    # Compute normalized device coordinates\n    pix_x_ndc = xnp.array(pix_x_int, dtype=xnp.float32) * 2.0 / xnp.array(\n      pixtocams[..., 0, 0], dtype=xnp.float32\n    ) - 1.0\n    pix_y_ndc = xnp.array(pix_y_int, dtype=xnp.float32) * 2.0 / xnp.array(\n      pixtocams[..., 1, 1], dtype=xnp.float32\n    ) - 1.0\n    # Apply distortion correction, if provided\n    if distortion_params is not None:\n      pix_x_ndc, pix_y_ndc = apply_distortion(\n        pix_x_ndc, pix_y_ndc, distortion_params, xnp=xnp\n      )\n    # Convert to camera coordinates\n    pix_x_cam = pix_x_ndc / xnp.array(pixtocams[..., 0, 0], dtype=xnp.float32)\n    pix_y_cam = pix_y_ndc / xnp.array(pixtocams[..., 1, 1], dtype=xnp.float32)\n    pix_z_cam = xnp.ones_like(pix_x_cam)\n    pix_cam = xnp.stack([pix_x_cam, pix_y_cam, pix_z_cam], axis=-1)\n  elif camtype == ProjectionType.FISHEYE:\n    # Compute normalized device coordinates\n    pix_x_ndc = xnp.array(pix_x_int, dtype=xnp.float32) * 2.0 / xnp.array(\n      pixtocams[..., 0, 0], dtype=xnp.float32\n    ) - 1.0"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates\n  pix_x_cam = (pix_x_int + 0.5) * pixtocams[..., 0, 0] + pixtocams[..., 0, 1]\n  pix_y_cam = (pix_y_int + 0.5) * pixtocams[..., 1, 1] + pixtocams[..., 1, 2]\n\n  # Convert camera coordinates to world coordinates\n  origins = xnp.einsum(\"...ij,...j->...i\", camtoworlds[..., :3, :3], pix_x_cam) + camtoworlds[..., :3, 3]\n  directions = xnp.einsum(\"...ij,...j->...i\", camtoworlds[..., :3, :3], pix_x_cam) - origins\n\n  # Normalize the directions\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute differential radii\n  radii = xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute image plane coordinates\n  imageplane = xnp.stack([pix_x_cam, pix_y_cam], axis=-1)\n\n  # Apply distortion correction if distortion_params are provided\n  if distortion_params is not None:\n    imageplane = distort_imageplane(imageplane, distortion_params)\n\n  # Convert image plane coordinates to NDC space if pixtocam_ndc is provided\n  if pixtocam_ndc is not None:\n    imageplane = xnp.einsum(\"...ij,...j->...i\", pixtocam_ndc, imageplane)\n\n  # Compute ray origins and directions in NDC space based on the camera model\n  if camtype == ProjectionType.PERSPECTIVE:\n    origins = origins / origins[..., -1:]\n    directions = directions / directions[..."}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if distortion_params is not None:\n    if \"k1\" in distortion_params:\n      k1 = distortion_params[\"k1\"]\n    else:\n      k1 = 0.0\n    if \"k2\" in distortion_params:\n      k2 = distortion_params[\"k2\"]\n    else:\n      k2 = 0.0\n    if \"k3\" in distortion_params:\n      k3 = distortion_params[\"k3\"]\n    else:\n      k3 = 0.0\n    if \"p1\" in distortion_params:\n      p1 = distortion_params[\"p1\"]\n    else:\n      p1 = 0.0\n    if \"p2\" in distortion_params:\n      p2 = distortion_params[\"p2\"]\n    else:\n      p2 = 0.0\n\n  # Convert pixel coordinates to camera coordinates.\n  pix_x = xnp.cast(pix_x_int, xnp.float32)\n  pix_y = xnp.cast(pix_y_int, xnp.float32)\n  pix_x = pix_x / pixtocams[..., 0, 0]\n  pix_y = pix_y / pixtocams[..., 1, 1]\n  pix_x = pix_x * 2 - 1\n  pix_y = pix_y * 2 - 1\n  pix_x = pix_x * pixtocams[..., 0, 2]\n  pix_y = pix_y * pixtocams[..., 1, 2]\n\n  # Apply lens distortion.\n  if distortion_params is not None:\n    r2 = pix_x * pix_x + pix_y * pix_y\n    r4 = r2 * r2\n    r6 = r4 * r2\n    r8 = r4 * r4\n    rad_dist = (1 + k1 * r2 + k2 * r4 + k3 * r6)\n    pix_x = pix_x * rad_dist + p1 * r"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates\n  pix_x_cam = (pix_x_int + 0.5) * pixtocams[..., 0, 0] + pixtocams[..., 0, 2]\n  pix_y_cam = (pix_y_int + 0.5) * pixtocams[..., 1, 1] + pixtocams[..., 1, 2]\n\n  # Compute ray directions in camera space\n  dirs_cam = xnp.stack([pix_x_cam, pix_y_cam, xnp.ones_like(pix_x_cam)], axis=-1)\n\n  # Correct for lens distortion if distortion parameters are provided\n  if distortion_params is not None:\n    # Compute lens distortion parameters\n    k1 = distortion_params[\"k1\"]\n    k2 = distortion_params[\"k2\"]\n    k3 = distortion_params[\"k3\"]\n    p1 = distortion_params[\"p1\"]\n    p2 = distortion_params[\"p2\"]\n\n    # Compute lens distortion correction factors\n    r2 = xnp.sum(dirs_cam ** 2, axis=-1, keepdims=True)\n    r4 = r2 ** 2\n    r6 = r2 * r4\n    r8 = r4 * r2\n    r_coeff = (\n        (1 + k1 * r2 + k2 * r4 + k3 * r6)\n        + p1 * r2 * (2 * pix_x_cam + 2 * pix_y_cam)\n        + p2 * (r2 + 2 * pix_x_cam ** 2 + 2 * pix_y_cam ** 2)\n    )\n\n    # Apply lens distortion correction to ray directions\n    dirs_cam = dirs_cam * r_coeff\n\n  # Compute ray origins and directions in world space\n  origins = xnp.einsum(\"...ij,...j->...i\", camtoworlds[..., :3, :3"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  pix_x_cam = (pix_x_int + 0.5) * pixtocams[..., 0, 0] + pixtocams[..., 0, 2]\n  pix_y_cam = (pix_y_int + 0.5) * pixtocams[..., 1, 1] + pixtocams[..., 1, 2]\n\n  # Compute ray origins and directions.\n  origins = xnp.broadcast_to(camtoworlds[..., 3, :], pix_x_cam.shape + (3,))\n  directions = xnp.stack(\n      [\n          (pix_x_cam - pixtocams[..., 0, 2]) / pixtocams[..., 0, 0],\n          (pix_y_cam - pixtocams[..., 1, 2]) / pixtocams[..., 1, 1],\n          xnp.ones_like(pix_x_cam),\n      ],\n      axis=-1,\n  )\n\n  # Compute normalized view directions.\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute ray differential radii.\n  if camtype == ProjectionType.FISHEYE:\n    # For fisheye cameras, the differential radii are the same as the ray lengths.\n    radii = xnp.linalg.norm(directions, axis=-1, keepdims=True)\n  else:\n    # For perspective cameras, the differential radii are the same as the ray lengths.\n    radii = xnp.ones_like(directions[..., 0:1])\n\n  # Compute image plane coordinates.\n  imageplane = xnp.stack([pix_x_cam, pix_y_cam], axis=-1)\n\n  # Apply distortion correction to ray directions.\n  if distortion_params is not None:\n   "}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Compute ray origins and directions in camera space\n  if camtype == ProjectionType.PERSPECTIVE:\n    origins, directions = pixels_to_rays_perspective(\n      pix_x_int, pix_y_int, pixtocams, camtoworlds, xnp\n    )\n  elif camtype == ProjectionType.FISHEYE:\n    origins, directions = pixels_to_rays_fisheye(\n      pix_x_int, pix_y_int, pixtocams, camtoworlds, xnp\n    )\n  elif camtype == ProjectionType.EQUIRECTANGULAR:\n    origins, directions = pixels_to_rays_equirectangular(\n      pix_x_int, pix_y_int, pixtocams, camtoworlds, xnp\n    )\n  elif camtype == ProjectionType.PANORAMIC:\n    origins, directions = pixels_to_rays_panoramic(\n      pix_x_int, pix_y_int, pixtocams, camtoworlds, xnp\n    )\n  else:\n    raise ValueError(f\"Unsupported camera type: {camtype}\")\n\n  # Convert ray origins and directions to NDC space\n  if pixtocam_ndc is not None:\n    origins = xnp.einsum(\"...ij,...j\", origins, pixtocam_ndc)\n    directions = xnp.einsum(\"...ij,...j\", directions, pixtocam_ndc)\n\n  # Apply lens distortion correction to ray directions\n  if distortion_params is not None:\n    directions = lens_distortion(\n      directions,\n      distortion_params[\"k1\"],\n      distortion_params[\"k2\"],\n      distortion_params[\"p1\"],\n      distortion_params[\"p2\"],\n      xnp,\n    )\n\n  # Compute ray view directions\n  viewdirs = xnp.einsum(\"...i,...i->...i\", directions, directions)\n  viewdirs = xnp.sqrt(viewdirs)"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    # Compute ray origins and directions in camera space\n    origins = xnp.zeros((pix_x_int.shape[0], 3), dtype=xnp.float32)\n    origins = xnp.concatenate([origins, xnp.ones((pix_x_int.shape[0], 1), dtype=xnp.float32)], axis=-1)\n    origins = xnp.matmul(origins, xnp.linalg.inv(pixtocams).transpose([0, 2, 1]))\n    directions = xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], axis=-1)\n    directions = xnp.matmul(directions, xnp.linalg.inv(pixtocams).transpose([0, 2, 1]))\n    directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n    # Apply distortion correction if available\n    if distortion_params is not None:\n      directions = distort_rays(directions, distortion_params)\n\n    # Transform ray origins and directions to world space\n    origins = xnp.matmul(origins, camtoworlds.transpose([0, 2, 1]))\n    directions = xnp.matmul(directions, camtoworlds[:, :3, :3].transpose([0, 2, 1]))\n\n    # Compute view directions and differential radii\n    viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n    radii = xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n    # Compute image plane coordinates\n    imageplane = xnp.stack([pix_x_int, pix_y_int], axis=-1)"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # compute camera coordinates\n  pix_x_int = xnp.asarray(pix_x_int)\n  pix_y_int = xnp.asarray(pix_y_int)\n  pixtocams = xnp.asarray(pixtocams)\n  camtoworlds = xnp.asarray(camtoworlds)\n\n  if pixtocam_ndc is not None:\n    pixtocam_ndc = xnp.asarray(pixtocam_ndc)\n\n  # compute the image plane coordinates\n  imageplane = xnp.stack([pix_x_int, pix_y_int], axis=-1)\n\n  # compute the camera coordinates\n  camtoworlds_flat = xnp.reshape(camtoworlds, [-1, 3, 4])\n  pixtocams_flat = xnp.reshape(pixtocams, [-1, 3, 3])\n  imageplane_flat = xnp.reshape(imageplane, [-1, 2])\n  if camtype == ProjectionType.PERSPECTIVE:\n    # compute the camera coordinates\n    camtoworlds_flat = xnp.reshape(camtoworlds, [-1, 3, 4])\n    pixtocams_flat = xnp.reshape(pixtocams, [-1, 3, 3])\n    imageplane_flat = xnp.reshape(imageplane, [-1, 2])\n    camtoworlds_flat = xnp.reshape(camtoworlds, [-1, 3, 4])\n    pixtocams_flat = xnp.reshape(pixtocams, [-1, 3, 3])\n    imageplane_flat = xnp.reshape(imageplane, [-1, 2])\n    camtoworlds_flat = xnp.reshape(camtoworlds, [-1, 3, 4])\n    pixtocams_flat = xnp.reshape(pixtocams, [-1, 3, 3])\n    imageplane_flat = xnp.resh"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Extract the camera intrinsics and extrinsics\n  pixtocams = xnp.array(pixtocams)\n  camtoworlds = xnp.array(camtoworlds)\n\n  # Convert pixel coordinates to camera coordinates\n  pix_x_cam = (pix_x_int + 0.5) * pixtocams[:, 0, 0] + pixtocams[:, 0, 2]\n  pix_y_cam = (pix_y_int + 0.5) * pixtocams[:, 1, 1] + pixtocams[:, 1, 2]\n\n  # Compute ray origins and directions\n  origins = xnp.zeros((pix_x_int.shape[0], 3))\n  directions = xnp.zeros((pix_x_int.shape[0], 3))\n  for i in range(3):\n    origins[:, i] = camtoworlds[:, i, 3]\n    directions[:, i] = (\n        -camtoworlds[:, i, 0] * pix_x_cam\n        + camtoworlds[:, i, 1] * pix_y_cam\n        + camtoworlds[:, i, 2]\n    )\n\n  # Normalize the directions\n  directions = directions / xnp.linalg.norm(directions, axis=1, keepdims=True)\n\n  # Compute the normalized view directions\n  viewdirs = directions\n\n  # Compute the differential radii\n  radii = xnp.zeros((pix_x_int.shape[0], 1))\n\n  # Compute the image plane coordinates\n  imageplane = xnp.zeros((pix_x_int.shape[0], 2))\n\n  # Return the computed values\n  return origins, directions, viewdirs, radii, imageplane\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  tdist = tdist * density\n  return compute_alpha_weights_helper(tdist, dirs, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the norm-adjusted distances between points\n  norm_tdist = np.linalg.norm(dirs, axis=1) * tdist\n\n  # Compute the product of density and norm-adjusted distances\n  prod_density_norm_tdist = density * norm_tdist\n\n  # Compute the alpha weights using the helper function\n  alpha_weights = compute_alpha_weights_helper(prod_density_norm_tdist, **kwargs)\n\n  return alpha_weights"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Calculate the norm-adjusted distances between points\n  norm_adjusted_distances = tdist / np.linalg.norm(dirs, axis=1)\n\n  # Compute the alpha compositing weights\n  alpha_weights = compute_alpha_weights_helper(density, norm_adjusted_distances, **kwargs)\n\n  return alpha_weights"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Calculate the norm-adjusted distances between points\n  norm_adjusted_tdist = tdist / np.linalg.norm(dirs, axis=1)\n\n  # Compute the alpha weights based on the density and norm-adjusted distances\n  alpha_weights = compute_alpha_weights_helper(density, norm_adjusted_tdist, **kwargs)\n\n  return alpha_weights"}
{"namespace": "render.compute_alpha_weights", "completion": "  from . import compute_alpha_weights_helper\n\n  return compute_alpha_weights_helper(\n    density=density,\n    tdist=tdist,\n    dirs=dirs,\n    **kwargs,\n  )"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the product of density and tdist\n  density_tdist = density * tdist\n\n  # Call the helper function to compute the alpha weights\n  return compute_alpha_weights_helper(density_tdist, dirs, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the adjusted distance between points\n  tdist_adjusted = tdist / np.linalg.norm(dirs, axis=1)\n\n  # Compute the alpha weights using the helper function\n  alpha_weights = compute_alpha_weights_helper(density, tdist_adjusted, **kwargs)\n\n  return alpha_weights"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Calculate the norm-adjusted distances between points\n  norm_dist = np.linalg.norm(dirs, axis=-1)\n\n  # Adjust the density values based on the norm-adjusted distances\n  adjusted_density = density * norm_dist\n\n  # Compute the alpha weights using the adjusted density values\n  alpha_weights = compute_alpha_weights_helper(adjusted_density, tdist, **kwargs)\n\n  return alpha_weights"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the product of density and norm-adjusted distances\n  tdist_adjusted = tdist / np.linalg.norm(dirs, axis=1)\n  density_tdist_product = density * tdist_adjusted\n\n  # Compute the alpha weights using the helper function\n  alpha_weights = compute_alpha_weights_helper(density_tdist_product, **kwargs)\n\n  return alpha_weights"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Calculate the norm-adjusted distances between points\n  tdist_norm = tdist / np.linalg.norm(dirs, axis=1)\n\n  # Compute the alpha compositing weights based on the density and norm-adjusted distances\n  return compute_alpha_weights_helper(density, tdist_norm, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the norm-adjusted distances between points\n  tdist = np.linalg.norm(dirs, axis=1) * tdist\n\n  # Compute the alpha weights using the helper function\n  return compute_alpha_weights_helper(density * tdist, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the norm-adjusted distances between points along a path or direction\n  norm_tdist = tdist / np.linalg.norm(dirs, axis=1)\n\n  # Compute the adjusted density values\n  adjusted_density = density * norm_tdist\n\n  # Compute the alpha weights using the adjusted density values\n  alpha_weights = compute_alpha_weights_helper(adjusted_density, **kwargs)\n\n  return alpha_weights"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Calculate the norm-adjusted distances between points\n  norm_dist = np.linalg.norm(dirs, axis=1)\n  # Adjust the density values based on the norm-adjusted distances\n  adjusted_density = density / norm_dist\n\n  # Compute the alpha weights using the helper function\n  return compute_alpha_weights_helper(adjusted_density, tdist, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Calculate the norm-adjusted distances between points\n  tdist = tdist / np.linalg.norm(dirs, axis=1)\n\n  # Compute the alpha weights based on the density and norm-adjusted distances\n  alpha_weights = compute_alpha_weights_helper(density, tdist, **kwargs)\n\n  return alpha_weights\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Calculate the norm-adjusted distances between points\n  norm_adjusted_tdist = tdist / np.linalg.norm(dirs, axis=-1)\n\n  # Compute the alpha compositing weights based on the adjusted distances\n  alpha_weights = compute_alpha_weights_helper(density * norm_adjusted_tdist, **kwargs)\n\n  return alpha_weights"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Calculate the adjusted distance between points\n  tdist_norm = tdist / np.linalg.norm(dirs, axis=1)\n\n  # Compute the alpha weights\n  alpha_weights = compute_alpha_weights_helper(density, tdist_norm, **kwargs)\n\n  return alpha_weights\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Calculate the norm-adjusted distances between points\n  tdist_norm = np.linalg.norm(dirs, axis=1)\n\n  # Calculate the adjusted distances between points\n  tdist_adj = tdist / tdist_norm\n\n  # Calculate the product of density and the adjusted distances\n  density_tdist_adj = density * tdist_adj\n\n  # Compute the alpha weights using the helper function\n  alpha_weights = compute_alpha_weights_helper(density_tdist_adj, **kwargs)\n\n  return alpha_weights"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Calculate the norm-adjusted distances between points\n  norm_tdist = np.linalg.norm(dirs, axis=1) * tdist\n\n  # Compute the alpha weights based on the density and norm-adjusted distances\n  alpha_weights = compute_alpha_weights_helper(density, norm_tdist, **kwargs)\n\n  return alpha_weights\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Calculate the norm-adjusted distances between points\n  norm_tdist = tdist * np.linalg.norm(dirs, axis=1)\n\n  # Compute the alpha weights\n  return compute_alpha_weights_helper(density, norm_tdist, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Calculate the norm-adjusted distances between points\n  tdist = norm_adjusted_distances(tdist, dirs)\n\n  # Compute the alpha weights\n  return compute_alpha_weights_helper(density * tdist, **kwargs)\n\n"}
{"namespace": "stepfun.sample", "completion": "  # Check input shapes\n  assert t.ndim == 1\n  assert w_logits.ndim == 1\n  assert t.shape[0] == w_logits.shape[0] + 1\n\n  # Define the bin endpoints and weights\n  t = t.astype(jnp.float32)\n  w_logits = w_logits.astype(jnp.float32)\n\n  # Normalize the weights\n  w_logits = w_logits - jnp.max(w_logits)\n  w_logits = jnp.exp(w_logits)\n  w_logits = w_logits / jnp.sum(w_logits)\n\n  # Define the bin endpoints and weights\n  t = t.astype(jnp.float32)\n  w_logits = w_logits.astype(jnp.float32)\n\n  # Define the bin endpoints and weights\n  t = t.astype(jnp.float32)\n  w_logits = w_logits.astype(jnp.float32)\n\n  # Define the bin endpoints and weights\n  t = t.astype(jnp.float32)\n  w_logits = w_logits.astype(jnp.float32)\n\n  # Define the bin endpoints and weights\n  t = t.astype(jnp.float32)\n  w_logits = w_logits.astype(jnp.float32)\n\n  # Define the bin endpoints and weights\n  t = t.astype(jnp.float32)\n  w_logits = w_logits.astype(jnp.float32)\n\n  # Define the bin endpoints and weights\n  t = t.astype(jnp.float32)\n  w_logits = w_logits.astype(jnp.float32)\n\n  # Define the bin endpoints and weights\n  t = t.astype(jnp.float32)\n  w_logits = w_logits."}
{"namespace": "stepfun.sample", "completion": "  # Compute the cumulative distribution function (CDF) from the weights\n  w_logits = jnp.asarray(w_logits)\n  cdf = jax.nn.softmax(w_logits, axis=-1)\n\n  # Compute the bin widths\n  bin_widths = t[1:] - t[:-1]\n\n  # Compute the bin cumulative widths\n  bin_cum_widths = jnp.cumsum(bin_widths)\n\n  # Compute the bin cumulative probabilities\n  bin_cum_probs = cdf * bin_widths\n  bin_cum_probs = jnp.concatenate([jnp.array([0.0]), bin_cum_probs], axis=0)\n\n  # Compute the bin cumulative probabilities\n  bin_cum_probs = jnp.cumsum(bin_cum_probs)\n\n  # Compute the bin cumulative probabilities\n  bin_cum_probs = jnp.concatenate([jnp.array([0.0]), bin_cum_probs], axis=0)\n\n  # Compute the bin cumulative probabilities\n  bin_cum_probs = jnp.cumsum(bin_cum_probs)\n\n  # Compute the bin cumulative probabilities\n  bin_cum_probs = jnp.concatenate([jnp.array([0.0]), bin_cum_probs], axis=0)\n\n  # Compute the bin cumulative probabilities\n  bin_cum_probs = jnp.cumsum(bin_cum_probs)\n\n  # Compute the bin cumulative probabilities\n  bin_cum_probs = jnp.concatenate([jnp.array([0.0]), bin_cum_probs], axis=0)\n\n  # Compute the bin cumulative probabilities\n  bin_cum_probs = jnp.cumsum(bin_cum_probs)\n\n  # Compute the bin cumulative probabilities\n  bin_cum_pro"}
{"namespace": "stepfun.sample", "completion": "  # Check input shapes\n  if t.ndim != 1:\n    raise ValueError(f\"t must be a 1D array, got shape {t.shape}\")\n  if w_logits.ndim != 1:\n    raise ValueError(f\"w_logits must be a 1D array, got shape {w_logits.shape}\")\n  if t.shape[0] != w_logits.shape[0] + 1:\n    raise ValueError(\n        f\"t and w_logits must have the same length + 1, got {t.shape[0]} and {w_logits.shape[0]}\"\n    )\n\n  # Check if t is sorted\n  if not jnp.all(jnp.diff(t) >= 0):\n    raise ValueError(\"t must be sorted\")\n\n  # Check if w_logits are non-negative\n  if jnp.any(w_logits < 0):\n    raise ValueError(\"w_logits must be non-negative\")\n\n  # Check if num_samples is positive\n  if num_samples <= 0:\n    raise ValueError(\"num_samples must be positive\")\n\n  # Check if single_jitter is a bool\n  if not isinstance(single_jitter, bool):\n    raise ValueError(\"single_jitter must be a bool\")\n\n  # Check if deterministic_center is a bool\n  if not isinstance(deterministic_center, bool):\n    raise ValueError(\"deterministic_center must be a bool\")\n\n  # Check if eps is a float\n  if not isinstance(eps, float):\n    raise ValueError(\"eps must be a float\")\n\n  # Check if eps is positive\n  if eps <= 0:\n    raise ValueError(\"eps must be positive\")\n\n  # Check if rng is None or a PRNGKey\n  if rng is not None and not isinstance(rng, jax.random.PRNGKey):\n    raise ValueError(\"rng must be None or a PRNGKey\")\n\n  # Compute the cumulative sum of w_logits"}
{"namespace": "stepfun.sample", "completion": "  # Convert the logits to weights.\n  w = jax.nn.softmax(w_logits, axis=-1)\n\n  # Compute the cumulative sum of weights.\n  cdf = jnp.cumsum(w, axis=-1)\n\n  # Compute the bin widths.\n  bin_widths = jnp.diff(t, axis=-1)\n\n  # Compute the bin centers.\n  bin_centers = t[..., :-1] + bin_widths / 2\n\n  # Compute the bin endpoints.\n  bin_endpoints = t[..., 1:]\n\n  # Compute the bin widths.\n  bin_widths = bin_endpoints - bin_centers\n\n  # Compute the bin centers.\n  bin_centers = bin_centers + bin_widths / 2\n\n  # Compute the bin endpoints.\n  bin_endpoints = t[..., 1:]\n\n  # Compute the bin widths.\n  bin_widths = bin_endpoints - bin_centers\n\n  # Compute the bin centers.\n  bin_centers = bin_centers + bin_widths / 2\n\n  # Compute the bin endpoints.\n  bin_endpoints = t[..., 1:]\n\n  # Compute the bin widths.\n  bin_widths = bin_endpoints - bin_centers\n\n  # Compute the bin centers.\n  bin_centers = bin_centers + bin_widths / 2\n\n  # Compute the bin endpoints.\n  bin_endpoints = t[..., 1:]\n\n  # Compute the bin widths.\n  bin_widths = bin_endpoints - bin_centers\n\n  # Compute the bin centers.\n  bin_centers = bin_centers + bin_widths / 2\n\n  # Compute the bin endpoints.\n  bin_endpoints = t[..., 1:]\n\n  # Compute the bin widths.\n  bin_widths = bin_endpoints -"}
{"namespace": "stepfun.sample", "completion": "  if rng is None:\n    # Deterministic sampling\n    if deterministic_center:\n      # Center sampling\n      t_mid = 0.5 * (t[1:] + t[:-1])\n      delta = t[1] - t[0]\n      u = jnp.linspace(eps, 1.0 - eps, num_samples)\n      u = u.reshape([1, -1])\n      u = u.repeat(t_mid.shape[0], axis=0)\n      t_samples = jnp.sum(u * delta, axis=1) + t_mid\n    else:\n      # Uniform sampling\n      u = jnp.linspace(eps, 1.0 - eps, num_samples)\n      u = u.reshape([1, -1])\n      u = u.repeat(t.shape[0] - 1, axis=0)\n      t_samples = u * (t[1:] - t[:-1]) + t[:-1]\n  else:\n    # Random sampling\n    if deterministic_center:\n      # Center sampling\n      t_mid = 0.5 * (t[1:] + t[:-1])\n      delta = t[1] - t[0]\n      if single_jitter:\n        u = jax.random.uniform(rng, shape=[1, t_mid.shape[0]])\n      else:\n        u = jax.random.uniform(rng, shape=[num_samples, t_mid.shape[0]])\n      t_samples = jnp.sum(u * delta, axis=1) + t_mid\n    else:\n      # Uniform sampling\n      if single_jitter:\n        u = jax.random.uniform(rng, shape=[1, t.shape[0] - 1])\n      else:\n        u = jax.random.uniform(rng, shape=[num_samples, t.shape[0] - 1])\n      t_samples = u * (t[1:] - t[:-1]) + t[:"}
{"namespace": "stepfun.sample", "completion": "  batch_size = t.shape[0]\n  if deterministic_center:\n    t = t[:, 1:] - (t[:, 1:] - t[:, :-1]) / 2\n  if rng is None:\n    u = jnp.linspace(eps, 1 - eps, num_samples, endpoint=False)\n    u = jnp.tile(u, (batch_size, 1))\n    if single_jitter:\n      u = u + jax.random.uniform(rng, u.shape, minval=0, maxval=1 / num_samples)\n    else:\n      u = u + jax.random.uniform(rng, u.shape, minval=0, maxval=1 / num_samples)\n  else:\n    u = jax.random.uniform(rng, (batch_size, num_samples), minval=eps, maxval=1 - eps)\n    if single_jitter:\n      u = u + jax.random.uniform(rng, u.shape, minval=0, maxval=1 / num_samples)\n  return jnp.sum(jnp.exp(w_logits) * (u > t), axis=-1)\n\n"}
{"namespace": "stepfun.sample", "completion": "  batch_size = t.shape[0]\n\n  # Step 1: Compute the cumulative weights\n  w_cum = jax.nn.softmax(w_logits, axis=-1)\n  w_cum = jnp.concatenate([jnp.zeros_like(w_cum[..., :1]), w_cum], axis=-1)\n\n  # Step 2: Compute the PDF widths\n  widths = t[..., 1:] - t[..., :-1]\n  widths = jnp.concatenate([widths, jnp.zeros_like(widths[..., :1])], axis=-1)\n\n  # Step 3: Compute the PDF heights\n  heights = w_cum[..., 1:] - w_cum[..., :-1]\n\n  # Step 4: Compute the PDF areas\n  areas = heights * widths\n\n  # Step 5: Compute the total area of the PDF\n  total_area = jnp.sum(areas, axis=-1)\n\n  # Step 6: Compute the cumulative areas\n  areas_cum = jnp.cumsum(areas, axis=-1)\n\n  # Step 7: Compute the PDF probabilities\n  probs = areas / total_area[..., None]\n\n  # Step 8: Compute the cumulative probabilities\n  probs_cum = jnp.cumsum(probs, axis=-1)\n\n  # Step 9: Compute the bin indices\n  bin_indices = jnp.sum(probs_cum < jnp.expand_dims(rng, axis=-1), axis=-1)\n\n  # Step 10: Compute the bin endpoints\n  bin_endpoints = jnp.concatenate([t[..., :1], t[..., 1:]], axis=-1)\n\n  # Step 11: Compute the bin widths\n  bin_widths = bin_endpoints[..., 1:] - bin_end"}
{"namespace": "stepfun.sample", "completion": "  # Check input dimensions\n  assert t.ndim == 1, \"t must be a 1D array\"\n  assert w_logits.ndim == 1, \"w_logits must be a 1D array\"\n  assert t.shape[0] == w_logits.shape[0] + 1, \"t and w_logits must have the same length\"\n\n  # Check input types\n  assert isinstance(num_samples, int), \"num_samples must be an integer\"\n  assert isinstance(single_jitter, bool), \"single_jitter must be a boolean\"\n  assert isinstance(deterministic_center, bool), \"deterministic_center must be a boolean\"\n\n  # Check input values\n  assert num_samples > 0, \"num_samples must be positive\"\n  assert jnp.all(jnp.diff(t) >= 0), \"t must be sorted\"\n\n  # Compute the bin weights\n  w = jax.nn.softmax(w_logits, axis=-1)\n\n  # Compute the cumulative sum of the bin weights\n  cdf = jnp.cumsum(w)\n\n  # Normalize the cumulative sum to the range [0, 1]\n  cdf = cdf / cdf[-1]\n\n  # Compute the bin widths\n  bin_widths = jnp.diff(t)\n\n  # Compute the bin centers\n  bin_centers = t[:-1] + bin_widths / 2\n\n  # Compute the bin start and end points\n  bin_starts = t[:-1]\n  bin_ends = t[1:]\n\n  # Compute the bin probabilities\n  bin_probs = w * bin_widths\n\n  # Compute the cumulative bin probabilities\n  cum_bin_probs = jnp.cumsum(bin_probs)\n\n  # Compute the normalized cumulative bin probabilities\n  norm_cum_bin_probs = cum_bin_probs / cum_bin_probs[-1]\n\n  # Compute the bin probabilities\n  bin"}
{"namespace": "stepfun.sample", "completion": "  # Define the step function as a piecewise constant PDF\n  step_function = jnp.cumsum(jax.nn.softmax(w_logits, axis=-1))\n\n  # Define the bin widths\n  bin_widths = t[1:] - t[:-1]\n\n  # Define the bin centers\n  bin_centers = t[:-1] + bin_widths / 2.0\n\n  # Define the bin endpoints\n  bin_endpoints = jnp.concatenate([t[0:1], t[1:]], axis=0)\n\n  # Define the bin endpoints and weights\n  bin_endpoints = jnp.concatenate([bin_endpoints, t[-1:]], axis=0)\n  bin_weights = jnp.concatenate([jnp.zeros_like(bin_endpoints[0:1]), step_function], axis=0)\n\n  # Define the bin endpoints and weights\n  bin_endpoints = jnp.concatenate([bin_endpoints, t[-1:]], axis=0)\n  bin_weights = jnp.concatenate([jnp.zeros_like(bin_endpoints[0:1]), step_function], axis=0)\n\n  # Define the bin endpoints and weights\n  bin_endpoints = jnp.concatenate([bin_endpoints, t[-1:]], axis=0)\n  bin_weights = jnp.concatenate([jnp.zeros_like(bin_endpoints[0:1]), step_function], axis=0)\n\n  # Define the bin endpoints and weights\n  bin_endpoints = jnp.concatenate([bin_endpoints, t[-1:]], axis=0)\n  bin_weights = jnp.concatenate([jnp.zeros_like(bin_endpoints[0:1]), step_function], axis=0)\n\n  # Define the bin endpoints and weights\n  bin_endpoints = jnp.concatenate([bin_endpoints, t[-1:]], axis=0)\n  bin_weights"}
{"namespace": "stepfun.sample", "completion": "  batch_size = t.shape[0]\n  t = t.reshape([batch_size, -1])\n  w_logits = w_logits.reshape([batch_size, -1])\n\n  if rng is None:\n    # Deterministic sampling.\n    if deterministic_center:\n      # Sample from the center of each bin.\n      t_mid = (t[:, :-1] + t[:, 1:]) / 2\n      u = jnp.linspace(0., 1., num_samples)\n      u = u.reshape([batch_size, -1])\n      u = u + (1. / num_samples / 2)\n    else:\n      # Sample uniformly from each bin.\n      u = jnp.linspace(0., 1., num_samples)\n      u = u.reshape([batch_size, -1])\n      u = u + eps\n    t = t_mid + (u - 0.5) * (t[:, 1:] - t[:, :-1])\n    t = t.reshape([batch_size, num_samples])\n  else:\n    # Random sampling.\n    if single_jitter:\n      # Sample uniformly from each bin.\n      u = jax.random.uniform(rng, shape=[batch_size, num_samples])\n      u = u + eps\n    else:\n      # Sample uniformly from each bin.\n      u = jax.random.uniform(rng, shape=[batch_size, num_samples])\n      u = u + eps\n    t = t[:, :-1] + u * (t[:, 1:] - t[:, :-1])\n\n  return t"}
{"namespace": "stepfun.sample", "completion": "  if rng is None:\n    if deterministic_center:\n      t = (t[1:] + t[:-1]) / 2\n    return jnp.linspace(t[0], t[-1], num_samples, endpoint=True)\n\n  w = jax.nn.softmax(w_logits, axis=-1)\n  w = jnp.concatenate([jnp.zeros((1,)), w], axis=0)\n  w_cum = jnp.cumsum(w, axis=0)\n\n  # sample the bin index\n  u = rng.uniform(w_cum.shape)\n  indices = jnp.sum(u[..., None] > w_cum, axis=-1) - 1\n\n  # sample the offset\n  m = jnp.diff(t)\n  if single_jitter:\n    offset = rng.uniform(m.shape)\n  else:\n    offset = rng.uniform(m.shape) * m\n\n  samples = t[indices] + offset\n\n  return samples"}
{"namespace": "stepfun.sample", "completion": "  # Compute the bin widths from the endpoints\n  width = t[1:] - t[:-1]\n\n  # Compute the cumulative sum of the bin widths\n  cum_width = jnp.concatenate([jnp.array([0.0]), jnp.cumsum(width)])\n\n  # Compute the cumulative sum of the bin weights\n  cum_weights = jax.nn.softmax(w_logits)\n  cum_weights = jnp.concatenate([jnp.array([0.0]), jnp.cumsum(cum_weights)])\n\n  # Compute the bin indices based on the cumulative weights\n  bin_indices = jnp.digitize(cum_weights, cum_width)\n\n  # Compute the bin start and end indices for each bin\n  bin_starts = jnp.concatenate([jnp.array([0]), bin_indices[:-1]])\n  bin_ends = bin_indices\n\n  # Compute the bin widths and weights for each bin\n  bin_widths = t[bin_ends] - t[bin_starts]\n  bin_weights = cum_weights[bin_ends] - cum_weights[bin_starts]\n\n  # Compute the bin probabilities based on the bin weights\n  bin_probs = bin_weights / jnp.sum(bin_weights)\n\n  # Compute the cumulative probabilities for each bin\n  cum_probs = jnp.concatenate([jnp.array([0.0]), jnp.cumsum(bin_probs)])\n\n  # Compute the bin indices for each sample\n  bin_indices = jnp.digitize(cum_probs, jnp.linspace(eps, 1 - eps, num_samples))\n\n  # Compute the bin start and end indices for each sample\n  bin_starts = jnp.concatenate([jnp.array([0]), bin_indices[:-1]])\n  bin_ends = bin_indices"}
{"namespace": "stepfun.sample", "completion": "  if rng is None:\n    rng = jax.random.PRNGKey(0)\n\n  batch_size = w_logits.shape[0]\n\n  # Compute the cumulative sum of weights\n  w_cumsum = jax.nn.softmax(w_logits, axis=-1)\n  w_cumsum = jnp.cumsum(w_cumsum, axis=-1)\n\n  # Determine the range of uniform sampling\n  if deterministic_center:\n    a = jnp.maximum(0.0, t[..., 1:] - t[..., :-1])\n    b = jnp.minimum(1.0, t[..., 1:] - t[..., :-1])\n  else:\n    a = 0.0\n    b = 1.0\n\n  # Generate uniform samples\n  u = jax.random.uniform(rng, shape=(batch_size, num_samples))\n  u = a + (b - a) * u\n\n  # Determine the jitter amount\n  if single_jitter:\n    jitter = jax.random.uniform(rng, shape=(batch_size, num_samples)) * eps\n  else:\n    jitter = jax.random.uniform(rng, shape=(batch_size, 1)) * eps\n\n  # Add the jitter to the uniform samples\n  u = u + jitter\n\n  # Find the indices of the bins corresponding to each sample\n  indices = jnp.sum(u[..., None] > w_cumsum, axis=-1)\n\n  # Compute the sampled values\n  return t[..., indices]"}
{"namespace": "stepfun.sample", "completion": "  if w_logits.ndim == 1:\n    w_logits = w_logits[None, :]\n\n  if deterministic_center:\n    if t.ndim == 1:\n      t = t[None, :]\n    if rng is None:\n      t = t[:, 1:] - (t[:, 1] - t[:, 0]) / 2.0\n    else:\n      t = t + jax.random.uniform(rng, t.shape, minval=-eps, maxval=eps)\n\n  if rng is None:\n    if single_jitter:\n      u = jnp.linspace(eps, 1 - eps, num_samples)\n    else:\n      u = jnp.linspace(eps, 1 - eps, num_samples)[None, :]\n    u = u.repeat(w_logits.shape[0], axis=0)\n    cdf = jax.nn.softmax(w_logits, axis=-1)\n    cdf = jnp.concatenate([jnp.zeros_like(cdf[..., :1]), cdf], axis=-1)\n    cdf = jnp.cumsum(cdf, axis=-1)\n    indices = jnp.sum((cdf[..., 1:] < u)[..., None], axis=-2)\n    indices = jnp.clip(indices, a_min=0, a_max=cdf.shape[-1] - 1)\n    indices = jnp.expand_dims(indices, axis=-1)\n    bin_width = jnp.expand_dims(t[..., 1] - t[..., 0], axis=-1)\n    return t[..., indices] + bin_width[..., indices] * (u - cdf[..., indices]) / (\n        cdf[..., indices + 1] - cdf[..., indices]\n    )\n  else:\n    if single_jitter:\n      u = jax.random.uniform(rng, ("}
{"namespace": "stepfun.sample", "completion": "  # Convert logits to probabilities\n  w = jax.nn.softmax(w_logits, axis=-1)\n\n  # Determine the number of bins\n  num_bins = t.shape[0] - 1\n\n  # Determine the width of each bin\n  bin_width = t[1:] - t[:-1]\n\n  # Calculate the cumulative probability distribution\n  cdf = jnp.cumsum(w, axis=-1)\n\n  # Determine the bin indices for each sample\n  if deterministic_center:\n    bin_indices = jnp.arange(num_bins)\n  else:\n    bin_indices = jnp.arange(num_bins)\n\n  # Generate random samples\n  if rng is None:\n    u = jnp.linspace(eps, 1.0 - eps, num_samples)\n  else:\n    u = jax.random.uniform(rng, shape=(num_samples,))\n\n  # Add jitter to the samples\n  if single_jitter:\n    jitter = jax.random.uniform(rng, shape=(num_samples,))\n    u = u + jitter\n  else:\n    jitter = jax.random.uniform(rng, shape=(num_samples, num_bins))\n    u = u + jitter\n\n  # Clip the samples to the range [0, 1]\n  u = jnp.clip(u, a_min=eps, a_max=1.0 - eps)\n\n  # Find the bin indices for each sample\n  bin_indices = jnp.sum(u[..., None] > cdf[..., :-1], axis=-1)\n\n  # Calculate the offsets for each sample\n  offsets = (u - cdf[..., bin_indices]) / (cdf[..., bin_indices + 1] - cdf[..., bin_indices])\n\n  # Calculate the final samples\n  samples = t[bin_indices] + bin_width"}
{"namespace": "stepfun.sample", "completion": "  batch_size = t.shape[0]\n\n  # Define the PDF as a step function defined by the bin endpoints and weights.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  pdf = jnp.concatenate([w[:, :-1], w[:, -1:]], axis=-1)\n\n  # Compute the cumulative sum of the PDF to get the cumulative distribution function (CDF).\n  cdf = jnp.cumsum(pdf, axis=-1)\n\n  # Normalize the CDF to ensure it ranges from 0 to 1.\n  cdf = cdf / cdf[..., -1:]\n\n  # Compute the bin widths.\n  bin_widths = t[..., 1:] - t[..., :-1]\n\n  # Compute the bin centers.\n  bin_centers = t[..., 1:] - bin_widths / 2\n\n  # Compute the bin endpoints.\n  bin_endpoints = jnp.concatenate([t[..., :1], bin_centers + bin_widths], axis=-1)\n\n  # Compute the bin midpoints.\n  bin_midpoints = bin_centers + bin_widths / 2\n\n  # Compute the bin startpoints.\n  bin_startpoints = jnp.concatenate([t[..., :1] - bin_widths / 2, bin_centers], axis=-1)\n\n  # Compute the bin endpoints and weights for each bin.\n  bin_endpoints_and_weights = jnp.stack([bin_endpoints, pdf], axis=-1)\n\n  # Compute the bin startpoints and weights for each bin.\n  bin_startpoints_and_weights = jnp.stack([bin_startpoints, pdf], axis=-1)\n\n  # Compute the bin midpoints and weights for each bin.\n  bin_midpoints_and_weights = jnp.stack([bin_midpoints, pdf], axis=-1)\n\n  # Compute the"}
{"namespace": "stepfun.sample", "completion": "  assert t.ndim == 1, \"t must be a 1D array\"\n  assert w_logits.ndim == 1, \"w_logits must be a 1D array\"\n\n  # Determine if we are using deterministic or random sampling\n  if rng is None:\n    # Deterministic sampling\n    # Generate samples evenly spaced between the bin endpoints\n    if deterministic_center:\n      # Center the samples around the bin endpoints\n      t_min = jnp.min(t)\n      t_max = jnp.max(t)\n      t_center = (t_min + t_max) / 2.0\n      t = jnp.linspace(t_center - 0.5, t_center + 0.5, num_samples)\n    else:\n      # Span the entire PDF\n      t = jnp.linspace(t[0], t[-1], num_samples)\n  else:\n    # Random sampling\n    if single_jitter:\n      # Jitter each sample by the same amount\n      t = jnp.linspace(t[0], t[-1], num_samples)\n      t += jax.random.uniform(rng, shape=[num_samples], minval=-eps, maxval=eps)\n    else:\n      # Jitter each sample independently\n      t = jax.random.uniform(\n          rng, shape=[num_samples], minval=t[0] + eps, maxval=t[-1] - eps\n      )\n\n  # Compute the cumulative weights\n  w_cumsum = jnp.cumsum(jax.nn.softmax(w_logits))\n\n  # Compute the bin indices for each sample\n  bin_indices = jnp.searchsorted(t, t, side=\"right\") - 1\n  bin_indices = jnp.clip(bin_indices, 0, t.shape[0] - 1)\n\n  # Compute the weights for each sample\n  w = w_cumsum[bin_indices]"}
{"namespace": "stepfun.sample", "completion": "  if rng is None:\n    # Deterministic sampling\n    if deterministic_center:\n      t_min = t[:-1]\n      t_max = t[1:]\n      t_center = (t_min + t_max) / 2\n      delta = (t_max - t_min) / 2\n      samples = jnp.linspace(t_center - delta, t_center + delta, num_samples)\n    else:\n      samples = jnp.linspace(t[0], t[-1], num_samples)\n\n    # Add eps to avoid log(0)\n    w_logits = jnp.log(jnp.exp(w_logits) + eps)\n    w_logits = jnp.concatenate([w_logits, jnp.zeros([1], dtype=w_logits.dtype)])\n\n    # Compute the cumulative sum of the logits\n    cum_logits = jnp.cumsum(w_logits)\n\n    # Normalize the cumulative logits\n    cum_logits = cum_logits - jnp.max(cum_logits)\n\n    # Compute the PDF\n    pdf = jnp.exp(cum_logits)\n\n    # Compute the CDF\n    cdf = jnp.cumsum(pdf)\n\n    # Normalize the CDF\n    cdf = cdf / jnp.sum(pdf)\n\n    # Compute the inverse CDF\n    inverse_cdf = jnp.searchsorted(cdf, samples)\n\n    # Map the inverse CDF to the bin endpoints\n    samples = t[inverse_cdf]\n\n    # Add eps to avoid log(0)\n    w_logits = jnp.log(jnp.exp(w_logits) + eps)\n\n    # Compute the cumulative sum of the logits\n    cum_logits = jnp.cumsum(w_logits)\n\n    # Normalize the cumulative logits\n    cum_logits = cum_logits - jnp"}
{"namespace": "stepfun.sample", "completion": "  batch_size = t.shape[0]\n  if rng is None:\n    rng = jax.random.PRNGKey(0)\n\n  # Compute the cumulative distribution function (CDF) from the weights.\n  w_logits = w_logits.reshape(batch_size, -1)\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cdf = jnp.cumsum(w, axis=-1)\n\n  # Determine the bin indices for each sample.\n  if deterministic_center:\n    # Deterministic sampling: Center each bin.\n    bin_indices = jnp.arange(cdf.shape[-1])\n    bin_indices = jnp.tile(bin_indices[None, :], (batch_size, 1))\n  else:\n    # Random sampling: Uniformly sample from the entire PDF.\n    bin_indices = jax.random.uniform(\n        rng, shape=(batch_size, num_samples), minval=0, maxval=cdf.shape[-1]\n    )\n\n  # Convert the bin indices to integers.\n  bin_indices = jnp.round(bin_indices).astype(jnp.int32)\n\n  # Compute the bin endpoints for each sample.\n  t = t.reshape(batch_size, -1)\n  t0 = t[:, bin_indices]\n  t1 = t[:, bin_indices + 1]\n\n  # Compute the uniform samples in the range [0, 1] for each sample.\n  u = jax.random.uniform(\n      rng, shape=(batch_size, num_samples), minval=0, maxval=1\n  )\n\n  # Jitter the uniform samples.\n  if single_jitter:\n    u += eps\n  else:\n    u = u + jax.random.uniform(\n        rng, shape=(batch_size, num_samples), minval=0, maxval=eps\n    )"}
{"namespace": "stepfun.sample", "completion": "  # Compute the cumulative sum of the weights to obtain the PDF\n  w_cumsum = jnp.cumsum(jax.nn.softmax(w_logits), axis=-1)\n\n  # Compute the PDF widths\n  pdf_width = jnp.diff(t, axis=-1)\n\n  # Determine the center of the PDF\n  if deterministic_center:\n    t_center = jnp.mean(t, axis=-1, keepdims=True)\n  else:\n    t_center = t[..., :-1] + pdf_width / 2\n\n  # Compute the PDF height\n  pdf_height = w_cumsum[..., -1]\n\n  # Normalize the PDF height to ensure it sums to 1\n  w_cumsum /= pdf_height\n\n  # Compute the PDF intervals\n  pdf_intervals = jnp.stack([t[..., :-1], t[..., 1:]], axis=-1)\n\n  # Compute the PDF interval lengths\n  pdf_interval_lengths = pdf_intervals[..., 1] - pdf_intervals[..., 0]\n\n  # Determine the jitter value\n  if single_jitter:\n    jitter = jnp.full(num_samples, eps)\n  else:\n    jitter = jax.random.uniform(rng, (num_samples,), minval=eps, maxval=eps)\n\n  # Generate random samples\n  if rng is None:\n    # Use linspace for deterministic sampling\n    samples = jnp.linspace(0, 1, num_samples)\n  else:\n    # Use uniform sampling for random sampling\n    samples = jax.random.uniform(rng, (num_samples,), minval=0, maxval=1)\n\n  # Add the jitter to the samples\n  samples = samples + jitter\n\n  # Find the bin indices for each sample\n  bin_indices = jnp.sum(samples[..., None] < w_cumsum,"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function using the provided bin endpoint coordinates and weights.\n  if rng is None:\n    # If rng is None, use linspace sampling.\n    t_samples = jnp.linspace(domain[0], domain[1], num_samples + 1)\n  else:\n    # If rng is not None, use inverse CDF sampling.\n    t_samples = jax.random.categorical(rng, w_logits, shape=(num_samples + 1,))\n\n  # Calculate midpoints between adjacent samples.\n  t_samples = (t_samples[1:] + t_samples[:-1]) / 2.0\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  return t_samples\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function\n  if rng is not None:\n    u = jax.random.uniform(rng, (num_samples,))\n  else:\n    u = jnp.linspace(0.0, 1.0, num_samples)\n\n  # Calculate the midpoints between adjacent samples\n  midpoints = (t[1:] + t[:-1]) / 2.0\n\n  # Sample points from the step function\n  if rng is not None:\n    u = jax.random.uniform(rng, (num_samples,))\n  else:\n    u = jnp.linspace(0.0, 1.0, num_samples)\n\n  # Calculate the midpoints between adjacent samples\n  midpoints = (t[1:] + t[:-1]) / 2.0\n\n  # Sample points from the step function\n  if rng is not None:\n    u = jax.random.uniform(rng, (num_samples,))\n  else:\n    u = jnp.linspace(0.0, 1.0, num_samples)\n\n  # Calculate the midpoints between adjacent samples\n  midpoints = (t[1:] + t[:-1]) / 2.0\n\n  # Sample points from the step function\n  if rng is not None:\n    u = jax.random.uniform(rng, (num_samples,))\n  else:\n    u = jnp.linspace(0.0, 1.0, num_samples)\n\n  # Calculate the midpoints between adjacent samples\n  midpoints = (t[1:] + t[:-1]) / 2.0\n\n  # Sample points from the step function\n  if rng is not None:\n    u = jax.random.uniform(rng, (num_samples,))\n  else:\n    u = jnp.linspace(0.0, 1.0, num_samples)\n\n  # Calculate the midpoints between adjacent samples\n  midpoints = (t[1:] + t[:-1])"}
{"namespace": "stepfun.sample_intervals", "completion": "  # sample points from the step function\n  if rng is None:\n    # linspace sampling\n    t_samples = jnp.linspace(t[0], t[-1], num_samples + 1)\n  else:\n    # random sampling\n    t_samples = jax.random.categorical(rng, w_logits, shape=(num_samples + 1,))\n\n  # calculate midpoints between adjacent samples\n  t_samples = (t_samples[1:] + t_samples[:-1]) / 2.0\n\n  # adjust first and last intervals to fit within the specified domain\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  # return the sampled intervals\n  return t_samples\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function\n  if rng is None:\n    # Use linspace sampling\n    t_samples = jnp.linspace(domain[0], domain[1], num_samples + 1)\n  else:\n    # Use inverse CDF sampling\n    t_samples = jax.random.uniform(rng, (num_samples + 1,))\n    t_samples = jnp.sort(t_samples)\n\n  # Calculate midpoints between adjacent samples\n  t_samples = (t_samples[1:] + t_samples[:-1]) / 2.0\n\n  # Adjust the first and last intervals to ensure they are within the domain\n  t_samples = jnp.concatenate([[domain[0]], t_samples, [domain[1]]])\n\n  # Apply jitter to the samples\n  if single_jitter:\n    t_samples = t_samples + jax.random.uniform(rng, t_samples.shape)\n  else:\n    t_samples = t_samples + jax.random.uniform(rng, t_samples.shape)\n\n  return t_samples\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function\n  if rng is None:\n    # If rng is None, use linspace to sample points\n    t_samples = jnp.linspace(domain[0], domain[1], num_samples)\n  else:\n    # If rng is provided, sample points from the step function using inverse CDF\n    t_samples = jax.random.cumulative_distribution(rng, t, w_logits, num_samples)\n\n  # Calculate midpoints between adjacent samples\n  t_samples = (t_samples[1:] + t_samples[:-1]) / 2.0\n\n  # Adjust the first and last intervals to ensure they are within the specified domain\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  # If single_jitter is True, jitter every sample by the same amount\n  if single_jitter:\n    t_samples = t_samples + jax.random.uniform(rng, (num_samples,))\n  # Otherwise, jitter each sample independently in the inverse CDF\n  else:\n    t_samples = t_samples + jax.random.cumulative_distribution(\n      rng, t, w_logits, num_samples\n    )\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  # check that t is sorted\n  assert jnp.all(jnp.diff(t) >= 0), \"t must be sorted\"\n\n  # check that t is within the specified domain\n  assert jnp.all(t >= domain[0]) and jnp.all(t <= domain[1]), \"t must be within the specified domain\"\n\n  # check that w_logits is a vector\n  assert w_logits.ndim == 1, \"w_logits must be a vector\"\n\n  # check that w_logits is the same length as t\n  assert w_logits.shape[0] == t.shape[0], \"w_logits must be the same length as t\"\n\n  # check that num_samples is a positive integer\n  assert isinstance(num_samples, int) and num_samples > 0, \"num_samples must be a positive integer\"\n\n  # check that domain is a tuple of two floats\n  assert isinstance(domain, tuple) and len(domain) == 2, \"domain must be a tuple of two floats\"\n\n  # check that domain is within the specified domain\n  assert domain[0] >= -jnp.inf and domain[1] <= jnp.inf, \"domain must be within the specified domain\"\n\n  # check that domain is not empty\n  assert domain[0] < domain[1], \"domain must not be empty\"\n\n  # check that domain is not too large\n  assert domain[1] - domain[0] <= 1e6, \"domain must not be too large\"\n\n  # check that domain is not too small\n  assert domain[1] - domain[0] >= 1e-6, \"domain must not be too small\"\n\n  # check that domain is not too small\n  assert domain[1] - domain[0] >= 1e-6, \"domain must not be too small\"\n\n  # check that domain is not too small\n  assert domain[1] - domain[0] >= 1e-6, \"domain must not be too small\"\n\n  # check that domain is not too small\n  assert domain[1] - domain[0] >= 1"}
{"namespace": "stepfun.sample_intervals", "completion": "  # assert t.ndim == 1\n  # assert w_logits.ndim == 1\n  # assert t.shape == w_logits.shape\n  # assert domain[0] <= domain[1]\n\n  # if rng is None:\n  #   # linspace sampling\n  #   t_samples = jnp.linspace(domain[0], domain[1], num_samples + 1)\n  #   return (t_samples[1:] + t_samples[:-1]) / 2.0\n\n  # # sample points from the step function\n  # t_samples = sample_points(rng, t, w_logits, num_samples, single_jitter)\n  # # calculate midpoints between adjacent samples\n  # return (t_samples[1:] + t_samples[:-1]) / 2.0\n  pass\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function using the provided logits\n  if rng is None:\n    # If rng is None, use linspace sampling\n    t_samples = jnp.linspace(domain[0], domain[1], num_samples + 1)\n  else:\n    # If rng is not None, use the provided rng to sample\n    t_samples = jax.random.categorical(rng, w_logits, shape=(num_samples + 1,))\n\n  # Calculate midpoints between adjacent samples\n  t_samples = (t_samples + jnp.roll(t_samples, -1)) / 2.0\n\n  # Adjust the first and last intervals to ensure they are within the specified domain\n  t_samples = t_samples.at[0].set(domain[0])\n  t_samples = t_samples.at[-1].set(domain[1])\n\n  # If single_jitter is True, jitter every sample by the same amount\n  if single_jitter:\n    t_samples += jax.random.uniform(rng, t_samples.shape)\n  # If single_jitter is False, jitter each sample independently in the inverse CDF\n  else:\n    t_samples += jax.random.uniform(rng, t_samples.shape) * (\n        jax.scipy.stats.norm.cdf(t_samples) - jax.scipy.stats.norm.cdf(t_samples[0])\n    ) / (t_samples[-1] - t_samples[0])\n\n  return t_samples\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Check if rng is None. If it is, we use linspace sampling.\n  if rng is None:\n    # linspace sampling\n    t_samples = jnp.linspace(domain[0], domain[1], num_samples + 1)\n  else:\n    # sample from the step function\n    t_samples = sample_step_function(rng, t, w_logits, num_samples + 1)\n\n  # calculate midpoints between adjacent samples\n  t_samples = (t_samples[1:] + t_samples[:-1]) / 2\n\n  # adjust the first and last intervals to ensure they are within the domain\n  t_samples = jnp.concatenate([[domain[0]], t_samples, [domain[1]]])\n\n  return t_samples\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Compute the number of bins\n  num_bins = t.shape[0] - 1\n\n  # Sample num_samples points from the step function\n  if rng is None:\n    # If rng is None, use linspace to sample points from the step function\n    t_samples = jnp.linspace(domain[0], domain[1], num_samples + 1)\n  else:\n    # Otherwise, sample points from the step function using the provided rng\n    t_samples = rng.uniform(\n        shape=(num_samples + 1,),\n        minval=domain[0],\n        maxval=domain[1],\n    )\n\n  # Calculate midpoints between adjacent samples\n  t_midpoints = (t_samples[1:] + t_samples[:-1]) / 2.0\n\n  # Adjust the first and last intervals to ensure they are within the specified domain\n  t_midpoints = jnp.concatenate([[domain[0]], t_midpoints, [domain[1]]])\n\n  # Return the adjusted midpoints\n  return t_midpoints\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  if rng is None:\n    t_samples = jnp.linspace(domain[0], domain[1], num_samples)\n  else:\n    t_samples = jax.random.uniform(\n      rng, (num_samples,), minval=domain[0], maxval=domain[1]\n    )\n\n  t_samples = jnp.sort(t_samples)\n\n  if single_jitter:\n    t_samples = t_samples + jax.random.uniform(\n      rng, (num_samples,), minval=-1.0 / num_samples, maxval=1.0 / num_samples\n    )\n  else:\n    t_samples = t_samples + jax.random.uniform(\n      rng, (num_samples,), minval=-1.0 / num_samples, maxval=1.0 / num_samples\n    )\n\n  t_samples = jnp.minimum(t_samples, domain[1])\n  t_samples = jnp.maximum(t_samples, domain[0])\n\n  t_samples = (t_samples[1:] + t_samples[:-1]) / 2.0\n\n  t_samples = jnp.concatenate(\n    [jnp.array([domain[0]]), t_samples, jnp.array([domain[1]])]\n  )\n\n  return t_samples\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # 1. Sample points from the step function\n  if rng is None:\n    # Use linspace for sampling\n    t_samples = jnp.linspace(domain[0], domain[1], num_samples)\n  else:\n    # Use inverse CDF for sampling\n    t_samples = jax.random.uniform(rng, (num_samples,))\n    t_samples = jax.scipy.stats.norm.ppf(t_samples)\n\n  # 2. Calculate midpoints between adjacent samples\n  t_midpoints = (t_samples[1:] + t_samples[:-1]) / 2\n\n  # 3. Adjust the first and last intervals to ensure they are within the specified domain\n  t_midpoints = jnp.concatenate([t_samples[:1] + t_midpoints[:1], t_midpoints, t_samples[-1:] + t_samples[-1:]]) / 2\n\n  # 4. Optionally jitter every sample by the same amount or each sample independently in the inverse CDF\n  if single_jitter:\n    t_midpoints = t_midpoints + jax.random.normal(rng, t_midpoints.shape) * jnp.exp(w_logits)\n  else:\n    t_midpoints = t_midpoints + jax.random.normal(rng, t_midpoints.shape) * jnp.exp(w_logits)\n\n  return t_midpoints"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Check if the input arguments are valid.\n  assert t.ndim == 1\n  assert w_logits.ndim == 1\n  assert t.shape == w_logits.shape\n  assert t.dtype == w_logits.dtype\n  assert num_samples > 0\n  assert domain[0] < domain[1]\n\n  # Check if the step function is valid.\n  assert jnp.all(jnp.diff(t) > 0)\n\n  # Sample points from the step function using the provided random number generator.\n  if rng is None:\n    t_samples = jnp.linspace(domain[0], domain[1], num_samples + 1)\n  else:\n    t_samples = sample_step_function(rng, t, w_logits, num_samples + 1)\n\n  # Calculate the midpoints between adjacent samples.\n  t_samples = (t_samples[1:] + t_samples[:-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.array([domain[0]]),\n          t_samples,\n          jnp.array([domain[1]]),\n      ]\n  )\n\n  # Jitter the samples.\n  if single_jitter:\n    t_samples = t_samples + jax.random.uniform(rng, shape=t_samples.shape)\n  else:\n    t_samples = t_samples + jax.random.uniform(\n        rng, shape=t_samples.shape, minval=0, maxval=1\n    )\n\n  # Return the sampled intervals.\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Check if the number of samples is less than the number of bins\n  if num_samples < t.shape[0]:\n    # If so, sample without replacement\n    idx = jax.random.choice(rng, t.shape[0], shape=(num_samples,), replace=False)\n  else:\n    # If not, sample with replacement\n    idx = jax.random.choice(rng, t.shape[0], shape=(num_samples,), replace=True)\n\n  # Get the sampled bin endpoints\n  t_samples = t[idx]\n\n  # Calculate the midpoints between adjacent samples\n  t_midpoints = (t_samples[1:] + t_samples[:-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain\n  t_samples = jnp.concatenate([t_samples[0:1], t_midpoints, t_samples[-1:]])\n\n  # Add jitter to the sampled intervals\n  if single_jitter:\n    # If 'single_jitter' is True, jitter all samples by the same amount\n    t_samples = t_samples + jax.random.uniform(rng, t_samples.shape, minval=-1, maxval=1)\n  else:\n    # If 'single_jitter' is False, jitter each sample independently in the inverse CDF\n    t_samples = t_samples + jax.random.uniform(rng, t_samples.shape, minval=-1, maxval=1)\n\n  # Clip the sampled intervals to fit within the specified domain\n  t_samples = jnp.clip(t_samples, a_min=domain[0], a_max=domain[1])\n\n  return t_samples\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Get the number of bins\n  n = t.shape[0]\n\n  # If rng is None, use linspace sampling\n  if rng is None:\n    # Calculate the step size\n    step = (domain[1] - domain[0]) / num_samples\n\n    # Generate a linspace of points\n    t = jnp.arange(domain[0], domain[1] + step, step)\n\n    # Calculate the midpoints between adjacent points\n    t = (t[1:] + t[:-1]) / 2\n\n    # Adjust the first and last intervals to ensure they are within the specified domain\n    t = jnp.concatenate([[domain[0]], t, [domain[1]]])\n\n    # Return the sampled intervals\n    return t\n\n  # Sample points from the step function\n  t = sample_step(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate the midpoints between adjacent points\n  t = (t[1:] + t[:-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain\n  t = jnp.concatenate([[domain[0]], t, [domain[1]]])\n\n  # Return the sampled intervals\n  return t\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  ### Check for valid inputs\n  assert t.ndim == 1, \"t must be a 1D array\"\n  assert w_logits.ndim == 1, \"w_logits must be a 1D array\"\n  assert num_samples > 0, \"num_samples must be a positive integer\"\n  assert domain[0] <= domain[1], \"domain[0] must be less than or equal to domain[1]\"\n\n  ### Check if rng is provided, otherwise use linspace sampling\n  if rng is None:\n    t = jnp.linspace(domain[0], domain[1], num_samples + 1)\n    return t[1:] - (t[1:] - t[:-1]) / 2\n\n  ### Sample points from the step function\n  t = t.at[0].set(domain[0])\n  t = t.at[-1].set(domain[1])\n  t = jnp.sort(t)\n  w_logits = jnp.concatenate([jnp.array([0.0]), w_logits, jnp.array([0.0])])\n  w = jax.nn.softmax(w_logits)\n  w = w / jnp.sum(w)\n  cdf = jnp.cumsum(w)\n  cdf = jnp.concatenate([jnp.array([0.0]), cdf])\n  cdf = cdf / cdf[-1]\n  u = rng.uniform(cdf.shape)\n  indices = jnp.searchsorted(cdf, u, side=\"right\") - 1\n  indices = jnp.clip(indices, 0, cdf.shape[0] - 2)\n  t_samples = t[indices]\n\n  ### Calculate midpoints between adjacent samples\n  t_samples = t_samples[1:] - (t_samples[1:] - t_samples[:-1]) / 2\n\n  ### Adjust first and last intervals to ensure they are within the specified domain\n  t_samples ="}
{"namespace": "stepfun.sample_intervals", "completion": "  # Get the number of bins\n  num_bins = t.shape[0] - 1\n\n  # Sample points from the step function\n  if rng is None:\n    # If rng is None, use linspace sampling\n    u = jnp.linspace(0, 1, num_samples)\n  else:\n    # Otherwise, use inverse CDF sampling\n    u = jax.random.uniform(rng, (num_samples,))\n    u = jnp.sort(u)\n\n  # Calculate the midpoints between adjacent samples\n  midpoints = (u[1:] + u[:-1]) / 2\n\n  # Calculate the indices of the bins corresponding to the midpoints\n  indices = jnp.minimum(jnp.searchsorted(u, t[:-1], side=\"right\"), num_bins - 1)\n\n  # Calculate the weights for each sample\n  w = jax.nn.softmax(w_logits, axis=-1)\n  w = w[indices]\n\n  # Calculate the jitter for each sample\n  if single_jitter:\n    # If single_jitter is True, use a single jitter value for all samples\n    jitter = jax.random.uniform(rng, (num_samples,))\n  else:\n    # Otherwise, use a different jitter value for each sample\n    jitter = jax.random.uniform(rng, (num_samples,))\n\n  # Calculate the sampled intervals\n  intervals = jnp.diff(t)[indices] * w + midpoints * jitter\n\n  # Adjust the first and last intervals to ensure they are within the specified domain\n  intervals = jnp.concatenate(\n      [\n          jnp.array([domain[0]], dtype=jnp.float32),\n          intervals,\n          jnp.array([domain[1]], dtype=jnp.float32),\n      ]\n  )\n\n  return intervals"}
{"namespace": "stepfun.sample_intervals", "completion": "  ### 1. Sample points from the step function\n  if rng is None:\n    # If rng is None, we use a linear sampling strategy\n    points = jnp.linspace(domain[0], domain[1], num_samples + 1)\n  else:\n    # If rng is not None, we use a rejection sampling strategy\n    points = jax.random.uniform(rng, (num_samples + 1,))\n    points = jnp.sort(points)\n\n  ### 2. Calculate midpoints between adjacent points\n  midpoints = (points[1:] + points[:-1]) / 2.0\n\n  ### 3. Adjust the first and last intervals to ensure they are within the specified domain\n  points = jnp.concatenate([[domain[0]], points, [domain[1]]])\n  midpoints = jnp.concatenate([[domain[0]], midpoints, [domain[1]]])\n\n  ### 4. Sample intervals from the step function\n  intervals = jax.random.categorical(rng, w_logits, shape=(num_samples,))\n\n  ### 5. Adjust the first and last intervals to ensure they are within the specified domain\n  intervals = jnp.concatenate([[0], intervals, [t.shape[0] - 1]])\n\n  ### 6. Calculate the midpoints of the sampled intervals\n  intervals = midpoints[intervals]\n\n  ### 7. Optionally jitter the intervals\n  if single_jitter:\n    intervals += jax.random.uniform(rng, intervals.shape)\n  else:\n    intervals += jax.random.uniform(rng, intervals.shape)\n\n  ### 8. Return the sampled intervals\n  return intervals\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Get the number of bins\n  n_bins = t.shape[0] - 1\n\n  # Check if the number of samples is less than or equal to the number of bins\n  if num_samples <= n_bins:\n    # If so, return the bin edges as the sampled intervals\n    return t\n\n  # Check if rng is None, indicating that 'linspace' sampling is to be used\n  if rng is None:\n    # Calculate the step size for 'linspace' sampling\n    step_size = (t[-1] - t[0]) / num_samples\n    # Use 'linspace' to generate the sampled intervals\n    return jnp.linspace(t[0], t[-1], num_samples, endpoint=True) + step_size / 2\n\n  # Sample indices from a categorical distribution using the provided weights\n  indices = rng.categorical(w_logits, num_samples)\n\n  # Calculate the midpoints between adjacent samples\n  midpoints = (t[indices] + t[indices + 1]) / 2\n\n  # Calculate the jitter amount for each sample\n  jitter = jnp.where(\n      single_jitter,\n      jnp.ones(num_samples) * (t[1] - t[0]) / num_samples,\n      rng.uniform(shape=(num_samples,), minval=0, maxval=1) * (t[1] - t[0]) / num_samples,\n  )\n\n  # Add the jitter to the midpoints\n  midpoints = midpoints + jitter\n\n  # Adjust the first and last intervals to ensure they are within the specified domain\n  midpoints = jnp.clip(midpoints, domain[0], domain[1])\n\n  # Return the sampled intervals\n  return midpoints"}
{"namespace": "stepfun.sample_intervals", "completion": "  # t is the bin edges, w_logits are the weights of the bins\n  # t is sorted, w_logits are logits\n  # num_samples is the number of intervals to sample\n  # single_jitter is a bool, if True, jitter every sample by the same amount, if False, jitter each sample independently in the inverse CDF\n  # domain is a tuple of two floats, the min and max values of t\n\n  # Get the number of bins\n  num_bins = t.shape[0] - 1\n\n  # Sample the bin indices\n  # if rng is None, sample uniformly\n  if rng is None:\n    bin_indices = jnp.linspace(0, num_bins - 1, num_samples, dtype=jnp.int32)\n  else:\n    bin_indices = jax.random.choice(\n        rng, jnp.arange(num_bins), shape=(num_samples,), replace=True\n    )\n\n  # Calculate the midpoints of the bins\n  # midpoints are the bin edges shifted by half a bin width\n  midpoints = (t[1:] + t[:-1]) / 2.0\n\n  # Get the midpoints of the sampled bins\n  sampled_midpoints = midpoints[bin_indices]\n\n  # Get the widths of the sampled bins\n  # widths are the bin edges shifted by half a bin width\n  widths = t[1:] - t[:-1]\n  # widths are the bin edges shifted by half a bin width\n  sampled_widths = widths[bin_indices]\n\n  # Calculate the jitter\n  # jitter is a random number between 0 and 1\n  # if single_jitter is True, jitter is the same for all samples\n  # if single_jitter is False, jitter is a random number for each sample\n  if single_jitter:\n    jitter = jax.random.uniform(rng, shape=(num_samples,))\n  else:\n    jitter = jax.random.uniform("}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  w = w / np.sum(w)\n\n  # Integrate the weights\n  w_cum = np.cumsum(w)\n\n  # Interpolate the percentiles into the integrated weights\n  ps_interp = np.interp(ps / 100, w_cum, t)\n\n  return ps_interp"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  w = w / np.sum(w)\n\n  # Compute the integrated weights\n  w_int = np.cumsum(w)\n\n  # Compute the weighted percentiles\n  ps_int = np.interp(ps, w_int, t)\n\n  return ps_int"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Check that the weights sum to 1\n  if not np.isclose(np.sum(w), 1):\n    raise ValueError(\"The weights must sum to 1.\")\n\n  # Check that the percentiles are between 0 and 100\n  if np.any(ps < 0) or np.any(ps > 100):\n    raise ValueError(\"The percentiles must be between 0 and 100.\")\n\n  # Integrate the weights\n  w_int = np.cumsum(w)\n\n  # Compute the weighted percentiles\n  wp = np.interp(ps, w_int, t)\n\n  return wp"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Make sure that the weights sum to 1\n  w = w / np.sum(w)\n\n  # Integrate the weights\n  w_int = np.cumsum(w)\n\n  # Compute the weighted percentiles\n  return np.interp(ps / 100, w_int, t)"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Compute the cumulative weights and normalize them to sum to 1\n  w_cum = np.cumsum(w)\n  w_cum = w_cum / w_cum[-1]\n\n  # Interpolate the percentiles into the cumulative weights\n  ps_interp = np.interp(ps, w_cum, t)\n\n  return ps_interp"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  w = w / np.sum(w)\n\n  # Integrate the weights\n  w_int = np.cumsum(w)\n\n  # Find the indices of the percentiles in the integrated weights\n  ps_ind = np.searchsorted(w_int, ps / 100)\n\n  # Interpolate the percentiles\n  ps_interp = np.interp(ps_ind, np.arange(len(t)), t)\n\n  return ps_interp"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Check if the weights sum to 1\n  if not np.isclose(np.sum(w), 1):\n    raise ValueError(\"The weights must sum to 1.\")\n\n  # Integrate the weights to obtain the cumulative distribution function (CDF)\n  cdf = np.cumsum(w)\n\n  # Interpolate the CDF to find the x-values corresponding to the desired percentiles\n  x_values = np.interp(ps / 100, cdf, t)\n\n  return x_values"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  w = w / np.sum(w)\n\n  # Integrate the weights to get the cumulative weights\n  cw = np.cumsum(w)\n\n  # Calculate the percentiles of the cumulative weights\n  ps = np.interp(ps, np.linspace(0, 100, len(cw)), cw * 100)\n\n  # Interpolate the percentiles into the step function\n  return np.interp(ps, t, t)"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Check if the weights sum to 1\n  if abs(sum(w) - 1) > 1e-8:\n    raise ValueError('The weights do not sum to 1.')\n\n  # Sort the weights and values\n  w = np.sort(w)\n  t = np.sort(t)\n\n  # Integrate the weights\n  w = np.cumsum(w)\n\n  # Compute the percentiles\n  ps = np.interp(ps, w, t)\n\n  return ps"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Check that the input arguments are valid\n  if len(t) != len(w):\n    raise ValueError(\"'t' and 'w' must have the same length.\")\n  if not (0 <= ps <= 100).all():\n    raise ValueError(\"All percentiles must be between 0 and 100.\")\n\n  # Integrate the weights\n  w_int = np.cumsum(w)\n\n  # Normalize the integrated weights to sum to 1\n  w_int /= w_int[-1]\n\n  # Interpolate the percentiles into the integrated weights\n  ps_int = np.interp(ps, w_int, t)\n\n  return ps_int"}
{"namespace": "stepfun.weighted_percentile", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Compute the integrated weights\n  w_int = np.cumsum(w)\n\n  # Ensure that the weights sum to 1\n  w_int /= w_int[-1]\n\n  # Interpolate the percentiles into the integrated weights\n  interp = interp1d(w_int, t, fill_value=\"extrapolate\")\n  return interp(ps / 100)"}
{"namespace": "stepfun.weighted_percentile", "completion": "  if len(t) != len(w):\n    raise ValueError(\"The length of 't' and 'w' must be the same.\")\n  if not 0 <= ps.min() and ps.max() <= 100:\n    raise ValueError(\"The values in 'ps' must be between 0 and 100.\")\n\n  # Integrate the weights\n  w = np.cumsum(w)\n\n  # Normalize the weights\n  w = w / w[-1]\n\n  # Interpolate the percentiles\n  ps_interp = np.interp(ps / 100, w, t)\n\n  return ps_interp"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Check input\n  if len(t) != len(w):\n    raise ValueError(\"t and w must have the same length.\")\n  if len(ps) == 0:\n    raise ValueError(\"ps must have at least one element.\")\n  if np.any(ps < 0) or np.any(ps > 100):\n    raise ValueError(\"ps must be between 0 and 100.\")\n  if not np.isclose(np.sum(w), 1.0):\n    raise ValueError(\"w must sum to 1.\")\n\n  # Integrate weights\n  w_int = np.cumsum(w)\n\n  # Compute weighted percentiles\n  ps_int = np.interp(ps, w_int, t)\n\n  return ps_int"}
{"namespace": "stepfun.weighted_percentile", "completion": "  import numpy as np\n  # Check that the weights sum to 1\n  if np.sum(w) != 1.0:\n    raise ValueError(\"The weights must sum to 1.\")\n  # Calculate the cumulative weights\n  w_cum = np.cumsum(w)\n  # Interpolate the percentiles\n  ps_int = np.interp(ps / 100, w_cum, t)\n  return ps_int"}
{"namespace": "stepfun.weighted_percentile", "completion": "  if len(t) != len(w):\n    raise ValueError('The length of t and w must be the same.')\n  if len(ps) == 0:\n    raise ValueError('The length of ps must be positive.')\n  if np.any(ps < 0) or np.any(ps > 100):\n    raise ValueError('The percentiles must be between 0 and 100.')\n\n  # Ensure that the weights sum to 1\n  w = w / np.sum(w)\n\n  # Integrate the weights\n  w_int = np.cumsum(w)\n\n  # Compute the weighted percentiles\n  ps_out = np.interp(ps, w_int, t)\n\n  return ps_out"}
{"namespace": "stepfun.weighted_percentile", "completion": "  w = np.array(w)\n  ps = np.array(ps)\n  assert w.ndim == 1\n  assert ps.ndim == 1\n  assert np.all(ps >= 0) and np.all(ps <= 100)\n  assert np.all(np.isfinite(w))\n  assert np.all(np.isfinite(t))\n  assert np.all(np.isfinite(ps))\n  assert np.all(np.isfinite(t[0]))\n  assert np.all(np.isfinite(t[-1]))\n  assert np.all(np.isfinite(ps[0]))\n  assert np.all(np.isfinite(ps[-1]))\n  assert np.all(w >= 0)\n  assert np.all(w <= 1)\n  assert np.all(w[0] == 0)\n  assert np.all(w[-1] == 1)\n  assert np.all(np.isfinite(w[0]))\n  assert np.all(np.isfinite(w[-1]))\n  assert np.all(np.isfinite(t[0]))\n  assert np.all(np.isfinite(t[-1]))\n  assert np.all(np.isfinite(ps[0]))\n  assert np.all(np.isfinite(ps[-1]))\n  assert np.all(np.isfinite(w[0]))\n  assert np.all(np.isfinite(w[-1]))\n  assert np.all(np.isfinite(t[0]))\n  assert np.all(np.isfinite(t[-1]))\n  assert np.all(np.isfinite(ps[0]))\n  assert np.all(np.isfinite(ps[-1]))\n  assert np.all(np.isfinite(w[0]))\n  assert np.all(np.isfinite(w[-1]))\n  assert np.all(np.isfinite(t[0]))\n  assert np.all(np.isfinite(t[-1]))\n  assert np."}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  w = w / np.sum(w)\n\n  # Integrate the weights to obtain the cumulative distribution\n  cdf = np.cumsum(w)\n\n  # Interpolate the cumulative distribution to obtain the percentiles\n  percentiles = np.interp(ps / 100, cdf, t)\n\n  return percentiles\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  import numpy as np\n\n  # Ensure that the weights sum to 1\n  w = w / np.sum(w)\n\n  # Integrate the weights to obtain the cumulative distribution\n  cdf = np.cumsum(w)\n\n  # Find the indices of the percentiles in the sorted cumulative distribution\n  indices = np.searchsorted(cdf, ps / 100)\n\n  # Interpolate the percentiles using the indices and the sorted cumulative distribution\n  return np.interp(ps, cdf[indices] * 100, t[indices])\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Check if t and w are of the same length\n  if len(t) != len(w):\n    raise ValueError(\"Length of t and w must be the same.\")\n\n  # Check if t is sorted in ascending order\n  if not np.all(np.diff(t) >= 0):\n    raise ValueError(\"t must be sorted in ascending order.\")\n\n  # Check if w sums to 1\n  if not np.isclose(np.sum(w), 1):\n    raise ValueError(\"w must sum to 1.\")\n\n  # Check if ps are within the range [0, 100]\n  if np.any(ps < 0) or np.any(ps > 100):\n    raise ValueError(\"ps must be within the range [0, 100].\")\n\n  # Sort ps in ascending order\n  ps = np.sort(ps)\n\n  # Integrate the weights\n  w_cum = np.cumsum(w)\n\n  # Interpolate the percentiles\n  f = interp1d(w_cum, t, kind='linear', fill_value=\"extrapolate\")\n  ps_interp = f(ps / 100)\n\n  return ps_interp"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Check input\n  if not isinstance(t, list):\n    t = list(t)\n  if not isinstance(w, list):\n    w = list(w)\n  if not isinstance(ps, list):\n    ps = list(ps)\n\n  # Check that the weights sum to 1\n  if not np.isclose(sum(w), 1.0):\n    raise ValueError('The weights must sum to 1.')\n\n  # Check that the percentiles are between 0 and 100\n  if any(p < 0 or p > 100 for p in ps):\n    raise ValueError('The percentiles must be between 0 and 100.')\n\n  # Interpolate into the integrated weights\n  xs = np.interp(ps, np.cumsum(w), t)\n\n  # Return the interpolated values\n  return xs\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  pdf = w / np.sum(w)\n\n  # Blur the PDF\n  pdf_blurred = blur_pdf(pdf, t, blur_halfwidth)\n\n  # Resample the PDF to match the new time points\n  wq = resample_pdf(pdf_blurred, t, tq)\n\n  return wq"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = w / np.sum(w)\n\n  # Blur the PDF using a Gaussian kernel\n  kernel = np.exp(-0.5 * ((t - tq[:, None]) / blur_halfwidth) ** 2)\n  pdf_blurred = np.sum(kernel * pdf[None, :], axis=1)\n\n  # Normalize the blurred PDF to obtain the resampled weights\n  wq = pdf_blurred / np.sum(pdf_blurred)\n\n  return wq"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = np.interp(t, t, w)\n  pdf = pdf / np.sum(pdf)\n\n  # Blur the PDF\n  pdf_blurred = gaussian_filter1d(pdf, sigma=blur_halfwidth)\n\n  # Resample the PDF to match the new time points\n  wq = np.interp(tq, t, pdf_blurred)\n\n  return wq"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = np.interp(t, t, w)\n  pdf = pdf / np.sum(pdf)\n\n  # Blur the PDF using a Gaussian kernel\n  kernel_size = int(2 * blur_halfwidth + 1)\n  kernel = np.exp(-0.5 * (np.arange(-kernel_size // 2 + 1, kernel_size // 2 + 1) / blur_halfwidth) ** 2)\n  pdf = np.convolve(pdf, kernel, mode='same')\n  pdf = pdf / np.sum(pdf)\n\n  # Resample the PDF to match the new time points `tq`\n  wq = np.interp(tq, t, pdf)\n\n  return wq"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = w / np.sum(w)\n\n  # Blur the PDF using a Gaussian kernel with a specified half-width\n  kernel = np.exp(-0.5 * (t[:, None] - t) ** 2 / blur_halfwidth ** 2)\n  pdf_blurred = np.dot(kernel, pdf)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  wq = np.interp(tq, t, pdf_blurred)\n\n  return wq\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = np.interp(t, t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  kernel = gaussian_kernel(blur_halfwidth)\n  pdf_blurred = np.convolve(pdf, kernel, mode='same')\n\n  # Normalize the blurred PDF\n  pdf_blurred /= np.sum(pdf_blurred)\n\n  # Resample the blurred PDF to match the new time points\n  wq = np.interp(tq, t, pdf_blurred)\n\n  return wq"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n  from scipy.signal import gaussian, convolve\n\n  # Convert the histogram to a probability density function (PDF)\n  pdf = w / np.sum(w)\n\n  # Compute the PDF's cumulative distribution function (CDF)\n  cdf = np.cumsum(pdf)\n\n  # Compute the PDF's complementary cumulative distribution function (CCDF)\n  ccdf = 1 - cdf\n\n  # Compute the PDF's complementary cumulative distribution function (CCDF)\n  ccdf = 1 - cdf\n\n  # Compute the PDF's complementary cumulative distribution function (CCDF)\n  ccdf = 1 - cdf\n\n  # Compute the PDF's complementary cumulative distribution function (CCDF)\n  ccdf = 1 - cdf\n\n  # Compute the PDF's complementary cumulative distribution function (CCDF)\n  ccdf = 1 - cdf\n\n  # Compute the PDF's complementary cumulative distribution function (CCDF)\n  ccdf = 1 - cdf\n\n  # Compute the PDF's complementary cumulative distribution function (CCDF)\n  ccdf = 1 - cdf\n\n  # Compute the PDF's complementary cumulative distribution function (CCDF)\n  ccdf = 1 - cdf\n\n  # Compute the PDF's complementary cumulative distribution function (CCDF)\n  ccdf = 1 - cdf\n\n  # Compute the PDF's complementary cumulative distribution function (CCDF)\n  ccdf = 1 - cdf\n\n  # Compute the PDF's complementary cumulative distribution function (CCDF)\n  ccdf = 1 - cdf\n\n  # Compute the PDF's complementary cumulative distribution function (CCDF)\n  ccdf = 1 - ccdf\n\n  # Compute the PDF's complementary cumulative distribution function (CCDF)\n  ccdf = 1 - ccdf\n\n  # Compute the PDF's complementary"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = w / np.sum(w)\n\n  # Blur the PDF using a Gaussian kernel\n  kernel_size = int(2 * np.ceil(blur_halfwidth) + 1)\n  kernel = scipy.signal.gaussian(kernel_size, std=blur_halfwidth)\n  pdf_blurred = np.convolve(pdf, kernel, mode='same')\n\n  # Normalize the blurred PDF\n  pdf_blurred /= np.sum(pdf_blurred)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  resampled_weights = np.interp(tq, t, pdf_blurred)\n\n  return resampled_weights"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = w / np.sum(w)\n\n  # Blur the PDF using a Gaussian kernel with a given half-width\n  kernel_size = int(blur_halfwidth * 2 + 1)\n  kernel = np.exp(-0.5 * (np.arange(-kernel_size, kernel_size + 1) / blur_halfwidth) ** 2)\n  pdf_blurred = np.convolve(pdf, kernel, mode='same')\n\n  # Resample the blurred PDF to match the new time points\n  resampled_weights = np.interp(tq, t, pdf_blurred)\n\n  return resampled_weights"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  from scipy.interpolate import interp1d\n  from scipy.stats import gaussian_kde\n\n  # Convert the histogram to a PDF\n  pdf = gaussian_kde(t, bw_method=blur_halfwidth)\n\n  # Evaluate the PDF at the new time points\n  pdf_tq = pdf(tq)\n\n  # Normalize the PDF to get the resampled weights\n  w_tq = pdf_tq / pdf_tq.sum()\n\n  return w_tq\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = np.zeros(len(t))\n  for i in range(len(t)):\n    pdf[i] = w[i] / np.sum(w)\n\n  # Blur the PDF using a Gaussian kernel\n  kernel = np.exp(-0.5 * (np.arange(-blur_halfwidth, blur_halfwidth + 1) / blur_halfwidth) ** 2)\n  kernel = kernel / np.sum(kernel)\n  pdf_blurred = np.convolve(pdf, kernel, mode='same')\n\n  # Resample the PDF using the new time points\n  w_resampled = np.interp(tq, t, pdf_blurred)\n\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  from scipy.interpolate import interp1d\n  import numpy as np\n  # Convert the histogram to a probability density function (PDF)\n  pdf = np.diff(w) / np.diff(t)\n  # Blur the PDF\n  pdf_blurred = np.convolve(pdf, np.ones(2 * blur_halfwidth + 1) / (2 * blur_halfwidth + 1), mode='same')\n  # Resample the blurred PDF to match the new time points\n  resampled_pdf = interp1d(t, pdf_blurred, kind='linear')(tq)\n  # Convert the resampled PDF back to weights\n  resampled_weights = np.diff(resampled_pdf) * np.diff(tq)\n  return resampled_weights"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Convert the histogram to a probability density function (PDF)\n  pdf = np.zeros(len(t))\n  pdf[0] = w[0]\n  for i in range(1, len(t)):\n    pdf[i] = pdf[i - 1] + w[i]\n  pdf /= pdf[-1]\n\n  # Blur the PDF using a Gaussian kernel\n  def gaussian_kernel(x, sigma):\n    return np.exp(-(x ** 2) / (2 * sigma ** 2)) / (sigma * np.sqrt(2 * np.pi))\n\n  kernel = gaussian_kernel(np.arange(-blur_halfwidth, blur_halfwidth + 1), blur_halfwidth)\n  pdf_blurred = np.convolve(pdf, kernel, mode='same')\n\n  # Resample the blurred PDF to match the new time points\n  f = interp1d(t, pdf_blurred, kind='linear', bounds_error=False, fill_value=0)\n  w_resampled = f(tq)\n\n  # Normalize the resampled weights\n  w_resampled /= np.sum(w_resampled)\n\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = w / np.sum(w)\n\n  # Blur the PDF using a Gaussian kernel\n  pdf_blurred = np.convolve(pdf, np.exp(-0.5 * (tq - t[:, None]) ** 2 / blur_halfwidth ** 2), mode='same')\n\n  # Normalize the blurred PDF to ensure it sums to 1\n  pdf_blurred /= np.sum(pdf_blurred)\n\n  # Resample the blurred PDF to match the new time points\n  wq = np.interp(tq, t, pdf_blurred)\n\n  return wq\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  from scipy.interpolate import interp1d\n  from scipy.integrate import cumtrapz\n  from scipy.signal import gaussian\n\n  # Convert the histogram to a PDF\n  pdf = interp1d(t, w, kind='cubic')\n\n  # Blur the PDF with a Gaussian kernel\n  kernel = gaussian(2 * blur_halfwidth + 1, blur_halfwidth)\n  pdf_blurred = np.convolve(pdf(t), kernel, mode='same') / np.sum(kernel)\n\n  # Normalize the blurred PDF to a CDF\n  cdf = cumtrapz(pdf_blurred, t, initial=0)\n  cdf /= cdf[-1]\n\n  # Resample the CDF to match the new time points\n  resampled_weights = interp1d(cdf, t, kind='cubic')(tq)\n\n  return resampled_weights\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  pdf = np.zeros_like(t)\n  pdf[1:-1] = 0.5 * (w[1:-1] + w[:-2])\n  pdf[0] = w[0]\n  pdf[-1] = w[-1]\n  pdf /= np.sum(pdf)\n\n  # Blur the PDF using a Gaussian kernel\n  kernel_size = int(2 * blur_halfwidth + 1)\n  kernel = np.exp(-0.5 * (np.arange(kernel_size) - blur_halfwidth) ** 2 / blur_halfwidth ** 2)\n  pdf_blurred = np.convolve(pdf, kernel, mode='same')\n\n  # Resample the PDF to match the new time points\n  w_resampled = np.interp(tq, t, pdf_blurred)\n  w_resampled /= np.sum(w_resampled)\n\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n\n  # Convert the histogram to a probability density function (PDF)\n  pdf = np.zeros(len(t))\n  pdf[0] = w[0]\n  for i in range(1, len(t)):\n    pdf[i] = pdf[i-1] + (t[i] - t[i-1]) * (w[i] + w[i-1]) / 2\n  pdf /= pdf[-1]\n\n  # Blur the PDF using a Gaussian kernel with a given half-width\n  blurred_pdf = np.zeros(len(pdf))\n  for i in range(len(pdf)):\n    blurred_pdf[i] = np.sum(pdf[max(0, i - blur_halfwidth):min(len(pdf), i + blur_halfwidth + 1)])\n  blurred_pdf /= np.sum(blurred_pdf)\n\n  # Resample the blurred PDF to match the new time points\n  resampled_pdf = interp1d(t, blurred_pdf, kind='linear', bounds_error=False, fill_value=0)(tq)\n\n  return resampled_pdf"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n\n  # Convert the histogram to a PDF\n  pdf = np.zeros(t.shape)\n  dt = t[1] - t[0]\n  for i in range(len(t)):\n    pdf[i] = w[i] * dt\n\n  # Blur the PDF\n  pdf = blur_pdf(pdf, blur_halfwidth)\n\n  # Resample the PDF to match the new time points\n  wq = np.interp(tq, t, pdf)\n\n  return wq\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  from scipy.interpolate import interp1d\n\n  # Convert the histogram to a probability density function (PDF)\n  w_pdf = w / np.sum(w)\n\n  # Blur the PDF using a Gaussian kernel\n  w_blur = gaussian_kernel(t, t, w_pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  resampled_w = interp1d(t, w_blur)(tq)\n\n  return resampled_w\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  from scipy.interpolate import interp1d\n  from scipy.stats import gaussian_kde\n\n  # Convert the histogram to a PDF\n  pdf = gaussian_kde(t, w)\n\n  # Blur the PDF\n  t_blur = np.linspace(t.min(), t.max(), 1000)\n  pdf_blur = pdf(t_blur)\n  pdf_blur = gaussian_kde(t_blur, pdf_blur)(t_blur)\n  pdf_blur = pdf_blur / pdf_blur.sum()\n\n  # Resample the blurred PDF\n  pdf_blur_resampled = interp1d(t_blur, pdf_blur, kind='cubic')(tq)\n  pdf_blur_resampled = pdf_blur_resampled / pdf_blur_resampled.sum()\n\n  return pdf_blur_resampled\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Check the dimensionality of the transformation matrix\n  if transform.shape[0] != transform.shape[1]:\n    raise ValueError(\"The transformation matrix must be square.\")\n\n  # Check the dimensionality of the vectors\n  if vectors.shape[-1] != transform.shape[0] - 1:\n    raise ValueError(\"The dimensionality of the vectors must be one less than the dimensionality of the transformation matrix.\")\n\n  # Add a column of ones to the vectors\n  vectors_with_ones = np.concatenate([vectors, np.ones((*vectors.shape[:-1], 1))], axis=-1)\n\n  # Apply the transformation\n  transformed_vectors = np.einsum('...ij,...j->...i', transform, vectors_with_ones)\n\n  # Divide the transformed vectors by their last component\n  transformed_vectors = transformed_vectors / transformed_vectors[..., -1:]\n\n  # Return the transformed vectors\n  return transformed_vectors[..., :-1]"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # The shape of the input vectors.\n  shape = vectors.shape\n\n  # Reshape the vectors to a 2D array with the last dimension being the dimensionality of the points.\n  vectors = vectors.reshape((-1, shape[-1]))\n\n  # Apply the homogeneous transformation.\n  vectors = np.dot(vectors, transform)\n\n  # Reshape the transformed vectors back to the original shape.\n  vectors = vectors.reshape(shape)\n\n  return vectors"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Check that the input vectors are 3D.\n  if vectors.shape[-1] != 3:\n    raise ValueError(\"The input vectors must be 3D.\")\n\n  # Check that the input transform is a homogeneous transformation matrix.\n  if transform.shape != (4,4):\n    raise ValueError(\"The input transform must be a homogeneous transformation matrix.\")\n\n  # Apply the homogeneous transformation to the input vectors.\n  vectors_h = np.concatenate([vectors, np.ones((*vectors.shape[:-1], 1))], axis=-1)\n  vectors_transformed_h = np.einsum(\"...ij,...j->...i\", transform, vectors_h)\n  vectors_transformed = vectors_transformed_h[...,:-1] / vectors_transformed_h[...,-1:]\n\n  # Return the transformed vectors.\n  return vectors_transformed"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Check that the dimensionality of the vectors is 3\n  if vectors.shape[-1] != 3:\n    raise ValueError(\"The last dimension of the vectors must be 3.\")\n\n  # Check that the last dimension of the transform is 4\n  if transform.shape[-1] != 4:\n    raise ValueError(\"The last dimension of the transform must be 4.\")\n\n  # Check that the last dimension of the transform is 4\n  if transform.shape[-1] != 4:\n    raise ValueError(\"The last dimension of the transform must be 4.\")\n\n  # Check that the dimensionality of the transform is 4\n  if transform.shape[-2] != 4:\n    raise ValueError(\"The second to last dimension of the transform must be 4.\")\n\n  # Check that the dimensionality of the transform is 4\n  if transform.shape[-3] != 4:\n    raise ValueError(\"The third to last dimension of the transform must be 4.\")\n\n  # Check that the dimensionality of the transform is 4\n  if transform.shape[-4] != 4:\n    raise ValueError(\"The fourth to last dimension of the transform must be 4.\")\n\n  # Check that the dimensionality of the transform is 4\n  if transform.shape[-5] != 4:\n    raise ValueError(\"The fifth to last dimension of the transform must be 4.\")\n\n  # Check that the dimensionality of the transform is 4\n  if transform.shape[-6] != 4:\n    raise ValueError(\"The sixth to last dimension of the transform must be 4.\")\n\n  # Check that the dimensionality of the transform is 4\n  if transform.shape[-7] != 4:\n    raise ValueError(\"The seventh to last dimension of the transform must be 4.\")\n\n  # Check that the dimensionality of the transform is 4\n  if transform.shape[-8] != 4:\n    raise ValueError(\"The eighth to last dimension of the transform must be 4.\")\n\n  # Check that the dimensionality of the transform is 4\n  if transform.shape[-9] != 4:\n    raise ValueError(\"The"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Check the input arguments\n  assert(transform.shape[0] == transform.shape[1] == vectors.shape[1] + 1)\n\n  # Apply the homogeneous transformation\n  return (transform[:-1,:-1].dot(vectors.T) + transform[:-1,-1]).T\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Make sure the transform is a homogeneous transformation matrix\n  assert transform.shape == (4,4)\n\n  # Make sure the vectors are 3D points\n  assert vectors.shape[-1] == 3\n\n  # Add a dimension to the vectors so that we can perform the transformation\n  vectors = vectors[..., np.newaxis]\n\n  # Perform the transformation\n  transformed = np.matmul(transform, vectors)\n\n  # Remove the added dimension\n  transformed = transformed[...,0]\n\n  return transformed"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  assert len(transform.shape) == 2\n  assert transform.shape[0] == transform.shape[1]\n  assert len(vectors.shape) >= 2\n  assert vectors.shape[-1] == transform.shape[0] - 1\n\n  vectors = vectors.reshape((-1, vectors.shape[-1]))\n  transformed_vectors = np.dot(vectors, transform[:-1, :-1].T) + transform[:-1, -1]\n  transformed_vectors = transformed_vectors.reshape(vectors.shape)\n\n  return transformed_vectors\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Check that the transformation matrix is valid\n  if transform.shape[0] != transform.shape[1]:\n    raise ValueError('The transformation matrix must be square')\n\n  if transform.shape[0] != 4:\n    raise ValueError('The transformation matrix must have 4 rows')\n\n  # Check that the last dimension of the input vectors is 3\n  if vectors.shape[-1] != 3:\n    raise ValueError('The last dimension of the input vectors must be 3')\n\n  # Append a column of ones to the vectors\n  ones = np.ones(vectors.shape[:-1] + (1,))\n  vectors = np.concatenate([vectors, ones], axis=-1)\n\n  # Apply the transformation to the vectors\n  transformed_vectors = np.dot(vectors, transform.T)\n\n  # Remove the last column of the transformed vectors\n  transformed_vectors = transformed_vectors[..., :-1]\n\n  return transformed_vectors"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Check that the transformation matrix has the correct dimensions.\n  assert transform.shape == (4, 4)\n\n  # Check that the input vectors have the correct dimensions.\n  assert vectors.shape[-1] == 3\n\n  # Convert the input vectors to homogeneous coordinates.\n  vectors_homogeneous = np.concatenate((vectors, np.ones(vectors.shape[:-1] + (1,))), axis=-1)\n\n  # Apply the transformation to the input vectors.\n  transformed_vectors_homogeneous = np.dot(vectors_homogeneous, transform.T)\n\n  # Convert the transformed vectors back to cartesian coordinates.\n  transformed_vectors = transformed_vectors_homogeneous[..., :-1] / transformed_vectors_homogeneous[..., -1:]\n\n  # Return the transformed vectors.\n  return transformed_vectors\n\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Get the dimensionality of the points\n  C = vectors.shape[-1]\n\n  # Expand the input vectors to include an additional dimension, to allow for broadcasting\n  vectors = np.expand_dims(vectors, axis=-1)\n\n  # Apply the transformation\n  transformed = np.matmul(transform, vectors)\n\n  # Remove the extra dimension\n  transformed = np.squeeze(transformed, axis=-1)\n\n  # Return the transformed points\n  return transformed\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Check that the input shape is valid\n  assert len(vectors.shape) >= 2\n  assert vectors.shape[-1] == 3\n\n  # Check that the transformation matrix is valid\n  assert transform.shape == (4,4)\n\n  # Add a fourth dimension to the vectors\n  vectors = vectors[...,None]\n\n  # Apply the homogeneous transformation\n  transformed_vectors = vectors @ transform.T\n\n  # Normalize the transformed vectors\n  transformed_vectors = transformed_vectors / transformed_vectors[...,3:4]\n\n  # Remove the fourth dimension\n  transformed_vectors = transformed_vectors[...,:3]\n\n  return transformed_vectors\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Check the input arguments\n  assert len(transform.shape) == 2, \"The 'transform' argument must be a 2D array.\"\n  assert transform.shape[0] == transform.shape[1], \"The 'transform' argument must be a square matrix.\"\n  assert transform.shape[0] == vectors.shape[-1] + 1, \"The 'transform' argument must be a (C+1,C+1) matrix.\"\n  assert len(vectors.shape) >= 2, \"The 'vectors' argument must be a 2D array.\"\n  assert vectors.shape[-1] == transform.shape[0] - 1, \"The 'vectors' argument must be a (...,C) array.\"\n\n  # Apply the homogeneous transformation\n  transformed = transform[0:-1, 0:-1] @ vectors.T + transform[0:-1, -1]\n\n  # Return the transformed points\n  return transformed.T\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  assert(vectors.shape[-1] == transform.shape[0]-1)\n\n  # Pad the vectors with a 1 in the last dimension, so that we can use matrix multiplication\n  vectors_padded = np.pad(vectors, ((0, 0), (0, 1)), 'constant', constant_values=1)\n\n  # Apply the transformation\n  vectors_transformed = np.dot(vectors_padded, transform.T)\n\n  # Remove the extra dimension\n  vectors_transformed = vectors_transformed[...,:-1]\n\n  return vectors_transformed\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # The transform matrix must be 3x4\n  assert transform.shape[0] == transform.shape[1] == 3\n  assert transform.shape[1] + 1 == vectors.shape[-1]\n\n  # The last row of the transform matrix must be [0,0,0,1]\n  assert np.allclose(transform[2, :], [0, 0, 0, 1])\n\n  # The transformed vectors must be 3xN\n  assert vectors.shape[-1] == 3\n\n  # Add a 1 to the end of each vector\n  vectors = np.concatenate([vectors, np.ones((*vectors.shape[:-1], 1))], axis=-1)\n\n  # Apply the transform\n  vectors = vectors @ transform.T\n\n  # Normalize the vectors\n  vectors = vectors / vectors[..., -1:]\n\n  # Return the vectors\n  return vectors[..., :-1]"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  assert vectors.shape[-1] == transform.shape[0] - 1\n  assert transform.shape[0] == transform.shape[1]\n\n  vectors_homogeneous = np.pad(vectors, ((0, 0), (0, 1)), 'constant', constant_values=1)\n  transformed_vectors = np.einsum('ij,...j->...i', transform, vectors_homogeneous)\n  return transformed_vectors[..., :-1] / transformed_vectors[..., -1:]\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Check that the last dimension of the input vectors is 3\n  assert(vectors.shape[-1] == 3)\n\n  # Check that the last row of the transformation matrix is [0,0,0,1]\n  assert(np.all(transform[-1,:-1] == 0))\n  assert(transform[-1,-1] == 1)\n\n  # Add a dimension to the vectors array, and then add a dimension to the transformation matrix, so that we can multiply them together\n  vectors = np.expand_dims(vectors, axis=-2)\n  transform = np.expand_dims(transform, axis=0)\n\n  # Perform the transformation\n  transformed_vectors = np.matmul(transform, vectors)\n\n  # Remove the added dimension from the vectors array\n  transformed_vectors = np.squeeze(transformed_vectors, axis=-2)\n\n  return transformed_vectors\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Add the constant 1 to each vector\n  vectors = np.concatenate([vectors, np.ones(vectors.shape[:-1] + (1,))], axis=-1)\n\n  # Apply the transformation\n  vectors = np.matmul(vectors, transform.T)\n\n  # Divide by the last coordinate\n  vectors = vectors[..., :-1] / vectors[..., -1:]\n\n  return vectors\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  ## Extract the transformation matrix from the homogeneous transformation.\n  transform = transform[:-1,:-1]\n\n  ## Concatenate the input vectors with ones to form a matrix of vectors.\n  vectors = np.concatenate((vectors, np.ones((vectors.shape[0],1))), axis=1)\n\n  ## Apply the transformation.\n  vectors = np.matmul(vectors, transform.T)\n\n  ## Return the transformed vectors.\n  return vectors\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Get the dimensionality of the points\n  C = vectors.shape[-1]\n\n  # Create a homogeneous version of the points\n  vectors_homogeneous = np.concatenate([vectors, np.ones((*vectors.shape[:-1], 1))], axis=-1)\n\n  # Apply the transformation\n  vectors_transformed = np.einsum('...c,...cd->...d', vectors_homogeneous, transform)\n\n  # Divide by the last coordinate\n  vectors_transformed = vectors_transformed[..., :C] / vectors_transformed[..., C:]\n\n  return vectors_transformed\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # The last row of the transform matrix is always [0,0,0,1]\n  # We can ignore it, and treat the remaining rows as the transformation matrix\n  transform = transform[:3, :3]\n\n  # Convert the vector array to a matrix\n  vectors = np.reshape(vectors, (-1, vectors.shape[-1]))\n\n  # Apply the transform\n  transformed = np.dot(vectors, transform.T)\n\n  # Reshape the output to match the input\n  transformed = np.reshape(transformed, vectors.shape)\n\n  return transformed\n\n"}
{"namespace": "stepfun.resample", "completion": "  t = t.reshape(-1)\n  tp = tp.reshape(-1)\n  vp = vp.reshape(-1)\n\n  if use_avg:\n    v = torch.zeros_like(t)\n    for i in range(len(t)):\n      v[i] = torch.mean(vp[(tp >= t[i]) & (tp < t[i + 1])])\n  else:\n    v = torch.zeros_like(t)\n    for i in range(len(t)):\n      v[i] = torch.sum(vp[(tp >= t[i]) & (tp < t[i + 1])])\n\n  return v"}
{"namespace": "stepfun.resample", "completion": "  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp are lists of tensors.\n  # TODO: Add support for the case where tp and vp"}
{"namespace": "stepfun.resample", "completion": "  t = t.unsqueeze(1)\n  tp = tp.unsqueeze(0)\n  vp = vp.unsqueeze(0)\n  v = torch.where(t >= tp, vp, torch.zeros_like(vp))\n  v = torch.cumsum(v, dim=1)\n  if use_avg:\n    v = v / (t[:, 1:] - t[:, :-1])\n  return v[:, :-1]"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input arguments are tensors\n  if not isinstance(t, torch.Tensor):\n    t = torch.tensor(t)\n  if not isinstance(tp, torch.Tensor):\n    tp = torch.tensor(tp)\n  if not isinstance(vp, torch.Tensor):\n    vp = torch.tensor(vp)\n\n  # Check if the input arguments are of the same shape\n  if t.shape != tp.shape or t.shape != vp.shape:\n    raise ValueError(\"Input arguments must have the same shape.\")\n\n  # Check if the input arguments are of the same device\n  if t.device != tp.device or t.device != vp.device:\n    raise ValueError(\"Input arguments must be on the same device.\")\n\n  # Check if the input arguments are on the same device\n  if t.device != tp.device or t.device != vp.device:\n    raise ValueError(\"Input arguments must be on the same device.\")\n\n  # Check if the input arguments are of the same dtype\n  if t.dtype != tp.dtype or t.dtype != vp.dtype:\n    raise ValueError(\"Input arguments must have the same dtype.\")\n\n  # Check if the input arguments are of the same dtype\n  if t.dtype != tp.dtype or t.dtype != vp.dtype:\n    raise ValueError(\"Input arguments must have the same dtype.\")\n\n  # Check if the input arguments are of the same dtype\n  if t.dtype != tp.dtype or t.dtype != vp.dtype:\n    raise ValueError(\"Input arguments must have the same dtype.\")\n\n  # Check if the input arguments are of the same dtype\n  if t.dtype != tp.dtype or t.dtype != vp.dtype:\n    raise ValueError(\"Input arguments must have the same dtype.\")\n\n  # Check if the input arguments are of the same dtype\n  if t.dtype !="}
{"namespace": "stepfun.resample", "completion": "  # Check if the input tensors have the same shape.\n  if tp.shape != vp.shape:\n    raise ValueError(\"The input tensors tp and vp must have the same shape.\")\n\n  # Check if the input tensors are 1D.\n  if len(tp.shape) != 1:\n    raise ValueError(\"The input tensors tp and vp must be 1D.\")\n\n  # Check if the input tensors are sorted in ascending order.\n  if tp[0] > tp[1]:\n    raise ValueError(\"The input tensors tp and vp must be sorted in ascending order.\")\n\n  # Check if the input tensors are of the same dtype.\n  if tp.dtype != vp.dtype:\n    raise ValueError(\"The input tensors tp and vp must have the same dtype.\")\n\n  # Check if the input tensors are of the same device.\n  if tp.device != vp.device:\n    raise ValueError(\"The input tensors tp and vp must be on the same device.\")\n\n  # Check if the input tensors are on the same device.\n  if t.device != tp.device:\n    raise ValueError(\"The input tensors t and tp must be on the same device.\")\n\n  # Check if the input tensors are on the same device.\n  if t.device != vp.device:\n    raise ValueError(\"The input tensors t and vp must be on the same device.\")\n\n  # Check if the input tensors are on the same device.\n  if t.dtype != tp.dtype:\n    raise ValueError(\"The input tensors t and tp must have the same dtype.\")\n\n  # Check if the input tensors are on the same device.\n  if t.dtype != vp.dtype:\n    raise ValueError(\"The input tensors t and vp must have the same dtype.\")\n\n  # Check if the input tensors are on the same device.\n  if t.device != vp.device:\n    raise ValueError(\"The"}
{"namespace": "stepfun.resample", "completion": "  t = t.reshape(-1, 1)\n  tp = tp.reshape(-1, 1)\n  vp = vp.reshape(-1, 1)\n  t_start = tp[:-1]\n  t_end = tp[1:]\n  v = vp[:-1]\n  t_start_new = t[:-1]\n  t_end_new = t[1:]\n  t_start_new = t_start_new.repeat_interleave(t_end - t_start, dim=0)\n  t_end_new = t_end_new.repeat_interleave(t_end - t_start, dim=0)\n  t_start_new = t_start_new.reshape(-1, 1)\n  t_end_new = t_end_new.reshape(-1, 1)\n  t_start_new = t_start_new.repeat_interleave(t_end - t_start, dim=0)\n  t_end_new = t_end_new.repeat_interleave(t_end - t_start, dim=0)\n  t_start_new = t_start_new.reshape(-1, 1)\n  t_end_new = t_end_new.reshape(-1, 1)\n  t_start_new = t_start_new.repeat_interleave(t_end - t_start, dim=0)\n  t_end_new = t_end_new.repeat_interleave(t_end - t_start, dim=0)\n  t_start_new = t_start_new.reshape(-1, 1)\n  t_end_new = t_end_new.reshape(-1, 1)\n  t_start_new = t_start_new.repeat_interleave(t_end - t_start, dim=0)\n  t_end_new = t_end_new.repeat_interleave(t_end - t_start, dim=0)\n  t_start_new = t_start_new.reshape"}
{"namespace": "stepfun.resample", "completion": "  tp = tp.clone()\n  vp = vp.clone()\n  tp = tp.view(-1, 1)\n  vp = vp.view(-1, 1)\n  t = t.view(-1, 1)\n  tp = torch.cat([tp, tp[-1:]], dim=0)\n  vp = torch.cat([vp, vp[-1:]], dim=0)\n  t_diff = tp[1:] - tp[:-1]\n  t_diff = t_diff.view(-1, 1)\n  t_diff_inv = 1.0 / t_diff\n  t_diff_inv = t_diff_inv.view(-1, 1)\n  t_diff_inv_cumsum = torch.cumsum(t_diff_inv, dim=0)\n  t_diff_inv_cumsum = t_diff_inv_cumsum.view(-1, 1)\n  t_diff_inv_cumsum_prev = torch.cat([torch.zeros(1, 1), t_diff_inv_cumsum[:-1]], dim=0)\n  t_diff_inv_cumsum_prev = t_diff_inv_cumsum_prev.view(-1, 1)\n  vp_diff = vp[1:] - vp[:-1]\n  vp_diff = vp_diff.view(-1, 1)\n  vp_diff_cumsum = torch.cumsum(vp_diff, dim=0)\n  vp_diff_cumsum = vp_diff_cumsum.view(-1, 1)\n  vp_diff_cumsum_prev = torch.cat([torch.zeros(1, 1), vp_diff_cumsum[:-1]], dim=0)\n  vp_diff_cumsum_prev = vp_diff_cumsum_prev.view(-1, 1)\n  t_diff_inv_cumsum_prev"}
{"namespace": "stepfun.resample", "completion": "  t = t.view(-1, 1)\n  tp = tp.view(-1, 1)\n  vp = vp.view(-1, 1)\n  t_left = t[:, 0]\n  t_right = t[:, 1]\n  tp_left = tp[:, 0]\n  tp_right = tp[:, 1]\n  vp_left = vp[:, 0]\n  vp_right = vp[:, 1]\n  if use_avg:\n    vp_left = vp_left * (t_right - tp_right) / (tp_right - tp_left)\n    vp_right = vp_right * (t_right - tp_left) / (tp_right - tp_left)\n  return (vp_left * (tp_right - t) / (tp_right - tp_left) +\n          vp_right * (t - tp_left) / (tp_right - tp_left))"}
{"namespace": "stepfun.resample", "completion": "  if use_avg:\n    vp = tf.repeat(vp, tf.reduce_sum(tf.cast(tf.less(t[1:], tp[1:]), tf.int32), axis=0))\n    tp = tf.repeat(tp, tf.reduce_sum(tf.cast(tf.less(t[1:], tp[1:]), tf.int32), axis=0))\n    t = tf.repeat(t, tf.reduce_sum(tf.cast(tf.less(t[1:], tp[1:]), tf.int32), axis=0))\n  else:\n    vp = tf.repeat(vp, tf.reduce_sum(tf.cast(tf.less(t[1:], tp[1:]), tf.int32), axis=0))\n    tp = tf.repeat(tp, tf.reduce_sum(tf.cast(tf.less(t[1:], tp[1:]), tf.int32), axis=0))\n    t = tf.repeat(t, tf.reduce_sum(tf.cast(tf.less(t[1:], tp[1:]), tf.int32), axis=0))\n  t = tf.sort(t)\n  tp = tf.sort(tp)\n  vp = tf.sort(vp)\n  t = tf.expand_dims(t, axis=-1)\n  tp = tf.expand_dims(tp, axis=0)\n  vp = tf.expand_dims(vp, axis=0)\n  vp = tf.where(tf.less(t, tp), vp, 0.)\n  vp = tf.reduce_sum(vp, axis=1)\n  return vp"}
{"namespace": "stepfun.resample", "completion": "  if use_avg:\n    return _resample_avg(t, tp, vp)\n  else:\n    return _resample_sum(t, tp, vp)\n\n"}
{"namespace": "stepfun.resample", "completion": "  if use_avg:\n    # Compute the widths of the intervals defined by t\n    widths = t[1:] - t[:-1]\n\n    # Compute the indices of the intervals in tp that overlap with each interval in t\n    left_indices = tp[:-1] < t[:-1]\n    right_indices = tp[1:] > t[1:]\n    indices = left_indices & right_indices\n\n    # Compute the average value of the step function for each interval in t\n    avg_vp = np.sum(vp[indices] * widths[indices], axis=0) / np.sum(widths[indices], axis=0)\n\n    # Return the average values of the step function for each interval in t\n    return avg_vp\n  else:\n    # Compute the indices of the intervals in tp that overlap with each interval in t\n    left_indices = tp[:-1] < t[:-1]\n    right_indices = tp[1:] > t[1:]\n    indices = left_indices & right_indices\n\n    # Compute the sum of the values of the step function for each interval in t\n    sum_vp = np.sum(vp[indices], axis=0)\n\n    # Return the sum of the values of the step function for each interval in t\n    return sum_vp"}
{"namespace": "stepfun.resample", "completion": "  t = t.clone()\n  tp = tp.clone()\n  vp = vp.clone()\n\n  t = t.reshape(-1)\n  tp = tp.reshape(-1)\n  vp = vp.reshape(-1)\n\n  t_diff = t[1:] - t[:-1]\n  tp_diff = tp[1:] - tp[:-1]\n  tp_diff_inv = 1. / tp_diff\n\n  t_diff_cumsum = torch.cumsum(t_diff, dim=0)\n  tp_diff_cumsum = torch.cumsum(tp_diff, dim=0)\n\n  t_diff_cumsum = t_diff_cumsum.reshape(-1, 1)\n  tp_diff_cumsum = tp_diff_cumsum.reshape(1, -1)\n\n  t_diff_cumsum_inv = 1. / t_diff_cumsum\n  tp_diff_cumsum_inv = 1. / tp_diff_cumsum\n\n  t_diff_cumsum_inv_diff = t_diff_cumsum_inv[1:] - t_diff_cumsum_inv[:-1]\n  tp_diff_cumsum_inv_diff = tp_diff_cumsum_inv[1:] - tp_diff_cumsum_inv[:-1]\n\n  t_diff_cumsum_inv_diff = t_diff_cumsum_inv_diff.reshape(-1, 1)\n  tp_diff_cumsum_inv_diff = tp_diff_cumsum_inv_diff.reshape(1, -1)\n\n  t_diff_cumsum_inv_diff_diff = t_diff_cumsum_inv_diff[1:] - t_diff_cumsum_inv_diff[:-1]\n  tp_diff_cumsum_inv_diff_diff = tp_diff_cumsum_"}
{"namespace": "stepfun.resample", "completion": "  t = t.reshape(-1)\n  tp = tp.reshape(-1)\n  vp = vp.reshape(-1)\n\n  # Sort the time points and values\n  idx = tp.argsort()\n  tp = tp[idx]\n  vp = vp[idx]\n\n  # Find the indices of the time points that are less than or equal to t\n  idx = tp.searchsorted(t)\n\n  # Calculate the difference between the current time point and the previous time point\n  delta = tp[idx] - tp[idx - 1]\n\n  # Calculate the value of the step function at the current time point\n  v = vp[idx - 1]\n\n  # If use_avg is True, calculate the average value of the step function for each interval\n  if use_avg:\n    v = v / delta\n\n  # Return the values of the step function at the new intervals defined by t\n  return v"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input arguments are valid\n  if len(tp.shape) != 1 or len(vp.shape) != 1:\n    raise ValueError(\"tp and vp must be 1D tensors.\")\n  if tp.shape[0] != vp.shape[0]:\n    raise ValueError(\"tp and vp must have the same length.\")\n  if len(t.shape) != 1:\n    raise ValueError(\"t must be a 1D tensor.\")\n\n  # Sort the time points and values in ascending order\n  tp, vp = tp.sort(), vp.sort()\n\n  # Initialize the output tensor\n  v = torch.zeros_like(t)\n\n  # Iterate over the new time points\n  for i in range(t.shape[0]):\n    # Find the indices of the time points in tp that are within the current interval\n    indices = torch.where((tp >= t[i]) & (tp <= t[i + 1]))[0]\n\n    # If there are no time points in tp within the current interval, set the corresponding value in v to 0\n    if indices.shape[0] == 0:\n      v[i] = 0\n    # Otherwise, sum or average the values of the step function within the current interval\n    else:\n      if use_avg:\n        v[i] = vp[indices].mean()\n      else:\n        v[i] = vp[indices].sum()\n\n  return v"}
{"namespace": "stepfun.resample", "completion": "  t = t.reshape(-1, 1)\n  tp = tp.reshape(-1, 1)\n  vp = vp.reshape(-1, 1)\n  t_repeat = t.repeat(tp.shape[0], 1)\n  tp_repeat = tp.repeat(t.shape[0], 1).transpose(1, 0)\n  vp_repeat = vp.repeat(t.shape[0], 1).transpose(1, 0)\n  if use_avg:\n    t_repeat = t_repeat.reshape(-1, 1, 2)\n    tp_repeat = tp_repeat.reshape(-1, 2, 1)\n    vp_repeat = vp_repeat.reshape(-1, 1, 2)\n    vp_repeat = vp_repeat * (tp_repeat[:, 1, :] - tp_repeat[:, 0, :])\n    vp_repeat = vp_repeat.sum(axis=1)\n    vp_repeat = vp_repeat / (tp_repeat[:, 1, :] - tp_repeat[:, 0, :])\n    vp_repeat = vp_repeat.reshape(-1, 1)\n  else:\n    t_repeat = t_repeat.reshape(-1, 1, 2)\n    tp_repeat = tp_repeat.reshape(-1, 2, 1)\n    vp_repeat = vp_repeat.reshape(-1, 1, 2)\n    vp_repeat = vp_repeat * ((tp_repeat[:, 1, :] >= t_repeat) & (t_repeat >= tp_repeat[:, 0, :])).astype(float)\n    vp_repeat = vp_repeat.sum(axis=1)\n    vp_repeat = vp_repeat.reshape(-1, 1)\n  return vp_repeat"}
{"namespace": "stepfun.resample", "completion": "  # Check if t is a 1D tensor\n  if len(t.shape) != 1:\n    raise ValueError(f'Input t should be a 1D tensor.')\n\n  # Check if tp and vp are 1D tensors\n  if len(tp.shape) != 1 or len(vp.shape) != 1:\n    raise ValueError(f'Input tp and vp should be 1D tensors.')\n\n  # Check if tp and vp have the same length\n  if tp.shape[0] != vp.shape[0]:\n    raise ValueError(f'Input tp and vp should have the same length.')\n\n  # Check if t is sorted\n  if not torch.all(torch.diff(t) > 0):\n    raise ValueError(f'Input t should be sorted in ascending order.')\n\n  # Check if tp is sorted\n  if not torch.all(torch.diff(tp) > 0):\n    raise ValueError(f'Input tp should be sorted in ascending order.')\n\n  # Check if tp and vp are the same device\n  if tp.device != vp.device:\n    raise ValueError(f'Input tp and vp should be on the same device.')\n\n  # Check if t and tp are the same device\n  if t.device != tp.device:\n    raise ValueError(f'Input t and tp should be on the same device.')\n\n  # Check if t and vp are on the same device\n  if t.device != vp.device:\n    raise ValueError(f'Input t and vp should be on the same device.')\n\n  # Check if tp and vp are on the same dtype\n  if tp.dtype != vp.dtype:\n    raise ValueError(f'Input tp and vp should be on the same dtype.')\n\n  # Check if t and tp are on the same dtype\n  if t.dtype != tp.dtype:\n    raise ValueError("}
{"namespace": "stepfun.resample", "completion": "  t = t.view(-1)\n  tp = tp.view(-1)\n  vp = vp.view(-1)\n\n  if use_avg:\n    # Calculate the width of each interval in t\n    widths = t[1:] - t[:-1]\n\n    # Calculate the average value of the step function for each interval in t\n    avg_vp = (vp[1:] + vp[:-1]) / 2.0\n\n    # Calculate the cumulative sum of the average values of the step function\n    avg_vp_cumsum = torch.cumsum(avg_vp, dim=0)\n\n    # Calculate the cumulative sum of the widths of the intervals\n    widths_cumsum = torch.cumsum(widths, dim=0)\n\n    # Calculate the resampled values of the step function\n    v = avg_vp_cumsum / widths_cumsum\n\n    # Add the first value of the step function to the resampled values\n    v = torch.cat([vp[0].view(1), v])\n\n    # Calculate the resampled values of the step function at the new intervals defined by t\n    v = v[torch.searchsorted(t, tp, right=True) - 1]\n  else:\n    # Calculate the cumulative sum of the values of the step function\n    vp_cumsum = torch.cumsum(vp, dim=0)\n\n    # Calculate the resampled values of the step function\n    v = vp_cumsum[torch.searchsorted(t, tp, right=True) - 1]\n\n  return v"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input tensors are of the correct shape\n  if t.ndim != 1:\n    raise ValueError(\"t must be a 1D tensor\")\n  if tp.ndim != 1:\n    raise ValueError(\"tp must be a 1D tensor\")\n  if vp.ndim != 1:\n    raise ValueError(\"vp must be a 1D tensor\")\n  if tp.shape[0] != vp.shape[0]:\n    raise ValueError(\"tp and vp must have the same length\")\n\n  # Check if the time points are sorted\n  if not torch.all(torch.diff(tp) >= 0):\n    raise ValueError(\"tp must be sorted\")\n\n  # Check if the time points are within the range of t\n  if not torch.all(t >= tp[0]) or not torch.all(t <= tp[-1]):\n    raise ValueError(\"t must be within the range of tp\")\n\n  # Create a new tensor to store the resampled values\n  vr = torch.zeros_like(t)\n\n  # Loop over the intervals in t\n  for i in range(t.shape[0]):\n    # Find the indices of the original time points that are within the current interval\n    idx = torch.where((tp >= t[i]) & (tp <= t[i + 1]))[0]\n\n    # If there are no original time points within the current interval, set the resampled value to 0\n    if idx.shape[0] == 0:\n      vr[i] = 0\n    else:\n      # If use_avg is True, compute the average value of the original time points within the current interval\n      if use_avg:\n        vr[i] = torch.mean(vp[idx])\n      # Otherwise, sum the values of the original time points within the current interval\n      else:\n        vr[i] = torch.sum(vp[idx])\n\n  # Return the resampled values\n  return vr\n\n"}
{"namespace": "stepfun.resample", "completion": "  ## Check if the time points are sorted\n  if tp.shape[0] > 1:\n    assert torch.all(tp[1:] >= tp[:-1])\n\n  ## Check if the time points are within the range of t\n  assert torch.all(tp >= t[0]) and torch.all(tp <= t[-1])\n\n  ## Check if the time points are within the range of t\n  assert torch.all(t >= tp[0]) and torch.all(t <= tp[-1])\n\n  ## Compute the cumulative sum of the values at the time points\n  cs = torch.cat((torch.zeros(1).to(vp.device), torch.cumsum(vp, dim=0)))\n\n  ## Compute the width of each interval in t\n  width = t[1:] - t[:-1]\n\n  ## Compute the cumulative sum of the widths of the intervals in t\n  cs_width = torch.cat((torch.zeros(1).to(width.device), torch.cumsum(width, dim=0)))\n\n  ## Compute the indices of the time points that are closest to the new time points in t\n  idx = torch.searchsorted(tp, t, right=True) - 1\n\n  ## Compute the values of the resampled step function at the new time points in t\n  if use_avg:\n    return (cs[idx + 1] - cs[idx]) / (cs_width[idx + 1] - cs_width[idx])\n  else:\n    return cs[idx + 1] - cs[idx]\n\n"}
{"namespace": "stepfun.resample", "completion": "  # sort the time points and values\n  tp, vp = tp[tf.argsort(tp)], vp[tf.argsort(tp)]\n\n  # compute the width of the intervals\n  dt = t[1:] - t[:-1]\n\n  # compute the indices of the time points that are within each interval\n  idx = tf.searchsorted(tp, t, side='right') - 1\n\n  # compute the values of the step function at the new time points\n  if use_avg:\n    v = tf.gather(vp, idx) * dt / tf.gather(tp[1:] - tp[:-1], idx)\n  else:\n    v = tf.gather(vp, idx) * dt\n\n  # return the values of the resampled step function at the new time points\n  return v\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  mean_scaled = mean / (2 ** min_deg)\n  var_scaled = var / (2 ** min_deg)\n\n  # Concatenate the scaled mean and variance\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply the sinusoidal encoding\n  x = jnp.concatenate([jnp.sin(2 ** i * jnp.pi * x) for i in range(min_deg, max_deg)], axis=-1)\n\n  return x"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  scaled_mean = mean * (2 ** jnp.arange(min_deg, max_deg))\n  scaled_var = var * (2 ** jnp.arange(min_deg, max_deg))\n\n  # Concatenate the scaled mean and variance\n  scaled_concat = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply the sinusoidal encoding\n  encoded_vars = jnp.sin(scaled_concat)\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  mean = mean * 2 ** jnp.arange(min_deg, max_deg)\n  var = var * 2 ** jnp.arange(min_deg, max_deg)\n\n  # Concatenate the scaled mean and variance\n  x = jnp.concatenate([mean, var], axis=-1)\n\n  # Apply the sinusoidal encoding\n  return jnp.sin(x[..., None] * jnp.arange(1, x.shape[-1] + 1))\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  mean_scaled = mean * 2 ** jnp.arange(min_deg, max_deg)\n  var_scaled = var * 2 ** jnp.arange(min_deg, max_deg)\n\n  # Concatenate the scaled mean and variance\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply sinusoidal encoding\n  return jnp.sin(x[..., None] * jnp.arange(1, x.shape[-1] + 1))\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance of the coordinates\n  scaled_mean = mean / (2.0 ** jnp.arange(min_deg, max_deg))\n  scaled_var = var / (2.0 ** jnp.arange(min_deg, max_deg))\n\n  # Concatenate the scaled mean and variance\n  scaled_mean_var = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply sinusoidal encoding\n  encoded_vars = jnp.concatenate([scaled_mean_var, jnp.sin(scaled_mean_var), jnp.cos(scaled_mean_var)], axis=-1)\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  scale = jnp.log2(var)\n  scale = jnp.maximum(scale, min_deg)\n  scale = jnp.minimum(scale, max_deg - 1)\n  scale = jnp.round(scale)\n  scale = 2 ** scale\n  mean = mean / scale\n\n  # Concatenate the mean and variance\n  x = jnp.concatenate([mean, var], axis=-1)\n\n  # Apply the sinusoidal encoding\n  x = jnp.concatenate([jnp.sin(2 ** i * x) for i in range(min_deg, max_deg)], axis=-1)\n\n  return x"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scaling the mean and variance\n  mean_scaled = mean / jnp.exp2(jnp.arange(min_deg, max_deg))\n  var_scaled = var / jnp.exp2(jnp.arange(min_deg, max_deg))\n\n  # Concatenating the scaled mean and variance\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Applying sinusoidal encoding\n  encoding = jnp.concatenate([jnp.sin(jnp.pi * x), jnp.cos(jnp.pi * x)], axis=-1)\n  return encoding\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scale = 2 ** jnp.arange(min_deg, max_deg)\n  scale = jnp.expand_dims(scale, 1)\n  scale = jnp.expand_dims(scale, 1)\n  scale = jnp.expand_dims(scale, 0)\n  scale = jnp.expand_dims(scale, 0)\n  scale = jnp.repeat(scale, mean.shape[0], axis=0)\n  scale = jnp.repeat(scale, mean.shape[1], axis=1)\n  scale = jnp.repeat(scale, mean.shape[2], axis=2)\n  scale = jnp.repeat(scale, mean.shape[3], axis=3)\n  scale = jnp.repeat(scale, mean.shape[4], axis=4)\n  scale = jnp.repeat(scale, mean.shape[5], axis=5)\n\n  mean = mean * scale\n  var = var * scale\n  return jnp.concatenate([mean, var], axis=-1)"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  mean = mean / 2.0\n  var = var / 2.0\n\n  # Concatenate the mean and variance\n  x = jnp.concatenate([mean, var], axis=-1)\n\n  # Apply the sinusoidal encoding\n  degs = jnp.arange(min_deg, max_deg)\n  x = x[..., None] * (2.0 ** degs)[None, None, :] * jnp.pi\n  x = jnp.concatenate([jnp.sin(x), jnp.cos(x)], axis=-1)\n\n  return x\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scaling the mean and variance\n  mean = mean / 2.0\n  var = var / 2.0\n\n  # Concatenating the mean and variance\n  x = jnp.concatenate([mean, var], axis=-1)\n\n  # Applying the sinusoidal encoding\n  x_enc = jnp.concatenate([x, jnp.sin(x), jnp.cos(x)], axis=-1)\n\n  # Applying the encoding scale\n  for i in range(min_deg, max_deg):\n    x_enc = jnp.concatenate([x_enc, jnp.sin((2 ** i) * x), jnp.cos((2 ** i) * x)], axis=-1)\n\n  return x_enc"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scale = 2.0 ** jnp.arange(min_deg, max_deg, dtype=jnp.float32)\n  scale = jnp.expand_dims(scale, -1)\n  scale = jnp.expand_dims(scale, -1)\n\n  # scale the mean and variance\n  mean = mean * scale\n  var = var * scale\n\n  # concatenate the mean and variance\n  x = jnp.concatenate([mean, var], axis=-1)\n\n  # apply sinusoidal encoding\n  x = jnp.concatenate([jnp.sin(x[..., ::2]), jnp.cos(x[..., 1::2])], axis=-1)\n\n  return x\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  mean_scaled = mean / jnp.exp2(jnp.arange(min_deg, max_deg))\n  var_scaled = var / jnp.exp2(jnp.arange(min_deg, max_deg))\n\n  # Concatenate the scaled mean and variance\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply sinusoidal encoding\n  encoding = jnp.sin(jnp.pi * x)\n\n  return encoding\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance of the coordinates\n  scaled_mean = mean / (2.0 ** jnp.arange(min_deg, max_deg))\n  scaled_var = var / (2.0 ** jnp.arange(min_deg, max_deg))\n\n  # Concatenate the scaled mean and variance\n  scaled_concat = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply the sinusoidal encoding\n  encoded_vars = jnp.sin(scaled_concat[..., None] * jnp.arange(1, scaled_concat.shape[-1] + 1))\n\n  return encoded_vars\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scale = 2 ** jnp.arange(min_deg, max_deg)\n  encoding = jnp.concatenate([jnp.sin(scale[:, None] * mean[None, :]),\n                              jnp.sin(scale[:, None] * var[None, :])], axis=0)\n  return encoding\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scale = jnp.log2(var)\n  scale = jnp.minimum(scale, jnp.ones_like(scale) * max_deg)\n  scale = jnp.maximum(scale, jnp.ones_like(scale) * min_deg)\n  scale = jnp.exp2(scale)\n  scale = jnp.expand_dims(scale, -1)\n  mean = mean * scale\n  var = var * scale\n  return sinusoidal_encoding(mean, var)\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  scale = jnp.log2(var) / (max_deg - min_deg)\n  scaled_mean = mean / jnp.exp(scale * max_deg)\n  scaled_var = var / jnp.exp(2 * scale * max_deg)\n\n  # Concatenate the scaled mean and variance\n  x = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply sinusoidal encoding\n  encoding = jnp.sin(jnp.arange(min_deg, max_deg + 1) * x[..., None])\n\n  return encoding\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scaling the mean and variance\n  scale = jnp.log2(var) ** 0.5\n  mean = mean * scale\n  var = var * scale ** 2\n\n  # Concatenating the mean and variance\n  x = jnp.concatenate([mean, var], axis=-1)\n\n  # Applying the sinusoidal encoding\n  out = jnp.concatenate([x[:, :1] * jnp.ones_like(x[:, 1:]),\n                         jnp.sin(jnp.pi * x[:, 1:]),\n                         jnp.cos(jnp.pi * x[:, 1:])], axis=-1)\n\n  # Applying the encoding scale\n  out = out * 2 ** jnp.arange(min_deg, max_deg)\n\n  return out\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  mean = mean / 2.0\n  var = var / 2.0\n\n  # Concatenate the mean and variance\n  x = jnp.concatenate([mean, var], axis=-1)\n\n  # Apply a sinusoidal encoding\n  x = sinusoidal_encoding(x, min_deg, max_deg)\n\n  return x\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scaling the mean and variance\n  mean = mean * jnp.exp(0.5 * var)\n  var = var * jnp.exp(var)\n\n  # Concatenating the mean and variance\n  x = jnp.concatenate([mean, var], axis=-1)\n\n  # Applying the sinusoidal encoding\n  x = sinusoidal_encoding(x, min_deg, max_deg)\n\n  return x\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scaling the mean and variance\n  scale = jnp.log2(var)\n  scale = jnp.maximum(scale, min_deg)\n  scale = jnp.minimum(scale, max_deg - 1)\n  scale = jnp.round(scale)\n  scale = jnp.exp2(scale)\n\n  # Concatenating the mean and variance\n  x = jnp.concatenate([mean, var], axis=-1)\n\n  # Applying the sinusoidal encoding\n  x = x / scale\n  x = 100.0 * jnp.concatenate([jnp.sin(x[..., ::2]), jnp.cos(x[..., 1::2])], axis=-1)\n\n  return x\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Generate the integrated directional encoding function\n  dir_enc_fn = generate_dir_enc_int_fn(deg_view)\n\n  # Define the function that takes a 3D point (or points) as input and returns its directional encoding\n  def dir_enc_fn_3d(pts):\n    return dir_enc_fn(pts[:, 0], pts[:, 1], pts[:, 2])\n\n  return dir_enc_fn_3d\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  def dir_enc_fn(pts):\n\n    \"\"\"\n    Evaluates the directional encoding for a given input point(s). This function internally uses a generated integrated directional encoding function with the specified degree of spherical harmonics.\n\n    Input-Output Arguments\n    :param pts: Numpy array. The input point(s) to evaluate the directional encoding for. It can be either a single point or an array of points.\n    :return: Numpy array. The directional encoding(s) for the input point(s).\n\n    \"\"\"\n\n    from .integrated_dir_enc_fn import integrated_dir_enc_fn\n\n    return integrated_dir_enc_fn(pts, deg_view)\n\n  return dir_enc_fn\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  from . import generate_dir_enc_fn_integrated\n  return generate_dir_enc_fn_integrated(deg_view)\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Generate directional encoding function\n  def dir_enc_fn(points):\n\n    # Import modules\n    from .integrated_directional_encoding import integrated_directional_encoding\n\n    # Generate integrated directional encoding function\n    id_enc_fn = integrated_directional_encoding(deg_view)\n\n    # Evaluate integrated directional encoding function\n    id_enc = id_enc_fn(points)\n\n    # Return directional encoding\n    return id_enc\n\n  # Return directional encoding function\n  return dir_enc_fn\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Generate directional encoding function\n  def dir_enc_fn(points):\n\n    # Import libraries\n    from .. import spherical_harmonics as sh\n    from .. import directional_encoding as de\n\n    # Generate directional encoding\n    dir_enc = de.directional_encoding(points, sh.sh_fn, deg_view)\n\n    # Return directional encoding\n    return dir_enc\n\n  # Return directional encoding function\n  return dir_enc_fn"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Importing the necessary modules\n  from ..utils.sh import get_sh_integrated_enc_fn\n\n  # Generating the integrated directional encoding function\n  integrated_enc_fn = get_sh_integrated_enc_fn(deg_view)\n\n  # Creating a function that takes a 3D point (or points) as input and returns its directional encoding\n  def dir_enc_fn(pts):\n    return integrated_enc_fn(pts)\n\n  # Returning the directional encoding function\n  return dir_enc_fn"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # imports\n  import numpy as np\n  from scipy.special import sph_harm\n\n  # generate directional encoding function\n  def dir_enc_fn(dir_vec):\n\n    # get directional encoding\n    dir_enc = np.zeros((dir_vec.shape[0], 2 * deg_view + 1))\n    for i in range(dir_vec.shape[0]):\n      for l in range(deg_view + 1):\n        for m in range(-l, l + 1):\n          dir_enc[i, l + m] = np.real(sph_harm(m, l, np.arctan2(dir_vec[i, 1], dir_vec[i, 0]), np.arccos(dir_vec[i, 2])))\n\n    # return directional encoding\n    return dir_enc\n\n  # return directional encoding function\n  return dir_enc_fn\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Generate a directional encoding function based on the specified number of spherical harmonics degrees\n  def dir_enc_fn(points):\n\n    # Import the required modules\n    from .. import utils\n    import numpy as np\n\n    # Compute the directional encoding using the generated integrated directional encoding function\n    dir_enc = utils.integrated_dir_enc_fn(deg_view)(points)\n\n    # Return the directional encoding\n    return dir_enc\n\n  # Return the directional encoding function\n  return dir_enc_fn\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  import numpy as np\n  from .generate_dir_enc import generate_dir_enc\n\n  # Generate the integrated directional encoding function\n  dir_enc_fn = generate_dir_enc(deg_view)\n\n  # Define the function that takes a 3D point as input and returns its directional encoding\n  def dir_enc_fn_3d(points):\n\n    # Check if the input is a single point or multiple points\n    if len(points.shape) == 1:\n      points = points[np.newaxis, :]\n\n    # Compute the directional encoding for each point\n    dir_enc = np.zeros((points.shape[0], dir_enc_fn.shape[0]))\n    for i in range(points.shape[0]):\n      dir_enc[i, :] = dir_enc_fn(points[i, :])\n\n    return dir_enc\n\n  return dir_enc_fn_3d\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  def dir_enc_fn(pts):\n\n    \"\"\"\n    Evaluates the directional encoding for a given set of points.\n\n    Input-Output Arguments\n    :param pts: Numpy array. The input points to evaluate the directional encoding for. It can be a single point or a set of points.\n    :return: Numpy array. The directional encoding for the input points.\n\n    \"\"\"\n\n    # Import the necessary modules\n    from . import dir_enc_fn_generator\n    from . import dir_enc_fn_evaluator\n\n    # Generate the directional encoding function\n    dir_enc_fn = dir_enc_fn_generator(deg_view)\n\n    # Evaluate the directional encoding function\n    dir_enc = dir_enc_fn_evaluator(dir_enc_fn, pts)\n\n    # Return the directional encoding\n    return dir_enc\n\n  # Return the directional encoding function\n  return dir_enc_fn\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Import required libraries\n  import numpy as np\n  from ..utils.integrate_dir_enc import integrate_dir_enc\n\n  # Generate the integrated directional encoding function with the specified degree of spherical harmonics\n  dir_enc_fn = integrate_dir_enc(deg_view)\n\n  # Define a function that takes a 3D point (or points) as input and returns its directional encoding\n  def dir_enc_fn_3D(points):\n\n    # Convert the input points to a NumPy array\n    points = np.array(points)\n\n    # Check if the input points are 3D points\n    if points.shape[-1] != 3:\n      raise ValueError(\"Input points must be 3D points.\")\n\n    # Calculate the directional encoding for each point\n    encodings = np.array([dir_enc_fn(point) for point in points])\n\n    # Return the directional encodings\n    return encodings\n\n  # Return the directional encoding function for 3D points\n  return dir_enc_fn_3D\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Import required packages\n  import numpy as np\n  import math\n  import scipy.special as sp\n\n  # Generate the integrated directional encoding function\n  def dir_enc_fn(pts):\n\n    # Convert the input points to numpy array\n    pts = np.array(pts)\n\n    # Initialize the directional encoding\n    dir_enc = np.zeros((pts.shape[0], 3 * (deg_view + 1) * (deg_view + 2) // 2))\n\n    # Iterate over the points and compute the directional encoding\n    for i in range(pts.shape[0]):\n\n      # Extract the point\n      pt = pts[i]\n\n      # Compute the spherical coordinates\n      r = np.linalg.norm(pt)\n      theta = math.acos(pt[2] / r)\n      phi = math.atan2(pt[1], pt[0])\n\n      # Compute the directional encoding\n      for l in range(deg_view + 1):\n        for m in range(-l, l + 1):\n          idx = l * (l + 1) + m\n          dir_enc[i, idx] = sp.sph_harm(m, l, phi, theta).real\n\n    return dir_enc\n\n  # Return the directional encoding function\n  return dir_enc_fn\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  def dir_enc_fn(points):\n    \"\"\"\n    Evaluates the directional encoding for the given input points.\n\n    Input-Output Arguments\n    :param points: 3D points (or points) to evaluate the directional encoding for.\n    :return: The directional encoding for the given points.\n\n    \"\"\"\n\n    from .spherical_harmonics import spherical_harmonics_dir\n    from .spherical_harmonics import spherical_harmonics_integrate\n    from .spherical_harmonics import spherical_harmonics_normalize\n\n    # Evaluate spherical harmonics for the given points\n    sh = spherical_harmonics_dir(points, deg_view)\n\n    # Integrate spherical harmonics to get the integrated directional encoding\n    dir_enc = spherical_harmonics_integrate(sh, deg_view)\n\n    # Normalize the integrated directional encoding\n    dir_enc = spherical_harmonics_normalize(dir_enc, deg_view)\n\n    return dir_enc\n\n  return dir_enc_fn"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  def dir_enc_fn(points):\n\n    \"\"\"\n    Evaluates the directional encoding for given input points. This function internally uses a generated integrated directional encoding function with the specified degree of spherical harmonics.\n\n    Input-Output Arguments\n    :param points: Numpy array. The 3D points (or points) to evaluate the directional encoding for.\n    :return: Numpy array. The directional encoding for the input points.\n\n    \"\"\"\n\n    from ..encodings.integrated_dir_enc import integrated_dir_enc\n    from ..encodings.integrated_dir_enc import integrated_dir_enc_fn\n    from ..encodings.integrated_dir_enc import integrated_dir_enc_fn_spherical\n\n    if deg_view % 2 == 0:\n      dir_enc_fn_spherical = integrated_dir_enc_fn_spherical(deg_view)\n    else:\n      dir_enc_fn_spherical = integrated_dir_enc_fn(deg_view)\n\n    dir_enc = dir_enc_fn_spherical(points)\n\n    return dir_enc\n\n  return dir_enc_fn\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  import numpy as np\n  import scipy.special\n  from sklearn.preprocessing import normalize\n\n  def dir_enc_fn(pts):\n\n    \"\"\"\n    Evaluates the directional encoding for the given input points. This function internally uses a generated integrated directional encoding function with the specified degree of spherical harmonics.\n\n    Input-Output Arguments\n    :param pts: Numpy array. The input points to evaluate the directional encoding for. It can be a single point or multiple points.\n    :return: Numpy array. The directional encoding for the input points.\n\n    \"\"\"\n\n    pts = np.array(pts)\n    pts = normalize(pts)\n\n    # Generate the integrated directional encoding function\n    def dir_enc_fn_int(pts):\n\n      \"\"\"\n      Evaluates the integrated directional encoding function for the given input points. This function internally uses the specified degree of spherical harmonics to generate the integrated directional encoding function.\n\n      Input-Output Arguments\n      :param pts: Numpy array. The input points to evaluate the integrated directional encoding function for. It can be a single point or multiple points.\n      :return: Numpy array. The integrated directional encoding function for the input points.\n\n      \"\"\"\n\n      pts = np.array(pts)\n      pts = normalize(pts)\n\n      # Generate the spherical harmonics coefficients\n      coeffs = np.zeros((pts.shape[0], deg_view * deg_view))\n      for i in range(deg_view):\n        for j in range(deg_view):\n          coeffs[:, i * deg_view + j] = scipy.special.sph_harm(j, i, np.arctan2(pts[:, 1], pts[:, 0]), np.arccos(pts[:, 2])).real\n\n      return coeffs\n\n    # Evaluate the integrated directional encoding function\n    dir_enc = dir_enc_fn_int(pts)\n\n    return dir_enc"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Importing necessary libraries\n  import numpy as np\n  from .sh_encoder import sh_encoder\n\n  # Generating integrated directional encoding function\n  dir_enc_fn = sh_encoder(deg_view)\n\n  # Generating directional encoding function\n  def dir_enc_fn_gen(points):\n\n    # Extracting coordinates\n    x = points[:, 0]\n    y = points[:, 1]\n    z = points[:, 2]\n\n    # Evaluating directional encoding\n    dir_enc = dir_enc_fn(x, y, z)\n\n    return dir_enc\n\n  return dir_enc_fn_gen\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Generate directional encoding function\n  def directional_encoding_fn(view_dir):\n\n    # Import required packages\n    from pytorch3d.ops import dir_encoder\n    import torch\n\n    # Generate directional encoding\n    view_dir_enc = dir_encoder.encode_dir(view_dir, deg_view)\n\n    # Return directional encoding\n    return view_dir_enc\n\n  # Return directional encoding function\n  return directional_encoding_fn\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Generate the integrated directional encoding function\n  dir_enc_fn = generate_dir_enc_fn_integrated(deg_view)\n\n  # Define a function that takes a 3D point (or points) as input and returns its directional encoding\n  def dir_enc_fn_3d(points):\n\n    # Evaluate the directional encoding function for the given points\n    return dir_enc_fn(points)\n\n  # Return the directional encoding function\n  return dir_enc_fn_3d\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Import required libraries\n  import numpy as np\n  import torch\n  from torch.autograd import Variable\n  from scipy.special import sph_harm\n\n  # Define a function to generate the directional encoding function\n  def generate_dir_enc_fn_int(deg_view):\n\n    # Define the function to generate the directional encoding function\n    def dir_enc_fn(p):\n\n      # Convert the input to a PyTorch tensor\n      p = torch.from_numpy(p).float()\n\n      # Check if the input is a single point or a batch of points\n      if len(p.shape) == 1:\n        p = p.unsqueeze(0)\n\n      # Create a PyTorch variable for the input\n      p = Variable(p)\n\n      # Initialize the output tensor\n      p_enc = torch.zeros(p.shape[0], (deg_view + 1) ** 2)\n\n      # Loop over the degrees of spherical harmonics\n      for l in range(deg_view + 1):\n        for m in range(-l, l + 1):\n\n          # Compute the spherical harmonic function\n          Y_lm = sph_harm(m, l, p[:, 1], p[:, 2])\n\n          # Compute the directional encoding\n          p_enc[:, l ** 2 + l + m] = torch.cos(l * p[:, 0]) * Y_lm.real - torch.sin(l * p[:, 0]) * Y_lm.imag\n\n      # Return the directional encoding\n      return p_enc.numpy()\n\n    # Return the directional encoding function\n    return dir_enc_fn\n\n  # Return the directional encoding function\n  return generate_dir_enc_fn_int(deg_view)\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Import required packages\n  import numpy as np\n  import pyvista as pv\n  from pyvista import examples\n  import pyvistaqt as pvqt\n\n  # Create a sphere with a radius of 1\n  sphere = pv.Sphere(radius=1)\n\n  # Define the number of samples for the sphere\n  samples = 10000\n\n  # Sample the sphere using the specified number of samples\n  points = sphere.sample(samples)\n\n  # Get the points as a NumPy array\n  points_array = points.points\n\n  # Normalize the points to have a mean of 0 and a standard deviation of 1\n  points_array = (points_array - np.mean(points_array, axis=0)) / np.std(points_array)\n\n  # Generate the directional encoding function\n  def dir_enc_fn(points):\n\n    # Normalize the points to have a mean of 0 and a standard deviation of 1\n    points = (points - np.mean(points, axis=0)) / np.std(points)\n\n    # Calculate the directional encoding\n    dir_enc = np.zeros((len(points), deg_view * deg_view))\n    for i in range(deg_view):\n      for j in range(deg_view):\n        if i + j <= deg_view - 1:\n          dir_enc[:, deg_view * i + j] = np.sum(points * np.sqrt(4 * np.pi) * np.sqrt(2 * i + 1) * np.sqrt(2 * j + 1) * np.sqrt(2) * np.cos(np.pi * (i + 0.5) * points[:, 0]) * np.cos(np.pi * (j + 0.5) * points[:, 1]), axis=1)\n\n    return dir_enc\n\n  # Return the directional encoding function\n  return dir_enc_fn"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    current_block = None\n    current_index = None\n    current_header = None\n    current_list = None\n    current_list_level = None\n    current_list_index = None\n    current_list_item = None\n    current_list_item_index = None\n    current_list_item_level = None\n    current_list_item_text = None\n    current_list_item_type = None\n    current_list_item_start = None\n    current_list_item_end = None\n    current_list_item_blocks = None\n    current_list_item_header = None\n    current_list_item_indent = None\n    current_list_item_list = None\n    current_list_item_list_level = None\n    current_list_item_list_index = None\n    current_list_item_list_item = None\n    current_list_item_list_item_index = None\n    current_list_item_list_item_level = None\n    current_list_item_list_item_text = None\n    current_list_item_list_item_type = None\n    current_list_item_list_item_start = None\n    current_list_item_list_item_end = None\n    current_list_item_list_item_blocks = None\n    current_list_item_list_item_header = None\n    current_list_item_list_item_indent = None\n    current_list_item_list_item_list = None\n    current_list_item_list_item_list_level = None\n    current_list_item_list_item_list_index = None\n    current_list_item_list_item_list_item = None\n    current_list_item_list_item_list_item_index = None\n    current_list_item_list_item_list_item_level = None\n    current_list_item_list_item_list_item_text = None\n    current_list_item_list_item_list_item_type = None\n    current_list_item"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    blocks = []\n    prev_block = None\n    prev_block_type = None\n    prev_block_indent = None\n    prev_block_list = None\n    prev_block_list_index = None\n    prev_block_list_level = None\n    prev_block_list_item = None\n    prev_block_list_item_index = None\n    prev_block_list_item_level = None\n    prev_block_list_item_text = None\n    prev_block_list_item_text_index = None\n    prev_block_list_item_text_level = None\n    prev_block_list_item_text_list = None\n    prev_block_list_item_text_list_index = None\n    prev_block_list_item_text_list_level = None\n    prev_block_list_item_text_list_item = None\n    prev_block_list_item_text_list_item_index = None\n    prev_block_list_item_text_list_item_level = None\n    prev_block_list_item_text_list_item_text = None\n    prev_block_list_item_text_list_item_text_index = None\n    prev_block_list_item_text_list_item_text_level = None\n    prev_block_list_item_text_list_item_text_list = None\n    prev_block_list_item_text_list_item_text_list_index = None\n    prev_block_list_item_text_list_item_text_list_level = None\n    prev_block_list_item_text_list_item_text_list_item = None\n    prev_block_list_item_text_list_item_text_list_item_index = None\n    prev_block_list_item_text_list_item_text_list_item_level = None\n    prev_block_list_item_text_list_item_text_list_item_text = None\n    prev_block_list_item_text_list_item_text_list_item_"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    blocks = []\n    blocks_num = []\n    blocks_num_level = []\n    blocks_num_level_start = []\n    blocks_num_level_end = []\n    blocks_num_level_start_end = []\n    blocks_num_level_start_end_num = []\n    blocks_num_level_start_end_num_level = []\n    blocks_num_level_start_end_num_level_start = []\n    blocks_num_level_start_end_num_level_start_end = []\n    blocks_num_level_start_end_num_level_start_end_num = []\n    blocks_num_level_start_end_num_level_start_end_num_level = []\n    blocks_num_level_start_end_num_level_start_end_num_level_start = []\n    blocks_num_level_start_end_num_level_start_end_num_level_start_end = []\n    blocks_num_level_start_end_num_level_start_end_num_level_start_end_num = []\n    blocks_num_level_start_end_num_level_start_end_num_level_start_end_num_level = []\n    blocks_num_level_start_end_num_level_start_end_num_level_start_end_num_level_start = []\n    blocks_num_level_start_end_num_level_start_end_num_level_start_end_num_level_start_end = []\n    blocks_num_level_start_end_num_level_start_end_num_level_start_end_num_level_start_end_num = []\n    blocks_num_level_start_end_num_level_start_end_num_level_start_end_num_level_start_end_num_level = []\n    blocks_num_level_start_end_num_level_start_end_num_level_start_end_num_level_start_end_num_level_start = []\n    blocks"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    current_block = None\n    current_header = None\n    current_list = None\n    current_list_level = None\n    current_list_item = None\n    current_list_item_level = None\n    current_list_item_index = None\n    current_list_item_index_list = None\n    current_list_item_index_list_level = None\n    current_list_item_index_list_level_list = None\n    current_list_item_index_list_level_list_list = None\n    current_list_item_index_list_level_list_list_list = None\n    current_list_item_index_list_level_list_list_list_list = None\n    current_list_item_index_list_level_list_list_list_list_list = None\n    current_list_item_index_list_level_list_list_list_list_list_list = None\n    current_list_item_index_list_level_list_list_list_list_list_list_list = None\n    current_list_item_index_list_level_list_list_list_list_list_list_list_list = None\n    current_list_item_index_list_level_list_list_list_list_list_list_list_list_list = None\n    current_list_item_index_list_level_list_list_list_list_list_list_list_list_list_list = None\n    current_list_item_index_list_level_list_list_list_list_list_list_list_list_list_list_list = None\n    current_list_item_index_list_level_list_list_list_list_list_list_list_list_list_list_list_list = None\n    current_list_item_index_list_level_list_list_list_list_list_list_list_list_list_list_list_list_list = None\n    current_list_item_index_list_level_list_list_list_list_"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    current_block = []\n    current_index = 0\n    current_header = None\n    current_list = None\n    current_list_level = None\n    current_list_item = None\n    current_list_item_level = None\n    current_list_item_index = None\n    current_list_item_text = None\n    current_list_item_text_level = None\n    current_list_item_text_index = None\n    current_list_item_text_type = None\n    current_list_item_text_start = None\n    current_list_item_text_end = None\n    current_list_item_text_indent = None\n    current_list_item_text_indent_level = None\n    current_list_item_text_indent_index = None\n    current_list_item_text_indent_type = None\n    current_list_item_text_indent_start = None\n    current_list_item_text_indent_end = None\n    current_list_item_text_indent_indent = None\n    current_list_item_text_indent_indent_level = None\n    current_list_item_text_indent_indent_index = None\n    current_list_item_text_indent_indent_type = None\n    current_list_item_text_indent_indent_start = None\n    current_list_item_text_indent_indent_end = None\n    current_list_item_text_indent_indent_indent = None\n    current_list_item_text_indent_indent_indent_level = None\n    current_list_item_text_indent_indent_indent_index = None\n    current_list_item_text_indent_indent_indent_type = None\n    current_list_item_text_indent_indent_indent_start = None\n    current_list_item_text_indent_indent_indent_end = None\n    current_list_item_text_indent_indent_indent_indent = None\n    current_list_item_text_indent_indent_indent_indent_level = None\n    current_"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    blocks = []\n    block_index = 0\n    header_index = -1\n    header_level = -1\n    list_level = -1\n    list_index = -1\n    list_start = -1\n    list_end = -1\n    list_items = []\n    list_item_index = 0\n    list_item_start = -1\n    list_item_end = -1\n    list_item_level = -1\n    list_item_text = \"\"\n    list_item_type = \"\"\n    list_item_blocks = []\n    list_item_header_index = -1\n    list_item_header_level = -1\n    list_item_header_text = \"\"\n    list_item_header_blocks = []\n    list_item_header_header_index = -1\n    list_item_header_header_level = -1\n    list_item_header_header_text = \"\"\n    list_item_header_header_blocks = []\n    list_item_header_header_header_index = -1\n    list_item_header_header_header_level = -1\n    list_item_header_header_header_text = \"\"\n    list_item_header_header_header_blocks = []\n    list_item_header_header_header_header_index = -1\n    list_item_header_header_header_header_level = -1\n    list_item_header_header_header_header_text = \"\"\n    list_item_header_header_header_header_blocks = []\n    list_item_header_header_header_header_header_index = -1\n    list_item_header_header_header_header_header_level = -1\n    list_item_header_header_header_header_header_text = \"\"\n    list_item_header_header_header_header_header_blocks = []\n    list_item_header_header_header_header_header_header_index = -1\n    list_item_header_header_header_header_header_header_level = -1"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    current_block = []\n    current_block_index = 0\n    current_header_block_index = None\n    current_list_item_index = 0\n    current_list_level = 0\n    current_list_item_level = 0\n    current_list_item_start_index = None\n    current_list_item_end_index = None\n    current_list_item_text = None\n    current_list_item_text_index = None\n    current_list_item_text_end_index = None\n    current_list_item_text_start_index = None\n    current_list_item_text_end_index = None\n    current_list_item_text_start_index = None\n    current_list_item_text_end_index = None\n    current_list_item_text_start_index = None\n    current_list_item_text_end_index = None\n    current_list_item_text_start_index = None\n    current_list_item_text_end_index = None\n    current_list_item_text_start_index = None\n    current_list_item_text_end_index = None\n    current_list_item_text_start_index = None\n    current_list_item_text_end_index = None\n    current_list_item_text_start_index = None\n    current_list_item_text_end_index = None\n    current_list_item_text_start_index = None\n    current_list_item_text_end_index = None\n    current_list_item_text_start_index = None\n    current_list_item_text_end_index = None\n    current_list_item_text_start_index = None\n    current_list_item_text_end_index = None\n    current_list_item_text_start_index = None\n    current_list_item_text_end_index = None\n    current_list_item_text_start_index = None\n    current_list_item_text_end_index = None\n    current_list_item_text_start"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize result list\n    result = []\n\n    # Initialize block variables\n    block_index = 0\n    block_text = \"\"\n    block_type = \"\"\n    block_start_index = 0\n    block_blocks = []\n    block_header_index = None\n    block_level = None\n\n    # Initialize list variables\n    list_item_index = 0\n    list_item_text = \"\"\n    list_item_type = \"\"\n    list_item_start_index = 0\n    list_item_blocks = []\n    list_item_header_index = None\n    list_item_level = None\n\n    # Initialize header variables\n    header_index = 0\n    header_text = \"\"\n    header_type = \"\"\n    header_start_index = 0\n    header_blocks = []\n    header_header_index = None\n    header_level = None\n\n    # Initialize list variables\n    list_item_index = 0\n    list_item_text = \"\"\n    list_item_type = \"\"\n    list_item_start_index = 0\n    list_item_blocks = []\n    list_item_header_index = None\n    list_item_level = None\n\n    # Initialize header variables\n    header_index = 0\n    header_text = \"\"\n    header_type = \"\"\n    header_start_index = 0\n    header_blocks = []\n    header_header_index = None\n    header_level = None\n\n    # Initialize list variables\n    list_item_index = 0\n    list_item_text = \"\"\n    list_item_type = \"\"\n    list_item_start_index = 0\n    list_item_blocks = []\n    list_item_header_index = None\n    list_item_level = None\n\n    # Initialize header variables\n    header_index = 0\n    header_text = \"\"\n    header_type = \"\"\n    header_start_index = 0\n    header_blocks = []\n    header_header_index = None\n    header_level = None\n\n    # Initialize list variables\n    list_item_index = 0"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize result list\n    result = []\n\n    # Initialize current block and index\n    current_block = []\n    current_index = 0\n\n    # Initialize current header block index\n    current_header_block_index = None\n\n    # Initialize list of blocks\n    blocks = []\n\n    # Initialize list of headers\n    headers = []\n\n    # Initialize list of lists\n    lists = []\n\n    # Initialize list of list items\n    list_items = []\n\n    # Initialize list of paragraphs\n    paragraphs = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to be processed\n    blocks_to_process = []\n\n    # Initialize list of blocks to"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    blocks = []\n    block_idx = 0\n    header_idx = -1\n    header_level = -1\n    list_idx = -1\n    list_level = -1\n    list_items = []\n    block_start = 0\n    block_end = 0\n    block_type = None\n    block_text = ''\n    block_text_clean = ''\n    block_text_clean_list = []\n    block_text_clean_list_idx = 0\n    block_text_clean_list_len = 0\n    block_text_clean_list_item = ''\n    block_text_clean_list_item_idx = 0\n    block_text_clean_list_item_len = 0\n    block_text_clean_list_item_start = 0\n    block_text_clean_list_item_end = 0\n    block_text_clean_list_item_type = None\n    block_text_clean_list_item_text = ''\n    block_text_clean_list_item_text_clean = ''\n    block_text_clean_list_item_text_clean_list = []\n    block_text_clean_list_item_text_clean_list_idx = 0\n    block_text_clean_list_item_text_clean_list_len = 0\n    block_text_clean_list_item_text_clean_list_item = ''\n    block_text_clean_list_item_text_clean_list_item_idx = 0\n    block_text_clean_list_item_text_clean_list_item_len = 0\n    block_text_clean_list_item_text_clean_list_item_start = 0\n    block_text_clean_list_item_text_clean_list_item_end = 0\n    block_text_clean_list_item_text_clean_list_item_type = None\n    block_text_clean_list_item_text_clean_list_item_text = ''\n    block_text_clean_list_item_text_clean_list_item_text_clean = ''\n    block_"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    blocks = []\n    current_block = {}\n    current_block[\"text\"] = []\n    current_block[\"type\"] = None\n    current_block[\"start_index\"] = None\n    current_block[\"blocks\"] = None\n    current_block[\"header_index\"] = None\n    current_block[\"level\"] = None\n    current_block[\"index\"] = None\n\n    # Remove duplicate lines\n    lines = remove_duplicate_lines(lines)\n\n    # Fix spaced characters\n    lines = fix_spaced_characters(lines)\n\n    # Connect incomplete lines\n    lines = connect_incomplete_lines(lines)\n\n    # Categorize lines into paragraphs, headers, and list items\n    for i, line in enumerate(lines):\n        if line.startswith(\"##\"):\n            # Header\n            header_level = line.count(\"#\")\n            header_text = line.replace(\"#\", \"\").strip()\n            current_block[\"type\"] = \"header\"\n            current_block[\"text\"] = header_text\n            current_block[\"level\"] = header_level\n            current_block[\"start_index\"] = i\n            result.append(current_block)\n            current_block = {}\n            current_block[\"text\"] = []\n            current_block[\"type\"] = None\n            current_block[\"start_index\"] = None\n            current_block[\"blocks\"] = None\n            current_block[\"header_index\"] = None\n            current_block[\"level\"] = None\n            current_block[\"index\"] = None\n        elif line.startswith(\"- \"):\n            # List item\n            list_text = line.replace(\"- \", \"\").strip()\n            current_block[\"type\"] = \"list\"\n            current_block[\"text\"] = list_text\n            current_block[\"start_index\"] = i\n            result.append(current_block)\n            current_block = {}\n            current_block[\"text\"] = []\n            current_block[\"type\"] = None\n            current_block[\"start_index\"] = None\n            current_block[\"blocks\"] = None"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize the result list and block index\n    result = []\n    block_index = 0\n\n    # Initialize a list to store the current block\n    current_block = []\n\n    # Initialize a list to store the header blocks\n    header_blocks = []\n\n    # Initialize a variable to store the current header level\n    current_header_level = 0\n\n    # Initialize a variable to store the index of the current header block\n    current_header_block_index = None\n\n    # Initialize a variable to store the index of the current list block\n    current_list_block_index = None\n\n    # Initialize a variable to store the level of indentation of the current list block\n    current_list_block_level = 0\n\n    # Initialize a variable to store the index of the current block\n    current_block_index = None\n\n    # Initialize a variable to store the index of the previous block\n    previous_block_index = None\n\n    # Initialize a variable to store the type of the current block\n    current_block_type = None\n\n    # Initialize a variable to store the type of the previous block\n    previous_block_type = None\n\n    # Initialize a variable to store the index of the previous header block\n    previous_header_block_index = None\n\n    # Initialize a variable to store the index of the previous list block\n    previous_list_block_index = None\n\n    # Initialize a variable to store the level of indentation of the previous list block\n    previous_list_block_level = 0\n\n    # Initialize a variable to store the index of the previous block with a header\n    previous_header_block_index = None\n\n    # Initialize a variable to store the index of the previous block with a list\n    previous_list_block_index = None\n\n    # Initialize a variable to store the level of indentation of the previous block with a list\n    previous_list_block_level = 0\n\n    # Initialize a variable to store the index of the previous block with a header\n    previous_header_block_index = None\n\n    # Initialize a variable to store the index of"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    blocks = []\n    block_index = 0\n    current_block = {}\n    current_block['index'] = block_index\n    current_block['text'] = []\n    current_block['type'] = None\n    current_block['start_index'] = None\n    current_block['blocks'] = []\n    current_block['header_index'] = None\n    current_block['level'] = None\n\n    # Loop through each line of text\n    for line_index, line in enumerate(lines):\n\n        # Clean the line\n        line = line.strip()\n        line = line.replace('\\t', ' ')\n        line = line.replace('\\n', ' ')\n        line = line.replace('\\r', ' ')\n\n        # Check if the line is empty or a duplicate of the previous line\n        if line == '' or (len(blocks) > 0 and blocks[-1]['text'] == line):\n            continue\n\n        # Check if the line is a header\n        if line.startswith('#'):\n            # Extract the header text\n            header_text = line.lstrip('#').strip()\n            # Create a new block for the header\n            header_block = {'index': block_index, 'text': header_text, 'type': 'header', 'start_index': line_index, 'blocks': [], 'header_index': None, 'level': None}\n            # Add the header block to the list of blocks\n            blocks.append(header_block)\n            # Update the current block\n            current_block = header_block\n            # Increment the block index\n            block_index += 1\n            # Skip to the next line\n            continue\n\n        # Check if the line is a list item\n        if line.startswith('- '):\n            # Extract the list item text\n            list_item_text = line.lstrip('- ').strip()\n            # Create a new block for the list item\n            list_item_block = {'index': block_index, 'text': list_item_text, 'type': 'list_item', 'start_index': line_"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    blocks = []\n    block_index = 0\n    block_start = 0\n    block_level = 0\n    block_type = \"paragraph\"\n    block_header_index = -1\n    block_list_items = []\n\n    for i, line in enumerate(lines):\n        # Remove duplicate lines\n        if i > 0 and line.strip() == lines[i - 1].strip():\n            continue\n\n        # Fix spaced characters\n        line = line.replace(\"  \", \" \")\n        line = line.replace(\"  \", \" \")\n\n        # Connect incomplete lines\n        if i > 0 and line.strip() != \"\":\n            if lines[i - 1].strip() != \"\":\n                lines[i - 1] = lines[i - 1].strip() + \" \" + line.strip()\n                continue\n\n        # Check for header\n        if line.strip().startswith(\"##\"):\n            block_type = \"header\"\n            block_level = 2\n            block_header_index = block_index\n        elif line.strip().startswith(\"#\"):\n            block_type = \"header\"\n            block_level = 1\n            block_header_index = block_index\n        elif line.strip().startswith(\"-\"):\n            block_type = \"list\"\n            block_level = 1\n            block_header_index = block_index\n            block_list_items.append(block_index)\n\n        # Check for list item\n        if line.strip().startswith(\"-\"):\n            block_type = \"list\"\n            block_level = 2\n            block_header_index = block_index\n            block_list_items.append(block_index)\n\n        # Check for code block\n        if line.strip().startswith(\"```\"):\n            block_type = \"code\"\n            block_level = 0\n            block_header_index = block_index\n\n        # Check for table\n        if line.strip().startswith(\"|\"):\n            block_type = \"table\"\n            block_level = 0\n            block_header_index"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    blocks = []\n    block_index = 0\n    header_index = 0\n    list_level = 0\n\n    # Loop through each line\n    for line_index, line in enumerate(lines):\n\n        # Remove duplicate lines\n        if line in lines[:line_index]:\n            continue\n\n        # Remove trailing and leading whitespace\n        line = line.strip()\n\n        # Fix spaced characters\n        line = line.replace('\\xa0', ' ')\n\n        # Connect incomplete lines\n        if line.endswith('-'):\n            line = line[:-1]\n            if line_index + 1 < len(lines):\n                line += lines[line_index + 1].strip()\n\n        # Check if line is a header\n        if line.startswith('#'):\n            header_index += 1\n            header_level = line.count('#')\n            block_index += 1\n            blocks.append({'index': block_index, 'text': line, 'type': 'header', 'header_level': header_level, 'header_index': header_index})\n            continue\n\n        # Check if line is a list item\n        if line.startswith('*'):\n            list_level += 1\n            block_index += 1\n            blocks.append({'index': block_index, 'text': line, 'type': 'list', 'list_level': list_level})\n            continue\n\n        # Check if line is a list item with a header\n        if line.startswith('* '):\n            list_level += 1\n            header_index += 1\n            header_level = line.count('#')\n            block_index += 1\n            blocks.append({'index': block_index, 'text': line, 'type': 'header', 'header_level': header_level, 'header_index': header_index})\n            continue\n\n        # Check if line is a list item with a sub-header\n        if line.startswith('**'):\n            list_level += 1\n            header_index += 1\n            header_level = line.count('#')"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    blocks = []\n    block_index = 0\n    block_start = 0\n    header_index = 0\n    header_level = 0\n    list_index = 0\n    list_level = 0\n    list_start = False\n    list_items = []\n    list_item_index = 0\n    list_item_level = 0\n    list_item_start = False\n    list_item_text = \"\"\n    list_item_text_start = 0\n    list_item_text_end = 0\n    list_item_text_started = False\n    list_item_text_ended = False\n\n    # Loop through each line in the input list\n    for i, line in enumerate(lines):\n\n        # Remove duplicate lines\n        if line in lines[:i]:\n            continue\n\n        # Remove leading and trailing whitespace\n        line = line.strip()\n\n        # Remove leading and trailing XML tags\n        if xml:\n            line = line.strip(\"<>\")\n\n        # Fix spaced characters\n        line = line.replace(\"  \", \" \")\n\n        # Connect incomplete lines\n        if line.endswith(\"-\"):\n            line = line[:-1]\n            if i < len(lines) - 1:\n                next_line = lines[i + 1].strip()\n                if next_line.startswith(\"-\"):\n                    line += next_line\n                    lines[i + 1] = \"\"\n\n        # Categorize lines into paragraphs, headers, or list items\n        if line.startswith(\"##\"):\n            header_level = 2\n            header_index += 1\n            block_index += 1\n            result.append(\n                {\n                    \"index\": block_index,\n                    \"text\": line[2:].strip(),\n                    \"type\": \"header\",\n                    \"start\": block_start,\n                    \"header_index\": header_index,\n                    \"header_level\": header_level,\n                }\n            )\n            block_start = i + 1\n            blocks.append(block_index)\n        elif line.startswith"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    blocks = []\n    prev_block = None\n    current_block = None\n    header_block = None\n    list_block = None\n    list_level = None\n    list_items = []\n    list_item_index = 0\n\n    # Iterate over the lines\n    for i, line in enumerate(lines):\n\n        # Clean the line\n        line = line.strip()\n\n        # Check if the line is empty or only contains whitespace\n        if not line:\n            continue\n\n        # Check if the line is a header\n        if line.startswith(\"## \"):\n            header_block = {\"index\": i, \"text\": line, \"type\": \"header\", \"start\": i}\n            blocks.append(header_block)\n            continue\n\n        # Check if the line is a list item\n        if line.startswith(\"* \"):\n            list_item = {\"index\": i, \"text\": line, \"type\": \"list_item\", \"start\": i}\n            list_items.append(list_item)\n            continue\n\n        # Check if the line is a list\n        if line.startswith(\"* \"):\n            list_block = {\"index\": i, \"text\": line, \"type\": \"list\", \"start\": i, \"list_items\": list_items}\n            blocks.append(list_block)\n            list_items = []\n            continue\n\n        # Check if the line is a paragraph\n        if line.startswith(\"## \"):\n            continue\n        if line.startswith(\"* \"):\n            continue\n        if line.startswith(\"### \"):\n            continue\n        if line.startswith(\"#### \"):\n            continue\n        if line.startswith(\"##### \"):\n            continue\n        if line.startswith(\"###### \"):\n            continue\n        if line.startswith(\"####### \"):\n            continue\n        if line.startswith(\"###### \"):\n            continue\n        if line.startswith(\"##### \"):\n            continue\n        if line.startswith(\"#### \"):\n            continue"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    blocks = []\n    headers = []\n    list_items = []\n    list_level = -1\n    list_start_index = -1\n    current_block = None\n    current_header = None\n    current_list_item = None\n    current_list_level = -1\n    current_list_start_index = -1\n\n    # Loop through each line in the input list\n    for i, line in enumerate(lines):\n        # Remove leading and trailing whitespace\n        line = line.strip()\n\n        # Ignore empty lines\n        if not line:\n            continue\n\n        # Check if the line is a header\n        if line.startswith(\"#\"):\n            # Add the current block to the list of blocks if it exists\n            if current_block:\n                blocks.append(current_block)\n\n            # Create a new header block\n            current_header = {\n                \"index\": len(blocks),\n                \"text\": line[1:].strip(),\n                \"type\": \"header\",\n                \"start_index\": i,\n                \"blocks\": [],\n                \"header_index\": -1,\n                \"indent\": 0\n            }\n\n            # Add the header to the list of headers\n            headers.append(current_header)\n\n            # Update the current block and header variables\n            current_block = current_header\n            current_header = headers[-1]\n\n            # Check if the header is a subheader\n            if len(headers) > 1:\n                headers[-2][\"blocks\"].append(current_header)\n                current_header[\"header_index\"] = headers[-2][\"index\"]\n\n            # Check if the header is a subsubheader\n            if len(headers) > 2:\n                headers[-3][\"blocks\"].append(current_header)\n                current_header[\"header_index\"] = headers[-3][\"index\"]\n\n            # Check if the header is a subsubsubheader\n            if len(headers) > 3:\n                headers[-4][\"blocks\"].append(current_header)\n                current_header[\"header_index\"] = headers"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    blocks = []\n    block_index = 0\n    block_start_index = 0\n    header_index = -1\n    header_level = -1\n    list_level = -1\n\n    for i, line in enumerate(lines):\n\n        # Remove duplicate lines\n        if i > 0 and line == lines[i - 1]:\n            continue\n\n        # Fix spaced characters\n        line = line.replace(\"  \", \" \")\n\n        # Connect incomplete lines\n        if i > 0 and line.startswith(\"-\") and not lines[i - 1].endswith(\"-\") and not lines[i - 1].endswith(\".\"):\n            lines[i - 1] += \" \" + line\n            continue\n\n        # Categorize lines into paragraphs, headers, or list items\n        if line.startswith(\"- \"):\n            block_type = \"list\"\n            list_level = line.count(\"-\") - 1\n        elif line.startswith(\"## \"):\n            block_type = \"header\"\n            header_level = 2\n        elif line.startswith(\"# \"):\n            block_type = \"header\"\n            header_level = 1\n        else:\n            block_type = \"paragraph\"\n            header_level = -1\n\n        # Append the block to the result list\n        result.append({\n            \"index\": block_index,\n            \"text\": line,\n            \"type\": block_type,\n            \"start_index\": block_start_index,\n            \"blocks\": blocks,\n            \"header_index\": header_index,\n            \"list_level\": list_level,\n            \"header_level\": header_level\n        })\n\n        # Update block variables\n        block_index += 1\n        block_start_index = i\n        blocks = []\n        header_index = -1\n        list_level = -1\n\n        # Process block types that require additional processing\n        if block_type == \"header\":\n            header_index = block_index\n\n    return result"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    block = None\n    blocks = []\n    header_index = None\n    header_level = 0\n    header_lines = []\n    list_items = []\n    list_level = 0\n\n    # Iterate through the lines\n    for i, line in enumerate(lines):\n\n        # Remove leading and trailing whitespace\n        line = line.strip()\n\n        # Remove duplicate lines (ignoring numbers)\n        if line and line not in [b[\"text\"] for b in result]:\n\n            # Check for header\n            if line.startswith(\"##\") and not xml:\n                header_level = 2\n                header_index = len(result)\n                header_lines.append(line)\n                continue\n\n            elif line.startswith(\"#\") and not xml:\n                header_level = 1\n                header_index = len(result)\n                header_lines.append(line)\n                continue\n\n            # Check for list item\n            if line.startswith(\"*\") and not xml:\n                list_level = line.count(\"*\")\n                list_items.append(line)\n                continue\n\n            # Check for indentation\n            if line.startswith(\"    \") and not xml:\n                list_level = 4\n                list_items.append(line)\n                continue\n\n            # Check for XML tags\n            if xml:\n                if line.startswith(\"<\") and line.endswith(\">\"):\n                    block = {\"index\": len(result), \"text\": line, \"type\": \"tag\", \"start\": i, \"blocks\": [], \"header_index\": None, \"list_level\": None}\n                    result.append(block)\n                    continue\n\n            # Add line to block\n            if not block:\n                block = {\"index\": len(result), \"text\": line, \"type\": \"paragraph\", \"start\": i, \"blocks\": [], \"header_index\": None, \"list_level\": None}\n                result.append(block)\n            else:\n                block[\"text\"] += \" \" + line\n\n            # Check for block end\n            if block"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == '':\n        return org_texts\n\n    space_rule = r'[^\\n]+\\n'\n    bracket_rule = r'[^\\(]+\\([^\\)]+\\)[^\\)]+'\n    rules = [\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n]+\\n',\n        r'[^\\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or len(org_texts) == 0:\n        return org_texts\n\n    space_rule = re.compile(r'(\\s+)')\n    bracket_rule = re.compile(r'(\\()')\n    rules = [\n        (r'([a-z])\\.([A-Z])', r'\\1. \\2'),\n        (r'([a-z])\\.([a-z])', r'\\1. \\2'),\n        (r'([a-z])\\.', r'\\1. '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r'([a-z])\\?', r'\\1? '),\n        (r"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # Apply tokenization rules\n    text = space_rule(org_texts)\n    text = bracket_rule(text)\n    text = rules(text)\n\n    # Tokenize the text using the tokenizer\n    tokenized_text = nltk_tokenizer.tokenize(text)\n\n    # Normalize quotation marks\n    tokenized_text = [quotation_pattern.sub(r\"\\1\\2\", sentence) for sentence in tokenized_text]\n\n    return tokenized_text"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # Normalize quotation marks\n    org_texts = org_texts.replace(\"\u2018\", \"'\").replace(\"\u2019\", \"'\").replace(\"\u201c\", '\"').replace(\"\u201d\", '\"')\n\n    # Tokenize\n    tokenized_texts = nltk_tokenizer.tokenize(org_texts)\n\n    # Apply rules\n    for rule in rules:\n        tokenized_texts = rule(tokenized_texts)\n\n    # Handle special cases\n    tokenized_texts = space_rule(tokenized_texts)\n    tokenized_texts = bracket_rule(tokenized_texts)\n\n    # Normalize quotation marks\n    tokenized_texts = [re.sub(quotation_pattern, r\"\\1'\", text) for text in tokenized_texts]\n\n    return tokenized_texts"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or len(org_texts) == 0:\n        return org_texts\n\n    # Normalize quotation marks\n    text = org_texts.replace('\u201c', '\"').replace('\u201d', '\"').replace('\u2019', \"'\")\n\n    # Tokenize into sentences\n    sentences = nltk_tokenizer.tokenize(text)\n\n    # Handle special cases\n    for rule in rules:\n        sentences = rule(sentences)\n\n    # Handle special cases\n    for rule in bracket_rules:\n        sentences = rule(sentences)\n\n    # Handle special cases\n    for rule in space_rules:\n        sentences = rule(sentences)\n\n    # Handle special cases\n    for rule in quotation_rules:\n        sentences = rule(sentences)\n\n    # Remove empty sentences\n    sentences = [sentence for sentence in sentences if sentence.strip()]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    import re\n\n    if org_texts is None or org_texts == '':\n        return org_texts\n\n    # Normalize quotation marks\n    quotation_pattern = re.compile('([\\'\\\"])')\n    org_texts = quotation_pattern.sub(r' \\1 ', org_texts)\n\n    # Tokenize\n    nltk_tokenizer = nltk.tokenize.sent_tokenize\n    space_rule = re.compile('\\s+')\n    bracket_rule = re.compile('\\([^\\)]+\\)')\n    rules = [\n        re.compile('\\n'),\n        re.compile('\\n\\n'),\n        re.compile('\\n\\n\\n'),\n        re.compile('\\n\\n\\n\\n'),\n        re.compile('\\n\\n\\n\\n\\n'),\n        re.compile('\\n\\n\\n\\n\\n\\n'),\n        re.compile('\\n\\n\\n\\n\\n\\n\\n'),\n        re.compile('\\n\\n\\n\\n\\n\\n\\n\\n'),\n        re.compile('\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n        re.compile('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n        re.compile('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n        re.compile('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n        re.compile('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n        re.compile('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n        re.compile('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n        re.compile('\\n\\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # Normalize quotation marks\n    quotation_pattern = r\"(\\\"|\\\u201c|\\\u201d|\\\u2018|\\\u2019)(.*?)(\\\"|\\\u201c|\\\u201d|\\\u2018|\\\u2019)\"\n    org_texts = re.sub(quotation_pattern, r\"\\1 \\2 \\3\", org_texts)\n\n    # Tokenize the text using the NLTK tokenizer\n    nltk_tokenizer = nltk.tokenize.sent_tokenize\n    sentences = nltk_tokenizer(org_texts)\n\n    # Define the rules for handling special cases\n    space_rule = r\"\\s+\"\n    bracket_rule = r\"\\([^\\(\\)]+?\\)\"\n    rules = [\n        (re.compile(space_rule), \" \"),\n        (re.compile(bracket_rule), \"\"),\n    ]\n\n    # Apply the rules to each sentence\n    for sentence in sentences:\n        for rule, replacement in rules:\n            sentence = rule.sub(replacement, sentence)\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # If the input text is empty or None, return the input as is\n    if not org_texts:\n        return org_texts\n\n    # Tokenize the text using the NLTK tokenizer\n    nltk_tokenizer = nltk.tokenize.sent_tokenize\n    sentences = nltk_tokenizer(org_texts)\n\n    # Apply the rules to the sentences\n    for i, sentence in enumerate(sentences):\n        # Normalize quotation marks\n        sentence = re.sub(r'\u201c|\u201d', '\"', sentence)\n        sentence = re.sub(r'\u2018|\u2019', \"'\", sentence)\n        # Apply the space rule\n        sentence = space_rule(sentence)\n        # Apply the bracket rule\n        sentence = bracket_rule(sentence)\n        # Apply the rules\n        sentence = rules(sentence)\n        # Apply the quotation pattern\n        sentence = quotation_pattern(sentence)\n        # Tokenize the sentence using the NLTK tokenizer\n        sentences[i] = nltk_tokenizer(sentence)\n\n    # Flatten the list of lists\n    sentences = [sentence for sublist in sentences for sentence in sublist]\n\n    # Return the tokenized sentences\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == '':\n        return org_texts\n\n    # Normalize quotation marks\n    org_texts = org_texts.replace('\u201c', '\"').replace('\u201d', '\"').replace('\u2018', \"'\").replace('\u2019', \"'\")\n\n    # Tokenize text into sentences\n    sentences = nltk_tokenizer.tokenize(org_texts)\n\n    # Handle special cases\n    space_rule = re.compile(r'\\s+')\n    bracket_rule = re.compile(r'(\\([^\\)]*\\))')\n    rules = [\n        (re.compile(r'^\\s*-\\s+'), ''),\n        (re.compile(r'^\\s*[a-z]\\)\\s+'), ''),\n        (re.compile(r'^\\s*[a-z]\\)\\.'), ''),\n        (re.compile(r'^\\s*\\d\\)'), ''),\n        (re.compile(r'^\\s*[a-z]\\.\\s+'), ''),\n        (re.compile(r'^\\s*[a-z]\\.'), ''),\n        (re.compile(r'^\\s*\\d\\.\\s+'), ''),\n        (re.compile(r'^\\s*\\d\\.'), ''),\n        (re.compile(r'^\\s*[a-z]\\)\\s*$'), ''),\n        (re.compile(r'^\\s*[a-z]\\)\\.'), ''),\n        (re.compile(r'^\\s*\\d\\)$'), ''),\n        (re.compile(r'^\\s*[a-z]\\.\\s*$'), ''),\n        (re.compile(r'^\\s*[a-z]\\.'), ''),\n        (re.compile(r'^\\s*\\d\\.\\s*$'), ''),\n        (re.compile(r'^\\s*\\d\\.'), ''),\n        (re.compile(r'^\\s*\\(\\s*$'), ''),\n        (re.compile(r"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # tokenize the text into sentences\n    nltk_tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n    sentences = nltk_tokenizer.tokenize(org_texts)\n\n    # normalize quotation marks\n    quotation_pattern = re.compile('\"')\n    sentences = [quotation_pattern.sub('``', sentence) for sentence in sentences]\n\n    # apply rules to sentences\n    rules = [\n        (re.compile(r'([a-z])\\.([A-Z])'), r'\\1. \\2'),\n        (re.compile(r'([a-z])(\\.[a-z])'), r'\\1\\2'),\n        (re.compile(r'([a-z])(\\.)\\s([A-Z])'), r'\\1\\2 \\3'),\n        (re.compile(r'([a-z])(\\.)\\s([a-z])'), r'\\1\\2 \\3'),\n        (re.compile(r'([a-z])(\\.)\\s([0-9])'), r'\\1\\2 \\3'),\n        (re.compile(r'([a-z])(\\.)\\s([a-z])\\s([a-z])'), r'\\1\\2 \\3 \\4'),\n        (re.compile(r'([a-z])(\\.)\\s([a-z])\\s([0-9])'), r'\\1\\2 \\3 \\4'),\n        (re.compile(r'([a-z])(\\.)\\s([a-z])\\s([a-z])\\s([a-z])'), r'\\1\\2 \\3 \\4 \\5'),\n        (re.compile(r'([a-z])(\\.)\\s([a-z])\\s([a-z])\\s([0-9])'), r'\\1\\2 \\3 \\4 \\5'),\n        (re.compile(r'([a-z])(\\.)\\s([a-z])\\s([a-z])\\s([a-z"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # If the input is empty or None, return the input as is\n    if not org_texts:\n        return org_texts\n\n    # Normalize quotation marks within the text\n    text = org_texts.replace('\u201c', '\"').replace('\u201d', '\"').replace('\u2018', \"'\").replace('\u2019', \"'\")\n\n    # Define a regular expression pattern for detecting quotation marks\n    quotation_pattern = re.compile(r'\"(.*?)\"')\n\n    # Apply the quotation pattern to the text and replace the matched quotation marks with a unique token\n    text = quotation_pattern.sub('<quotation>', text)\n\n    # Tokenize the text into sentences using the NLTK tokenizer\n    nltk_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n    sentences = nltk_tokenizer.tokenize(text)\n\n    # Define a regular expression pattern for detecting brackets\n    bracket_pattern = re.compile(r'\\[(.*?)\\]')\n\n    # Apply the bracket pattern to the text and replace the matched brackets with a unique token\n    text = bracket_pattern.sub('<bracket>', text)\n\n    # Define a regular expression pattern for detecting parentheses\n    parentheses_pattern = re.compile(r'\\((.*?)\\)')\n\n    # Apply the parentheses pattern to the text and replace the matched parentheses with a unique token\n    text = parentheses_pattern.sub('<parentheses>', text)\n\n    # Define a regular expression pattern for detecting braces\n    brace_pattern = re.compile(r'{(.*?)}')\n\n    # Apply the brace pattern to the text and replace the matched braces with a unique token\n    text = brace_pattern.sub('<brace>', text)\n\n    # Define a regular expression pattern for detecting angle brackets\n    angle_bracket_pattern = re.compile(r'<(.*?)>')\n\n    # Apply the angle bracket pattern to the text and replace the matched angle brackets with a unique token\n   "}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # Check if the input is empty or None\n    if org_texts is None or org_texts == '':\n        return org_texts\n\n    # Define the space rule\n    space_rule = re.compile(r'([^\\s\\w]+)(\\s+)([^\\s\\w]+)')\n\n    # Define the bracket rule\n    bracket_rule = re.compile(r'(\\([^\\)]*\\))')\n\n    # Define the rules\n    rules = [\n        {r'>\\s+': '>'},  # remove spaces after a tag\n        {r'\\s+': ' '},  # shorten multiple spaces\n        {r'\\s+\\.\\s+': '.'},  # shorten spaces between sentences\n        {r'\\s+\\!': '!'},  # shorten spaces after sentences\n        {r'\\?\\s+': '?'},  # shorten spaces after sentences\n        {r'\\s+\\.\\\"\\s+': '.\\\"'},  # shorten spaces after sentences\n        {r'\\s+\\?\\\"\\s+': '?\\\"'},  # shorten spaces after sentences\n        {r'\\s+\\!\\?\\s+': '!\\?'},  # shorten spaces after sentences\n        {r'\\s+\\?\\!\\s+': '?\\!'},  # shorten spaces after sentences\n        {r'\\s+\\.\\\"\\?\\s+': '.\\\"?'},  # shorten spaces after sentences\n        {r'\\s+\\?\\\"\\!\\s+': '?\\\"!'},  # shorten spaces after sentences\n        {r'\\s+\\.\\\"\\!\\s+': '.\\\"?!'},  # shorten spaces after sentences\n        {r'\\s+\\?\\!\\?\\s+': '?\\!?'},  # shorten spaces after sentences\n        {r'\\s+\\.\\\"\\?\\!\\s+': '.\\\"?\\!'},  # shorten spaces after sentences\n        {r'\\s+\\?\\\"\\!\\?\\s+': '?\\\"\\!?'},  # shorten spaces after sentences\n        {r'\\s+\\.\\\"\\!\\?\\s+': '.\\\"?\\!?'"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == '':\n        return org_texts\n\n    space_rule = re.compile(r'([^\\n])\\n([^\\n])')\n    bracket_rule = re.compile(r'\\([^\\)]+\\)')\n    rules = [\n        re.compile(r'(\\S)(\\.) ([A-Z])'),\n        re.compile(r'(\\.) ([A-Z])'),\n        re.compile(r'(\\.)$'),\n        re.compile(r'(\\?)\\\"'),\n        re.compile(r'(\\?)\\n'),\n        re.compile(r'(\\?)\\\"'),\n        re.compile(r'(\\?)\\\"'),\n        re.compile(r'(\\.)\\\"'),\n        re.compile(r'(\\.)\\n'),\n        re.compile(r'(\\.)\\\"'),\n        re.compile(r'(\\.)\\\"'),\n        re.compile(r'(\\\")\\\"'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\\"'),\n        re.compile(r'(\\\")\\\"'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),\n        re.compile(r'(\\\")\\n'),"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    import re\n    from nltk.tokenize import sent_tokenize as nltk_tokenizer\n\n    # Check if the input text is empty or None\n    if not org_texts:\n        return org_texts\n\n    # Normalize quotation marks\n    quotation_pattern = re.compile(r'[\"\u201c\u201d]')\n    org_texts = quotation_pattern.sub('\"', org_texts)\n\n    # Tokenize the text into sentences\n    sentences = nltk_tokenizer.sent_tokenize(org_texts)\n\n    # Define a space rule\n    space_rule = re.compile(r'\\s+')\n\n    # Define a bracket rule\n    bracket_rule = re.compile(r'^\\s*\\(.*\\)\\s*$')\n\n    # Define a rule to check if the sentence ends with a question mark\n    question_mark_rule = re.compile(r'\\?$')\n\n    # Define a rule to check if the sentence ends with a colon\n    colon_rule = re.compile(r':$')\n\n    # Define a rule to check if the sentence ends with a comma\n    comma_rule = re.compile(r',$')\n\n    # Define a rule to check if the sentence ends with a period\n    period_rule = re.compile(r'\\.$')\n\n    # Define a rule to check if the sentence ends with a semicolon\n    semicolon_rule = re.compile(r';$')\n\n    # Define a rule to check if the sentence ends with a exclamation mark\n    exclamation_rule = re.compile(r'\\!$')\n\n    # Define a rule to check if the sentence ends with a question mark\n    question_mark_rule = re.compile(r'\\?$')\n\n    # Define a rule to check if the sentence ends with a exclamation mark\n    exclamation_rule = re.compile(r'\\!$')\n\n    # Define a rule to check if the sentence ends with a colon\n    colon_rule = re.compile(r':$')\n\n    # Define a rule to check if the sentence ends with"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # If the input text is empty or None, return the input as is\n    if org_texts is None or org_texts == '':\n        return org_texts\n\n    # Normalize quotation marks within the text\n    quotation_pattern = re.compile(r'\u201c([^\u201d]*)\u201d|\u2018([^\u2019]*)\u2019|\"([^\"]*)\"|\\'([^\\']*)\\'')\n    org_texts = quotation_pattern.sub(r' \\1\\2\\3\\4 ', org_texts)\n\n    # Define a rule to handle special cases of paragraphs separated by new lines\n    space_rule = re.compile(r'\\n\\s*\\n')\n\n    # Define a rule to handle special cases of sentences within brackets\n    bracket_rule = re.compile(r'(?<=[\\.\\?\\!])\\s*(?=\\[)')\n\n    # Define a set of rules to apply to the text\n    rules = [space_rule, bracket_rule]\n\n    # Apply the defined rules to the text\n    for rule in rules:\n        org_texts = rule.sub(' ', org_texts)\n\n    # Tokenize the text using the NLTK tokenizer\n    nltk_tokenizer = nltk.tokenize.sent_tokenize\n    sentences = nltk_tokenizer(org_texts)\n\n    # Return the tokenized sentences\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == '':\n        return org_texts\n\n    # Normalize quotation marks\n    org_texts = org_texts.replace('\u201c', '\"').replace('\u201d', '\"').replace('\u2018', \"'\").replace('\u2019', \"'\")\n\n    # Tokenize the text into sentences\n    sentences = nltk_tokenizer.tokenize(org_texts)\n\n    # Apply the space rule to handle special cases\n    sentences = [space_rule(sentence) for sentence in sentences]\n\n    # Apply the bracket rule to handle special cases within brackets\n    sentences = [bracket_rule(sentence) for sentence in sentences]\n\n    # Apply the rules to handle special cases\n    sentences = [rules(sentence) for sentence in sentences]\n\n    # Apply the quotation pattern to handle quotation marks\n    sentences = [quotation_pattern.sub('', sentence) for sentence in sentences]\n\n    # Remove empty sentences\n    sentences = [sentence for sentence in sentences if sentence.strip()]\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == '':\n        return org_texts\n\n    space_rule = re.compile(r'\\s+')\n    bracket_rule = re.compile(r'\\[(.*?)\\]')\n    rules = [\n        (re.compile(r'(\\s+)([?.!\"](?:\\s|$))'), r'\\2\\1'),\n        (re.compile(r'([?.!\"](?:\\s|$))(\\s+)'), r'\\1\\2'),\n    ]\n    quotation_pattern = re.compile(r'(\\'|\\\")')\n\n    # Tokenize the text using the NLTK tokenizer\n    tokenized_text = nltk_tokenizer.tokenize(org_texts)\n\n    # Apply the rules and quotation pattern to the tokenized text\n    tokenized_text = [space_rule.sub(' ', sent) for sent in tokenized_text]\n    tokenized_text = [bracket_rule.sub(r' \\1 ', sent) for sent in tokenized_text]\n    tokenized_text = [re.sub(r'([?.!\"](?:\\s|$))', r' \\1 ', sent) for sent in tokenized_text]\n    tokenized_text = [re.sub(r'(\\s+)([?.!\"](?:\\s|$))', r'\\1\\2 ', sent) for sent in tokenized_text]\n    tokenized_text = [re.sub(r'([?.!\"](?:\\s|$))(\\s+)', r'\\1\\2 ', sent) for sent in tokenized_text]\n    tokenized_text = [re.sub(r'(\\s+)', ' ', sent) for sent in tokenized_text]\n    tokenized_text = [re.sub(quotation_pattern, r\"'\", sent) for sent in tokenized_text]\n\n    return tokenized_text"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == '':\n        return org_texts\n\n    # Normalize quotation marks\n    org_texts = org_texts.replace(\"\u2019\", \"'\").replace(\"\u2018\", \"'\").replace(\"\u201c\", '\"').replace(\"\u201d\", '\"')\n\n    # Apply the space rule\n    org_texts = space_rule(org_texts)\n\n    # Apply the bracket rule\n    org_texts = bracket_rule(org_texts)\n\n    # Tokenize the text using the NLTK tokenizer\n    sentences = nltk_tokenizer.tokenize(org_texts)\n\n    # Apply the quotation pattern\n    sentences = quotation_pattern(sentences)\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None:\n        return org_texts\n\n    if len(org_texts) == 0:\n        return org_texts\n\n    # 1. \ubb38\uc7a5 \uc0ac\uc774\uc5d0 \uc904\ubc14\uafc8 \ubb38\uc790\uac00 \uc788\ub294 \uacbd\uc6b0, \ubb38\uc7a5 \uc0ac\uc774\uc5d0 \uacf5\ubc31 \ubb38\uc790\ub97c \uc0bd\uc785\ud558\uc5ec \ubb38\uc7a5\uc744 \uad6c\ubd84\ud569\ub2c8\ub2e4.\n    space_rule = re.compile(r'(\\n)')\n    org_texts = space_rule.sub(r' \\1', org_texts)\n\n    # 2. \uad04\ud638 \uc548\uc758 \ubb38\uc7a5\uc744 \ubd84\ub9ac\ud558\uc5ec \ubb38\uc7a5\uc744 \uad6c\ubd84\ud569\ub2c8\ub2e4.\n    bracket_rule = re.compile(r'(\\([^()]+\\))')\n    org_texts = bracket_rule.sub(r' \\1 ', org_texts)\n\n    # 3. \ubb38\uc7a5 \uc0ac\uc774\uc5d0 \uc904\ubc14\uafc8 \ubb38\uc790\ub97c \uc81c\uac70\ud558\uace0 \ubb38\uc7a5\uc744 \uad6c\ubd84\ud569\ub2c8\ub2e4.\n    org_texts = org_texts.replace('\\n', ' ')\n\n    # 4. \ubb38\uc7a5 \uc0ac\uc774\uc5d0 \uc904\ubc14\uafc8 \ubb38\uc790\uac00 \uc5c6\ub294 \uacbd\uc6b0, \ubb38\uc7a5\uc744 \uad6c\ubd84"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    import re\n\n    # If the input text is empty or None, return the input as is\n    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # Define the rules to handle special cases\n    space_rule = re.compile(r\"\\s+\")\n    bracket_rule = re.compile(r\"\\([^\\)]+\\)\")\n    rules = re.compile(r\"[^\\w\\s]\")\n    quotation_pattern = re.compile(r\"[\\\"\\']\")\n\n    # Apply the rules and tokenize the text\n    text = re.sub(space_rule, \" \", org_texts)\n    text = re.sub(bracket_rule, \" \", text)\n    text = re.sub(rules, \"\", text)\n    text = re.sub(quotation_pattern, \" \", text)\n    text = text.strip()\n    sentences = nltk_tokenizer.tokenize(text)\n\n    return sentences\n\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            return self.get_positions(token, key)\n        else:\n            return self.get_positions(token)\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return [self.positions[token] for token in self.positions.keys()]\n        else:\n            return [self.positions[token][key]]\n\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return self.positions_across_all_docs[token]\n        else:\n            return self.positions_across_all_docs[token][key]\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return self.positions[token]\n        else:\n            return self.positions[token][key]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return self._positions[token]\n        else:\n            return self._positions[token][key]\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return [self.positions_dict[token][key] for key in self.positions_dict[token]]\n        else:\n            return self.positions_dict[token][key]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return [self.doc_pos[doc][token] for doc in self.doc_pos]\n        else:\n            return self.doc_pos[key][token]\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return self.pos_dict[token]\n        else:\n            return self.pos_dict[token][key]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return self.pos.get(token, [])\n        else:\n            return self.pos.get(token, {}).get(key, [])\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            if key not in self.index:\n                raise KeyError(f\"Document with key '{key}' not found in the index.\")\n            return self.index[key].positions(token)\n        else:\n            return [doc.positions(token) for doc in self.index.values()]\n\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return [self.positions_dict[token]]\n        else:\n            return self.positions_dict[token][key]\n\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return self.positions_across_all_docs(token)\n        else:\n            return self.positions_in_a_doc(token, key)\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return self.pos_dict.get(token, [])\n        else:\n            return self.pos_dict.get(token, {}).get(key, [])\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return self.positions_all_docs[token]\n        else:\n            return self.positions_all_docs[token][key]\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return self.positions_dict[token]\n        else:\n            return self.positions_dict[token][key]\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return self.positions[token]\n        else:\n            return self.positions[token][key]\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            return self.dictionary[token][key]\n        else:\n            return [self.dictionary[token][k] for k in self.dictionary[token]]\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return self.positions_dict.get(token, [])\n        else:\n            return self.positions_dict.get(token, {}).get(key, [])\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            if key not in self.doc_keys:\n                raise KeyError(f\"Document key '{key}' not found in the SearchArray.\")\n            return [np.array(self.doc_positions[key])]\n        else:\n            return [np.array(positions) for positions in self.doc_positions.values()]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key:\n            return self._positions[key][token]\n        else:\n            return [self._positions[k][token] for k in self._positions.keys()]\n    "}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == '0':\n        return 0\n    elif spec == '1':\n        return 1\n    elif spec == '2':\n        return 2\n    elif spec == '3':\n        return 3\n    elif spec == '4':\n        return 4\n    elif spec == '5':\n        return 5\n    elif spec == '6':\n        return 6\n    elif spec == '7':\n        return 7\n    elif spec == '8':\n        return 8\n    elif spec == '9':\n        return 9\n    elif spec == '10':\n        return 10\n    elif spec == '11':\n        return 11\n    elif spec == '12':\n        return 12\n    elif spec == '13':\n        return 13\n    elif spec == '14':\n        return 14\n    elif spec == '15':\n        return 15\n    elif spec == '16':\n        return 16\n    elif spec == '17':\n        return 17\n    elif spec == '18':\n        return 18\n    elif spec == '19':\n        return 19\n    elif spec == '20':\n        return 20\n    elif spec == '21':\n        return 21\n    elif spec == '22':\n        return 22\n    elif spec == '23':\n        return 23\n    elif spec == '24':\n        return 24\n    elif spec == '25':\n        return 25\n    elif spec == '26':\n        return 26\n    elif spec == '27':\n        return 27\n    elif spec == '28':\n        return 28\n    elif spec == '29':\n        return 29\n    elif spec == '30':\n        return 30\n    elif spec == '31':\n        return 31\n    elif spec == '32':\n        return 32\n    elif spec == '33':\n        return 33\n    elif spec == '34':"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == '1':\n        return 1\n    if spec == '0':\n        return 0\n    if spec == '0<1':\n        return 0\n    if spec == '0<2':\n        return 0\n    if spec == '0<3':\n        return 0\n    if spec == '0<4':\n        return 0\n    if spec == '0<5':\n        return 0\n    if spec == '0<6':\n        return 0\n    if spec == '0<7':\n        return 0\n    if spec == '0<8':\n        return 0\n    if spec == '0<9':\n        return 0\n    if spec == '0<10':\n        return 0\n    if spec == '0<11':\n        return 0\n    if spec == '0<12':\n        return 0\n    if spec == '0<13':\n        return 0\n    if spec == '0<14':\n        return 0\n    if spec == '0<15':\n        return 0\n    if spec == '0<16':\n        return 0\n    if spec == '0<17':\n        return 0\n    if spec == '0<18':\n        return 0\n    if spec == '0<19':\n        return 0\n    if spec == '0<20':\n        return 0\n    if spec == '0<21':\n        return 0\n    if spec == '0<22':\n        return 0\n    if spec == '0<23':\n        return 0\n    if spec == '0<24':\n        return 0\n    if spec == '0<25':\n        return 0\n    if spec == '0<26':\n        return 0\n    if spec == '0<27':\n        return 0\n    if spec == '0<28':\n        return 0\n    if spec == '0<29':\n        return 0\n    if spec == '0<30':\n        return 0\n    if"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == '0':\n        return 0\n\n    if spec == '1':\n        return 1\n\n    if spec == '2':\n        return 2\n\n    if spec == '3':\n        return 3\n\n    if spec == '4':\n        return 4\n\n    if spec == '5':\n        return 5\n\n    if spec == '6':\n        return 6\n\n    if spec == '7':\n        return 7\n\n    if spec == '8':\n        return 8\n\n    if spec == '9':\n        return 9\n\n    if spec == '10':\n        return 10\n\n    if spec == '11':\n        return 11\n\n    if spec == '12':\n        return 12\n\n    if spec == '13':\n        return 13\n\n    if spec == '14':\n        return 14\n\n    if spec == '15':\n        return 15\n\n    if spec == '16':\n        return 16\n\n    if spec == '17':\n        return 17\n\n    if spec == '18':\n        return 18\n\n    if spec == '19':\n        return 19\n\n    if spec == '20':\n        return 20\n\n    if spec == '21':\n        return 21\n\n    if spec == '22':\n        return 22\n\n    if spec == '23':\n        return 23\n\n    if spec == '24':\n        return 24\n\n    if spec == '25':\n        return 25\n\n    if spec == '26':\n        return 26\n\n    if spec == '27':\n        return 27\n\n    if spec == '28':\n        return 28\n\n    if spec == '29':\n        return 29\n\n    if spec == '30':\n        return 30\n\n    if spec == '31':\n        return 31\n\n    if spec == '3"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == '1':\n        return 1\n    elif spec.endswith('%'):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    elif spec.endswith('<'):\n        return int(spec[:-1])\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.isdigit():\n        return int(spec)\n    elif spec.endswith('%'):\n        return int(num_clauses * float(spec[:-1]) / 100)\n    elif '<' in spec:\n        parts = spec.split('<')\n        if parts[0].isdigit():\n            return int(parts[0])\n        elif parts[0].endswith('%'):\n            return int(num_clauses * float(parts[0][:-1]) / 100)\n        else:\n            raise ValueError('Invalid min should match spec: {}'.format(spec))\n    else:\n        raise ValueError('Invalid min should match spec: {}'.format(spec))\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == '1':\n        return 1\n    elif spec == '0':\n        return 0\n    elif spec == '0<1':\n        return 0\n    elif spec == '0<2':\n        return 0\n    elif spec == '1<2':\n        return 1\n    elif spec == '1<3':\n        return 1\n    elif spec == '1<4':\n        return 1\n    elif spec == '2<3':\n        return 2\n    elif spec == '2<4':\n        return 2\n    elif spec == '3<4':\n        return 3\n    elif spec == '0<100%':\n        return 0\n    elif spec == '1<100%':\n        return 1\n    elif spec == '2<100%':\n        return 2\n    elif spec == '3<100%':\n        return 3\n    elif spec == '4<100%':\n        return 4\n    elif spec == '5<100%':\n        return 5\n    elif spec == '6<100%':\n        return 6\n    elif spec == '7<100%':\n        return 7\n    elif spec == '8<100%':\n        return 8\n    elif spec == '9<100%':\n        return 9\n    elif spec == '10<100%':\n        return 10\n    elif spec == '11<100%':\n        return 11\n    elif spec == '12<100%':\n        return 12\n    elif spec == '13<100%':\n        return 13\n    elif spec == '14<100%':\n        return 14\n    elif spec == '15<100%':\n        return 15\n    elif spec == '16<100%':\n        return 16\n    elif spec == '17<100%':\n        return 17\n    elif spec == '18<100%':"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"0\":\n        return 0\n    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    if spec.endswith(\"<\"):\n        return int(spec[:-1])\n    return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.startswith(\"<\"):\n        return int(num_clauses * float(spec[1:]))\n    elif spec.endswith(\"%\"):\n        return int(num_clauses * float(spec[:-1]) / 100)\n    else:\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith('%'):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    elif '<' in spec:\n        return int(spec.split('<')[1])\n    else:\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == '':\n        return 0\n    elif spec.endswith('%'):\n        return int(float(spec[:-1]) * num_clauses / 100)\n    elif spec.endswith('<'):\n        return int(spec[:-1])\n    else:\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.startswith(\"<\"):\n        return int(float(spec.split(\"<\")[1]) * num_clauses)\n    elif spec.endswith(\"%\"):\n        return int(float(spec.split(\"%\")[0]) / 100 * num_clauses)\n    else:\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec into individual parts\n    parts = spec.split()\n\n    # Initialize the minimum number of clauses to match to 0\n    min_should_match = 0\n\n    # Iterate over the parts of the spec\n    for part in parts:\n        # Check if the part is a percentage\n        if '%' in part:\n            # Calculate the minimum number of clauses based on the percentage\n            min_should_match += int(float(part[:-1]) / 100 * num_clauses)\n        # Check if the part is a conditional expression\n        elif '<' in part:\n            # Split the part into the condition and the value\n            condition, value = part.split('<')\n            # Calculate the minimum number of clauses based on the condition\n            if condition == '':\n                min_should_match = num_clauses - int(value)\n            elif condition == '+':\n                min_should_match = int(value)\n            elif condition == '-':\n                min_should_match = num_clauses - int(value)\n        # Otherwise, the part is an absolute number\n        else:\n            # Add the absolute number to the minimum number of clauses\n            min_should_match += int(part)\n\n    # Return the minimum number of clauses that must match\n    return min_should_match"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == '1':\n        return 1\n\n    if spec.endswith('%'):\n        percentage = int(spec[:-1])\n        return int(num_clauses * percentage / 100)\n\n    if '<' in spec:\n        parts = spec.split('<')\n        if len(parts) != 2:\n            raise ValueError(f\"Invalid 'min should match' specification: {spec}\")\n        num_clauses_threshold, percentage = parts\n        if num_clauses_threshold.endswith('%'):\n            num_clauses_threshold = int(num_clauses_threshold[:-1])\n            num_clauses_threshold = int(num_clauses * num_clauses_threshold / 100)\n        else:\n            num_clauses_threshold = int(num_clauses_threshold)\n        percentage = int(percentage[:-1])\n        return int(num_clauses * percentage / 100) if num_clauses > num_clauses_threshold else 1\n\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec into individual parts\n    parts = spec.split()\n\n    # Initialize the minimum number of clauses to match\n    min_should_match = 0\n\n    # Iterate over each part of the spec\n    for part in parts:\n        # If the part is a percentage, calculate the corresponding number of clauses\n        if '%' in part:\n            percentage = int(part.replace('%', ''))\n            min_should_match += int(num_clauses * percentage / 100)\n        # If the part is a conditional expression, calculate the corresponding number of clauses based on the condition\n        elif '<' in part:\n            condition, value = part.split('<')\n            if condition == 'numClauses':\n                min_should_match = int(value)\n        # If the part is an absolute number, use it as the minimum number of clauses to match\n        else:\n            min_should_match = int(part)\n\n    # Return the calculated minimum number of clauses to match\n    return min_should_match"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"0\":\n        return 0\n\n    if spec.endswith(\"%\"):\n        return int(num_clauses * float(spec[:-1]) / 100)\n\n    if \"<\" in spec:\n        parts = spec.split(\"<\")\n        if len(parts) != 2:\n            raise ValueError(f\"Invalid 'mm' spec: {spec}\")\n        min_clauses, max_clauses = parts\n        if min_clauses.endswith(\"%\"):\n            min_clauses = int(num_clauses * float(min_clauses[:-1]) / 100)\n        else:\n            min_clauses = int(min_clauses)\n        if max_clauses.endswith(\"%\"):\n            max_clauses = int(num_clauses * float(max_clauses[:-1]) / 100)\n        else:\n            max_clauses = int(max_clauses)\n        return min(max_clauses, min_clauses)\n\n    return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec string into its components\n    components = spec.split(' ')\n\n    # Initialize the minimum number of clauses to match to the total number of clauses\n    min_clauses_to_match = num_clauses\n\n    # Iterate over the components of the spec\n    for component in components:\n        # If the component is a percentage, calculate the minimum number of clauses based on the percentage\n        if '%' in component:\n            percentage = float(component.replace('%', ''))\n            min_clauses_to_match = int(num_clauses * percentage / 100)\n        # If the component is an absolute number, set the minimum number of clauses to match to that number\n        elif component.isdigit():\n            min_clauses_to_match = int(component)\n        # If the component is a conditional expression, set the minimum number of clauses to match to the absolute number specified in the condition\n        elif '<' in component:\n            condition, value = component.split('<')\n            if condition.isdigit():\n                min_clauses_to_match = int(condition)\n            elif condition.endswith('%'):\n                percentage = float(condition.replace('%', ''))\n                min_clauses_to_match = int(num_clauses * percentage / 100)\n        # If the component is not a valid spec, raise an exception\n        else:\n            raise ValueError(f\"Invalid 'min should match' spec: {spec}\")\n\n    # Return the calculated minimum number of clauses to match\n    return min_clauses_to_match"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec into its components\n    spec_components = spec.split(\" \")\n\n    # If the spec is a percentage, calculate the minimum number of clauses based on the percentage\n    if \"%\" in spec_components:\n        percentage = float(spec_components[0])\n        min_should_match = int(percentage * num_clauses)\n\n    # If the spec is an absolute number, use that number as the minimum number of clauses\n    else:\n        min_should_match = int(spec_components[0])\n\n    # If the spec contains a conditional expression, apply it to the minimum number of clauses\n    if \"<\" in spec_components:\n        condition_components = spec_components[1].split(\":\")\n        condition_field = condition_components[0]\n        condition_value = condition_components[1]\n        if condition_field == \"numTerms\" and condition_value == \"max\":\n            min_should_match = min(min_should_match, num_clauses)\n\n    return min_should_match"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec into its components\n    spec_components = spec.split()\n\n    # If the spec is empty, return 1\n    if not spec_components:\n        return 1\n\n    # If the spec is a number, return that number\n    if spec_components[0].isdigit():\n        return int(spec_components[0])\n\n    # If the spec is a percentage, calculate the corresponding number of clauses\n    if spec_components[0].endswith('%'):\n        percentage = int(spec_components[0][:-1])\n        return int((num_clauses * percentage) / 100)\n\n    # If the spec is a conditional expression, calculate the corresponding number of clauses\n    if spec_components[0].endswith('<'):\n        if spec_components[1].isdigit():\n            return int(spec_components[1])\n        elif spec_components[1].endswith('%'):\n            percentage = int(spec_components[1][:-1])\n            return int((num_clauses * percentage) / 100)\n\n    # If the spec is not recognized, return 1\n    return 1\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec into parts\n    parts = spec.split()\n\n    # Initialize the minimum number of clauses that must match\n    min_should_match = 0\n\n    # Iterate over the parts\n    for part in parts:\n        # Check if the part is a percentage\n        if \"%\" in part:\n            # Calculate the minimum number of clauses that must match based on the percentage\n            min_should_match += int(float(part[:-1]) * num_clauses / 100)\n        # Check if the part is a conditional expression\n        elif \"<\" in part:\n            # Split the part into the condition and the number\n            condition, number = part.split(\"<\")\n            # Calculate the minimum number of clauses that must match based on the condition\n            if condition == \"\":\n                min_should_match += int(number)\n            elif condition == \"!\":\n                min_should_match = num_clauses - int(number)\n        # Otherwise, the part is an absolute number\n        else:\n            # Add the absolute number to the minimum number of clauses that must match\n            min_should_match += int(part)\n\n    # Return the minimum number of clauses that must match\n    return min_should_match\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec into parts using '<' as the delimiter\n    parts = spec.split('<')\n\n    # If there is only one part, it is either an absolute number or a percentage\n    if len(parts) == 1:\n        if '%' in parts[0]:\n            # If the part ends with '%', it is a percentage\n            percentage = int(parts[0][:-1])\n            return int((num_clauses * percentage) / 100)\n        else:\n            # Otherwise, it is an absolute number\n            return int(parts[0])\n\n    # If there are multiple parts, the first part is the minimum number of clauses, and the second part is the condition\n    min_clauses = int(parts[0])\n    condition = parts[1]\n\n    # If the condition is '<', the minimum number of clauses must be greater than the total number of clauses\n    if condition == '<':\n        return num_clauses + 1\n\n    # If the condition is a percentage, calculate the minimum number of clauses based on the percentage\n    elif '%' in condition:\n        percentage = int(condition[:-1])\n        return int((num_clauses * percentage) / 100)\n\n    # Otherwise, the condition is an absolute number, and the minimum number of clauses must be greater than or equal to it\n    else:\n        return max(min_clauses, num_clauses)\n\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_unique(tokens)\n        else:\n            return self._phrase_freq_non_unique(tokens, slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_slop1(tokens)\n        else:\n            return self.phrase_freq_slop(tokens, slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_direct(tokens)\n        else:\n            return self._phrase_freq_slop(tokens, slop)\n    "}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_direct(tokens)\n        else:\n            return self._phrase_freq_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_1(tokens)\n        else:\n            return self._phrase_freq_non_1(tokens, slop)\n\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_unique_tokens(tokens)\n        else:\n            return self._phrase_freq_non_unique_tokens(tokens, slop)\n    "}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_unique_tokens(tokens)\n        else:\n            return self.phrase_freq_non_unique_tokens(tokens, slop)\n    "}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_slop_1(tokens)\n        else:\n            return self.phrase_freq_slop_n(tokens, slop)\n    "}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_unique(tokens)\n        else:\n            return self.phrase_freq_non_unique(tokens, slop)\n\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_1_slop(tokens)\n        else:\n            return self.phrase_freq_slop(tokens, slop)\n    "}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_direct(tokens)\n        else:\n            return self._phrase_freq_indirect(tokens, slop)\n    "}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_unique_tokens(tokens)\n        return self._phrase_freq_non_unique_tokens(tokens, slop)\n    "}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_slop_1(tokens)\n        else:\n            return self._phrase_freq_slop_gt_1(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            # If the slop is 1 and all tokens are unique, we can calculate the phrase frequencies directly\n            return self.phrase_freq_unique_tokens(tokens)\n        else:\n            # Otherwise, we delegate the calculation to another method\n            return self.phrase_freq_non_unique_tokens(tokens, slop)\n    "}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_slop(tokens, slop)\n    "}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_direct(tokens)\n        else:\n            return self._phrase_freq_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_slop_one(tokens)\n        else:\n            return self.phrase_freq_slop_n(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_slop_1(tokens)\n        else:\n            return self._phrase_freq_slop_n(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_direct(tokens)\n        else:\n            return self._phrase_freq_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_1(tokens)\n        else:\n            return self.phrase_freq_slop(tokens, slop)\n\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = truncate_to_fit(array)\n\n        term_matrix, positions, term_dict, doc_lengths = index_array(array, tokenizer, batch_size, avoid_copies)\n\n        return cls(term_matrix, positions, term_dict, doc_lengths)"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = list(array)\n            if len(array) > batch_size:\n                array = array[:batch_size]\n\n        if avoid_copies:\n            array = np.array(array, dtype=object)\n\n        term_matrix, positions, term_dict, avg_doc_len, doc_lens = cls.build_index(array, tokenizer)\n        return cls(term_matrix, positions, term_dict, avg_doc_len, doc_lens)"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Create a new instance of SearchArray to hold the indexed data\n        search_array = cls()\n\n        # Handle large arrays by processing them in batches if necessary\n        if len(array) > batch_size:\n            batches = [array[i:i + batch_size] for i in range(0, len(array), batch_size)]\n        else:\n            batches = [array]\n\n        # Process each batch of data and update the indexed data in the SearchArray instance\n        for batch in batches:\n            search_array.update(batch, tokenizer, truncate, avoid_copies)\n\n        # Return the SearchArray instance containing the indexed data\n        return search_array"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check if the input array is empty\n        if not array:\n            raise ValueError(\"Input array is empty\")\n\n        # Check if the tokenizer is callable\n        if not callable(tokenizer):\n            raise ValueError(\"Invalid tokenizer\")\n\n        # Check if the batch size is valid\n        if batch_size <= 0:\n            raise ValueError(\"Invalid batch size\")\n\n        # Check if the input array is a list\n        if not isinstance(array, list):\n            raise ValueError(\"Input array must be a list\")\n\n        # Check if the input array is a list of strings\n        if not all(isinstance(item, str) for item in array):\n            raise ValueError(\"Input array must be a list of strings\")\n\n        # Check if the tokenizer returns a list of tokens\n        if not isinstance(tokenizer(array[0]), list):\n            raise ValueError(\"Tokenizer must return a list of tokens\")\n\n        # Check if the tokenizer returns a list of strings\n        if not all(isinstance(token, str) for token in tokenizer(array[0])):\n            raise ValueError(\"Tokenizer must return a list of strings\")\n\n        # Check if the tokenizer returns a list of tokens\n        if not isinstance(tokenizer(array[0]), list):\n            raise ValueError(\"Tokenizer must return a list of tokens\")\n\n        # Check if the tokenizer returns a list of strings\n        if not all(isinstance(token, str) for token in tokenizer(array[0])):\n            raise ValueError(\"Tokenizer must return a list of strings\")\n\n        # Check if the tokenizer returns a list of tokens\n        if not isinstance(tokenizer(array[0]), list):\n            raise ValueError(\"Tokenizer must return a list of tokens\")\n\n        # Check if the tokenizer returns a list of strings\n        if not all(isinstance(token, str) for token in tokenizer(array[0])):\n            raise ValueError(\"Tokenizer must return a list of strings\")\n\n        # Check if the tokenizer returns a list of tokens\n        if not isinstance(tokenizer(array[0]), list"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Create a new instance of SearchArray\n        search_array = cls()\n\n        # Initialize the term dictionary and term matrix\n        search_array.term_dict = {}\n        search_array.term_matrix = defaultdict(list)\n\n        # Initialize the document lengths\n        search_array.document_lengths = []\n\n        # Initialize the average document length\n        search_array.average_document_length = 0\n\n        # Initialize the positions\n        search_array.positions = defaultdict(list)\n\n        # Initialize the document ids\n        search_array.document_ids = []\n\n        # Initialize the number of documents\n        search_array.num_documents = 0\n\n        # Initialize the number of terms\n        search_array.num_terms = 0\n\n        # Initialize the number of terms\n        search_array.num_terms = 0\n\n        # Initialize the number of terms\n        search_array.num_terms = 0\n\n        # Initialize the number of terms\n        search_array.num_terms = 0\n\n        # Initialize the number of terms\n        search_array.num_terms = 0\n\n        # Initialize the number of terms\n        search_array.num_terms = 0\n\n        # Initialize the number of terms\n        search_array.num_terms = 0\n\n        # Initialize the number of terms\n        search_array.num_terms = 0\n\n        # Initialize the number of terms\n        search_array.num_terms = 0\n\n        # Initialize the number of terms\n        search_array.num_terms = 0\n\n        # Initialize the number of terms\n        search_array.num_terms = 0\n\n        # Initialize the number of terms\n        search_array.num_terms = 0\n\n        # Initialize the number of terms\n        search_array.num_terms = 0\n\n        # Initialize the number of terms\n        search_array.num_terms = 0\n\n        # Initialize the number of terms\n        search_array.num_terms = 0\n\n        # Initialize the number of terms\n        search_array.num_"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not isinstance(array, Iterable):\n            raise ValueError(\"array must be an iterable\")\n\n        if not callable(tokenizer):\n            raise ValueError(\"tokenizer must be a callable function\")\n\n        if not isinstance(truncate, bool):\n            raise ValueError(\"truncate must be a boolean\")\n\n        if not isinstance(batch_size, int):\n            raise ValueError(\"batch_size must be an integer\")\n\n        if not isinstance(avoid_copies, bool):\n            raise ValueError(\"avoid_copies must be a boolean\")\n\n        if truncate:\n            array = truncate_array(array)\n\n        if avoid_copies:\n            array = copy_free(array)\n\n        term_matrix, positions, term_dict, avg_doc_len, doc_lens = build_index(array, tokenizer, batch_size)\n\n        return cls(term_matrix, positions, term_dict, avg_doc_len, doc_lens)\n\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check if the array is empty\n        if not array:\n            raise ValueError(\"Array is empty\")\n\n        # Check if the tokenizer is callable\n        if not callable(tokenizer):\n            raise TypeError(\"Tokenizer is not callable\")\n\n        # Check if the batch size is valid\n        if not isinstance(batch_size, int) or batch_size <= 0:\n            raise ValueError(\"Invalid batch size\")\n\n        # Check if the avoid_copies flag is valid\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Avoid copies flag must be a boolean\")\n\n        # Check if the array is a list of strings\n        if not all(isinstance(x, str) for x in array):\n            raise TypeError(\"Array must be a list of strings\")\n\n        # Check if the array is too large to fit within memory constraints\n        if len(array) > 100000000 and not truncate:\n            raise ValueError(\"Array is too large to fit within memory constraints\")\n\n        # Check if the tokenizer is a function\n        if not callable(tokenizer):\n            raise TypeError(\"Tokenizer must be a function\")\n\n        # Check if the tokenizer returns a list of strings\n        if not all(isinstance(x, str) for x in tokenizer(array[0])):\n            raise TypeError(\"Tokenizer must return a list of strings\")\n\n        # Check if the tokenizer returns a list of strings\n        if not all(isinstance(x, str) for x in tokenizer(array[0])):\n            raise TypeError(\"Tokenizer must return a list of strings\")\n\n        # Check if the tokenizer returns a list of strings\n        if not all(isinstance(x, str) for x in tokenizer(array[0])):\n            raise TypeError(\"Tokenizer must return a list of strings\")\n\n        # Check if the tokenizer returns a list of strings\n        if not all(isinstance(x, str) for x in tokenizer(array[0])):\n            raise TypeError(\"Tokenizer must return a list of strings\")\n\n        # Check if the token"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if avoid_copies:\n            array = iter(array)\n        else:\n            array = list(array)\n\n        if truncate:\n            array = list(array)[:100000]\n\n        term_matrix = defaultdict(list)\n        positions = defaultdict(list)\n        term_dictionary = {}\n        doc_lengths = []\n        total_length = 0\n\n        for i, doc in enumerate(array):\n            doc_lengths.append(len(doc))\n            total_length += len(doc)\n            doc_tokens = tokenizer(doc)\n\n            for j, token in enumerate(doc_tokens):\n                if token not in term_dictionary:\n                    term_dictionary[token] = len(term_dictionary)\n                term_matrix[term_dictionary[token]].append(i)\n                positions[term_dictionary[token]].append(j)\n\n        average_doc_length = total_length / len(array)\n        return cls(term_matrix, positions, term_dictionary, average_doc_length, doc_lengths)"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not isinstance(array, Iterable):\n            raise TypeError('array must be iterable')\n\n        if not callable(tokenizer):\n            raise TypeError('tokenizer must be callable')\n\n        if not isinstance(truncate, bool):\n            raise TypeError('truncate must be a bool')\n\n        if not isinstance(batch_size, int):\n            raise TypeError('batch_size must be an int')\n\n        if not isinstance(avoid_copies, bool):\n            raise TypeError('avoid_copies must be a bool')\n\n        if batch_size < 1:\n            raise ValueError('batch_size must be >= 1')\n\n        if truncate:\n            array = truncate_to_fit_memory(array, batch_size)\n\n        if avoid_copies:\n            array = copy_to_contiguous_memory(array)\n\n        term_matrix, positions, term_dict, doc_lengths = build_index(array, tokenizer, batch_size)\n\n        avg_doc_length = np.mean(doc_lengths)\n\n        return cls(term_matrix, positions, term_dict, avg_doc_length, doc_lengths)"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check if the input is an array of strings\n        if not isinstance(array, np.ndarray) or array.dtype != np.dtype('O'):\n            raise ValueError(\"Input array must be an array of strings\")\n\n        # Check if the input array is empty\n        if len(array) == 0:\n            raise ValueError(\"Input array must not be empty\")\n\n        # Check if the tokenizer is callable\n        if not callable(tokenizer):\n            raise ValueError(\"Tokenizer must be callable\")\n\n        # Check if the batch size is an integer\n        if not isinstance(batch_size, int) or batch_size <= 0:\n            raise ValueError(\"Batch size must be a positive integer\")\n\n        # Check if the avoid_copies flag is a boolean\n        if not isinstance(avoid_copies, bool):\n            raise ValueError(\"Avoid copies flag must be a boolean\")\n\n        # Check if the truncate flag is a boolean\n        if not isinstance(truncate, bool):\n            raise ValueError(\"Truncate flag must be a boolean\")\n\n        # Check if the input array is empty\n        if len(array) == 0:\n            raise ValueError(\"Input array must not be empty\")\n\n        # Check if the tokenizer is callable\n        if not callable(tokenizer):\n            raise ValueError(\"Tokenizer must be callable\")\n\n        # Check if the batch size is an integer\n        if not isinstance(batch_size, int) or batch_size <= 0:\n            raise ValueError(\"Batch size must be a positive integer\")\n\n        # Check if the avoid_copies flag is a boolean\n        if not isinstance(avoid_copies, bool):\n            raise ValueError(\"Avoid copies flag must be a boolean\")\n\n        # Check if the truncate flag is a boolean\n        if not isinstance(truncate, bool):\n            raise ValueError(\"Truncate flag must be a boolean\")\n\n        # Check if the input array is empty\n        if len(array) == 0:\n            raise ValueError(\"Input array must not be empty\")\n\n        # Check if the token"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not isinstance(array, Iterable):\n            raise TypeError(\"array must be an iterable\")\n        if not callable(tokenizer):\n            raise TypeError(\"tokenizer must be callable\")\n        if not isinstance(truncate, bool):\n            raise TypeError(\"truncate must be a bool\")\n        if not isinstance(batch_size, int):\n            raise TypeError(\"batch_size must be an int\")\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"avoid_copies must be a bool\")\n\n        if avoid_copies:\n            array = np.array(array, dtype=object)\n        else:\n            array = np.array(array, dtype=str)\n\n        if truncate:\n            array = array[:batch_size]\n\n        term_matrix, positions, term_dict, avg_doc_length, doc_lengths = cls.build_index(array, tokenizer)\n\n        return cls(term_matrix, positions, term_dict, avg_doc_length, doc_lengths)\n\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if avoid_copies:\n            array = array.copy()\n\n        if truncate:\n            array = array[:100000000]\n\n        term_matrix = None\n        positions = None\n        term_dict = {}\n        doc_lengths = []\n        doc_length = 0\n        for i, doc in enumerate(array):\n            doc_length += len(doc)\n            doc_lengths.append(doc_length)\n            tokens = tokenizer(doc)\n            for j, token in enumerate(tokens):\n                if token not in term_dict:\n                    term_dict[token] = len(term_dict)\n                if term_matrix is None:\n                    term_matrix = np.zeros((len(term_dict), len(array)))\n                    positions = np.zeros((len(term_dict), len(array)), dtype=np.int32)\n                term_matrix[term_dict[token], i] += 1\n                positions[term_dict[token], i] = j\n            if (i + 1) % batch_size == 0:\n                print(f'Processed {i + 1} documents')\n\n        if term_matrix is None:\n            term_matrix = np.zeros((len(term_dict), len(array)))\n            positions = np.zeros((len(term_dict), len(array)), dtype=np.int32)\n\n        return cls(term_matrix, positions, term_dict, np.mean(doc_lengths), doc_lengths)"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Create an instance of SearchArray\n        search_array = cls()\n\n        # Iterate over the array in batches\n        for i in range(0, len(array), batch_size):\n            batch = array[i:i + batch_size]\n\n            # Tokenize the batch\n            batch = [tokenizer(x) for x in batch]\n\n            # Compute the term matrix\n            term_matrix = compute_term_matrix(batch)\n\n            # Compute the positions\n            positions = compute_positions(batch)\n\n            # Compute the term dictionary\n            term_dict = compute_term_dict(batch)\n\n            # Compute the average document length\n            avg_doc_length = compute_avg_doc_length(term_matrix)\n\n            # Compute the document lengths\n            doc_lengths = compute_doc_lengths(term_matrix)\n\n            # Add the data to the SearchArray instance\n            search_array.add_data(term_matrix, positions, term_dict, avg_doc_length, doc_lengths)\n\n        # Truncate the data if specified\n        if truncate:\n            search_array.truncate()\n\n        # Avoid making copies of the data if specified\n        if avoid_copies:\n            search_array.avoid_copies()\n\n        # Return the SearchArray instance\n        return search_array"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not isinstance(array, Iterable):\n            raise TypeError(\"array must be an iterable\")\n        if not callable(tokenizer):\n            raise TypeError(\"tokenizer must be callable\")\n        if not isinstance(truncate, bool):\n            raise TypeError(\"truncate must be a bool\")\n        if not isinstance(batch_size, int):\n            raise TypeError(\"batch_size must be an int\")\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"avoid_copies must be a bool\")\n\n        # Create an instance of SearchArray to hold the indexed data\n        sa = cls()\n\n        # Initialize the term dictionary and term matrix\n        sa.term_dict = {}\n        sa.term_matrix = np.zeros((0, 0), dtype=np.int32)\n\n        # Initialize the document lengths and average document length\n        sa.doc_lengths = np.zeros(0, dtype=np.int32)\n        sa.avg_doc_length = 0\n\n        # Initialize the positions array\n        sa.positions = np.zeros(0, dtype=np.int32)\n\n        # Initialize the number of documents\n        sa.num_docs = 0\n\n        # Initialize the number of terms\n        sa.num_terms = 0\n\n        # Initialize the number of tokens\n        sa.num_tokens = 0\n\n        # Initialize the number of batches\n        sa.num_batches = 0\n\n        # Initialize the current batch number\n        sa.cur_batch = 0\n\n        # Initialize the current batch size\n        sa.batch_size = batch_size\n\n        # Initialize the current batch data\n        sa.cur_batch_data = []\n\n        # Initialize the current batch positions\n        sa.cur_batch_positions = []\n\n        # Initialize the current batch term matrix\n        sa.cur_batch_term_matrix = None\n\n        # Initialize the current batch term dictionary\n        sa.cur_batch_term_dict = {}\n\n        # Initialize"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not isinstance(array, Iterable):\n            raise ValueError('array must be iterable')\n\n        if not callable(tokenizer):\n            raise ValueError('tokenizer must be callable')\n\n        if not isinstance(truncate, bool):\n            raise ValueError('truncate must be bool')\n\n        if not isinstance(batch_size, int):\n            raise ValueError('batch_size must be int')\n\n        if not isinstance(avoid_copies, bool):\n            raise ValueError('avoid_copies must be bool')\n\n        # Create a new instance of SearchArray\n        search_array = cls()\n\n        # Initialize the term dictionary and term matrix\n        search_array.term_dict = {}\n        search_array.term_matrix = np.zeros((0, 0), dtype=np.int32)\n\n        # Initialize the positions and document lengths lists\n        search_array.positions = []\n        search_array.doc_lengths = []\n\n        # Initialize the average document length\n        search_array.avg_doc_length = 0\n\n        # Initialize the number of documents processed\n        num_docs = 0\n\n        # Process the array in batches\n        for i in range(0, len(array), batch_size):\n            # Get the batch of strings\n            batch = array[i:i+batch_size]\n\n            # Process the batch\n            search_array._process_batch(batch, tokenizer, avoid_copies)\n\n            # Update the number of documents processed\n            num_docs += len(batch)\n\n            # Update the average document length\n            search_array.avg_doc_length = search_array.avg_doc_length * (num_docs - len(batch)) / num_docs + len(batch) / num_docs\n\n            # Check if the data needs to be truncated\n            if truncate and search_array.term_matrix.nbytes > MAX_MEMORY:\n                # Truncate the data\n                search_array.term_matrix = search_array.term_matrix[:, :len(search_array"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Create an instance of SearchArray\n        sa = cls()\n\n        # Initialize the term matrix\n        sa.term_matrix = defaultdict(list)\n\n        # Initialize the term dictionary\n        sa.term_dict = {}\n\n        # Initialize the document lengths\n        sa.doc_lengths = []\n\n        # Initialize the positions\n        sa.positions = []\n\n        # Initialize the total number of documents\n        sa.total_docs = 0\n\n        # Initialize the total number of terms\n        sa.total_terms = 0\n\n        # Initialize the total number of tokens\n        sa.total_tokens = 0\n\n        # Initialize the average document length\n        sa.avg_doc_length = 0.0\n\n        # Initialize the current document index\n        sa.doc_index = 0\n\n        # Initialize the current term index\n        sa.term_index = 0\n\n        # Initialize the current token index\n        sa.token_index = 0\n\n        # Initialize the current term count\n        sa.term_count = 0\n\n        # Initialize the current document length\n        sa.doc_length = 0\n\n        # Initialize the current term\n        sa.current_term = None\n\n        # Initialize the current token\n        sa.current_token = None\n\n        # Initialize the current position\n        sa.current_position = 0\n\n        # Initialize the current term frequency\n        sa.current_tf = 0\n\n        # Initialize the current document frequency\n        sa.current_df = 0\n\n        # Initialize the current document id\n        sa.current_doc_id = 0\n\n        # Initialize the current term id\n        sa.current_term_id = 0\n\n        # Initialize the current token id\n        sa.current_token_id = 0\n\n        # Initialize the current document length\n        sa.current_doc_length = 0\n\n        # Initialize the current term frequency\n        sa.current_term_frequency = 0\n\n        # Initialize the current document frequency\n        sa.current_document_frequency = 0\n\n        # Initialize the current term count"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not isinstance(array, Iterable):\n            raise TypeError(f\"array must be an iterable, not {type(array)}\")\n\n        if not callable(tokenizer):\n            raise TypeError(f\"tokenizer must be callable, not {type(tokenizer)}\")\n\n        if not isinstance(truncate, bool):\n            raise TypeError(f\"truncate must be a bool, not {type(truncate)}\")\n\n        if not isinstance(batch_size, int):\n            raise TypeError(f\"batch_size must be an int, not {type(batch_size)}\")\n\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(f\"avoid_copies must be a bool, not {type(avoid_copies)}\")\n\n        if batch_size <= 0:\n            raise ValueError(f\"batch_size must be positive, not {batch_size}\")\n\n        # Convert the array to a list if it is not already a list\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Create an empty list to hold the term matrix\n        term_matrix = []\n\n        # Create an empty list to hold the positions\n        positions = []\n\n        # Create an empty dictionary to hold the term dictionary\n        term_dict = {}\n\n        # Create a list to hold the document lengths\n        doc_lengths = []\n\n        # Create a variable to hold the total number of tokens\n        total_tokens = 0\n\n        # Create a variable to hold the total number of documents\n        total_docs = 0\n\n        # Create a variable to hold the average document length\n        avg_doc_length = 0\n\n        # Iterate over the array in batches\n        for batch in batch_generator(array, batch_size):\n            # Iterate over the batch\n            for doc_id, doc in enumerate(batch):\n                # Tokenize the document\n                doc_tokens = tokenizer(doc)\n\n                # Add the tokens to the term matrix\n                term_matrix.append(doc_tokens)\n\n                #"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = list(array)\n            if len(array) > batch_size:\n                array = array[:batch_size]\n\n        if avoid_copies:\n            array = iter(array)\n\n        term_matrix, positions, term_dict, doc_lengths = cls.build_index(array, tokenizer, batch_size)\n        average_doc_length = sum(doc_lengths) / len(doc_lengths)\n\n        return cls(term_matrix, positions, term_dict, average_doc_length, doc_lengths)\n\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if avoid_copies:\n            array = copy_on_write_list(array)\n\n        if truncate:\n            array = truncate_array(array)\n\n        if len(array) == 0:\n            raise ValueError(\"The array is empty.\")\n\n        if not isinstance(array, Iterable):\n            raise ValueError(\"The array must be an iterable.\")\n\n        if not callable(tokenizer):\n            raise ValueError(\"The tokenizer must be callable.\")\n\n        if not isinstance(batch_size, int):\n            raise ValueError(\"The batch size must be an integer.\")\n\n        if batch_size <= 0:\n            raise ValueError(\"The batch size must be positive.\")\n\n        if not isinstance(avoid_copies, bool):\n            raise ValueError(\"Avoid copies must be a boolean.\")\n\n        # Initialize an empty SearchArray instance\n        search_array = cls()\n\n        # Create an empty dictionary to store the term matrix\n        term_matrix = {}\n\n        # Create an empty dictionary to store the positions\n        positions = {}\n\n        # Create an empty dictionary to store the term dictionary\n        term_dict = {}\n\n        # Initialize the document length counter\n        doc_len = 0\n\n        # Initialize the average document length counter\n        avg_doc_len = 0\n\n        # Initialize the document length list\n        doc_len_list = []\n\n        # Initialize the term counter\n        term_count = 0\n\n        # Iterate through the array in batches\n        for i in range(0, len(array), batch_size):\n            # Get the batch of strings\n            batch = array[i:i + batch_size]\n\n            # Iterate through the batch of strings\n            for j, string in enumerate(batch):\n                # Tokenize the string\n                tokens = tokenizer(string)\n\n                # Iterate through the tokens\n                for k, token in enumerate(tokens):\n                    # Add the token to the term matrix\n                    if token not in term_matrix:\n                        term_matrix[token] = []\n                    term_matrix[token].append(j"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Get the number of documents in the array\n        n_docs = len(array)\n        # Initialize the document lengths and term dictionary\n        doc_lengths = np.zeros(n_docs, dtype=np.int32)\n        term_dict = {}\n        # Initialize the term matrix and positions\n        term_matrix = None\n        positions = None\n        # Initialize the total number of terms\n        total_terms = 0\n        # Initialize the total number of tokens\n        total_tokens = 0\n        # Initialize the document index\n        doc_idx = 0\n        # Iterate over the array in batches\n        for batch in batches(array, batch_size):\n            # Tokenize the batch of strings\n            tokens = tokenizer(batch)\n            # Update the total number of tokens\n            total_tokens += len(tokens)\n            # Get the number of documents in the batch\n            n_docs_in_batch = len(batch)\n            # Initialize the term matrix for the batch\n            term_matrix_batch = None\n            # Initialize the positions for the batch\n            positions_batch = None\n            # Initialize the document index for the batch\n            doc_idx_batch = 0\n            # Iterate over the batch of strings\n            for doc, tokens_in_doc in zip(batch, tokens):\n                # Get the document index\n                doc_idx = doc_idx_batch + doc_idx\n                # Get the document length\n                doc_length = len(tokens_in_doc)\n                # Update the document lengths array\n                doc_lengths[doc_idx] = doc_length\n                # Initialize the term dictionary for the document\n                term_dict_doc = {}\n                # Initialize the term matrix for the document\n                term_matrix_doc = None\n                # Initialize the positions for the document\n                positions_doc = None\n                # Initialize the term index\n                term_idx = 0\n                # Iterate over the tokens in the document\n                for token in tokens_in_doc:\n                    # Check if the term is already in the term dictionary\n                    if token in term_dict_doc:\n                        # Get the"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Set up the server with specific configurations\n        self.server = socketserver.TCPServer((self.config.get(\"ip\"), self.config.get(\"port\")), ProxifierRequestHandler)\n        self.server.allow_reuse_address = True\n        self.server.timeout = self.config.get(\"timeout\")\n        self.server.max_connections = self.config.get(\"max_connections\")\n        self.server.max_connections_per_ip = self.config.get(\"max_connections_per_ip\")\n        self.server.max_connections_per_ip_per_port = self.config.get(\"max_connections_per_ip_per_port\")\n        self.server.max_connections_per_ip_per_port_per_protocol = self.config.get(\"max_connections_per_ip_per_port_per_protocol\")\n        self.server.max_connections_per_ip_per_port_per_protocol_per_method = self.config.get(\"max_connections_per_ip_per_port_per_protocol_per_method\")\n        self.server.max_connections_per_ip_per_port_per_protocol_per_method_per_path = self.config.get(\"max_connections_per_ip_per_port_per_protocol_per_method_per_path\")\n        self.server.max_connections_per_ip_per_port_per_protocol_per_method_per_path_per_query = self.config.get(\"max_connections_per_ip_per_port_per_protocol_per_method_per_path_per_query\")\n        self.server.max_connections_per_ip_per_port_per_protocol_per_method_per_path_per_query_per_fragment = self.config.get(\"max_connections_per_ip_per_port_per_protocol_per_method_per_path_per_query_per_fragment\")\n        self.server.max_connections_per_ip_per_port_per_protocol_"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Initializing ProxifierMessageInterceptor\")\n        self.server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n        self.server.add_insecure_port(f\"{self.config.get('PROXIFIER_MESSAGE_INTERCEPTOR_HOST')}:{self.config.get('PROXIFIER_MESSAGE_INTERCEPTOR_PORT')}\")\n        ProxifierMessageInterceptorService.add_ProxifierMessageInterceptorServiceServicer_to_server(self, self.server)\n        self.server.start()\n        self.logger.info(\"ProxifierMessageInterceptor initialized\")\n\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Initializing ProxifierMessageInterceptor\")\n        self.server = ProxifierMessageInterceptorServer(self.config)\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.logger.info(\"ProxifierMessageInterceptor initialized\")\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Initializing ProxifierMessageInterceptor\")\n        self.logger.info(\"Starting ProxifierMessageInterceptor\")\n        self.server = ProxifierMessageInterceptor.Server(self.config.get_proxifier_message_interceptor_port(), self.config.get_proxifier_message_interceptor_host(), self.config.get_proxifier_message_interceptor_max_connections(), self.config.get_proxifier_message_interceptor_max_buffer_size(), self.config.get_proxifier_message_interceptor_timeout(), self.config.get_proxifier_message_interceptor_max_timeout_retries(), self.config.get_proxifier_message_interceptor_max_timeout_retries_interval(), self.config.get_proxifier_message_interceptor_max_timeout_retries_backoff_factor(), self.config.get_proxifier_message_interceptor_max_timeout_retries_max_interval(), self.config.get_proxifier_message_interceptor_max_timeout_retries_jitter(), self.config.get_proxifier_message_interceptor_max_timeout_retries_jitter_factor(), self.config.get_proxifier_message_interceptor_max_timeout_retries_jitter_factor_max(), self.config.get_proxifier_message_interceptor_max_timeout_retries_jitter_factor_min(), self.config.get_proxifier_message_interceptor_max_timeout_retries_jitter_factor_min(), self.config.get_proxifier_message_interceptor_max_timeout_retries_jitter_factor_min(), self.config.get_proxifier_message_interceptor_max_timeout_retries_jitter_factor_min(), self.config.get_proxifier_message_interceptor_max_timeout_retries_jitter_factor_min(), self.config.get_proxifier_message_intercept"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Initializing ProxifierMessageInterceptor\")\n        self.server = socketserver.TCPServer((self.config.get(\"server\", \"host\"), int(self.config.get(\"server\", \"port\"))), ProxifierMessageInterceptorHandler)\n        self.server.connections = {}\n        self.server.lock = threading.Lock()\n        self.server.logger = self.logger\n        self.server.config = self.config\n        self.server.start()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Starting ProxifierMessageInterceptor\")\n        self.server = ProxifierMessageInterceptorServer(self.config, self.logger)\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.logger.info(\"ProxifierMessageInterceptor started\")\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Set up the server\n        self.server = ProxifierServer(\n            self.config.get(\"server_host\"),\n            self.config.get(\"server_port\"),\n            self.config.get(\"server_timeout\"),\n            self.config.get(\"server_max_connections\"),\n            self.config.get(\"server_max_buffer_size\"),\n            self.config.get(\"server_max_message_size\"),\n            self.config.get(\"server_max_threads\"),\n            self.config.get(\"server_max_workers\"),\n            self.config.get(\"server_max_queue\"),\n            self.config.get(\"server_max_messages\"),\n            self.config.get(\"server_max_messages_per_connection\"),\n            self.config.get(\"server_max_messages_per_second\"),\n            self.config.get(\"server_max_messages_per_connection_per_second\"),\n            self.config.get(\"server_max_messages_per_second_per_connection\"),\n            self.config.get(\"server_max_messages_per_second_per_connection_per_second\"),\n            self.config.get(\"server_max_messages_per_second_per_connection_per_second_per_connection\"),\n            self.config.get(\"server_max_messages_per_second_per_connection_per_second_per_connection_per_connection\"),\n            self.config.get(\"server_max_messages_per_second_per_connection_per_second_per_connection_per_connection_per_connection\"),\n            self.config.get(\"server_max_messages_per_second_per_connection_per_second_per_connection_per_connection_per_connection_per_connection\"),\n            self.config.get(\"server_max_messages_per_second_per_connection_per_second_per_connection_per_connection_per_connection_per_connection_per_connection\"),\n            self.config.get(\"server_max_messages_per_second_per_connection_per_second_per_connection_per_connection_per_connection_per_connection_per_connection"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Initializing Proxifier Message Interceptor\")\n        self.server = ProxifierMessageInterceptorServer(self.config, self.logger)\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.debug(\"Initializing ProxifierMessageInterceptor\")\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.server = ProxifierMessageInterceptorServer(self.config, self.logger, self.connections, self.lock)\n        self.server.start()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Initializing ProxifierMessageInterceptor\")\n        self.server = ProxifierServer(self.config.get(\"server\", {}))\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Initializing ProxifierMessageInterceptor\")\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.server = socketserver.ThreadingTCPServer(\n            (self.config.get(\"ip\"), self.config.get(\"port\")),\n            ProxifierMessageInterceptorHandler,\n        )\n        self.server.allow_reuse_address = True\n        self.server.daemon_threads = True\n        self.server.timeout = 1\n        self.server.start()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Initializing ProxifierMessageInterceptor\")\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.server = self.createServer()\n        self.server.start()\n        self.logger.info(\"ProxifierMessageInterceptor initialized\")\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Starting ProxifierMessageInterceptor\")\n        self.server = ProxifierServer(self.config.get_interceptor_host(), self.config.get_interceptor_port())\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Initializing ProxifierMessageInterceptor\")\n        self.server = ProxifierServer(self.config.get_config_value(\"PROXY_IP\"),\n                                      self.config.get_config_value(\"PROXY_PORT\"),\n                                      self.config.get_config_value(\"PROXY_TIMEOUT\"),\n                                      self.config.get_config_value(\"PROXY_BUFFER_SIZE\"))\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.logger.info(\"ProxifierMessageInterceptor initialized\")\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Initializing the ProxifierMessageInterceptor\")\n        self.server = ProxifierMessageInterceptor.create_server(self.config)\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.logger.info(\"ProxifierMessageInterceptor has been initialized\")\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.debug(\"ProxifierMessageInterceptor.init()\")\n        self.server = ProxifierMessageInterceptorServer(self.config.get_server_address(), self.config.get_server_port(), self.config.get_server_timeout())\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Initializing ProxifierMessageInterceptor\")\n        self.logger.info(\"Setting up server\")\n        self.server = ProxifierMessageInterceptorServer(self.config.get_proxifier_message_interceptor_port(), self.config.get_proxifier_message_interceptor_max_connections(), self.config.get_proxifier_message_interceptor_max_connections_per_ip(), self.config.get_proxifier_message_interceptor_max_connections_per_ip_per_port())\n        self.logger.info(\"Starting server\")\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.logger.info(\"ProxifierMessageInterceptor initialized\")\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Initializing ProxifierMessageInterceptor\")\n        self.server = ProxifierMessageInterceptorServer(self.config.get(\"host\", \"localhost\"),\n                                                        self.config.get(\"port\", 5555),\n                                                        self.config.get(\"timeout\", 5),\n                                                        self.config.get(\"max_connections\", 10))\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Initializing ProxifierMessageInterceptor\")\n        self.server = ProxifierMessageInterceptorServer(self.config, self.logger)\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.debug(\"Initializing ProxifierMessageInterceptor\")\n        self.server = ProxifierMessageInterceptorServer(self.config, self.logger)\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr.astype(np.uint64)\n    arr = (arr & 0x5555555555555555) + ((arr >> 1) & 0x5555555555555555)\n    arr = (arr & 0x3333333333333333) + ((arr >> 2) & 0x3333333333333333)\n    arr = (arr & 0x0f0f0f0f0f0f0f0f) + ((arr >> 4) & 0x0f0f0f0f0f0f0f0f)\n    arr = (arr & 0x00ff00ff00ff00ff) + ((arr >> 8) & 0x00ff00ff00ff00ff)\n    arr = (arr & 0x0000ffff0000ffff) + ((arr >> 16) & 0x0000ffff0000ffff)\n    arr = (arr & 0x00000000ffffffff) + ((arr >> 32) & 0x00000000ffffffff)\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr[:] = [bin(x).count('1') for x in arr]\n    return arr\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    for i in range(len(arr)):\n        arr[i] = bit_count(arr[i])\n    return arr\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize the result array with zeros\n    result = [0] * len(arr)\n\n    # Iterate over the elements of the input array\n    for i in range(len(arr)):\n        # Initialize the count variable to 0\n        count = 0\n        # Iterate over the bits of the current element\n        for j in range(64):\n            # Check if the jth bit is set to 1\n            if arr[i] & (1 << j):\n                # If it is, increment the count\n                count += 1\n        # Store the count in the result array\n        result[i] = count\n\n    # Return the result array\n    return result\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize an empty array to store the bit counts\n    bit_counts = [0] * len(arr)\n\n    # Iterate over each element in the input array\n    for i in range(len(arr)):\n        # Initialize a variable to store the current element\n        num = arr[i]\n\n        # Iterate over each bit in the element\n        for j in range(64):\n            # Check if the current bit is set to 1\n            if num & (1 << j):\n                # If the bit is set to 1, increment the corresponding bit count in the output array\n                bit_counts[i] += 1\n\n    # Return the output array\n    return bit_counts"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize the bit count array with zeros\n    bit_count_arr = [0] * len(arr)\n\n    # Iterate over the elements of the input array\n    for i in range(len(arr)):\n        # Initialize the bit count for the current element\n        bit_count = 0\n\n        # Iterate over the bits of the current element\n        for j in range(64):\n            # Check if the j-th bit is set to 1\n            if (arr[i] >> j) & 1:\n                # Increment the bit count\n                bit_count += 1\n\n        # Store the bit count in the corresponding element of the bit count array\n        bit_count_arr[i] = bit_count\n\n    # Return the bit count array\n    return bit_count_arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Convert the array to a NumPy array\n    arr = np.array(arr, dtype=np.uint64)\n\n    # Calculate the bit count for each element\n    arr = arr.view(np.uint8)\n    arr = np.unpackbits(arr, axis=1)\n    arr = np.sum(arr, axis=1)\n\n    # Return the modified array\n    return arr\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize an empty list to store the bit counts\n    bit_counts = []\n\n    # Loop through each element in the array\n    for element in arr:\n        # Initialize a variable to store the bit count for the current element\n        count = 0\n\n        # Loop through each bit in the element\n        for i in range(64):\n            # Check if the current bit is set to 1\n            if element & (1 << i):\n                # If it is, increment the bit count\n                count += 1\n\n        # Append the bit count to the list of bit counts\n        bit_counts.append(count)\n\n    # Return the list of bit counts\n    return bit_counts"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize the result array with zeros\n    result = [0] * len(arr)\n\n    # Iterate over each element in the input array\n    for i in range(len(arr)):\n        # Initialize a variable to store the bit count for the current element\n        bit_count = 0\n\n        # Iterate over each bit in the element\n        for j in range(64):\n            # Check if the jth bit is set to 1\n            if (arr[i] >> j) & 1:\n                # If the bit is set to 1, increment the bit count\n                bit_count += 1\n\n        # Store the bit count in the result array\n        result[i] = bit_count\n\n    # Return the result array\n    return result\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize the bit count array\n    bit_count = [0] * len(arr)\n\n    # Loop through each element in the array\n    for i in range(len(arr)):\n        # Initialize the count variable to 0\n        count = 0\n        # Loop through each bit in the element\n        for j in range(64):\n            # Check if the jth bit of the element is set to 1\n            if (arr[i] & (1 << j)) != 0:\n                # Increment the count variable\n                count += 1\n        # Store the count in the bit count array\n        bit_count[i] = count\n\n    # Return the bit count array\n    return bit_count"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize the result array\n    result = [0] * len(arr)\n\n    # Iterate over the array and calculate the bit count for each element\n    for i, num in enumerate(arr):\n        # Initialize the count to 0\n        count = 0\n\n        # Iterate over the bits of the number\n        while num:\n            # If the least significant bit is 1, increment the count\n            if num & 1:\n                count += 1\n\n            # Shift the number to the right by 1 bit\n            num >>= 1\n\n        # Store the count in the result array\n        result[i] = count\n\n    # Return the result array\n    return result\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize the output array\n    output = arr.copy()\n\n    # Iterate over the elements of the input array\n    for i in range(len(arr)):\n        # Initialize the bit count to 0\n        bit_count = 0\n\n        # Iterate over the 64 bits of the element\n        for j in range(64):\n            # Check if the jth bit is set to 1\n            if (arr[i] >> j) & 1:\n                # Increment the bit count\n                bit_count += 1\n\n        # Store the bit count in the output array\n        output[i] = bit_count\n\n    # Return the output array\n    return output"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.asarray(arr, dtype=np.uint64)\n    # Shift the array to the right by 1 bit and XOR it with the original array.\n    arr = np.bitwise_xor(arr, np.right_shift(arr, 1))\n    # Shift the array to the right by 2 bits and XOR it with the original array.\n    arr = np.bitwise_xor(arr, np.right_shift(arr, 2))\n    # Shift the array to the right by 4 bits and XOR it with the original array.\n    arr = np.bitwise_xor(arr, np.right_shift(arr, 4))\n    # Shift the array to the right by 8 bits and XOR it with the original array.\n    arr = np.bitwise_xor(arr, np.right_shift(arr, 8))\n    # Shift the array to the right by 16 bits and XOR it with the original array.\n    arr = np.bitwise_xor(arr, np.right_shift(arr, 16))\n    # Shift the array to the right by 32 bits and XOR it with the original array.\n    arr = np.bitwise_xor(arr, np.right_shift(arr, 32))\n    # Return the modified array.\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize a 64-bit integer to store the result of the bitwise AND operation\n    res = 0\n    # Iterate over the elements of the input array\n    for i in range(len(arr)):\n        # Perform a bitwise AND operation between the current element and the previous result\n        res &= arr[i]\n        # Replace the current element with the result of the bitwise AND operation\n        arr[i] = res\n    # Return the modified array\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize the output array with zeros\n    out = np.zeros_like(arr)\n\n    # Iterate over the array\n    for i in range(len(arr)):\n        # Initialize the bit count to zero\n        count = 0\n\n        # Iterate over the 64 bits of the current element\n        for j in range(64):\n            # Check if the jth bit of the current element is set to 1\n            if arr[i] & (1 << j):\n                # Increment the bit count\n                count += 1\n\n        # Store the bit count in the output array\n        out[i] = count\n\n    # Return the output array\n    return out\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Check if the input is a 64-bit integer array\n    if not isinstance(arr, np.ndarray) or arr.dtype != np.uint64:\n        raise TypeError(\"Input must be a 64-bit integer array\")\n\n    # Initialize the result array\n    result = np.zeros_like(arr, dtype=np.int64)\n\n    # Iterate over the input array and calculate the bit count for each element\n    for i in range(arr.shape[0]):\n        x = arr[i]\n        count = 0\n        while x != 0:\n            count += 1\n            x &= x - 1\n        result[i] = count\n\n    return result"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize a counter to keep track of the number of bits set to 1 in each element\n    count = 0\n\n    # Iterate over each element in the array\n    for i in range(len(arr)):\n        # Initialize a temporary variable to store the current element\n        temp = arr[i]\n\n        # Iterate over each bit in the element\n        for j in range(64):\n            # Check if the current bit is set to 1\n            if temp & 1:\n                # If it is, increment the counter\n                count += 1\n            # Shift the element to the right by 1 bit\n            temp >>= 1\n\n        # Replace the current element with its bit count\n        arr[i] = count\n        # Reset the counter to 0 for the next element\n        count = 0\n\n    # Return the modified array\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize the result array\n    result = [0] * len(arr)\n\n    # Loop through the input array\n    for i, num in enumerate(arr):\n        # Initialize the bit count for the current number\n        count = 0\n\n        # Loop through the bits of the number\n        while num:\n            # Increment the bit count if the least significant bit is 1\n            count += num & 1\n\n            # Right shift the number to check the next bit\n            num >>= 1\n\n        # Store the bit count in the result array\n        result[i] = count\n\n    # Return the result array\n    return result\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize a variable to store the number of bits set to 1 in each element of the array\n    count = 0\n\n    # Iterate through each element of the array\n    for i in range(len(arr)):\n        # Initialize a variable to store the current element of the array\n        current_element = arr[i]\n\n        # Count the number of bits set to 1 in the current element\n        while current_element != 0:\n            current_element &= (current_element - 1)\n            count += 1\n\n        # Set the current element of the array to the count of bits set to 1 in the current element\n        arr[i] = count\n\n        # Reset the count variable for the next element\n        count = 0\n\n    # Return the modified array\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Define the bit mask for 64-bit integers\n    bit_mask = 0xFFFFFFFFFFFFFFFF\n    # Initialize the result array\n    result = [0] * len(arr)\n    # Iterate through the array and calculate the bit count for each element\n    for i in range(len(arr)):\n        # Calculate the bit count for the current element\n        count = 0\n        # Iterate through the 64 bits of the element\n        for j in range(64):\n            # Check if the j-th bit of the element is set to 1\n            if arr[i] & (1 << j):\n                # Increment the bit count\n                count += 1\n        # Store the bit count in the result array\n        result[i] = count\n    # Return the result array\n    return result"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Initialize the explainer\n    explainer = explainers.ExplainQuery(frame, q, qf, mm, pf, pf2, pf3, q_op, similarity)\n\n    # Perform the search\n    scores, explain = explainer.explain()\n\n    return scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    if pf is None:\n        pf = []\n\n    if pf2 is None:\n        pf2 = []\n\n    if pf3 is None:\n        pf3 = []\n\n    if q_op not in [\"OR\", \"AND\"]:\n        q_op = \"OR\"\n\n    qf_str = \" \".join(qf)\n    pf_str = \" \".join(pf)\n    pf2_str = \" \".join(pf2)\n    pf3_str = \" \".join(pf3)\n\n    query = f'{qf_str}~{q}^100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Calculate the number of query terms\n    n_q = len(q.split())\n\n    # Calculate the number of query fields\n    n_qf = len(qf)\n\n    # Calculate the number of phrase fields\n    n_pf = len(pf) if pf else 0\n\n    # Calculate the number of bigram fields\n    n_pf2 = len(pf2) if pf2 else 0\n\n    # Calculate the number of trigram fields\n    n_pf3 = len(pf3) if pf3 else 0\n\n    # Calculate the number of fields\n    n_f = n_qf + n_pf + n_pf2 + n_pf3\n\n    # Calculate the number of query terms in the phrase fields\n    n_qf_pf = sum([len(pf_i.split()) for pf_i in pf]) if pf else 0\n\n    # Calculate the number of query terms in the bigram fields\n    n_qf_pf2 = sum([len(pf2_i.split()) for pf2_i in pf2]) if pf2 else 0\n\n    # Calculate the number of query terms in the trigram fields\n    n_qf_pf3 = sum([len(pf3_i.split()) for pf3_i in pf3]) if pf3 else 0\n\n    # Calculate the number of query terms in the query fields\n    n_qf_qf = n_qf - n_qf_pf - n_qf_pf2 - n_qf_pf3\n\n    # Calculate the number of query terms in the query fields\n    n_qf_qf = n_qf - n_qf_pf - n_qf_pf2 - n_qf_pf3\n\n    # Calculate the number of query terms in the query fields\n    n_qf_qf = n_qf - n_qf_pf - n_qf_pf2 - n_qf_pf3\n\n    # Calculate the number of query terms in the query fields"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Create a new index\n    index = Index()\n\n    # Add the DataFrame to the index\n    index.add_documents(frame)\n\n    # Perform the search\n    results = index.search(\n        q,\n        qf=qf,\n        mm=mm,\n        pf=pf,\n        pf2=pf2,\n        pf3=pf3,\n        q_op=q_op,\n        similarity=similarity,\n    )\n\n    # Get the scores and explanation\n    scores = results.get_scores()\n    explanation = results.get_explanation()\n\n    return scores, explanation\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"1<1<2\"\n\n    if pf is None:\n        pf = []\n\n    if pf2 is None:\n        pf2 = []\n\n    if pf3 is None:\n        pf3 = []\n\n    if len(pf) > 0:\n        pf = \" \".join(pf)\n\n    if len(pf2) > 0:\n        pf2 = \" \".join(pf2)\n\n    if len(pf3) > 0:\n        pf3 = \" \".join(pf3)\n\n    qf = \" \".join(qf)\n\n    q = f\"({q})\"\n\n    q = f\"{q}~{mm}~{q_op}\"\n\n    if len(pf) > 0:\n        q = f\"{q}~{pf}\"\n\n    if len(pf2) > 0:\n        q = f\"{q}~{pf2}\"\n\n    if len(pf3) > 0:\n        q = f\"{q}~{pf3}\"\n\n    q = f\"{q}~{q_op}\"\n\n    frame[\"_score\"] = frame.apply(lambda x: similarity(x, q), axis=1)\n\n    frame = frame.sort_values(\"_score\", ascending=False)\n\n    return frame[\"_score\"].values, f\"{similarity.explanation} {mm} {q_op} {pf} {pf2} {pf3}\""}
{"namespace": "searcharray.solr.edismax", "completion": "    # check if qf is empty\n    if not qf:\n        raise ValueError(\"qf cannot be empty\")\n\n    # check if pf, pf2, and pf3 are all empty\n    if not pf and not pf2 and not pf3:\n        raise ValueError(\"At least one of pf, pf2, or pf3 must be specified\")\n\n    # check if mm is a valid value\n    if mm not in [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"]:\n        raise ValueError(\"mm must be a valid value\")\n\n    # check if q_op is a valid value\n    if q_op not in [\"OR\", \"AND\"]:\n        raise ValueError(\"q_op must be either 'OR' or 'AND'\")\n\n    # check if q is empty\n    if not q:\n        raise ValueError(\"q cannot be empty\")\n\n    # check if qf, pf, pf2, and pf3 are all lists\n    if not all(isinstance(x, list) for x in [qf, pf, pf2, pf3]):\n        raise ValueError(\"qf, pf, pf2, and pf3 must be lists\")\n\n    # check if qf, pf, pf2, and pf3 are not empty\n    if not all(x for x in [qf, pf, pf2, pf3]):\n        raise ValueError(\"qf, pf, pf2, and pf3 cannot be empty\")\n\n    # check if qf, pf, pf2, and pf3 are not all empty\n    if all(not x for x in [qf, pf, pf2, pf3]):\n        raise ValueError(\"qf, pf, pf2, and pf3 cannot all be empty\")\n\n    # check if qf, pf, pf2, and pf3 are not all the same\n    if len(set([qf, pf, pf2, pf3])) == 1:\n        raise Value"}
{"namespace": "searcharray.solr.edismax", "completion": "    # define the query\n    q = f\"{q}~{q_op}\"\n\n    # define the query fields\n    qf = \",\".join(qf)\n\n    # define the minimum match specification\n    if mm:\n        mm = f\"+({mm})\"\n    else:\n        mm = \"\"\n\n    # define the phrase fields\n    if pf:\n        pf = \",\".join(pf)\n    else:\n        pf = \"\"\n\n    # define the bigram fields\n    if pf2:\n        pf2 = \",\".join(pf2)\n    else:\n        pf2 = \"\"\n\n    # define the trigram fields\n    if pf3:\n        pf3 = \",\".join(pf3)\n    else:\n        pf3 = \"\"\n\n    # define the query string\n    query_string = f\"{q}~{mm}~{pf}~{pf2}~{pf3}\"\n\n    # calculate the scores\n    scores = similarity(frame, query_string)\n\n    # return the scores and explanation\n    return scores, f\"{q}~{mm}~{pf}~{pf2}~{pf3}\""}
{"namespace": "searcharray.solr.edismax", "completion": "    # Create a new index\n    index = Index()\n\n    # Add the dataframe to the index\n    index.add_documents(frame.to_dict(orient=\"records\"))\n\n    # Define the query\n    query = Query(\n        q,\n        qf=qf,\n        mm=mm,\n        pf=pf,\n        pf2=pf2,\n        pf3=pf3,\n        q_op=q_op,\n    )\n\n    # Search for the query\n    results = index.search(query)\n\n    # Calculate the relevance scores for the results\n    scores = np.array([similarity.score(result.text) for result in results])\n\n    # Return the scores and an explanation of how they were computed\n    return scores, similarity.explain(results)"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Create a query object\n    query = Query(q, qf, mm, pf, pf2, pf3, q_op)\n\n    # Get the number of documents in the DataFrame\n    n_docs = len(frame)\n\n    # Initialize a list to store the scores for each document\n    scores = []\n\n    # Iterate over each document in the DataFrame\n    for idx, doc in frame.iterrows():\n\n        # Get the document's text\n        text = doc[\"text\"]\n\n        # Calculate the score for the document\n        score = similarity(query, text)\n\n        # Append the score to the list of scores\n        scores.append(score)\n\n    # Convert the list of scores to a numpy array\n    scores = np.array(scores)\n\n    # Create an explanation of the scoring\n    explanation = f\"The relevance scores were calculated using the {similarity.__name__} similarity measure.\"\n\n    # Return the scores and the explanation\n    return scores, explanation"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Create a copy of the DataFrame to avoid modifying the original\n    frame = frame.copy()\n\n    # Create a new column with the search query\n    frame['query'] = q\n\n    # Create a new column with the query fields\n    frame['qf'] = qf\n\n    # Create a new column with the minimum match specification\n    frame['mm'] = mm\n\n    # Create a new column with the phrase fields\n    frame['pf'] = pf\n\n    # Create a new column with the bigram fields\n    frame['pf2'] = pf2\n\n    # Create a new column with the trigram fields\n    frame['pf3'] = pf3\n\n    # Create a new column with the query operator\n    frame['q_op'] = q_op\n\n    # Create a new column with the similarity measure\n    frame['similarity'] = similarity\n\n    # Create a new column with the relevance scores\n    frame['relevance'] = frame.apply(lambda x: similarity(x['query'], x['qf'], x['mm'], x['pf'], x['pf2'], x['pf3'], x['q_op']), axis=1)\n\n    # Sort the DataFrame by relevance scores in descending order\n    frame = frame.sort_values('relevance', ascending=False)\n\n    # Return the relevance scores and an explanation of how they were computed\n    return frame['relevance'].values, \"The relevance scores were calculated using the similarity measure '{}'.\".format(similarity.name)"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = f\"{len(q.split(' '))}<{len(q.split(' '))}\"\n\n    if pf is None:\n        pf = []\n\n    if pf2 is None:\n        pf2 = []\n\n    if pf3 is None:\n        pf3 = []\n\n    query = {\n        \"query\": {\n            \"bool\": {\n                \"must\": [\n                    {\n                        \"dis_max\": {\n                            \"queries\": [\n                                {\n                                    \"multi_match\": {\n                                        \"query\": q,\n                                        \"fields\": qf,\n                                        \"type\": \"cross_fields\",\n                                        \"operator\": q_op,\n                                        \"minimum_should_match\": mm\n                                    }\n                                }\n                            ],\n                            \"tie_breaker\": 0.0\n                        }\n                    },\n                    {\n                        \"dis_max\": {\n                            \"queries\": [\n                                {\n                                    \"match_phrase\": {\n                                        \"query\": q,\n                                        \"fields\": pf,\n                                        \"slop\": 0\n                                    }\n                                }\n                            ],\n                            \"tie_breaker\": 0.0\n                        }\n                    },\n                    {\n                        \"dis_max\": {\n                            \"queries\": [\n                                {\n                                    \"match_phrase\": {\n                                        \"query\": q,\n                                        \"fields\": pf2,\n                                        \"slop\": 1\n                                    }\n                                }\n                            ],\n                            \"tie_breaker\": 0.0\n                        }\n                    },\n                    {\n                        \"dis_max\": {\n                            \"queries\": [\n                                {\n                                    \"match_phrase\": {\n                                        \"query\": q,\n                                        \"fields\": pf3,\n                                        \"slop\": 2\n                                    }\n                                }\n                            ],\n                            \"tie_breaker\": 0.0\n                        }\n                    }\n                ]\n            }\n        }\n    }\n\n    response = frame.search(query, similarity=similarity)\n    scores = response.get_opensearch_response()[\"hits\"][\"hits"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Define the query using the qf and mm parameters\n    query = f\"{q}\"\n    if mm:\n        query += f\"~{mm}\"\n\n    # Define the fields to search for phrase matches\n    if pf:\n        query += \" \" + \" \".join(f\"\\\"{field}:{q}\\\"^10\" for field in pf)\n\n    # Define the fields to search for bigram matches\n    if pf2:\n        query += \" \" + \" \".join(f\"\\\"{field}:{q}\\\"^10\" for field in pf2)\n\n    # Define the fields to search for trigram matches\n    if pf3:\n        query += \" \" + \" \".join(f\"\\\"{field}:{q}\\\"^10\" for field in pf3)\n\n    # Define the fields to search against\n    fields = \" \".join(qf)\n\n    # Perform the search using the query and fields\n    scores, explanation = similarity(frame, query, fields, q_op)\n\n    # Return the scores and explanation\n    return scores, explanation"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n    if pf is None:\n        pf = []\n    if pf2 is None:\n        pf2 = []\n    if pf3 is None:\n        pf3 = []\n\n    qf_str = \" \".join(qf)\n    pf_str = \" \".join(pf)\n    pf2_str = \" \".join(pf2)\n    pf3_str = \" \".join(pf3)\n\n    q_str = f\"{q_op} ({qf_str}) ({pf_str}) ({pf2_str}) ({pf3_str})\"\n\n    query = {\n        \"query\": {\n            \"edismax\": {\n                \"query\": q_str,\n                \"tie\": 0.0,\n                \"mm\": mm,\n                \"qf\": qf_str,\n                \"pf\": pf_str,\n                \"pf2\": pf2_str,\n                \"pf3\": pf3_str,\n            }\n        }\n    }\n\n    response = es.search(index=\"test-index\", body=query)\n\n    scores = np.array([hit[\"_score\"] for hit in response[\"hits\"][\"hits\"]])\n\n    return scores, f\"{q_str} {mm} {q_op}\"\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Create a new Elasticsearch client\n    client = Elasticsearch(hosts=[f\"http://{os.environ['ELASTIC_HOST']}:9200\"],\n                           basic_auth=(os.environ[\"ELASTIC_USER\"], os.environ[\"ELASTIC_PASSWORD\"]))\n\n    # Define the query parameters\n    query = {\n        \"query\": {\n            \"bool\": {\n                \"must\": [\n                    {\n                        \"dis_max\": {\n                            \"queries\": [\n                                {\n                                    \"multi_match\": {\n                                        \"query\": q,\n                                        \"fields\": qf,\n                                        \"type\": \"best_fields\",\n                                        \"operator\": q_op,\n                                        \"minimum_should_match\": mm\n                                    }\n                                }\n                            ]\n                        }\n                    }\n                ]\n            }\n        },\n        \"explain\": True,\n        \"size\": 10000\n    }\n\n    # If phrase fields are provided, add them to the query\n    if pf:\n        query[\"query\"][\"bool\"][\"must\"].append({\n            \"dis_max\": {\n                \"queries\": [\n                    {\n                        \"match_phrase\": {\n                            \"query\": q,\n                            \"fields\": pf\n                        }\n                    }\n                ]\n            }\n        })\n\n    # If bigram fields are provided, add them to the query\n    if pf2:\n        query[\"query\"][\"bool\"][\"must\"].append({\n            \"dis_max\": {\n                \"queries\": [\n                    {\n                        \"match_phrase\": {\n                            \"query\": q,\n                            \"fields\": pf2,\n                            \"slop\": 1\n                        }\n                    }\n                ]\n            }\n        })\n\n    # If trigram fields are provided, add them to the query\n    if pf3:\n        query[\"query\"][\"bool\"][\"must\"].append({\n            \"dis_max\": {\n                \"queries\": [\n                    {\n                        \"match_phrase\": {\n                            \"query\": q,\n                            \"fields\": pf3"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Convert query to string\n    q = str(q)\n\n    # Define the query parser\n    parser = QueryParser(\"_all\", schema=frame.index.name)\n\n    # Set the default operator for the query\n    parser.set_default_operator(q_op)\n\n    # Set the minimum match specification\n    if mm is not None:\n        parser.set_minimum_should_match(mm)\n\n    # Set the query fields\n    parser.add_fields(*qf)\n\n    # Set the phrase fields\n    if pf is not None:\n        parser.add_phrase_fields(*pf)\n\n    # Set the bigram fields\n    if pf2 is not None:\n        parser.add_bigram_fields(*pf2)\n\n    # Set the trigram fields\n    if pf3 is not None:\n        parser.add_trigram_fields(*pf3)\n\n    # Parse the query\n    query = parser.parse(q)\n\n    # Perform the search\n    result = frame.search(query, similarity=similarity)\n\n    # Return the search results\n    return result.scores, result.explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Create a query string for the search\n    query = \" \".join([f\"({q})~{qf}\" for qf in qf])\n\n    # Check if minimum match specification is provided\n    if mm is not None:\n        query += f\"~{mm}\"\n\n    # Check if phrase fields are provided\n    if pf is not None:\n        query += \" \" + \" \".join([f\"({q})~{pf}\" for pf in pf])\n\n    # Check if bigram fields are provided\n    if pf2 is not None:\n        query += \" \" + \" \".join([f\"({q})~{pf2}\" for pf2 in pf2])\n\n    # Check if trigram fields are provided\n    if pf3 is not None:\n        query += \" \" + \" \".join([f\"({q})~{pf3}\" for pf3 in pf3])\n\n    # Check if query operator is provided\n    if q_op == \"AND\":\n        query = f\"+({query})\"\n\n    # Perform the search\n    scores, explanation = similarity(frame, query)\n\n    # Return the scores and explanation\n    return scores, explanation\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = 1\n\n    if pf is None:\n        pf = []\n\n    if pf2 is None:\n        pf2 = []\n\n    if pf3 is None:\n        pf3 = []\n\n    if len(qf) == 0:\n        raise ValueError(\"qf must be a non-empty list\")\n\n    if len(pf) == 0 and len(pf2) == 0 and len(pf3) == 0:\n        raise ValueError(\"At least one of pf, pf2, or pf3 must be non-empty\")\n\n    if q_op not in [\"OR\", \"AND\"]:\n        raise ValueError(\"q_op must be either 'OR' or 'AND'\")\n\n    if not isinstance(similarity, Similarity):\n        raise TypeError(\"similarity must be a subclass of Similarity\")\n\n    # Create a new Elasticsearch client\n    es = Elasticsearch()\n\n    # Define the search query\n    query = {\n        \"query\": {\n            \"bool\": {\n                \"should\": [\n                    {\n                        \"dis_max\": {\n                            \"queries\": [\n                                {\n                                    \"match\": {\n                                        \"query\": {\n                                            \"query\": q,\n                                            \"operator\": q_op,\n                                            \"fuzziness\": \"AUTO\",\n                                            \"max_expansions\": 10,\n                                            \"prefix_length\": 1,\n                                            \"fuzzy_transpositions\": True,\n                                            \"lenient\": True,\n                                            \"zero_terms_query\": \"NONE\",\n                                            \"boost\": 10,\n                                        }\n                                    }\n                                },\n                                {\n                                    \"match\": {\n                                        \"query\": {\n                                            \"query\": q,\n                                            \"operator\": q_op,\n                                            \"fuzziness\": \"AUTO\",\n                                            \"max_expansions\": 10,\n                                            \"prefix_length\": 1,\n                                            \"fuzzy_transpositions\": True,\n                                            \"lenient\": True,\n                                            \"zero_terms_query\": \"NONE\",\n                                            \"boost\": 5"}
{"namespace": "searcharray.solr.edismax", "completion": "    if pf is None:\n        pf = []\n    if pf2 is None:\n        pf2 = []\n    if pf3 is None:\n        pf3 = []\n\n    if mm is not None:\n        q = f\"+({q})~{mm}\"\n\n    if pf:\n        pf = \" \".join(pf)\n        q = f\"{q} +({pf})\"\n\n    if pf2:\n        pf2 = \" \".join(pf2)\n        q = f\"{q} +({pf2})\"\n\n    if pf3:\n        pf3 = \" \".join(pf3)\n        q = f\"{q} +({pf3})\"\n\n    qf = \" \".join(qf)\n    qf = f\"{qf}^1000\"\n\n    q = f\"{qf} {q}\"\n\n    if q_op == \"AND\":\n        q = f\"+({q})\"\n\n    scores = similarity.get_scores(frame, q)\n    scores = scores.reshape(-1)\n\n    return scores, f\"{q} {q_op}\"\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"1<100%>\"\n\n    if pf is None:\n        pf = []\n\n    if pf2 is None:\n        pf2 = []\n\n    if pf3 is None:\n        pf3 = []\n\n    if q_op == \"AND\":\n        q_op = \"+\"\n    elif q_op == \"OR\":\n        q_op = \"|\"\n\n    qf = \",\".join(qf)\n    pf = \",\".join(pf)\n    pf2 = \",\".join(pf2)\n    pf3 = \",\".join(pf3)\n\n    query = f\"{q}~{q_op}\"\n\n    if pf:\n        query += f\"~{pf}\"\n\n    if pf2:\n        query += f\"~{pf2}\"\n\n    if pf3:\n        query += f\"~{pf3}\"\n\n    if mm:\n        query += f\"~{mm}\"\n\n    query += \"~\"\n\n    q = query\n\n    # Use the similarity measure to compute the relevance scores\n    scores = similarity(frame, q)\n\n    # Return the scores and an explanation of how they were computed\n    return scores, f\"{q} {similarity.__name__}\"\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = f\"{len(q.split())}<100%\"\n\n    if pf is None:\n        pf = []\n    if pf2 is None:\n        pf2 = []\n    if pf3 is None:\n        pf3 = []\n\n    qf = \" \".join(qf)\n    pf = \" \".join(pf)\n    pf2 = \" \".join(pf2)\n    pf3 = \" \".join(pf3)\n\n    qf = f\"{qf}^1.0\"\n    pf = f\"{pf}^1.5\"\n    pf2 = f\"{pf2}^1.0\"\n    pf3 = f\"{pf3}^1.0\"\n\n    qf = f\"{qf} {pf} {pf2} {pf3}\"\n\n    qf = qf.replace(\" \", \" OR \")\n\n    q = f\"({q})\"\n\n    q = f\"{q}~{mm}\"\n\n    q = f\"{q}~{q_op}\"\n\n    frame[\"_score\"] = frame.apply(lambda x: similarity(x, q), axis=1)\n\n    frame = frame.sort_values(\"_score\", ascending=False)\n\n    frame = frame.reset_index(drop=True)\n\n    scores = frame[\"_score\"].values\n\n    explanation = f\"\"\"\n    The search results are sorted by their relevance scores in descending order.\n    The relevance scores are calculated using the BM25 similarity measure, which is a probabilistic scoring function that takes into account the term frequency, document frequency, and inverse document frequency of each term in the query and document.\n    The BM25 similarity measure is based on the Okapi BM25 ranking function, which is a scoring function used in information retrieval systems to rank documents based on their relevance to a given query.\n    The BM25 similarity measure is a combination of two components: the term frequency (tf) component, which measures how frequently a term appears in a document, and the inverse document frequency ("}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.close()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.close()\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.close()\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.close()\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = process.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.close()\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = process.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.close()\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()\n        else:\n            raise ValueError(\"Invalid message type\")\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()\n        else:\n            raise ValueError(\"Invalid message type\")\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message.data, CloseMessage):\n            self.connection.close()\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self.connection.c2s(message.data)\n        elif isinstance(message.type == RecvMessage):\n            message.data = self.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.connection.close()\n        else:\n            raise NotImplementedError()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections:\n            connection.stop()\n\n        # Stop the server\n        if self.server:\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections:\n            connection.stop()\n\n        # Stop the server\n        if self.server:\n            self.server.stop()\n\n        # Clear the connections list\n        self.connections.clear()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections:\n            connection.stop()\n\n        # Stop the server\n        if self.server is not None:\n            self.server.stop()\n\n        # Clear the connections list\n        self.connections.clear()\n\n        # Set the server attribute to None\n        self.server = None"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections:\n            connection.stop()\n\n        # Stop the server\n        if self.server:\n            self.server.stop()\n\n        # Clear the connections and server attributes\n        self.connections = []\n        self.server = None"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.close()\n        if self.server is not None:\n            self.server.shutdown()\n            self.server.server_close()\n            self.server = None\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections.\n        for connection in self.connections:\n            connection.stop()\n\n        # Stop the server.\n        if self.server is not None:\n            self.server.stop()\n\n        # Close the logger.\n        self.logger.close()\n\n        # Stop the thread.\n        self.thread.stop()\n\n        # Remove the interceptor from the Proxifier instance.\n        self.proxifier.remove_interceptor(self)\n\n        # Remove the interceptor from the Proxifier instance.\n        self.proxifier.remove_interceptor(self)\n\n        # Remove the interceptor from the Proxifier instance.\n        self.proxifier.remove_interceptor(self)\n\n        # Remove the interceptor from the Proxifier instance.\n        self.proxifier.remove_interceptor(self)\n\n        # Remove the interceptor from the Proxifier instance.\n        self.proxifier.remove_interceptor(self)\n\n        # Remove the interceptor from the Proxifier instance.\n        self.proxifier.remove_interceptor(self)\n\n        # Remove the interceptor from the Proxifier instance.\n        self.proxifier.remove_interceptor(self)\n\n        # Remove the interceptor from the Proxifier instance.\n        self.proxifier.remove_interceptor(self)\n\n        # Remove the interceptor from the Proxifier instance.\n        self.proxifier.remove_interceptor(self)\n\n        # Remove the interceptor from the Proxifier instance.\n        self.proxifier.remove_interceptor(self)\n\n        # Remove the interceptor from the Proxifier instance.\n        self.proxifier.remove_interceptor(self)\n\n        # Remove the interceptor from the Proxifier instance.\n        self.proxifier.remove_interceptor(self)\n\n        # Remove the interceptor from the Proxifier instance.\n        self.proxifier.remove_interceptor(self)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for conn in self.connections:\n            conn.close()\n        if self.server is not None:\n            self.server.close()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections:\n            connection.close()\n\n        # Stop the server\n        if self.server:\n            self.server.stop()\n\n        # Set the server and connections to None\n        self.server = None\n        self.connections = []\n\n        # Print a message indicating that the ProxifierMessageInterceptor instance has been destroyed\n        print(\"ProxifierMessageInterceptor destroyed\")\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for conn in self.connections:\n            conn.close()\n\n        # Stop the server\n        if self.server is not None:\n            self.server.stop()\n\n        # Reset the connections and server attributes\n        self.connections = []\n        self.server = None\n\n        print(\"ProxifierMessageInterceptor destroyed\")\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n        if self.server:\n            self.server.stop()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections:\n            connection.stop()\n\n        # Stop the server\n        if self.server is not None:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections:\n            connection.stop()\n        # Stop the server if it exists\n        if self.server:\n            self.server.stop()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.close()\n\n        if self.server is not None:\n            self.server.shutdown()\n            self.server.server_close()\n\n        self.server = None\n        self.connections = []\n\n        return None\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.close()\n        if self.server:\n            self.server.shutdown()\n            self.server.server_close()\n        self.server = None\n        self.connections = []\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.close()\n\n        if self.server:\n            self.server.shutdown()\n            self.server.server_close()\n\n        self.connections = []\n        self.server = None\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.close()\n        if self.server is not None:\n            self.server.shutdown()\n            self.server.server_close()\n\n        self.connections = []\n        self.server = None\n\n        logging.info(\"ProxifierMessageInterceptor destroyed.\")\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections:\n            connection.stop()\n\n        # Stop the server if it exists\n        if self.server is not None:\n            self.server.stop()\n\n        # Reset the connections and server attributes\n        self.connections = []\n        self.server = None\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.close()\n        if self.server is not None:\n            self.server.close()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.close()\n        if self.server:\n            self.server.server_close()\n            self.server.shutdown()\n        self.server = None\n        self.connections = []\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections:\n            connection.stop()\n\n        if self.server is not None:\n            self.server.stop()\n\n        self.server = None\n        self.connections = []\n"}
